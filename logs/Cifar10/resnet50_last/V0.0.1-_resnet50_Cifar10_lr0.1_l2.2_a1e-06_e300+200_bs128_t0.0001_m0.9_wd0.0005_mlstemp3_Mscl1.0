V0.0.1-_resnet50_Cifar10_lr0.1_l2.2_a1e-06_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5254763960838318, Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.24196940660476685, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.15759095549583435, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.13501641154289246, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.3461485505104065, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.17473100125789642, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21617008745670319, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.14946585893630981, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09060623496770859, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08498729020357132, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11497705429792404, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11804789304733276, Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08379501849412918, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1424030363559723, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.19753389060497284, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.16684924066066742, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.22829987108707428, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12801074981689453, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09603530913591385, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06901206821203232, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12272872775793076, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0835055485367775, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.07954221963882446, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12703275680541992, Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.19747452437877655, Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.15407174825668335, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1602816879749298, Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.10645194351673126, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10600411146879196, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.13483507931232452, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.21709460020065308, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11353497207164764, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0660422295331955, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10686782747507095, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06808818876743317, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.05323619768023491, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.08759226649999619, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07965513318777084, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06643471866846085, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12164679169654846, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09185265004634857, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06330689787864685, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1110600158572197, Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12582343816757202, Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.09035182744264603, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07410024106502533, Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.07018566876649857, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0687103122472763, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.04065759852528572, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.03755198046565056, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.04725675657391548, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.045549724251031876, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06192586570978165, Linear(in_features=2048, out_features=100, bias=True): 0.44340774416923523}
current lr 1.00000e-01
Grad=  tensor(7263.2817, device='cuda:0')
Epoch: [0][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 7.4151 (7.4151) ([4.773]+[2.642])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 5.8732 (8.2099) ([2.404]+[3.469])	Prec@1 15.625 (10.566)
Epoch: [0][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 5.2958 (6.9446) ([2.225]+[3.071])	Prec@1 16.406 (11.602)
Epoch: [0][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 4.8092 (6.3338) ([2.104]+[2.705])	Prec@1 14.844 (13.138)
Test: [0/79]	Time 0.169 (0.169)	Loss 4.5930 (4.5930) ([2.194]+[2.399])	Prec@1 19.531 (19.531)
 * Prec@1 19.890
current lr 1.00000e-01
Grad=  tensor(0.5925, device='cuda:0')
Epoch: [1][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 4.3552 (4.3552) ([1.956]+[2.399])	Prec@1 18.750 (18.750)
Epoch: [1][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 3.9306 (4.1832) ([1.838]+[2.092])	Prec@1 30.469 (25.789)
Epoch: [1][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 3.8205 (4.0064) ([1.996]+[1.824])	Prec@1 26.562 (26.990)
Epoch: [1][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 3.3749 (3.8432) ([1.775]+[1.600])	Prec@1 36.719 (28.693)
Test: [0/79]	Time 0.160 (0.160)	Loss 3.2715 (3.2715) ([1.823]+[1.448])	Prec@1 38.281 (38.281)
 * Prec@1 34.300
current lr 1.00000e-01
Grad=  tensor(0.6892, device='cuda:0')
Epoch: [2][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 3.1955 (3.1955) ([1.747]+[1.448])	Prec@1 33.594 (33.594)
Epoch: [2][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 2.8837 (3.0956) ([1.560]+[1.323])	Prec@1 42.969 (36.417)
Epoch: [2][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 3.0273 (2.9918) ([1.846]+[1.181])	Prec@1 33.594 (37.457)
Epoch: [2][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 2.6861 (2.9052) ([1.618]+[1.069])	Prec@1 42.969 (38.600)
Test: [0/79]	Time 0.166 (0.166)	Loss 2.4793 (2.4793) ([1.517]+[0.963])	Prec@1 44.531 (44.531)
 * Prec@1 39.990
current lr 1.00000e-01
Grad=  tensor(0.9292, device='cuda:0')
Epoch: [3][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 2.6097 (2.6097) ([1.647]+[0.963])	Prec@1 39.844 (39.844)
Epoch: [3][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 2.2971 (2.4471) ([1.424]+[0.874])	Prec@1 51.562 (43.031)
Epoch: [3][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 2.2728 (2.4031) ([1.425]+[0.848])	Prec@1 49.219 (43.742)
Epoch: [3][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 2.3329 (2.3537) ([1.587]+[0.746])	Prec@1 39.062 (44.666)
Test: [0/79]	Time 0.167 (0.167)	Loss 2.0279 (2.0279) ([1.338]+[0.690])	Prec@1 51.562 (51.562)
 * Prec@1 48.060
current lr 1.00000e-01
Grad=  tensor(1.0947, device='cuda:0')
Epoch: [4][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 2.0559 (2.0559) ([1.366]+[0.690])	Prec@1 53.125 (53.125)
Epoch: [4][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.9302 (2.0466) ([1.293]+[0.637])	Prec@1 53.125 (48.948)
Epoch: [4][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.9064 (1.9876) ([1.318]+[0.588])	Prec@1 52.344 (50.358)
Epoch: [4][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 1.8977 (1.9422) ([1.341]+[0.556])	Prec@1 49.219 (51.282)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.8738 (1.8738) ([1.345]+[0.529])	Prec@1 50.000 (50.000)
 * Prec@1 51.240
current lr 1.00000e-01
Grad=  tensor(1.2885, device='cuda:0')
Epoch: [5][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 1.6309 (1.6309) ([1.102]+[0.529])	Prec@1 54.688 (54.688)
Epoch: [5][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.8336 (1.7324) ([1.327]+[0.507])	Prec@1 55.469 (55.577)
Epoch: [5][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.7496 (1.7020) ([1.269]+[0.481])	Prec@1 48.438 (56.487)
Epoch: [5][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.4794 (1.6691) ([1.016]+[0.463])	Prec@1 64.844 (57.319)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.6868 (1.6868) ([1.247]+[0.440])	Prec@1 54.688 (54.688)
 * Prec@1 56.020
current lr 1.00000e-01
Grad=  tensor(1.7054, device='cuda:0')
Epoch: [6][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 1.5667 (1.5667) ([1.127]+[0.440])	Prec@1 58.594 (58.594)
Epoch: [6][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.4591 (1.5267) ([1.030]+[0.429])	Prec@1 64.844 (61.069)
Epoch: [6][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.6199 (1.5264) ([1.162]+[0.458])	Prec@1 58.594 (61.828)
Epoch: [6][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.7040 (1.5199) ([1.270]+[0.434])	Prec@1 51.562 (61.859)
Test: [0/79]	Time 0.168 (0.168)	Loss 2.0424 (2.0424) ([1.633]+[0.409])	Prec@1 45.312 (45.312)
 * Prec@1 46.950
current lr 1.00000e-01
Grad=  tensor(1.2190, device='cuda:0')
Epoch: [7][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 1.3285 (1.3285) ([0.919]+[0.409])	Prec@1 68.750 (68.750)
Epoch: [7][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.4148 (1.4073) ([1.020]+[0.395])	Prec@1 66.406 (64.759)
Epoch: [7][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.4893 (1.3985) ([1.105]+[0.384])	Prec@1 58.594 (64.762)
Epoch: [7][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.1633 (1.3841) ([0.789]+[0.374])	Prec@1 73.438 (64.888)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.4169 (1.4169) ([1.047]+[0.370])	Prec@1 58.594 (58.594)
 * Prec@1 57.330
current lr 1.00000e-01
Grad=  tensor(2.2230, device='cuda:0')
Epoch: [8][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 1.4973 (1.4973) ([1.128]+[0.370])	Prec@1 59.375 (59.375)
Epoch: [8][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.3959 (1.3241) ([1.027]+[0.369])	Prec@1 71.094 (66.244)
Epoch: [8][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 1.2024 (1.3144) ([0.841]+[0.362])	Prec@1 67.969 (66.430)
Epoch: [8][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.3808 (1.3022) ([1.021]+[0.360])	Prec@1 64.062 (66.663)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.4089 (1.4089) ([1.056]+[0.353])	Prec@1 64.844 (64.844)
 * Prec@1 61.270
current lr 1.00000e-01
Grad=  tensor(1.6754, device='cuda:0')
Epoch: [9][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 1.3302 (1.3302) ([0.977]+[0.353])	Prec@1 64.844 (64.844)
Epoch: [9][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.3097 (1.2281) ([0.963]+[0.347])	Prec@1 64.062 (69.369)
Epoch: [9][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.9453 (1.2171) ([0.609]+[0.337])	Prec@1 78.906 (69.349)
Epoch: [9][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.2211 (1.2110) ([0.881]+[0.341])	Prec@1 70.312 (69.498)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.4080 (1.4080) ([1.069]+[0.339])	Prec@1 64.844 (64.844)
 * Prec@1 58.950
current lr 1.00000e-01
Grad=  tensor(2.0017, device='cuda:0')
Epoch: [10][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 1.1332 (1.1332) ([0.794]+[0.339])	Prec@1 66.406 (66.406)
Epoch: [10][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.2127 (1.1902) ([0.876]+[0.336])	Prec@1 70.312 (70.104)
Epoch: [10][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.1089 (1.1773) ([0.775]+[0.333])	Prec@1 72.656 (70.686)
Epoch: [10][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9859 (1.1560) ([0.658]+[0.328])	Prec@1 78.906 (71.333)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.4723 (1.4723) ([1.142]+[0.331])	Prec@1 65.625 (65.625)
 * Prec@1 58.380
current lr 1.00000e-01
Grad=  tensor(2.1351, device='cuda:0')
Epoch: [11][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 1.1750 (1.1750) ([0.844]+[0.331])	Prec@1 67.969 (67.969)
Epoch: [11][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.1265 (1.1261) ([0.801]+[0.325])	Prec@1 69.531 (71.937)
Epoch: [11][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0998 (1.1310) ([0.771]+[0.329])	Prec@1 72.656 (71.999)
Epoch: [11][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9426 (1.1118) ([0.621]+[0.321])	Prec@1 76.562 (72.682)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.1957 (1.1957) ([0.876]+[0.320])	Prec@1 66.406 (66.406)
 * Prec@1 65.100
current lr 1.00000e-01
Grad=  tensor(1.5283, device='cuda:0')
Epoch: [12][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 1.0023 (1.0023) ([0.683]+[0.320])	Prec@1 74.219 (74.219)
Epoch: [12][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.1061 (1.0776) ([0.782]+[0.324])	Prec@1 78.125 (74.010)
Epoch: [12][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.1148 (1.0698) ([0.797]+[0.317])	Prec@1 73.438 (73.989)
Epoch: [12][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.1154 (1.0640) ([0.803]+[0.312])	Prec@1 69.531 (74.232)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.0944 (1.0944) ([0.785]+[0.309])	Prec@1 72.656 (72.656)
 * Prec@1 72.300
current lr 1.00000e-01
Grad=  tensor(1.3630, device='cuda:0')
Epoch: [13][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.8699 (0.8699) ([0.560]+[0.309])	Prec@1 82.031 (82.031)
Epoch: [13][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.0582 (1.0311) ([0.752]+[0.306])	Prec@1 77.344 (75.124)
Epoch: [13][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0442 (1.0153) ([0.742]+[0.302])	Prec@1 78.125 (75.501)
Epoch: [13][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.9885 (1.0110) ([0.688]+[0.301])	Prec@1 78.906 (75.664)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.1515 (1.1515) ([0.852]+[0.299])	Prec@1 76.562 (76.562)
 * Prec@1 70.100
current lr 1.00000e-01
Grad=  tensor(1.4991, device='cuda:0')
Epoch: [14][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.8867 (0.8867) ([0.588]+[0.299])	Prec@1 82.031 (82.031)
Epoch: [14][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 1.0091 (0.9963) ([0.714]+[0.296])	Prec@1 70.312 (75.890)
Epoch: [14][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9797 (0.9857) ([0.686]+[0.294])	Prec@1 76.562 (76.116)
Epoch: [14][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9930 (0.9905) ([0.701]+[0.292])	Prec@1 77.344 (75.924)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.3460 (1.3460) ([1.055]+[0.291])	Prec@1 67.188 (67.188)
 * Prec@1 64.410
current lr 1.00000e-01
Grad=  tensor(2.0300, device='cuda:0')
Epoch: [15][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.8998 (0.8998) ([0.609]+[0.291])	Prec@1 78.125 (78.125)
Epoch: [15][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0805 (0.9564) ([0.791]+[0.289])	Prec@1 72.656 (76.810)
Epoch: [15][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0946 (0.9625) ([0.808]+[0.287])	Prec@1 69.531 (76.617)
Epoch: [15][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9770 (0.9624) ([0.692]+[0.285])	Prec@1 71.875 (76.508)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.2077 (1.2077) ([0.924]+[0.284])	Prec@1 68.750 (68.750)
 * Prec@1 70.600
current lr 1.00000e-01
Grad=  tensor(2.1567, device='cuda:0')
Epoch: [16][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.9667 (0.9667) ([0.683]+[0.284])	Prec@1 75.781 (75.781)
Epoch: [16][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.0016 (0.9379) ([0.717]+[0.285])	Prec@1 73.438 (77.398)
Epoch: [16][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0207 (0.9401) ([0.737]+[0.284])	Prec@1 66.406 (77.449)
Epoch: [16][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.0979 (0.9445) ([0.816]+[0.282])	Prec@1 68.750 (77.281)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.0978 (1.0978) ([0.817]+[0.281])	Prec@1 74.219 (74.219)
 * Prec@1 73.660
current lr 1.00000e-01
Grad=  tensor(1.5833, device='cuda:0')
Epoch: [17][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.8547 (0.8547) ([0.574]+[0.281])	Prec@1 80.469 (80.469)
Epoch: [17][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.9618 (0.9415) ([0.681]+[0.281])	Prec@1 75.781 (77.475)
Epoch: [17][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8365 (0.9284) ([0.559]+[0.278])	Prec@1 79.688 (77.736)
Epoch: [17][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9614 (0.9255) ([0.684]+[0.277])	Prec@1 75.781 (77.803)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.1396 (1.1396) ([0.863]+[0.276])	Prec@1 69.531 (69.531)
 * Prec@1 73.310
current lr 1.00000e-01
Grad=  tensor(1.2944, device='cuda:0')
Epoch: [18][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.8589 (0.8589) ([0.583]+[0.276])	Prec@1 82.031 (82.031)
Epoch: [18][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8543 (0.9050) ([0.579]+[0.275])	Prec@1 79.688 (78.489)
Epoch: [18][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.0659 (0.9071) ([0.791]+[0.275])	Prec@1 72.656 (78.343)
Epoch: [18][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8027 (0.9067) ([0.526]+[0.277])	Prec@1 83.594 (78.263)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.9738 (0.9738) ([0.699]+[0.275])	Prec@1 73.438 (73.438)
 * Prec@1 73.770
current lr 1.00000e-01
Grad=  tensor(1.9520, device='cuda:0')
Epoch: [19][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.9098 (0.9098) ([0.635]+[0.275])	Prec@1 78.906 (78.906)
Epoch: [19][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8543 (0.8778) ([0.580]+[0.274])	Prec@1 76.562 (79.007)
Epoch: [19][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8178 (0.8952) ([0.544]+[0.274])	Prec@1 78.906 (78.448)
Epoch: [19][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9853 (0.9001) ([0.710]+[0.275])	Prec@1 78.125 (78.247)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.4861 (1.4861) ([1.212]+[0.274])	Prec@1 65.625 (65.625)
 * Prec@1 64.230
current lr 1.00000e-01
Grad=  tensor(1.5752, device='cuda:0')
Epoch: [20][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.8548 (0.8548) ([0.581]+[0.274])	Prec@1 78.906 (78.906)
Epoch: [20][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8989 (0.8906) ([0.626]+[0.273])	Prec@1 75.000 (78.659)
Epoch: [20][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8622 (0.8872) ([0.592]+[0.270])	Prec@1 79.688 (78.778)
Epoch: [20][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8926 (0.8865) ([0.621]+[0.272])	Prec@1 78.125 (78.865)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0762 (1.0762) ([0.806]+[0.270])	Prec@1 74.219 (74.219)
 * Prec@1 71.580
current lr 1.00000e-01
Grad=  tensor(2.9362, device='cuda:0')
Epoch: [21][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.9314 (0.9314) ([0.661]+[0.270])	Prec@1 73.438 (73.438)
Epoch: [21][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7461 (0.8669) ([0.475]+[0.271])	Prec@1 87.500 (79.370)
Epoch: [21][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9711 (0.8819) ([0.700]+[0.271])	Prec@1 75.781 (79.019)
Epoch: [21][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7724 (0.8776) ([0.503]+[0.270])	Prec@1 85.156 (79.213)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.4387 (1.4387) ([1.169]+[0.270])	Prec@1 64.062 (64.062)
 * Prec@1 65.750
current lr 1.00000e-01
Grad=  tensor(2.2165, device='cuda:0')
Epoch: [22][0/391]	Time 0.262 (0.262)	Data 0.137 (0.137)	Loss 0.9413 (0.9413) ([0.671]+[0.270])	Prec@1 81.250 (81.250)
Epoch: [22][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8736 (0.8682) ([0.605]+[0.269])	Prec@1 81.250 (79.602)
Epoch: [22][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.0043 (0.8780) ([0.735]+[0.269])	Prec@1 73.438 (79.167)
Epoch: [22][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9005 (0.8790) ([0.632]+[0.268])	Prec@1 77.344 (79.026)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.7720 (0.7720) ([0.506]+[0.266])	Prec@1 82.812 (82.812)
 * Prec@1 77.350
current lr 1.00000e-01
Grad=  tensor(1.7848, device='cuda:0')
Epoch: [23][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.8094 (0.8094) ([0.543]+[0.266])	Prec@1 82.031 (82.031)
Epoch: [23][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8210 (0.8566) ([0.554]+[0.267])	Prec@1 82.812 (79.347)
Epoch: [23][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.8069 (0.8599) ([0.541]+[0.266])	Prec@1 84.375 (79.575)
Epoch: [23][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.0775 (0.8622) ([0.811]+[0.267])	Prec@1 72.656 (79.516)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.9240 (0.9240) ([0.658]+[0.266])	Prec@1 79.688 (79.688)
 * Prec@1 76.200
current lr 1.00000e-01
Grad=  tensor(2.1087, device='cuda:0')
Epoch: [24][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.8319 (0.8319) ([0.566]+[0.266])	Prec@1 78.125 (78.125)
Epoch: [24][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8303 (0.8566) ([0.564]+[0.266])	Prec@1 76.562 (79.533)
Epoch: [24][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9474 (0.8590) ([0.681]+[0.267])	Prec@1 75.000 (79.649)
Epoch: [24][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7511 (0.8566) ([0.486]+[0.266])	Prec@1 82.812 (79.682)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0668 (1.0668) ([0.802]+[0.265])	Prec@1 71.094 (71.094)
 * Prec@1 71.710
current lr 1.00000e-01
Grad=  tensor(2.4223, device='cuda:0')
Epoch: [25][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.8774 (0.8774) ([0.612]+[0.265])	Prec@1 77.344 (77.344)
Epoch: [25][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8880 (0.8570) ([0.622]+[0.266])	Prec@1 82.031 (79.788)
Epoch: [25][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9188 (0.8562) ([0.653]+[0.266])	Prec@1 79.688 (79.831)
Epoch: [25][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8404 (0.8521) ([0.575]+[0.266])	Prec@1 81.250 (79.906)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.3357 (1.3357) ([1.071]+[0.265])	Prec@1 67.969 (67.969)
 * Prec@1 69.010
current lr 1.00000e-01
Grad=  tensor(1.8816, device='cuda:0')
Epoch: [26][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.7501 (0.7501) ([0.485]+[0.265])	Prec@1 85.938 (85.938)
Epoch: [26][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.0719 (0.8613) ([0.807]+[0.265])	Prec@1 71.875 (79.695)
Epoch: [26][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8306 (0.8604) ([0.567]+[0.264])	Prec@1 78.906 (79.559)
Epoch: [26][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8906 (0.8528) ([0.626]+[0.265])	Prec@1 78.906 (79.851)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.1861 (1.1861) ([0.921]+[0.265])	Prec@1 67.188 (67.188)
 * Prec@1 68.060
current lr 1.00000e-01
Grad=  tensor(2.0474, device='cuda:0')
Epoch: [27][0/391]	Time 0.263 (0.263)	Data 0.136 (0.136)	Loss 0.8469 (0.8469) ([0.582]+[0.265])	Prec@1 81.250 (81.250)
Epoch: [27][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8704 (0.8172) ([0.607]+[0.263])	Prec@1 80.469 (81.196)
Epoch: [27][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9411 (0.8354) ([0.678]+[0.263])	Prec@1 77.344 (80.519)
Epoch: [27][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 1.0093 (0.8369) ([0.747]+[0.263])	Prec@1 71.875 (80.466)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.2237 (1.2237) ([0.961]+[0.263])	Prec@1 67.188 (67.188)
 * Prec@1 72.020
current lr 1.00000e-01
Grad=  tensor(1.4756, device='cuda:0')
Epoch: [28][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.7148 (0.7148) ([0.452]+[0.263])	Prec@1 85.938 (85.938)
Epoch: [28][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8789 (0.8303) ([0.616]+[0.263])	Prec@1 80.469 (80.399)
Epoch: [28][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8875 (0.8314) ([0.625]+[0.262])	Prec@1 80.469 (80.566)
Epoch: [28][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9140 (0.8382) ([0.651]+[0.263])	Prec@1 79.688 (80.246)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.1408 (1.1408) ([0.878]+[0.263])	Prec@1 75.000 (75.000)
 * Prec@1 72.730
current lr 1.00000e-01
Grad=  tensor(2.1606, device='cuda:0')
Epoch: [29][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.9562 (0.9562) ([0.694]+[0.263])	Prec@1 75.781 (75.781)
Epoch: [29][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9079 (0.8311) ([0.645]+[0.263])	Prec@1 79.688 (80.515)
Epoch: [29][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7489 (0.8245) ([0.486]+[0.263])	Prec@1 82.812 (80.822)
Epoch: [29][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7678 (0.8246) ([0.505]+[0.263])	Prec@1 82.031 (80.746)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.5123 (1.5123) ([1.250]+[0.262])	Prec@1 64.844 (64.844)
 * Prec@1 70.490
current lr 1.00000e-01
Grad=  tensor(1.7651, device='cuda:0')
Epoch: [30][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.7969 (0.7969) ([0.535]+[0.262])	Prec@1 81.250 (81.250)
Epoch: [30][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8103 (0.8350) ([0.547]+[0.263])	Prec@1 79.688 (80.507)
Epoch: [30][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9630 (0.8355) ([0.700]+[0.263])	Prec@1 74.219 (80.379)
Epoch: [30][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9897 (0.8306) ([0.728]+[0.262])	Prec@1 79.688 (80.482)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.0063 (1.0063) ([0.745]+[0.261])	Prec@1 77.344 (77.344)
 * Prec@1 73.230
current lr 1.00000e-01
Grad=  tensor(2.5585, device='cuda:0')
Epoch: [31][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.9197 (0.9197) ([0.659]+[0.261])	Prec@1 75.781 (75.781)
Epoch: [31][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9724 (0.8068) ([0.712]+[0.260])	Prec@1 75.000 (81.088)
Epoch: [31][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7351 (0.8065) ([0.474]+[0.261])	Prec@1 81.250 (81.301)
Epoch: [31][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7797 (0.8172) ([0.519]+[0.260])	Prec@1 81.250 (80.980)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.8688 (0.8688) ([0.609]+[0.260])	Prec@1 75.781 (75.781)
 * Prec@1 77.760
current lr 1.00000e-01
Grad=  tensor(1.6519, device='cuda:0')
Epoch: [32][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.7887 (0.7887) ([0.529]+[0.260])	Prec@1 81.250 (81.250)
Epoch: [32][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8776 (0.8125) ([0.618]+[0.260])	Prec@1 81.250 (81.405)
Epoch: [32][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6156 (0.8164) ([0.355]+[0.260])	Prec@1 90.625 (81.141)
Epoch: [32][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8123 (0.8159) ([0.553]+[0.259])	Prec@1 82.031 (80.990)
Test: [0/79]	Time 0.163 (0.163)	Loss 1.5015 (1.5015) ([1.242]+[0.259])	Prec@1 58.594 (58.594)
 * Prec@1 60.020
current lr 1.00000e-01
Grad=  tensor(2.0283, device='cuda:0')
Epoch: [33][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.8129 (0.8129) ([0.554]+[0.259])	Prec@1 78.906 (78.906)
Epoch: [33][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9243 (0.8225) ([0.665]+[0.259])	Prec@1 78.125 (81.219)
Epoch: [33][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8039 (0.8262) ([0.545]+[0.258])	Prec@1 81.250 (80.912)
Epoch: [33][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9576 (0.8221) ([0.699]+[0.259])	Prec@1 73.438 (80.894)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.6665 (1.6665) ([1.409]+[0.258])	Prec@1 65.625 (65.625)
 * Prec@1 60.110
current lr 1.00000e-01
Grad=  tensor(2.3396, device='cuda:0')
Epoch: [34][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.8088 (0.8088) ([0.551]+[0.258])	Prec@1 80.469 (80.469)
Epoch: [34][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8391 (0.7947) ([0.581]+[0.258])	Prec@1 81.250 (82.101)
Epoch: [34][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8228 (0.8069) ([0.565]+[0.258])	Prec@1 80.469 (81.507)
Epoch: [34][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7657 (0.8084) ([0.508]+[0.258])	Prec@1 81.250 (81.325)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.0171 (1.0171) ([0.759]+[0.258])	Prec@1 72.656 (72.656)
 * Prec@1 72.970
current lr 1.00000e-01
Grad=  tensor(2.4980, device='cuda:0')
Epoch: [35][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.8197 (0.8197) ([0.562]+[0.258])	Prec@1 79.688 (79.688)
Epoch: [35][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7815 (0.7959) ([0.524]+[0.257])	Prec@1 82.812 (81.683)
Epoch: [35][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8298 (0.7949) ([0.573]+[0.257])	Prec@1 82.812 (81.693)
Epoch: [35][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7382 (0.8011) ([0.481]+[0.257])	Prec@1 80.469 (81.585)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.6324 (1.6324) ([1.376]+[0.257])	Prec@1 64.844 (64.844)
 * Prec@1 64.750
current lr 1.00000e-01
Grad=  tensor(1.4717, device='cuda:0')
Epoch: [36][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.7248 (0.7248) ([0.468]+[0.257])	Prec@1 82.031 (82.031)
Epoch: [36][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7212 (0.7838) ([0.465]+[0.256])	Prec@1 85.938 (81.985)
Epoch: [36][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7432 (0.7986) ([0.486]+[0.258])	Prec@1 85.156 (81.289)
Epoch: [36][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8106 (0.8048) ([0.553]+[0.258])	Prec@1 80.469 (81.099)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.4829 (1.4829) ([1.226]+[0.257])	Prec@1 67.188 (67.188)
 * Prec@1 65.170
current lr 1.00000e-01
Grad=  tensor(2.4274, device='cuda:0')
Epoch: [37][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.9086 (0.9086) ([0.651]+[0.257])	Prec@1 78.125 (78.125)
Epoch: [37][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9922 (0.7868) ([0.735]+[0.257])	Prec@1 78.125 (81.807)
Epoch: [37][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9099 (0.7962) ([0.653]+[0.257])	Prec@1 77.344 (81.701)
Epoch: [37][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7202 (0.8006) ([0.463]+[0.257])	Prec@1 82.812 (81.507)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9525 (0.9525) ([0.695]+[0.257])	Prec@1 72.656 (72.656)
 * Prec@1 74.850
current lr 1.00000e-01
Grad=  tensor(1.8209, device='cuda:0')
Epoch: [38][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.8551 (0.8551) ([0.598]+[0.257])	Prec@1 78.906 (78.906)
Epoch: [38][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7849 (0.8025) ([0.528]+[0.257])	Prec@1 82.031 (81.536)
Epoch: [38][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8279 (0.8018) ([0.571]+[0.256])	Prec@1 76.562 (81.526)
Epoch: [38][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7558 (0.8036) ([0.500]+[0.256])	Prec@1 81.250 (81.439)
Test: [0/79]	Time 0.163 (0.163)	Loss 1.0091 (1.0091) ([0.754]+[0.255])	Prec@1 72.656 (72.656)
 * Prec@1 73.660
current lr 1.00000e-01
Grad=  tensor(1.7019, device='cuda:0')
Epoch: [39][0/391]	Time 0.254 (0.254)	Data 0.130 (0.130)	Loss 0.6946 (0.6946) ([0.439]+[0.255])	Prec@1 88.281 (88.281)
Epoch: [39][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7143 (0.7769) ([0.459]+[0.255])	Prec@1 85.156 (82.085)
Epoch: [39][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8441 (0.7865) ([0.588]+[0.256])	Prec@1 82.031 (81.790)
Epoch: [39][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.5664 (0.7904) ([0.310]+[0.256])	Prec@1 91.406 (81.657)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.3848 (1.3848) ([1.129]+[0.256])	Prec@1 69.531 (69.531)
 * Prec@1 67.540
current lr 1.00000e-01
Grad=  tensor(1.5320, device='cuda:0')
Epoch: [40][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.6660 (0.6660) ([0.410]+[0.256])	Prec@1 86.719 (86.719)
Epoch: [40][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7142 (0.7828) ([0.458]+[0.256])	Prec@1 83.594 (81.900)
Epoch: [40][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8933 (0.7940) ([0.637]+[0.256])	Prec@1 77.344 (81.728)
Epoch: [40][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7973 (0.7986) ([0.541]+[0.257])	Prec@1 81.250 (81.504)
Test: [0/79]	Time 0.163 (0.163)	Loss 1.0435 (1.0435) ([0.788]+[0.256])	Prec@1 71.875 (71.875)
 * Prec@1 73.630
current lr 1.00000e-01
Grad=  tensor(1.4464, device='cuda:0')
Epoch: [41][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.6386 (0.6386) ([0.383]+[0.256])	Prec@1 87.500 (87.500)
Epoch: [41][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8691 (0.7887) ([0.613]+[0.256])	Prec@1 79.688 (82.016)
Epoch: [41][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7840 (0.7997) ([0.528]+[0.256])	Prec@1 79.688 (81.487)
Epoch: [41][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8843 (0.7935) ([0.629]+[0.255])	Prec@1 77.344 (81.717)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.1840 (1.1840) ([0.929]+[0.255])	Prec@1 67.969 (67.969)
 * Prec@1 73.230
current lr 1.00000e-01
Grad=  tensor(1.7176, device='cuda:0')
Epoch: [42][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.7460 (0.7460) ([0.491]+[0.255])	Prec@1 85.156 (85.156)
Epoch: [42][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8634 (0.8040) ([0.608]+[0.256])	Prec@1 78.906 (81.180)
Epoch: [42][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6713 (0.7894) ([0.416]+[0.256])	Prec@1 83.594 (81.518)
Epoch: [42][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8037 (0.7908) ([0.548]+[0.256])	Prec@1 81.250 (81.447)
Test: [0/79]	Time 0.161 (0.161)	Loss 0.9789 (0.9789) ([0.723]+[0.256])	Prec@1 73.438 (73.438)
 * Prec@1 74.560
current lr 1.00000e-01
Grad=  tensor(1.5634, device='cuda:0')
Epoch: [43][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.8028 (0.8028) ([0.547]+[0.256])	Prec@1 83.594 (83.594)
Epoch: [43][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8575 (0.7898) ([0.602]+[0.255])	Prec@1 78.125 (81.598)
Epoch: [43][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6760 (0.7895) ([0.422]+[0.254])	Prec@1 87.500 (81.821)
Epoch: [43][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9044 (0.7938) ([0.651]+[0.254])	Prec@1 75.000 (81.619)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.1895 (1.1895) ([0.936]+[0.253])	Prec@1 67.969 (67.969)
 * Prec@1 71.360
current lr 1.00000e-01
Grad=  tensor(1.7200, device='cuda:0')
Epoch: [44][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.6729 (0.6729) ([0.419]+[0.253])	Prec@1 88.281 (88.281)
Epoch: [44][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9124 (0.7698) ([0.659]+[0.253])	Prec@1 79.688 (82.387)
Epoch: [44][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9268 (0.7795) ([0.673]+[0.254])	Prec@1 75.000 (82.047)
Epoch: [44][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7005 (0.7878) ([0.446]+[0.254])	Prec@1 84.375 (81.844)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.4995 (1.4995) ([1.246]+[0.254])	Prec@1 68.750 (68.750)
 * Prec@1 65.660
current lr 1.00000e-01
Grad=  tensor(1.4794, device='cuda:0')
Epoch: [45][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.6198 (0.6198) ([0.366]+[0.254])	Prec@1 89.062 (89.062)
Epoch: [45][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.7860 (0.7894) ([0.532]+[0.254])	Prec@1 81.250 (81.490)
Epoch: [45][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7250 (0.7916) ([0.472]+[0.253])	Prec@1 82.031 (81.475)
Epoch: [45][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8499 (0.7924) ([0.596]+[0.254])	Prec@1 75.781 (81.543)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.8564 (0.8564) ([0.603]+[0.253])	Prec@1 75.781 (75.781)
 * Prec@1 78.300
current lr 1.00000e-01
Grad=  tensor(2.2292, device='cuda:0')
Epoch: [46][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.8698 (0.8698) ([0.617]+[0.253])	Prec@1 75.781 (75.781)
Epoch: [46][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7158 (0.7660) ([0.463]+[0.252])	Prec@1 85.156 (82.712)
Epoch: [46][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8704 (0.7788) ([0.617]+[0.254])	Prec@1 75.000 (81.992)
Epoch: [46][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.8919 (0.7832) ([0.638]+[0.254])	Prec@1 76.562 (81.829)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.9801 (0.9801) ([0.727]+[0.253])	Prec@1 75.000 (75.000)
 * Prec@1 71.900
current lr 1.00000e-01
Grad=  tensor(2.4336, device='cuda:0')
Epoch: [47][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.8979 (0.8979) ([0.645]+[0.253])	Prec@1 76.562 (76.562)
Epoch: [47][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7103 (0.7629) ([0.458]+[0.252])	Prec@1 84.375 (82.550)
Epoch: [47][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9456 (0.7634) ([0.693]+[0.252])	Prec@1 75.000 (82.420)
Epoch: [47][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7499 (0.7783) ([0.497]+[0.252])	Prec@1 82.031 (82.000)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.3692 (1.3692) ([1.118]+[0.251])	Prec@1 67.969 (67.969)
 * Prec@1 66.980
current lr 1.00000e-01
Grad=  tensor(1.2622, device='cuda:0')
Epoch: [48][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.6385 (0.6385) ([0.387]+[0.251])	Prec@1 85.156 (85.156)
Epoch: [48][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6414 (0.7637) ([0.390]+[0.251])	Prec@1 87.500 (82.596)
Epoch: [48][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7693 (0.7796) ([0.518]+[0.252])	Prec@1 80.469 (81.977)
Epoch: [48][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8463 (0.7786) ([0.595]+[0.252])	Prec@1 76.562 (82.086)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.9796 (0.9796) ([0.728]+[0.252])	Prec@1 71.875 (71.875)
 * Prec@1 75.700
current lr 1.00000e-01
Grad=  tensor(2.1777, device='cuda:0')
Epoch: [49][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.7978 (0.7978) ([0.546]+[0.252])	Prec@1 82.031 (82.031)
Epoch: [49][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8772 (0.7669) ([0.626]+[0.252])	Prec@1 76.562 (82.186)
Epoch: [49][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7832 (0.7748) ([0.531]+[0.252])	Prec@1 80.469 (81.992)
Epoch: [49][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6836 (0.7785) ([0.432]+[0.252])	Prec@1 86.719 (81.860)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.0281 (1.0281) ([0.777]+[0.251])	Prec@1 71.875 (71.875)
 * Prec@1 73.590
current lr 1.00000e-01
Grad=  tensor(2.0067, device='cuda:0')
Epoch: [50][0/391]	Time 0.258 (0.258)	Data 0.132 (0.132)	Loss 0.7391 (0.7391) ([0.488]+[0.251])	Prec@1 80.469 (80.469)
Epoch: [50][100/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.5828 (0.7658) ([0.332]+[0.251])	Prec@1 89.844 (82.279)
Epoch: [50][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.7428 (0.7749) ([0.491]+[0.252])	Prec@1 89.062 (82.105)
Epoch: [50][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.9064 (0.7779) ([0.655]+[0.251])	Prec@1 77.344 (81.953)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.9372 (0.9372) ([0.686]+[0.251])	Prec@1 77.344 (77.344)
 * Prec@1 79.200
current lr 1.00000e-01
Grad=  tensor(2.2902, device='cuda:0')
Epoch: [51][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.7236 (0.7236) ([0.473]+[0.251])	Prec@1 83.594 (83.594)
Epoch: [51][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8119 (0.7738) ([0.562]+[0.250])	Prec@1 82.031 (81.954)
Epoch: [51][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6299 (0.7757) ([0.380]+[0.250])	Prec@1 87.500 (82.078)
Epoch: [51][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7588 (0.7737) ([0.510]+[0.249])	Prec@1 83.594 (82.107)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.7178 (1.7178) ([1.469]+[0.249])	Prec@1 62.500 (62.500)
 * Prec@1 60.340
current lr 1.00000e-01
Grad=  tensor(1.6651, device='cuda:0')
Epoch: [52][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.6829 (0.6829) ([0.434]+[0.249])	Prec@1 86.719 (86.719)
Epoch: [52][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9708 (0.7790) ([0.722]+[0.249])	Prec@1 77.344 (81.907)
Epoch: [52][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6760 (0.7758) ([0.427]+[0.249])	Prec@1 85.938 (82.000)
Epoch: [52][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7608 (0.7688) ([0.512]+[0.249])	Prec@1 84.375 (82.143)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.1387 (1.1387) ([0.890]+[0.249])	Prec@1 71.094 (71.094)
 * Prec@1 69.770
current lr 1.00000e-01
Grad=  tensor(2.1042, device='cuda:0')
Epoch: [53][0/391]	Time 0.267 (0.267)	Data 0.142 (0.142)	Loss 0.8463 (0.8463) ([0.597]+[0.249])	Prec@1 77.344 (77.344)
Epoch: [53][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7744 (0.7665) ([0.526]+[0.249])	Prec@1 80.469 (82.201)
Epoch: [53][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8522 (0.7725) ([0.603]+[0.249])	Prec@1 77.344 (82.035)
Epoch: [53][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7255 (0.7755) ([0.477]+[0.249])	Prec@1 81.250 (81.907)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.8523 (0.8523) ([0.604]+[0.248])	Prec@1 78.906 (78.906)
 * Prec@1 77.860
current lr 1.00000e-01
Grad=  tensor(1.9764, device='cuda:0')
Epoch: [54][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.7235 (0.7235) ([0.475]+[0.248])	Prec@1 80.469 (80.469)
Epoch: [54][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8722 (0.7623) ([0.624]+[0.248])	Prec@1 77.344 (82.588)
Epoch: [54][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7100 (0.7612) ([0.461]+[0.249])	Prec@1 82.812 (82.385)
Epoch: [54][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7617 (0.7638) ([0.513]+[0.249])	Prec@1 81.250 (82.353)
Test: [0/79]	Time 0.174 (0.174)	Loss 1.1289 (1.1289) ([0.880]+[0.249])	Prec@1 73.438 (73.438)
 * Prec@1 71.140
current lr 1.00000e-01
Grad=  tensor(1.3293, device='cuda:0')
Epoch: [55][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.6122 (0.6122) ([0.363]+[0.249])	Prec@1 87.500 (87.500)
Epoch: [55][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7417 (0.7698) ([0.494]+[0.248])	Prec@1 82.812 (82.047)
Epoch: [55][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7289 (0.7683) ([0.482]+[0.247])	Prec@1 87.500 (82.253)
Epoch: [55][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6567 (0.7680) ([0.409]+[0.247])	Prec@1 89.844 (82.348)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.2114 (1.2114) ([0.964]+[0.247])	Prec@1 69.531 (69.531)
 * Prec@1 70.570
current lr 1.00000e-01
Grad=  tensor(1.6671, device='cuda:0')
Epoch: [56][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.7821 (0.7821) ([0.535]+[0.247])	Prec@1 82.031 (82.031)
Epoch: [56][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5718 (0.7431) ([0.325]+[0.247])	Prec@1 89.062 (83.014)
Epoch: [56][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7413 (0.7592) ([0.494]+[0.247])	Prec@1 83.594 (82.564)
Epoch: [56][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8547 (0.7681) ([0.608]+[0.247])	Prec@1 80.469 (82.319)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.0468 (1.0468) ([0.800]+[0.246])	Prec@1 77.344 (77.344)
 * Prec@1 75.050
current lr 1.00000e-01
Grad=  tensor(2.1876, device='cuda:0')
Epoch: [57][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.6920 (0.6920) ([0.446]+[0.246])	Prec@1 84.375 (84.375)
Epoch: [57][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9397 (0.7652) ([0.693]+[0.247])	Prec@1 75.781 (82.395)
Epoch: [57][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7906 (0.7611) ([0.544]+[0.247])	Prec@1 81.250 (82.400)
Epoch: [57][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7558 (0.7669) ([0.509]+[0.247])	Prec@1 82.812 (82.148)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0082 (1.0082) ([0.762]+[0.246])	Prec@1 71.094 (71.094)
 * Prec@1 75.170
current lr 1.00000e-01
Grad=  tensor(2.1241, device='cuda:0')
Epoch: [58][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.7265 (0.7265) ([0.480]+[0.246])	Prec@1 84.375 (84.375)
Epoch: [58][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7551 (0.7588) ([0.509]+[0.246])	Prec@1 81.250 (82.488)
Epoch: [58][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6205 (0.7634) ([0.375]+[0.246])	Prec@1 87.500 (82.319)
Epoch: [58][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7937 (0.7640) ([0.548]+[0.245])	Prec@1 81.250 (82.213)
Test: [0/79]	Time 0.163 (0.163)	Loss 1.0191 (1.0191) ([0.773]+[0.246])	Prec@1 74.219 (74.219)
 * Prec@1 76.350
current lr 1.00000e-01
Grad=  tensor(1.3888, device='cuda:0')
Epoch: [59][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.6379 (0.6379) ([0.392]+[0.246])	Prec@1 85.156 (85.156)
Epoch: [59][100/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6425 (0.7462) ([0.398]+[0.244])	Prec@1 89.062 (82.828)
Epoch: [59][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7746 (0.7501) ([0.529]+[0.245])	Prec@1 78.906 (82.816)
Epoch: [59][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6212 (0.7531) ([0.376]+[0.245])	Prec@1 88.281 (82.851)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.5344 (1.5344) ([1.290]+[0.245])	Prec@1 62.500 (62.500)
 * Prec@1 62.130
current lr 1.00000e-01
Grad=  tensor(2.0822, device='cuda:0')
Epoch: [60][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.6832 (0.6832) ([0.439]+[0.245])	Prec@1 82.812 (82.812)
Epoch: [60][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8990 (0.7510) ([0.654]+[0.245])	Prec@1 78.906 (82.627)
Epoch: [60][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7944 (0.7544) ([0.550]+[0.244])	Prec@1 85.156 (82.727)
Epoch: [60][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6920 (0.7653) ([0.447]+[0.245])	Prec@1 86.719 (82.348)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.8907 (0.8907) ([0.647]+[0.244])	Prec@1 78.906 (78.906)
 * Prec@1 75.280
current lr 1.00000e-01
Grad=  tensor(2.3827, device='cuda:0')
Epoch: [61][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.8336 (0.8336) ([0.590]+[0.244])	Prec@1 81.250 (81.250)
Epoch: [61][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7748 (0.7776) ([0.530]+[0.245])	Prec@1 79.688 (81.057)
Epoch: [61][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8650 (0.7705) ([0.619]+[0.245])	Prec@1 82.812 (81.670)
Epoch: [61][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.8484 (0.7653) ([0.604]+[0.245])	Prec@1 84.375 (81.969)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.0141 (1.0141) ([0.770]+[0.244])	Prec@1 75.000 (75.000)
 * Prec@1 74.000
current lr 1.00000e-01
Grad=  tensor(2.3353, device='cuda:0')
Epoch: [62][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.8230 (0.8230) ([0.579]+[0.244])	Prec@1 84.375 (84.375)
Epoch: [62][100/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.8094 (0.7504) ([0.565]+[0.244])	Prec@1 83.594 (82.843)
Epoch: [62][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6266 (0.7577) ([0.383]+[0.244])	Prec@1 88.281 (82.490)
Epoch: [62][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7924 (0.7584) ([0.548]+[0.244])	Prec@1 79.688 (82.483)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.2027 (1.2027) ([0.959]+[0.243])	Prec@1 75.000 (75.000)
 * Prec@1 69.350
current lr 1.00000e-01
Grad=  tensor(1.7792, device='cuda:0')
Epoch: [63][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.6999 (0.6999) ([0.456]+[0.243])	Prec@1 85.938 (85.938)
Epoch: [63][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7555 (0.7417) ([0.512]+[0.243])	Prec@1 83.594 (82.998)
Epoch: [63][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8565 (0.7534) ([0.612]+[0.244])	Prec@1 78.906 (82.626)
Epoch: [63][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7264 (0.7641) ([0.481]+[0.246])	Prec@1 83.594 (82.262)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.0324 (1.0324) ([0.787]+[0.246])	Prec@1 71.875 (71.875)
 * Prec@1 73.370
current lr 1.00000e-01
Grad=  tensor(2.0347, device='cuda:0')
Epoch: [64][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.6925 (0.6925) ([0.447]+[0.246])	Prec@1 81.250 (81.250)
Epoch: [64][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8959 (0.7459) ([0.651]+[0.245])	Prec@1 79.688 (82.867)
Epoch: [64][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8030 (0.7593) ([0.559]+[0.244])	Prec@1 82.812 (82.494)
Epoch: [64][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.9616 (0.7550) ([0.718]+[0.244])	Prec@1 75.781 (82.556)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.0312 (1.0312) ([0.788]+[0.243])	Prec@1 78.125 (78.125)
 * Prec@1 76.900
current lr 1.00000e-01
Grad=  tensor(1.8840, device='cuda:0')
Epoch: [65][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.7968 (0.7968) ([0.553]+[0.243])	Prec@1 78.125 (78.125)
Epoch: [65][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.6650 (0.7410) ([0.422]+[0.243])	Prec@1 85.156 (83.277)
Epoch: [65][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7610 (0.7526) ([0.517]+[0.244])	Prec@1 83.594 (82.762)
Epoch: [65][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7110 (0.7500) ([0.469]+[0.242])	Prec@1 80.469 (82.763)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.8434 (0.8434) ([0.601]+[0.242])	Prec@1 82.812 (82.812)
 * Prec@1 77.900
current lr 1.00000e-01
Grad=  tensor(1.9165, device='cuda:0')
Epoch: [66][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.7550 (0.7550) ([0.513]+[0.242])	Prec@1 83.594 (83.594)
Epoch: [66][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6384 (0.7724) ([0.396]+[0.243])	Prec@1 87.500 (81.536)
Epoch: [66][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6687 (0.7600) ([0.426]+[0.243])	Prec@1 85.156 (81.946)
Epoch: [66][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7353 (0.7590) ([0.493]+[0.242])	Prec@1 81.250 (82.088)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.1090 (1.1090) ([0.866]+[0.243])	Prec@1 75.000 (75.000)
 * Prec@1 69.490
current lr 1.00000e-01
Grad=  tensor(2.0535, device='cuda:0')
Epoch: [67][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.7991 (0.7991) ([0.557]+[0.243])	Prec@1 80.469 (80.469)
Epoch: [67][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7722 (0.7266) ([0.530]+[0.243])	Prec@1 80.469 (83.308)
Epoch: [67][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8217 (0.7455) ([0.578]+[0.243])	Prec@1 78.125 (82.844)
Epoch: [67][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6839 (0.7524) ([0.441]+[0.243])	Prec@1 82.812 (82.652)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.1341 (1.1341) ([0.891]+[0.243])	Prec@1 72.656 (72.656)
 * Prec@1 72.410
current lr 1.00000e-01
Grad=  tensor(1.9451, device='cuda:0')
Epoch: [68][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.8144 (0.8144) ([0.571]+[0.243])	Prec@1 77.344 (77.344)
Epoch: [68][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8247 (0.7672) ([0.581]+[0.244])	Prec@1 80.469 (81.629)
Epoch: [68][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7543 (0.7558) ([0.512]+[0.243])	Prec@1 82.812 (82.148)
Epoch: [68][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5625 (0.7563) ([0.320]+[0.243])	Prec@1 92.969 (82.322)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.8016 (0.8016) ([0.559]+[0.243])	Prec@1 81.250 (81.250)
 * Prec@1 81.190
current lr 1.00000e-01
Grad=  tensor(1.5091, device='cuda:0')
Epoch: [69][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.6291 (0.6291) ([0.386]+[0.243])	Prec@1 85.156 (85.156)
Epoch: [69][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7242 (0.7408) ([0.482]+[0.242])	Prec@1 80.469 (82.797)
Epoch: [69][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6807 (0.7422) ([0.439]+[0.242])	Prec@1 83.594 (82.882)
Epoch: [69][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7513 (0.7459) ([0.510]+[0.241])	Prec@1 82.812 (82.779)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.2353 (1.2353) ([0.993]+[0.242])	Prec@1 67.188 (67.188)
 * Prec@1 69.770
current lr 1.00000e-01
Grad=  tensor(2.7209, device='cuda:0')
Epoch: [70][0/391]	Time 0.255 (0.255)	Data 0.130 (0.130)	Loss 0.7919 (0.7919) ([0.550]+[0.242])	Prec@1 82.031 (82.031)
Epoch: [70][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6981 (0.7708) ([0.455]+[0.243])	Prec@1 80.469 (81.706)
Epoch: [70][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7367 (0.7552) ([0.495]+[0.242])	Prec@1 78.906 (82.366)
Epoch: [70][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.0244 (0.7567) ([0.783]+[0.242])	Prec@1 74.219 (82.309)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.0215 (1.0215) ([0.780]+[0.242])	Prec@1 73.438 (73.438)
 * Prec@1 73.160
current lr 1.00000e-01
Grad=  tensor(1.2778, device='cuda:0')
Epoch: [71][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.6613 (0.6613) ([0.419]+[0.242])	Prec@1 88.281 (88.281)
Epoch: [71][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8797 (0.7475) ([0.639]+[0.241])	Prec@1 77.344 (82.851)
Epoch: [71][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7131 (0.7473) ([0.472]+[0.241])	Prec@1 82.812 (82.715)
Epoch: [71][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6796 (0.7500) ([0.439]+[0.241])	Prec@1 85.156 (82.644)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.1590 (1.1590) ([0.916]+[0.243])	Prec@1 71.875 (71.875)
 * Prec@1 71.010
current lr 1.00000e-01
Grad=  tensor(1.7641, device='cuda:0')
Epoch: [72][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.6450 (0.6450) ([0.402]+[0.243])	Prec@1 87.500 (87.500)
Epoch: [72][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8292 (0.7444) ([0.587]+[0.242])	Prec@1 77.344 (82.751)
Epoch: [72][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9272 (0.7421) ([0.685]+[0.242])	Prec@1 80.469 (82.820)
Epoch: [72][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6893 (0.7500) ([0.448]+[0.242])	Prec@1 88.281 (82.665)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0652 (1.0652) ([0.823]+[0.242])	Prec@1 73.438 (73.438)
 * Prec@1 70.420
current lr 1.00000e-01
Grad=  tensor(1.9583, device='cuda:0')
Epoch: [73][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.7266 (0.7266) ([0.485]+[0.242])	Prec@1 82.031 (82.031)
Epoch: [73][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7434 (0.7520) ([0.502]+[0.241])	Prec@1 82.812 (82.426)
Epoch: [73][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9287 (0.7516) ([0.688]+[0.241])	Prec@1 77.344 (82.467)
Epoch: [73][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7364 (0.7545) ([0.495]+[0.241])	Prec@1 85.156 (82.472)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.8982 (0.8982) ([0.657]+[0.241])	Prec@1 77.344 (77.344)
 * Prec@1 78.160
current lr 1.00000e-01
Grad=  tensor(1.9678, device='cuda:0')
Epoch: [74][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.7341 (0.7341) ([0.493]+[0.241])	Prec@1 85.156 (85.156)
Epoch: [74][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6733 (0.7493) ([0.432]+[0.241])	Prec@1 85.938 (82.519)
Epoch: [74][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7070 (0.7543) ([0.465]+[0.242])	Prec@1 84.375 (82.463)
Epoch: [74][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8579 (0.7576) ([0.616]+[0.242])	Prec@1 78.906 (82.379)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.0653 (1.0653) ([0.825]+[0.240])	Prec@1 78.125 (78.125)
 * Prec@1 73.090
current lr 1.00000e-01
Grad=  tensor(1.6581, device='cuda:0')
Epoch: [75][0/391]	Time 0.256 (0.256)	Data 0.131 (0.131)	Loss 0.6843 (0.6843) ([0.444]+[0.240])	Prec@1 85.938 (85.938)
Epoch: [75][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.6662 (0.7301) ([0.427]+[0.240])	Prec@1 85.938 (83.292)
Epoch: [75][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6232 (0.7421) ([0.384]+[0.239])	Prec@1 87.500 (82.801)
Epoch: [75][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5517 (0.7463) ([0.313]+[0.239])	Prec@1 90.625 (82.610)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.1320 (1.1320) ([0.893]+[0.239])	Prec@1 76.562 (76.562)
 * Prec@1 71.840
current lr 1.00000e-01
Grad=  tensor(2.0676, device='cuda:0')
Epoch: [76][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.7065 (0.7065) ([0.468]+[0.239])	Prec@1 85.938 (85.938)
Epoch: [76][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7060 (0.7462) ([0.466]+[0.240])	Prec@1 86.719 (82.511)
Epoch: [76][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7035 (0.7442) ([0.464]+[0.239])	Prec@1 84.375 (82.882)
Epoch: [76][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6461 (0.7457) ([0.407]+[0.239])	Prec@1 85.156 (82.805)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.3123 (1.3123) ([1.074]+[0.239])	Prec@1 69.531 (69.531)
 * Prec@1 69.550
current lr 1.00000e-01
Grad=  tensor(2.4162, device='cuda:0')
Epoch: [77][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.6747 (0.6747) ([0.436]+[0.239])	Prec@1 85.156 (85.156)
Epoch: [77][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6893 (0.7501) ([0.449]+[0.240])	Prec@1 82.812 (82.604)
Epoch: [77][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6658 (0.7462) ([0.425]+[0.240])	Prec@1 87.500 (82.824)
Epoch: [77][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6976 (0.7421) ([0.458]+[0.240])	Prec@1 82.812 (82.807)
Test: [0/79]	Time 0.163 (0.163)	Loss 1.3758 (1.3758) ([1.136]+[0.240])	Prec@1 70.312 (70.312)
 * Prec@1 63.640
current lr 1.00000e-01
Grad=  tensor(2.2314, device='cuda:0')
Epoch: [78][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.7770 (0.7770) ([0.537]+[0.240])	Prec@1 78.125 (78.125)
Epoch: [78][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.5985 (0.7171) ([0.360]+[0.239])	Prec@1 89.844 (83.532)
Epoch: [78][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8485 (0.7341) ([0.609]+[0.239])	Prec@1 80.469 (83.030)
Epoch: [78][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6530 (0.7412) ([0.414]+[0.239])	Prec@1 87.500 (82.696)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.9633 (0.9633) ([0.725]+[0.239])	Prec@1 77.344 (77.344)
 * Prec@1 74.140
current lr 1.00000e-01
Grad=  tensor(1.7465, device='cuda:0')
Epoch: [79][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.6912 (0.6912) ([0.452]+[0.239])	Prec@1 82.812 (82.812)
Epoch: [79][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7314 (0.7097) ([0.494]+[0.237])	Prec@1 78.906 (83.841)
Epoch: [79][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8061 (0.7271) ([0.568]+[0.238])	Prec@1 80.469 (83.174)
Epoch: [79][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7516 (0.7353) ([0.514]+[0.237])	Prec@1 84.375 (82.880)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9634 (0.9634) ([0.726]+[0.238])	Prec@1 75.781 (75.781)
 * Prec@1 70.240
current lr 1.00000e-01
Grad=  tensor(2.0974, device='cuda:0')
Epoch: [80][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.7316 (0.7316) ([0.494]+[0.238])	Prec@1 79.688 (79.688)
Epoch: [80][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7580 (0.7407) ([0.520]+[0.238])	Prec@1 78.125 (82.959)
Epoch: [80][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6595 (0.7462) ([0.421]+[0.238])	Prec@1 84.375 (82.673)
Epoch: [80][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8331 (0.7485) ([0.596]+[0.238])	Prec@1 78.906 (82.600)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.0704 (1.0704) ([0.834]+[0.237])	Prec@1 75.781 (75.781)
 * Prec@1 74.800
current lr 1.00000e-01
Grad=  tensor(1.7671, device='cuda:0')
Epoch: [81][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.6755 (0.6755) ([0.439]+[0.237])	Prec@1 86.719 (86.719)
Epoch: [81][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6349 (0.7230) ([0.398]+[0.237])	Prec@1 85.156 (83.478)
Epoch: [81][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8497 (0.7414) ([0.612]+[0.238])	Prec@1 81.250 (82.661)
Epoch: [81][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.8751 (0.7446) ([0.638]+[0.237])	Prec@1 82.812 (82.748)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2485 (1.2485) ([1.011]+[0.238])	Prec@1 70.312 (70.312)
 * Prec@1 66.970
current lr 1.00000e-01
Grad=  tensor(2.2051, device='cuda:0')
Epoch: [82][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.7734 (0.7734) ([0.536]+[0.238])	Prec@1 79.688 (79.688)
Epoch: [82][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7243 (0.7401) ([0.487]+[0.237])	Prec@1 80.469 (82.743)
Epoch: [82][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6817 (0.7348) ([0.444]+[0.237])	Prec@1 85.938 (82.902)
Epoch: [82][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7231 (0.7403) ([0.485]+[0.238])	Prec@1 86.719 (82.800)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.7399 (0.7399) ([0.504]+[0.236])	Prec@1 81.250 (81.250)
 * Prec@1 80.100
current lr 1.00000e-01
Grad=  tensor(2.2372, device='cuda:0')
Epoch: [83][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.7198 (0.7198) ([0.484]+[0.236])	Prec@1 81.250 (81.250)
Epoch: [83][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8017 (0.7380) ([0.565]+[0.236])	Prec@1 78.906 (82.565)
Epoch: [83][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6778 (0.7355) ([0.441]+[0.236])	Prec@1 85.156 (82.875)
Epoch: [83][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7213 (0.7336) ([0.485]+[0.236])	Prec@1 83.594 (83.036)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.0015 (1.0015) ([0.765]+[0.236])	Prec@1 75.781 (75.781)
 * Prec@1 73.110
current lr 1.00000e-01
Grad=  tensor(1.3723, device='cuda:0')
Epoch: [84][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.6319 (0.6319) ([0.396]+[0.236])	Prec@1 87.500 (87.500)
Epoch: [84][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8426 (0.7371) ([0.605]+[0.237])	Prec@1 81.250 (82.712)
Epoch: [84][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7654 (0.7447) ([0.528]+[0.237])	Prec@1 83.594 (82.451)
Epoch: [84][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7692 (0.7489) ([0.533]+[0.237])	Prec@1 82.812 (82.379)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0948 (1.0948) ([0.859]+[0.236])	Prec@1 75.000 (75.000)
 * Prec@1 73.410
current lr 1.00000e-01
Grad=  tensor(2.3709, device='cuda:0')
Epoch: [85][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.7027 (0.7027) ([0.467]+[0.236])	Prec@1 85.156 (85.156)
Epoch: [85][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.6558 (0.7355) ([0.419]+[0.237])	Prec@1 84.375 (83.199)
Epoch: [85][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5664 (0.7331) ([0.330]+[0.237])	Prec@1 89.844 (83.143)
Epoch: [85][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7734 (0.7341) ([0.536]+[0.237])	Prec@1 82.031 (83.168)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.8289 (0.8289) ([0.592]+[0.237])	Prec@1 76.562 (76.562)
 * Prec@1 79.130
current lr 1.00000e-01
Grad=  tensor(2.4254, device='cuda:0')
Epoch: [86][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.7760 (0.7760) ([0.539]+[0.237])	Prec@1 82.031 (82.031)
Epoch: [86][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7524 (0.7331) ([0.516]+[0.237])	Prec@1 88.281 (82.975)
Epoch: [86][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9037 (0.7381) ([0.667]+[0.236])	Prec@1 77.344 (82.801)
Epoch: [86][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7318 (0.7336) ([0.495]+[0.236])	Prec@1 83.594 (82.927)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.8531 (0.8531) ([0.617]+[0.236])	Prec@1 79.688 (79.688)
 * Prec@1 79.680
current lr 1.00000e-01
Grad=  tensor(2.2854, device='cuda:0')
Epoch: [87][0/391]	Time 0.260 (0.260)	Data 0.135 (0.135)	Loss 0.7452 (0.7452) ([0.509]+[0.236])	Prec@1 80.469 (80.469)
Epoch: [87][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8010 (0.7425) ([0.565]+[0.236])	Prec@1 82.812 (82.751)
Epoch: [87][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6514 (0.7459) ([0.415]+[0.236])	Prec@1 85.156 (82.614)
Epoch: [87][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6110 (0.7407) ([0.375]+[0.236])	Prec@1 88.281 (82.771)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.0745 (1.0745) ([0.838]+[0.236])	Prec@1 71.875 (71.875)
 * Prec@1 71.360
current lr 1.00000e-01
Grad=  tensor(1.4525, device='cuda:0')
Epoch: [88][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.6961 (0.6961) ([0.460]+[0.236])	Prec@1 85.156 (85.156)
Epoch: [88][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6076 (0.7339) ([0.371]+[0.236])	Prec@1 87.500 (82.990)
Epoch: [88][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6517 (0.7375) ([0.416]+[0.236])	Prec@1 84.375 (82.968)
Epoch: [88][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6828 (0.7407) ([0.447]+[0.236])	Prec@1 82.812 (82.716)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.1950 (1.1950) ([0.959]+[0.236])	Prec@1 72.656 (72.656)
 * Prec@1 72.330
current lr 1.00000e-01
Grad=  tensor(1.8920, device='cuda:0')
Epoch: [89][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.6437 (0.6437) ([0.408]+[0.236])	Prec@1 89.844 (89.844)
Epoch: [89][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6716 (0.7084) ([0.435]+[0.236])	Prec@1 85.938 (83.609)
Epoch: [89][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7234 (0.7217) ([0.487]+[0.237])	Prec@1 82.812 (83.333)
Epoch: [89][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6576 (0.7335) ([0.421]+[0.237])	Prec@1 82.812 (82.932)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.5541 (1.5541) ([1.318]+[0.236])	Prec@1 64.844 (64.844)
 * Prec@1 64.620
current lr 1.00000e-01
Grad=  tensor(1.9637, device='cuda:0')
Epoch: [90][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.7631 (0.7631) ([0.527]+[0.236])	Prec@1 83.594 (83.594)
Epoch: [90][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8145 (0.7310) ([0.578]+[0.237])	Prec@1 77.344 (83.091)
Epoch: [90][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.7985 (0.7407) ([0.562]+[0.237])	Prec@1 80.469 (82.606)
Epoch: [90][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6364 (0.7399) ([0.401]+[0.235])	Prec@1 88.281 (82.678)
Test: [0/79]	Time 0.161 (0.161)	Loss 1.7981 (1.7981) ([1.563]+[0.235])	Prec@1 64.844 (64.844)
 * Prec@1 61.120
current lr 1.00000e-01
Grad=  tensor(2.6185, device='cuda:0')
Epoch: [91][0/391]	Time 0.254 (0.254)	Data 0.130 (0.130)	Loss 0.8901 (0.8901) ([0.655]+[0.235])	Prec@1 78.125 (78.125)
Epoch: [91][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6996 (0.7136) ([0.465]+[0.235])	Prec@1 83.594 (83.555)
Epoch: [91][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9966 (0.7236) ([0.762]+[0.235])	Prec@1 73.438 (83.240)
Epoch: [91][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6318 (0.7273) ([0.397]+[0.234])	Prec@1 85.938 (83.002)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.8899 (0.8899) ([0.655]+[0.235])	Prec@1 82.031 (82.031)
 * Prec@1 77.170
current lr 1.00000e-01
Grad=  tensor(2.6372, device='cuda:0')
Epoch: [92][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.8628 (0.8628) ([0.628]+[0.235])	Prec@1 78.906 (78.906)
Epoch: [92][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8288 (0.7362) ([0.594]+[0.235])	Prec@1 79.688 (82.929)
Epoch: [92][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.5661 (0.7283) ([0.332]+[0.234])	Prec@1 90.625 (83.116)
Epoch: [92][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8479 (0.7285) ([0.613]+[0.235])	Prec@1 77.344 (83.064)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.0153 (1.0153) ([0.781]+[0.234])	Prec@1 77.344 (77.344)
 * Prec@1 73.900
current lr 1.00000e-01
Grad=  tensor(1.3645, device='cuda:0')
Epoch: [93][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.6263 (0.6263) ([0.392]+[0.234])	Prec@1 88.281 (88.281)
Epoch: [93][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.7111 (0.7147) ([0.477]+[0.234])	Prec@1 82.812 (83.400)
Epoch: [93][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7734 (0.7323) ([0.540]+[0.234])	Prec@1 81.250 (82.844)
Epoch: [93][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6284 (0.7294) ([0.394]+[0.234])	Prec@1 84.375 (82.846)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.0083 (1.0083) ([0.774]+[0.235])	Prec@1 71.875 (71.875)
 * Prec@1 67.970
current lr 1.00000e-01
Grad=  tensor(2.0948, device='cuda:0')
Epoch: [94][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.7614 (0.7614) ([0.527]+[0.235])	Prec@1 83.594 (83.594)
Epoch: [94][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7726 (0.7163) ([0.538]+[0.235])	Prec@1 81.250 (83.524)
Epoch: [94][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7835 (0.7263) ([0.549]+[0.234])	Prec@1 78.906 (83.155)
Epoch: [94][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7149 (0.7379) ([0.481]+[0.234])	Prec@1 80.469 (82.646)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.0402 (1.0402) ([0.805]+[0.235])	Prec@1 74.219 (74.219)
 * Prec@1 76.440
current lr 1.00000e-01
Grad=  tensor(1.1910, device='cuda:0')
Epoch: [95][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.6189 (0.6189) ([0.384]+[0.235])	Prec@1 87.500 (87.500)
Epoch: [95][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7644 (0.7438) ([0.529]+[0.235])	Prec@1 83.594 (82.735)
Epoch: [95][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7895 (0.7332) ([0.555]+[0.235])	Prec@1 81.250 (83.120)
Epoch: [95][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6421 (0.7362) ([0.407]+[0.235])	Prec@1 85.938 (82.914)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.7163 (0.7163) ([0.482]+[0.235])	Prec@1 84.375 (84.375)
 * Prec@1 78.070
current lr 1.00000e-01
Grad=  tensor(2.2195, device='cuda:0')
Epoch: [96][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.7699 (0.7699) ([0.535]+[0.235])	Prec@1 80.469 (80.469)
Epoch: [96][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6248 (0.7239) ([0.390]+[0.235])	Prec@1 87.500 (83.346)
Epoch: [96][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7696 (0.7257) ([0.535]+[0.234])	Prec@1 86.719 (83.209)
Epoch: [96][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.6766 (0.7271) ([0.443]+[0.234])	Prec@1 84.375 (83.202)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.9162 (0.9162) ([0.682]+[0.234])	Prec@1 75.781 (75.781)
 * Prec@1 75.120
current lr 1.00000e-01
Grad=  tensor(1.3085, device='cuda:0')
Epoch: [97][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.5894 (0.5894) ([0.355]+[0.234])	Prec@1 86.719 (86.719)
Epoch: [97][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7587 (0.7462) ([0.525]+[0.234])	Prec@1 82.812 (82.302)
Epoch: [97][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9071 (0.7427) ([0.673]+[0.234])	Prec@1 79.688 (82.568)
Epoch: [97][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6463 (0.7443) ([0.412]+[0.234])	Prec@1 85.938 (82.571)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.9778 (0.9778) ([0.745]+[0.233])	Prec@1 77.344 (77.344)
 * Prec@1 71.680
current lr 1.00000e-01
Grad=  tensor(2.3987, device='cuda:0')
Epoch: [98][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 0.7473 (0.7473) ([0.514]+[0.233])	Prec@1 80.469 (80.469)
Epoch: [98][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8574 (0.7162) ([0.625]+[0.232])	Prec@1 78.125 (83.137)
Epoch: [98][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8367 (0.7239) ([0.604]+[0.233])	Prec@1 82.031 (83.061)
Epoch: [98][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6785 (0.7286) ([0.446]+[0.233])	Prec@1 86.719 (82.901)
Test: [0/79]	Time 0.164 (0.164)	Loss 1.2664 (1.2664) ([1.033]+[0.233])	Prec@1 64.844 (64.844)
 * Prec@1 68.590
current lr 1.00000e-01
Grad=  tensor(1.6166, device='cuda:0')
Epoch: [99][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.6818 (0.6818) ([0.449]+[0.233])	Prec@1 83.594 (83.594)
Epoch: [99][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.5319 (0.7399) ([0.298]+[0.234])	Prec@1 91.406 (82.836)
Epoch: [99][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9013 (0.7432) ([0.668]+[0.233])	Prec@1 78.125 (82.599)
Epoch: [99][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8582 (0.7469) ([0.625]+[0.233])	Prec@1 82.812 (82.574)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.9559 (0.9559) ([0.723]+[0.233])	Prec@1 76.562 (76.562)
 * Prec@1 77.860
current lr 1.00000e-02
Grad=  tensor(2.0284, device='cuda:0')
Epoch: [100][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.7289 (0.7289) ([0.496]+[0.233])	Prec@1 85.938 (85.938)
Epoch: [100][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4581 (0.5742) ([0.246]+[0.212])	Prec@1 90.625 (87.910)
Epoch: [100][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5850 (0.5452) ([0.374]+[0.211])	Prec@1 86.719 (88.724)
Epoch: [100][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.4838 (0.5272) ([0.275]+[0.209])	Prec@1 91.406 (89.317)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4687 (0.4687) ([0.261]+[0.208])	Prec@1 92.188 (92.188)
 * Prec@1 89.520
current lr 1.00000e-02
Grad=  tensor(1.3957, device='cuda:0')
Epoch: [101][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.4835 (0.4835) ([0.276]+[0.208])	Prec@1 90.625 (90.625)
Epoch: [101][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4613 (0.4617) ([0.255]+[0.206])	Prec@1 92.188 (91.476)
Epoch: [101][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4428 (0.4586) ([0.238]+[0.205])	Prec@1 92.969 (91.445)
Epoch: [101][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3932 (0.4576) ([0.190]+[0.203])	Prec@1 94.531 (91.367)
Test: [0/79]	Time 0.159 (0.159)	Loss 0.5074 (0.5074) ([0.306]+[0.202])	Prec@1 92.188 (92.188)
 * Prec@1 90.030
current lr 1.00000e-02
Grad=  tensor(1.1515, device='cuda:0')
Epoch: [102][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.4248 (0.4248) ([0.223]+[0.202])	Prec@1 96.094 (96.094)
Epoch: [102][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4181 (0.4287) ([0.218]+[0.200])	Prec@1 90.625 (92.141)
Epoch: [102][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4680 (0.4300) ([0.269]+[0.199])	Prec@1 87.500 (92.129)
Epoch: [102][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.4191 (0.4294) ([0.221]+[0.198])	Prec@1 92.969 (92.136)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4734 (0.4734) ([0.277]+[0.196])	Prec@1 93.750 (93.750)
 * Prec@1 90.460
current lr 1.00000e-02
Grad=  tensor(1.2086, device='cuda:0')
Epoch: [103][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.3584 (0.3584) ([0.162]+[0.196])	Prec@1 96.094 (96.094)
Epoch: [103][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4017 (0.4065) ([0.207]+[0.195])	Prec@1 92.969 (92.729)
Epoch: [103][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3498 (0.4058) ([0.156]+[0.194])	Prec@1 94.531 (92.786)
Epoch: [103][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3912 (0.4047) ([0.199]+[0.193])	Prec@1 92.969 (92.818)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4332 (0.4332) ([0.242]+[0.192])	Prec@1 91.406 (91.406)
 * Prec@1 90.340
current lr 1.00000e-02
Grad=  tensor(1.7173, device='cuda:0')
Epoch: [104][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.3302 (0.3302) ([0.139]+[0.192])	Prec@1 94.531 (94.531)
Epoch: [104][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4009 (0.3959) ([0.211]+[0.190])	Prec@1 92.188 (92.907)
Epoch: [104][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3464 (0.3993) ([0.157]+[0.189])	Prec@1 93.750 (92.844)
Epoch: [104][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3631 (0.3983) ([0.175]+[0.188])	Prec@1 96.094 (92.870)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4450 (0.4450) ([0.258]+[0.187])	Prec@1 92.188 (92.188)
 * Prec@1 90.710
current lr 1.00000e-02
Grad=  tensor(2.6700, device='cuda:0')
Epoch: [105][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.4648 (0.4648) ([0.278]+[0.187])	Prec@1 92.969 (92.969)
Epoch: [105][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2704 (0.3678) ([0.085]+[0.186])	Prec@1 97.656 (94.021)
Epoch: [105][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3072 (0.3701) ([0.122]+[0.185])	Prec@1 96.875 (93.898)
Epoch: [105][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3735 (0.3761) ([0.190]+[0.184])	Prec@1 92.188 (93.529)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4389 (0.4389) ([0.256]+[0.183])	Prec@1 92.969 (92.969)
 * Prec@1 91.140
current lr 1.00000e-02
Grad=  tensor(2.1946, device='cuda:0')
Epoch: [106][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.4070 (0.4070) ([0.224]+[0.183])	Prec@1 92.969 (92.969)
Epoch: [106][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3202 (0.3606) ([0.139]+[0.182])	Prec@1 95.312 (93.967)
Epoch: [106][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4410 (0.3622) ([0.260]+[0.181])	Prec@1 89.062 (93.851)
Epoch: [106][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3838 (0.3635) ([0.204]+[0.180])	Prec@1 93.750 (93.753)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4137 (0.4137) ([0.235]+[0.179])	Prec@1 92.969 (92.969)
 * Prec@1 90.670
current lr 1.00000e-02
Grad=  tensor(1.9214, device='cuda:0')
Epoch: [107][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.3429 (0.3429) ([0.164]+[0.179])	Prec@1 94.531 (94.531)
Epoch: [107][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3479 (0.3527) ([0.170]+[0.178])	Prec@1 94.531 (93.897)
Epoch: [107][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4182 (0.3572) ([0.241]+[0.177])	Prec@1 92.969 (93.781)
Epoch: [107][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.5039 (0.3595) ([0.328]+[0.176])	Prec@1 91.406 (93.688)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4322 (0.4322) ([0.257]+[0.175])	Prec@1 92.188 (92.188)
 * Prec@1 90.710
current lr 1.00000e-02
Grad=  tensor(3.8505, device='cuda:0')
Epoch: [108][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.4071 (0.4071) ([0.232]+[0.175])	Prec@1 91.406 (91.406)
Epoch: [108][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3464 (0.3444) ([0.172]+[0.174])	Prec@1 95.312 (94.361)
Epoch: [108][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2917 (0.3447) ([0.119]+[0.173])	Prec@1 96.875 (94.349)
Epoch: [108][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3291 (0.3501) ([0.157]+[0.172])	Prec@1 94.531 (94.043)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4465 (0.4465) ([0.275]+[0.172])	Prec@1 89.844 (89.844)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(3.0234, device='cuda:0')
Epoch: [109][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.3262 (0.3262) ([0.155]+[0.172])	Prec@1 94.531 (94.531)
Epoch: [109][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3727 (0.3397) ([0.202]+[0.171])	Prec@1 93.750 (94.392)
Epoch: [109][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3422 (0.3393) ([0.172]+[0.170])	Prec@1 96.094 (94.216)
Epoch: [109][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3439 (0.3398) ([0.175]+[0.169])	Prec@1 93.750 (94.087)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4033 (0.4033) ([0.235]+[0.168])	Prec@1 91.406 (91.406)
 * Prec@1 90.480
current lr 1.00000e-02
Grad=  tensor(4.0892, device='cuda:0')
Epoch: [110][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.3458 (0.3458) ([0.177]+[0.168])	Prec@1 92.969 (92.969)
Epoch: [110][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3833 (0.3271) ([0.216]+[0.168])	Prec@1 92.188 (94.508)
Epoch: [110][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3736 (0.3275) ([0.207]+[0.167])	Prec@1 93.750 (94.430)
Epoch: [110][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2751 (0.3323) ([0.109]+[0.166])	Prec@1 96.875 (94.209)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3923 (0.3923) ([0.227]+[0.165])	Prec@1 92.188 (92.188)
 * Prec@1 91.110
current lr 1.00000e-02
Grad=  tensor(2.4080, device='cuda:0')
Epoch: [111][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.2581 (0.2581) ([0.093]+[0.165])	Prec@1 98.438 (98.438)
Epoch: [111][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2753 (0.3188) ([0.110]+[0.165])	Prec@1 96.875 (94.701)
Epoch: [111][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3558 (0.3248) ([0.192]+[0.164])	Prec@1 93.750 (94.395)
Epoch: [111][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2729 (0.3257) ([0.109]+[0.163])	Prec@1 96.094 (94.420)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4176 (0.4176) ([0.255]+[0.163])	Prec@1 93.750 (93.750)
 * Prec@1 90.830
current lr 1.00000e-02
Grad=  tensor(5.9068, device='cuda:0')
Epoch: [112][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.3416 (0.3416) ([0.179]+[0.163])	Prec@1 92.969 (92.969)
Epoch: [112][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3230 (0.3099) ([0.161]+[0.162])	Prec@1 94.531 (95.297)
Epoch: [112][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3168 (0.3204) ([0.155]+[0.161])	Prec@1 97.656 (94.733)
Epoch: [112][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3403 (0.3196) ([0.180]+[0.161])	Prec@1 95.312 (94.713)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4635 (0.4635) ([0.303]+[0.160])	Prec@1 90.625 (90.625)
 * Prec@1 90.760
current lr 1.00000e-02
Grad=  tensor(1.9172, device='cuda:0')
Epoch: [113][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.2544 (0.2544) ([0.094]+[0.160])	Prec@1 98.438 (98.438)
Epoch: [113][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3175 (0.3160) ([0.158]+[0.160])	Prec@1 94.531 (94.539)
Epoch: [113][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3381 (0.3148) ([0.179]+[0.159])	Prec@1 92.969 (94.687)
Epoch: [113][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3591 (0.3179) ([0.201]+[0.159])	Prec@1 93.750 (94.599)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4746 (0.4746) ([0.317]+[0.158])	Prec@1 89.062 (89.062)
 * Prec@1 89.860
current lr 1.00000e-02
Grad=  tensor(2.4904, device='cuda:0')
Epoch: [114][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2605 (0.2605) ([0.102]+[0.158])	Prec@1 96.875 (96.875)
Epoch: [114][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3621 (0.3028) ([0.205]+[0.158])	Prec@1 91.406 (95.050)
Epoch: [114][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3101 (0.3117) ([0.153]+[0.157])	Prec@1 95.312 (94.702)
Epoch: [114][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2734 (0.3131) ([0.117]+[0.156])	Prec@1 95.312 (94.645)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4226 (0.4226) ([0.267]+[0.156])	Prec@1 92.188 (92.188)
 * Prec@1 89.710
current lr 1.00000e-02
Grad=  tensor(2.3714, device='cuda:0')
Epoch: [115][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2460 (0.2460) ([0.090]+[0.156])	Prec@1 98.438 (98.438)
Epoch: [115][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3544 (0.3063) ([0.199]+[0.156])	Prec@1 92.188 (94.825)
Epoch: [115][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3615 (0.3107) ([0.206]+[0.155])	Prec@1 92.969 (94.694)
Epoch: [115][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2865 (0.3134) ([0.132]+[0.155])	Prec@1 96.094 (94.568)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3964 (0.3964) ([0.242]+[0.154])	Prec@1 91.406 (91.406)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(4.9070, device='cuda:0')
Epoch: [116][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2816 (0.2816) ([0.127]+[0.154])	Prec@1 95.312 (95.312)
Epoch: [116][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2277 (0.3062) ([0.074]+[0.154])	Prec@1 97.656 (94.957)
Epoch: [116][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2707 (0.3062) ([0.117]+[0.153])	Prec@1 96.875 (94.799)
Epoch: [116][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3838 (0.3070) ([0.231]+[0.153])	Prec@1 93.750 (94.812)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4680 (0.4680) ([0.316]+[0.152])	Prec@1 89.844 (89.844)
 * Prec@1 90.540
current lr 1.00000e-02
Grad=  tensor(5.8251, device='cuda:0')
Epoch: [117][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.2867 (0.2867) ([0.134]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [117][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3144 (0.3032) ([0.162]+[0.152])	Prec@1 93.750 (94.500)
Epoch: [117][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2599 (0.3053) ([0.108]+[0.152])	Prec@1 96.875 (94.477)
Epoch: [117][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3053 (0.3078) ([0.154]+[0.151])	Prec@1 94.531 (94.547)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.5094 (0.5094) ([0.358]+[0.151])	Prec@1 90.625 (90.625)
 * Prec@1 89.690
current lr 1.00000e-02
Grad=  tensor(6.5522, device='cuda:0')
Epoch: [118][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2414 (0.2414) ([0.090]+[0.151])	Prec@1 95.312 (95.312)
Epoch: [118][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.4021 (0.2978) ([0.251]+[0.151])	Prec@1 92.969 (95.011)
Epoch: [118][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2759 (0.3001) ([0.126]+[0.150])	Prec@1 93.750 (94.831)
Epoch: [118][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2972 (0.3039) ([0.147]+[0.150])	Prec@1 94.531 (94.682)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4932 (0.4932) ([0.343]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 90.460
current lr 1.00000e-02
Grad=  tensor(6.7550, device='cuda:0')
Epoch: [119][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.3819 (0.3819) ([0.232]+[0.150])	Prec@1 94.531 (94.531)
Epoch: [119][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3077 (0.2905) ([0.158]+[0.149])	Prec@1 93.750 (95.204)
Epoch: [119][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2419 (0.2944) ([0.093]+[0.149])	Prec@1 97.656 (94.970)
Epoch: [119][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2017 (0.2975) ([0.053]+[0.149])	Prec@1 100.000 (94.936)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5322 (0.5322) ([0.384]+[0.149])	Prec@1 86.719 (86.719)
 * Prec@1 88.790
current lr 1.00000e-02
Grad=  tensor(4.0551, device='cuda:0')
Epoch: [120][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2323 (0.2323) ([0.084]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [120][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2300 (0.3048) ([0.082]+[0.148])	Prec@1 96.094 (94.570)
Epoch: [120][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2200 (0.3044) ([0.072]+[0.148])	Prec@1 98.438 (94.570)
Epoch: [120][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3452 (0.3025) ([0.197]+[0.148])	Prec@1 93.750 (94.596)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5102 (0.5102) ([0.362]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 90.170
current lr 1.00000e-02
Grad=  tensor(6.5982, device='cuda:0')
Epoch: [121][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.3866 (0.3866) ([0.239]+[0.148])	Prec@1 91.406 (91.406)
Epoch: [121][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2935 (0.2971) ([0.146]+[0.148])	Prec@1 94.531 (94.756)
Epoch: [121][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3120 (0.3014) ([0.165]+[0.147])	Prec@1 94.531 (94.570)
Epoch: [121][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3319 (0.3061) ([0.185]+[0.147])	Prec@1 91.406 (94.461)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4256 (0.4256) ([0.279]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.560
current lr 1.00000e-02
Grad=  tensor(7.8228, device='cuda:0')
Epoch: [122][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.3441 (0.3441) ([0.197]+[0.147])	Prec@1 91.406 (91.406)
Epoch: [122][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3122 (0.2934) ([0.166]+[0.147])	Prec@1 95.312 (94.910)
Epoch: [122][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2125 (0.2976) ([0.066]+[0.146])	Prec@1 99.219 (94.745)
Epoch: [122][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3539 (0.3005) ([0.208]+[0.146])	Prec@1 91.406 (94.594)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5343 (0.5343) ([0.388]+[0.146])	Prec@1 92.188 (92.188)
 * Prec@1 88.570
current lr 1.00000e-02
Grad=  tensor(3.6265, device='cuda:0')
Epoch: [123][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2525 (0.2525) ([0.106]+[0.146])	Prec@1 95.312 (95.312)
Epoch: [123][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3164 (0.2882) ([0.171]+[0.146])	Prec@1 95.312 (95.142)
Epoch: [123][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3611 (0.2932) ([0.216]+[0.146])	Prec@1 90.625 (94.858)
Epoch: [123][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2795 (0.2947) ([0.134]+[0.145])	Prec@1 93.750 (94.848)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3298 (0.3298) ([0.185]+[0.145])	Prec@1 95.312 (95.312)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(6.3866, device='cuda:0')
Epoch: [124][0/391]	Time 0.262 (0.262)	Data 0.137 (0.137)	Loss 0.3401 (0.3401) ([0.195]+[0.145])	Prec@1 93.750 (93.750)
Epoch: [124][100/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2269 (0.2836) ([0.082]+[0.145])	Prec@1 98.438 (95.166)
Epoch: [124][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3089 (0.2923) ([0.164]+[0.145])	Prec@1 94.531 (94.842)
Epoch: [124][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.2998 (0.2969) ([0.155]+[0.145])	Prec@1 93.750 (94.778)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.5050 (0.5050) ([0.360]+[0.145])	Prec@1 88.281 (88.281)
 * Prec@1 89.820
current lr 1.00000e-02
Grad=  tensor(7.3164, device='cuda:0')
Epoch: [125][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.3478 (0.3478) ([0.203]+[0.145])	Prec@1 92.969 (92.969)
Epoch: [125][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2449 (0.2865) ([0.100]+[0.144])	Prec@1 98.438 (95.142)
Epoch: [125][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2859 (0.2862) ([0.142]+[0.144])	Prec@1 93.750 (95.037)
Epoch: [125][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3123 (0.2958) ([0.168]+[0.144])	Prec@1 93.750 (94.760)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4706 (0.4706) ([0.327]+[0.144])	Prec@1 90.625 (90.625)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(4.3186, device='cuda:0')
Epoch: [126][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2450 (0.2450) ([0.101]+[0.144])	Prec@1 95.312 (95.312)
Epoch: [126][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2269 (0.2802) ([0.083]+[0.144])	Prec@1 97.656 (95.297)
Epoch: [126][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2823 (0.2897) ([0.139]+[0.144])	Prec@1 93.750 (94.951)
Epoch: [126][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2900 (0.2974) ([0.146]+[0.144])	Prec@1 93.750 (94.627)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3610 (0.3610) ([0.217]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 89.390
current lr 1.00000e-02
Grad=  tensor(7.2965, device='cuda:0')
Epoch: [127][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.3363 (0.3363) ([0.193]+[0.144])	Prec@1 92.188 (92.188)
Epoch: [127][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3478 (0.2903) ([0.204]+[0.143])	Prec@1 91.406 (95.111)
Epoch: [127][200/391]	Time 0.117 (0.114)	Data 0.000 (0.001)	Loss 0.3436 (0.2956) ([0.200]+[0.143])	Prec@1 90.625 (94.792)
Epoch: [127][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3907 (0.2983) ([0.247]+[0.143])	Prec@1 91.406 (94.666)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4740 (0.4740) ([0.331]+[0.143])	Prec@1 91.406 (91.406)
 * Prec@1 89.110
current lr 1.00000e-02
Grad=  tensor(3.5167, device='cuda:0')
Epoch: [128][0/391]	Time 0.253 (0.253)	Data 0.129 (0.129)	Loss 0.2665 (0.2665) ([0.123]+[0.143])	Prec@1 97.656 (97.656)
Epoch: [128][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3311 (0.2821) ([0.188]+[0.143])	Prec@1 93.750 (95.166)
Epoch: [128][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2085 (0.2868) ([0.066]+[0.143])	Prec@1 97.656 (95.083)
Epoch: [128][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2831 (0.2946) ([0.140]+[0.143])	Prec@1 92.969 (94.721)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.3338 (0.3338) ([0.191]+[0.143])	Prec@1 95.312 (95.312)
 * Prec@1 89.120
current lr 1.00000e-02
Grad=  tensor(5.9165, device='cuda:0')
Epoch: [129][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.2557 (0.2557) ([0.113]+[0.143])	Prec@1 96.094 (96.094)
Epoch: [129][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3270 (0.2928) ([0.184]+[0.143])	Prec@1 92.969 (94.802)
Epoch: [129][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3799 (0.3007) ([0.237]+[0.143])	Prec@1 88.281 (94.450)
Epoch: [129][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2413 (0.3029) ([0.099]+[0.143])	Prec@1 97.656 (94.376)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4729 (0.4729) ([0.330]+[0.143])	Prec@1 89.844 (89.844)
 * Prec@1 89.740
current lr 1.00000e-02
Grad=  tensor(7.0348, device='cuda:0')
Epoch: [130][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.3181 (0.3181) ([0.175]+[0.143])	Prec@1 93.750 (93.750)
Epoch: [130][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2528 (0.2900) ([0.110]+[0.143])	Prec@1 95.312 (94.941)
Epoch: [130][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3739 (0.2930) ([0.231]+[0.142])	Prec@1 92.188 (94.792)
Epoch: [130][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3703 (0.3012) ([0.228]+[0.143])	Prec@1 90.625 (94.469)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4967 (0.4967) ([0.354]+[0.143])	Prec@1 90.625 (90.625)
 * Prec@1 89.060
current lr 1.00000e-02
Grad=  tensor(4.0739, device='cuda:0')
Epoch: [131][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2611 (0.2611) ([0.119]+[0.143])	Prec@1 94.531 (94.531)
Epoch: [131][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3588 (0.2904) ([0.216]+[0.142])	Prec@1 91.406 (94.763)
Epoch: [131][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3176 (0.2905) ([0.175]+[0.142])	Prec@1 93.750 (94.811)
Epoch: [131][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3022 (0.2972) ([0.160]+[0.142])	Prec@1 95.312 (94.619)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3333 (0.3333) ([0.191]+[0.142])	Prec@1 95.312 (95.312)
 * Prec@1 89.100
current lr 1.00000e-02
Grad=  tensor(3.0467, device='cuda:0')
Epoch: [132][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2064 (0.2064) ([0.064]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [132][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2638 (0.2803) ([0.122]+[0.142])	Prec@1 94.531 (95.421)
Epoch: [132][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2566 (0.2861) ([0.114]+[0.142])	Prec@1 97.656 (95.165)
Epoch: [132][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3262 (0.2923) ([0.184]+[0.142])	Prec@1 92.188 (94.879)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3779 (0.3779) ([0.236]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(2.6832, device='cuda:0')
Epoch: [133][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.2230 (0.2230) ([0.081]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [133][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3006 (0.2827) ([0.159]+[0.142])	Prec@1 96.875 (95.297)
Epoch: [133][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3284 (0.2947) ([0.186]+[0.142])	Prec@1 92.188 (94.683)
Epoch: [133][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2530 (0.2977) ([0.111]+[0.142])	Prec@1 95.312 (94.539)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4408 (0.4408) ([0.299]+[0.142])	Prec@1 87.500 (87.500)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(3.8664, device='cuda:0')
Epoch: [134][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2538 (0.2538) ([0.112]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [134][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2516 (0.2892) ([0.110]+[0.142])	Prec@1 96.094 (94.895)
Epoch: [134][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3888 (0.2947) ([0.247]+[0.142])	Prec@1 87.500 (94.601)
Epoch: [134][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3757 (0.2998) ([0.234]+[0.142])	Prec@1 92.188 (94.438)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4687 (0.4687) ([0.327]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 88.650
current lr 1.00000e-02
Grad=  tensor(5.8925, device='cuda:0')
Epoch: [135][0/391]	Time 0.265 (0.265)	Data 0.136 (0.136)	Loss 0.2543 (0.2543) ([0.112]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [135][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3223 (0.2949) ([0.180]+[0.142])	Prec@1 95.312 (94.771)
Epoch: [135][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2357 (0.2987) ([0.094]+[0.142])	Prec@1 97.656 (94.679)
Epoch: [135][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2720 (0.3008) ([0.130]+[0.142])	Prec@1 95.312 (94.599)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4354 (0.4354) ([0.293]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 89.230
current lr 1.00000e-02
Grad=  tensor(8.4086, device='cuda:0')
Epoch: [136][0/391]	Time 0.252 (0.252)	Data 0.126 (0.126)	Loss 0.3076 (0.3076) ([0.166]+[0.142])	Prec@1 93.750 (93.750)
Epoch: [136][100/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2515 (0.2867) ([0.110]+[0.142])	Prec@1 96.875 (94.980)
Epoch: [136][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2804 (0.2912) ([0.139]+[0.142])	Prec@1 96.094 (94.858)
Epoch: [136][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2931 (0.2945) ([0.151]+[0.142])	Prec@1 92.188 (94.786)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3810 (0.3810) ([0.239]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 89.840
current lr 1.00000e-02
Grad=  tensor(10.7151, device='cuda:0')
Epoch: [137][0/391]	Time 0.266 (0.266)	Data 0.141 (0.141)	Loss 0.3378 (0.3378) ([0.196]+[0.142])	Prec@1 92.969 (92.969)
Epoch: [137][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2880 (0.2753) ([0.146]+[0.142])	Prec@1 92.188 (95.483)
Epoch: [137][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2689 (0.2823) ([0.127]+[0.142])	Prec@1 94.531 (95.149)
Epoch: [137][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3080 (0.2879) ([0.166]+[0.142])	Prec@1 92.969 (94.869)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3786 (0.3786) ([0.237]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(3.0247, device='cuda:0')
Epoch: [138][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2118 (0.2118) ([0.070]+[0.142])	Prec@1 99.219 (99.219)
Epoch: [138][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2199 (0.2917) ([0.078]+[0.142])	Prec@1 97.656 (94.825)
Epoch: [138][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2636 (0.2945) ([0.122]+[0.142])	Prec@1 95.312 (94.737)
Epoch: [138][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3735 (0.2981) ([0.232]+[0.142])	Prec@1 90.625 (94.526)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4110 (0.4110) ([0.269]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(6.7973, device='cuda:0')
Epoch: [139][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.2848 (0.2848) ([0.143]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [139][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2637 (0.2828) ([0.122]+[0.142])	Prec@1 95.312 (95.282)
Epoch: [139][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2614 (0.2904) ([0.120]+[0.142])	Prec@1 97.656 (94.951)
Epoch: [139][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2909 (0.2938) ([0.149]+[0.142])	Prec@1 93.750 (94.773)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5569 (0.5569) ([0.415]+[0.142])	Prec@1 88.281 (88.281)
 * Prec@1 88.960
current lr 1.00000e-02
Grad=  tensor(4.4229, device='cuda:0')
Epoch: [140][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2278 (0.2278) ([0.086]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [140][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2940 (0.2850) ([0.152]+[0.142])	Prec@1 93.750 (95.173)
Epoch: [140][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3021 (0.2823) ([0.161]+[0.142])	Prec@1 94.531 (95.262)
Epoch: [140][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3069 (0.2913) ([0.165]+[0.142])	Prec@1 92.969 (94.918)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4410 (0.4410) ([0.299]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 89.880
current lr 1.00000e-02
Grad=  tensor(3.5031, device='cuda:0')
Epoch: [141][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2781 (0.2781) ([0.137]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [141][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3034 (0.2885) ([0.162]+[0.142])	Prec@1 93.750 (94.903)
Epoch: [141][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3200 (0.2862) ([0.178]+[0.142])	Prec@1 93.750 (95.037)
Epoch: [141][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3030 (0.2912) ([0.161]+[0.142])	Prec@1 93.750 (94.812)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4899 (0.4899) ([0.348]+[0.142])	Prec@1 89.062 (89.062)
 * Prec@1 88.850
current lr 1.00000e-02
Grad=  tensor(10.8662, device='cuda:0')
Epoch: [142][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.3093 (0.3093) ([0.168]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [142][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2628 (0.2864) ([0.121]+[0.142])	Prec@1 96.094 (94.980)
Epoch: [142][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2141 (0.2855) ([0.073]+[0.141])	Prec@1 96.094 (95.052)
Epoch: [142][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3236 (0.2880) ([0.182]+[0.142])	Prec@1 94.531 (95.035)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3759 (0.3759) ([0.234]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 90.000
current lr 1.00000e-02
Grad=  tensor(7.5504, device='cuda:0')
Epoch: [143][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2662 (0.2662) ([0.125]+[0.142])	Prec@1 93.750 (93.750)
Epoch: [143][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2552 (0.2835) ([0.114]+[0.142])	Prec@1 94.531 (95.142)
Epoch: [143][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2799 (0.2855) ([0.138]+[0.142])	Prec@1 95.312 (95.056)
Epoch: [143][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2867 (0.2897) ([0.145]+[0.142])	Prec@1 96.094 (94.962)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3801 (0.3801) ([0.238]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(5.9547, device='cuda:0')
Epoch: [144][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2766 (0.2766) ([0.135]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [144][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2730 (0.2949) ([0.131]+[0.142])	Prec@1 96.094 (94.609)
Epoch: [144][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3009 (0.2962) ([0.159]+[0.142])	Prec@1 95.312 (94.644)
Epoch: [144][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2641 (0.2961) ([0.122]+[0.142])	Prec@1 95.312 (94.677)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.4527 (0.4527) ([0.311]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 88.400
current lr 1.00000e-02
Grad=  tensor(3.9578, device='cuda:0')
Epoch: [145][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2489 (0.2489) ([0.107]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [145][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2040 (0.2771) ([0.062]+[0.142])	Prec@1 97.656 (95.150)
Epoch: [145][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3188 (0.2824) ([0.177]+[0.142])	Prec@1 92.188 (95.052)
Epoch: [145][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2897 (0.2826) ([0.148]+[0.142])	Prec@1 95.312 (95.120)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4699 (0.4699) ([0.328]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 88.920
current lr 1.00000e-02
Grad=  tensor(13.3955, device='cuda:0')
Epoch: [146][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.3962 (0.3962) ([0.254]+[0.142])	Prec@1 89.844 (89.844)
Epoch: [146][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2795 (0.2950) ([0.138]+[0.142])	Prec@1 94.531 (94.670)
Epoch: [146][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3119 (0.2905) ([0.170]+[0.142])	Prec@1 92.969 (94.877)
Epoch: [146][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2649 (0.2929) ([0.123]+[0.142])	Prec@1 95.312 (94.786)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.4406 (0.4406) ([0.299]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(11.8239, device='cuda:0')
Epoch: [147][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.3051 (0.3051) ([0.163]+[0.142])	Prec@1 93.750 (93.750)
Epoch: [147][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2773 (0.2953) ([0.135]+[0.142])	Prec@1 95.312 (94.771)
Epoch: [147][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2722 (0.2920) ([0.130]+[0.142])	Prec@1 95.312 (94.831)
Epoch: [147][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2648 (0.2908) ([0.123]+[0.142])	Prec@1 96.875 (94.835)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5678 (0.5678) ([0.426]+[0.142])	Prec@1 88.281 (88.281)
 * Prec@1 87.820
current lr 1.00000e-02
Grad=  tensor(5.8838, device='cuda:0')
Epoch: [148][0/391]	Time 0.264 (0.264)	Data 0.139 (0.139)	Loss 0.2792 (0.2792) ([0.137]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [148][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2954 (0.2911) ([0.153]+[0.142])	Prec@1 92.969 (94.686)
Epoch: [148][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3695 (0.2917) ([0.227]+[0.142])	Prec@1 91.406 (94.663)
Epoch: [148][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2345 (0.2902) ([0.092]+[0.142])	Prec@1 96.094 (94.729)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3611 (0.3611) ([0.219]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 89.310
current lr 1.00000e-02
Grad=  tensor(10.9797, device='cuda:0')
Epoch: [149][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.2940 (0.2940) ([0.152]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [149][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3195 (0.2820) ([0.177]+[0.142])	Prec@1 93.750 (95.127)
Epoch: [149][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2713 (0.2811) ([0.129]+[0.142])	Prec@1 95.312 (95.215)
Epoch: [149][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3572 (0.2858) ([0.215]+[0.142])	Prec@1 92.969 (95.027)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5170 (0.5170) ([0.375]+[0.142])	Prec@1 88.281 (88.281)
 * Prec@1 87.280
current lr 1.00000e-02
Grad=  tensor(10.1361, device='cuda:0')
Epoch: [150][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.3042 (0.3042) ([0.162]+[0.142])	Prec@1 93.750 (93.750)
Epoch: [150][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2896 (0.2835) ([0.148]+[0.142])	Prec@1 95.312 (95.080)
Epoch: [150][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2418 (0.2831) ([0.100]+[0.142])	Prec@1 98.438 (95.340)
Epoch: [150][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3299 (0.2873) ([0.188]+[0.142])	Prec@1 92.969 (95.123)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4373 (0.4373) ([0.295]+[0.142])	Prec@1 88.281 (88.281)
 * Prec@1 88.320
current lr 1.00000e-02
Grad=  tensor(6.6872, device='cuda:0')
Epoch: [151][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.2581 (0.2581) ([0.116]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [151][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3055 (0.2830) ([0.163]+[0.142])	Prec@1 94.531 (95.235)
Epoch: [151][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2706 (0.2871) ([0.128]+[0.142])	Prec@1 94.531 (95.013)
Epoch: [151][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2693 (0.2922) ([0.127]+[0.142])	Prec@1 94.531 (94.786)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4492 (0.4492) ([0.307]+[0.142])	Prec@1 90.625 (90.625)
 * Prec@1 89.250
current lr 1.00000e-02
Grad=  tensor(3.9148, device='cuda:0')
Epoch: [152][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2369 (0.2369) ([0.095]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [152][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3621 (0.2704) ([0.220]+[0.142])	Prec@1 92.188 (95.599)
Epoch: [152][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2458 (0.2803) ([0.103]+[0.142])	Prec@1 95.312 (95.231)
Epoch: [152][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3508 (0.2881) ([0.208]+[0.143])	Prec@1 92.969 (94.884)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4660 (0.4660) ([0.323]+[0.143])	Prec@1 90.625 (90.625)
 * Prec@1 88.890
current lr 1.00000e-02
Grad=  tensor(11.6728, device='cuda:0')
Epoch: [153][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.3526 (0.3526) ([0.210]+[0.143])	Prec@1 92.969 (92.969)
Epoch: [153][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3222 (0.2905) ([0.180]+[0.143])	Prec@1 95.312 (94.988)
Epoch: [153][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2905 (0.2970) ([0.148]+[0.143])	Prec@1 94.531 (94.753)
Epoch: [153][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2638 (0.2946) ([0.121]+[0.143])	Prec@1 95.312 (94.889)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3947 (0.3947) ([0.252]+[0.143])	Prec@1 93.750 (93.750)
 * Prec@1 89.430
current lr 1.00000e-02
Grad=  tensor(4.2212, device='cuda:0')
Epoch: [154][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.2265 (0.2265) ([0.084]+[0.143])	Prec@1 98.438 (98.438)
Epoch: [154][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2845 (0.2840) ([0.142]+[0.143])	Prec@1 93.750 (95.220)
Epoch: [154][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3014 (0.2869) ([0.159]+[0.143])	Prec@1 95.312 (95.091)
Epoch: [154][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3086 (0.2852) ([0.166]+[0.143])	Prec@1 94.531 (95.165)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4578 (0.4578) ([0.315]+[0.143])	Prec@1 90.625 (90.625)
 * Prec@1 87.620
current lr 1.00000e-02
Grad=  tensor(9.0932, device='cuda:0')
Epoch: [155][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.3106 (0.3106) ([0.168]+[0.143])	Prec@1 92.969 (92.969)
Epoch: [155][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3081 (0.2844) ([0.165]+[0.143])	Prec@1 91.406 (94.918)
Epoch: [155][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2214 (0.2851) ([0.078]+[0.143])	Prec@1 98.438 (94.955)
Epoch: [155][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2873 (0.2867) ([0.144]+[0.143])	Prec@1 94.531 (94.965)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4272 (0.4272) ([0.284]+[0.143])	Prec@1 91.406 (91.406)
 * Prec@1 89.560
current lr 1.00000e-02
Grad=  tensor(6.7044, device='cuda:0')
Epoch: [156][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2688 (0.2688) ([0.126]+[0.143])	Prec@1 94.531 (94.531)
Epoch: [156][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2781 (0.2767) ([0.135]+[0.143])	Prec@1 94.531 (95.312)
Epoch: [156][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4279 (0.2839) ([0.285]+[0.143])	Prec@1 89.844 (95.068)
Epoch: [156][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.3797 (0.2873) ([0.237]+[0.143])	Prec@1 92.969 (94.996)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4426 (0.4426) ([0.300]+[0.143])	Prec@1 89.062 (89.062)
 * Prec@1 89.660
current lr 1.00000e-02
Grad=  tensor(9.8118, device='cuda:0')
Epoch: [157][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.3019 (0.3019) ([0.159]+[0.143])	Prec@1 92.969 (92.969)
Epoch: [157][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2017 (0.2765) ([0.059]+[0.143])	Prec@1 98.438 (95.413)
Epoch: [157][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2975 (0.2813) ([0.155]+[0.143])	Prec@1 94.531 (95.289)
Epoch: [157][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3641 (0.2844) ([0.221]+[0.143])	Prec@1 91.406 (95.229)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4594 (0.4594) ([0.316]+[0.143])	Prec@1 86.719 (86.719)
 * Prec@1 88.420
current lr 1.00000e-02
Grad=  tensor(7.6581, device='cuda:0')
Epoch: [158][0/391]	Time 0.256 (0.256)	Data 0.131 (0.131)	Loss 0.2557 (0.2557) ([0.113]+[0.143])	Prec@1 96.875 (96.875)
Epoch: [158][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2413 (0.2796) ([0.098]+[0.143])	Prec@1 96.094 (95.367)
Epoch: [158][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2436 (0.2813) ([0.100]+[0.143])	Prec@1 97.656 (95.254)
Epoch: [158][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3027 (0.2832) ([0.160]+[0.143])	Prec@1 96.875 (95.203)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4278 (0.4278) ([0.285]+[0.143])	Prec@1 92.188 (92.188)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(8.1283, device='cuda:0')
Epoch: [159][0/391]	Time 0.254 (0.254)	Data 0.130 (0.130)	Loss 0.3343 (0.3343) ([0.191]+[0.143])	Prec@1 94.531 (94.531)
Epoch: [159][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2620 (0.2902) ([0.119]+[0.143])	Prec@1 95.312 (94.972)
Epoch: [159][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2558 (0.2914) ([0.112]+[0.143])	Prec@1 97.656 (94.900)
Epoch: [159][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3257 (0.2924) ([0.182]+[0.143])	Prec@1 95.312 (94.905)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4862 (0.4862) ([0.343]+[0.143])	Prec@1 90.625 (90.625)
 * Prec@1 89.420
current lr 1.00000e-02
Grad=  tensor(5.2659, device='cuda:0')
Epoch: [160][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.2225 (0.2225) ([0.079]+[0.143])	Prec@1 96.875 (96.875)
Epoch: [160][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2915 (0.2792) ([0.148]+[0.143])	Prec@1 93.750 (95.111)
Epoch: [160][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2546 (0.2838) ([0.111]+[0.143])	Prec@1 96.094 (95.029)
Epoch: [160][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2988 (0.2850) ([0.155]+[0.144])	Prec@1 94.531 (94.983)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4516 (0.4516) ([0.308]+[0.144])	Prec@1 92.969 (92.969)
 * Prec@1 88.160
current lr 1.00000e-02
Grad=  tensor(5.1730, device='cuda:0')
Epoch: [161][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2512 (0.2512) ([0.108]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [161][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2544 (0.2766) ([0.111]+[0.144])	Prec@1 96.875 (95.444)
Epoch: [161][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2448 (0.2834) ([0.101]+[0.144])	Prec@1 95.312 (95.079)
Epoch: [161][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.3333 (0.2837) ([0.190]+[0.143])	Prec@1 92.188 (95.175)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3742 (0.3742) ([0.231]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 90.290
current lr 1.00000e-02
Grad=  tensor(5.4143, device='cuda:0')
Epoch: [162][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2355 (0.2355) ([0.092]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [162][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2230 (0.2751) ([0.079]+[0.144])	Prec@1 96.875 (95.668)
Epoch: [162][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2993 (0.2846) ([0.156]+[0.144])	Prec@1 94.531 (95.293)
Epoch: [162][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3433 (0.2858) ([0.200]+[0.144])	Prec@1 91.406 (95.180)
Test: [0/79]	Time 0.159 (0.159)	Loss 0.4931 (0.4931) ([0.349]+[0.144])	Prec@1 86.719 (86.719)
 * Prec@1 87.470
current lr 1.00000e-02
Grad=  tensor(7.6144, device='cuda:0')
Epoch: [163][0/391]	Time 0.261 (0.261)	Data 0.133 (0.133)	Loss 0.2804 (0.2804) ([0.137]+[0.144])	Prec@1 94.531 (94.531)
Epoch: [163][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2969 (0.2710) ([0.153]+[0.144])	Prec@1 92.969 (95.630)
Epoch: [163][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3208 (0.2836) ([0.177]+[0.144])	Prec@1 93.750 (95.192)
Epoch: [163][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2923 (0.2868) ([0.148]+[0.144])	Prec@1 94.531 (95.107)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3193 (0.3193) ([0.175]+[0.144])	Prec@1 95.312 (95.312)
 * Prec@1 89.430
current lr 1.00000e-02
Grad=  tensor(4.9100, device='cuda:0')
Epoch: [164][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.2582 (0.2582) ([0.114]+[0.144])	Prec@1 96.094 (96.094)
Epoch: [164][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2867 (0.2759) ([0.143]+[0.144])	Prec@1 94.531 (95.514)
Epoch: [164][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3018 (0.2754) ([0.158]+[0.144])	Prec@1 94.531 (95.445)
Epoch: [164][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3258 (0.2772) ([0.182]+[0.144])	Prec@1 93.750 (95.364)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3895 (0.3895) ([0.246]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 90.650
current lr 1.00000e-02
Grad=  tensor(7.2113, device='cuda:0')
Epoch: [165][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.2766 (0.2766) ([0.133]+[0.144])	Prec@1 96.875 (96.875)
Epoch: [165][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2232 (0.2799) ([0.079]+[0.144])	Prec@1 97.656 (95.374)
Epoch: [165][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2676 (0.2805) ([0.124]+[0.144])	Prec@1 95.312 (95.351)
Epoch: [165][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3557 (0.2823) ([0.212]+[0.144])	Prec@1 93.750 (95.196)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4567 (0.4567) ([0.313]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 87.900
current lr 1.00000e-02
Grad=  tensor(6.5098, device='cuda:0')
Epoch: [166][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2301 (0.2301) ([0.086]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [166][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2680 (0.2715) ([0.124]+[0.144])	Prec@1 95.312 (95.498)
Epoch: [166][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2561 (0.2774) ([0.112]+[0.144])	Prec@1 94.531 (95.297)
Epoch: [166][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2771 (0.2825) ([0.133]+[0.144])	Prec@1 93.750 (95.159)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4644 (0.4644) ([0.320]+[0.144])	Prec@1 93.750 (93.750)
 * Prec@1 88.220
current lr 1.00000e-02
Grad=  tensor(2.0942, device='cuda:0')
Epoch: [167][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.2187 (0.2187) ([0.075]+[0.144])	Prec@1 99.219 (99.219)
Epoch: [167][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2455 (0.2764) ([0.101]+[0.144])	Prec@1 96.875 (95.467)
Epoch: [167][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3088 (0.2802) ([0.165]+[0.144])	Prec@1 94.531 (95.344)
Epoch: [167][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3804 (0.2869) ([0.236]+[0.144])	Prec@1 92.969 (95.144)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4105 (0.4105) ([0.266]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 90.300
current lr 1.00000e-02
Grad=  tensor(6.2897, device='cuda:0')
Epoch: [168][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2717 (0.2717) ([0.127]+[0.144])	Prec@1 96.875 (96.875)
Epoch: [168][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2433 (0.2717) ([0.099]+[0.144])	Prec@1 96.094 (95.506)
Epoch: [168][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2423 (0.2805) ([0.098]+[0.144])	Prec@1 96.094 (95.235)
Epoch: [168][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3012 (0.2785) ([0.157]+[0.144])	Prec@1 96.875 (95.344)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5961 (0.5961) ([0.452]+[0.144])	Prec@1 86.719 (86.719)
 * Prec@1 88.360
current lr 1.00000e-02
Grad=  tensor(4.3832, device='cuda:0')
Epoch: [169][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.2276 (0.2276) ([0.083]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [169][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2527 (0.2665) ([0.108]+[0.144])	Prec@1 96.094 (95.653)
Epoch: [169][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2566 (0.2773) ([0.112]+[0.144])	Prec@1 96.094 (95.390)
Epoch: [169][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3457 (0.2807) ([0.201]+[0.145])	Prec@1 92.969 (95.201)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.4979 (0.4979) ([0.353]+[0.145])	Prec@1 89.062 (89.062)
 * Prec@1 88.450
current lr 1.00000e-02
Grad=  tensor(4.1050, device='cuda:0')
Epoch: [170][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2230 (0.2230) ([0.078]+[0.145])	Prec@1 97.656 (97.656)
Epoch: [170][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2720 (0.2824) ([0.127]+[0.145])	Prec@1 95.312 (95.336)
Epoch: [170][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3042 (0.2827) ([0.160]+[0.145])	Prec@1 95.312 (95.328)
Epoch: [170][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3634 (0.2871) ([0.219]+[0.145])	Prec@1 92.969 (95.193)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.4061 (0.4061) ([0.261]+[0.145])	Prec@1 90.625 (90.625)
 * Prec@1 89.250
current lr 1.00000e-02
Grad=  tensor(6.3937, device='cuda:0')
Epoch: [171][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2774 (0.2774) ([0.133]+[0.145])	Prec@1 95.312 (95.312)
Epoch: [171][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2298 (0.2699) ([0.085]+[0.145])	Prec@1 97.656 (95.668)
Epoch: [171][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2638 (0.2756) ([0.119]+[0.145])	Prec@1 95.312 (95.402)
Epoch: [171][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2815 (0.2796) ([0.137]+[0.145])	Prec@1 95.312 (95.328)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4092 (0.4092) ([0.264]+[0.145])	Prec@1 89.844 (89.844)
 * Prec@1 88.040
current lr 1.00000e-02
Grad=  tensor(5.9279, device='cuda:0')
Epoch: [172][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.2762 (0.2762) ([0.131]+[0.145])	Prec@1 95.312 (95.312)
Epoch: [172][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3085 (0.2716) ([0.164]+[0.145])	Prec@1 96.094 (95.699)
Epoch: [172][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.3074 (0.2742) ([0.163]+[0.145])	Prec@1 96.094 (95.596)
Epoch: [172][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3345 (0.2767) ([0.190]+[0.145])	Prec@1 92.969 (95.502)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4799 (0.4799) ([0.335]+[0.145])	Prec@1 89.062 (89.062)
 * Prec@1 88.630
current lr 1.00000e-02
Grad=  tensor(5.6009, device='cuda:0')
Epoch: [173][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2372 (0.2372) ([0.093]+[0.145])	Prec@1 97.656 (97.656)
Epoch: [173][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2433 (0.2726) ([0.099]+[0.145])	Prec@1 96.875 (95.614)
Epoch: [173][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2325 (0.2808) ([0.088]+[0.145])	Prec@1 97.656 (95.289)
Epoch: [173][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3032 (0.2822) ([0.158]+[0.145])	Prec@1 94.531 (95.211)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3993 (0.3993) ([0.254]+[0.145])	Prec@1 90.625 (90.625)
 * Prec@1 88.950
current lr 1.00000e-02
Grad=  tensor(8.1380, device='cuda:0')
Epoch: [174][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.3122 (0.3122) ([0.167]+[0.145])	Prec@1 93.750 (93.750)
Epoch: [174][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2807 (0.2675) ([0.136]+[0.145])	Prec@1 95.312 (95.869)
Epoch: [174][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3354 (0.2720) ([0.191]+[0.145])	Prec@1 93.750 (95.740)
Epoch: [174][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2479 (0.2774) ([0.103]+[0.145])	Prec@1 96.094 (95.520)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3865 (0.3865) ([0.242]+[0.145])	Prec@1 93.750 (93.750)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(8.6001, device='cuda:0')
Epoch: [175][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2678 (0.2678) ([0.123]+[0.145])	Prec@1 95.312 (95.312)
Epoch: [175][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2025 (0.2663) ([0.058]+[0.145])	Prec@1 99.219 (95.924)
Epoch: [175][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3376 (0.2722) ([0.193]+[0.144])	Prec@1 93.750 (95.670)
Epoch: [175][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3383 (0.2775) ([0.194]+[0.145])	Prec@1 93.750 (95.437)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4360 (0.4360) ([0.291]+[0.145])	Prec@1 92.969 (92.969)
 * Prec@1 89.670
current lr 1.00000e-02
Grad=  tensor(6.8098, device='cuda:0')
Epoch: [176][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2623 (0.2623) ([0.118]+[0.145])	Prec@1 95.312 (95.312)
Epoch: [176][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2468 (0.2673) ([0.102]+[0.145])	Prec@1 96.094 (95.838)
Epoch: [176][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3897 (0.2764) ([0.245]+[0.145])	Prec@1 92.188 (95.417)
Epoch: [176][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2484 (0.2766) ([0.104]+[0.145])	Prec@1 97.656 (95.440)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4697 (0.4697) ([0.325]+[0.145])	Prec@1 91.406 (91.406)
 * Prec@1 90.060
current lr 1.00000e-02
Grad=  tensor(8.0752, device='cuda:0')
Epoch: [177][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.2724 (0.2724) ([0.128]+[0.145])	Prec@1 95.312 (95.312)
Epoch: [177][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.3818 (0.2615) ([0.237]+[0.145])	Prec@1 91.406 (95.955)
Epoch: [177][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2874 (0.2701) ([0.143]+[0.145])	Prec@1 93.750 (95.596)
Epoch: [177][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3129 (0.2811) ([0.168]+[0.145])	Prec@1 93.750 (95.201)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4932 (0.4932) ([0.348]+[0.145])	Prec@1 89.062 (89.062)
 * Prec@1 89.100
current lr 1.00000e-02
Grad=  tensor(14.0283, device='cuda:0')
Epoch: [178][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.3276 (0.3276) ([0.183]+[0.145])	Prec@1 92.188 (92.188)
Epoch: [178][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2336 (0.2599) ([0.089]+[0.145])	Prec@1 96.875 (96.125)
Epoch: [178][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2187 (0.2617) ([0.074]+[0.144])	Prec@1 98.438 (96.020)
Epoch: [178][300/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.2545 (0.2680) ([0.110]+[0.145])	Prec@1 96.094 (95.829)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4115 (0.4115) ([0.267]+[0.145])	Prec@1 92.969 (92.969)
 * Prec@1 90.580
current lr 1.00000e-02
Grad=  tensor(5.6036, device='cuda:0')
Epoch: [179][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.2614 (0.2614) ([0.117]+[0.145])	Prec@1 97.656 (97.656)
Epoch: [179][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.3434 (0.2768) ([0.199]+[0.145])	Prec@1 92.188 (95.537)
Epoch: [179][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2654 (0.2795) ([0.121]+[0.145])	Prec@1 96.094 (95.410)
Epoch: [179][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3028 (0.2808) ([0.158]+[0.145])	Prec@1 93.750 (95.344)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4772 (0.4772) ([0.332]+[0.145])	Prec@1 88.281 (88.281)
 * Prec@1 89.130
current lr 1.00000e-02
Grad=  tensor(9.6744, device='cuda:0')
Epoch: [180][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.2913 (0.2913) ([0.146]+[0.145])	Prec@1 94.531 (94.531)
Epoch: [180][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2177 (0.2733) ([0.073]+[0.145])	Prec@1 96.875 (95.707)
Epoch: [180][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3240 (0.2781) ([0.179]+[0.145])	Prec@1 93.750 (95.503)
Epoch: [180][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3609 (0.2783) ([0.216]+[0.145])	Prec@1 92.188 (95.486)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3491 (0.3491) ([0.204]+[0.145])	Prec@1 93.750 (93.750)
 * Prec@1 89.320
current lr 1.00000e-02
Grad=  tensor(6.5066, device='cuda:0')
Epoch: [181][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.2530 (0.2530) ([0.108]+[0.145])	Prec@1 96.875 (96.875)
Epoch: [181][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3289 (0.2765) ([0.184]+[0.145])	Prec@1 93.750 (95.382)
Epoch: [181][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2776 (0.2759) ([0.133]+[0.145])	Prec@1 95.312 (95.468)
Epoch: [181][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3082 (0.2800) ([0.163]+[0.145])	Prec@1 92.969 (95.344)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4160 (0.4160) ([0.271]+[0.145])	Prec@1 92.969 (92.969)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(5.3381, device='cuda:0')
Epoch: [182][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2307 (0.2307) ([0.085]+[0.145])	Prec@1 96.875 (96.875)
Epoch: [182][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2140 (0.2713) ([0.069]+[0.145])	Prec@1 97.656 (95.722)
Epoch: [182][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3119 (0.2737) ([0.167]+[0.145])	Prec@1 93.750 (95.612)
Epoch: [182][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2758 (0.2735) ([0.131]+[0.145])	Prec@1 96.875 (95.616)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4957 (0.4957) ([0.350]+[0.145])	Prec@1 90.625 (90.625)
 * Prec@1 88.320
current lr 1.00000e-02
Grad=  tensor(7.8377, device='cuda:0')
Epoch: [183][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.2744 (0.2744) ([0.129]+[0.145])	Prec@1 96.094 (96.094)
Epoch: [183][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3390 (0.2754) ([0.194]+[0.145])	Prec@1 94.531 (95.374)
Epoch: [183][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2761 (0.2719) ([0.131]+[0.145])	Prec@1 95.312 (95.577)
Epoch: [183][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2676 (0.2760) ([0.122]+[0.145])	Prec@1 94.531 (95.419)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4374 (0.4374) ([0.292]+[0.145])	Prec@1 91.406 (91.406)
 * Prec@1 89.020
current lr 1.00000e-02
Grad=  tensor(5.4326, device='cuda:0')
Epoch: [184][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2331 (0.2331) ([0.088]+[0.145])	Prec@1 96.875 (96.875)
Epoch: [184][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2908 (0.2668) ([0.145]+[0.145])	Prec@1 94.531 (95.761)
Epoch: [184][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1740 (0.2688) ([0.029]+[0.145])	Prec@1 100.000 (95.709)
Epoch: [184][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2493 (0.2735) ([0.104]+[0.145])	Prec@1 96.875 (95.577)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4593 (0.4593) ([0.314]+[0.145])	Prec@1 89.844 (89.844)
 * Prec@1 89.680
current lr 1.00000e-02
Grad=  tensor(13.8518, device='cuda:0')
Epoch: [185][0/391]	Time 0.256 (0.256)	Data 0.135 (0.135)	Loss 0.3241 (0.3241) ([0.179]+[0.145])	Prec@1 93.750 (93.750)
Epoch: [185][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3508 (0.2722) ([0.205]+[0.145])	Prec@1 92.969 (95.645)
Epoch: [185][200/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.2649 (0.2751) ([0.119]+[0.145])	Prec@1 96.875 (95.472)
Epoch: [185][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3685 (0.2805) ([0.223]+[0.146])	Prec@1 92.969 (95.279)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4394 (0.4394) ([0.294]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 89.130
current lr 1.00000e-02
Grad=  tensor(8.4826, device='cuda:0')
Epoch: [186][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2985 (0.2985) ([0.153]+[0.146])	Prec@1 94.531 (94.531)
Epoch: [186][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3326 (0.2689) ([0.187]+[0.146])	Prec@1 94.531 (95.831)
Epoch: [186][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2549 (0.2761) ([0.109]+[0.146])	Prec@1 96.094 (95.515)
Epoch: [186][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3603 (0.2800) ([0.215]+[0.146])	Prec@1 92.969 (95.357)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4867 (0.4867) ([0.341]+[0.146])	Prec@1 89.062 (89.062)
 * Prec@1 90.430
current lr 1.00000e-02
Grad=  tensor(10.3889, device='cuda:0')
Epoch: [187][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2834 (0.2834) ([0.138]+[0.146])	Prec@1 94.531 (94.531)
Epoch: [187][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2496 (0.2695) ([0.104]+[0.146])	Prec@1 96.875 (95.753)
Epoch: [187][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2782 (0.2702) ([0.133]+[0.145])	Prec@1 94.531 (95.728)
Epoch: [187][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3003 (0.2741) ([0.155]+[0.146])	Prec@1 94.531 (95.525)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4588 (0.4588) ([0.313]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 88.840
current lr 1.00000e-02
Grad=  tensor(8.3527, device='cuda:0')
Epoch: [188][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2898 (0.2898) ([0.144]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [188][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2472 (0.2586) ([0.102]+[0.146])	Prec@1 94.531 (96.179)
Epoch: [188][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2565 (0.2652) ([0.111]+[0.146])	Prec@1 96.094 (95.888)
Epoch: [188][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2644 (0.2720) ([0.119]+[0.146])	Prec@1 96.094 (95.632)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4823 (0.4823) ([0.337]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 88.620
current lr 1.00000e-02
Grad=  tensor(8.9866, device='cuda:0')
Epoch: [189][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.3180 (0.3180) ([0.172]+[0.146])	Prec@1 95.312 (95.312)
Epoch: [189][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2673 (0.2858) ([0.122]+[0.146])	Prec@1 96.094 (95.289)
Epoch: [189][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3898 (0.2819) ([0.244]+[0.146])	Prec@1 90.625 (95.278)
Epoch: [189][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3140 (0.2805) ([0.168]+[0.146])	Prec@1 95.312 (95.333)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3510 (0.3510) ([0.205]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 89.390
current lr 1.00000e-02
Grad=  tensor(4.4827, device='cuda:0')
Epoch: [190][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2113 (0.2113) ([0.065]+[0.146])	Prec@1 97.656 (97.656)
Epoch: [190][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2133 (0.2655) ([0.068]+[0.146])	Prec@1 96.094 (95.808)
Epoch: [190][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2796 (0.2713) ([0.134]+[0.146])	Prec@1 94.531 (95.670)
Epoch: [190][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2361 (0.2752) ([0.090]+[0.146])	Prec@1 98.438 (95.590)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.5399 (0.5399) ([0.394]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 88.350
current lr 1.00000e-02
Grad=  tensor(6.0008, device='cuda:0')
Epoch: [191][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2385 (0.2385) ([0.093]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [191][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2929 (0.2675) ([0.147]+[0.146])	Prec@1 94.531 (95.730)
Epoch: [191][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2660 (0.2692) ([0.120]+[0.146])	Prec@1 96.875 (95.752)
Epoch: [191][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2617 (0.2697) ([0.116]+[0.146])	Prec@1 96.875 (95.749)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4333 (0.4333) ([0.288]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 89.020
current lr 1.00000e-02
Grad=  tensor(5.3374, device='cuda:0')
Epoch: [192][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2338 (0.2338) ([0.088]+[0.146])	Prec@1 95.312 (95.312)
Epoch: [192][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2163 (0.2764) ([0.071]+[0.146])	Prec@1 98.438 (95.405)
Epoch: [192][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2759 (0.2824) ([0.130]+[0.146])	Prec@1 94.531 (95.227)
Epoch: [192][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2806 (0.2807) ([0.135]+[0.146])	Prec@1 95.312 (95.320)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3817 (0.3817) ([0.236]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 88.500
current lr 1.00000e-02
Grad=  tensor(6.5880, device='cuda:0')
Epoch: [193][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2312 (0.2312) ([0.086]+[0.146])	Prec@1 98.438 (98.438)
Epoch: [193][100/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3067 (0.2662) ([0.161]+[0.146])	Prec@1 94.531 (96.009)
Epoch: [193][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2199 (0.2683) ([0.074]+[0.146])	Prec@1 97.656 (95.837)
Epoch: [193][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3024 (0.2724) ([0.157]+[0.146])	Prec@1 92.188 (95.673)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4026 (0.4026) ([0.257]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 89.580
current lr 1.00000e-02
Grad=  tensor(5.6806, device='cuda:0')
Epoch: [194][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2385 (0.2385) ([0.093]+[0.146])	Prec@1 98.438 (98.438)
Epoch: [194][100/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.2916 (0.2590) ([0.146]+[0.146])	Prec@1 96.094 (96.117)
Epoch: [194][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2273 (0.2682) ([0.082]+[0.146])	Prec@1 97.656 (95.655)
Epoch: [194][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2828 (0.2713) ([0.137]+[0.146])	Prec@1 94.531 (95.629)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3916 (0.3916) ([0.246]+[0.146])	Prec@1 92.188 (92.188)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(4.3012, device='cuda:0')
Epoch: [195][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.2289 (0.2289) ([0.083]+[0.146])	Prec@1 98.438 (98.438)
Epoch: [195][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2221 (0.2511) ([0.077]+[0.146])	Prec@1 97.656 (96.326)
Epoch: [195][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2602 (0.2660) ([0.114]+[0.146])	Prec@1 95.312 (95.946)
Epoch: [195][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2932 (0.2693) ([0.147]+[0.146])	Prec@1 93.750 (95.793)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.5121 (0.5121) ([0.366]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 89.970
current lr 1.00000e-02
Grad=  tensor(6.9262, device='cuda:0')
Epoch: [196][0/391]	Time 0.262 (0.262)	Data 0.140 (0.140)	Loss 0.2450 (0.2450) ([0.099]+[0.146])	Prec@1 96.094 (96.094)
Epoch: [196][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2813 (0.2711) ([0.136]+[0.146])	Prec@1 96.094 (95.661)
Epoch: [196][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2652 (0.2704) ([0.120]+[0.146])	Prec@1 96.875 (95.686)
Epoch: [196][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3294 (0.2733) ([0.184]+[0.146])	Prec@1 92.969 (95.582)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4791 (0.4791) ([0.333]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(5.3271, device='cuda:0')
Epoch: [197][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.2267 (0.2267) ([0.081]+[0.146])	Prec@1 97.656 (97.656)
Epoch: [197][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2377 (0.2655) ([0.092]+[0.146])	Prec@1 98.438 (96.055)
Epoch: [197][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2743 (0.2718) ([0.129]+[0.146])	Prec@1 95.312 (95.643)
Epoch: [197][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2830 (0.2768) ([0.137]+[0.146])	Prec@1 94.531 (95.471)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5578 (0.5578) ([0.412]+[0.146])	Prec@1 89.062 (89.062)
 * Prec@1 87.620
current lr 1.00000e-02
Grad=  tensor(11.7595, device='cuda:0')
Epoch: [198][0/391]	Time 0.258 (0.258)	Data 0.137 (0.137)	Loss 0.2817 (0.2817) ([0.136]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [198][100/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.2489 (0.2741) ([0.103]+[0.146])	Prec@1 96.875 (95.692)
Epoch: [198][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2785 (0.2798) ([0.133]+[0.146])	Prec@1 93.750 (95.452)
Epoch: [198][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2246 (0.2738) ([0.079]+[0.146])	Prec@1 96.875 (95.627)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4161 (0.4161) ([0.270]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(7.6803, device='cuda:0')
Epoch: [199][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.2671 (0.2671) ([0.121]+[0.146])	Prec@1 94.531 (94.531)
Epoch: [199][100/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3859 (0.2683) ([0.240]+[0.146])	Prec@1 92.188 (95.815)
Epoch: [199][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2517 (0.2696) ([0.106]+[0.146])	Prec@1 96.875 (95.693)
Epoch: [199][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3147 (0.2693) ([0.169]+[0.146])	Prec@1 93.750 (95.728)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.5711 (0.5711) ([0.425]+[0.146])	Prec@1 88.281 (88.281)
 * Prec@1 89.460
current lr 1.00000e-02
Grad=  tensor(12.4439, device='cuda:0')
Epoch: [200][0/391]	Time 0.345 (0.345)	Data 0.180 (0.180)	Loss 0.3098 (0.3098) ([0.164]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [200][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3386 (0.2593) ([0.193]+[0.146])	Prec@1 92.188 (96.032)
Epoch: [200][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3087 (0.2694) ([0.163]+[0.146])	Prec@1 95.312 (95.670)
Epoch: [200][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2375 (0.2734) ([0.091]+[0.146])	Prec@1 96.875 (95.569)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4779 (0.4779) ([0.332]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 87.950
current lr 1.00000e-02
Grad=  tensor(13.9522, device='cuda:0')
Epoch: [201][0/391]	Time 0.262 (0.262)	Data 0.137 (0.137)	Loss 0.3167 (0.3167) ([0.170]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [201][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.3432 (0.2744) ([0.197]+[0.146])	Prec@1 95.312 (95.653)
Epoch: [201][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2554 (0.2714) ([0.109]+[0.146])	Prec@1 96.094 (95.721)
Epoch: [201][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2217 (0.2770) ([0.075]+[0.146])	Prec@1 98.438 (95.525)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4848 (0.4848) ([0.338]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 89.460
current lr 1.00000e-02
Grad=  tensor(7.9747, device='cuda:0')
Epoch: [202][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2474 (0.2474) ([0.101]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [202][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2779 (0.2559) ([0.132]+[0.146])	Prec@1 92.969 (96.295)
Epoch: [202][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2601 (0.2582) ([0.114]+[0.146])	Prec@1 95.312 (96.234)
Epoch: [202][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2179 (0.2663) ([0.072]+[0.146])	Prec@1 97.656 (95.896)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4487 (0.4487) ([0.303]+[0.146])	Prec@1 92.188 (92.188)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(6.4321, device='cuda:0')
Epoch: [203][0/391]	Time 0.260 (0.260)	Data 0.139 (0.139)	Loss 0.2476 (0.2476) ([0.101]+[0.146])	Prec@1 96.094 (96.094)
Epoch: [203][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2551 (0.2556) ([0.109]+[0.146])	Prec@1 96.094 (96.032)
Epoch: [203][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3165 (0.2649) ([0.170]+[0.146])	Prec@1 92.969 (95.771)
Epoch: [203][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3730 (0.2718) ([0.227]+[0.146])	Prec@1 90.625 (95.634)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4260 (0.4260) ([0.280]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 89.770
current lr 1.00000e-02
Grad=  tensor(11.1746, device='cuda:0')
Epoch: [204][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.3086 (0.3086) ([0.162]+[0.146])	Prec@1 94.531 (94.531)
Epoch: [204][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2469 (0.2632) ([0.101]+[0.146])	Prec@1 94.531 (96.040)
Epoch: [204][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2746 (0.2627) ([0.128]+[0.146])	Prec@1 96.094 (96.016)
Epoch: [204][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2269 (0.2699) ([0.081]+[0.146])	Prec@1 96.875 (95.684)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.3796 (0.3796) ([0.233]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 88.800
current lr 1.00000e-02
Grad=  tensor(8.2580, device='cuda:0')
Epoch: [205][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2872 (0.2872) ([0.141]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [205][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2104 (0.2734) ([0.064]+[0.147])	Prec@1 99.219 (95.506)
Epoch: [205][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2456 (0.2749) ([0.099]+[0.147])	Prec@1 95.312 (95.480)
Epoch: [205][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2337 (0.2782) ([0.087]+[0.147])	Prec@1 97.656 (95.486)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4753 (0.4753) ([0.329]+[0.147])	Prec@1 88.281 (88.281)
 * Prec@1 89.060
current lr 1.00000e-02
Grad=  tensor(3.3902, device='cuda:0')
Epoch: [206][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.1976 (0.1976) ([0.051]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [206][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3022 (0.2643) ([0.156]+[0.146])	Prec@1 91.406 (96.001)
Epoch: [206][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2432 (0.2616) ([0.097]+[0.146])	Prec@1 96.875 (96.113)
Epoch: [206][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2560 (0.2639) ([0.110]+[0.146])	Prec@1 98.438 (95.954)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3867 (0.3867) ([0.240]+[0.146])	Prec@1 92.188 (92.188)
 * Prec@1 88.570
current lr 1.00000e-02
Grad=  tensor(11.5645, device='cuda:0')
Epoch: [207][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.3030 (0.3030) ([0.157]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [207][100/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.2334 (0.2669) ([0.087]+[0.146])	Prec@1 98.438 (95.916)
Epoch: [207][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2439 (0.2689) ([0.098]+[0.146])	Prec@1 93.750 (95.787)
Epoch: [207][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2741 (0.2704) ([0.128]+[0.146])	Prec@1 96.094 (95.725)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5051 (0.5051) ([0.359]+[0.146])	Prec@1 86.719 (86.719)
 * Prec@1 89.090
current lr 1.00000e-02
Grad=  tensor(6.5700, device='cuda:0')
Epoch: [208][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2357 (0.2357) ([0.089]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [208][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2923 (0.2606) ([0.146]+[0.146])	Prec@1 95.312 (96.101)
Epoch: [208][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2324 (0.2652) ([0.086]+[0.146])	Prec@1 96.875 (96.024)
Epoch: [208][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2911 (0.2707) ([0.145]+[0.146])	Prec@1 95.312 (95.777)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.4576 (0.4576) ([0.311]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 89.250
current lr 1.00000e-02
Grad=  tensor(13.1077, device='cuda:0')
Epoch: [209][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.3273 (0.3273) ([0.181]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [209][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.3019 (0.2676) ([0.156]+[0.146])	Prec@1 93.750 (95.823)
Epoch: [209][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3245 (0.2647) ([0.178]+[0.146])	Prec@1 93.750 (95.923)
Epoch: [209][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2940 (0.2719) ([0.148]+[0.146])	Prec@1 95.312 (95.621)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4889 (0.4889) ([0.342]+[0.146])	Prec@1 92.188 (92.188)
 * Prec@1 88.500
current lr 1.00000e-02
Grad=  tensor(10.1266, device='cuda:0')
Epoch: [210][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.2942 (0.2942) ([0.148]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [210][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2345 (0.2713) ([0.088]+[0.146])	Prec@1 97.656 (95.560)
Epoch: [210][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3085 (0.2776) ([0.162]+[0.147])	Prec@1 96.094 (95.281)
Epoch: [210][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2304 (0.2792) ([0.084]+[0.147])	Prec@1 96.875 (95.318)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.5140 (0.5140) ([0.367]+[0.147])	Prec@1 88.281 (88.281)
 * Prec@1 87.680
current lr 1.00000e-02
Grad=  tensor(5.1540, device='cuda:0')
Epoch: [211][0/391]	Time 0.262 (0.262)	Data 0.135 (0.135)	Loss 0.2132 (0.2132) ([0.067]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [211][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2493 (0.2623) ([0.103]+[0.147])	Prec@1 95.312 (96.047)
Epoch: [211][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3028 (0.2621) ([0.156]+[0.147])	Prec@1 96.094 (96.004)
Epoch: [211][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2684 (0.2665) ([0.122]+[0.147])	Prec@1 96.094 (95.852)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4161 (0.4161) ([0.269]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 89.260
current lr 1.00000e-02
Grad=  tensor(9.7391, device='cuda:0')
Epoch: [212][0/391]	Time 0.264 (0.264)	Data 0.136 (0.136)	Loss 0.3780 (0.3780) ([0.231]+[0.147])	Prec@1 92.969 (92.969)
Epoch: [212][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3518 (0.2590) ([0.205]+[0.147])	Prec@1 95.312 (96.156)
Epoch: [212][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2853 (0.2691) ([0.139]+[0.147])	Prec@1 95.312 (95.756)
Epoch: [212][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2824 (0.2710) ([0.136]+[0.147])	Prec@1 96.094 (95.668)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3768 (0.3768) ([0.230]+[0.147])	Prec@1 95.312 (95.312)
 * Prec@1 90.510
current lr 1.00000e-02
Grad=  tensor(2.9707, device='cuda:0')
Epoch: [213][0/391]	Time 0.254 (0.254)	Data 0.130 (0.130)	Loss 0.2032 (0.2032) ([0.056]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [213][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2432 (0.2533) ([0.097]+[0.147])	Prec@1 95.312 (96.248)
Epoch: [213][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3277 (0.2560) ([0.181]+[0.146])	Prec@1 93.750 (96.206)
Epoch: [213][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2417 (0.2638) ([0.095]+[0.147])	Prec@1 96.875 (95.964)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.5563 (0.5563) ([0.410]+[0.147])	Prec@1 86.719 (86.719)
 * Prec@1 89.460
current lr 1.00000e-02
Grad=  tensor(5.9196, device='cuda:0')
Epoch: [214][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.2317 (0.2317) ([0.085]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [214][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2241 (0.2530) ([0.078]+[0.146])	Prec@1 97.656 (96.519)
Epoch: [214][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2997 (0.2616) ([0.153]+[0.146])	Prec@1 95.312 (96.086)
Epoch: [214][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2529 (0.2679) ([0.106]+[0.147])	Prec@1 96.094 (95.865)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3496 (0.3496) ([0.203]+[0.147])	Prec@1 94.531 (94.531)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(8.3938, device='cuda:0')
Epoch: [215][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2898 (0.2898) ([0.143]+[0.147])	Prec@1 92.969 (92.969)
Epoch: [215][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2595 (0.2520) ([0.113]+[0.146])	Prec@1 96.094 (96.488)
Epoch: [215][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2897 (0.2561) ([0.143]+[0.146])	Prec@1 95.312 (96.269)
Epoch: [215][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2931 (0.2634) ([0.147]+[0.147])	Prec@1 96.094 (96.031)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5217 (0.5217) ([0.375]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(5.7369, device='cuda:0')
Epoch: [216][0/391]	Time 0.262 (0.262)	Data 0.137 (0.137)	Loss 0.2592 (0.2592) ([0.113]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [216][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2896 (0.2651) ([0.143]+[0.147])	Prec@1 95.312 (95.900)
Epoch: [216][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2655 (0.2649) ([0.119]+[0.147])	Prec@1 96.094 (95.911)
Epoch: [216][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2673 (0.2675) ([0.121]+[0.147])	Prec@1 96.094 (95.855)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4511 (0.4511) ([0.304]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(5.9331, device='cuda:0')
Epoch: [217][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.2294 (0.2294) ([0.083]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [217][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2170 (0.2611) ([0.070]+[0.147])	Prec@1 97.656 (96.094)
Epoch: [217][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2549 (0.2621) ([0.108]+[0.147])	Prec@1 95.312 (95.997)
Epoch: [217][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3225 (0.2645) ([0.176]+[0.147])	Prec@1 93.750 (95.912)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5581 (0.5581) ([0.411]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 88.900
current lr 1.00000e-02
Grad=  tensor(10.1067, device='cuda:0')
Epoch: [218][0/391]	Time 0.256 (0.256)	Data 0.131 (0.131)	Loss 0.2748 (0.2748) ([0.128]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [218][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2277 (0.2533) ([0.081]+[0.147])	Prec@1 99.219 (96.364)
Epoch: [218][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2867 (0.2584) ([0.140]+[0.146])	Prec@1 96.875 (96.179)
Epoch: [218][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2252 (0.2656) ([0.079]+[0.147])	Prec@1 97.656 (95.956)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4869 (0.4869) ([0.340]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(2.9587, device='cuda:0')
Epoch: [219][0/391]	Time 0.266 (0.266)	Data 0.142 (0.142)	Loss 0.2068 (0.2068) ([0.060]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [219][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2139 (0.2582) ([0.067]+[0.147])	Prec@1 97.656 (96.163)
Epoch: [219][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2384 (0.2598) ([0.092]+[0.147])	Prec@1 96.875 (96.203)
Epoch: [219][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3292 (0.2664) ([0.182]+[0.147])	Prec@1 92.188 (95.876)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.4511 (0.4511) ([0.304]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 88.400
current lr 1.00000e-02
Grad=  tensor(6.5391, device='cuda:0')
Epoch: [220][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.2448 (0.2448) ([0.098]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [220][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2729 (0.2599) ([0.126]+[0.147])	Prec@1 95.312 (96.117)
Epoch: [220][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2526 (0.2665) ([0.106]+[0.147])	Prec@1 95.312 (95.857)
Epoch: [220][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3090 (0.2713) ([0.162]+[0.147])	Prec@1 93.750 (95.694)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.6186 (0.6186) ([0.472]+[0.147])	Prec@1 87.500 (87.500)
 * Prec@1 88.630
current lr 1.00000e-02
Grad=  tensor(10.8511, device='cuda:0')
Epoch: [221][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.3138 (0.3138) ([0.167]+[0.147])	Prec@1 93.750 (93.750)
Epoch: [221][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2637 (0.2590) ([0.117]+[0.147])	Prec@1 96.094 (96.210)
Epoch: [221][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.3108 (0.2625) ([0.164]+[0.147])	Prec@1 94.531 (96.144)
Epoch: [221][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2176 (0.2650) ([0.071]+[0.147])	Prec@1 97.656 (96.021)
Test: [0/79]	Time 0.160 (0.160)	Loss 0.4984 (0.4984) ([0.351]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 89.470
current lr 1.00000e-02
Grad=  tensor(8.3368, device='cuda:0')
Epoch: [222][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2649 (0.2649) ([0.118]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [222][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2649 (0.2616) ([0.118]+[0.147])	Prec@1 94.531 (96.078)
Epoch: [222][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2945 (0.2697) ([0.147]+[0.147])	Prec@1 93.750 (95.802)
Epoch: [222][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2738 (0.2708) ([0.127]+[0.147])	Prec@1 95.312 (95.710)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3680 (0.3680) ([0.221]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 90.920
current lr 1.00000e-02
Grad=  tensor(7.3837, device='cuda:0')
Epoch: [223][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2542 (0.2542) ([0.107]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [223][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2180 (0.2628) ([0.071]+[0.147])	Prec@1 98.438 (96.171)
Epoch: [223][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2188 (0.2672) ([0.072]+[0.147])	Prec@1 97.656 (95.907)
Epoch: [223][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2212 (0.2681) ([0.074]+[0.147])	Prec@1 96.875 (95.891)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3786 (0.3786) ([0.232]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(6.0624, device='cuda:0')
Epoch: [224][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.2555 (0.2555) ([0.108]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [224][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3842 (0.2666) ([0.237]+[0.147])	Prec@1 93.750 (95.885)
Epoch: [224][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2730 (0.2656) ([0.126]+[0.147])	Prec@1 96.094 (95.969)
Epoch: [224][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2902 (0.2731) ([0.143]+[0.147])	Prec@1 96.094 (95.588)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.5506 (0.5506) ([0.403]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 88.970
current lr 1.00000e-02
Grad=  tensor(12.4458, device='cuda:0')
Epoch: [225][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.3239 (0.3239) ([0.176]+[0.147])	Prec@1 92.969 (92.969)
Epoch: [225][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3323 (0.2538) ([0.185]+[0.147])	Prec@1 91.406 (96.357)
Epoch: [225][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3634 (0.2611) ([0.216]+[0.147])	Prec@1 92.188 (96.129)
Epoch: [225][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1979 (0.2644) ([0.051]+[0.147])	Prec@1 99.219 (95.993)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5420 (0.5420) ([0.395]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 86.350
current lr 1.00000e-02
Grad=  tensor(8.0912, device='cuda:0')
Epoch: [226][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2595 (0.2595) ([0.112]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [226][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2926 (0.2635) ([0.145]+[0.147])	Prec@1 93.750 (96.187)
Epoch: [226][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2435 (0.2650) ([0.096]+[0.147])	Prec@1 98.438 (96.012)
Epoch: [226][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3400 (0.2683) ([0.193]+[0.147])	Prec@1 92.969 (95.811)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4631 (0.4631) ([0.316]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(5.6034, device='cuda:0')
Epoch: [227][0/391]	Time 0.260 (0.260)	Data 0.132 (0.132)	Loss 0.2580 (0.2580) ([0.111]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [227][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2477 (0.2556) ([0.101]+[0.147])	Prec@1 96.875 (96.334)
Epoch: [227][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3211 (0.2662) ([0.174]+[0.147])	Prec@1 96.094 (95.919)
Epoch: [227][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2170 (0.2652) ([0.070]+[0.147])	Prec@1 99.219 (96.021)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3860 (0.3860) ([0.239]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(4.5806, device='cuda:0')
Epoch: [228][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2307 (0.2307) ([0.083]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [228][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2096 (0.2670) ([0.062]+[0.147])	Prec@1 97.656 (95.924)
Epoch: [228][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2855 (0.2637) ([0.138]+[0.147])	Prec@1 96.875 (96.024)
Epoch: [228][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2498 (0.2667) ([0.103]+[0.147])	Prec@1 96.875 (95.954)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5429 (0.5429) ([0.396]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 88.300
current lr 1.00000e-02
Grad=  tensor(6.5329, device='cuda:0')
Epoch: [229][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 0.2642 (0.2642) ([0.117]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [229][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2728 (0.2610) ([0.126]+[0.147])	Prec@1 96.094 (96.194)
Epoch: [229][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2620 (0.2616) ([0.115]+[0.147])	Prec@1 96.875 (96.070)
Epoch: [229][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.2559 (0.2655) ([0.109]+[0.147])	Prec@1 96.875 (95.951)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.4602 (0.4602) ([0.313]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 90.560
current lr 1.00000e-02
Grad=  tensor(12.2441, device='cuda:0')
Epoch: [230][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.3068 (0.3068) ([0.160]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [230][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3431 (0.2442) ([0.196]+[0.147])	Prec@1 93.750 (96.542)
Epoch: [230][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3248 (0.2551) ([0.178]+[0.147])	Prec@1 94.531 (96.187)
Epoch: [230][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2925 (0.2621) ([0.145]+[0.147])	Prec@1 95.312 (95.951)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.3082 (0.3082) ([0.161]+[0.147])	Prec@1 94.531 (94.531)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(7.2934, device='cuda:0')
Epoch: [231][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.2443 (0.2443) ([0.097]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [231][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2650 (0.2624) ([0.118]+[0.147])	Prec@1 95.312 (95.985)
Epoch: [231][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2904 (0.2648) ([0.143]+[0.147])	Prec@1 96.094 (95.892)
Epoch: [231][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2489 (0.2669) ([0.102]+[0.147])	Prec@1 96.094 (95.858)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3295 (0.3295) ([0.182]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(7.6562, device='cuda:0')
Epoch: [232][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2734 (0.2734) ([0.126]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [232][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2848 (0.2655) ([0.138]+[0.147])	Prec@1 96.094 (95.955)
Epoch: [232][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3261 (0.2663) ([0.179]+[0.147])	Prec@1 93.750 (95.907)
Epoch: [232][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2516 (0.2672) ([0.104]+[0.147])	Prec@1 95.312 (95.767)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4129 (0.4129) ([0.266]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 90.410
current lr 1.00000e-02
Grad=  tensor(9.3457, device='cuda:0')
Epoch: [233][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2699 (0.2699) ([0.123]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [233][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3275 (0.2571) ([0.180]+[0.147])	Prec@1 94.531 (96.272)
Epoch: [233][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2435 (0.2595) ([0.096]+[0.147])	Prec@1 96.094 (96.074)
Epoch: [233][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.4296 (0.2638) ([0.282]+[0.147])	Prec@1 89.062 (95.886)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4882 (0.4882) ([0.341]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(3.6942, device='cuda:0')
Epoch: [234][0/391]	Time 0.260 (0.260)	Data 0.135 (0.135)	Loss 0.2083 (0.2083) ([0.061]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [234][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2599 (0.2535) ([0.113]+[0.147])	Prec@1 96.094 (96.465)
Epoch: [234][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2165 (0.2652) ([0.069]+[0.147])	Prec@1 98.438 (95.942)
Epoch: [234][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2221 (0.2673) ([0.075]+[0.147])	Prec@1 96.875 (95.858)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3797 (0.3797) ([0.232]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 87.500
current lr 1.00000e-02
Grad=  tensor(9.3352, device='cuda:0')
Epoch: [235][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2642 (0.2642) ([0.117]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [235][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2667 (0.2687) ([0.119]+[0.147])	Prec@1 95.312 (95.831)
Epoch: [235][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3286 (0.2648) ([0.181]+[0.147])	Prec@1 93.750 (95.954)
Epoch: [235][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3590 (0.2673) ([0.212]+[0.147])	Prec@1 94.531 (95.868)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4042 (0.4042) ([0.257]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 90.730
current lr 1.00000e-02
Grad=  tensor(5.4529, device='cuda:0')
Epoch: [236][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2215 (0.2215) ([0.074]+[0.148])	Prec@1 97.656 (97.656)
Epoch: [236][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2780 (0.2538) ([0.131]+[0.147])	Prec@1 95.312 (96.264)
Epoch: [236][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2835 (0.2631) ([0.136]+[0.147])	Prec@1 96.875 (96.000)
Epoch: [236][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2315 (0.2664) ([0.084]+[0.147])	Prec@1 98.438 (95.917)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3595 (0.3595) ([0.212]+[0.147])	Prec@1 96.094 (96.094)
 * Prec@1 91.360
current lr 1.00000e-02
Grad=  tensor(1.9013, device='cuda:0')
Epoch: [237][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.1897 (0.1897) ([0.042]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [237][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2107 (0.2513) ([0.064]+[0.147])	Prec@1 98.438 (96.550)
Epoch: [237][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2887 (0.2552) ([0.142]+[0.147])	Prec@1 94.531 (96.354)
Epoch: [237][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2629 (0.2617) ([0.116]+[0.147])	Prec@1 96.875 (96.070)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4822 (0.4822) ([0.335]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 88.830
current lr 1.00000e-02
Grad=  tensor(11.8160, device='cuda:0')
Epoch: [238][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.3757 (0.3757) ([0.229]+[0.147])	Prec@1 93.750 (93.750)
Epoch: [238][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2561 (0.2556) ([0.109]+[0.147])	Prec@1 96.094 (96.349)
Epoch: [238][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2326 (0.2573) ([0.086]+[0.147])	Prec@1 97.656 (96.280)
Epoch: [238][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3352 (0.2618) ([0.188]+[0.147])	Prec@1 96.094 (96.127)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4812 (0.4812) ([0.334]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 88.350
current lr 1.00000e-02
Grad=  tensor(6.9285, device='cuda:0')
Epoch: [239][0/391]	Time 0.256 (0.256)	Data 0.131 (0.131)	Loss 0.2630 (0.2630) ([0.116]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [239][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2381 (0.2522) ([0.091]+[0.147])	Prec@1 96.875 (96.511)
Epoch: [239][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3064 (0.2579) ([0.159]+[0.147])	Prec@1 96.094 (96.253)
Epoch: [239][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3314 (0.2635) ([0.184]+[0.147])	Prec@1 93.750 (96.063)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3890 (0.3890) ([0.242]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 89.450
current lr 1.00000e-02
Grad=  tensor(11.8911, device='cuda:0')
Epoch: [240][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2786 (0.2786) ([0.131]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [240][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2460 (0.2639) ([0.099]+[0.147])	Prec@1 96.094 (96.032)
Epoch: [240][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2720 (0.2642) ([0.125]+[0.147])	Prec@1 95.312 (95.911)
Epoch: [240][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3123 (0.2624) ([0.165]+[0.147])	Prec@1 94.531 (95.980)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.4013 (0.4013) ([0.254]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(4.2147, device='cuda:0')
Epoch: [241][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2030 (0.2030) ([0.056]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [241][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2805 (0.2494) ([0.134]+[0.147])	Prec@1 96.094 (96.542)
Epoch: [241][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3014 (0.2534) ([0.155]+[0.147])	Prec@1 94.531 (96.350)
Epoch: [241][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2232 (0.2614) ([0.076]+[0.147])	Prec@1 96.875 (96.096)
Test: [0/79]	Time 0.161 (0.161)	Loss 0.4233 (0.4233) ([0.276]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(11.8257, device='cuda:0')
Epoch: [242][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2879 (0.2879) ([0.141]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [242][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3031 (0.2612) ([0.156]+[0.147])	Prec@1 93.750 (95.869)
Epoch: [242][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3504 (0.2650) ([0.203]+[0.147])	Prec@1 96.094 (95.806)
Epoch: [242][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2581 (0.2720) ([0.111]+[0.148])	Prec@1 96.094 (95.616)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4254 (0.4254) ([0.278]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 89.990
current lr 1.00000e-02
Grad=  tensor(9.1026, device='cuda:0')
Epoch: [243][0/391]	Time 0.264 (0.264)	Data 0.139 (0.139)	Loss 0.2647 (0.2647) ([0.117]+[0.148])	Prec@1 96.094 (96.094)
Epoch: [243][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2881 (0.2553) ([0.141]+[0.147])	Prec@1 93.750 (96.287)
Epoch: [243][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2172 (0.2560) ([0.070]+[0.147])	Prec@1 97.656 (96.358)
Epoch: [243][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2379 (0.2619) ([0.091]+[0.147])	Prec@1 96.875 (96.076)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4062 (0.4062) ([0.259]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 90.480
current lr 1.00000e-02
Grad=  tensor(5.0228, device='cuda:0')
Epoch: [244][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2444 (0.2444) ([0.097]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [244][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2790 (0.2520) ([0.132]+[0.147])	Prec@1 94.531 (96.272)
Epoch: [244][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2701 (0.2566) ([0.123]+[0.147])	Prec@1 96.094 (96.269)
Epoch: [244][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2439 (0.2610) ([0.097]+[0.147])	Prec@1 96.875 (96.013)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4257 (0.4257) ([0.278]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 89.590
current lr 1.00000e-02
Grad=  tensor(6.5418, device='cuda:0')
Epoch: [245][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2431 (0.2431) ([0.096]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [245][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.3123 (0.2606) ([0.165]+[0.147])	Prec@1 93.750 (96.187)
Epoch: [245][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2802 (0.2600) ([0.133]+[0.147])	Prec@1 94.531 (96.144)
Epoch: [245][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3153 (0.2623) ([0.168]+[0.147])	Prec@1 95.312 (96.060)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4872 (0.4872) ([0.340]+[0.147])	Prec@1 89.062 (89.062)
 * Prec@1 89.170
current lr 1.00000e-02
Grad=  tensor(4.6606, device='cuda:0')
Epoch: [246][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.2292 (0.2292) ([0.082]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [246][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2396 (0.2606) ([0.092]+[0.147])	Prec@1 97.656 (95.955)
Epoch: [246][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2420 (0.2606) ([0.095]+[0.147])	Prec@1 96.094 (95.973)
Epoch: [246][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2856 (0.2627) ([0.138]+[0.147])	Prec@1 96.094 (95.938)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3525 (0.3525) ([0.205]+[0.147])	Prec@1 96.094 (96.094)
 * Prec@1 89.730
current lr 1.00000e-02
Grad=  tensor(5.4321, device='cuda:0')
Epoch: [247][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2248 (0.2248) ([0.078]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [247][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2266 (0.2625) ([0.079]+[0.147])	Prec@1 98.438 (96.094)
Epoch: [247][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2829 (0.2647) ([0.136]+[0.147])	Prec@1 95.312 (96.016)
Epoch: [247][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2951 (0.2731) ([0.148]+[0.148])	Prec@1 95.312 (95.681)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5168 (0.5168) ([0.369]+[0.148])	Prec@1 91.406 (91.406)
 * Prec@1 89.440
current lr 1.00000e-02
Grad=  tensor(7.4141, device='cuda:0')
Epoch: [248][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2298 (0.2298) ([0.082]+[0.148])	Prec@1 96.875 (96.875)
Epoch: [248][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2650 (0.2566) ([0.118]+[0.147])	Prec@1 96.094 (96.264)
Epoch: [248][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2148 (0.2553) ([0.067]+[0.147])	Prec@1 96.875 (96.327)
Epoch: [248][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2561 (0.2599) ([0.109]+[0.147])	Prec@1 95.312 (96.104)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4227 (0.4227) ([0.275]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(3.0873, device='cuda:0')
Epoch: [249][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.2029 (0.2029) ([0.055]+[0.148])	Prec@1 99.219 (99.219)
Epoch: [249][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2281 (0.2509) ([0.081]+[0.147])	Prec@1 97.656 (96.388)
Epoch: [249][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2337 (0.2616) ([0.086]+[0.147])	Prec@1 96.875 (96.016)
Epoch: [249][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2757 (0.2645) ([0.128]+[0.148])	Prec@1 95.312 (95.930)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3937 (0.3937) ([0.246]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 91.170
current lr 1.00000e-03
Grad=  tensor(7.6089, device='cuda:0')
Epoch: [250][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2296 (0.2296) ([0.082]+[0.148])	Prec@1 97.656 (97.656)
Epoch: [250][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2264 (0.2210) ([0.081]+[0.146])	Prec@1 96.875 (97.757)
Epoch: [250][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1899 (0.2152) ([0.044]+[0.145])	Prec@1 99.219 (97.862)
Epoch: [250][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1805 (0.2098) ([0.035]+[0.145])	Prec@1 100.000 (98.059)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3416 (0.3416) ([0.197]+[0.145])	Prec@1 94.531 (94.531)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(1.8434, device='cuda:0')
Epoch: [251][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1757 (0.1757) ([0.031]+[0.145])	Prec@1 99.219 (99.219)
Epoch: [251][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1845 (0.1878) ([0.040]+[0.145])	Prec@1 98.438 (98.824)
Epoch: [251][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1752 (0.1864) ([0.030]+[0.145])	Prec@1 99.219 (98.861)
Epoch: [251][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1632 (0.1844) ([0.019]+[0.145])	Prec@1 100.000 (98.951)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3483 (0.3483) ([0.204]+[0.144])	Prec@1 93.750 (93.750)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(1.8451, device='cuda:0')
Epoch: [252][0/391]	Time 0.266 (0.266)	Data 0.142 (0.142)	Loss 0.1709 (0.1709) ([0.026]+[0.144])	Prec@1 100.000 (100.000)
Epoch: [252][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1640 (0.1789) ([0.020]+[0.144])	Prec@1 100.000 (99.118)
Epoch: [252][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1705 (0.1762) ([0.026]+[0.144])	Prec@1 100.000 (99.242)
Epoch: [252][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1803 (0.1777) ([0.036]+[0.144])	Prec@1 99.219 (99.167)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3254 (0.3254) ([0.182]+[0.144])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-03
Grad=  tensor(6.8681, device='cuda:0')
Epoch: [253][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1990 (0.1990) ([0.055]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [253][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1639 (0.1736) ([0.020]+[0.144])	Prec@1 99.219 (99.257)
Epoch: [253][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1979 (0.1725) ([0.054]+[0.144])	Prec@1 97.656 (99.289)
Epoch: [253][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1576 (0.1720) ([0.014]+[0.143])	Prec@1 100.000 (99.291)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3329 (0.3329) ([0.190]+[0.143])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-03
Grad=  tensor(1.2653, device='cuda:0')
Epoch: [254][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.1694 (0.1694) ([0.026]+[0.143])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1683 (0.1692) ([0.025]+[0.143])	Prec@1 98.438 (99.327)
Epoch: [254][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1821 (0.1698) ([0.039]+[0.143])	Prec@1 99.219 (99.324)
Epoch: [254][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1687 (0.1698) ([0.026]+[0.143])	Prec@1 100.000 (99.323)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3335 (0.3335) ([0.191]+[0.143])	Prec@1 95.312 (95.312)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(0.4993, device='cuda:0')
Epoch: [255][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1536 (0.1536) ([0.011]+[0.143])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1641 (0.1658) ([0.021]+[0.143])	Prec@1 99.219 (99.528)
Epoch: [255][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1629 (0.1659) ([0.020]+[0.142])	Prec@1 100.000 (99.502)
Epoch: [255][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1606 (0.1655) ([0.018]+[0.142])	Prec@1 100.000 (99.515)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3178 (0.3178) ([0.176]+[0.142])	Prec@1 95.312 (95.312)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(3.9727, device='cuda:0')
Epoch: [256][0/391]	Time 0.266 (0.266)	Data 0.141 (0.141)	Loss 0.1728 (0.1728) ([0.031]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [256][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1570 (0.1654) ([0.015]+[0.142])	Prec@1 100.000 (99.404)
Epoch: [256][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1770 (0.1651) ([0.035]+[0.142])	Prec@1 99.219 (99.495)
Epoch: [256][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1556 (0.1646) ([0.014]+[0.142])	Prec@1 100.000 (99.535)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3340 (0.3340) ([0.192]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(0.3399, device='cuda:0')
Epoch: [257][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.1489 (0.1489) ([0.007]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [257][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1626 (0.1617) ([0.021]+[0.142])	Prec@1 100.000 (99.590)
Epoch: [257][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1493 (0.1615) ([0.008]+[0.141])	Prec@1 100.000 (99.569)
Epoch: [257][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1529 (0.1621) ([0.012]+[0.141])	Prec@1 100.000 (99.538)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3216 (0.3216) ([0.180]+[0.141])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-03
Grad=  tensor(1.9801, device='cuda:0')
Epoch: [258][0/391]	Time 0.260 (0.260)	Data 0.135 (0.135)	Loss 0.1619 (0.1619) ([0.021]+[0.141])	Prec@1 99.219 (99.219)
Epoch: [258][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1654 (0.1624) ([0.024]+[0.141])	Prec@1 98.438 (99.559)
Epoch: [258][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1660 (0.1608) ([0.025]+[0.141])	Prec@1 100.000 (99.576)
Epoch: [258][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1592 (0.1608) ([0.018]+[0.141])	Prec@1 99.219 (99.582)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3240 (0.3240) ([0.183]+[0.141])	Prec@1 96.094 (96.094)
 * Prec@1 93.550
current lr 1.00000e-03
Grad=  tensor(2.8191, device='cuda:0')
Epoch: [259][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.1660 (0.1660) ([0.025]+[0.141])	Prec@1 99.219 (99.219)
Epoch: [259][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1680 (0.1595) ([0.028]+[0.141])	Prec@1 100.000 (99.567)
Epoch: [259][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1550 (0.1587) ([0.015]+[0.140])	Prec@1 100.000 (99.619)
Epoch: [259][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1530 (0.1591) ([0.013]+[0.140])	Prec@1 99.219 (99.593)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3314 (0.3314) ([0.191]+[0.140])	Prec@1 96.094 (96.094)
 * Prec@1 93.700
current lr 1.00000e-03
Grad=  tensor(1.4010, device='cuda:0')
Epoch: [260][0/391]	Time 0.261 (0.261)	Data 0.134 (0.134)	Loss 0.1563 (0.1563) ([0.016]+[0.140])	Prec@1 99.219 (99.219)
Epoch: [260][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1636 (0.1567) ([0.024]+[0.140])	Prec@1 99.219 (99.644)
Epoch: [260][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1569 (0.1570) ([0.017]+[0.140])	Prec@1 99.219 (99.666)
Epoch: [260][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1549 (0.1578) ([0.015]+[0.140])	Prec@1 100.000 (99.639)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3111 (0.3111) ([0.171]+[0.140])	Prec@1 96.094 (96.094)
 * Prec@1 93.670
current lr 1.00000e-03
Grad=  tensor(3.0006, device='cuda:0')
Epoch: [261][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.1701 (0.1701) ([0.030]+[0.140])	Prec@1 99.219 (99.219)
Epoch: [261][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1473 (0.1576) ([0.008]+[0.140])	Prec@1 100.000 (99.590)
Epoch: [261][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1523 (0.1570) ([0.013]+[0.139])	Prec@1 100.000 (99.619)
Epoch: [261][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1709 (0.1570) ([0.032]+[0.139])	Prec@1 99.219 (99.626)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3397 (0.3397) ([0.201]+[0.139])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-03
Grad=  tensor(4.8150, device='cuda:0')
Epoch: [262][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.1646 (0.1646) ([0.025]+[0.139])	Prec@1 99.219 (99.219)
Epoch: [262][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1508 (0.1545) ([0.012]+[0.139])	Prec@1 99.219 (99.698)
Epoch: [262][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1527 (0.1550) ([0.014]+[0.139])	Prec@1 100.000 (99.701)
Epoch: [262][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1640 (0.1548) ([0.025]+[0.139])	Prec@1 99.219 (99.691)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3381 (0.3381) ([0.199]+[0.139])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.6214, device='cuda:0')
Epoch: [263][0/391]	Time 0.266 (0.266)	Data 0.137 (0.137)	Loss 0.1480 (0.1480) ([0.009]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1653 (0.1540) ([0.027]+[0.139])	Prec@1 99.219 (99.683)
Epoch: [263][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1488 (0.1540) ([0.010]+[0.138])	Prec@1 100.000 (99.705)
Epoch: [263][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1477 (0.1542) ([0.009]+[0.138])	Prec@1 100.000 (99.707)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3300 (0.3300) ([0.192]+[0.138])	Prec@1 95.312 (95.312)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.8863, device='cuda:0')
Epoch: [264][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.1474 (0.1474) ([0.009]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1504 (0.1523) ([0.012]+[0.138])	Prec@1 100.000 (99.729)
Epoch: [264][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1643 (0.1524) ([0.026]+[0.138])	Prec@1 99.219 (99.701)
Epoch: [264][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1445 (0.1526) ([0.007]+[0.138])	Prec@1 100.000 (99.702)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3399 (0.3399) ([0.202]+[0.138])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-03
Grad=  tensor(0.6251, device='cuda:0')
Epoch: [265][0/391]	Time 0.252 (0.252)	Data 0.128 (0.128)	Loss 0.1476 (0.1476) ([0.010]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1432 (0.1512) ([0.006]+[0.138])	Prec@1 100.000 (99.776)
Epoch: [265][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1471 (0.1512) ([0.010]+[0.137])	Prec@1 100.000 (99.747)
Epoch: [265][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1437 (0.1512) ([0.006]+[0.137])	Prec@1 100.000 (99.730)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3406 (0.3406) ([0.203]+[0.137])	Prec@1 95.312 (95.312)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(1.1003, device='cuda:0')
Epoch: [266][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.1474 (0.1474) ([0.010]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1405 (0.1512) ([0.003]+[0.137])	Prec@1 100.000 (99.752)
Epoch: [266][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1453 (0.1513) ([0.008]+[0.137])	Prec@1 100.000 (99.747)
Epoch: [266][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1778 (0.1509) ([0.041]+[0.137])	Prec@1 97.656 (99.743)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3469 (0.3469) ([0.210]+[0.137])	Prec@1 96.094 (96.094)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(2.0055, device='cuda:0')
Epoch: [267][0/391]	Time 0.257 (0.257)	Data 0.132 (0.132)	Loss 0.1516 (0.1516) ([0.015]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [267][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1525 (0.1503) ([0.016]+[0.137])	Prec@1 100.000 (99.722)
Epoch: [267][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1492 (0.1504) ([0.013]+[0.136])	Prec@1 100.000 (99.751)
Epoch: [267][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1500 (0.1500) ([0.014]+[0.136])	Prec@1 99.219 (99.756)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3360 (0.3360) ([0.200]+[0.136])	Prec@1 95.312 (95.312)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.7191, device='cuda:0')
Epoch: [268][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.1458 (0.1458) ([0.010]+[0.136])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1411 (0.1491) ([0.005]+[0.136])	Prec@1 100.000 (99.760)
Epoch: [268][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1517 (0.1496) ([0.016]+[0.136])	Prec@1 99.219 (99.771)
Epoch: [268][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1472 (0.1494) ([0.011]+[0.136])	Prec@1 100.000 (99.753)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.3508 (0.3508) ([0.215]+[0.136])	Prec@1 93.750 (93.750)
 * Prec@1 93.820
current lr 1.00000e-03
Grad=  tensor(0.6933, device='cuda:0')
Epoch: [269][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1458 (0.1458) ([0.010]+[0.136])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1469 (0.1477) ([0.011]+[0.136])	Prec@1 100.000 (99.760)
Epoch: [269][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1613 (0.1484) ([0.026]+[0.136])	Prec@1 99.219 (99.755)
Epoch: [269][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1404 (0.1480) ([0.005]+[0.135])	Prec@1 100.000 (99.777)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3429 (0.3429) ([0.208]+[0.135])	Prec@1 96.094 (96.094)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(1.0136, device='cuda:0')
Epoch: [270][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.1509 (0.1509) ([0.016]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1549 (0.1478) ([0.020]+[0.135])	Prec@1 98.438 (99.698)
Epoch: [270][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1442 (0.1480) ([0.009]+[0.135])	Prec@1 100.000 (99.736)
Epoch: [270][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1547 (0.1477) ([0.020]+[0.135])	Prec@1 99.219 (99.748)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3423 (0.3423) ([0.207]+[0.135])	Prec@1 96.094 (96.094)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(0.8823, device='cuda:0')
Epoch: [271][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1456 (0.1456) ([0.011]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1396 (0.1476) ([0.005]+[0.135])	Prec@1 100.000 (99.737)
Epoch: [271][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1817 (0.1472) ([0.047]+[0.135])	Prec@1 98.438 (99.732)
Epoch: [271][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1466 (0.1471) ([0.012]+[0.134])	Prec@1 99.219 (99.738)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3246 (0.3246) ([0.190]+[0.134])	Prec@1 95.312 (95.312)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(1.0508, device='cuda:0')
Epoch: [272][0/391]	Time 0.263 (0.263)	Data 0.138 (0.138)	Loss 0.1487 (0.1487) ([0.014]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1405 (0.1460) ([0.006]+[0.134])	Prec@1 100.000 (99.799)
Epoch: [272][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1532 (0.1464) ([0.019]+[0.134])	Prec@1 99.219 (99.778)
Epoch: [272][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1476 (0.1462) ([0.014]+[0.134])	Prec@1 99.219 (99.795)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3217 (0.3217) ([0.188]+[0.134])	Prec@1 96.094 (96.094)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.4557, device='cuda:0')
Epoch: [273][0/391]	Time 0.267 (0.267)	Data 0.143 (0.143)	Loss 0.1400 (0.1400) ([0.006]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1480 (0.1463) ([0.014]+[0.134])	Prec@1 99.219 (99.783)
Epoch: [273][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1380 (0.1457) ([0.004]+[0.134])	Prec@1 100.000 (99.802)
Epoch: [273][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1641 (0.1454) ([0.031]+[0.134])	Prec@1 99.219 (99.813)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3359 (0.3359) ([0.202]+[0.133])	Prec@1 96.094 (96.094)
 * Prec@1 93.740
current lr 1.00000e-03
Grad=  tensor(0.6571, device='cuda:0')
Epoch: [274][0/391]	Time 0.258 (0.258)	Data 0.133 (0.133)	Loss 0.1444 (0.1444) ([0.011]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1440 (0.1441) ([0.011]+[0.133])	Prec@1 100.000 (99.830)
Epoch: [274][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1479 (0.1442) ([0.015]+[0.133])	Prec@1 100.000 (99.833)
Epoch: [274][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1653 (0.1439) ([0.032]+[0.133])	Prec@1 99.219 (99.836)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3327 (0.3327) ([0.200]+[0.133])	Prec@1 96.094 (96.094)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.4215, device='cuda:0')
Epoch: [275][0/391]	Time 0.261 (0.261)	Data 0.132 (0.132)	Loss 0.1414 (0.1414) ([0.008]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1431 (0.1431) ([0.010]+[0.133])	Prec@1 100.000 (99.830)
Epoch: [275][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1363 (0.1434) ([0.004]+[0.133])	Prec@1 100.000 (99.833)
Epoch: [275][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1406 (0.1436) ([0.008]+[0.133])	Prec@1 100.000 (99.816)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3252 (0.3252) ([0.193]+[0.133])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.1835, device='cuda:0')
Epoch: [276][0/391]	Time 0.260 (0.260)	Data 0.135 (0.135)	Loss 0.1356 (0.1356) ([0.003]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1374 (0.1426) ([0.005]+[0.132])	Prec@1 100.000 (99.830)
Epoch: [276][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1378 (0.1425) ([0.006]+[0.132])	Prec@1 100.000 (99.833)
Epoch: [276][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1386 (0.1426) ([0.006]+[0.132])	Prec@1 100.000 (99.839)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3436 (0.3436) ([0.212]+[0.132])	Prec@1 94.531 (94.531)
 * Prec@1 93.520
current lr 1.00000e-03
Grad=  tensor(0.8661, device='cuda:0')
Epoch: [277][0/391]	Time 0.263 (0.263)	Data 0.135 (0.135)	Loss 0.1458 (0.1458) ([0.014]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1367 (0.1428) ([0.005]+[0.132])	Prec@1 100.000 (99.799)
Epoch: [277][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1412 (0.1423) ([0.009]+[0.132])	Prec@1 100.000 (99.813)
Epoch: [277][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1425 (0.1421) ([0.011]+[0.132])	Prec@1 99.219 (99.813)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3308 (0.3308) ([0.199]+[0.132])	Prec@1 95.312 (95.312)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.4791, device='cuda:0')
Epoch: [278][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1389 (0.1389) ([0.007]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1448 (0.1403) ([0.013]+[0.131])	Prec@1 100.000 (99.899)
Epoch: [278][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1383 (0.1404) ([0.007]+[0.131])	Prec@1 100.000 (99.895)
Epoch: [278][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1466 (0.1405) ([0.015]+[0.131])	Prec@1 100.000 (99.875)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3403 (0.3403) ([0.209]+[0.131])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-03
Grad=  tensor(0.2679, device='cuda:0')
Epoch: [279][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1363 (0.1363) ([0.005]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1424 (0.1402) ([0.011]+[0.131])	Prec@1 100.000 (99.915)
Epoch: [279][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1597 (0.1408) ([0.029]+[0.131])	Prec@1 98.438 (99.860)
Epoch: [279][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1422 (0.1409) ([0.011]+[0.131])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3348 (0.3348) ([0.204]+[0.131])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.4218, device='cuda:0')
Epoch: [280][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.1394 (0.1394) ([0.009]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1396 (0.1406) ([0.009]+[0.131])	Prec@1 100.000 (99.845)
Epoch: [280][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1417 (0.1402) ([0.011]+[0.130])	Prec@1 99.219 (99.837)
Epoch: [280][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1337 (0.1401) ([0.003]+[0.130])	Prec@1 100.000 (99.816)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3142 (0.3142) ([0.184]+[0.130])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.4173, device='cuda:0')
Epoch: [281][0/391]	Time 0.270 (0.270)	Data 0.142 (0.142)	Loss 0.1373 (0.1373) ([0.007]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1405 (0.1399) ([0.010]+[0.130])	Prec@1 100.000 (99.845)
Epoch: [281][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1381 (0.1397) ([0.008]+[0.130])	Prec@1 100.000 (99.833)
Epoch: [281][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1334 (0.1394) ([0.004]+[0.130])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3065 (0.3065) ([0.177]+[0.130])	Prec@1 96.094 (96.094)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.1973, device='cuda:0')
Epoch: [282][0/391]	Time 0.256 (0.256)	Data 0.131 (0.131)	Loss 0.1327 (0.1327) ([0.003]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1374 (0.1393) ([0.008]+[0.130])	Prec@1 100.000 (99.899)
Epoch: [282][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1427 (0.1390) ([0.013]+[0.130])	Prec@1 100.000 (99.872)
Epoch: [282][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1452 (0.1388) ([0.016]+[0.129])	Prec@1 99.219 (99.870)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3297 (0.3297) ([0.200]+[0.129])	Prec@1 96.094 (96.094)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.3560, device='cuda:0')
Epoch: [283][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.1356 (0.1356) ([0.006]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1346 (0.1389) ([0.005]+[0.129])	Prec@1 100.000 (99.807)
Epoch: [283][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1340 (0.1381) ([0.005]+[0.129])	Prec@1 100.000 (99.856)
Epoch: [283][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1354 (0.1381) ([0.006]+[0.129])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3101 (0.3101) ([0.181]+[0.129])	Prec@1 96.094 (96.094)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(1.2504, device='cuda:0')
Epoch: [284][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1421 (0.1421) ([0.013]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [284][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1323 (0.1365) ([0.004]+[0.129])	Prec@1 100.000 (99.923)
Epoch: [284][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1351 (0.1367) ([0.006]+[0.129])	Prec@1 100.000 (99.914)
Epoch: [284][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1364 (0.1370) ([0.008]+[0.129])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3037 (0.3037) ([0.175]+[0.128])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.5171, device='cuda:0')
Epoch: [285][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.1343 (0.1343) ([0.006]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1448 (0.1373) ([0.016]+[0.128])	Prec@1 100.000 (99.838)
Epoch: [285][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1371 (0.1375) ([0.009]+[0.128])	Prec@1 100.000 (99.837)
Epoch: [285][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1312 (0.1377) ([0.003]+[0.128])	Prec@1 100.000 (99.829)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3295 (0.3295) ([0.202]+[0.128])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.2182, device='cuda:0')
Epoch: [286][0/391]	Time 0.265 (0.265)	Data 0.141 (0.141)	Loss 0.1316 (0.1316) ([0.004]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1330 (0.1365) ([0.005]+[0.128])	Prec@1 100.000 (99.892)
Epoch: [286][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1304 (0.1367) ([0.003]+[0.128])	Prec@1 100.000 (99.860)
Epoch: [286][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1342 (0.1363) ([0.007]+[0.128])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3350 (0.3350) ([0.207]+[0.128])	Prec@1 96.094 (96.094)
 * Prec@1 93.610
current lr 1.00000e-03
Grad=  tensor(0.2058, device='cuda:0')
Epoch: [287][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.1310 (0.1310) ([0.003]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1371 (0.1354) ([0.010]+[0.127])	Prec@1 100.000 (99.899)
Epoch: [287][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1341 (0.1359) ([0.007]+[0.127])	Prec@1 100.000 (99.876)
Epoch: [287][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1352 (0.1359) ([0.008]+[0.127])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3056 (0.3056) ([0.179]+[0.127])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.2282, device='cuda:0')
Epoch: [288][0/391]	Time 0.262 (0.262)	Data 0.137 (0.137)	Loss 0.1303 (0.1303) ([0.003]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1358 (0.1346) ([0.009]+[0.127])	Prec@1 100.000 (99.853)
Epoch: [288][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1308 (0.1349) ([0.004]+[0.127])	Prec@1 100.000 (99.860)
Epoch: [288][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1382 (0.1352) ([0.011]+[0.127])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3138 (0.3138) ([0.187]+[0.127])	Prec@1 96.094 (96.094)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(0.2489, device='cuda:0')
Epoch: [289][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.1297 (0.1297) ([0.003]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1341 (0.1350) ([0.008]+[0.127])	Prec@1 100.000 (99.892)
Epoch: [289][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1292 (0.1346) ([0.003]+[0.126])	Prec@1 100.000 (99.895)
Epoch: [289][300/391]	Time 0.116 (0.113)	Data 0.000 (0.001)	Loss 0.1305 (0.1346) ([0.004]+[0.126])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3205 (0.3205) ([0.194]+[0.126])	Prec@1 96.094 (96.094)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.9790, device='cuda:0')
Epoch: [290][0/391]	Time 0.260 (0.260)	Data 0.135 (0.135)	Loss 0.1344 (0.1344) ([0.008]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1323 (0.1336) ([0.006]+[0.126])	Prec@1 100.000 (99.899)
Epoch: [290][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1388 (0.1339) ([0.013]+[0.126])	Prec@1 99.219 (99.887)
Epoch: [290][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1580 (0.1342) ([0.032]+[0.126])	Prec@1 99.219 (99.883)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3122 (0.3122) ([0.186]+[0.126])	Prec@1 96.094 (96.094)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.8548, device='cuda:0')
Epoch: [291][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.1351 (0.1351) ([0.009]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1297 (0.1336) ([0.004]+[0.126])	Prec@1 100.000 (99.899)
Epoch: [291][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1347 (0.1338) ([0.009]+[0.126])	Prec@1 100.000 (99.895)
Epoch: [291][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.1372 (0.1337) ([0.012]+[0.125])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2937 (0.2937) ([0.168]+[0.125])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.2534, device='cuda:0')
Epoch: [292][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1292 (0.1292) ([0.004]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1294 (0.1330) ([0.004]+[0.125])	Prec@1 100.000 (99.892)
Epoch: [292][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1339 (0.1327) ([0.009]+[0.125])	Prec@1 100.000 (99.930)
Epoch: [292][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1317 (0.1326) ([0.007]+[0.125])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3058 (0.3058) ([0.181]+[0.125])	Prec@1 95.312 (95.312)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.5042, device='cuda:0')
Epoch: [293][0/391]	Time 0.263 (0.263)	Data 0.141 (0.141)	Loss 0.1315 (0.1315) ([0.007]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1306 (0.1325) ([0.006]+[0.125])	Prec@1 100.000 (99.892)
Epoch: [293][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1416 (0.1319) ([0.017]+[0.125])	Prec@1 98.438 (99.903)
Epoch: [293][300/391]	Time 0.113 (0.111)	Data 0.000 (0.001)	Loss 0.1290 (0.1321) ([0.004]+[0.125])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2990 (0.2990) ([0.175]+[0.124])	Prec@1 96.094 (96.094)
 * Prec@1 93.970
current lr 1.00000e-03
Grad=  tensor(4.9676, device='cuda:0')
Epoch: [294][0/391]	Time 0.263 (0.263)	Data 0.138 (0.138)	Loss 0.1493 (0.1493) ([0.025]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [294][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1330 (0.1317) ([0.009]+[0.124])	Prec@1 100.000 (99.923)
Epoch: [294][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1362 (0.1319) ([0.012]+[0.124])	Prec@1 100.000 (99.903)
Epoch: [294][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1271 (0.1318) ([0.003]+[0.124])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3136 (0.3136) ([0.190]+[0.124])	Prec@1 95.312 (95.312)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.3378, device='cuda:0')
Epoch: [295][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1293 (0.1293) ([0.005]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1255 (0.1316) ([0.002]+[0.124])	Prec@1 100.000 (99.884)
Epoch: [295][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1312 (0.1315) ([0.007]+[0.124])	Prec@1 100.000 (99.883)
Epoch: [295][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1287 (0.1315) ([0.005]+[0.124])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2975 (0.2975) ([0.174]+[0.124])	Prec@1 95.312 (95.312)
 * Prec@1 93.820
current lr 1.00000e-03
Grad=  tensor(2.1570, device='cuda:0')
Epoch: [296][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1371 (0.1371) ([0.014]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [296][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1250 (0.1317) ([0.001]+[0.123])	Prec@1 100.000 (99.822)
Epoch: [296][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1330 (0.1314) ([0.010]+[0.123])	Prec@1 100.000 (99.852)
Epoch: [296][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1399 (0.1312) ([0.017]+[0.123])	Prec@1 99.219 (99.862)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3219 (0.3219) ([0.199]+[0.123])	Prec@1 95.312 (95.312)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(1.6117, device='cuda:0')
Epoch: [297][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1323 (0.1323) ([0.009]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1274 (0.1298) ([0.004]+[0.123])	Prec@1 100.000 (99.946)
Epoch: [297][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1382 (0.1301) ([0.015]+[0.123])	Prec@1 100.000 (99.922)
Epoch: [297][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1287 (0.1301) ([0.006]+[0.123])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3059 (0.3059) ([0.183]+[0.123])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.2761, device='cuda:0')
Epoch: [298][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.1268 (0.1268) ([0.004]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1264 (0.1305) ([0.004]+[0.123])	Prec@1 100.000 (99.876)
Epoch: [298][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1333 (0.1301) ([0.011]+[0.123])	Prec@1 100.000 (99.880)
Epoch: [298][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1289 (0.1298) ([0.007]+[0.122])	Prec@1 100.000 (99.886)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3077 (0.3077) ([0.185]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.2802, device='cuda:0')
Epoch: [299][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.1270 (0.1270) ([0.005]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1252 (0.1290) ([0.003]+[0.122])	Prec@1 100.000 (99.930)
Epoch: [299][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1258 (0.1297) ([0.004]+[0.122])	Prec@1 100.000 (99.895)
Epoch: [299][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1317 (0.1294) ([0.010]+[0.122])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3081 (0.3081) ([0.186]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.860

 Elapsed time for training  3:57:46.029262

 sparsity of   [0.0, 0.8148148059844971, 0.5185185074806213, 0.9259259104728699, 0.9629629850387573, 0.8148148059844971, 0.9629629850387573, 0.9629629850387573, 0.0, 0.9629629850387573, 0.9629629850387573, 0.9259259104728699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9259259104728699, 0.0, 0.8888888955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.7037037014961243, 0.0, 0.0, 0.9629629850387573, 0.0, 0.5555555820465088, 0.0, 0.0, 0.6666666865348816, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.4444444477558136, 0.0, 0.9629629850387573, 0.0, 0.0, 0.6666666865348816, 0.9629629850387573]

 sparsity of   [0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.9947916865348816, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9930555820465088, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.1302083283662796, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.453125, 0.0, 0.0, 0.0, 0.453125, 0.65625, 0.0, 0.984375, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.546875, 0.0, 0.96875, 0.96875, 0.984375, 0.09375, 0.0, 0.453125, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.953125, 0.984375, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.453125, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.453125, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.453125, 0.96875, 0.96875, 0.984375, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.453125]

 sparsity of   [0.359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.359375, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.984375, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.359375, 0.96875, 0.984375, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.359375, 0.0, 0.96875, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.359375, 0.96875, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0625, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.23828125, 0.23828125, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.23828125, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.23828125, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.23828125, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.23828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.390625, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9982638955116272, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.484375, 0.0, 0.0, 0.96875, 0.484375, 0.96875, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.96875, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.234375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.484375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.484375, 0.484375, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.9375, 0.0, 0.484375, 0.96875, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.4375, 0.484375]

 sparsity of   [0.24609375, 0.03125, 0.0, 0.1328125, 0.01171875, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.1015625, 0.05859375, 0.03125, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0234375, 0.1015625, 0.0, 0.82421875, 0.90625, 0.0, 0.0, 0.90234375, 0.0625, 0.20703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2890625, 0.0, 0.0078125, 0.59765625, 0.6328125, 0.98828125, 0.3203125, 0.98828125, 0.4765625, 0.0, 0.0, 0.9921875, 0.06640625, 0.0, 0.2578125, 0.9921875, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0694444477558136, 0.0, 0.1163194477558136, 0.01909722201526165, 0.0, 0.0, 0.8211805820465088, 0.0, 0.9947916865348816, 0.8645833134651184, 0.09375, 0.9965277910232544, 0.0, 0.0451388880610466, 0.02604166604578495, 0.0, 0.0, 0.01909722201526165, 0.0, 0.0329861119389534, 0.0, 0.5034722089767456, 0.0, 0.0, 0.819444477558136, 0.0, 0.0, 0.0, 0.9079861044883728, 0.0381944440305233, 0.0, 0.9704861044883728, 0.7934027910232544, 0.0, 0.0555555559694767, 0.0, 0.0, 0.0, 0.0173611119389534, 0.0850694477558136, 0.0729166641831398, 0.0, 0.0, 0.0, 0.02604166604578495, 0.0, 0.0, 0.0, 0.013888888992369175, 0.0, 0.0, 0.0, 0.0, 0.1145833358168602, 0.0, 0.0, 0.1128472238779068, 0.3819444477558136, 0.0, 0.0, 0.03125, 0.0, 0.9965277910232544, 0.0]

 sparsity of   [0.0, 0.0, 0.015625, 0.4375, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.4375, 0.4375, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.4375, 0.0, 0.0, 0.4375, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.03125, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34375, 0.0, 0.0, 0.4375, 0.4375, 0.0, 0.0, 0.0, 0.671875, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.4375]

 sparsity of   [0.0, 0.55859375, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.015625, 0.0, 0.1328125, 0.0, 0.98828125, 0.98828125, 0.1953125, 0.0, 0.0, 0.0234375, 0.0234375, 0.9921875, 0.0, 0.00390625, 0.24609375, 0.0, 0.99609375, 0.0, 0.015625, 0.0234375, 0.0, 0.0234375, 0.0234375, 0.0234375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0234375, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.9921875, 0.796875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0234375, 0.0, 0.0234375, 0.015625, 0.0, 0.0, 0.0078125, 0.40234375, 0.0234375, 0.98828125, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.9921875, 0.0, 0.0, 0.0234375, 0.99609375, 0.99609375, 0.0703125, 0.0, 0.06640625, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.7578125, 0.0234375, 0.0, 0.0, 0.015625, 0.01171875, 0.0, 0.00390625, 0.98828125, 0.0, 0.0]

 sparsity of   [0.1041666641831398, 0.953125, 0.0338541679084301, 0.8611111044883728, 0.0, 0.02951388992369175, 0.013888888992369175, 0.0364583320915699, 0.0416666679084301, 0.0164930559694767, 0.6449652910232544, 0.944444477558136, 0.02604166604578495, 0.8532986044883728, 0.0668402761220932, 0.4375, 0.0, 0.9982638955116272, 0.9227430820465088, 0.1458333283662796, 0.0, 0.9982638955116272, 0.9019097089767456, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9539930820465088, 0.2005208283662796, 0.9149305820465088, 0.0, 0.8871527910232544, 0.8871527910232544, 0.0, 0.9982638955116272, 0.553819477558136, 0.9661458134651184, 0.328125, 0.8532986044883728, 0.0, 0.0598958320915699, 0.0, 0.03125, 0.02604166604578495, 0.140625, 0.0, 0.0243055559694767, 0.0, 0.02083333395421505, 0.0381944440305233, 0.9982638955116272, 0.0, 0.165798619389534, 0.1241319477558136, 0.9973958134651184, 0.9149305820465088, 0.0, 0.0590277798473835, 0.25, 0.0815972238779068, 0.0347222238779068, 0.0, 0.0, 0.0, 0.0355902798473835, 0.0243055559694767, 0.0164930559694767, 0.9079861044883728, 0.0381944440305233, 0.0442708320915699, 0.0, 0.0, 0.0, 0.0243055559694767, 0.999131977558136, 0.0, 0.9973958134651184, 0.0321180559694767, 0.905381977558136, 0.8715277910232544, 0.9253472089767456, 0.0008680555620230734, 0.0, 0.796875, 0.0, 0.9982638955116272, 0.0, 0.0677083358168602, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0520833320915699, 0.01215277798473835, 0.9982638955116272, 0.9973958134651184, 0.0651041641831398, 0.9270833134651184, 0.0, 0.9982638955116272, 0.0, 0.0, 0.1145833358168602, 0.999131977558136, 0.323784738779068, 0.0, 0.0, 0.0, 0.874131977558136, 0.0, 0.8645833134651184, 0.9982638955116272, 0.0, 0.0243055559694767, 0.0381944440305233, 0.0, 0.0, 0.0347222238779068, 0.9973958134651184, 0.2543402910232544, 0.0989583358168602, 0.0460069440305233, 0.0329861119389534, 0.8828125, 0.0078125, 0.0564236119389534]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.3203125, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.640625, 0.046875, 0.640625, 0.640625, 0.9921875, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.1015625, 0.9921875, 0.984375, 0.8203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.640625, 0.0, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.546875, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.984375, 0.0, 0.6328125, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.640625, 0.796875, 0.0, 0.0, 0.640625, 0.3515625, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9765625, 0.640625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0546875, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.984375, 0.9765625, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.625, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.640625, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.640625, 0.0, 0.640625, 0.984375, 0.984375, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.3671875, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.1328125, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.9921875, 0.109375, 0.0, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.984375, 0.0, 0.1015625, 0.984375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.1484375, 0.0, 0.984375, 0.0, 0.0, 0.5625, 0.640625, 0.0, 0.0, 0.96875, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.40625, 0.984375, 0.640625, 0.0, 0.9765625, 0.0, 0.984375, 0.9765625, 0.0, 0.640625, 0.0, 0.65625, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.640625, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.28125, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0234375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0234375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0234375, 0.8046875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.01953125, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0234375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.0234375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0234375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0234375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0234375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0234375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0234375, 0.0, 0.0, 0.0234375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875]

 sparsity of   [0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.994140625, 0.0, 0.0, 0.998046875, 0.296875, 0.99609375, 0.296875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.296875, 0.99609375, 0.99609375, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.99609375, 0.0, 0.296875, 0.2890625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.296875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.296875, 0.99609375, 0.0, 0.0, 0.296875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.296875, 0.0, 0.0, 0.28515625, 0.0, 0.294921875, 0.296875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.296875, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.994140625, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.5, 0.9765625, 0.9921875, 0.0, 0.5, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0546875, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.9765625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.203125, 0.0, 0.0, 0.5, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.9921875, 0.0, 0.53125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.8671875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.984375, 0.0, 0.0, 0.0546875, 0.5, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.5, 0.0, 0.9765625, 0.5, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.5, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.1015625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.1953125, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.0, 0.5, 0.4296875, 0.0, 0.0, 0.2421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.1796875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.890625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.5, 0.4296875, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.40625, 0.9921875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.984375]

 sparsity of   [0.99609375, 0.99609375, 0.486328125, 0.0, 0.005859375, 0.900390625, 0.0, 0.080078125, 0.0, 0.0, 0.0, 0.0, 0.376953125, 0.12890625, 0.99609375, 0.0, 0.576171875, 0.0, 0.8828125, 0.0, 0.0, 0.8046875, 0.0, 0.994140625, 0.193359375, 0.173828125, 0.0, 0.07421875, 0.0, 0.048828125, 0.87109375, 0.587890625, 0.0234375, 0.0390625, 0.994140625, 0.02734375, 0.169921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0859375, 0.0, 0.9921875, 0.998046875, 0.99609375, 0.265625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.021484375, 0.08984375, 0.0, 0.3671875, 0.0, 0.125, 0.03125, 0.0, 0.0, 0.046875, 0.0, 0.49609375, 0.07421875, 0.0, 0.0, 0.212890625, 0.16015625, 0.0, 0.99609375, 0.0, 0.07421875, 0.01953125, 0.021484375, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.892578125, 0.00390625, 0.0, 0.99609375, 0.041015625, 0.0859375, 0.759765625, 0.072265625, 0.330078125, 0.99609375, 0.080078125, 0.064453125, 0.0, 0.0, 0.130859375, 0.0, 0.99609375, 0.0, 0.0, 0.146484375, 0.0, 0.0, 0.0, 0.994140625, 0.330078125, 0.99609375, 0.013671875, 0.0, 0.048828125, 0.1484375, 0.06640625, 0.0, 0.0, 0.314453125, 0.025390625, 0.1171875, 0.99609375, 0.0, 0.0390625, 0.154296875, 0.0, 0.99609375, 0.037109375, 0.0, 0.0, 0.150390625]

 sparsity of   [0.0434027798473835, 0.126736119389534, 0.944444477558136, 0.1006944477558136, 0.0607638880610466, 0.0451388880610466, 0.02604166604578495, 0.013888888992369175, 0.9982638955116272, 0.0225694440305233, 0.5972222089767456, 0.0460069440305233, 0.0243055559694767, 0.078125, 0.01128472201526165, 0.0, 0.0329861119389534, 0.0, 0.02951388992369175, 0.0, 0.0329861119389534, 0.01215277798473835, 0.7664930820465088, 0.0, 0.4609375, 0.2560763955116272, 0.0434027798473835, 0.0225694440305233, 0.01128472201526165, 0.1553819477558136, 0.1206597238779068, 0.010416666977107525, 0.09375, 0.0, 0.0, 0.046875, 0.02604166604578495, 0.0225694440305233, 0.0555555559694767, 0.1354166716337204, 0.02083333395421505, 0.0, 0.013020833022892475, 0.953125, 0.0850694477558136, 0.02951388992369175, 0.0086805559694767, 0.1241319477558136, 0.999131977558136, 0.0, 0.8090277910232544, 0.1310763955116272, 0.1692708283662796, 0.0460069440305233, 0.0173611119389534, 0.0, 0.0, 0.1354166716337204, 0.0, 0.6831597089767456, 0.1354166716337204, 0.0, 0.8463541865348816, 0.0833333358168602, 0.212673619389534, 0.0, 0.0894097238779068, 0.0876736119389534, 0.0086805559694767, 0.015625, 0.0529513880610466, 0.0164930559694767, 0.1328125, 0.4809027910232544, 0.0321180559694767, 0.0755208358168602, 0.1727430522441864, 0.1041666641831398, 0.0338541679084301, 0.0, 0.9973958134651184, 0.0894097238779068, 0.999131977558136, 0.0538194440305233, 0.952256977558136, 0.0173611119389534, 0.0, 0.6362847089767456, 0.2013888955116272, 0.1440972238779068, 0.02951388992369175, 0.1536458283662796, 0.9774305820465088, 0.953125, 0.0, 0.1032986119389534, 0.1215277761220932, 0.8463541865348816, 0.9392361044883728, 0.0434027798473835, 0.0737847238779068, 0.0, 0.01996527798473835, 0.0, 0.0, 0.0546875, 0.0616319440305233, 0.0303819440305233, 0.0746527761220932, 0.0512152798473835, 0.0, 0.0, 0.0833333358168602, 0.01128472201526165, 0.140625, 0.0460069440305233, 0.01128472201526165, 0.9982638955116272, 0.0, 0.0, 0.0, 0.1371527761220932, 0.0703125, 0.9982638955116272, 0.0164930559694767, 0.0, 0.1119791641831398, 0.0173611119389534]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7421875, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.75, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.765625, 0.75, 0.0, 0.0, 0.0, 0.765625, 0.75, 0.0, 0.0, 0.0, 0.984375, 0.7421875, 0.0, 0.0, 0.7265625, 0.0, 0.984375, 0.0, 0.7109375, 0.0, 0.0, 0.7265625, 0.0, 0.0, 0.0, 0.7578125, 0.7421875, 0.0, 0.7265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.984375, 0.7421875, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.765625, 0.0, 0.71875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7265625, 0.984375, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.734375, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0234375, 0.0, 0.0, 0.7421875, 0.765625, 0.0, 0.7421875, 0.0, 0.9921875, 0.0, 0.0, 0.7421875, 0.765625, 0.984375, 0.7265625, 0.0, 0.765625, 0.765625, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.75, 0.7421875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7421875, 0.7578125, 0.75, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.703125, 0.0, 0.0, 0.765625, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.734375, 0.765625, 0.0, 0.7421875, 0.1484375, 0.0, 0.0, 0.0, 0.7421875, 0.75, 0.765625, 0.0, 0.0, 0.0, 0.75, 0.734375, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1484375, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.734375, 0.7265625, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.265625, 0.0, 0.0, 0.7421875, 0.0, 0.7421875, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.734375, 0.0, 0.734375, 0.0, 0.765625, 0.0, 0.0, 0.046875, 0.71875, 0.0, 0.0, 0.0, 0.75, 0.1875, 0.9921875, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.734375, 0.7421875, 0.734375, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.84375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.7421875, 0.9921875, 0.6640625, 0.765625, 0.0, 0.0, 0.7578125, 0.984375, 0.0, 0.734375, 0.0, 0.21875, 0.984375, 0.7421875, 0.0, 0.0, 0.7421875, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.7421875, 0.7421875, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.75, 0.765625, 0.7265625, 0.0, 0.734375, 0.0, 0.75, 0.0, 0.734375, 0.7578125, 0.0, 0.9921875, 0.0, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.75, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.75, 0.0, 0.1484375, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7421875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.75, 0.0, 0.0, 0.0, 0.984375, 0.0078125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.703125, 0.0, 0.0, 0.7265625, 0.734375, 0.75, 0.9765625, 0.7578125, 0.984375, 0.0, 0.0, 0.7421875, 0.0, 0.984375, 0.765625, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.734375, 0.0, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.984375, 0.0, 0.75, 0.75, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.75, 0.765625, 0.0, 0.0, 0.0, 0.7265625, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.1875, 0.984375, 0.0, 0.75, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.75, 0.0, 0.71875, 0.0, 0.7421875, 0.75, 0.1328125, 0.0, 0.0, 0.0, 0.984375, 0.7578125, 0.0, 0.7421875, 0.0, 0.0]

 sparsity of   [0.9375, 0.0, 0.017578125, 0.0, 0.88671875, 0.072265625, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7890625, 0.0, 0.083984375, 0.99609375, 0.083984375, 0.8828125, 0.0, 0.8984375, 0.0078125, 0.0625, 0.013671875, 0.0, 0.99609375, 0.001953125, 0.046875, 0.0, 0.0, 0.00390625, 0.0078125, 0.146484375, 0.955078125, 0.0, 0.01953125, 0.83203125, 0.0, 0.994140625, 0.017578125, 0.0, 0.0, 0.0, 0.037109375, 0.01171875, 0.00390625, 0.123046875, 0.0, 0.80078125, 0.962890625, 0.0, 0.0, 0.009765625, 0.0, 0.029296875, 0.0, 0.0, 0.0, 0.88671875, 0.013671875, 0.046875, 0.0, 0.009765625, 0.0, 0.99609375, 0.94140625, 0.0, 0.0078125, 0.0, 0.99609375, 0.0, 0.0, 0.966796875, 0.87109375, 0.99609375, 0.0, 0.0, 0.109375, 0.060546875, 0.0, 0.876953125, 0.09765625, 0.041015625, 0.0, 0.01171875, 0.0234375, 0.005859375, 0.0, 0.0, 0.029296875, 0.033203125, 0.0, 0.974609375, 0.0, 0.0, 0.076171875, 0.00390625, 0.01171875, 0.0546875, 0.0, 0.001953125, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.154296875, 0.0, 0.884765625, 0.0, 0.931640625, 0.0, 0.0, 0.00390625, 0.99609375, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.99609375, 0.0, 0.021484375, 0.0, 0.99609375, 0.99609375, 0.072265625]

 sparsity of   [0.0720486119389534, 0.1041666641831398, 0.0928819477558136, 0.0034722222480922937, 0.999131977558136, 0.0, 0.0, 0.0, 0.9262152910232544, 0.0425347238779068, 0.01909722201526165, 0.9973958134651184, 0.0607638880610466, 0.0, 0.0, 0.9340277910232544, 0.0, 0.01822916604578495, 0.014756944961845875, 0.0, 0.0998263880610466, 0.0859375, 0.0, 0.0, 0.01822916604578495, 0.999131977558136, 0.8515625, 0.0, 0.0, 0.999131977558136, 0.0850694477558136, 0.9487847089767456, 0.2039930522441864, 0.0008680555620230734, 0.1796875, 0.02083333395421505, 0.0390625, 0.9982638955116272, 0.8142361044883728, 0.0, 0.0, 0.0338541679084301, 0.078993059694767, 0.0, 0.9982638955116272, 0.0, 0.796875, 0.0, 0.9166666865348816, 0.0694444477558136, 0.02777777798473835, 0.9557291865348816, 0.9973958134651184, 0.03125, 0.2586805522441864, 0.0, 0.8940972089767456, 0.0034722222480922937, 0.9105902910232544, 0.0, 0.811631977558136, 0.013888888992369175, 0.0, 0.9045138955116272, 0.01822916604578495, 0.8793402910232544, 0.0815972238779068, 0.1059027761220932, 0.0, 0.09375, 0.1493055522441864, 0.0, 0.0, 0.0, 0.02083333395421505, 0.0, 0.0, 0.0164930559694767, 0.0373263880610466, 0.1336805522441864, 0.0, 0.9982638955116272, 0.013888888992369175, 0.1510416716337204, 0.0, 0.1241319477558136, 0.0885416641831398, 0.01128472201526165, 0.9184027910232544, 0.0, 0.0, 0.0, 0.0, 0.0364583320915699, 0.9982638955116272, 0.9982638955116272, 0.0, 0.3064236044883728, 0.0, 0.1015625, 0.9626736044883728, 0.1067708358168602, 0.1519097238779068, 0.009548611007630825, 0.126736119389534, 0.0407986119389534, 0.0685763880610466, 0.8611111044883728, 0.0460069440305233, 0.0243055559694767, 0.046875, 0.013020833022892475, 0.9982638955116272, 0.0494791679084301, 0.0, 0.8454861044883728, 0.671875, 0.0164930559694767, 0.013888888992369175, 0.0, 0.0390625, 0.0173611119389534, 0.1293402761220932, 0.0, 0.0329861119389534, 0.0, 0.921875, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6796875, 0.0, 0.6796875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.6796875, 0.0, 0.0, 0.6875, 0.6875, 0.6796875, 0.0, 0.6875, 0.0, 0.9921875, 0.0, 0.0, 0.0546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.3828125, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.6875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.984375, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.6875, 0.6875, 0.8359375, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.6796875, 0.6875, 0.0, 0.0859375, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.4609375, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.3984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.6875, 0.0625, 0.6875, 0.0, 0.9921875, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.6875, 0.9765625, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.71875, 0.0, 0.0, 0.6875, 0.984375, 0.6796875, 0.984375, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.5546875, 0.0, 0.0, 0.6484375, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6796875, 0.6875, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.984375, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.984375, 0.6875, 0.6875, 0.6796875, 0.0, 0.0]

 sparsity of   [0.0, 0.99609375, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.041015625, 0.0, 0.0, 0.0, 0.041015625, 0.041015625, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.0390625, 0.0, 0.0390625, 0.0, 0.998046875, 0.0, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.041015625, 0.0, 0.0, 0.99609375, 0.037109375, 0.0, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.041015625, 0.0, 0.99609375, 0.998046875, 0.041015625, 0.0, 0.0, 0.0, 0.041015625, 0.99609375, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.041015625, 0.99609375, 0.041015625, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.041015625, 0.0, 0.99609375, 0.0390625, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.041015625, 0.99609375, 0.0, 0.99609375, 0.0, 0.041015625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.041015625, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.037109375, 0.037109375, 0.037109375, 0.998046875, 0.041015625, 0.99609375, 0.041015625, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.041015625, 0.998046875, 0.041015625, 0.0, 0.0390625, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.041015625, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.041015625, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.041015625, 0.041015625, 0.041015625, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.041015625, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.041015625]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.1115451380610466, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1067708358168602, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.3680555522441864, 0.33984375, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.1254340261220932, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136]

 sparsity of   [0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.98828125, 0.0, 0.7265625, 0.98828125, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.7265625, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7265625, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.7265625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.7265625, 0.0, 0.0, 0.9921875, 0.40234375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.0, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.0, 0.98828125, 0.0, 0.7421875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.8515625, 0.7265625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.7265625, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.7265625, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.7265625, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.7265625, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.7265625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.9921875, 0.7265625, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.47265625, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.0, 0.7265625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.7265625, 0.9921875, 0.5078125, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.7265625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.72265625, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7265625, 0.72265625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.7265625, 0.9921875, 0.9921875, 0.1328125, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.15234375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.7265625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.7265625, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.72265625, 0.9921875, 0.0, 0.99609375, 0.0, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.71875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.7265625, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.7265625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.19140625, 0.7265625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6640625, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.484375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.44921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.7265625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7265625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0859375, 0.0, 0.7265625, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.7265625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.34765625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.72265625, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.7265625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.7265625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.7265625, 0.7265625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.72265625, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.40234375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875]

 sparsity of   [0.99609375, 0.8359375, 0.0, 0.0, 0.072265625, 0.0, 0.99609375, 0.0, 0.0, 0.78515625, 0.080078125, 0.99609375, 0.0390625, 0.767578125, 0.01171875, 0.041015625, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.87109375, 0.0, 0.021484375, 0.064453125, 0.21875, 0.05859375, 0.0, 0.900390625, 0.99609375, 0.0, 0.99609375, 0.072265625, 0.0, 0.056640625, 0.0, 0.99609375, 0.99609375, 0.017578125, 0.0546875, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.83203125, 0.0703125, 0.033203125, 0.77734375, 0.0, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.083984375, 0.0, 0.0, 0.99609375, 0.86328125, 0.0625, 0.998046875, 0.8984375, 0.99609375, 0.08203125, 0.044921875, 0.99609375, 0.998046875, 0.99609375, 0.087890625, 0.166015625, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.16015625, 0.99609375, 0.0, 0.0703125, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.142578125, 0.0, 0.0, 0.115234375, 0.068359375, 0.994140625, 0.99609375, 0.0, 0.0, 0.994140625, 0.99609375, 0.03515625, 0.01953125, 0.994140625, 0.0, 0.994140625, 0.091796875, 0.994140625, 0.0, 0.154296875, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.994140625, 0.0, 0.08203125, 0.095703125, 0.0, 0.99609375, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.1484375, 0.771484375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0390625, 0.0, 0.998046875, 0.99609375, 0.03125, 0.103515625, 0.0, 0.99609375, 0.994140625, 0.306640625, 0.0, 0.037109375, 0.99609375, 0.0390625, 0.98828125, 0.0, 0.009765625, 0.884765625, 0.02734375, 0.99609375, 0.0, 0.99609375, 0.0, 0.2109375, 0.01171875, 0.1015625, 0.99609375, 0.0, 0.0, 0.859375, 0.0390625, 0.0, 0.998046875, 0.99609375, 0.12890625, 0.99609375, 0.10546875, 0.064453125, 0.0, 0.0, 0.99609375, 0.994140625, 0.060546875, 0.076171875, 0.0, 0.015625, 0.0625, 0.0, 0.01953125, 0.0, 0.99609375, 0.0, 0.22265625, 0.998046875, 0.0, 0.998046875, 0.0, 0.99609375, 0.095703125, 0.994140625, 0.0, 0.0, 0.046875, 0.0, 0.146484375, 0.998046875, 0.09765625, 0.99609375, 0.99609375, 0.99609375, 0.001953125, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.14453125, 0.099609375, 0.25390625, 0.00390625, 0.99609375, 0.005859375, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.041015625, 0.05078125, 0.0, 0.0, 0.99609375, 0.998046875, 0.046875, 0.021484375, 0.0, 0.99609375, 0.029296875, 0.99609375, 0.99609375, 0.994140625, 0.0, 0.994140625, 0.998046875, 0.0, 0.01171875, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.255859375, 0.998046875, 0.0, 0.0078125, 0.99609375, 0.998046875, 0.005859375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.2578125, 0.06640625, 0.0, 0.99609375, 0.142578125, 0.07421875, 0.474609375, 0.04296875, 0.169921875, 0.0, 0.99609375, 0.87890625, 0.0, 0.111328125, 0.0, 0.99609375, 0.99609375, 0.0, 0.07421875, 0.99609375, 0.0, 0.0, 0.083984375, 0.02734375, 0.75390625, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0078125, 0.99609375, 0.0, 0.0, 0.412109375, 0.849609375, 0.12109375, 0.0, 0.0, 0.037109375, 0.302734375, 0.0, 0.0, 0.013671875, 0.99609375, 0.041015625, 0.037109375, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.017578125, 0.994140625, 0.0, 0.0, 0.06640625, 0.0, 0.99609375, 0.095703125, 0.0, 0.0, 0.0, 0.0, 0.048828125, 0.0, 0.775390625, 0.0, 0.212890625, 0.99609375, 0.998046875, 0.041015625, 0.99609375, 0.99609375, 0.0, 0.0, 0.994140625, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.052734375, 0.48828125, 0.0, 0.771484375, 0.998046875, 0.794921875, 0.193359375, 0.998046875, 0.205078125, 0.99609375, 0.0078125, 0.0, 0.095703125, 0.99609375, 0.04296875, 0.0, 0.0, 0.99609375, 0.08984375, 0.0, 0.849609375, 0.0, 0.99609375, 0.0, 0.466796875, 0.005859375, 0.9921875, 0.0, 0.0, 0.99609375, 0.052734375, 0.013671875, 0.0, 0.05859375, 0.998046875, 0.998046875, 0.99609375, 0.00390625, 0.22265625, 0.99609375, 0.21484375, 0.998046875, 0.0, 0.994140625, 0.0, 0.08984375, 0.0, 0.392578125, 0.853515625, 0.99609375, 0.0078125, 0.244140625, 0.009765625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.013671875, 0.99609375, 0.302734375, 0.0, 0.767578125, 0.056640625, 0.8125, 0.16015625, 0.0, 0.041015625, 0.0, 0.99609375, 0.2265625, 0.06640625, 0.796875, 0.0, 0.0, 0.03125, 0.146484375, 0.072265625, 0.009765625, 0.99609375, 0.99609375, 0.109375, 0.0, 0.181640625, 0.998046875, 0.0, 0.1484375, 0.994140625, 0.99609375, 0.0, 0.0078125, 0.1015625, 0.107421875, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.087890625, 0.998046875, 0.83203125, 0.0, 0.052734375, 0.080078125, 0.009765625, 0.068359375, 0.0, 0.99609375, 0.99609375, 0.037109375, 0.99609375, 0.048828125, 0.13671875, 0.99609375, 0.998046875, 0.021484375, 0.00390625, 0.0, 0.998046875, 0.109375, 0.994140625, 0.029296875, 0.005859375, 0.998046875, 0.99609375, 0.05078125, 0.998046875, 0.0, 0.0, 0.994140625, 0.0, 0.708984375, 0.076171875, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.0390625, 0.0, 0.064453125, 0.0703125, 0.91796875, 0.193359375, 0.794921875, 0.99609375, 0.998046875, 0.0, 0.806640625, 0.99609375, 0.03125, 0.830078125, 0.0, 0.0, 0.12109375, 0.0, 0.0390625, 0.0, 0.0, 0.0390625, 0.99609375, 0.16015625, 0.037109375, 0.15625, 0.994140625, 0.197265625, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.994140625, 0.84375, 0.99609375, 0.041015625, 0.99609375, 0.994140625, 0.994140625, 0.998046875, 0.0, 0.087890625, 0.0078125, 0.0, 0.0, 0.0, 0.134765625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.02734375, 0.0, 0.013671875, 0.1796875, 0.0, 0.033203125, 0.0, 0.998046875, 0.005859375, 0.994140625, 0.0, 0.041015625, 0.998046875, 0.244140625, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.38671875, 0.125, 0.998046875, 0.076171875, 0.083984375, 0.99609375, 0.998046875, 0.0, 0.720703125, 0.056640625, 0.08984375, 0.0, 0.99609375, 0.998046875, 0.0, 0.994140625, 0.99609375, 0.99609375, 0.0, 0.0, 0.07421875, 0.068359375, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.05859375, 0.99609375, 0.998046875, 0.056640625, 0.0, 0.12890625, 0.99609375, 0.037109375, 0.99609375, 0.0, 0.009765625, 0.99609375, 0.99609375, 0.0, 0.853515625, 0.0, 0.99609375, 0.0859375, 0.1875, 0.0, 0.328125, 0.0, 0.15625, 0.10546875, 0.0, 0.99609375, 0.998046875, 0.85546875, 0.0, 0.015625, 0.830078125, 0.0, 0.0, 0.115234375, 0.44140625, 0.2734375, 0.857421875, 0.998046875, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.501953125, 0.111328125, 0.04296875, 0.0, 0.0, 0.1484375, 0.0, 0.302734375, 0.998046875, 0.0, 0.0, 0.025390625, 0.998046875, 0.99609375, 0.0, 0.517578125, 0.99609375, 0.998046875, 0.99609375, 0.833984375, 0.052734375, 0.0625, 0.994140625, 0.0, 0.0, 0.4296875, 0.0, 0.99609375, 0.005859375, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.115234375, 0.99609375, 0.078125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.205078125, 0.99609375, 0.99609375, 0.19921875, 0.138671875, 0.998046875, 0.046875, 0.0, 0.00390625, 0.248046875, 0.0, 0.998046875, 0.0, 0.47265625, 0.0, 0.041015625, 0.99609375, 0.041015625, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.05078125, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.998046875, 0.998046875, 0.99609375, 0.994140625, 0.490234375, 0.044921875, 0.99609375, 0.0, 0.15625, 0.986328125, 0.0, 0.0, 0.05859375, 0.37890625, 0.99609375, 0.0, 0.470703125, 0.015625, 0.298828125, 0.99609375, 0.998046875, 0.849609375, 0.03125, 0.99609375, 0.99609375, 0.0, 0.994140625, 0.115234375, 0.0, 0.99609375, 0.814453125, 0.0, 0.0, 0.994140625, 0.99609375, 0.0, 0.994140625, 0.0, 0.16796875, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.05859375, 0.15625, 0.0, 0.0, 0.0, 0.044921875, 0.03515625, 0.05859375, 0.087890625, 0.875, 0.0, 0.0, 0.0, 0.060546875, 0.998046875, 0.0, 0.99609375, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.09765625, 0.99609375, 0.13671875, 0.0, 0.02734375, 0.0, 0.0234375, 0.0, 0.0, 0.4453125, 0.0, 0.150390625, 0.365234375, 0.0, 0.0, 0.0, 0.0, 0.076171875, 0.0, 0.0, 0.998046875, 0.05859375, 0.021484375, 0.494140625, 0.013671875, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.130859375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.126953125, 0.111328125, 0.068359375, 0.0859375, 0.99609375, 0.998046875, 0.02734375, 0.998046875, 0.0, 0.388671875, 0.0, 0.07421875, 0.853515625, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.037109375, 0.0, 0.0, 0.0, 0.99609375, 0.009765625, 0.0078125, 0.0, 0.0, 0.0, 0.095703125, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.083984375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.14453125, 0.99609375, 0.994140625, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.99609375, 0.99609375, 0.380859375, 0.138671875, 0.994140625, 0.99609375, 0.912109375, 0.99609375, 0.99609375, 0.029296875, 0.029296875, 0.0234375, 0.3125, 0.033203125, 0.0, 0.0078125, 0.412109375, 0.33203125, 0.046875, 0.09375, 0.083984375, 0.0, 0.0, 0.005859375, 0.037109375, 0.033203125, 0.08984375, 0.99609375, 0.99609375, 0.09375, 0.99609375, 0.99609375, 0.99609375, 0.03515625, 0.140625, 0.025390625, 0.0, 0.064453125, 0.99609375, 0.087890625, 0.017578125, 0.21484375, 0.19140625, 0.0, 0.99609375, 0.85546875, 0.34765625, 0.0, 0.998046875, 0.99609375, 0.994140625, 0.099609375, 0.0546875, 0.005859375, 0.0, 0.0, 0.99609375, 0.30859375, 0.0390625, 0.0, 0.478515625, 0.078125, 0.998046875, 0.390625, 0.0, 0.001953125, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.994140625, 0.54296875, 0.009765625, 0.0, 0.99609375, 0.3359375, 0.111328125, 0.07421875, 0.0390625, 0.99609375, 0.01171875, 0.0, 0.015625, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0703125, 0.0, 0.267578125, 0.0, 0.998046875, 0.0, 0.193359375, 0.013671875, 0.115234375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.091796875, 0.041015625, 0.001953125, 0.0, 0.99609375, 0.45703125, 0.380859375, 0.015625, 0.0, 0.0, 0.046875, 0.3828125, 0.056640625, 0.99609375, 0.4921875, 0.029296875, 0.0, 0.0, 0.99609375, 0.044921875, 0.0234375, 0.0, 0.99609375, 0.107421875, 0.998046875, 0.99609375, 0.99609375, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.177734375, 0.568359375, 0.99609375, 0.99609375, 0.78515625, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.072265625, 0.998046875, 0.205078125, 0.998046875]

 sparsity of   [0.0927734375, 0.0712890625, 0.0693359375, 0.9208984375, 0.0, 0.5771484375, 0.998046875, 0.9970703125, 0.0576171875, 0.0, 0.0, 0.9990234375, 0.576171875, 0.998046875, 0.998046875, 0.0380859375, 0.0, 0.0439453125, 0.0, 0.0, 0.0, 0.36328125, 0.56640625, 0.998046875, 0.998046875, 0.0390625, 0.123046875, 0.0517578125, 0.576171875, 0.029296875, 0.998046875, 0.0, 0.0, 0.9970703125, 0.111328125, 0.99609375, 0.5771484375, 0.2451171875, 0.8173828125, 0.1005859375, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.5517578125, 0.9970703125, 0.998046875, 0.2216796875, 0.0732421875, 0.998046875, 0.1982421875, 0.9990234375, 0.998046875, 0.5576171875, 0.576171875, 0.998046875, 0.0, 0.8759765625, 0.552734375, 0.0, 0.0322265625, 0.140625, 0.9990234375, 0.998046875, 0.0, 0.0, 0.205078125, 0.998046875, 0.0, 0.0, 0.998046875, 0.5771484375, 0.0, 0.0, 0.998046875, 0.0, 0.091796875, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.998046875, 0.173828125, 0.0, 0.03125, 0.998046875, 0.0, 0.0, 0.998046875, 0.02734375, 0.0, 0.9990234375, 0.998046875, 0.03515625, 0.998046875, 0.115234375, 0.169921875, 0.0, 0.0595703125, 0.607421875, 0.0, 0.998046875, 0.3935546875, 0.576171875, 0.0244140625, 0.0, 0.51953125, 0.18359375, 0.5654296875, 0.9990234375, 0.0, 0.998046875, 0.9990234375, 0.0, 0.1015625, 0.0, 0.0, 0.998046875, 0.0, 0.5771484375, 0.998046875, 0.0, 0.0, 0.1689453125, 0.083984375, 0.078125, 0.9970703125, 0.095703125, 0.576171875, 0.576171875, 0.9990234375, 0.5771484375, 0.1259765625, 0.052734375, 0.1337890625, 0.2900390625, 0.06640625, 0.0, 0.744140625, 0.998046875, 0.576171875, 0.0, 0.0, 0.9990234375, 0.998046875, 0.0, 0.365234375, 0.0, 0.9990234375, 0.0595703125, 0.9990234375, 0.5703125, 0.4306640625, 0.998046875, 0.0, 0.998046875, 0.0, 0.3935546875, 0.0, 0.548828125, 0.9990234375, 0.7177734375, 0.998046875, 0.998046875, 0.064453125, 0.08203125, 0.998046875, 0.9990234375, 0.4072265625, 0.9970703125, 0.0927734375, 0.0, 0.998046875, 0.55859375, 0.0, 0.0283203125, 0.052734375, 0.064453125, 0.0322265625, 0.0615234375, 0.099609375, 0.8984375, 0.998046875, 0.0, 0.90625, 0.576171875, 0.9970703125, 0.5556640625, 0.052734375, 0.2099609375, 0.0263671875, 0.9970703125, 0.626953125, 0.8232421875, 0.056640625, 0.029296875, 0.4912109375, 0.2080078125, 0.5751953125, 0.0, 0.998046875, 0.0625, 0.0, 0.033203125, 0.0556640625, 0.0, 0.7158203125, 0.998046875, 0.13671875, 0.99609375, 0.115234375, 0.5771484375, 0.744140625, 0.0, 0.0, 0.9990234375, 0.0, 0.0, 0.1015625, 0.080078125, 0.5771484375, 0.0, 0.4970703125, 0.03515625, 0.576171875, 0.5869140625, 0.5634765625, 0.998046875, 0.4970703125, 0.5478515625, 0.998046875, 0.1484375, 0.072265625, 0.9990234375, 0.9990234375, 0.9990234375, 0.15234375, 0.888671875, 0.111328125, 0.998046875, 0.03515625, 0.068359375, 0.576171875, 0.138671875, 0.576171875, 0.9990234375, 0.052734375, 0.177734375, 0.9990234375, 0.8642578125, 0.0, 0.2255859375, 0.0869140625]

 sparsity of   [0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0850694477558136, 0.0, 0.827256977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.0, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0694444477558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.3346354067325592, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.8285590410232544, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.3411458432674408, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3532986044883728, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.2999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.0, 0.2252604216337204, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.2903645932674408, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.3246527910232544, 0.9995659589767456, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136]

 sparsity of   [0.9921875, 0.81640625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.81640625, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.81640625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.81640625, 0.0, 0.98828125, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.8125, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.8125, 0.0, 0.9921875, 0.0, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.81640625, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.98828125, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.81640625, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.81640625, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.81640625, 0.99609375, 0.9921875, 0.81640625, 0.81640625, 0.0, 0.81640625, 0.81640625, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.81640625, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.0, 0.9921875, 0.0, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.99609375, 0.81640625, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.0, 0.0, 0.99609375, 0.0, 0.81640625, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.81640625, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.9921875, 0.81640625, 0.8125, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8125, 0.9921875, 0.0, 0.81640625, 0.81640625, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.81640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.81640625, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.8125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.8125, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.81640625, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.81640625, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.81640625, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.81640625, 0.9921875, 0.9921875, 0.9921875, 0.81640625, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.8125, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.81640625, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.81640625, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875]

 sparsity of   [0.0, 0.0, 0.998046875, 0.9990234375, 0.998046875, 0.0, 0.0, 0.9990234375, 0.0, 0.998046875, 0.9990234375, 0.1796875, 0.05859375, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.8740234375, 0.9970703125, 0.0, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.212890625, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.9970703125, 0.0, 0.0, 0.2119140625, 0.9970703125, 0.0, 0.9970703125, 0.0, 0.998046875, 0.0, 0.0, 0.8388671875, 0.9990234375, 0.9970703125, 0.998046875, 0.0, 0.0, 0.0, 0.119140625, 0.998046875, 0.5419921875, 0.302734375, 0.0, 0.998046875, 0.0703125, 0.0859375, 0.1005859375, 0.0, 0.083984375, 0.13671875, 0.0, 0.02734375, 0.998046875, 0.998046875, 0.87890625, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.998046875, 0.09765625, 0.998046875, 0.9970703125, 0.998046875, 0.0, 0.9990234375, 0.275390625, 0.0, 0.998046875, 0.9970703125, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.9990234375, 0.0, 0.0, 0.998046875, 0.0, 0.9990234375, 0.8330078125, 0.818359375, 0.0, 0.998046875, 0.0, 0.0, 0.9970703125, 0.998046875, 0.0, 0.3955078125, 0.2314453125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.5419921875, 0.998046875, 0.0, 0.998046875, 0.0, 0.12890625, 0.0634765625, 0.0, 0.3271484375, 0.9990234375, 0.998046875, 0.0, 0.9970703125, 0.0, 0.9990234375, 0.111328125, 0.998046875, 0.873046875, 0.998046875, 0.087890625, 0.998046875, 0.0, 0.998046875, 0.5419921875, 0.0966796875, 0.9990234375, 0.998046875, 0.0, 0.1142578125, 0.2138671875, 0.0771484375, 0.0, 0.0, 0.0693359375, 0.0908203125, 0.998046875, 0.9990234375, 0.0, 0.126953125, 0.998046875, 0.8291015625, 0.3515625, 0.0, 0.9970703125, 0.998046875, 0.9990234375, 0.1357421875, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.048828125, 0.5224609375, 0.998046875, 0.0, 0.01171875, 0.0, 0.998046875, 0.0830078125, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.998046875, 0.1513671875, 0.099609375, 0.341796875, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.9990234375, 0.0, 0.0, 0.0, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.09375, 0.1162109375, 0.9970703125, 0.0986328125, 0.0, 0.0576171875, 0.0, 0.998046875, 0.0, 0.5400390625, 0.0, 0.0615234375, 0.998046875, 0.9990234375, 0.525390625, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.41015625, 0.0, 0.998046875, 0.0, 0.0732421875, 0.1181640625, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.0, 0.998046875, 0.9970703125, 0.1123046875, 0.9990234375, 0.998046875, 0.9970703125, 0.0, 0.26171875, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.0, 0.998046875, 0.1435546875, 0.9990234375, 0.0, 0.5419921875, 0.998046875, 0.169921875, 0.0]

 sparsity of   [0.2586805522441864, 0.5659722089767456, 0.8697916865348816, 0.7339409589767456, 0.01171875, 0.0, 0.1388888955116272, 0.098524309694767, 0.999131977558136, 0.0451388880610466, 0.0, 0.0846354141831398, 0.02560763992369175, 0.03515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0546875, 0.0, 0.03125, 0.0, 0.02213541604578495, 0.0499131940305233, 0.0625, 0.0316840298473835, 0.0833333358168602, 0.0638020858168602, 0.0451388880610466, 0.02083333395421505, 0.0, 0.0, 0.9995659589767456, 0.0, 0.02387152798473835, 0.0, 0.9075520634651184, 0.7078993320465088, 0.05078125, 0.075086809694767, 0.0, 0.0, 0.2096354216337204, 0.0, 0.03515625, 0.9986979365348816, 0.1128472238779068, 0.0798611119389534, 0.181423619389534, 0.0837673619389534, 0.01909722201526165, 0.0, 0.27734375, 0.0329861119389534, 0.9995659589767456, 0.01996527798473835, 0.0442708320915699, 0.999131977558136, 0.01822916604578495, 0.1349826455116272, 0.02083333395421505, 0.999131977558136, 0.1740451455116272, 0.97265625, 0.0, 0.0, 0.1089409738779068, 0.1124131977558136, 0.0368923619389534, 0.014756944961845875, 0.93359375, 0.1788194477558136, 0.9279513955116272, 0.1553819477558136, 0.0, 0.6540798544883728, 0.0329861119389534, 0.0529513880610466, 0.90234375, 0.0, 0.005642361007630825, 0.0568576380610466, 0.1749131977558136, 0.02690972201526165, 0.02170138992369175, 0.0, 0.01779513992369175, 0.03125, 0.999131977558136, 0.0, 0.02213541604578495, 0.0881076380610466, 0.0164930559694767, 0.0659722238779068, 0.999131977558136, 0.0863715261220932, 0.0316840298473835, 0.68359375, 0.0394965298473835, 0.01605902798473835, 0.0, 0.0720486119389534, 0.0434027798473835, 0.121961809694767, 0.0316840298473835, 0.06640625, 0.9995659589767456, 0.0078125, 0.7183159589767456, 0.0, 0.9995659589767456, 0.0, 0.0212673619389534, 0.015625, 0.0533854179084301, 0.9010416865348816, 0.015625, 0.015625, 0.1801215261220932, 0.1041666641831398, 0.0, 0.2669270932674408, 0.0173611119389534, 0.0325520820915699, 0.1137152761220932, 0.0642361119389534, 0.014756944961845875, 0.0516493059694767, 0.0, 0.0282118059694767, 0.193142369389534, 0.9539930820465088, 0.02994791604578495, 0.03081597201526165, 0.04296875, 0.0, 0.0516493059694767, 0.1228298619389534, 0.7261284589767456, 0.1705729216337204, 0.014322916977107525, 0.0, 0.9097222089767456, 0.0447048619389534, 0.0, 0.173611119389534, 0.014322916977107525, 0.0, 0.612413227558136, 0.154079869389534, 0.126736119389534, 0.0620659738779068, 0.0, 0.0251736119389534, 0.5972222089767456, 0.0173611119389534, 0.9188368320465088, 0.01779513992369175, 0.88671875, 0.0724826380610466, 0.189236119389534, 0.0, 0.0494791679084301, 0.0, 0.02083333395421505, 0.0329861119389534, 0.02083333395421505, 0.01909722201526165, 0.5177951455116272, 0.0, 0.078125, 0.0638020858168602, 0.01953125, 0.010850694961845875, 0.009982638992369175, 0.2677951455116272, 0.0, 0.03125, 0.01519097201526165, 0.01519097201526165, 0.0412326380610466, 0.0, 0.0, 0.02690972201526165, 0.015625, 0.0455729179084301, 0.0, 0.0455729179084301, 0.01779513992369175, 0.0, 0.002170138992369175, 0.0, 0.02300347201526165, 0.0243055559694767, 0.9431423544883728, 0.01605902798473835, 0.0173611119389534, 0.2404513955116272, 0.0950520858168602, 0.181423619389534, 0.0815972238779068, 0.0451388880610466, 0.0, 0.0, 0.0368923619389534, 0.0, 0.0, 0.0, 0.16796875, 0.02777777798473835, 0.3263888955116272, 0.0859375, 0.0607638880610466, 0.0329861119389534, 0.0052083334885537624, 0.0776909738779068, 0.0950520858168602, 0.6284722089767456, 0.01519097201526165, 0.0377604179084301, 0.9986979365348816, 0.999131977558136, 0.5407986044883728, 0.01215277798473835, 0.0434027798473835, 0.1206597238779068, 0.999131977558136, 0.0355902798473835, 0.0, 0.02560763992369175, 0.0490451380610466, 0.0264756940305233, 0.0390625, 0.0316840298473835, 0.9995659589767456, 0.0, 0.0416666679084301, 0.0, 0.0, 0.0, 0.0, 0.1818576455116272, 0.0251736119389534, 0.1297743022441864, 0.0724826380610466, 0.9986979365348816, 0.071180559694767, 0.0173611119389534, 0.1171875, 0.007378472480922937, 0.24609375, 0.0186631940305233, 0.0, 0.0, 0.2178819477558136]

 sparsity of   [0.5390625, 0.7890625, 0.0, 0.15625, 0.1171875, 0.7890625, 0.109375, 0.0, 0.0, 0.98828125, 0.0, 0.69140625, 0.7265625, 0.6640625, 0.72265625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.7890625, 0.67578125, 0.0, 0.7890625, 0.58203125, 0.6640625, 0.09765625, 0.78515625, 0.99609375, 0.125, 0.0, 0.06640625, 0.08203125, 0.04296875, 0.9921875, 0.0, 0.7890625, 0.6640625, 0.9921875, 0.9921875, 0.74609375, 0.99609375, 0.75, 0.9921875, 0.7890625, 0.6953125, 0.609375, 0.78515625, 0.7890625, 0.69140625, 0.0, 0.9921875, 0.7578125, 0.07421875, 0.0, 0.0, 0.6171875, 0.984375, 0.7890625, 0.7890625, 0.99609375, 0.6796875, 0.04296875, 0.04296875, 0.0703125, 0.8515625, 0.7265625, 0.04296875, 0.7890625, 0.76171875, 0.69921875, 0.0, 0.01171875, 0.7890625, 0.1484375, 0.08984375, 0.0, 0.7890625, 0.765625, 0.09765625, 0.0, 0.0, 0.7421875, 0.71875, 0.01171875, 0.0, 0.0, 0.77734375, 0.0, 0.0546875, 0.69921875, 0.9921875, 0.9921875, 0.109375, 0.7890625, 0.0, 0.046875, 0.68359375, 0.71484375, 0.03515625, 0.98828125, 0.7890625, 0.9921875, 0.765625, 0.62109375, 0.7890625, 0.75390625, 0.0, 0.984375, 0.76953125, 0.09375, 0.7734375, 0.0, 0.75390625, 0.04296875, 0.71875, 0.98828125, 0.66796875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.4375, 0.0, 0.1484375, 0.77734375, 0.98828125, 0.69140625, 0.0, 0.0, 0.7890625, 0.0, 0.0, 0.0, 0.06640625, 0.13671875, 0.0, 0.078125, 0.9140625, 0.9921875, 0.9921875, 0.66015625, 0.6171875, 0.7578125, 0.99609375, 0.1015625, 0.0, 0.609375, 0.6953125, 0.9921875, 0.9921875, 0.7890625, 0.75390625, 0.078125, 0.71484375, 0.98828125, 0.0, 0.73046875, 0.0, 0.7890625, 0.7421875, 0.99609375, 0.9921875, 0.0, 0.63671875, 0.75, 0.74609375, 0.0, 0.09375, 0.9921875, 0.75390625, 0.7421875, 0.66015625, 0.04296875, 0.59375, 0.7890625, 0.83203125, 0.765625, 0.7890625, 0.11328125, 0.0, 0.74609375, 0.74609375, 0.09765625, 0.1171875, 0.02734375, 0.28125, 0.0, 0.8203125, 0.1171875, 0.0, 0.99609375, 0.7890625, 0.6640625, 0.98828125, 0.71484375, 0.7890625, 0.03515625, 0.7265625, 0.0, 0.734375, 0.27734375, 0.99609375, 0.703125, 0.9921875, 0.05078125, 0.7890625, 0.4609375, 0.03125, 0.75390625, 0.99609375, 0.2265625, 0.0, 0.07421875, 0.0, 0.0, 0.9921875, 0.65625, 0.98828125, 0.62890625, 0.9921875, 0.7578125, 0.9921875, 0.63671875, 0.0, 0.78515625, 0.7890625, 0.69140625, 0.75, 0.0859375, 0.6796875, 0.05859375, 0.98828125, 0.0, 0.7890625, 0.7890625, 0.68359375, 0.99609375, 0.765625, 0.0625, 0.75, 0.9921875, 0.390625, 0.04296875, 0.98828125, 0.6875, 0.69140625, 0.7578125, 0.99609375, 0.7421875, 0.7890625, 0.171875, 0.765625, 0.08203125, 0.99609375, 0.70703125, 0.7890625, 0.7890625, 0.9921875, 0.98828125, 0.9921875, 0.5859375, 0.99609375, 0.6875, 0.66796875, 0.75390625, 0.1484375, 0.7578125, 0.6875, 0.9921875, 0.75390625, 0.0234375, 0.7890625, 0.7890625, 0.9921875, 0.7890625, 0.65625, 0.75, 0.0, 0.63671875, 0.734375, 0.0, 0.0, 0.9296875, 0.765625, 0.9921875, 0.65625, 0.7890625, 0.0, 0.0, 0.15234375, 0.78515625, 0.09375, 0.75390625, 0.7890625, 0.7890625, 0.77734375, 0.671875, 0.0, 0.0, 0.75, 0.30078125, 0.7890625, 0.0390625, 0.0, 0.0546875, 0.0, 0.99609375, 0.6953125, 0.0, 0.1953125, 0.05859375, 0.0, 0.0, 0.76953125, 0.0, 0.0, 0.9921875, 0.0, 0.69140625, 0.7578125, 0.0, 0.6953125, 0.0, 0.0, 0.73828125, 0.76171875, 0.65234375, 0.0234375, 0.71484375, 0.9921875, 0.7578125, 0.0, 0.99609375, 0.9921875, 0.0, 0.7890625, 0.65625, 0.0, 0.0625, 0.734375, 0.76953125, 0.0, 0.734375, 0.73828125, 0.75, 0.47265625, 0.7890625, 0.9921875, 0.99609375, 0.078125, 0.74609375, 0.99609375, 0.7890625, 0.9921875, 0.99609375, 0.0, 0.6796875, 0.9921875, 0.859375, 0.0, 0.7578125, 0.92578125, 0.03125, 0.12890625, 0.9921875, 0.0, 0.7890625, 0.0, 0.0546875, 0.76953125, 0.99609375, 0.0, 0.75390625, 0.7109375, 0.78125, 0.05078125, 0.0, 0.08984375, 0.69921875, 0.99609375, 0.9609375, 0.0, 0.76953125, 0.74609375, 0.75390625, 0.0390625, 0.0, 0.73828125, 0.0, 0.734375, 0.0, 0.99609375, 0.76953125, 0.9921875, 0.03515625, 0.03515625, 0.7890625, 0.0, 0.6875, 0.4140625, 0.0, 0.0, 0.7578125, 0.99609375, 0.08984375, 0.0, 0.6796875, 0.9921875, 0.62109375, 0.4765625, 0.7421875, 0.76171875, 0.0, 0.703125, 0.9921875, 0.9921875, 0.13671875, 0.078125, 0.76171875, 0.0, 0.76171875, 0.78515625, 0.7109375, 0.7890625, 0.74609375, 0.9921875, 0.7890625, 0.0859375, 0.09765625, 0.78515625, 0.08203125, 0.9921875, 0.98828125, 0.0, 0.0, 0.98828125, 0.7890625, 0.140625, 0.99609375, 0.7890625, 0.6953125, 0.0, 0.0, 0.9921875, 0.9921875, 0.7890625, 0.0, 0.9921875, 0.9921875, 0.0, 0.7421875, 0.0, 0.76171875, 0.9921875, 0.73046875, 0.7578125, 0.71484375, 0.7890625, 0.640625, 0.99609375, 0.7890625, 0.734375, 0.0, 0.0859375, 0.10546875, 0.7890625, 0.9921875, 0.66015625, 0.76953125, 0.9921875, 0.99609375, 0.98828125, 0.0, 0.0, 0.84375, 0.0703125, 0.7890625, 0.76953125, 0.9921875, 0.03515625, 0.7890625, 0.1484375, 0.9921875, 0.7890625, 0.0, 0.07421875, 0.203125, 0.7578125, 0.078125, 0.01953125, 0.65234375, 0.28125, 0.0, 0.76953125, 0.99609375, 0.98828125, 0.6796875, 0.0, 0.7890625, 0.10546875, 0.703125, 0.12109375, 0.0, 0.75, 0.64453125, 0.734375, 0.78125, 0.01171875, 0.6875, 0.99609375, 0.0625, 0.0, 0.9921875, 0.0, 0.71484375, 0.8359375, 0.98828125, 0.7890625, 0.05859375, 0.99609375, 0.08984375, 0.109375, 0.7890625, 0.9921875, 0.78515625, 0.6640625, 0.7890625, 0.0, 0.0234375, 0.03125, 0.640625, 0.7890625, 0.9921875, 0.0, 0.0390625, 0.6875, 0.63671875, 0.0, 0.0, 0.7109375, 0.75, 0.7890625, 0.75390625, 0.9921875, 0.91015625, 0.9921875, 0.69140625, 0.78515625, 0.7109375, 0.06640625, 0.7890625, 0.0, 0.9921875, 0.9921875, 0.0, 0.73046875, 0.74609375, 0.72265625, 0.69921875, 0.9921875, 0.9921875, 0.66796875, 0.74609375, 0.76171875, 0.80859375, 0.9921875, 0.7890625, 0.078125, 0.75390625, 0.09765625, 0.0, 0.74609375, 0.98828125, 0.7734375, 0.74609375, 0.7109375, 0.14453125, 0.7890625, 0.7578125, 0.7890625, 0.0, 0.65234375, 0.0, 0.07421875, 0.7890625, 0.7890625, 0.0, 0.9921875, 0.01953125, 0.0, 0.0, 0.75390625, 0.703125, 0.109375, 0.765625, 0.0, 0.6875, 0.9921875, 0.7890625, 0.7578125, 0.77734375, 0.73828125, 0.765625, 0.9921875, 0.0, 0.08203125, 0.78515625, 0.1015625, 0.9921875, 0.74609375, 0.0, 0.9921875, 0.00390625, 0.9921875, 0.76171875, 0.7890625, 0.0546875, 0.71875, 0.9921875, 0.0, 0.8359375, 0.99609375, 0.078125, 0.74609375, 0.6796875, 0.7890625, 0.06640625, 0.9921875, 0.69921875, 0.05859375, 0.71484375, 0.015625, 0.9921875, 0.9921875, 0.68359375, 0.1171875, 0.9921875, 0.0, 0.7890625, 0.7890625, 0.0, 0.9921875, 0.01171875, 0.0, 0.7890625, 0.7890625, 0.24609375, 0.7890625, 0.0, 0.7734375, 0.9921875, 0.15234375, 0.74609375, 0.234375, 0.671875, 0.0234375, 0.7890625, 0.0, 0.0, 0.7265625, 0.0, 0.78125, 0.734375, 0.0, 0.7890625, 0.05859375, 0.7890625, 0.046875, 0.0, 0.75390625, 0.03515625, 0.765625, 0.9921875, 0.7890625, 0.0, 0.0, 0.76171875, 0.9921875, 0.9921875, 0.0703125, 0.38671875, 0.7890625, 0.99609375, 0.0, 0.7890625, 0.9921875, 0.7890625, 0.72265625, 0.12109375, 0.0, 0.23046875, 0.76953125, 0.0625, 0.75390625, 0.0, 0.99609375, 0.7890625, 0.7421875, 0.0, 0.7890625, 0.9921875, 0.7890625, 0.9921875, 0.82421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.7890625, 0.14453125, 0.10546875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.765625, 0.7890625, 0.9921875, 0.12890625, 0.0, 0.7578125, 0.98828125, 0.0, 0.0, 0.9921875, 0.578125, 0.55078125, 0.0, 0.65234375, 0.0, 0.7578125, 0.63671875, 0.63671875, 0.9921875, 0.07421875, 0.0546875, 0.99609375, 0.0, 0.6796875, 0.76171875, 0.0, 0.5859375, 0.11328125, 0.0, 0.0, 0.0703125, 0.6953125, 0.0, 0.0, 0.0625, 0.70703125, 0.9921875, 0.046875, 0.7890625, 0.7890625, 0.7890625, 0.72265625, 0.5546875, 0.7890625, 0.0078125, 0.7890625, 0.7890625, 0.08203125, 0.1015625, 0.99609375, 0.76171875, 0.0, 0.625, 0.0, 0.7109375, 0.6875, 0.0859375, 0.7890625, 0.9921875, 0.0, 0.7890625, 0.0, 0.0, 0.73828125, 0.0, 0.0, 0.76953125, 0.765625, 0.0625, 0.578125, 0.7890625, 0.7890625, 0.73828125, 0.16796875, 0.0, 0.76171875, 0.0, 0.99609375, 0.74609375, 0.0, 0.0, 0.7890625, 0.7890625, 0.9921875, 0.7265625, 0.0, 0.69140625, 0.6796875, 0.75, 0.7890625, 0.9921875, 0.7890625, 0.71484375, 0.74609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.7890625, 0.7890625, 0.0546875, 0.99609375, 0.03515625, 0.7265625, 0.28515625, 0.7890625, 0.9921875, 0.68359375, 0.69140625, 0.69140625, 0.7890625, 0.6953125, 0.7890625, 0.328125, 0.7890625, 0.9921875, 0.9921875, 0.703125, 0.0, 0.9921875, 0.0, 0.7890625, 0.74609375, 0.0, 0.08984375, 0.7890625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.625, 0.71484375, 0.7890625, 0.66796875, 0.0, 0.0, 0.9921875, 0.9921875, 0.7890625, 0.23828125, 0.08203125, 0.0, 0.0, 0.0, 0.0703125, 0.28125, 0.98828125, 0.98828125, 0.7890625, 0.7890625, 0.0, 0.0, 0.1953125, 0.0, 0.9921875, 0.69921875, 0.98828125, 0.9921875, 0.7109375, 0.7890625, 0.77734375, 0.9921875, 0.265625, 0.0, 0.77734375, 0.71875, 0.7109375, 0.0, 0.7890625, 0.75390625, 0.15234375, 0.9921875, 0.9921875, 0.75390625, 0.75, 0.0, 0.7421875, 0.11328125, 0.99609375, 0.98828125, 0.1171875, 0.046875, 0.09765625, 0.0, 0.7890625, 0.99609375, 0.75390625, 0.9921875, 0.9921875, 0.7890625, 0.70703125, 0.04296875, 0.68359375, 0.28515625, 0.0546875, 0.04296875, 0.31640625, 0.69140625, 0.7109375, 0.7890625, 0.0, 0.07421875, 0.76953125, 0.8046875, 0.6875, 0.99609375, 0.9921875, 0.7578125, 0.71484375, 0.15234375, 0.73828125, 0.76171875, 0.0, 0.9921875, 0.734375, 0.05859375, 0.7265625, 0.7890625, 0.9921875, 0.9921875, 0.9921875, 0.72265625, 0.0, 0.9921875, 0.76171875, 0.73828125, 0.76171875, 0.68359375, 0.69921875, 0.046875, 0.99609375, 0.765625, 0.7890625, 0.0, 0.0, 0.29296875, 0.69921875, 0.6953125, 0.9921875, 0.0234375, 0.7890625, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0703125, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.015625, 0.15234375, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.66015625, 0.0, 0.7890625, 0.0234375, 0.0, 0.7890625, 0.6484375, 0.84765625, 0.125, 0.6953125, 0.9921875, 0.98828125, 0.0, 0.0, 0.1015625, 0.9921875, 0.7421875, 0.0, 0.7421875, 0.03125, 0.9921875, 0.09765625, 0.7578125, 0.76171875, 0.7890625, 0.0, 0.7890625, 0.0, 0.0703125, 0.05078125, 0.72265625, 0.9921875, 0.7890625, 0.0, 0.0, 0.8984375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0859375, 0.62890625]

 sparsity of   [0.0244140625, 0.6943359375, 0.021484375, 0.0, 0.01953125, 0.072265625, 0.0078125, 0.0087890625, 0.2001953125, 0.0263671875, 0.7265625, 0.009765625, 0.015625, 0.1103515625, 0.0458984375, 0.0048828125, 0.033203125, 0.9970703125, 0.8193359375, 0.0166015625, 0.0341796875, 0.998046875, 0.0029296875, 0.0185546875, 0.021484375, 0.0673828125, 0.251953125, 0.0673828125, 0.009765625, 0.6787109375, 0.333984375, 0.0634765625, 0.0302734375, 0.998046875, 0.36328125, 0.0703125, 0.9970703125, 0.0107421875, 0.029296875, 0.1552734375, 0.9970703125, 0.0625, 0.0341796875, 0.01953125, 0.017578125, 0.015625, 0.0166015625, 0.9990234375, 0.0634765625, 0.9990234375, 0.8642578125, 0.0107421875, 0.017578125, 0.0771484375, 0.7744140625, 0.037109375, 0.1767578125, 0.052734375, 0.0107421875, 0.998046875, 0.169921875, 0.791015625, 0.0029296875, 0.1572265625, 0.013671875, 0.2021484375, 0.0068359375, 0.205078125, 0.021484375, 0.0205078125, 0.134765625, 0.0302734375, 0.9990234375, 0.1552734375, 0.0234375, 0.0263671875, 0.0302734375, 0.8251953125, 0.0341796875, 0.05078125, 0.119140625, 0.146484375, 0.01953125, 0.2724609375, 0.0654296875, 0.947265625, 0.02734375, 0.0185546875, 0.0341796875, 0.046875, 0.0078125, 0.0302734375, 0.025390625, 0.2001953125, 0.029296875, 0.03515625, 0.015625, 0.0087890625, 0.009765625, 0.197265625, 0.3203125, 0.015625, 0.1845703125, 0.8173828125, 0.0244140625, 0.837890625, 0.0185546875, 0.1259765625, 0.0078125, 0.033203125, 0.029296875, 0.11328125, 0.8564453125, 0.04296875, 0.0859375, 0.177734375, 0.9384765625, 0.22265625, 0.025390625, 0.998046875, 0.0576171875, 0.1181640625, 0.04296875, 0.0146484375, 0.009765625, 0.1796875, 0.015625, 0.0009765625, 0.03125, 0.02734375, 0.0673828125, 0.048828125, 0.01171875, 0.0615234375, 0.9326171875, 0.068359375, 0.0146484375, 0.013671875, 0.0166015625, 0.0712890625, 0.1640625, 0.9990234375, 0.9580078125, 0.9970703125, 0.0244140625, 0.0146484375, 0.0234375, 0.5322265625, 0.9326171875, 0.0078125, 0.01171875, 0.998046875, 0.037109375, 0.0126953125, 0.0673828125, 0.7978515625, 0.1845703125, 0.162109375, 0.7001953125, 0.7666015625, 0.0361328125, 0.0185546875, 0.099609375, 0.03515625, 0.8896484375, 0.0771484375, 0.025390625, 0.001953125, 0.0224609375, 0.0791015625, 0.0986328125, 0.03515625, 0.025390625, 0.12109375, 0.080078125, 0.962890625, 0.0205078125, 0.1328125, 0.0986328125, 0.0810546875, 0.029296875, 0.0166015625, 0.78515625, 0.947265625, 0.998046875, 0.05859375, 0.32421875, 0.0771484375, 0.1259765625, 0.0751953125, 0.80859375, 0.9482421875, 0.0146484375, 0.0673828125, 0.6357421875, 0.125, 0.0107421875, 0.0107421875, 0.177734375, 0.03515625, 0.0283203125, 0.142578125, 0.1591796875, 0.0263671875, 0.205078125, 0.8505859375, 0.0263671875, 0.0205078125, 0.14453125, 0.025390625, 0.99609375, 0.1865234375, 0.0205078125, 0.013671875, 0.052734375, 0.0439453125, 0.162109375, 0.02734375, 0.0205078125, 0.11328125, 0.01953125, 0.111328125, 0.8857421875, 0.013671875, 0.0615234375, 0.0263671875, 0.1171875, 0.01171875, 0.0146484375, 0.1103515625, 0.0341796875, 0.0263671875, 0.001953125, 0.025390625, 0.03125, 0.029296875, 0.2900390625, 0.23828125, 0.021484375, 0.0234375, 0.1875, 0.029296875, 0.833984375, 0.33203125, 0.0185546875, 0.0263671875, 0.06640625, 0.0478515625, 0.0595703125, 0.134765625, 0.00390625, 0.0048828125, 0.9697265625, 0.05078125, 0.044921875, 0.07421875]

 sparsity of   [0.140625, 0.0442708320915699, 0.0173611119389534, 0.0125868059694767, 0.8980034589767456, 0.3541666567325592, 0.0282118059694767, 0.0403645820915699, 0.0577256940305233, 0.009114583022892475, 0.03125, 0.0078125, 0.59765625, 0.823350727558136, 0.0394965298473835, 0.0, 0.013888888992369175, 0.0186631940305233, 0.071180559694767, 0.0164930559694767, 0.0334201380610466, 0.013888888992369175, 0.7361111044883728, 0.01822916604578495, 0.01692708395421505, 0.0416666679084301, 0.0303819440305233, 0.0, 0.02951388992369175, 0.3723958432674408, 0.0794270858168602, 0.0303819440305233, 0.0677083358168602, 0.0342881940305233, 0.0581597238779068, 0.0525173619389534, 0.1566840261220932, 0.0486111119389534, 0.0251736119389534, 0.0264756940305233, 0.046875, 0.1176215261220932, 0.063368059694767, 0.02213541604578495, 0.0516493059694767, 0.007378472480922937, 0.0746527761220932, 0.0373263880610466, 0.2721354067325592, 0.0759548619389534, 0.098524309694767, 0.0034722222480922937, 0.01171875, 0.0611979179084301, 0.9965277910232544, 0.0824652761220932, 0.0620659738779068, 0.0451388880610466, 0.0737847238779068, 0.0611979179084301, 0.0980902761220932, 0.5013020634651184, 0.0798611119389534, 0.0338541679084301, 0.1215277761220932, 0.3298611044883728, 0.0069444444961845875, 0.0407986119389534, 0.0520833320915699, 0.02777777798473835, 0.00434027798473835, 0.5681423544883728, 0.8359375, 0.010850694961845875, 0.0243055559694767, 0.0802951380610466, 0.0598958320915699, 0.1380208283662796, 0.0690104141831398, 0.02300347201526165, 0.01822916604578495, 0.1323784738779068, 0.01909722201526165, 0.1085069477558136, 0.010850694961845875, 0.0729166641831398, 0.0008680555620230734, 0.05078125, 0.0460069440305233, 0.0668402761220932, 0.121961809694767, 0.2408854216337204, 0.3385416567325592, 0.0360243059694767, 0.01909722201526165, 0.02951388992369175, 0.1610243022441864, 0.1271701455116272, 0.9995659589767456, 0.02994791604578495, 0.1154513880610466, 0.01953125, 0.0373263880610466, 0.0572916679084301, 0.10546875, 0.02734375, 0.0564236119389534, 0.1458333283662796, 0.2074652761220932, 0.9743923544883728, 0.0473090298473835, 0.0173611119389534, 0.0164930559694767, 0.1493055522441864, 0.0473090298473835, 0.0590277798473835, 0.0, 0.0434027798473835, 0.01953125, 0.009982638992369175, 0.02300347201526165, 0.0, 0.02387152798473835, 0.00390625, 0.0620659738779068, 0.009114583022892475, 0.114149309694767, 0.010850694961845875, 0.0034722222480922937, 0.0329861119389534, 0.0425347238779068, 0.02864583395421505, 0.007378472480922937, 0.0386284738779068, 0.0, 0.0915798619389534, 0.02300347201526165, 0.013020833022892475, 0.169704869389534, 0.0499131940305233, 0.2734375, 0.9995659589767456, 0.0451388880610466, 0.03125, 0.02951388992369175, 0.0364583320915699, 0.0004340277810115367, 0.0794270858168602, 0.5390625, 0.0399305559694767, 0.013888888992369175, 0.01996527798473835, 0.0729166641831398, 0.0746527761220932, 0.01996527798473835, 0.1731770783662796, 0.0855034738779068, 0.0086805559694767, 0.02777777798473835, 0.014756944961845875, 0.02170138992369175, 0.1532118022441864, 0.0455729179084301, 0.010416666977107525, 0.03125, 0.9995659589767456, 0.0768229141831398, 0.1236979141831398, 0.02951388992369175, 0.9995659589767456, 0.1124131977558136, 0.0798611119389534, 0.5581597089767456, 0.02864583395421505, 0.1006944477558136, 0.01692708395421505, 0.0368923619389534, 0.02734375, 0.0902777761220932, 0.0360243059694767, 0.0416666679084301, 0.8151041865348816, 0.1636284738779068, 0.0833333358168602, 0.0460069440305233, 0.01909722201526165, 0.0442708320915699, 0.1184895858168602, 0.0416666679084301, 0.1497395783662796, 0.0325520820915699, 0.005642361007630825, 0.0303819440305233, 0.4583333432674408, 0.03081597201526165, 0.0321180559694767, 0.009114583022892475, 0.4700520932674408, 0.0, 0.015625, 0.04296875, 0.0863715261220932, 0.0399305559694767, 0.2782118022441864, 0.009548611007630825, 0.0763888880610466, 0.0008680555620230734, 0.2973090410232544, 0.1150173619389534, 0.01822916604578495, 0.0451388880610466, 0.0889756977558136, 0.0542534738779068, 0.0785590261220932, 0.0203993059694767, 0.0603298619389534, 0.1263020783662796, 0.013020833022892475, 0.0004340277810115367, 0.01128472201526165, 0.02994791604578495, 0.0234375, 0.0325520820915699, 0.0625, 0.0, 0.0768229141831398, 0.0186631940305233, 0.0381944440305233, 0.0594618059694767, 0.0125868059694767, 0.0920138880610466, 0.0321180559694767, 0.00824652798473835, 0.0846354141831398, 0.02734375, 0.0703125, 0.01692708395421505, 0.1714409738779068, 0.0303819440305233, 0.078125, 0.0303819440305233, 0.0473090298473835, 0.1019965261220932, 0.110243059694767, 0.11328125, 0.8667534589767456, 0.01171875, 0.0416666679084301, 0.0403645820915699, 0.00824652798473835, 0.3372395932674408, 0.0186631940305233, 0.013020833022892475, 0.0555555559694767, 0.3172743022441864, 0.0837673619389534]

 sparsity of   [0.03515625, 0.0390625, 0.0, 0.26171875, 0.0625, 0.01953125, 0.9921875, 0.5859375, 0.0546875, 0.02734375, 0.9921875, 0.02734375, 0.0, 0.01171875, 0.79296875, 0.0078125, 0.04296875, 0.98828125, 0.04296875, 0.9140625, 0.0546875, 0.0078125, 0.0546875, 0.3828125, 0.9921875, 0.00390625, 0.03125, 0.01953125, 0.0234375, 0.0625, 0.1015625, 0.015625, 0.0, 0.9921875, 0.05078125, 0.00390625, 0.05078125, 0.09765625, 0.05078125, 0.1484375, 0.90625, 0.296875, 0.0078125, 0.2421875, 0.01953125, 0.9921875, 0.01953125, 0.015625, 0.90234375, 0.97265625, 0.9921875, 0.03125, 0.01171875, 0.9921875, 0.015625, 0.01171875, 0.9921875, 0.9921875, 0.05078125, 0.9921875, 0.01171875, 0.3046875, 0.07421875, 0.02734375, 0.03125, 0.01171875, 0.04296875, 0.9921875, 0.93359375, 0.08984375, 0.78515625, 0.22265625, 0.0234375, 0.00390625, 0.00390625, 0.02734375, 0.03125, 0.96875, 0.98828125, 0.91015625, 0.85546875, 0.87890625, 0.7734375, 0.0078125, 0.0703125, 0.015625, 0.01171875, 0.015625, 0.08203125, 0.03125, 0.98828125, 0.17578125, 0.83984375, 0.09375, 0.0234375, 0.01171875, 0.06640625, 0.015625, 0.08984375, 0.03515625, 0.109375, 0.0234375, 0.00390625, 0.98828125, 0.140625, 0.015625, 0.00390625, 0.015625, 0.0078125, 0.85546875, 0.2109375, 0.0546875, 0.93359375, 0.9921875, 0.015625, 0.01953125, 0.96875, 0.05078125, 0.98828125, 0.0, 0.0546875, 0.0, 0.02734375, 0.015625, 0.24609375, 0.02734375, 0.0078125, 0.01171875, 0.0625, 0.890625, 0.140625, 0.9921875, 0.12890625, 0.0078125, 0.03515625, 0.03125, 0.12890625, 0.98828125, 0.27734375, 0.9921875, 0.99609375, 0.09765625, 0.0234375, 0.0390625, 0.015625, 0.98828125, 0.9921875, 0.0078125, 0.8671875, 0.1171875, 0.109375, 0.015625, 0.99609375, 0.01171875, 0.0078125, 0.9921875, 0.03125, 0.08203125, 0.9921875, 0.0078125, 0.93359375, 0.0234375, 0.9921875, 0.03515625, 0.05078125, 0.01953125, 0.09375, 0.09765625, 0.10546875, 0.0390625, 0.03125, 0.0703125, 0.015625, 0.05859375, 0.1953125, 0.015625, 0.67578125, 0.0546875, 0.98828125, 0.0859375, 0.9140625, 0.27734375, 0.28125, 0.03515625, 0.23046875, 0.01171875, 0.9921875, 0.01953125, 0.07421875, 0.11328125, 0.51953125, 0.015625, 0.9921875, 0.0234375, 0.2265625, 0.06640625, 0.0078125, 0.00390625, 0.91796875, 0.08203125, 0.0703125, 0.9921875, 0.03125, 0.0, 0.0078125, 0.05859375, 0.05078125, 0.0, 0.0234375, 0.9921875, 0.0546875, 0.9921875, 0.0, 0.015625, 0.9921875, 0.01953125, 0.20703125, 0.00390625, 0.84765625, 0.92578125, 0.015625, 0.9921875, 0.0078125, 0.27734375, 0.08984375, 0.33984375, 0.046875, 0.015625, 0.015625, 0.01171875, 0.9609375, 0.01953125, 0.05859375, 0.05859375, 0.23046875, 0.03515625, 0.9921875, 0.0, 0.03515625, 0.01953125, 0.96875, 0.01953125, 0.03125, 0.03125, 0.015625, 0.0234375, 0.828125, 0.06640625, 0.01953125, 0.03515625, 0.0234375, 0.0234375, 0.2265625, 0.04296875, 0.0859375, 0.9765625, 0.9765625, 0.5703125, 0.25, 0.2734375, 0.16015625, 0.01953125, 0.0234375, 0.9921875, 0.01953125, 0.03125, 0.3359375, 0.04296875, 0.04296875, 0.01171875, 0.0078125, 0.015625, 0.07421875, 0.00390625, 0.98828125, 0.02734375, 0.01171875, 0.171875, 0.9921875, 0.00390625, 0.9921875, 0.015625, 0.01171875, 0.01953125, 0.015625, 0.0703125, 0.0703125, 0.03125, 0.0, 0.01953125, 0.0546875, 0.98828125, 0.02734375, 0.953125, 0.9921875, 0.0078125, 0.01953125, 0.00390625, 0.140625, 0.03125, 0.9921875, 0.06640625, 0.2265625, 0.9921875, 0.859375, 0.98046875, 0.015625, 0.97265625, 0.76953125, 0.98828125, 0.02734375, 0.52734375, 0.9921875, 0.9296875, 0.19140625, 0.05078125, 0.0078125, 0.11328125, 0.98828125, 0.20703125, 0.36328125, 0.05078125, 0.03125, 0.3046875, 0.44921875, 0.91015625, 0.83203125, 0.00390625, 0.0078125, 0.25390625, 0.01953125, 0.9921875, 0.00390625, 0.01171875, 0.328125, 0.14453125, 0.33203125, 0.04296875, 0.01171875, 0.703125, 0.8046875, 0.02734375, 0.01171875, 0.0703125, 0.0078125, 0.9921875, 0.85546875, 0.046875, 0.86328125, 0.98828125, 0.25, 0.01953125, 0.96875, 0.15234375, 0.0546875, 0.03515625, 0.0390625, 0.02734375, 0.16796875, 0.03125, 0.0078125, 0.22265625, 0.35546875, 0.140625, 0.02734375, 0.03515625, 0.08984375, 0.0234375, 0.91015625, 0.390625, 0.93359375, 0.84375, 0.9921875, 0.0703125, 0.2109375, 0.96875, 0.0234375, 0.97265625, 0.33984375, 0.328125, 0.01171875, 0.0078125, 0.8828125, 0.9921875, 0.01171875, 0.03125, 0.01171875, 0.9375, 0.88671875, 0.05078125, 0.17578125, 0.7734375, 0.0078125, 0.0078125, 0.05078125, 0.01953125, 0.01171875, 0.1484375, 0.03125, 0.2734375, 0.20703125, 0.9921875, 0.00390625, 0.03515625, 0.4296875, 0.26953125, 0.16015625, 0.98828125, 0.34375, 0.0234375, 0.05078125, 0.01953125, 0.9921875, 0.03125, 0.06640625, 0.9921875, 0.01171875, 0.01953125, 0.01953125, 0.57421875, 0.015625, 0.9765625, 0.03515625, 0.0234375, 0.01171875, 0.0546875, 0.96875, 0.03125, 0.01171875, 0.98828125, 0.015625, 0.05078125, 0.01953125, 0.0234375, 0.91015625, 0.9921875, 0.015625, 0.97265625, 0.9921875, 0.046875, 0.0390625, 0.01171875, 0.2109375, 0.98828125, 0.02734375, 0.08203125, 0.11328125, 0.01171875, 0.03125, 0.88671875, 0.05859375, 0.0625, 0.87890625, 0.015625, 0.43359375, 0.16015625, 0.94140625, 0.234375, 0.71875, 0.9921875, 0.98828125, 0.6015625, 0.9921875, 0.02734375, 0.01953125, 0.19140625, 0.421875, 0.02734375, 0.01171875, 0.01953125, 0.2734375, 0.98828125, 0.91015625, 0.0234375, 0.125, 0.1015625, 0.0859375, 0.9921875, 0.06640625, 0.015625, 0.0234375, 0.04296875, 0.265625, 0.5078125, 0.16015625, 0.59375, 0.2109375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.05859375, 0.96875, 0.0078125, 0.90234375, 0.16015625, 0.00390625, 0.86328125, 0.2734375, 0.00390625, 0.140625, 0.04296875, 0.01171875, 0.015625, 0.9921875, 0.05078125, 0.07421875, 0.0859375, 0.89453125, 0.00390625, 0.04296875, 0.046875, 0.49609375, 0.0390625, 0.875, 0.0390625, 0.89453125, 0.02734375, 0.48046875, 0.28515625, 0.0234375, 0.0078125, 0.02734375, 0.0078125, 0.046875, 0.98828125, 0.9921875, 0.96875, 0.25, 0.35546875, 0.19921875, 0.03515625, 0.5625, 0.98828125, 0.03515625, 0.0703125, 0.19921875, 0.1328125, 0.01171875, 0.96875, 0.0234375, 0.0, 0.27734375, 0.01171875, 0.015625, 0.10546875, 0.0390625, 0.0703125, 0.875, 0.00390625, 0.01953125, 0.1640625, 0.5078125, 0.03125, 0.01953125, 0.38671875, 0.57421875, 0.03515625, 0.0390625, 0.14453125, 0.0234375, 0.828125, 0.11328125, 0.9140625, 0.8046875, 0.01953125, 0.05859375, 0.1015625, 0.2265625, 0.859375, 0.48828125, 0.05078125, 0.0390625, 0.83203125, 0.390625, 0.015625, 0.0078125, 0.4453125, 0.89453125, 0.21875, 0.10546875, 0.03515625, 0.375, 0.28515625, 0.0703125, 0.9921875, 0.01171875, 0.91015625, 0.0078125, 0.12109375, 0.08203125, 0.02734375, 0.01171875, 0.02734375, 0.09375, 0.8984375, 0.13671875, 0.01953125, 0.0390625, 0.01953125, 0.01953125, 0.10546875, 0.89453125, 0.015625, 0.77734375, 0.015625, 0.015625, 0.015625, 0.0234375, 0.0390625, 0.9921875, 0.03515625, 0.03125, 0.23828125, 0.39453125, 0.9921875, 0.9921875, 0.9921875, 0.515625, 0.01171875, 0.7265625, 0.19921875, 0.03125, 0.0234375, 0.20703125, 0.89453125, 0.7578125, 0.015625, 0.015625, 0.9375, 0.2890625, 0.125, 0.015625, 0.07421875, 0.1171875, 0.25390625, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.01171875, 0.046875, 0.140625, 0.8203125, 0.05859375, 0.9609375, 0.91015625, 0.98046875, 0.4765625, 0.2578125, 0.9921875, 0.0234375, 0.98828125, 0.94921875, 0.98828125, 0.03125, 0.96875, 0.9921875, 0.0078125, 0.00390625, 0.03125, 0.96875, 0.03125, 0.09375, 0.9921875, 0.015625, 0.046875, 0.890625, 0.015625, 0.0546875, 0.02734375, 0.94140625, 0.00390625, 0.046875, 0.21484375, 0.96875, 0.12890625, 0.00390625, 0.890625, 0.03125, 0.00390625, 0.11328125, 0.9921875, 0.0859375, 0.08984375, 0.01953125, 0.1015625, 0.015625, 0.24609375, 0.0, 0.0078125, 0.0390625, 0.99609375, 0.8515625, 0.02734375, 0.9609375, 0.0078125, 0.91796875, 0.0078125, 0.0078125, 0.03125, 0.01171875, 0.9921875, 0.05859375, 0.9921875, 0.01953125, 0.0234375, 0.01953125, 0.0234375, 0.01171875, 0.03515625, 0.03515625, 0.03125, 0.9921875, 0.38671875, 0.02734375, 0.5078125, 0.01171875, 0.01953125, 0.03125, 0.1015625, 0.0234375, 0.9453125, 0.04296875, 0.9921875, 0.05859375, 0.0625, 0.875, 0.23828125, 0.01953125, 0.0234375, 0.96875, 0.0234375, 0.01953125, 0.9921875, 0.98828125, 0.01953125, 0.03515625, 0.890625, 0.9921875, 0.02734375, 0.00390625, 0.06640625, 0.046875, 0.2734375, 0.03515625, 0.9921875, 0.02734375, 0.5234375, 0.98828125, 0.05078125, 0.2734375, 0.9375, 0.08984375, 0.0078125, 0.01953125, 0.0234375, 0.9921875, 0.15234375, 0.05078125, 0.01171875, 0.06640625, 0.30859375, 0.00390625, 0.921875, 0.0234375, 0.828125, 0.02734375, 0.38671875, 0.01953125, 0.015625, 0.015625, 0.13671875, 0.7265625, 0.01171875, 0.98828125, 0.046875, 0.27734375, 0.25, 0.06640625, 0.0078125, 0.16796875, 0.23046875, 0.01171875, 0.98828125, 0.05859375, 0.01171875, 0.03125, 0.0390625, 0.16796875, 0.046875, 0.83203125, 0.90234375, 0.015625, 0.3046875, 0.9140625, 0.98828125, 0.87890625, 0.9921875, 0.23828125, 0.00390625, 0.859375, 0.05078125, 0.51953125, 0.0390625, 0.96875, 0.25, 0.39453125, 0.01953125, 0.23828125, 0.03125, 0.19921875, 0.9921875, 0.92578125, 0.0234375, 0.2421875, 0.79296875, 0.9921875, 0.2578125, 0.9921875, 0.04296875, 0.00390625, 0.9921875, 0.98828125, 0.90234375, 0.05859375, 0.03515625, 0.9921875, 0.03125, 0.05859375, 0.92578125, 0.99609375, 0.1328125, 0.03515625, 0.0, 0.046875, 0.05078125, 0.3046875, 0.23828125, 0.015625, 0.41015625, 0.0078125, 0.01171875, 0.4765625, 0.03125, 0.03125, 0.0234375, 0.16796875, 0.88671875, 0.02734375, 0.859375, 0.015625, 0.93359375, 0.23828125, 0.01171875, 0.2734375, 0.9921875, 0.00390625, 0.65625, 0.02734375, 0.1015625, 0.01953125, 0.00390625, 0.05078125, 0.9921875, 0.125, 0.15625, 0.11328125, 0.03515625, 0.0078125, 0.9296875, 0.03515625, 0.0234375, 0.8984375, 0.09375, 0.07421875, 0.0625, 0.1171875, 0.0078125, 0.06640625, 0.078125, 0.9921875, 0.01171875, 0.01953125, 0.9921875, 0.7265625, 0.0, 0.10546875, 0.28515625, 0.00390625, 0.03515625, 0.9921875, 0.96875, 0.09765625, 0.9921875, 0.0859375, 0.01171875, 0.01953125, 0.15234375, 0.0390625, 0.9921875, 0.8671875, 0.1953125, 0.27734375, 0.2421875, 0.05859375, 0.99609375, 0.9921875, 0.3203125, 0.453125, 0.9921875, 0.03125, 0.01953125, 0.953125, 0.02734375, 0.07421875, 0.046875, 0.9921875, 0.20703125, 0.9921875, 0.046875, 0.0234375, 0.04296875, 0.01953125, 0.984375, 0.07421875, 0.03515625, 0.015625, 0.01171875, 0.02734375, 0.0078125, 0.109375, 0.00390625, 0.01171875, 0.03515625, 0.24609375, 0.37109375, 0.22265625, 0.09375, 0.96875, 0.9921875, 0.77734375, 0.91015625, 0.00390625, 0.01953125, 0.9296875, 0.85546875, 0.47265625, 0.0, 0.34375, 0.9921875, 0.94921875, 0.05859375, 0.9921875, 0.98828125, 0.0546875, 0.0234375, 0.08203125, 0.9921875, 0.02734375, 0.02734375, 0.01953125, 0.9921875, 0.03515625, 0.00390625, 0.01953125, 0.0078125, 0.12109375, 0.20703125, 0.05859375, 0.015625, 0.98828125, 0.9921875, 0.03125, 0.9921875, 0.04296875, 0.0390625, 0.046875, 0.00390625, 0.9921875, 0.0078125, 0.09765625, 0.97265625, 0.1171875, 0.2578125, 0.0234375, 0.9609375, 0.06640625, 0.0078125, 0.9921875, 0.0, 0.01953125, 0.58984375, 0.0390625, 0.015625, 0.9921875, 0.71484375, 0.9375, 0.046875, 0.03515625, 0.015625, 0.9921875, 0.01953125, 0.01953125, 0.19140625, 0.0703125, 0.015625, 0.01171875, 0.01953125, 0.03125, 0.0078125, 0.02734375, 0.34765625, 0.078125, 0.22265625, 0.03125, 0.046875, 0.07421875, 0.01171875, 0.22265625, 0.15234375]

 sparsity of   [0.1611328125, 0.0, 0.9970703125, 0.0087890625, 0.998046875, 0.01953125, 0.0068359375, 0.01953125, 0.0537109375, 0.0166015625, 0.03125, 0.6728515625, 0.0283203125, 0.998046875, 0.029296875, 0.0537109375, 0.0439453125, 0.322265625, 0.0, 0.02734375, 0.044921875, 0.02734375, 0.0341796875, 0.7734375, 0.01171875, 0.0654296875, 0.05078125, 0.05859375, 0.0439453125, 0.04296875, 0.1826171875, 0.0302734375, 0.015625, 0.0185546875, 0.0498046875, 0.0166015625, 0.044921875, 0.91796875, 0.021484375, 0.9521484375, 0.0185546875, 0.998046875, 0.107421875, 0.076171875, 0.2666015625, 0.9970703125, 0.01953125, 0.033203125, 0.9970703125, 0.0166015625, 0.9970703125, 0.037109375, 0.9970703125, 0.0595703125, 0.0, 0.033203125, 0.01953125, 0.94140625, 0.998046875, 0.9501953125, 0.9970703125, 0.9970703125, 0.962890625, 0.0244140625, 0.712890625, 0.009765625, 0.916015625, 0.0478515625, 0.998046875, 0.08203125, 0.896484375, 0.0361328125, 0.26171875, 0.0205078125, 0.037109375, 0.0185546875, 0.9208984375, 0.150390625, 0.150390625, 0.01953125, 0.0087890625, 0.9619140625, 0.0009765625, 0.029296875, 0.0654296875, 0.0166015625, 0.0380859375, 0.099609375, 0.998046875, 0.150390625, 0.0048828125, 0.0712890625, 0.0771484375, 0.015625, 0.0390625, 0.0107421875, 0.8720703125, 0.0166015625, 0.025390625, 0.681640625, 0.0517578125, 0.0185546875, 0.80078125, 0.025390625, 0.9990234375, 0.0048828125, 0.01953125, 0.00390625, 0.998046875, 0.0244140625, 0.048828125, 0.115234375, 0.802734375, 0.9990234375, 0.05859375, 0.017578125, 0.9970703125, 0.0732421875, 0.0107421875, 0.9375, 0.224609375, 0.111328125, 0.9970703125, 0.0361328125, 0.8125, 0.099609375, 0.1064453125, 0.05078125, 0.0244140625, 0.0185546875, 0.0380859375, 0.013671875, 0.82421875, 0.05859375, 0.998046875, 0.294921875, 0.0341796875, 0.0263671875, 0.0205078125, 0.998046875, 0.068359375, 0.0927734375, 0.0185546875, 0.890625, 0.025390625, 0.998046875, 0.9326171875, 0.052734375, 0.998046875, 0.0068359375, 0.9990234375, 0.0634765625, 0.0390625, 0.873046875, 0.1796875, 0.0087890625, 0.0224609375, 0.0126953125, 0.9970703125, 0.1103515625, 0.8935546875, 0.1962890625, 0.01171875, 0.1396484375, 0.03125, 0.9443359375, 0.0, 0.1328125, 0.0263671875, 0.17578125, 0.01953125, 0.9970703125, 0.998046875, 0.0068359375, 0.109375, 0.0, 0.0439453125, 0.0302734375, 0.998046875, 0.0224609375, 0.0107421875, 0.998046875, 0.0205078125, 0.0498046875, 0.9990234375, 0.0146484375, 0.9990234375, 0.1279296875, 0.0146484375, 0.056640625, 0.0869140625, 0.005859375, 0.0625, 0.05078125, 0.0712890625, 0.099609375, 0.02734375, 0.73046875, 0.0166015625, 0.0869140625, 0.01953125, 0.9970703125, 0.0283203125, 0.0078125, 0.015625, 0.04296875, 0.9970703125, 0.82421875, 0.068359375, 0.8017578125, 0.044921875, 0.0400390625, 0.9501953125, 0.01953125, 0.0732421875, 0.048828125, 0.0400390625, 0.080078125, 0.9970703125, 0.041015625, 0.166015625, 0.0185546875, 0.001953125, 0.0166015625, 0.162109375, 0.0263671875, 0.998046875, 0.982421875, 0.0400390625, 0.0625, 0.01953125, 0.0439453125, 0.0224609375, 0.8857421875, 0.072265625, 0.0537109375, 0.03515625, 0.015625, 0.0166015625, 0.9990234375, 0.9501953125, 0.998046875, 0.048828125, 0.0458984375, 0.99609375, 0.9970703125, 0.9990234375, 0.0341796875, 0.0361328125, 0.029296875, 0.033203125, 0.208984375, 0.3798828125, 0.0146484375, 0.0087890625, 0.1005859375]

 sparsity of   [0.9995659589767456, 0.0390625, 0.1319444477558136, 0.0490451380610466, 0.0017361111240461469, 0.0399305559694767, 0.8854166865348816, 0.0525173619389534, 0.999131977558136, 0.02690972201526165, 0.796006977558136, 0.8363715410232544, 0.014322916977107525, 0.0264756940305233, 0.1085069477558136, 0.0355902798473835, 0.7447916865348816, 0.9995659589767456, 0.01605902798473835, 0.0902777761220932, 0.014756944961845875, 0.013020833022892475, 0.046875, 0.00824652798473835, 0.01822916604578495, 0.01171875, 0.150173619389534, 0.3684895932674408, 0.02994791604578495, 0.01996527798473835, 0.0542534738779068, 0.01822916604578495, 0.03081597201526165, 0.94140625, 0.0451388880610466, 0.0399305559694767, 0.02473958395421505, 0.03081597201526165, 0.0342881940305233, 0.0859375, 0.009982638992369175, 0.01171875, 0.0399305559694767, 0.1089409738779068, 0.0638020858168602, 0.02300347201526165, 0.491753488779068, 0.03125, 0.01692708395421505, 0.0425347238779068, 0.013888888992369175, 0.4752604067325592, 0.9995659589767456, 0.2565104067325592, 0.0920138880610466, 0.1792534738779068, 0.0125868059694767, 0.7074652910232544, 0.1723090261220932, 0.0785590261220932, 0.01909722201526165, 0.0980902761220932, 0.007378472480922937, 0.0885416641831398, 0.0234375, 0.9995659589767456, 0.0655381977558136, 0.0464409738779068, 0.02083333395421505, 0.8515625, 0.7825520634651184, 0.0494791679084301, 0.5833333134651184, 0.01692708395421505, 0.0264756940305233, 0.02604166604578495, 0.0342881940305233, 0.013454861007630825, 0.07421875, 0.01996527798473835, 0.014756944961845875, 0.0325520820915699, 0.01953125, 0.009982638992369175, 0.010850694961845875, 0.03081597201526165, 0.0551215298473835, 0.9696180820465088, 0.4778645932674408, 0.6059027910232544, 0.9986979365348816, 0.5590277910232544, 0.0651041641831398, 0.0234375, 0.0373263880610466, 0.0364583320915699, 0.9986979365348816, 0.01171875, 0.0329861119389534, 0.05859375, 0.838975727558136, 0.3046875, 0.1401909738779068, 0.9995659589767456, 0.0876736119389534, 0.0651041641831398, 0.0859375, 0.03515625, 0.01519097201526165, 0.1393229216337204, 0.0125868059694767, 0.8276909589767456, 0.01171875, 0.51953125, 0.0594618059694767, 0.02864583395421505, 0.0086805559694767, 0.1032986119389534, 0.0607638880610466, 0.9986979365348816, 0.0078125, 0.999131977558136, 0.0, 0.92578125, 0.1575520783662796, 0.9474826455116272, 0.01779513992369175, 0.2504340410232544, 0.02690972201526165, 0.03125, 0.0442708320915699, 0.0234375, 0.1293402761220932, 0.0438368059694767, 0.0581597238779068, 0.0086805559694767, 0.01779513992369175, 0.581163227558136, 0.5325520634651184, 0.1662326455116272, 0.02777777798473835, 0.8185763955116272, 0.177517369389534, 0.02864583395421505, 0.0733506977558136, 0.999131977558136, 0.0881076380610466, 0.7035590410232544, 0.010416666977107525, 0.0390625, 0.0872395858168602, 0.1072048619389534, 0.0568576380610466, 0.1002604141831398, 0.0243055559694767, 0.1592881977558136, 0.8146701455116272, 0.596788227558136, 0.9986979365348816, 0.0125868059694767, 0.0373263880610466, 0.846788227558136, 0.013454861007630825, 0.0373263880610466, 0.0651041641831398, 0.944444477558136, 0.0533854179084301, 0.0620659738779068, 0.0364583320915699, 0.5924479365348816, 0.0863715261220932, 0.0282118059694767, 0.1393229216337204, 0.0533854179084301, 0.0329861119389534, 0.472222238779068, 0.0442708320915699, 0.8385416865348816, 0.0629340261220932, 0.0425347238779068, 0.0407986119389534, 0.0442708320915699, 0.0512152798473835, 0.0234375, 0.010416666977107525, 0.1875, 0.4344618022441864, 0.0125868059694767, 0.0390625, 0.0572916679084301, 0.0212673619389534, 0.01909722201526165, 0.02777777798473835, 0.0290798619389534, 0.9986979365348816, 0.01128472201526165, 0.0173611119389534, 0.0772569477558136, 0.16796875, 0.0933159738779068, 0.067274309694767, 0.9986979365348816, 0.0368923619389534, 0.6041666865348816, 0.999131977558136, 0.0659722238779068, 0.02213541604578495, 0.0581597238779068, 0.01822916604578495, 0.1688368022441864, 0.0512152798473835, 0.0564236119389534, 0.02170138992369175, 0.0325520820915699, 0.01605902798473835, 0.03515625, 0.9379340410232544, 0.005642361007630825, 0.0334201380610466, 0.01822916604578495, 0.8441840410232544, 0.01909722201526165, 0.0603298619389534, 0.0520833320915699, 0.6019965410232544, 0.1432291716337204, 0.01692708395421505, 0.0078125, 0.2118055522441864, 0.999131977558136, 0.014756944961845875, 0.0598958320915699, 0.0338541679084301, 0.1032986119389534, 0.7473958134651184, 0.0824652761220932, 0.086805559694767, 0.1315104216337204, 0.0173611119389534, 0.0416666679084301, 0.0052083334885537624, 0.013454861007630825, 0.8424479365348816, 0.1605902761220932, 0.7894965410232544, 0.009982638992369175, 0.0321180559694767, 0.0512152798473835, 0.1801215261220932, 0.0364583320915699, 0.0533854179084301, 0.0659722238779068, 0.721788227558136, 0.0078125, 0.01822916604578495, 0.01909722201526165]

 sparsity of   [0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.99609375, 0.80859375, 0.99609375, 0.98828125, 0.109375, 0.984375, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.98828125, 0.875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.89453125, 0.9921875, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.8671875, 0.9921875, 0.90625, 0.98828125, 0.9921875, 0.9921875, 0.66796875, 0.9921875, 0.98828125, 0.99609375, 0.984375, 0.984375, 0.81640625, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.82421875, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.99609375, 0.703125, 0.44921875, 0.9921875, 0.9921875, 0.82421875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.89453125, 0.84765625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.99609375, 0.8828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.87890625, 0.9140625, 0.61328125, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.984375, 0.984375, 0.9609375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.984375, 0.9296875, 0.9921875, 0.9921875, 0.99609375, 0.1953125, 0.9921875, 0.8515625, 0.66796875, 0.98828125, 0.71484375, 0.8828125, 0.08984375, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.6640625, 0.9921875, 0.98828125, 0.9921875, 0.84375, 0.98828125, 0.9921875, 0.828125, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.84765625, 0.9921875, 0.828125, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.8203125, 0.9921875, 0.9921875, 0.9921875, 0.83984375, 0.9921875, 0.99609375, 0.99609375, 0.80859375, 0.9921875, 0.98828125, 0.984375, 0.82421875, 0.6875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.71484375, 0.9921875, 0.984375, 0.99609375, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.99609375, 0.85546875, 0.91796875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.98828125, 0.82421875, 0.9921875, 0.984375, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.84765625, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.60546875, 0.9921875, 0.80859375, 0.98828125, 0.9921875, 0.99609375, 0.98828125, 0.09765625, 0.9921875, 0.98828125, 0.984375, 0.99609375, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.1875, 0.70703125, 0.9921875, 0.875, 0.8359375, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.6875, 0.98828125, 0.98828125, 0.5625, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.89453125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.8359375, 0.80078125, 0.69921875, 0.9921875, 0.98828125, 0.9921875, 0.84765625, 0.90234375, 0.78515625, 0.87890625, 0.8515625, 0.890625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.7109375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.7265625, 0.9921875, 0.9921875, 0.9921875, 0.74609375, 0.9921875, 0.84765625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.80859375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.07421875, 0.859375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.80859375, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.6015625, 0.98828125, 0.99609375, 0.9921875, 0.984375, 0.6640625, 0.98828125, 0.9921875, 0.6328125, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.84375, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.79296875, 0.9921875, 0.99609375, 0.98828125, 0.99609375, 0.99609375, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.87109375, 0.9921875, 0.9921875, 0.984375, 0.99609375, 0.9921875, 0.8984375, 0.9921875, 0.98828125, 0.10546875, 0.0, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.78515625, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.80859375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.8046875, 0.9921875, 0.98828125, 0.67578125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.8203125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.85546875, 0.9921875, 0.85546875, 0.9921875, 0.984375, 0.19921875, 0.9921875, 0.625, 0.9921875, 0.875, 0.984375, 0.99609375, 0.7421875, 0.98828125, 0.9921875, 0.859375, 0.7265625, 0.62109375, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.79296875, 0.98828125, 0.8671875, 0.83203125, 0.9921875, 0.83984375, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.62109375, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.765625, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.98828125, 0.09375, 0.984375, 0.83984375, 0.8359375, 0.10546875, 0.984375, 0.984375, 0.9921875, 0.609375, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.98828125, 0.99609375, 0.6953125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.66796875, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.84765625, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.8203125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.765625, 0.9921875, 0.8828125, 0.9921875, 0.8125, 0.9921875, 0.890625, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.99609375, 0.08984375, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.87890625, 0.98828125, 0.9921875, 0.9453125, 0.98828125, 0.86328125, 0.91015625, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.99609375, 0.88671875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.89453125, 0.9921875, 0.8359375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.80078125, 0.9921875, 0.984375, 0.9921875, 0.98828125, 0.9921875, 0.88671875, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.7265625, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.8359375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.828125, 0.98828125, 0.7890625, 0.99609375, 0.984375, 0.80859375, 0.9921875, 0.98828125, 0.81640625, 0.70703125, 0.83203125, 0.9921875, 0.9921875, 0.625, 0.9921875, 0.8671875, 0.98828125, 0.89453125, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.86328125, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.984375, 0.98828125, 0.90234375, 0.9921875, 0.98828125, 0.9921875, 0.81640625, 0.98828125, 0.9921875, 0.9921875, 0.80078125, 0.99609375, 0.66015625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.98828125, 0.6796875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.0, 0.69140625, 0.84375, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.80859375, 0.9921875, 0.9921875, 0.9921875, 0.3671875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.87109375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.7890625, 0.9921875, 0.98828125, 0.98828125, 0.71484375, 0.9921875, 0.984375, 0.984375, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.1328125, 0.984375, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.66796875, 0.7421875, 0.98828125, 0.984375, 0.99609375, 0.7890625, 0.98828125, 0.99609375, 0.86328125, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9140625, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.84765625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.80859375, 0.87890625, 0.9921875, 0.8203125, 0.67578125, 0.203125, 0.9921875, 0.84765625, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.81640625, 0.98828125, 0.875, 0.9921875, 0.9921875, 0.5859375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.87109375, 0.984375, 0.9921875, 0.8515625, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.65625, 0.8046875, 0.9921875, 0.9921875, 0.84375, 0.98828125, 0.8671875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.875, 0.98828125, 0.0, 0.984375, 0.99609375, 0.98828125, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.984375, 0.9921875, 0.99609375, 0.9921875, 0.85546875, 0.60546875, 0.796875, 0.98828125, 0.9921875, 0.99609375, 0.79296875, 0.98828125, 0.890625, 0.99609375, 0.99609375, 0.98828125, 0.85546875, 0.12890625, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.05859375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.8828125, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.828125, 0.9921875, 0.98828125, 0.99609375, 0.25390625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.66796875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.87109375, 0.86328125, 0.828125, 0.98828125, 0.9921875, 0.734375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.8984375, 0.98828125, 0.984375, 0.10546875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.86328125, 0.98828125, 0.99609375, 0.83203125, 0.9921875, 0.734375, 0.9921875, 0.9921875, 0.06640625, 0.9921875, 0.99609375, 0.22265625, 0.9921875, 0.9921875, 0.875, 0.9921875, 0.98828125, 0.83203125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.984375, 0.6875, 0.96875, 0.98828125, 0.84765625, 0.98828125, 0.9921875, 0.8046875, 0.98828125, 0.9921875, 0.78125, 0.9921875, 0.9921875, 0.66015625, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.6484375, 0.9921875, 0.984375, 0.83203125, 0.98828125, 0.98828125]

 sparsity of   [0.0205078125, 0.9970703125, 0.998046875, 0.1865234375, 0.90625, 0.142578125, 0.1650390625, 0.025390625, 0.1376953125, 0.998046875, 0.8662109375, 0.1240234375, 0.0771484375, 0.0380859375, 0.998046875, 0.0654296875, 0.09375, 0.998046875, 0.3583984375, 0.0908203125, 0.0576171875, 0.0478515625, 0.0654296875, 0.91015625, 0.8994140625, 0.9970703125, 0.03515625, 0.037109375, 0.05859375, 0.4287109375, 0.2021484375, 0.0244140625, 0.12890625, 0.998046875, 0.9970703125, 0.310546875, 0.005859375, 0.0, 0.908203125, 0.9375, 0.0263671875, 0.892578125, 0.9990234375, 0.3212890625, 0.11328125, 0.08203125, 0.123046875, 0.0146484375, 0.6015625, 0.04296875, 0.8984375, 0.201171875, 0.998046875, 0.0556640625, 0.0615234375, 0.998046875, 0.1572265625, 0.015625, 0.8720703125, 0.095703125, 0.0556640625, 0.212890625, 0.072265625, 0.548828125, 0.158203125, 0.4150390625, 0.6943359375, 0.1123046875, 0.880859375, 0.16796875, 0.0087890625, 0.19140625, 0.998046875, 0.0244140625, 0.1845703125, 0.99609375, 0.0283203125, 0.0322265625, 0.421875, 0.998046875, 0.03515625, 0.9970703125, 0.07421875, 0.3759765625, 0.1953125, 0.1455078125, 0.61328125, 0.4443359375, 0.9970703125, 0.998046875, 0.23046875, 0.8828125, 0.0263671875, 0.26953125, 0.01171875, 0.8173828125, 0.2705078125, 0.998046875, 0.87890625, 0.998046875, 0.0478515625, 0.03515625, 0.23828125, 0.0439453125, 0.0390625, 0.142578125, 0.3154296875, 0.0068359375, 0.998046875, 0.9990234375, 0.0244140625, 0.99609375, 0.0615234375, 0.1162109375, 0.962890625, 0.998046875, 0.8056640625, 0.0205078125, 0.998046875, 0.017578125, 0.841796875, 0.1513671875, 0.0546875, 0.046875, 0.892578125, 0.888671875, 0.8134765625, 0.998046875, 0.958984375, 0.8125, 0.24609375, 0.0380859375, 0.0185546875, 0.0263671875, 0.998046875, 0.056640625, 0.1083984375, 0.0458984375, 0.966796875, 0.998046875, 0.0693359375, 0.998046875, 0.8994140625, 0.5634765625, 0.9228515625, 0.998046875, 0.072265625, 0.0888671875, 0.091796875, 0.998046875, 0.216796875, 0.998046875, 0.1064453125, 0.0322265625, 0.998046875, 0.900390625, 0.998046875, 0.99609375, 0.0380859375, 0.1015625, 0.998046875, 0.0712890625, 0.1728515625, 0.998046875, 0.998046875, 0.083984375, 0.4404296875, 0.1220703125, 0.9970703125, 0.0361328125, 0.99609375, 0.8662109375, 0.1337890625, 0.1591796875, 0.998046875, 0.9970703125, 0.8974609375, 0.02734375, 0.0380859375, 0.2216796875, 0.0361328125, 0.998046875, 0.7958984375, 0.998046875, 0.09375, 0.14453125, 0.248046875, 0.107421875, 0.1181640625, 0.0205078125, 0.2734375, 0.05859375, 0.9970703125, 0.998046875, 0.998046875, 0.525390625, 0.998046875, 0.109375, 0.998046875, 0.123046875, 0.080078125, 0.0859375, 0.998046875, 0.099609375, 0.0966796875, 0.0908203125, 0.1240234375, 0.1591796875, 0.998046875, 0.0947265625, 0.0361328125, 0.0625, 0.998046875, 0.892578125, 0.07421875, 0.8408203125, 0.0390625, 0.998046875, 0.0986328125, 0.9970703125, 0.0234375, 0.998046875, 0.998046875, 0.9970703125, 0.9248046875, 0.1962890625, 0.9970703125, 0.0380859375, 0.5, 0.84765625, 0.111328125, 0.0478515625, 0.853515625, 0.021484375, 0.0400390625, 0.0224609375, 0.998046875, 0.55859375, 0.89453125, 0.9990234375, 0.181640625, 0.900390625, 0.0791015625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.8642578125, 0.802734375, 0.998046875, 0.0478515625, 0.70703125, 0.216796875, 0.044921875, 0.998046875, 0.37109375]

 sparsity of   [0.04296875, 0.1202256977558136, 0.2664930522441864, 0.9782986044883728, 0.1006944477558136, 0.0924479141831398, 0.999131977558136, 0.999131977558136, 0.177517369389534, 0.0282118059694767, 0.0355902798473835, 0.007378472480922937, 0.2703993022441864, 0.0824652761220932, 0.02387152798473835, 0.0525173619389534, 0.1228298619389534, 0.0225694440305233, 0.234375, 0.009548611007630825, 0.05078125, 0.11328125, 0.02604166604578495, 0.0034722222480922937, 0.0, 0.01953125, 0.01822916604578495, 0.1206597238779068, 0.0755208358168602, 0.0334201380610466, 0.0455729179084301, 0.02170138992369175, 0.0802951380610466, 0.2256944477558136, 0.9986979365348816, 0.0355902798473835, 0.2018229216337204, 0.02473958395421505, 0.0290798619389534, 0.1206597238779068, 0.0572916679084301, 0.046875, 0.01953125, 0.0412326380610466, 0.5056423544883728, 0.0442708320915699, 0.098524309694767, 0.0564236119389534, 0.7634548544883728, 0.1089409738779068, 0.9986979365348816, 0.0434027798473835, 0.0442708320915699, 0.0390625, 0.02560763992369175, 0.0442708320915699, 0.01953125, 0.6384548544883728, 0.0455729179084301, 0.1271701455116272, 0.0533854179084301, 0.0490451380610466, 0.02387152798473835, 0.014756944961845875, 0.0373263880610466, 0.7252604365348816, 0.6458333134651184, 0.3151041567325592, 0.0421006940305233, 0.0651041641831398, 0.7578125, 0.0425347238779068, 0.9986979365348816, 0.2717013955116272, 0.0125868059694767, 0.9040798544883728, 0.02560763992369175, 0.0364583320915699, 0.5711805820465088, 0.0451388880610466, 0.9986979365348816, 0.00824652798473835, 0.015625, 0.7873263955116272, 0.0915798619389534, 0.0334201380610466, 0.2191840261220932, 0.0872395858168602, 0.0338541679084301, 0.0824652761220932, 0.0477430559694767, 0.7144097089767456, 0.2599826455116272, 0.014322916977107525, 0.09765625, 0.06640625, 0.01779513992369175, 0.228298619389534, 0.0486111119389534, 0.173611119389534, 0.0251736119389534, 0.1019965261220932, 0.01822916604578495, 0.1254340261220932, 0.0772569477558136, 0.6098090410232544, 0.1050347238779068, 0.02604166604578495, 0.009982638992369175, 0.01128472201526165, 0.0572916679084301, 0.0399305559694767, 0.0963541641831398, 0.01692708395421505, 0.0494791679084301, 0.0355902798473835, 0.1215277761220932, 0.0008680555620230734, 0.0381944440305233, 0.02951388992369175, 0.0334201380610466, 0.0164930559694767, 0.0594618059694767, 0.0733506977558136, 0.9986979365348816, 0.0512152798473835, 0.0785590261220932, 0.09765625, 0.7196180820465088, 0.0846354141831398, 0.901475727558136, 0.01215277798473835, 0.1566840261220932, 0.0438368059694767, 0.1206597238779068, 0.0290798619389534, 0.9986979365348816, 0.02951388992369175, 0.0225694440305233, 0.0125868059694767, 0.8689236044883728, 0.02864583395421505, 0.02951388992369175, 0.1015625, 0.1684027761220932, 0.01171875, 0.8211805820465088, 0.01519097201526165, 0.97265625, 0.0638020858168602, 0.02951388992369175, 0.098524309694767, 0.6371527910232544, 0.0933159738779068, 0.014756944961845875, 0.0594618059694767, 0.3537326455116272, 0.0998263880610466, 0.02690972201526165, 0.8046875, 0.0360243059694767, 0.114149309694767, 0.1388888955116272, 0.0915798619389534, 0.4079861044883728, 0.013888888992369175, 0.54296875, 0.9995659589767456, 0.0399305559694767, 0.3793402910232544, 0.02690972201526165, 0.0724826380610466, 0.067274309694767, 0.01909722201526165, 0.1119791641831398, 0.05859375, 0.0173611119389534, 0.05859375, 0.0243055559694767, 0.0707465261220932, 0.0347222238779068, 0.12109375, 0.0473090298473835, 0.0516493059694767, 0.0447048619389534, 0.1263020783662796, 0.1458333283662796, 0.1111111119389534, 0.9739583134651184, 0.0360243059694767, 0.1215277761220932, 0.0460069440305233, 0.02170138992369175, 0.0768229141831398, 0.9995659589767456, 0.0798611119389534, 0.5885416865348816, 0.0186631940305233, 0.02690972201526165, 0.0421006940305233, 0.8706597089767456, 0.01605902798473835, 0.433159738779068, 0.0720486119389534, 0.0455729179084301, 0.7595486044883728, 0.0442708320915699, 0.8064236044883728, 0.0655381977558136, 0.9986979365348816, 0.02951388992369175, 0.2252604216337204, 0.999131977558136, 0.010416666977107525, 0.8159722089767456, 0.0881076380610466, 0.01779513992369175, 0.0598958320915699, 0.014756944961845875, 0.01605902798473835, 0.009982638992369175, 0.009982638992369175, 0.075086809694767, 0.0533854179084301, 0.0460069440305233, 0.8359375, 0.02300347201526165, 0.0334201380610466, 0.0416666679084301, 0.0065104165114462376, 0.0403645820915699, 0.0542534738779068, 0.6041666865348816, 0.086805559694767, 0.0972222238779068, 0.0959201380610466, 0.9995659589767456, 0.0425347238779068, 0.0342881940305233, 0.0212673619389534, 0.02777777798473835, 0.7894965410232544, 0.33984375, 0.1245659738779068, 0.0360243059694767, 0.009982638992369175, 0.0377604179084301, 0.01519097201526165, 0.9401041865348816, 0.02734375, 0.0477430559694767, 0.0638020858168602, 0.5681423544883728, 0.0360243059694767, 0.0755208358168602, 0.0520833320915699]

 sparsity of   [0.94140625, 0.9921875, 0.99609375, 0.99609375, 0.98828125, 0.99609375, 0.9921875, 0.10546875, 0.99609375, 0.9921875, 0.99609375, 0.82421875, 0.87890625, 0.94921875, 0.89453125, 0.921875, 0.9921875, 0.85546875, 0.99609375, 0.9921875, 0.99609375, 0.91015625, 0.9921875, 0.99609375, 0.99609375, 0.87890625, 0.921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.93359375, 0.12109375, 0.99609375, 0.9921875, 0.94921875, 0.921875, 0.98828125, 0.828125, 0.9921875, 0.06640625, 0.99609375, 0.8359375, 0.921875, 0.9921875, 0.99609375, 0.99609375, 0.7890625, 0.9921875, 0.9921875, 0.9921875, 0.78515625, 0.9921875, 0.81640625, 0.12890625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.93359375, 0.90625, 0.8671875, 0.9921875, 0.99609375, 0.9453125, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.8984375, 0.734375, 0.99609375, 0.99609375, 0.9921875, 0.109375, 0.99609375, 0.99609375, 0.8515625, 0.99609375, 0.99609375, 0.8671875, 0.8671875, 0.9921875, 0.8671875, 0.17578125, 0.99609375, 0.9921875, 0.9921875, 0.12109375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9375, 0.9140625, 0.91015625, 0.81640625, 0.99609375, 0.84765625, 0.95703125, 0.796875, 0.99609375, 0.99609375, 0.9921875, 0.93359375, 0.9921875, 0.90625, 0.83203125, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.92578125, 0.99609375, 0.9921875, 0.92578125, 0.9921875, 0.9921875, 0.99609375, 0.23828125, 0.93359375, 0.9453125, 0.99609375, 0.99609375, 0.99609375, 0.875, 0.78125, 0.99609375, 0.92578125, 0.99609375, 0.9296875, 0.96484375, 0.85546875, 0.9921875, 0.99609375, 0.9921875, 0.27734375, 0.99609375, 0.9921875, 0.13671875, 0.93359375, 0.9921875, 0.9296875, 0.99609375, 0.9921875, 0.1015625, 0.9921875, 0.98828125, 0.59375, 0.9921875, 0.98828125, 0.9921875, 0.90625, 0.99609375, 0.9921875, 0.95703125, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.09375, 0.99609375, 0.99609375, 0.99609375, 0.890625, 0.93359375, 0.9921875, 0.80859375, 0.99609375, 0.9921875, 0.81640625, 0.8984375, 0.9921875, 0.99609375, 0.9921875, 0.1015625, 0.92578125, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.953125, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.28125, 0.99609375, 0.20703125, 0.9921875, 0.99609375, 0.80859375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.8671875, 0.95703125, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.98046875, 0.9921875, 0.99609375, 0.9921875, 0.0859375, 0.99609375, 0.8203125, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.953125, 0.99609375, 0.99609375, 0.89453125, 0.96484375, 0.80078125, 0.99609375, 0.98828125, 0.9296875, 0.99609375, 0.1171875, 0.99609375, 0.8984375, 0.9921875, 0.9296875, 0.98828125, 0.99609375, 0.87890625, 0.94921875, 0.99609375, 0.92578125, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.93359375, 0.9921875, 0.08203125, 0.12109375, 0.9921875, 0.96875, 0.9921875, 0.9921875, 0.85546875, 0.9921875, 0.109375, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9609375, 0.99609375, 0.8359375, 0.890625, 0.9921875, 0.8984375, 0.99609375, 0.9921875, 0.9921875, 0.765625, 0.9921875, 0.9140625, 0.99609375, 0.99609375, 0.828125, 0.99609375, 0.9921875, 0.84765625, 0.8515625, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.84375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.890625, 0.99609375, 0.99609375, 0.99609375, 0.703125, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.94921875, 0.99609375, 0.99609375, 0.94921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.90234375, 0.9921875, 0.99609375, 0.859375, 0.89453125, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.99609375, 0.99609375, 0.95703125, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.859375, 0.99609375, 0.69140625, 0.88671875, 0.99609375, 0.99609375, 0.11328125, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.8984375, 0.9921875, 0.9921875, 0.87890625, 0.9921875, 0.9921875, 0.9921875, 0.8203125, 0.99609375, 0.9921875, 0.98828125, 0.88671875, 0.99609375, 0.99609375, 0.98046875, 0.9921875, 0.9921875, 0.9921875, 0.2734375, 0.9453125, 0.99609375, 0.9921875, 0.24609375, 0.9921875, 0.99609375, 0.99609375, 0.84375, 0.921875, 0.8671875, 0.99609375, 0.9921875, 0.9921875, 0.953125, 0.9921875, 0.96875, 0.9453125, 0.9921875, 0.9921875, 0.8359375, 0.03515625, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.82421875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.890625, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.98828125, 0.9296875, 0.8359375, 0.953125, 0.86328125, 0.99609375, 0.9921875, 0.98828125, 0.99609375, 0.81640625, 0.99609375, 0.99609375, 0.9921875, 0.66796875, 0.97265625, 0.99609375, 0.89453125, 0.99609375, 0.90234375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.87109375, 0.99609375, 0.9296875, 0.9609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.84765625, 0.80859375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.98828125, 0.84765625, 0.87890625, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.921875, 0.99609375, 0.9921875, 0.90625, 0.9921875, 0.9921875, 0.9921875, 0.89453125, 0.97265625, 0.96484375, 0.9921875, 0.91796875, 0.99609375, 0.9921875, 0.9453125, 0.9375, 0.87890625, 0.9921875, 0.99609375, 0.89453125, 0.93359375, 0.9921875, 0.99609375, 0.99609375, 0.83203125, 0.99609375, 0.99609375, 0.9921875, 0.8203125, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.08984375, 0.94140625, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.88671875, 0.99609375, 0.99609375, 0.99609375, 0.8515625, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.87890625, 0.99609375, 0.91015625, 0.9921875, 0.9921875, 0.85546875, 0.984375, 0.99609375, 0.99609375, 0.92578125, 0.99609375, 0.84375, 0.9921875, 0.99609375, 0.87890625, 0.9921875, 0.9296875, 0.9296875, 0.99609375, 0.99609375, 0.89453125, 0.99609375, 0.84765625, 0.9921875, 0.80078125, 0.09375, 0.85546875, 0.99609375, 0.99609375, 0.01953125, 0.9921875, 0.99609375, 0.99609375, 0.8671875, 0.10546875, 0.37890625, 0.9921875, 0.9921875, 0.93359375, 0.99609375, 0.9921875, 0.9921875, 0.90625, 0.80078125, 0.99609375, 0.99609375, 0.9921875, 0.88671875, 0.99609375, 0.99609375, 0.85546875, 0.96484375, 0.83203125, 0.99609375, 0.9921875, 0.734375, 0.9921875, 0.99609375, 0.890625, 0.9921875, 0.99609375, 0.9921875, 0.92578125, 0.84765625, 0.99609375, 0.9921875, 0.9453125, 0.9921875, 0.9140625, 0.8984375, 0.9921875, 0.890625, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.1640625, 0.99609375, 0.99609375, 0.8125, 0.9921875, 0.99609375, 0.9921875, 0.9765625, 0.9453125, 0.97265625, 0.890625, 0.98828125, 0.94140625, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.953125, 0.9921875, 0.9921875, 0.95703125, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.8359375, 0.99609375, 0.99609375, 0.89453125, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.94140625, 0.9921875, 0.90625, 0.80859375, 0.875, 0.9140625, 0.99609375, 0.953125, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.1328125, 0.9921875, 0.99609375, 0.8984375, 0.99609375, 0.44140625, 0.99609375, 0.91015625, 0.9921875, 0.99609375, 0.9921875, 0.8984375, 0.80078125, 0.9921875, 0.94140625, 0.984375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.953125, 0.99609375, 0.99609375, 0.99609375, 0.9140625, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.59765625, 0.99609375, 0.99609375, 0.99609375, 0.8828125, 0.96875, 0.96484375, 0.9921875, 0.9921875, 0.04296875, 0.8125, 0.83984375, 0.9921875, 0.99609375, 0.90234375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.83984375, 0.99609375, 0.9921875, 0.90625, 0.99609375, 0.99609375, 0.10546875, 0.89453125, 0.88671875, 0.765625, 0.8359375, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.89453125, 0.9921875, 0.9921875, 0.828125, 0.84765625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.890625, 0.99609375, 0.87109375, 0.8515625, 0.99609375, 0.99609375, 0.99609375, 0.91015625, 0.83984375, 0.99609375, 0.98828125, 0.99609375, 0.83984375, 0.921875, 0.9921875, 0.9921875, 0.1796875, 0.96484375, 0.99609375, 0.9921875, 0.83203125, 0.1171875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.89453125, 0.99609375, 0.99609375, 0.99609375, 0.87890625, 0.875, 0.99609375, 0.8984375, 0.93359375, 0.8515625, 0.84765625, 0.9921875, 0.890625, 0.99609375, 0.99609375, 0.92578125, 0.99609375, 0.9765625, 0.95703125, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.87890625, 0.9921875, 0.12890625, 0.890625, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.95703125, 0.9765625, 0.99609375, 0.0546875, 0.99609375, 0.14453125, 0.99609375, 0.98828125, 0.99609375, 0.875, 0.16015625, 0.99609375, 0.9921875, 0.828125, 0.99609375, 0.9453125, 0.9921875, 0.86328125, 0.80078125, 0.99609375, 0.99609375, 0.99609375, 0.8671875, 0.140625, 0.9609375, 0.9921875, 0.953125, 0.99609375, 0.9921875, 0.92578125, 0.99609375, 0.99609375, 0.94140625, 0.9921875, 0.9921875, 0.90234375, 0.99609375, 0.0, 0.99609375, 0.9140625, 0.8203125, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.859375, 0.8359375, 0.98828125, 0.99609375, 0.99609375, 0.99609375, 0.8671875, 0.86328125, 0.96484375, 0.99609375, 0.83203125, 0.99609375, 0.83203125, 0.96875, 0.99609375, 0.9921875, 0.9921875, 0.90234375, 0.99609375, 0.99609375, 0.92578125, 0.99609375, 0.99609375, 0.8515625, 0.98828125, 0.16015625, 0.87890625, 0.99609375, 0.99609375, 0.9921875, 0.91015625, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.93359375, 0.91015625, 0.8046875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.91796875, 0.99609375, 0.9921875, 0.953125, 0.90234375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.89453125, 0.91015625, 0.828125, 0.95703125, 0.98828125, 0.99609375, 0.99609375, 0.21875, 0.99609375, 0.8125, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.88671875, 0.9921875, 0.92578125, 0.9921875, 0.9140625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9140625, 0.99609375, 0.8984375, 0.9921875, 0.125, 0.99609375, 0.88671875, 0.95703125, 0.99609375, 0.0390625, 0.99609375, 0.9921875, 0.93359375, 0.99609375, 0.99609375, 0.9921875, 0.953125, 0.98828125, 0.890625, 0.9921875, 0.9921875, 0.99609375, 0.9140625, 0.9921875, 0.890625, 0.09765625, 0.99609375, 0.9921875, 0.859375, 0.9921875, 0.48046875, 0.98828125, 0.171875, 0.9921875, 0.89453125, 0.99609375, 0.99609375, 0.875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.90625, 0.9921875, 0.84765625, 0.859375, 0.9921875, 0.99609375, 0.83984375, 0.99609375, 0.8125, 0.80859375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.0390625, 0.96484375, 0.8984375, 0.9609375, 0.9296875, 0.109375, 0.98828125, 0.9453125, 0.99609375, 0.9375, 0.90625, 0.99609375, 0.9921875, 0.83203125, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.87109375, 0.99609375, 0.21484375, 0.1875, 0.9921875, 0.9453125, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.91015625, 0.8359375, 0.9375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.859375, 0.99609375, 0.81640625, 0.9921875, 0.875, 0.8984375]

 sparsity of   [0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.1376953125, 0.31640625, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.4580078125, 0.998046875, 0.998046875, 0.4716796875, 0.998046875, 0.9990234375, 0.1240234375, 0.544921875, 0.998046875, 0.9990234375, 0.1435546875, 0.9970703125, 0.1240234375, 0.998046875, 0.083984375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.09765625, 0.125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.171875, 0.9970703125, 0.998046875, 0.9970703125, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.125, 0.16015625, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.5361328125, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.484375, 0.998046875, 0.998046875, 0.126953125, 0.9990234375, 0.328125, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.1298828125, 0.998046875, 0.9990234375, 0.4052734375, 0.998046875, 0.998046875, 0.9970703125, 0.685546875, 0.998046875, 0.998046875, 0.15625, 0.9990234375, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.1181640625, 0.1005859375, 0.9990234375, 0.4697265625, 0.9990234375, 0.9970703125, 0.083984375, 0.9990234375, 0.99609375, 0.9990234375, 0.998046875, 0.998046875, 0.1220703125, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.125, 0.9990234375, 0.1005859375, 0.302734375, 0.998046875, 0.998046875, 0.9990234375, 0.1044921875, 0.9990234375, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.28515625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.8837890625, 0.998046875, 0.998046875, 0.998046875, 0.1552734375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1552734375, 0.998046875, 0.0712890625, 0.9970703125, 0.9990234375, 0.9990234375, 0.09765625, 0.9990234375, 0.998046875, 0.9990234375, 0.1025390625, 0.2763671875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.99609375, 0.9990234375, 0.44921875, 0.9990234375, 0.99609375, 0.998046875, 0.248046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.51953125, 0.9990234375, 0.431640625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.5634765625, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.0126953125, 0.9990234375, 0.46484375, 0.998046875, 0.998046875, 0.0205078125, 0.998046875, 0.9990234375, 0.53515625, 0.998046875, 0.9990234375, 0.9990234375, 0.4296875, 0.9990234375, 0.998046875, 0.9990234375, 0.13671875, 0.9970703125, 0.9990234375, 0.998046875, 0.1328125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.4287109375, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.1396484375, 0.1142578125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9970703125, 0.529296875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.45703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.146484375, 0.9990234375, 0.998046875, 0.4013671875, 0.1474609375, 0.998046875, 0.4013671875, 0.5439453125, 0.1005859375, 0.9990234375, 0.482421875, 0.4560546875, 0.1103515625, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.1025390625, 0.095703125, 0.998046875, 0.2294921875, 0.998046875, 0.998046875, 0.9990234375, 0.1025390625, 0.998046875, 0.9990234375, 0.1123046875, 0.9970703125, 0.2666015625, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.9970703125, 0.9990234375, 0.4423828125, 0.998046875, 0.9970703125, 0.119140625, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.14453125, 0.998046875, 0.998046875, 0.998046875, 0.619140625, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.6064453125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.1083984375, 0.9990234375, 0.9990234375, 0.119140625, 0.998046875, 0.9990234375, 0.1298828125, 0.998046875, 0.9990234375, 0.0, 0.109375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.4853515625, 0.9970703125, 0.9970703125, 0.1328125, 0.998046875, 0.103515625, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1025390625, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.10546875, 0.1015625, 0.998046875, 0.1025390625, 0.9990234375, 0.998046875, 0.1123046875, 0.634765625, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.435546875, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.24609375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.5009765625, 0.9990234375, 0.1396484375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.1083984375, 0.1171875, 0.822265625, 0.482421875, 0.998046875, 0.2783203125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.1181640625, 0.1259765625, 0.0, 0.3369140625, 0.99609375, 0.998046875, 0.9990234375, 0.998046875, 0.3994140625, 0.998046875, 0.2880859375, 0.99609375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875]

 sparsity of   [0.9887152910232544, 0.955078125, 0.9995659589767456, 0.1026475727558136, 0.1786024272441864, 0.6888020634651184, 0.0646701380610466, 0.9993489384651184, 0.1519097238779068, 0.4377170205116272, 0.9995659589767456, 0.9505208134651184, 0.9995659589767456, 0.896484375, 0.067274309694767, 0.8817274570465088, 0.5006510615348816, 0.5483940839767456, 0.942491352558136, 0.1710069477558136, 0.1779513955116272, 0.9993489384651184, 0.2977430522441864, 0.2096354216337204, 0.9995659589767456, 0.6688368320465088, 0.1360677033662796, 0.134548619389534, 0.014973958022892475, 0.4249131977558136, 0.9995659589767456, 0.0740017369389534, 0.9995659589767456, 0.3342013955116272, 0.5562065839767456, 0.3253038227558136, 0.0381944440305233, 0.9997829794883728, 0.0859375, 0.497612863779068, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0668402761220932, 0.9995659589767456, 0.0520833320915699, 0.0651041641831398, 0.7779948115348816, 0.8997395634651184, 0.9993489384651184, 0.9995659589767456, 0.283203125, 0.0963541641831398, 0.0963541641831398, 0.9995659589767456, 0.1475694477558136, 0.2977430522441864, 0.1514756977558136, 0.063368059694767, 0.1119791641831398, 0.9192708134651184, 0.9995659589767456, 0.830078125, 0.8849826455116272, 0.1623263955116272, 0.9997829794883728, 0.288628488779068, 0.9995659589767456, 0.4544270932674408, 0.07421875, 0.0577256940305233, 0.6605902910232544, 0.0944010391831398, 0.9995659589767456, 0.1525607705116272, 0.021484375, 0.9995659589767456, 0.0646701380610466, 0.7951388955116272, 0.154079869389534, 0.9993489384651184, 0.8001301884651184, 0.0950520858168602, 0.0464409738779068, 0.0766059011220932, 0.9995659589767456, 0.9995659589767456, 0.6569010615348816, 0.942491352558136, 0.1291232705116272, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.1293402761220932, 0.9535590410232544, 0.0473090298473835, 0.2907986044883728, 0.8943142294883728, 0.0588107630610466, 0.9995659589767456, 0.9993489384651184, 0.2072482705116272, 0.9995659589767456, 0.0746527761220932, 0.130859375, 0.236328125, 0.6095920205116272, 0.9995659589767456, 0.9995659589767456, 0.1707899272441864, 0.6078559160232544, 0.1087239608168602, 0.9997829794883728, 0.23828125, 0.84375, 0.9997829794883728, 0.142361119389534, 0.999131977558136, 0.3005642294883728, 0.1234809011220932, 0.9314236044883728, 0.0618489570915699, 0.9993489384651184, 0.9997829794883728, 0.2384982705116272, 0.4691840410232544, 0.9995659589767456, 0.0501302070915699, 0.2884114682674408, 0.9995659589767456, 0.0837673619389534, 0.0811631977558136, 0.0464409738779068, 0.849609375, 0.9995659589767456, 0.9993489384651184, 0.9997829794883728, 0.096571184694767, 0.9995659589767456, 0.02951388992369175, 0.9995659589767456, 0.0271267369389534, 0.2660590410232544, 0.6794704794883728, 0.1959635466337204, 0.0590277798473835, 0.2855902910232544, 0.6870659589767456, 0.5930989384651184, 0.0785590261220932, 0.8582899570465088, 0.3153211772441864, 0.1430121511220932, 0.9995659589767456, 0.0438368059694767, 0.0974392369389534, 0.163845494389534, 0.0349392369389534, 0.0679253488779068, 0.9995659589767456, 0.9995659589767456, 0.0388454869389534, 0.0473090298473835, 0.8326823115348816, 0.0627170130610466, 0.9995659589767456, 0.245876744389534, 0.3111979067325592, 0.9993489384651184, 0.53125, 0.107421875, 0.9995659589767456, 0.171875, 0.9995659589767456, 0.4641927182674408, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.1297743022441864, 0.1890190988779068, 0.9997829794883728, 0.0379774309694767, 0.0603298619389534, 0.9858940839767456, 0.4694010317325592, 0.7094184160232544, 0.9993489384651184, 0.970703125, 0.1046006977558136, 0.9609375, 0.991319477558136, 0.2024739533662796, 0.8352864384651184, 0.1076388880610466, 0.4516059160232544, 0.3270399272441864, 0.9995659589767456, 0.0514322929084301, 0.4880642294883728, 0.0490451380610466, 0.6490885615348816, 0.7677951455116272, 0.7369791865348816, 0.1098090261220932, 0.23046875, 0.0516493059694767, 0.9518229365348816, 0.082899309694767, 0.9993489384651184, 0.9995659589767456, 0.02170138992369175, 0.0405815988779068, 0.9995659589767456, 0.046875, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.35546875, 0.1019965261220932, 0.1844618022441864, 0.0447048619389534, 0.0557725690305233, 0.963975727558136, 0.9995659589767456, 0.1032986119389534, 0.1013454869389534, 0.9819878339767456, 0.9997829794883728, 0.3337673544883728, 0.2265625, 0.4674479067325592, 0.7814670205116272, 0.10546875, 0.0772569477558136, 0.1091579869389534, 0.6848958134651184, 0.1158854141831398, 0.1354166716337204, 0.5677083134651184, 0.9995659589767456, 0.052734375, 0.9995659589767456, 0.9997829794883728, 0.9118923544883728, 0.9995659589767456, 0.0579427070915699, 0.503038227558136, 0.0546875, 0.9995659589767456, 0.0436197929084301, 0.9995659589767456, 0.02864583395421505, 0.1343315988779068, 0.5162760615348816, 0.9995659589767456, 0.9995659589767456, 0.037109375, 0.9995659589767456, 0.9993489384651184, 0.8146701455116272, 0.9995659589767456, 0.4236111044883728, 0.2641059160232544, 0.9357638955116272, 0.9995659589767456, 0.1974826455116272, 0.41015625, 0.0479600690305233, 0.460503488779068, 0.0496961809694767, 0.9997829794883728, 0.132595494389534, 0.971788227558136, 0.9995659589767456, 0.9995659589767456, 0.0501302070915699, 0.8318142294883728, 0.9997829794883728, 0.0362413190305233, 0.9995659589767456, 0.0974392369389534, 0.1338975727558136, 0.1258680522441864, 0.4739583432674408, 0.8884548544883728, 0.9995659589767456, 0.9995659589767456, 0.9806857705116272, 0.9967448115348816, 0.9997829794883728, 0.0928819477558136, 0.1046006977558136, 0.0774739608168602, 0.9997829794883728, 0.9995659589767456, 0.4173177182674408, 0.0366753488779068, 0.1825086772441864, 0.0833333358168602, 0.0473090298473835, 0.999131977558136, 0.0531684048473835, 0.6128472089767456, 0.4787326455116272, 0.5026041865348816, 0.0852864608168602, 0.9997829794883728, 0.8539496660232544, 0.9995659589767456, 0.0414496548473835, 0.9995659589767456, 0.9995659589767456, 0.978515625, 0.978515625, 0.954210102558136, 0.123046875, 0.476128488779068, 0.530381977558136, 0.0440538190305233, 0.9995659589767456, 0.9995659589767456, 0.8424479365348816, 0.5954861044883728, 0.9995659589767456, 0.0581597238779068, 0.478081613779068, 0.0355902798473835, 0.758897602558136, 0.991319477558136, 0.6688368320465088, 0.0438368059694767, 0.9995659589767456, 0.1545138955116272, 0.9995659589767456, 0.1252170205116272, 0.9993489384651184, 0.9995659589767456, 0.9806857705116272, 0.0499131940305233, 0.1013454869389534, 0.1028645858168602, 0.9997829794883728, 0.0776909738779068, 0.9997829794883728, 0.0911458358168602, 0.0614149309694767, 0.0846354141831398, 0.9995659589767456, 0.9995659589767456, 0.5729166865348816, 0.9997829794883728, 0.978515625, 0.8537326455116272, 0.0687934011220932, 0.9995659589767456, 0.983506977558136, 0.9995659589767456, 0.4906684160232544, 0.5397135615348816, 0.753038227558136, 0.0390625, 0.0501302070915699, 0.1032986119389534, 0.156032994389534, 0.094618059694767, 0.1399739533662796, 0.0570746548473835, 0.93359375, 0.9568142294883728, 0.2074652761220932, 0.9995659589767456, 0.9995659589767456, 0.9266493320465088, 0.9997829794883728, 0.8828125, 0.978515625, 0.9995659589767456, 0.197265625, 0.9841579794883728, 0.9995659589767456, 0.146484375, 0.9995659589767456, 0.1323784738779068, 0.6052517294883728, 0.9995659589767456, 0.1048177108168602, 0.7384982705116272, 0.583984375, 0.273003488779068, 0.111328125, 0.2938368022441864, 0.9993489384651184, 0.9997829794883728, 0.12109375, 0.9997829794883728, 0.8606770634651184, 0.1148003488779068, 0.9995659589767456, 0.9995659589767456, 0.3953993022441864, 0.878038227558136, 0.0470920130610466, 0.9995659589767456, 0.9995659589767456, 0.0362413190305233, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.0455729179084301, 0.2951388955116272, 0.1792534738779068, 0.9995659589767456, 0.029296875, 0.6686198115348816, 0.5262587070465088, 0.1117621511220932, 0.9995659589767456, 0.9995659589767456, 0.1792534738779068, 0.8684895634651184, 0.4496527910232544, 0.0379774309694767, 0.0659722238779068, 0.9372829794883728, 0.9995659589767456, 0.0466579869389534, 0.1221788227558136, 0.1213107630610466, 0.9995659589767456, 0.7927517294883728, 0.1032986119389534, 0.1733940988779068, 0.1976996511220932, 0.998046875, 0.2777777910232544, 0.1662326455116272, 0.0844184011220932, 0.9995659589767456, 0.0394965298473835, 0.1888020783662796, 0.9392361044883728, 0.1167534738779068, 0.0473090298473835, 0.963975727558136, 0.9993489384651184, 0.8129340410232544, 0.629991352558136, 0.2740885317325592, 0.533203125, 0.671006977558136, 0.9915364384651184, 0.8990885615348816, 0.5735676884651184, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.9997829794883728, 0.1030815988779068, 0.0421006940305233, 0.0598958320915699, 0.1569010466337204, 0.9782986044883728, 0.1391059011220932, 0.140407994389534, 0.0542534738779068, 0.9995659589767456, 0.9166666865348816, 0.9398871660232544, 0.9995659589767456, 0.9995659589767456, 0.8834635615348816, 0.0407986119389534, 0.8515625, 0.896484375, 0.0559895820915699, 0.0375434048473835, 0.598741352558136, 0.9995659589767456, 0.9995659589767456, 0.146267369389534, 0.983506977558136, 0.3849826455116272, 0.548828125, 0.9995659589767456, 0.9880642294883728, 0.9995659589767456, 0.2931857705116272, 0.767578125, 0.9563801884651184, 0.076171875, 0.8342013955116272, 0.5091145634651184, 0.1241319477558136, 0.6098090410232544, 0.833984375, 0.0961371511220932, 0.1840277761220932, 0.0523003488779068, 0.8153212070465088, 0.498046875, 0.1330295205116272, 0.8914930820465088, 0.094618059694767, 0.1085069477558136, 0.63671875, 0.9995659589767456, 0.9995659589767456, 0.1176215261220932]

 sparsity of   [0.14453125, 0.6953125, 0.6953125, 0.0234375, 0.068359375, 0.384765625, 0.138671875, 0.025390625, 0.037109375, 0.244140625, 0.078125, 0.833984375, 0.0234375, 0.1484375, 0.802734375, 0.06640625, 0.994140625, 0.01171875, 0.01171875, 0.05859375, 0.080078125, 0.671875, 0.8125, 0.0859375, 0.05078125, 0.349609375, 0.48828125, 0.7890625, 0.041015625, 0.900390625, 0.119140625, 0.04296875, 0.349609375, 0.05078125, 0.216796875, 0.841796875, 0.41015625, 0.001953125, 0.021484375, 0.208984375, 0.111328125, 0.35546875, 0.365234375, 0.01171875, 0.287109375, 0.048828125, 0.037109375, 0.11328125, 0.150390625, 0.203125, 0.7109375, 0.09375, 0.31640625, 0.01953125, 0.7265625, 0.044921875, 0.01171875, 0.779296875, 0.0, 0.890625, 0.70703125, 0.380859375, 0.099609375, 0.1171875, 0.998046875, 0.021484375, 0.021484375, 0.7890625, 0.13671875, 0.017578125, 0.740234375, 0.677734375, 0.140625, 0.916015625, 0.99609375, 0.03515625, 0.03125, 0.0390625, 0.02734375, 0.8125, 0.005859375, 0.0625, 0.017578125, 0.111328125, 0.837890625, 0.015625, 0.16015625, 0.029296875, 0.052734375, 0.01171875, 0.966796875, 0.345703125, 0.810546875, 0.373046875, 0.056640625, 0.341796875, 0.861328125, 0.966796875, 0.998046875, 0.794921875, 0.048828125, 0.08203125, 0.076171875, 0.017578125, 0.525390625, 0.064453125, 0.44140625, 0.02734375, 0.828125, 0.025390625, 0.966796875, 0.8359375, 0.998046875, 0.033203125, 0.54296875, 0.04296875, 0.01171875, 0.380859375, 0.966796875, 0.326171875, 0.033203125, 0.03125, 0.03125, 0.033203125, 0.03125, 0.396484375, 0.3984375, 0.041015625, 0.037109375, 0.142578125, 0.01953125, 0.689453125, 0.77734375, 0.015625, 0.935546875, 0.01953125, 0.263671875, 0.193359375, 0.904296875, 0.447265625, 0.005859375, 0.052734375, 0.021484375, 0.068359375, 0.046875, 0.384765625, 0.10546875, 0.01171875, 0.302734375, 0.005859375, 0.0078125, 0.0390625, 0.306640625, 0.0234375, 0.0234375, 0.0, 0.490234375, 0.005859375, 0.00390625, 0.689453125, 0.33984375, 0.05859375, 0.03515625, 0.384765625, 0.021484375, 0.0390625, 0.966796875, 0.02734375, 0.046875, 0.03125, 0.03515625, 0.365234375, 0.056640625, 0.005859375, 0.427734375, 0.029296875, 0.3203125, 0.009765625, 0.033203125, 0.060546875, 0.0234375, 0.015625, 0.0, 0.94140625, 0.13671875, 0.994140625, 0.060546875, 0.017578125, 0.3671875, 0.05078125, 0.07421875, 0.998046875, 0.005859375, 0.017578125, 0.35546875, 0.02734375, 0.33984375, 0.443359375, 0.99609375, 0.966796875, 0.029296875, 0.03125, 0.037109375, 0.068359375, 0.392578125, 0.998046875, 0.947265625, 0.05078125, 0.46875, 0.896484375, 0.01953125, 0.41015625, 0.083984375, 0.015625, 0.966796875, 0.01953125, 0.94140625, 0.1015625, 0.162109375, 0.375, 0.087890625, 0.400390625, 0.904296875, 0.234375, 0.791015625, 0.0078125, 0.43359375, 0.05859375, 0.029296875, 0.021484375, 0.884765625, 0.888671875, 0.03515625, 0.01953125, 0.416015625, 0.09765625, 0.392578125, 0.01171875, 0.060546875, 0.58984375, 0.01171875, 0.072265625, 0.865234375, 0.505859375, 0.185546875, 0.404296875, 0.4453125, 0.384765625, 0.08203125, 0.369140625, 0.875, 0.078125, 0.32421875, 0.943359375, 0.0, 0.3359375, 0.1171875, 0.091796875, 0.37890625, 0.875, 0.8828125, 0.001953125, 0.0390625, 0.966796875, 0.998046875, 0.8671875, 0.0859375, 0.005859375, 0.908203125, 0.0234375, 0.025390625, 0.025390625, 0.79296875, 0.013671875, 0.072265625, 0.01953125, 0.048828125, 0.310546875, 0.037109375, 0.107421875, 0.03515625, 0.025390625, 0.0, 0.048828125, 0.02734375, 0.052734375, 0.080078125, 0.142578125, 0.03515625, 0.09765625, 0.0625, 0.236328125, 0.498046875, 0.166015625, 0.107421875, 0.892578125, 0.033203125, 0.048828125, 0.0234375, 0.013671875, 0.373046875, 0.09375, 0.16796875, 0.02734375, 0.04296875, 0.1328125, 0.8359375, 0.025390625, 0.03515625, 0.0625, 0.033203125, 0.154296875, 0.736328125, 0.462890625, 0.15625, 0.947265625, 0.078125, 0.833984375, 0.0078125, 0.01171875, 0.013671875, 0.396484375, 0.642578125, 0.033203125, 0.427734375, 0.224609375, 0.875, 0.017578125, 0.056640625, 0.005859375, 0.11328125, 0.009765625, 0.015625, 0.708984375, 0.021484375, 0.037109375, 0.966796875, 0.99609375, 0.9765625, 0.486328125, 0.052734375, 0.091796875, 0.037109375, 0.966796875, 0.1015625, 0.109375, 0.859375, 0.013671875, 0.037109375, 0.044921875, 0.17578125, 0.068359375, 0.041015625, 0.376953125, 0.15234375, 0.119140625, 0.013671875, 0.99609375, 0.740234375, 0.201171875, 0.02734375, 0.0078125, 0.0234375, 0.017578125, 0.669921875, 0.078125, 0.921875, 0.08203125, 0.02734375, 0.935546875, 0.033203125, 0.02734375, 0.068359375, 0.025390625, 0.0859375, 0.765625, 0.013671875, 0.966796875, 0.666015625, 0.361328125, 0.109375, 0.029296875, 0.091796875, 0.99609375, 0.037109375, 0.021484375, 0.064453125, 0.953125, 0.1875, 0.81640625, 0.353515625, 0.033203125, 0.021484375, 0.953125, 0.087890625, 0.107421875, 0.830078125, 0.0234375, 0.009765625, 0.724609375, 0.03125, 0.826171875, 0.015625, 0.44140625, 0.853515625, 0.599609375, 0.64453125, 0.044921875, 0.875, 0.08984375, 0.009765625, 0.029296875, 0.107421875, 0.64453125, 0.927734375, 0.703125, 0.052734375, 0.05078125, 0.4609375, 0.02734375, 0.537109375, 0.041015625, 0.017578125, 0.03515625, 0.01171875, 0.966796875, 0.7421875, 0.013671875, 0.029296875, 0.15234375, 0.009765625, 0.58984375, 0.044921875, 0.017578125, 0.080078125, 0.0234375, 0.0390625, 0.0234375, 0.0703125, 0.10546875, 0.025390625, 0.013671875, 0.103515625, 0.966796875, 0.0, 0.861328125, 0.71875, 0.52734375, 0.0390625, 0.744140625, 0.572265625, 0.046875, 0.71484375, 0.99609375, 0.064453125, 0.0546875, 0.99609375, 0.35546875, 0.0078125, 0.03515625, 0.41796875, 0.892578125, 0.06640625, 0.197265625, 0.1953125, 0.0234375, 0.044921875, 0.31640625, 0.96875, 0.05859375, 0.419921875, 0.0859375, 0.8984375, 0.908203125, 0.00390625, 0.494140625, 0.076171875, 0.623046875, 0.123046875, 0.02734375, 0.0, 0.966796875, 0.076171875, 0.05859375, 0.017578125, 0.876953125, 0.015625, 0.623046875, 0.001953125, 0.017578125, 0.3828125, 0.041015625, 0.7890625, 0.064453125, 0.0078125, 0.087890625, 0.0546875, 0.994140625, 0.03515625, 0.0703125, 0.021484375, 0.107421875, 0.05859375, 0.005859375, 0.392578125, 0.904296875, 0.052734375, 0.05859375, 0.05078125, 0.037109375, 0.072265625, 0.01953125, 0.041015625, 0.29296875, 0.0390625, 0.62109375, 0.419921875, 0.67578125, 0.052734375, 0.03125, 0.318359375, 0.009765625, 0.021484375, 0.994140625, 0.54296875, 0.0625, 0.140625, 0.03125, 0.03515625, 0.0703125, 0.013671875, 0.05078125, 0.017578125, 0.994140625, 0.048828125, 0.322265625, 0.0234375, 0.0234375, 0.7578125, 0.1015625, 0.033203125, 0.0390625, 0.03515625, 0.7734375, 0.01171875, 0.361328125, 0.015625, 0.044921875, 0.056640625, 0.470703125, 0.814453125, 0.771484375, 0.994140625, 0.01953125, 0.015625, 0.005859375, 0.029296875, 0.71484375, 0.38671875, 0.693359375, 0.033203125, 0.134765625, 0.064453125, 0.046875, 0.064453125, 0.455078125, 0.12890625, 0.767578125, 0.05859375, 0.359375, 0.0078125, 0.904296875, 0.064453125, 0.515625, 0.44921875, 0.171875, 0.017578125, 0.109375, 0.0390625, 0.0234375, 0.865234375, 0.08203125, 0.349609375, 0.0234375, 0.03125, 0.2421875, 0.349609375, 0.013671875, 0.009765625, 0.966796875, 0.345703125, 0.0859375, 0.01171875, 0.568359375, 0.046875, 0.087890625, 0.015625, 0.8515625, 0.802734375, 0.044921875, 0.865234375, 0.654296875, 0.0546875, 0.341796875, 0.041015625, 0.037109375, 0.365234375, 0.869140625, 0.22265625, 0.044921875, 0.966796875, 0.8125, 0.015625, 0.046875, 0.03515625, 0.34375, 0.634765625, 0.861328125, 0.099609375, 0.90625, 0.064453125, 0.41796875, 0.015625, 0.69140625, 0.0, 0.08984375, 0.009765625, 0.376953125, 0.39453125, 0.361328125, 0.044921875, 0.03515625, 0.029296875, 0.021484375, 0.70703125, 0.052734375, 0.361328125, 0.90234375, 0.02734375, 0.998046875, 0.005859375, 0.533203125, 0.013671875, 0.486328125, 0.23828125, 0.0546875, 0.0078125, 0.07421875, 0.02734375, 0.00390625, 0.03515625, 0.009765625, 0.013671875, 0.017578125, 0.02734375, 0.8125, 0.314453125, 0.048828125, 0.029296875, 0.88671875, 0.416015625, 0.068359375, 0.078125, 0.033203125, 0.904296875, 0.009765625, 0.060546875, 0.060546875, 0.41796875, 0.013671875, 0.03515625, 0.916015625, 0.03125, 0.009765625, 0.02734375, 0.90625, 0.017578125, 0.828125, 0.060546875, 0.966796875, 0.03515625, 0.138671875, 0.02734375, 0.025390625, 0.966796875, 0.966796875, 0.89453125, 0.046875, 0.041015625, 0.015625, 0.708984375, 0.9296875, 0.927734375, 0.021484375, 0.037109375, 0.02734375, 0.890625, 0.01953125, 0.4375, 0.85546875, 0.1015625, 0.345703125, 0.046875, 0.00390625, 0.025390625, 0.36328125, 0.3203125, 0.029296875, 0.7734375, 0.09765625, 0.072265625, 0.01171875, 0.138671875, 0.05078125, 0.037109375, 0.32421875, 0.037109375, 0.37890625, 0.02734375, 0.033203125, 0.853515625, 0.3359375, 0.015625, 0.146484375, 0.814453125, 0.021484375, 0.029296875, 0.12109375, 0.78515625, 0.7734375, 0.107421875, 0.34375, 0.05078125, 0.9296875, 0.388671875, 0.1484375, 0.583984375, 0.03125, 0.65234375, 0.025390625, 0.03125, 0.015625, 0.015625, 0.375, 0.3671875, 0.025390625, 0.14453125, 0.044921875, 0.212890625, 0.0078125, 0.017578125, 0.01953125, 0.048828125, 0.0234375, 0.1484375, 0.892578125, 0.904296875, 0.0, 0.087890625, 0.19140625, 0.673828125, 0.056640625, 0.890625, 0.01953125, 0.00390625, 0.021484375, 0.966796875, 0.876953125, 0.001953125, 0.02734375, 0.091796875, 0.169921875, 0.3203125, 0.31640625, 0.966796875, 0.02734375, 0.65234375, 0.439453125, 0.09375, 0.0, 0.392578125, 0.580078125, 0.79296875, 0.6796875, 0.01953125, 0.626953125, 0.966796875, 0.9375, 0.8515625, 0.005859375, 0.09765625, 0.0234375, 0.1015625, 0.021484375, 0.017578125, 0.037109375, 0.021484375, 0.01953125, 0.626953125, 0.79296875, 0.068359375, 0.01953125, 0.705078125, 0.36328125, 0.99609375, 0.798828125, 0.6640625, 0.02734375, 0.037109375, 0.0546875, 0.375, 0.015625, 0.02734375, 0.41015625, 0.994140625, 0.8671875, 0.857421875, 0.06640625, 0.072265625, 0.06640625, 0.03515625, 0.99609375, 0.966796875, 0.025390625, 0.08203125, 0.013671875, 0.021484375, 0.4140625, 0.0703125, 0.99609375, 0.076171875, 0.8046875, 0.021484375, 0.048828125, 0.96484375, 0.240234375, 0.013671875, 0.05859375, 0.0078125, 0.01953125, 0.837890625, 0.048828125, 0.017578125, 0.998046875, 0.041015625, 0.900390625, 0.076171875, 0.03515625, 0.16796875, 0.849609375, 0.013671875, 0.669921875, 0.099609375, 0.064453125, 0.99609375, 0.02734375, 0.2890625, 0.615234375, 0.01953125, 0.951171875, 0.337890625, 0.060546875, 0.037109375, 0.033203125, 0.04296875, 0.87890625, 0.27734375, 0.017578125, 0.853515625, 0.0390625, 0.751953125, 0.0546875, 0.3671875, 0.017578125, 0.533203125, 0.025390625, 0.046875, 0.966796875, 0.0234375, 0.01953125, 0.341796875, 0.029296875, 0.994140625, 0.0703125, 0.009765625, 0.966796875, 0.0234375, 0.787109375, 0.12109375, 0.5, 0.115234375, 0.017578125, 0.46484375, 0.0546875, 0.14453125, 0.046875, 0.134765625, 0.0, 0.015625, 0.150390625, 0.40625, 0.419921875, 0.0078125, 0.841796875, 0.333984375, 0.060546875, 0.935546875, 0.72265625, 0.16015625, 0.396484375, 0.0625, 0.125, 0.033203125, 0.2265625, 0.833984375, 0.76953125, 0.01171875, 0.99609375, 0.716796875, 0.361328125, 0.021484375, 0.021484375, 0.40625, 0.009765625, 0.0390625, 0.05078125, 0.931640625, 0.7578125, 0.724609375, 0.0, 0.05859375, 0.060546875, 0.966796875, 0.115234375, 0.166015625, 0.017578125, 0.0390625, 0.021484375, 0.25, 0.103515625, 0.41015625, 0.052734375, 0.017578125, 0.025390625, 0.787109375, 0.892578125, 0.966796875, 0.89453125, 0.966796875, 0.068359375, 0.0390625, 0.048828125, 0.052734375, 0.84375, 0.13671875, 0.82421875, 0.81640625, 0.078125, 0.083984375, 0.072265625, 0.05859375, 0.36328125, 0.0234375, 0.154296875, 0.9609375, 0.0234375, 0.35546875, 0.017578125, 0.908203125, 0.017578125, 0.02734375, 0.16015625, 0.025390625, 0.03515625, 0.392578125, 0.40234375, 0.296875, 0.873046875, 0.064453125, 0.921875, 0.03125, 0.41796875, 0.966796875, 0.99609375, 0.021484375, 0.017578125, 0.03125, 0.05859375, 0.009765625, 0.015625, 0.890625, 0.83984375, 0.048828125, 0.1015625, 0.01953125, 0.58203125, 0.025390625, 0.39453125, 0.634765625, 0.966796875, 0.029296875, 0.009765625, 0.025390625, 0.080078125, 0.13671875, 0.048828125, 0.103515625, 0.8203125, 0.037109375, 0.037109375, 0.0625, 0.71875, 0.03515625, 0.966796875, 0.037109375, 0.455078125, 0.87109375, 0.048828125, 0.009765625, 0.921875, 0.763671875, 0.1015625, 0.884765625, 0.017578125, 0.89453125, 0.01171875, 0.048828125, 0.037109375, 0.015625, 0.421875, 0.029296875, 0.216796875, 0.095703125, 0.017578125, 0.017578125, 0.427734375, 0.701171875, 0.01953125, 0.84765625, 0.017578125, 0.45703125, 0.142578125, 0.53125, 0.966796875, 0.01171875, 0.669921875, 0.017578125, 0.40234375, 0.080078125, 0.041015625, 0.01953125, 0.07421875, 0.015625, 0.037109375, 0.009765625, 0.796875, 0.623046875, 0.033203125, 0.158203125, 0.6796875, 0.76171875, 0.4765625, 0.01953125, 0.033203125, 0.3828125, 0.216796875, 0.0234375, 0.171875, 0.01953125, 0.0, 0.009765625, 0.90625, 0.001953125, 0.35546875, 0.025390625, 0.38671875, 0.390625, 0.029296875, 0.130859375, 0.966796875, 0.00390625, 0.005859375, 0.052734375, 0.189453125, 0.37109375, 0.083984375, 0.03515625, 0.466796875, 0.26953125, 0.205078125, 0.384765625, 0.029296875, 0.349609375, 0.091796875, 0.08984375, 0.888671875, 0.046875, 0.384765625, 0.03515625, 0.0859375, 0.91796875, 0.83203125, 0.8984375, 0.083984375, 0.03515625, 0.423828125, 0.01953125, 0.033203125, 0.03125, 0.056640625, 0.837890625, 0.751953125, 0.14453125, 0.7109375, 0.0234375, 0.416015625, 0.041015625, 0.03125, 0.337890625, 0.998046875, 0.916015625, 0.080078125, 0.36328125, 0.0625, 0.455078125, 0.796875, 0.1015625, 0.01171875, 0.1796875, 0.009765625, 0.03125, 0.99609375, 0.431640625, 0.0234375, 0.0, 0.388671875, 0.193359375, 0.02734375, 0.40625, 0.029296875, 0.966796875, 0.966796875, 0.017578125, 0.056640625, 0.1171875, 0.015625, 0.6875, 0.865234375, 0.998046875, 0.09765625, 0.009765625, 0.99609375, 0.0546875, 0.017578125, 0.623046875, 0.03125, 0.033203125, 0.7578125, 0.078125, 0.0078125, 0.921875, 0.029296875, 0.033203125, 0.005859375, 0.0078125, 0.064453125, 0.033203125, 0.03125, 0.03515625, 0.04296875, 0.994140625, 0.033203125, 0.83984375, 0.87109375, 0.0234375, 0.7890625, 0.037109375, 0.041015625, 0.423828125, 0.38671875, 0.39453125, 0.49609375, 0.908203125, 0.01953125, 0.84765625, 0.001953125, 0.083984375, 0.142578125, 0.646484375, 0.01953125, 0.0859375, 0.787109375, 0.029296875, 0.08203125, 0.890625, 0.947265625, 0.396484375, 0.037109375, 0.021484375, 0.390625, 0.900390625, 0.005859375, 0.046875, 0.029296875, 0.140625, 0.04296875, 0.369140625, 0.01953125, 0.013671875, 0.251953125, 0.013671875, 0.533203125, 0.037109375, 0.0, 0.0390625, 0.0234375, 0.86328125, 0.421875, 0.876953125, 0.296875, 0.3515625, 0.06640625, 0.826171875, 0.44140625, 0.296875, 0.029296875, 0.0859375, 0.00390625, 0.103515625, 0.078125, 0.90625, 0.0234375, 0.083984375, 0.025390625, 0.04296875, 0.033203125, 0.052734375, 0.67578125, 0.03125, 0.005859375, 0.708984375, 0.80859375, 0.0078125, 0.951171875, 0.927734375, 0.025390625, 0.001953125, 0.48828125, 0.01171875, 0.130859375, 0.9140625, 0.03515625, 0.794921875, 0.025390625, 0.568359375, 0.99609375, 0.025390625, 0.18359375, 0.048828125, 0.037109375, 0.052734375, 0.01953125, 0.0234375, 0.814453125, 0.966796875, 0.99609375, 0.01953125, 0.37890625, 0.1328125, 0.041015625, 0.05859375, 0.083984375, 0.03515625, 0.025390625, 0.0703125, 0.078125, 0.037109375, 0.01953125, 0.896484375, 0.05078125, 0.9375, 0.072265625, 0.056640625, 0.03125, 0.71875, 0.068359375, 0.009765625, 0.052734375, 0.99609375, 0.333984375, 0.021484375, 0.9375, 0.01171875, 0.04296875, 0.380859375, 0.728515625, 0.916015625, 0.322265625, 0.9140625, 0.439453125, 0.966796875, 0.806640625, 0.04296875, 0.333984375, 0.951171875, 0.01953125, 0.677734375, 0.99609375, 0.091796875, 0.0078125, 0.033203125, 0.0390625, 0.517578125, 0.041015625, 0.724609375, 0.0234375, 0.431640625, 0.01953125, 0.8359375, 0.263671875, 0.44140625, 0.015625, 0.02734375, 0.029296875, 0.02734375, 0.998046875, 0.048828125, 0.4765625, 0.080078125, 0.04296875, 0.044921875, 0.080078125, 0.994140625, 0.044921875, 0.73828125, 0.369140625, 0.169921875, 0.041015625, 0.01953125, 0.390625, 0.142578125, 0.08984375, 0.01953125, 0.984375, 0.646484375, 0.064453125, 0.322265625, 0.525390625, 0.12109375, 0.763671875, 0.69140625, 0.21484375, 0.541015625, 0.060546875, 0.162109375, 0.025390625, 0.017578125, 0.912109375, 0.287109375, 0.03125, 0.0234375, 0.0, 0.021484375, 0.109375, 0.07421875, 0.052734375, 0.021484375, 0.390625, 0.224609375, 0.865234375, 0.1171875, 0.849609375, 0.017578125, 0.130859375, 0.802734375, 0.013671875, 0.853515625, 0.0078125, 0.521484375, 0.072265625, 0.01953125, 0.0078125, 0.025390625, 0.033203125, 0.01171875, 0.03125, 0.84375, 0.091796875, 0.77734375, 0.033203125, 0.04296875, 0.03125, 0.10546875, 0.060546875, 0.40625, 0.0234375, 0.84375, 0.056640625, 0.14453125, 0.38671875, 0.033203125, 0.025390625, 0.05078125, 0.0234375, 0.919921875, 0.0546875, 0.9140625, 0.037109375, 0.365234375, 0.05859375, 0.58984375, 0.912109375, 0.03515625, 0.943359375, 0.046875, 0.0078125, 0.939453125, 0.06640625, 0.01171875, 0.552734375, 0.0390625, 0.046875, 0.455078125, 0.033203125, 0.015625, 0.01171875, 0.42578125, 0.00390625, 0.41015625, 0.966796875, 0.173828125, 0.994140625, 0.78515625, 0.919921875, 0.814453125, 0.0625, 0.029296875, 0.759765625, 0.00390625, 0.025390625, 0.083984375, 0.798828125, 0.150390625, 0.048828125, 0.95703125, 0.01953125, 0.005859375, 0.40234375, 0.009765625, 0.033203125, 0.056640625, 0.384765625, 0.05078125, 0.05859375, 0.05078125, 0.025390625, 0.01171875, 0.45703125, 0.052734375, 0.892578125, 0.01171875, 0.04296875, 0.025390625, 0.029296875, 0.45703125, 0.962890625, 0.966796875, 0.017578125, 0.021484375, 0.03125, 0.07421875, 0.041015625, 0.013671875, 0.421875, 0.02734375, 0.0234375, 0.029296875, 0.2265625, 0.73828125, 0.0546875, 0.037109375, 0.0546875, 0.90234375, 0.033203125, 0.013671875, 0.318359375, 0.001953125, 0.173828125, 0.068359375, 0.009765625, 0.017578125, 0.072265625, 0.67578125, 0.994140625, 0.013671875, 0.041015625, 0.900390625, 0.017578125, 0.044921875, 0.150390625, 0.8671875, 0.302734375, 0.0234375, 0.041015625, 0.857421875, 0.41796875, 0.0, 0.00390625, 0.021484375, 0.04296875, 0.009765625, 0.029296875, 0.951171875, 0.919921875, 0.384765625, 0.01171875, 0.025390625, 0.04296875, 0.033203125, 0.814453125, 0.12890625, 0.0234375, 0.158203125, 0.86328125, 0.103515625, 0.77734375, 0.060546875, 0.095703125, 0.03125, 0.048828125, 0.966796875, 0.0390625, 0.0078125, 0.013671875, 0.896484375, 0.015625, 0.029296875, 0.064453125, 0.5390625, 0.0390625, 0.10546875, 0.015625, 0.03515625, 0.013671875, 0.01953125, 0.064453125, 0.236328125, 0.392578125, 0.58203125, 0.798828125, 0.966796875, 0.96484375, 0.03125, 0.359375, 0.400390625, 0.296875, 0.013671875, 0.794921875, 0.75, 0.833984375, 0.025390625, 0.017578125, 0.99609375, 0.0, 0.349609375, 0.033203125, 0.521484375, 0.037109375, 0.802734375, 0.10546875, 0.0546875, 0.017578125, 0.052734375, 0.037109375, 0.0390625, 0.88671875, 0.8125, 0.95703125, 0.365234375, 0.0390625, 0.154296875, 0.015625, 0.060546875, 0.044921875, 0.384765625, 0.91015625, 0.025390625, 0.4140625, 0.029296875, 0.03515625, 0.125, 0.818359375, 0.75390625, 0.029296875, 0.951171875, 0.912109375, 0.033203125, 0.7421875, 0.033203125, 0.099609375, 0.025390625, 0.771484375, 0.046875, 0.375, 0.005859375, 0.998046875, 0.013671875, 0.068359375, 0.91015625, 0.064453125, 0.86328125, 0.048828125, 0.01171875, 0.775390625, 0.037109375, 0.375, 0.05078125, 0.060546875, 0.046875, 0.01171875, 0.0234375, 0.0234375, 0.005859375, 0.80859375, 0.34375, 0.99609375, 0.02734375, 0.9296875, 0.01953125, 0.078125, 0.0078125, 0.029296875, 0.091796875, 0.0, 0.966796875, 0.888671875, 0.009765625, 0.408203125, 0.009765625, 0.751953125, 0.029296875, 0.072265625, 0.966796875, 0.998046875, 0.3984375, 0.01953125, 0.908203125, 0.06640625, 0.01171875, 0.884765625, 0.052734375, 0.6171875, 0.875, 0.142578125, 0.115234375, 0.1875, 0.044921875, 0.013671875, 0.052734375, 0.041015625, 0.017578125, 0.021484375, 0.0390625, 0.103515625, 0.052734375, 0.837890625, 0.07421875, 0.9453125, 0.017578125, 0.02734375, 0.5078125, 0.0234375, 0.751953125, 0.0625, 0.666015625, 0.0234375, 0.931640625, 0.103515625, 0.123046875, 0.111328125, 0.966796875, 0.36328125, 0.0, 0.044921875, 0.02734375, 0.029296875, 0.048828125, 0.01171875, 0.966796875, 0.0546875, 0.181640625, 0.04296875, 0.00390625, 0.826171875, 0.0859375, 0.849609375, 0.384765625, 0.046875, 0.0234375, 0.2421875, 0.349609375, 0.078125, 0.01171875, 0.0625, 0.12109375, 0.0078125, 0.638671875, 0.009765625, 0.73046875, 0.0, 0.025390625, 0.99609375, 0.845703125, 0.798828125, 0.904296875, 0.099609375, 0.03125, 0.009765625, 0.021484375, 0.939453125, 0.109375, 0.0703125, 0.017578125, 0.189453125, 0.197265625, 0.763671875, 0.048828125, 0.857421875, 0.34375, 0.82421875, 0.00390625, 0.994140625, 0.1015625, 0.00390625, 0.046875, 0.99609375, 0.029296875, 0.140625, 0.388671875, 0.8984375, 0.03125, 0.697265625, 0.01171875, 0.0703125, 0.037109375, 0.046875, 0.029296875, 0.1015625, 0.037109375, 0.69140625, 0.048828125, 0.0390625, 0.037109375, 0.046875, 0.072265625, 0.041015625, 0.86328125, 0.03515625, 0.013671875, 0.0, 0.01953125, 0.015625, 0.2109375, 0.03125, 0.0546875, 0.029296875, 0.2109375, 0.115234375, 0.998046875, 0.78125, 0.064453125, 0.341796875, 0.046875, 0.009765625, 0.01171875, 0.015625, 0.029296875, 0.22265625, 0.12890625, 0.021484375, 0.966796875, 0.041015625, 0.029296875, 0.685546875, 0.140625, 0.966796875, 0.068359375, 0.40234375, 0.037109375, 0.8046875, 0.6015625, 0.796875, 0.064453125, 0.177734375, 0.96484375, 0.392578125, 0.05859375, 0.0078125, 0.06640625, 0.998046875, 0.966796875, 0.99609375, 0.041015625, 0.0625, 0.361328125, 0.4921875, 0.060546875, 0.4140625, 0.0625, 0.03125, 0.46875, 0.99609375, 0.0078125, 0.396484375, 0.966796875, 0.99609375, 0.05859375, 0.087890625, 0.359375, 0.021484375, 0.09765625, 0.048828125, 0.361328125, 0.0546875, 0.783203125, 0.603515625, 0.744140625, 0.04296875, 0.736328125, 0.03125, 0.96484375, 0.029296875, 0.04296875, 0.86328125, 0.86328125, 0.033203125, 0.009765625, 0.013671875, 0.24609375, 0.017578125, 0.046875, 0.78125, 0.99609375, 0.033203125, 0.01953125, 0.05859375, 0.869140625, 0.01171875, 0.32421875, 0.0234375, 0.05859375, 0.009765625, 0.67578125, 0.009765625, 0.40625, 0.05078125, 0.677734375, 0.087890625, 0.109375, 0.021484375, 0.05078125, 0.099609375, 0.81640625, 0.048828125, 0.05859375, 0.0625, 0.021484375, 0.958984375, 0.138671875, 0.021484375, 0.03515625, 0.02734375, 0.09375, 0.068359375, 0.001953125, 0.04296875, 0.8125, 0.021484375, 0.009765625, 0.41796875, 0.900390625, 0.333984375, 0.029296875, 0.99609375, 0.02734375, 0.42578125, 0.40625, 0.943359375, 0.056640625, 0.25, 0.466796875, 0.140625, 0.115234375, 0.234375, 0.890625, 0.046875, 0.134765625, 0.671875, 0.875, 0.044921875, 0.701171875, 0.00390625, 0.8984375, 0.291015625, 0.0625, 0.001953125, 0.99609375, 0.02734375, 0.00390625, 0.263671875, 0.1640625, 0.048828125, 0.0, 0.619140625, 0.0078125, 0.0078125, 0.107421875, 0.13671875, 0.033203125, 0.83203125, 0.9140625, 0.88671875, 0.078125, 0.966796875, 0.939453125, 0.111328125, 0.966796875, 0.076171875, 0.01953125, 0.083984375, 0.009765625, 0.01953125, 0.01953125, 0.095703125, 0.0, 0.037109375, 0.931640625, 0.2265625, 0.072265625, 0.01953125, 0.833984375, 0.966796875, 0.8671875, 0.01953125, 0.3984375, 0.2421875, 0.90234375, 0.466796875, 0.9296875, 0.005859375, 0.5234375, 0.78515625, 0.04296875, 0.033203125, 0.01171875, 0.033203125, 0.111328125, 0.048828125, 0.673828125, 0.4609375, 0.044921875, 0.92578125, 0.015625, 0.13671875, 0.185546875, 0.263671875, 0.02734375, 0.03125, 0.048828125, 0.48828125, 0.89453125, 0.115234375, 0.8671875, 0.064453125, 0.7890625, 0.05859375, 0.005859375, 0.02734375, 0.67578125, 0.775390625, 0.107421875, 0.03125, 0.90234375, 0.044921875, 0.130859375, 0.037109375, 0.015625, 0.6171875, 0.357421875, 0.728515625, 0.0390625, 0.017578125, 0.845703125, 0.35546875, 0.06640625, 0.349609375, 0.0234375, 0.642578125, 0.072265625, 0.041015625, 0.00390625, 0.921875, 0.119140625, 0.990234375, 0.80078125, 0.916015625, 0.01953125, 0.205078125, 0.806640625, 0.02734375, 0.0546875, 0.060546875, 0.84765625, 0.021484375, 0.994140625, 0.017578125, 0.021484375, 0.09375, 0.109375, 0.01953125, 0.0234375, 0.005859375, 0.08203125, 0.009765625, 0.86328125, 0.0390625, 0.0390625, 0.125, 0.02734375, 0.10546875, 0.021484375, 0.0546875, 0.033203125, 0.11328125, 0.025390625, 0.056640625, 0.064453125, 0.76953125, 0.01171875, 0.158203125, 0.8125, 0.939453125, 0.0859375, 0.0390625, 0.90234375, 0.013671875, 0.0234375, 0.85546875]

 sparsity of   [0.501953125, 0.0048828125, 0.0048828125, 0.01953125, 0.03125, 0.087890625, 0.0029296875, 0.208984375, 0.0048828125, 0.75, 0.0234375, 0.033203125, 0.9091796875, 0.02734375, 0.0185546875, 0.0263671875, 0.150390625, 0.0087890625, 0.984375, 0.087890625, 0.0830078125, 0.181640625, 0.1015625, 0.1298828125, 0.044921875, 0.0029296875, 0.001953125, 0.0830078125, 0.017578125, 0.001953125, 0.009765625, 0.0068359375, 0.0693359375, 0.021484375, 0.0087890625, 0.3173828125, 0.8271484375, 0.7880859375, 0.958984375, 0.0, 0.0068359375, 0.0078125, 0.01171875, 0.0126953125, 0.1494140625, 0.001953125, 0.0, 0.1015625, 0.0810546875, 0.001953125, 0.001953125, 0.041015625, 0.0908203125, 0.0302734375, 0.0068359375, 0.9560546875, 0.7314453125, 0.7626953125, 0.005859375, 0.01953125, 0.00390625, 0.083984375, 0.05859375, 0.001953125, 0.0263671875, 0.0615234375, 0.0263671875, 0.005859375, 0.08203125, 0.8896484375, 0.021484375, 0.1025390625, 0.0205078125, 0.6826171875, 0.0517578125, 0.5205078125, 0.1298828125, 0.0322265625, 0.09375, 0.0908203125, 0.0390625, 0.111328125, 0.0771484375, 0.033203125, 0.0556640625, 0.0576171875, 0.783203125, 0.01953125, 0.2216796875, 0.998046875, 0.0546875, 0.0029296875, 0.0166015625, 0.0380859375, 0.8994140625, 0.017578125, 0.009765625, 0.025390625, 0.3310546875, 0.00390625, 0.087890625, 0.013671875, 0.11328125, 0.8046875, 0.0205078125, 0.18359375, 0.015625, 0.0166015625, 0.001953125, 0.0263671875, 0.0, 0.056640625, 0.044921875, 0.1328125, 0.015625, 0.0126953125, 0.091796875, 0.1005859375, 0.0, 0.0029296875, 0.015625, 0.041015625, 0.0625, 0.1259765625, 0.09765625, 0.0771484375, 0.015625, 0.099609375, 0.05859375, 0.0546875, 0.9736328125, 0.013671875, 0.1005859375, 0.76171875, 0.3671875, 0.0263671875, 0.08203125, 0.099609375, 0.548828125, 0.0078125, 0.001953125, 0.1240234375, 0.759765625, 0.03125, 0.0166015625, 0.0009765625, 0.025390625, 0.0986328125, 0.0068359375, 0.140625, 0.0126953125, 0.005859375, 0.0048828125, 0.998046875, 0.8671875, 0.0, 0.0234375, 0.9677734375, 0.021484375, 0.1396484375, 0.001953125, 0.0244140625, 0.046875, 0.0009765625, 0.9970703125, 0.9970703125, 0.0, 0.64453125, 0.0068359375, 0.9990234375, 0.1044921875, 0.0048828125, 0.08203125, 0.0087890625, 0.04296875, 0.3251953125, 0.033203125, 0.0390625, 0.021484375, 0.15625, 0.037109375, 0.0859375, 0.0263671875, 0.16796875, 0.0146484375, 0.015625, 0.1484375, 0.298828125, 0.0712890625, 0.0498046875, 0.0419921875, 0.078125, 0.1142578125, 0.9990234375, 0.015625, 0.0009765625, 0.00390625, 0.0703125, 0.0146484375, 0.0087890625, 0.005859375, 0.0068359375, 0.6884765625, 0.0087890625, 0.1025390625, 0.1171875, 0.908203125, 0.046875, 0.0029296875, 0.00390625, 0.75, 0.04296875, 0.0068359375, 0.09375, 0.01171875, 0.015625, 0.056640625, 0.0498046875, 0.1005859375, 0.015625, 0.1162109375, 0.0224609375, 0.001953125, 0.009765625, 0.271484375, 0.0283203125, 0.029296875, 0.033203125, 0.1513671875, 0.029296875, 0.3134765625, 0.6875, 0.91015625, 0.0283203125, 0.001953125, 0.0185546875, 0.0244140625, 0.041015625, 0.0693359375, 0.0048828125, 0.1474609375, 0.1162109375, 0.005859375, 0.0087890625, 0.052734375, 0.005859375, 0.248046875, 0.044921875, 0.9970703125, 0.0419921875, 0.021484375, 0.998046875, 0.0048828125, 0.306640625, 0.0, 0.0849609375, 0.1796875, 0.0283203125, 0.0087890625, 0.0068359375, 0.58203125, 0.8955078125, 0.0029296875, 0.0498046875, 0.2158203125, 0.0498046875, 0.861328125, 0.041015625, 0.109375, 0.0537109375, 0.01953125, 0.9521484375, 0.01953125, 0.1220703125, 0.0478515625, 0.90234375, 0.1142578125, 0.00390625, 0.9990234375, 0.037109375, 0.1640625, 0.076171875, 0.921875, 0.0166015625, 0.0703125, 0.2998046875, 0.10546875, 0.068359375, 0.005859375, 0.0, 0.2109375, 0.013671875, 0.0888671875, 0.0732421875, 0.81640625, 0.0380859375, 0.0322265625, 0.2431640625, 0.0849609375, 0.90234375, 0.0146484375, 0.03515625, 0.1357421875, 0.9189453125, 0.068359375, 0.0576171875, 0.005859375, 0.0322265625, 0.369140625, 0.08984375, 0.1083984375, 0.255859375, 0.095703125, 0.111328125, 0.083984375, 0.046875, 0.2802734375, 0.171875, 0.7861328125, 0.2001953125, 0.0478515625, 0.0029296875, 0.0, 0.0673828125, 0.1025390625, 0.0302734375, 0.0126953125, 0.8896484375, 0.9091796875, 0.041015625, 0.9970703125, 0.0166015625, 0.005859375, 0.0029296875, 0.0888671875, 0.05859375, 0.0, 0.02734375, 0.013671875, 0.0634765625, 0.044921875, 0.0341796875, 0.263671875, 0.001953125, 0.287109375, 0.0078125, 0.951171875, 0.0234375, 0.1083984375, 0.806640625, 0.8056640625, 0.0, 0.0361328125, 0.044921875, 0.9970703125, 0.0419921875, 0.017578125, 0.064453125, 0.115234375, 0.021484375, 0.212890625, 0.015625, 0.8154296875, 0.0302734375, 0.18359375, 0.9990234375, 0.0927734375, 0.1318359375, 0.0166015625, 0.0244140625, 0.0517578125, 0.02734375, 0.5771484375, 0.0390625, 0.8701171875, 0.00390625, 0.115234375, 0.0595703125, 0.005859375, 0.0390625, 0.2109375, 0.056640625, 0.0439453125, 0.1396484375, 0.9970703125, 0.1513671875, 0.0830078125, 0.796875, 0.865234375, 0.001953125, 0.0615234375, 0.0205078125, 0.0087890625, 0.017578125, 0.9990234375, 0.998046875, 0.1025390625, 0.298828125, 0.015625, 0.015625, 0.1923828125, 0.017578125, 0.1826171875, 0.015625, 0.01171875, 0.03125, 0.1376953125, 0.1533203125, 0.0166015625, 0.0341796875, 0.9072265625, 0.0302734375, 0.091796875, 0.087890625, 0.015625, 0.0087890625, 0.1845703125, 0.0302734375, 0.001953125, 0.0322265625, 0.9970703125, 0.5546875, 0.2978515625, 0.9970703125, 0.0302734375, 0.0, 0.1220703125, 0.9990234375, 0.0419921875, 0.0322265625, 0.041015625, 0.10546875, 0.048828125, 0.841796875, 0.0078125, 0.3525390625, 0.056640625, 0.037109375, 0.076171875, 0.03515625, 0.0439453125, 0.1484375, 0.310546875, 0.0, 0.0, 0.0830078125, 0.0419921875, 0.001953125, 0.0, 0.09375, 0.015625, 0.033203125, 0.0546875, 0.0322265625, 0.9052734375, 0.0068359375, 0.044921875, 0.310546875, 0.009765625, 0.7763671875, 0.0029296875, 0.009765625, 0.0205078125, 0.115234375, 0.271484375, 0.228515625, 0.0888671875, 0.1025390625, 0.0478515625, 0.0185546875, 0.021484375, 0.0595703125, 0.0205078125, 0.9521484375, 0.0185546875, 0.103515625, 0.1533203125, 0.080078125, 0.00390625, 0.04296875, 0.11328125, 0.0009765625, 0.0146484375, 0.0078125, 0.0380859375, 0.1474609375, 0.01171875, 0.076171875, 0.0498046875, 0.0126953125, 0.00390625, 0.240234375, 0.00390625, 0.0927734375, 0.091796875, 0.201171875, 0.0419921875, 0.9970703125, 0.0419921875, 0.029296875, 0.15625, 0.998046875, 0.103515625, 0.0205078125, 0.029296875, 0.0517578125, 0.1328125, 0.05078125, 0.490234375, 0.6787109375, 0.1103515625, 0.107421875, 0.0400390625, 0.998046875, 0.0546875, 0.08984375, 0.037109375, 0.001953125, 0.017578125, 0.01171875, 0.00390625, 0.013671875, 0.5458984375, 0.0205078125, 0.0146484375, 0.0185546875, 0.369140625, 0.373046875, 0.056640625, 0.08203125, 0.01953125, 0.0244140625, 0.9970703125, 0.060546875, 0.8056640625, 0.0751953125, 0.125, 0.029296875, 0.1748046875, 0.0087890625, 0.005859375, 0.1025390625, 0.0126953125, 0.1025390625, 0.0771484375, 0.0048828125, 0.0107421875, 0.0224609375, 0.2880859375, 0.0458984375, 0.04296875, 0.8466796875, 0.0322265625, 0.765625, 0.025390625, 0.05859375, 0.640625, 0.01953125, 0.052734375, 0.03125, 0.1640625, 0.01953125, 0.06640625, 0.216796875, 0.0625, 0.01171875, 0.0859375, 0.0009765625, 0.00390625, 0.0966796875, 0.111328125, 0.1484375, 0.28515625, 0.150390625, 0.3212890625, 0.166015625, 0.025390625, 0.9970703125, 0.0283203125, 0.0322265625, 0.998046875, 0.1025390625, 0.1689453125, 0.044921875, 0.0634765625, 0.1611328125, 0.0009765625, 0.9423828125, 0.5185546875, 0.0, 0.0029296875, 0.0068359375, 0.017578125, 0.068359375, 0.1083984375, 0.064453125, 0.0, 0.0048828125, 0.412109375, 0.0341796875, 0.0693359375, 0.857421875, 0.0009765625, 0.0009765625, 0.412109375, 0.0302734375, 0.0380859375, 0.0126953125, 0.076171875, 0.0224609375, 0.0, 0.0087890625, 0.1064453125, 0.8046875, 0.9970703125, 0.01171875, 0.0263671875, 0.07421875, 0.1220703125, 0.998046875, 0.0380859375, 0.0908203125, 0.009765625, 0.0947265625, 0.0, 0.08203125, 0.05078125, 0.0654296875, 0.015625, 0.0029296875, 0.0341796875, 0.013671875, 0.0927734375, 0.0322265625, 0.119140625, 0.0380859375, 0.0009765625, 0.0576171875, 0.96484375, 0.1005859375, 0.9990234375, 0.013671875, 0.0244140625, 0.013671875, 0.0263671875, 0.0234375, 0.0478515625, 0.1142578125, 0.033203125, 0.77734375, 0.2509765625, 0.9990234375, 0.9169921875, 0.0, 0.119140625, 0.111328125, 0.0068359375, 0.0263671875, 0.0107421875, 0.8232421875, 0.0029296875, 0.1982421875, 0.0927734375, 0.140625, 0.109375, 0.2509765625, 0.01953125, 0.052734375, 0.0029296875, 0.0087890625, 0.18359375, 0.02734375, 0.0107421875, 0.1796875, 0.0986328125, 0.12109375, 0.0966796875, 0.0, 0.0400390625, 0.0, 0.013671875, 0.0986328125, 0.0712890625, 0.017578125, 0.0, 0.0, 0.931640625, 0.009765625, 0.8583984375, 0.8447265625, 0.0205078125, 0.0419921875, 0.0048828125, 0.0283203125, 0.115234375, 0.833984375, 0.091796875, 0.0380859375, 0.15234375, 0.041015625, 0.109375, 0.001953125, 0.02734375, 0.0537109375, 0.9609375, 0.0009765625, 0.0029296875, 0.0546875, 0.0439453125, 0.0771484375, 0.0244140625, 0.126953125, 0.0673828125, 0.0146484375, 0.0869140625, 0.0791015625, 0.9970703125, 0.0908203125, 0.2333984375, 0.005859375, 0.0107421875, 0.00390625, 0.0380859375, 0.0087890625, 0.0048828125, 0.8193359375, 0.03125, 0.9013671875, 0.001953125, 0.01953125, 0.0029296875, 0.0185546875, 0.123046875, 0.0166015625, 0.1025390625, 0.0107421875, 0.7421875, 0.0107421875, 0.0126953125, 0.427734375, 0.17578125, 0.01953125, 0.041015625, 0.087890625, 0.0771484375, 0.171875, 0.0771484375, 0.9208984375, 0.71875, 0.0693359375, 0.05859375, 0.0126953125, 0.146484375, 0.6630859375, 0.015625, 0.0263671875, 0.0322265625, 0.017578125, 0.0693359375, 0.013671875, 0.00390625, 0.068359375, 0.4775390625, 0.9560546875, 0.0654296875, 0.9677734375, 0.00390625, 0.94140625, 0.037109375, 0.0322265625, 0.013671875, 0.1669921875, 0.005859375, 0.0107421875, 0.0, 0.0361328125, 0.044921875, 0.0693359375, 0.6962890625, 0.0185546875, 0.048828125, 0.1806640625, 0.05078125, 0.083984375, 0.8173828125, 0.296875, 0.07421875, 0.865234375, 0.0908203125, 0.1103515625, 0.0, 0.9287109375, 0.9619140625, 0.8427734375, 0.02734375, 0.0234375, 0.05078125, 0.0615234375, 0.951171875, 0.998046875, 0.01953125, 0.1728515625, 0.0693359375, 0.1025390625, 0.0078125, 0.1806640625, 0.00390625, 0.8232421875, 0.98828125, 0.408203125, 0.369140625, 0.017578125, 0.376953125, 0.0029296875, 0.044921875, 0.03125, 0.0927734375, 0.029296875, 0.015625, 0.126953125, 0.998046875, 0.03125, 0.0, 0.0126953125, 0.03125, 0.0244140625, 0.630859375, 0.001953125, 0.0400390625, 0.0478515625, 0.0087890625, 0.0185546875, 0.0458984375, 0.0107421875, 0.0849609375, 0.080078125, 0.0283203125, 0.1025390625, 0.00390625, 0.9677734375, 0.0849609375, 0.0966796875, 0.1259765625, 0.4423828125, 0.169921875, 0.5537109375, 0.0, 0.0078125, 0.9453125, 0.0791015625, 0.0556640625, 0.03125, 0.24609375, 0.5498046875, 0.2451171875, 0.013671875, 0.0107421875, 0.0283203125, 0.11328125, 0.072265625, 0.0126953125, 0.169921875, 0.0, 0.0, 0.080078125, 0.181640625, 0.03125, 0.0146484375, 0.041015625, 0.0341796875, 0.0029296875, 0.158203125, 0.0107421875, 0.017578125, 0.0810546875, 0.0126953125, 0.0732421875, 0.001953125, 0.013671875, 0.880859375, 0.0078125, 0.0615234375, 0.75390625, 0.080078125, 0.060546875, 0.001953125, 0.0458984375, 0.9697265625, 0.0048828125, 0.9453125, 0.044921875, 0.021484375, 0.00390625, 0.015625, 0.74609375, 0.9970703125, 0.0947265625, 0.0, 0.0166015625, 0.0, 0.0419921875, 0.0107421875, 0.3095703125, 0.228515625, 0.00390625, 0.095703125, 0.0400390625, 0.224609375, 0.1025390625, 0.0205078125, 0.2353515625, 0.689453125, 0.998046875, 0.513671875, 0.0439453125, 0.091796875, 0.0263671875, 0.021484375, 0.0947265625, 0.1748046875, 0.658203125, 0.02734375, 0.025390625, 0.7705078125, 0.005859375, 0.9970703125, 0.9990234375, 0.052734375, 0.998046875, 0.037109375, 0.0, 0.0029296875, 0.08203125, 0.0068359375, 0.0908203125, 0.248046875, 0.2294921875, 0.0400390625, 0.142578125, 0.01953125, 0.0078125, 0.0205078125, 0.951171875, 0.068359375, 0.00390625, 0.9970703125, 0.0, 0.0478515625, 0.04296875, 0.0263671875, 0.9970703125, 0.0166015625, 0.0712890625, 0.1064453125, 0.1513671875, 0.0244140625, 0.0078125, 0.0244140625, 0.017578125, 0.173828125, 0.294921875, 0.0830078125, 0.05078125, 0.056640625, 0.0048828125, 0.0166015625, 0.0615234375, 0.01953125, 0.0244140625, 0.5419921875, 0.001953125, 0.095703125, 0.998046875, 0.998046875, 0.0087890625, 0.0029296875, 0.2568359375, 0.0546875, 0.099609375, 0.689453125, 0.0712890625, 0.0341796875, 0.0, 0.0810546875, 0.0078125, 0.095703125, 0.03515625, 0.0517578125, 0.0205078125, 0.9990234375, 0.2958984375, 0.0263671875, 0.005859375, 0.0029296875, 0.001953125, 0.0458984375, 0.02734375, 0.0869140625, 0.078125, 0.0849609375, 0.0, 0.900390625, 0.1689453125, 0.00390625, 0.7802734375, 0.0634765625, 0.9453125, 0.1494140625, 0.005859375, 0.0107421875, 0.0322265625, 0.013671875, 0.08203125, 0.0, 0.7021484375, 0.00390625, 0.109375, 0.041015625, 0.4169921875, 0.001953125, 0.0927734375, 0.1396484375, 0.021484375, 0.1220703125, 0.6455078125, 0.0244140625, 0.265625, 0.095703125, 0.013671875, 0.3046875, 0.1083984375, 0.93359375, 0.876953125, 0.015625, 0.0146484375, 0.0771484375, 0.0908203125, 0.5654296875, 0.65625, 0.138671875, 0.1259765625, 0.025390625, 0.0478515625, 0.0869140625, 0.1416015625, 0.017578125, 0.138671875, 0.060546875, 0.03515625, 0.08203125, 0.998046875, 0.017578125, 0.0283203125, 0.1162109375, 0.587890625, 0.2822265625, 0.046875, 0.9970703125, 0.0205078125, 0.017578125, 0.0419921875, 0.0029296875, 0.0302734375, 0.998046875, 0.0947265625, 0.5595703125, 0.025390625, 0.08984375, 0.021484375, 0.0263671875, 0.0458984375, 0.998046875, 0.021484375, 0.1025390625, 0.0478515625, 0.078125, 0.0576171875, 0.4677734375, 0.015625, 0.001953125, 0.0263671875, 0.0380859375, 0.9560546875, 0.001953125, 0.0048828125, 0.001953125, 0.0302734375, 0.1025390625, 0.080078125, 0.1552734375, 0.0029296875, 0.037109375, 0.0185546875, 0.087890625, 0.1123046875, 0.1298828125, 0.04296875, 0.0380859375, 0.015625, 0.8994140625, 0.1708984375, 0.0068359375, 0.01171875, 0.0224609375, 0.0341796875, 0.1025390625, 0.0205078125, 0.02734375, 0.1240234375, 0.001953125, 0.001953125, 0.00390625, 0.2490234375, 0.06640625, 0.0048828125, 0.005859375, 0.025390625, 0.7578125, 0.06640625, 0.0302734375, 0.1435546875, 0.01953125, 0.00390625, 0.03515625, 0.2626953125, 0.09375, 0.9697265625, 0.0087890625, 0.01171875, 0.0078125, 0.134765625, 0.0576171875, 0.0068359375, 0.01171875, 0.0234375, 0.0009765625, 0.80078125, 0.595703125, 0.0498046875, 0.056640625, 0.0107421875, 0.0, 0.9375, 0.7783203125, 0.0283203125, 0.9970703125, 0.021484375, 0.9970703125, 0.0205078125, 0.001953125, 0.0556640625, 0.005859375, 0.0302734375, 0.0234375, 0.0947265625, 0.998046875, 0.048828125, 0.0458984375, 0.654296875, 0.3232421875, 0.00390625, 0.1533203125, 0.1025390625, 0.998046875, 0.0947265625, 0.0537109375, 0.1865234375, 0.0205078125, 0.142578125, 0.162109375, 0.259765625, 0.01171875, 0.056640625, 0.0048828125, 0.064453125, 0.0078125, 0.0859375, 0.1416015625, 0.044921875, 0.01171875, 0.0029296875, 0.3212890625, 0.9990234375, 0.9150390625, 0.0498046875, 0.0400390625, 0.2841796875, 0.0048828125, 0.001953125, 0.078125, 0.173828125, 0.146484375, 0.0791015625, 0.1005859375, 0.0234375, 0.2900390625, 0.00390625, 0.833984375, 0.0078125, 0.013671875, 0.875, 0.8173828125, 0.1025390625, 0.998046875, 0.033203125, 0.849609375, 0.009765625, 0.2041015625, 0.0126953125, 0.87109375, 0.01171875, 0.02734375, 0.6689453125, 0.080078125, 0.1552734375, 0.80859375, 0.4296875, 0.00390625, 0.03125, 0.087890625, 0.0703125, 0.16015625, 0.0166015625, 0.0126953125, 0.80078125, 0.12109375, 0.0166015625, 0.9990234375, 0.044921875, 0.0185546875, 0.0361328125, 0.86328125, 0.037109375, 0.060546875, 0.1943359375, 0.03125, 0.2080078125, 0.0048828125, 0.8232421875, 0.0439453125, 0.791015625, 0.19921875, 0.9091796875, 0.015625, 0.0146484375, 0.10546875, 0.0908203125, 0.0078125, 0.115234375, 0.0263671875, 0.923828125, 0.0869140625, 0.90234375, 0.7744140625, 0.1259765625, 0.0302734375, 0.0048828125, 0.123046875, 0.912109375, 0.005859375, 0.3583984375, 0.0859375, 0.041015625, 0.091796875, 0.0, 0.0185546875, 0.009765625, 0.0634765625, 0.091796875, 0.0751953125, 0.0703125, 0.998046875, 0.865234375, 0.0322265625, 0.05859375, 0.03125, 0.025390625, 0.888671875, 0.013671875, 0.1025390625, 0.95703125, 0.029296875, 0.017578125, 0.9990234375, 0.1552734375, 0.123046875, 0.0126953125, 0.0205078125, 0.0380859375, 0.1025390625, 0.9970703125, 0.0224609375, 0.017578125, 0.0166015625, 0.0146484375, 0.0, 0.0126953125, 0.0029296875, 0.177734375, 0.01953125, 0.0, 0.109375, 0.091796875, 0.00390625, 0.1025390625, 0.037109375, 0.0087890625, 0.47265625, 0.0029296875, 0.34375, 0.599609375, 0.1904296875, 0.0048828125, 0.91796875, 0.10546875, 0.03125, 0.0068359375, 0.0439453125, 0.0107421875, 0.1826171875, 0.001953125, 0.0234375, 0.0234375, 0.0625, 0.01953125, 0.005859375, 0.123046875, 0.044921875, 0.9970703125, 0.052734375, 0.0146484375, 0.3212890625, 0.005859375, 0.0087890625, 0.017578125, 0.0166015625, 0.01171875, 0.1162109375, 0.0185546875, 0.0380859375, 0.9990234375, 0.9716796875, 0.697265625, 0.0751953125, 0.005859375, 0.0654296875, 0.0458984375, 0.0615234375, 0.923828125, 0.1298828125, 0.05078125, 0.8974609375, 0.08203125, 0.916015625, 0.5732421875, 0.060546875, 0.58203125, 0.11328125, 0.0126953125, 0.1318359375, 0.013671875, 0.0771484375, 0.025390625, 0.1962890625, 0.0185546875, 0.00390625, 0.1025390625, 0.0537109375, 0.0546875, 0.625, 0.0322265625, 0.375, 0.095703125, 0.03515625, 0.0166015625, 0.146484375, 0.05078125, 0.7314453125, 0.083984375, 0.078125, 0.107421875, 0.91015625, 0.052734375, 0.134765625, 0.3037109375, 0.1796875, 0.0234375, 0.1025390625, 0.0048828125, 0.001953125, 0.6455078125, 0.115234375, 0.017578125, 0.3916015625, 0.0146484375, 0.068359375, 0.0498046875, 0.0224609375, 0.1025390625, 0.0087890625, 0.0205078125, 0.2109375, 0.0205078125, 0.0732421875, 0.1162109375, 0.2783203125, 0.2470703125, 0.1015625, 0.2509765625, 0.109375, 0.212890625, 0.6650390625, 0.138671875, 0.1328125, 0.037109375, 0.0458984375, 0.0302734375, 0.013671875, 0.0302734375, 0.0, 0.212890625, 0.025390625, 0.0205078125, 0.998046875, 0.099609375, 0.021484375, 0.00390625, 0.0166015625, 0.1181640625, 0.060546875, 0.2255859375, 0.65234375, 0.34765625, 0.0, 0.0029296875, 0.0234375, 0.01171875, 0.025390625, 0.009765625, 0.1259765625, 0.0634765625, 0.884765625, 0.0244140625, 0.0205078125, 0.9970703125, 0.1689453125, 0.57421875, 0.0751953125, 0.029296875, 0.009765625, 0.033203125, 0.0439453125, 0.2265625, 0.009765625, 0.0234375, 0.0126953125, 0.0400390625, 0.1015625, 0.029296875, 0.091796875, 0.01171875, 0.7822265625, 0.041015625, 0.138671875, 0.0048828125, 0.9970703125, 0.0029296875, 0.77734375, 0.0087890625, 0.017578125, 0.556640625, 0.0400390625, 0.0205078125, 0.0869140625, 0.013671875, 0.0380859375, 0.044921875, 0.0771484375, 0.3486328125, 0.482421875, 0.0, 0.02734375, 0.0673828125, 0.1533203125, 0.1904296875, 0.0087890625, 0.9208984375, 0.0322265625, 0.0537109375, 0.349609375, 0.0263671875, 0.0205078125, 0.017578125, 0.1484375, 0.0107421875, 0.0, 0.0126953125, 0.150390625, 0.1162109375, 0.0341796875, 0.1669921875, 0.0, 0.1298828125, 0.998046875, 0.283203125, 0.01171875, 0.017578125, 0.041015625, 0.95703125, 0.0712890625, 0.033203125, 0.16015625, 0.017578125, 0.0546875, 0.0244140625, 0.0283203125, 0.021484375, 0.1708984375, 0.0595703125, 0.0498046875, 0.2353515625, 0.958984375, 0.28125, 0.998046875, 0.0087890625, 0.0302734375, 0.0205078125, 0.076171875, 0.0341796875, 0.908203125, 0.0, 0.9072265625, 0.9306640625, 0.228515625, 0.94140625, 0.484375, 0.5205078125, 0.025390625, 0.4912109375, 0.0185546875, 0.0, 0.9189453125, 0.091796875, 0.0166015625, 0.0302734375, 0.0283203125, 0.2294921875, 0.015625, 0.048828125, 0.7529296875, 0.083984375, 0.0830078125, 0.021484375, 0.0, 0.0009765625, 0.017578125, 0.9326171875, 0.0400390625, 0.8974609375, 0.21875, 0.0185546875, 0.8916015625, 0.896484375, 0.025390625, 0.0322265625, 0.96875, 0.0107421875, 0.033203125, 0.208984375, 0.9970703125, 0.0, 0.0810546875, 0.0595703125, 0.0615234375, 0.0703125, 0.2099609375, 0.09375, 0.72265625, 0.0283203125, 0.025390625, 0.0322265625, 0.0205078125, 0.0263671875, 0.3212890625, 0.076171875, 0.8125, 0.171875, 0.001953125, 0.23828125, 0.9970703125, 0.091796875, 0.04296875, 0.021484375, 0.041015625, 0.0498046875, 0.029296875, 0.998046875, 0.0947265625, 0.01171875, 0.0625, 0.009765625, 0.0224609375, 0.9990234375, 0.0029296875, 0.9970703125, 0.029296875, 0.7734375, 0.103515625, 0.8359375, 0.216796875, 0.0107421875, 0.1044921875, 0.0400390625, 0.0107421875, 0.0087890625, 0.00390625, 0.0146484375, 0.6240234375, 0.0, 0.0029296875, 0.04296875, 0.02734375, 0.0126953125, 0.560546875, 0.0, 0.0576171875, 0.8515625, 0.0908203125, 0.8349609375, 0.15625, 0.998046875, 0.943359375, 0.0380859375, 0.0048828125, 0.0087890625, 0.841796875, 0.0234375, 0.033203125, 0.916015625, 0.099609375, 0.0048828125, 0.1103515625, 0.0029296875, 0.4892578125, 0.0732421875, 0.041015625, 0.0205078125, 0.134765625, 0.220703125, 0.076171875, 0.013671875, 0.017578125, 0.0576171875, 0.0205078125, 0.8916015625, 0.01953125, 0.013671875, 0.0888671875, 0.2099609375, 0.0810546875, 0.693359375, 0.013671875, 0.0556640625, 0.0185546875, 0.1240234375, 0.7490234375, 0.08203125, 0.9189453125, 0.0751953125, 0.0810546875, 0.0830078125, 0.13671875, 0.0380859375, 0.0341796875, 0.0029296875, 0.0419921875, 0.6318359375, 0.580078125, 0.4189453125, 0.0927734375, 0.087890625, 0.00390625, 0.0126953125, 0.998046875, 0.0224609375, 0.0634765625, 0.2119140625, 0.0654296875, 0.00390625, 0.0859375, 0.0732421875, 0.96484375, 0.7529296875, 0.9169921875, 0.9990234375, 0.302734375, 0.0849609375, 0.1875, 0.0107421875, 0.9130859375, 0.080078125, 0.1923828125, 0.9990234375, 0.0048828125, 0.998046875, 0.0029296875, 0.033203125, 0.9990234375, 0.0322265625, 0.1328125, 0.158203125, 0.0263671875, 0.017578125, 0.00390625, 0.0048828125, 0.0361328125, 0.0615234375, 0.779296875, 0.064453125, 0.009765625, 0.1787109375, 0.01953125, 0.087890625, 0.1337890625, 0.1357421875, 0.0625, 0.00390625, 0.328125, 0.0830078125, 0.00390625, 0.0068359375, 0.025390625, 0.0478515625, 0.876953125, 0.2255859375, 0.173828125, 0.02734375, 0.0234375, 0.0654296875, 0.0244140625, 0.03125, 0.09765625, 0.9990234375, 0.0751953125, 0.044921875, 0.10546875, 0.9970703125, 0.1337890625, 0.32421875, 0.0068359375, 0.8095703125, 0.1005859375, 0.36328125, 0.01953125, 0.044921875, 0.0146484375, 0.0654296875, 0.029296875, 0.015625, 0.0068359375, 0.046875, 0.0029296875, 0.1435546875, 0.0126953125, 0.9306640625, 0.1484375, 0.013671875, 0.0361328125, 0.009765625, 0.06640625, 0.0224609375, 0.0341796875, 0.0, 0.029296875, 0.119140625, 0.9970703125, 0.060546875, 0.23828125, 0.091796875, 0.041015625, 0.0185546875, 0.0361328125, 0.0029296875, 0.0546875, 0.150390625, 0.013671875, 0.00390625, 0.1025390625, 0.998046875, 0.2060546875, 0.9970703125, 0.0009765625, 0.9970703125, 0.142578125, 0.998046875, 0.0703125, 0.9990234375, 0.341796875, 0.0029296875, 0.2509765625, 0.154296875, 0.9970703125, 0.01953125, 0.0625, 0.01953125, 0.0634765625, 0.9189453125, 0.0732421875, 0.130859375, 0.71875, 0.208984375, 0.037109375, 0.9970703125, 0.701171875, 0.0, 0.0205078125, 0.0341796875, 0.224609375, 0.01171875, 0.0703125, 0.0673828125, 0.0986328125, 0.0009765625, 0.2412109375, 0.0009765625, 0.94140625, 0.0146484375, 0.005859375, 0.0009765625, 0.0283203125, 0.0009765625, 0.0029296875, 0.0576171875, 0.1201171875, 0.8857421875, 0.1953125, 0.017578125, 0.9580078125, 0.0380859375, 0.0830078125, 0.1025390625, 0.07421875, 0.08984375, 0.0810546875, 0.021484375, 0.013671875, 0.09375, 0.033203125, 0.041015625, 0.1826171875, 0.4658203125, 0.0283203125, 0.0732421875, 0.12890625, 0.0009765625, 0.23046875, 0.009765625, 0.0791015625, 0.11328125, 0.0205078125, 0.5712890625, 0.6630859375, 0.1875, 0.896484375, 0.0478515625, 0.001953125, 0.9970703125, 0.083984375, 0.0537109375, 0.0673828125, 0.0234375, 0.923828125, 0.0859375, 0.2138671875, 0.0078125, 0.1142578125, 0.107421875, 0.0966796875, 0.0244140625, 0.005859375, 0.0224609375, 0.095703125, 0.146484375, 0.9189453125, 0.044921875, 0.0009765625, 0.068359375, 0.033203125, 0.298828125, 0.0029296875, 0.0048828125, 0.087890625, 0.9970703125, 0.02734375, 0.2001953125, 0.025390625, 0.08203125, 0.001953125, 0.0986328125, 0.0947265625, 0.0673828125, 0.021484375, 0.009765625, 0.0068359375, 0.3662109375, 0.1904296875, 0.0244140625, 0.1884765625, 0.296875, 0.0341796875, 0.0205078125, 0.140625, 0.84375, 0.046875, 0.0439453125, 0.005859375, 0.865234375, 0.0810546875, 0.04296875, 0.03515625, 0.2041015625, 0.884765625, 0.1201171875, 0.0, 0.0771484375, 0.0048828125, 0.0029296875, 0.0185546875, 0.00390625, 0.0576171875, 0.9560546875, 0.0390625, 0.03125, 0.0283203125, 0.005859375, 0.0126953125, 0.89453125, 0.1982421875, 0.0478515625, 0.0390625, 0.1806640625, 0.0234375, 0.27734375, 0.0673828125, 0.0048828125, 0.0908203125, 0.0166015625, 0.0146484375, 0.03125, 0.8984375, 0.998046875, 0.0068359375, 0.0322265625, 0.0458984375, 0.0205078125, 0.1171875, 0.25390625, 0.0986328125, 0.0419921875, 0.0009765625, 0.0185546875, 0.6767578125, 0.0009765625, 0.095703125, 0.1630859375, 0.4365234375, 0.041015625, 0.298828125, 0.0048828125, 0.00390625, 0.5400390625, 0.0166015625, 0.9345703125, 0.09765625, 0.181640625, 0.4638671875, 0.0341796875, 0.0126953125, 0.0146484375, 0.9384765625, 0.29296875, 0.0322265625, 0.0732421875, 0.0556640625, 0.0546875, 0.013671875, 0.9990234375, 0.7021484375, 0.0830078125, 0.0888671875, 0.0087890625, 0.94921875, 0.0498046875, 0.0419921875, 0.9990234375, 0.0029296875, 0.12890625, 0.1201171875, 0.81640625, 0.0, 0.06640625, 0.044921875, 0.9033203125, 0.9990234375, 0.1513671875, 0.0224609375, 0.029296875, 0.244140625, 0.1220703125, 0.158203125, 0.046875, 0.00390625, 0.1455078125, 0.0703125, 0.0458984375, 0.013671875, 0.013671875, 0.9970703125, 0.0, 0.96875, 0.0625, 0.013671875, 0.2392578125, 0.0185546875, 0.052734375, 0.2392578125, 0.14453125, 0.1513671875, 0.7314453125, 0.1328125, 0.0146484375, 0.1513671875, 0.1376953125, 0.8583984375, 0.056640625, 0.041015625, 0.0, 0.0302734375, 0.6591796875, 0.03515625, 0.0068359375, 0.0673828125, 0.0068359375, 0.0283203125, 0.0078125, 0.3701171875, 0.0146484375, 0.955078125]

 sparsity of   [0.06591796875, 0.068359375, 0.0390625, 0.0458984375, 0.80078125, 0.19189453125, 0.47119140625, 0.6865234375, 0.6435546875, 0.92578125, 0.3076171875, 0.52978515625, 0.1748046875, 0.0400390625, 0.18994140625, 0.06982421875, 0.0625, 0.0283203125, 0.0712890625, 0.96044921875, 0.24658203125, 0.16015625, 0.99853515625, 0.15576171875, 0.02880859375, 0.71337890625, 0.0537109375, 0.04296875, 0.7890625, 0.0400390625, 0.04248046875, 0.236328125, 0.79931640625, 0.14013671875, 0.14306640625, 0.7587890625, 0.638671875, 0.03125, 0.06787109375, 0.55029296875, 0.0498046875, 0.16796875, 0.02734375, 0.63037109375, 0.33740234375, 0.041015625, 0.6650390625, 0.1357421875, 0.55126953125, 0.1123046875, 0.08056640625, 0.060546875, 0.0703125, 0.00732421875, 0.0556640625, 0.21923828125, 0.8125, 0.60888671875, 0.03955078125, 0.896484375, 0.7958984375, 0.037109375, 0.11865234375, 0.02197265625, 0.0771484375, 0.07275390625, 0.6162109375, 0.36572265625, 0.2470703125, 0.048828125, 0.1201171875, 0.99853515625, 0.05078125, 0.798828125, 0.052734375, 0.75634765625, 0.087890625, 0.736328125, 0.99951171875, 0.0771484375, 0.01123046875, 0.09228515625, 0.99951171875, 0.15185546875, 0.03466796875, 0.61328125, 0.99951171875, 0.0244140625, 0.06982421875, 0.9033203125, 0.07763671875, 0.5830078125, 0.09423828125, 0.0986328125, 0.09765625, 0.56689453125, 0.99951171875, 0.54296875, 0.123046875, 0.8701171875, 0.05908203125, 0.6328125, 0.0595703125, 0.0244140625, 0.5947265625, 0.17333984375, 0.6669921875, 0.0224609375, 0.5302734375, 0.1787109375, 0.0517578125, 0.02490234375, 0.46875, 0.07470703125, 0.64111328125, 0.03173828125, 0.19091796875, 0.02001953125, 0.5634765625, 0.69482421875, 0.0419921875, 0.52587890625, 0.068359375, 0.02197265625, 0.1337890625, 0.0205078125, 0.27685546875, 0.06103515625, 0.45654296875, 0.21630859375, 0.07177734375, 0.11083984375, 0.04345703125, 0.189453125, 0.04443359375, 0.0615234375, 0.041015625, 0.0751953125, 0.21630859375, 0.01953125, 0.146484375, 0.05224609375, 0.615234375, 0.0, 0.59033203125, 0.03173828125, 0.060546875, 0.314453125, 0.04638671875, 0.01953125, 0.0, 0.72265625, 0.64990234375, 0.04248046875, 0.19189453125, 0.03759765625, 0.03564453125, 0.13671875, 0.0556640625, 0.02587890625, 0.7666015625, 0.07080078125, 0.56005859375, 0.05224609375, 0.12841796875, 0.8232421875, 0.9990234375, 0.34033203125, 0.09423828125, 0.60986328125, 0.06982421875, 0.1552734375, 0.5263671875, 0.150390625, 0.30126953125, 0.96044921875, 0.04541015625, 0.119140625, 0.728515625, 0.08349609375, 0.765625, 0.01416015625, 0.37353515625, 0.62548828125, 0.02734375, 0.02978515625, 0.041015625, 0.62744140625, 0.10107421875, 0.1572265625, 0.99951171875, 0.07666015625, 0.056640625, 0.1044921875, 0.037109375, 0.99951171875, 0.61865234375, 0.0185546875, 0.865234375, 0.05517578125, 0.3974609375, 0.0849609375, 0.53515625, 0.99951171875, 0.04638671875, 0.85498046875, 0.01220703125, 0.119140625, 0.0498046875, 0.03955078125, 0.11181640625, 0.2509765625, 0.05078125, 0.0703125, 0.56103515625, 0.15869140625, 0.0458984375, 0.03662109375, 0.10302734375, 0.50341796875, 0.0888671875, 0.0556640625, 0.12451171875, 0.05908203125, 0.048828125, 0.49658203125, 0.0, 0.03662109375, 0.59033203125, 0.015625, 0.0322265625, 0.11376953125, 0.03857421875, 0.47802734375, 0.021484375, 0.99951171875, 0.029296875, 0.58203125, 0.11181640625, 0.0185546875, 0.05615234375, 0.07421875, 0.0146484375, 0.599609375, 0.76904296875, 0.12060546875, 0.53662109375, 0.05029296875, 0.11767578125, 0.69091796875, 0.04248046875, 0.04248046875, 0.60595703125, 0.0693359375, 0.36572265625, 0.0634765625, 0.0869140625, 0.79248046875, 0.7421875, 0.03515625, 0.146484375, 0.16845703125, 0.02197265625, 0.556640625, 0.10400390625, 0.15283203125, 0.064453125, 0.1142578125, 0.03515625, 0.03564453125, 0.046875, 0.4775390625, 0.51171875, 0.1240234375, 0.04833984375, 0.01318359375, 0.01513671875, 0.09228515625, 0.119140625, 0.07568359375, 0.10791015625, 0.642578125, 0.1298828125, 0.01318359375, 0.03466796875, 0.1279296875, 0.04052734375, 0.51953125, 0.6572265625, 0.1533203125, 0.06640625, 0.58154296875, 0.05615234375, 0.6806640625, 0.2177734375, 0.044921875, 0.5419921875, 0.54443359375, 0.0625, 0.10693359375, 0.74560546875, 0.06787109375, 0.01416015625, 0.57373046875, 0.076171875, 0.01708984375, 0.04833984375, 0.5380859375, 0.05029296875, 0.06298828125, 0.58056640625, 0.5302734375, 0.0078125, 0.55908203125, 0.99951171875, 0.09228515625, 0.08349609375, 0.2744140625, 0.06494140625, 0.03271484375, 0.10498046875, 0.015625, 0.53271484375, 0.0146484375, 0.11572265625, 0.69580078125, 0.21044921875, 0.4443359375, 0.56787109375, 0.01171875, 0.05810546875, 0.4638671875, 0.64111328125, 0.0400390625, 0.0625, 0.60791015625, 0.6220703125, 0.09326171875, 0.576171875, 0.02392578125, 0.0517578125, 0.25048828125, 0.04541015625, 0.0537109375, 0.68212890625, 0.1025390625, 0.10595703125, 0.369140625, 0.09228515625, 0.02685546875, 0.06591796875, 0.1337890625, 0.072265625, 0.599609375, 0.05224609375, 0.01123046875, 0.033203125, 0.66796875, 0.04833984375, 0.0830078125, 0.02392578125, 0.0, 0.09814453125, 0.09228515625, 0.02587890625, 0.2021484375, 0.03515625, 0.08251953125, 0.12109375, 0.076171875, 0.0185546875, 0.19482421875, 0.05712890625, 0.0966796875, 0.13330078125, 0.14794921875, 0.0390625, 0.60986328125, 0.62060546875, 0.0732421875, 0.05029296875, 0.51513671875, 0.17333984375, 0.4873046875, 0.04833984375, 0.0849609375, 0.02001953125, 0.396484375, 0.05322265625, 0.13916015625, 0.76708984375, 0.8349609375, 0.419921875, 0.13623046875, 0.1123046875, 0.0107421875, 0.11962890625, 0.888671875, 0.55859375, 0.0478515625, 0.296875, 0.04736328125, 0.70849609375, 0.0380859375, 0.03466796875, 0.0546875, 0.01123046875, 0.8515625, 0.03369140625, 0.17529296875, 0.07763671875, 0.041015625, 0.06005859375, 0.13916015625, 0.12744140625, 0.55859375, 0.0908203125, 0.6513671875, 0.03466796875, 0.0244140625, 0.86767578125, 0.38525390625, 0.04931640625, 0.30029296875, 0.0576171875, 0.70166015625, 0.0146484375, 0.0927734375, 0.59130859375, 0.8486328125, 0.0283203125, 0.06787109375, 0.08203125, 0.041015625, 0.1220703125, 0.05908203125, 0.36474609375, 0.09912109375, 0.64013671875, 0.125, 0.0556640625, 0.0478515625, 0.123046875, 0.6787109375, 0.0302734375, 0.0771484375, 0.12109375, 0.07080078125, 0.7666015625, 0.876953125, 0.37841796875, 0.060546875, 0.15087890625, 0.05078125, 0.27978515625, 0.05322265625, 0.55908203125, 0.05419921875, 0.54541015625, 0.77783203125, 0.04248046875, 0.0078125, 0.578125, 0.08251953125, 0.171875, 0.2392578125, 0.99951171875, 0.01025390625, 0.01025390625, 0.06103515625, 0.08642578125, 0.03564453125, 0.04443359375, 0.12158203125, 0.0654296875, 0.09326171875, 0.02978515625, 0.66259765625, 0.525390625, 0.0458984375, 0.60693359375, 0.01953125, 0.61181640625, 0.09814453125, 0.0732421875, 0.2392578125, 0.4775390625, 0.09228515625, 0.123046875, 0.09228515625, 0.248046875, 0.06591796875, 0.30029296875, 0.103515625, 0.04248046875, 0.0546875, 0.73974609375, 0.12744140625, 0.2001953125, 0.759765625, 0.2373046875, 0.71533203125, 0.0537109375, 0.04248046875, 0.02734375, 0.7333984375, 0.02001953125, 0.03076171875, 0.08349609375, 0.23681640625, 0.12646484375, 0.04296875]

 sparsity of   [0.02170138992369175, 0.00021701389050576836, 0.905381977558136, 0.0, 0.0436197929084301, 0.0412326380610466, 0.02365451492369175, 0.0, 0.0106336809694767, 0.021484375, 0.01019965298473835, 0.0625, 0.01822916604578495, 0.212890625, 0.0, 0.02083333395421505, 0.02018229104578495, 0.02973090298473835, 0.0, 0.0529513880610466, 0.0167100690305233, 0.0451388880610466, 0.0401475690305233, 0.0, 0.0, 0.0319010429084301, 0.014973958022892475, 0.0004340277810115367, 0.0071614584885537624, 0.02170138992369175, 0.02734375, 0.0694444477558136, 0.008029513992369175, 0.2085503488779068, 0.0, 0.0327690988779068, 0.0145399309694767, 0.0640190988779068, 0.0648871511220932, 0.0, 0.0232204869389534, 0.0206163190305233, 0.0338541679084301, 0.0, 0.0034722222480922937, 0.0486111119389534, 0.0874565988779068, 0.999131977558136, 0.0373263880610466, 0.0086805559694767, 0.02604166604578495, 0.029296875, 0.0594618059694767, 0.05078125, 0.02170138992369175, 0.0006510416860692203, 0.0, 0.00021701389050576836, 0.0145399309694767, 0.01888020895421505, 0.0264756940305233, 0.02365451492369175, 0.0, 0.0232204869389534, 0.004123263992369175, 0.0421006940305233, 0.0049913194961845875, 0.0004340277810115367, 0.00933159701526165, 0.009982638992369175, 0.02408854104578495, 0.0047743055038154125, 0.0, 0.0023871527519077063, 0.0, 0.01692708395421505, 0.0, 0.0212673619389534, 0.0186631940305233, 0.0325520820915699, 0.123914934694767, 0.0206163190305233, 0.0026041667442768812, 0.0186631940305233, 0.0004340277810115367, 0.0, 0.0622829869389534, 0.013454861007630825, 0.0316840298473835, 0.02473958395421505, 0.015625, 0.0245225690305233, 0.0245225690305233, 0.010416666977107525, 0.0505642369389534, 0.044921875, 0.01171875, 0.009982638992369175, 0.0466579869389534, 0.0, 0.0, 0.01323784701526165, 0.03059895895421505, 0.0349392369389534, 0.0164930559694767, 0.0334201380610466, 0.0466579869389534, 0.02973090298473835, 0.0577256940305233, 0.013454861007630825, 0.00021701389050576836, 0.00021701389050576836, 0.00021701389050576836, 0.02582465298473835, 0.021484375, 0.005859375, 0.00824652798473835, 0.02951388992369175, 0.01974826492369175, 0.009114583022892475, 0.00390625, 0.01019965298473835, 0.01801215298473835, 0.0, 0.0334201380610466, 0.0483940988779068, 0.01410590298473835, 0.0173611119389534, 0.0405815988779068, 0.0392795130610466, 0.1072048619389534, 0.0264756940305233, 0.0164930559694767, 0.0, 0.01215277798473835, 0.0455729179084301, 0.01128472201526165, 0.01519097201526165, 0.0347222238779068, 0.0, 0.0505642369389534, 0.0, 0.0, 0.3873697817325592, 0.0290798619389534, 0.011067708022892475, 0.017578125, 0.0494791679084301, 0.0078125, 0.0, 0.0, 0.999131977558136, 0.014973958022892475, 0.0206163190305233, 0.0223524309694767, 0.01974826492369175, 0.01888020895421505, 0.02408854104578495, 0.01953125, 0.004123263992369175, 0.0223524309694767, 0.0, 0.0479600690305233, 0.01171875, 0.0622829869389534, 0.02278645895421505, 0.00021701389050576836, 0.00021701389050576836, 0.008463541977107525, 0.011935763992369175, 0.0, 0.011935763992369175, 0.0, 0.00021701389050576836, 0.0026041667442768812, 0.0243055559694767, 0.0338541679084301, 0.014322916977107525, 0.9997829794883728, 0.00021701389050576836, 0.0594618059694767, 0.004123263992369175, 0.002170138992369175, 0.01953125, 0.0447048619389534, 0.0262586809694767, 0.0, 0.02690972201526165, 0.01323784701526165, 0.0, 0.006076388992369175, 0.01519097201526165, 0.012803819961845875, 0.0, 0.0338541679084301, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.004123263992369175, 0.029296875, 0.0373263880610466, 0.0, 0.0047743055038154125, 0.0310329869389534, 0.0, 0.025390625, 0.02951388992369175, 0.0069444444961845875, 0.0364583320915699, 0.0427517369389534, 0.0015190972480922937, 0.015625, 0.0264756940305233, 0.0, 0.0, 0.0264756940305233, 0.01171875, 0.0, 0.0193142369389534, 0.0010850694961845875, 0.0, 0.0034722222480922937, 0.0364583320915699, 0.0713975727558136, 0.0, 0.078993059694767, 0.009765625, 0.0, 0.9993489384651184, 0.8491753339767456, 0.012803819961845875, 0.0407986119389534, 0.0212673619389534, 0.0264756940305233, 0.0167100690305233, 0.010850694961845875, 0.0065104165114462376, 0.02495659701526165, 0.0353732630610466, 0.0, 0.0, 0.0544704869389534, 0.0, 0.0184461809694767, 0.0017361111240461469, 0.0, 0.0, 0.0451388880610466, 0.0004340277810115367, 0.02604166604578495, 0.021484375, 0.8621962070465088, 0.0251736119389534, 0.00021701389050576836, 0.0, 0.0, 0.010850694961845875, 0.0323350690305233, 0.0, 0.0034722222480922937, 0.0264756940305233, 0.01019965298473835, 0.002170138992369175, 0.0045572915114462376, 0.0, 0.0360243059694767, 0.0598958320915699, 0.01128472201526165, 0.0401475690305233, 0.005642361007630825, 0.007595486007630825, 0.01996527798473835, 0.0338541679084301, 0.02170138992369175, 0.029296875, 0.02560763992369175, 0.00021701389050576836, 0.0492621548473835, 0.00021701389050576836, 0.0386284738779068, 0.00629340298473835, 0.0457899309694767, 0.007595486007630825, 0.02886284701526165, 0.01019965298473835, 0.0, 0.014322916977107525, 0.02799479104578495, 0.0010850694961845875, 0.228515625, 0.0184461809694767, 0.4118923544883728, 0.009548611007630825, 0.0, 0.0071614584885537624, 0.013888888992369175, 0.0164930559694767, 0.0, 0.734375, 0.0284288190305233, 0.0319010429084301, 0.0, 0.02365451492369175, 0.0184461809694767, 0.046875, 0.0069444444961845875, 0.9993489384651184, 0.0, 0.041015625, 0.0, 0.0184461809694767, 0.0, 0.0, 0.0145399309694767, 0.0464409738779068, 0.008897569961845875, 0.0049913194961845875, 0.0212673619389534, 0.009982638992369175, 0.0106336809694767, 0.0, 0.0772569477558136, 0.9997829794883728, 0.0206163190305233, 0.013671875, 0.0503472238779068, 0.0, 0.0232204869389534, 0.009114583022892475, 0.1182725727558136, 0.0047743055038154125, 0.0013020833721384406, 0.0017361111240461469, 0.0323350690305233, 0.0325520820915699, 0.0611979179084301, 0.9769965410232544, 0.0, 0.0010850694961845875, 0.0303819440305233, 0.0, 0.0685763880610466, 0.0314670130610466, 0.0407986119389534, 0.0, 0.0, 0.0520833320915699, 0.0, 0.007378472480922937, 0.013888888992369175, 0.00933159701526165, 0.0496961809694767, 0.0243055559694767, 0.01128472201526165, 0.01888020895421505, 0.0505642369389534, 0.02278645895421505, 0.0, 0.0008680555620230734, 0.0, 0.0052083334885537624, 0.0, 0.012803819961845875, 0.00021701389050576836, 0.0, 0.0, 0.0321180559694767, 0.011501736007630825, 0.025390625, 0.0282118059694767, 0.0381944440305233, 0.02690972201526165, 0.00629340298473835, 0.0106336809694767, 0.0698784738779068, 0.4140625, 0.0086805559694767, 0.00629340298473835, 0.0049913194961845875, 0.01822916604578495, 0.0418836809694767, 0.041015625, 0.00021701389050576836, 0.0713975727558136, 0.0, 0.00021701389050576836, 0.0004340277810115367, 0.0, 0.02777777798473835, 0.01953125, 0.0, 0.0167100690305233, 0.0, 0.009982638992369175, 0.0319010429084301, 0.0, 0.0, 0.0915798619389534, 0.1236979141831398, 0.0, 0.0, 0.0, 0.01128472201526165, 0.0, 0.0290798619389534, 0.0, 0.041015625, 0.1362847238779068, 0.013454861007630825, 0.0, 0.0, 0.02973090298473835, 0.0065104165114462376, 0.078125, 0.9993489384651184, 0.0407986119389534, 0.0, 0.013454861007630825, 0.0, 0.0234375, 0.015407986007630825, 0.0, 0.025390625, 0.0, 0.0, 0.013454861007630825, 0.01888020895421505, 0.02473958395421505, 0.01128472201526165, 0.0, 0.013454861007630825, 0.0008680555620230734, 0.0, 0.0069444444961845875, 0.0, 0.0, 0.0065104165114462376, 0.0525173619389534, 0.0167100690305233, 0.0173611119389534, 0.005859375, 0.02582465298473835, 0.0325520820915699, 0.00824652798473835, 0.01584201492369175, 0.0, 0.01171875, 0.0028211805038154125, 0.005642361007630825, 0.1870659738779068, 0.0618489570915699, 0.03515625, 0.0, 0.0052083334885537624, 0.02777777798473835, 0.0, 0.014756944961845875, 0.046875, 0.0687934011220932, 0.0, 0.0, 0.0212673619389534, 0.0125868059694767, 0.0, 0.01519097201526165, 0.0, 0.009765625, 0.0427517369389534, 0.02690972201526165, 0.0, 0.0164930559694767, 0.0, 0.0, 0.0, 0.0036892362404614687, 0.0327690988779068, 0.021484375, 0.0394965298473835, 0.009114583022892475, 0.017578125, 0.001953125, 0.02777777798473835, 0.014756944961845875, 0.01822916604578495, 0.009548611007630825, 0.01019965298473835, 0.0310329869389534, 0.01779513992369175, 0.02994791604578495, 0.0, 0.0533854179084301, 0.0577256940305233, 0.00390625, 0.01605902798473835, 0.0047743055038154125, 0.0, 0.013671875, 0.0, 0.03059895895421505, 0.0264756940305233, 0.0245225690305233, 0.002170138992369175, 0.009765625, 0.0386284738779068, 0.00629340298473835, 0.0, 0.01605902798473835, 0.0086805559694767, 0.0407986119389534]

 sparsity of   [0.0, 0.01953125, 0.0, 0.01171875, 0.015625, 0.017578125, 0.033203125, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.087890625, 0.009765625, 0.05859375, 0.009765625, 0.01171875, 0.009765625, 0.0, 0.65234375, 0.96484375, 0.0, 0.029296875, 0.005859375, 0.017578125, 0.005859375, 0.025390625, 0.0, 0.3671875, 0.00390625, 0.0, 0.0, 0.005859375, 0.01953125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.009765625, 0.005859375, 0.03125, 0.0, 0.609375, 0.00390625, 0.0, 0.001953125, 0.001953125, 0.03125, 0.01953125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.986328125, 0.01953125, 0.0, 0.0, 0.0234375, 0.00390625, 0.0, 0.0, 0.001953125, 0.025390625, 0.005859375, 0.001953125, 0.125, 0.021484375, 0.03515625, 0.033203125, 0.0, 0.0, 0.015625, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.05078125, 0.046875, 0.97265625, 0.02734375, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0234375, 0.0, 0.046875, 0.03515625, 0.0, 0.0, 0.029296875, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0078125, 0.0, 0.009765625, 0.0, 0.0, 0.673828125, 0.0390625, 0.0, 0.015625, 0.65625, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.005859375, 0.015625, 0.00390625, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01953125, 0.01171875, 0.0, 0.015625, 0.0, 0.001953125, 0.0, 0.021484375, 0.05859375, 0.0, 0.0, 0.017578125, 0.0, 0.001953125, 0.001953125, 0.005859375, 0.009765625, 0.015625, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.0625, 0.0, 0.01953125, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.001953125, 0.0, 0.0546875, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.009765625, 0.005859375, 0.015625, 0.0, 0.01171875, 0.021484375, 0.0, 0.001953125, 0.0, 0.0, 0.005859375, 0.00390625, 0.029296875, 0.0, 0.0, 0.017578125, 0.01171875, 0.0, 0.001953125, 0.00390625, 0.01953125, 0.0, 0.01171875, 0.00390625, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.08203125, 0.0, 0.001953125, 0.0, 0.021484375, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0078125, 0.01171875, 0.279296875, 0.0, 0.154296875, 0.01171875, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.017578125, 0.00390625, 0.01171875, 0.0, 0.037109375, 0.0, 0.025390625, 0.0, 0.0, 0.001953125, 0.0078125, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.55859375, 0.00390625, 0.0, 0.01953125, 0.03515625, 0.021484375, 0.0, 0.033203125, 0.001953125, 0.0, 0.0, 0.001953125, 0.01171875, 0.0, 0.013671875, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.078125, 0.130859375, 0.009765625, 0.005859375, 0.0, 0.048828125, 0.11328125, 0.00390625, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.849609375, 0.037109375, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.478515625, 0.021484375, 0.00390625, 0.9921875, 0.0, 0.0, 0.0, 0.107421875, 0.03515625, 0.009765625, 0.00390625, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.97265625, 0.0234375, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.005859375, 0.00390625, 0.0, 0.017578125, 0.0, 0.01953125, 0.021484375, 0.037109375, 0.0, 0.013671875, 0.0, 0.0, 0.009765625, 0.0, 0.0078125, 0.015625, 0.001953125, 0.0, 0.0, 0.0, 0.033203125, 0.9921875, 0.0, 0.033203125, 0.0, 0.0, 0.00390625, 0.021484375, 0.0, 0.001953125, 0.0, 0.0, 0.08203125, 0.017578125, 0.001953125, 0.0, 0.04296875, 0.03125, 0.021484375, 0.720703125, 0.0, 0.0, 0.0, 0.05859375, 0.005859375, 0.013671875, 0.0, 0.001953125, 0.0, 0.0, 0.0234375, 0.015625, 0.06640625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04296875, 0.00390625, 0.04296875, 0.025390625, 0.0, 0.048828125, 0.0, 0.0, 0.0, 0.029296875, 0.01171875, 0.017578125, 0.056640625, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.001953125, 0.00390625, 0.00390625, 0.0078125, 0.0, 0.01171875, 0.009765625, 0.07421875, 0.0, 0.013671875, 0.005859375, 0.04296875, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.013671875, 0.009765625, 0.078125, 0.02734375, 0.51171875, 0.00390625, 0.001953125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.005859375, 0.01171875, 0.99609375, 0.00390625, 0.021484375, 0.00390625, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.015625, 0.0, 0.05859375, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.005859375, 0.001953125, 0.9921875, 0.029296875, 0.0, 0.005859375, 0.0, 0.001953125, 0.0, 0.001953125, 0.02734375, 0.037109375, 0.01171875, 0.013671875, 0.060546875, 0.033203125, 0.0, 0.025390625, 0.0, 0.09765625, 0.0, 0.021484375, 0.021484375, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.01171875, 0.025390625, 0.001953125, 0.0, 0.0234375, 0.0, 0.01953125, 0.0, 0.048828125, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.06640625, 0.0, 0.0, 0.0, 0.001953125, 0.009765625, 0.009765625, 0.033203125, 0.017578125, 0.029296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.052734375, 0.0234375, 0.02734375, 0.0, 0.001953125, 0.0, 0.0, 0.01171875, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0703125, 0.037109375, 0.0, 0.00390625, 0.001953125, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.021484375, 0.005859375, 0.009765625, 0.013671875, 0.05078125, 0.0, 0.009765625, 0.01953125, 0.013671875, 0.001953125, 0.0, 0.025390625, 0.001953125, 0.0, 0.9921875, 0.021484375, 0.1171875, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.037109375, 0.0, 0.005859375, 0.015625, 0.0078125, 0.0, 0.177734375, 0.0, 0.0, 0.966796875, 0.0390625, 0.0, 0.001953125, 0.068359375, 0.056640625, 0.0234375, 0.0, 0.0, 0.009765625, 0.01171875, 0.01953125, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.052734375, 0.0, 0.0, 0.0, 0.01953125, 0.015625, 0.00390625, 0.109375, 0.0, 0.0, 0.017578125, 0.0, 0.0078125, 0.0546875, 0.001953125, 0.00390625, 0.0, 0.0, 0.015625, 0.005859375, 0.017578125, 0.0, 0.005859375, 0.001953125, 0.0, 0.3125, 0.00390625, 0.0, 0.0, 0.0, 0.029296875, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.009765625, 0.001953125, 0.015625, 0.0234375, 0.041015625, 0.0, 0.033203125, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.009765625, 0.0234375, 0.060546875, 0.0, 0.009765625, 0.015625, 0.0, 0.0, 0.037109375, 0.041015625, 0.0, 0.04296875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.013671875, 0.046875, 0.00390625, 0.0, 0.0, 0.0, 0.001953125, 0.001953125, 0.01171875, 0.0, 0.021484375, 0.00390625, 0.0, 0.015625, 0.685546875, 0.0, 0.0, 0.0, 0.013671875, 0.005859375, 0.0, 0.0, 0.021484375, 0.044921875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.064453125, 0.0, 0.0, 0.0, 0.05078125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0234375, 0.005859375, 0.001953125, 0.0, 0.0, 0.00390625, 0.017578125, 0.01953125, 0.0, 0.009765625, 0.0, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.015625, 0.0234375, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.01953125, 0.01171875, 0.01953125, 0.04296875, 0.0, 0.041015625, 0.00390625, 0.0, 0.00390625, 0.001953125, 0.052734375, 0.009765625, 0.001953125, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.029296875, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.666015625, 0.0, 0.025390625, 0.0, 0.0, 0.0, 0.01171875, 0.005859375, 0.0, 0.005859375, 0.0, 0.0, 0.3046875, 0.0, 0.0390625, 0.001953125, 0.0, 0.0, 0.0, 0.056640625, 0.009765625, 0.0, 0.0, 0.021484375, 0.0, 0.0, 0.021484375, 0.0, 0.0234375, 0.0, 0.001953125, 0.021484375, 0.013671875, 0.0078125, 0.0, 0.0, 0.017578125, 0.0, 0.021484375, 0.0, 0.005859375, 0.02734375, 0.0, 0.013671875, 0.017578125, 0.048828125, 0.0078125, 0.0, 0.0, 0.017578125, 0.001953125, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.02734375, 0.0, 0.0, 0.03125, 0.08203125, 0.0, 0.0, 0.0, 0.017578125, 0.009765625, 0.0, 0.001953125, 0.033203125, 0.337890625, 0.4296875, 0.0, 0.00390625, 0.0234375, 0.0078125, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.03125, 0.0, 0.021484375, 0.0, 0.015625, 0.0, 0.029296875, 0.05859375, 0.0, 0.001953125, 0.017578125, 0.001953125, 0.0, 0.0, 0.0, 0.013671875, 0.017578125, 0.001953125, 0.0, 0.0, 0.021484375, 0.154296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.01171875, 0.00390625, 0.0, 0.025390625, 0.0, 0.0, 0.0, 0.01953125, 0.0, 0.0, 0.01171875, 0.0390625, 0.037109375, 0.0, 0.0, 0.013671875, 0.01171875, 0.0, 0.0, 0.017578125, 0.0, 0.015625, 0.916015625, 0.0, 0.0, 0.0, 0.001953125, 0.00390625, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.01953125, 0.0, 0.0, 0.001953125, 0.9921875, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.033203125, 0.013671875, 0.009765625, 0.0, 0.021484375, 0.029296875, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.44921875, 0.0, 0.0, 0.0, 0.0, 0.056640625, 0.0, 0.0, 0.001953125, 0.0, 0.0078125, 0.013671875, 0.009765625, 0.0, 0.0, 0.001953125, 0.0078125, 0.0, 0.015625, 0.00390625, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.041015625, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.013671875, 0.005859375, 0.0, 0.052734375, 0.052734375, 0.0, 0.0, 0.01171875, 0.021484375, 0.0, 0.0, 0.0, 0.048828125, 0.0, 0.0, 0.025390625, 0.0, 0.021484375, 0.021484375, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.01953125, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.009765625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0078125, 0.0234375, 0.02734375, 0.0, 0.033203125, 0.04296875, 0.0, 0.005859375, 0.0, 0.001953125, 0.0390625, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.001953125, 0.0, 0.005859375, 0.013671875, 0.0, 0.0078125, 0.001953125, 0.013671875, 0.0, 0.0, 0.052734375, 0.01171875, 0.009765625, 0.037109375, 0.009765625, 0.0, 0.00390625, 0.025390625, 0.0, 0.041015625, 0.037109375, 0.03125, 0.0, 0.0, 0.0, 0.01171875, 0.0703125, 0.02734375, 0.0, 0.001953125, 0.01953125, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.025390625, 0.0, 0.0078125, 0.0, 0.015625, 0.001953125, 0.0, 0.0, 0.00390625, 0.0, 0.017578125, 0.0, 0.025390625, 0.00390625, 0.052734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.119140625, 0.005859375, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.00390625, 0.0, 0.013671875, 0.0, 0.0078125, 0.693359375, 0.01171875, 0.03515625, 0.0, 0.01171875, 0.005859375, 0.0, 0.005859375, 0.025390625, 0.001953125, 0.01953125, 0.0234375, 0.021484375, 0.015625, 0.00390625, 0.0, 0.013671875, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.208984375, 0.017578125, 0.0, 0.0234375, 0.19140625, 0.0, 0.068359375, 0.0, 0.001953125, 0.0, 0.0, 0.03125, 0.0625, 0.0, 0.0, 0.015625, 0.0, 0.046875, 0.001953125, 0.08984375, 0.05078125, 0.03125, 0.037109375, 0.0, 0.0, 0.0, 0.119140625, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.02734375, 0.0, 0.021484375, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.017578125, 0.009765625, 0.0, 0.001953125, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.001953125, 0.0, 0.009765625, 0.0, 0.001953125, 0.0, 0.03125, 0.009765625, 0.0, 0.150390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.37890625, 0.0078125, 0.013671875, 0.021484375, 0.017578125, 0.01171875, 0.0, 0.0390625, 0.0, 0.021484375, 0.232421875, 0.001953125, 0.001953125, 0.0, 0.03125, 0.001953125, 0.0, 0.0, 0.646484375, 0.02734375, 0.013671875, 0.0390625, 0.0, 0.0, 0.0, 0.001953125, 0.01171875, 0.001953125, 0.0, 0.0, 0.009765625, 0.017578125, 0.0390625, 0.0234375, 0.001953125, 0.001953125, 0.013671875, 0.0, 0.0234375, 0.009765625, 0.015625, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.00390625, 0.0, 0.0, 0.97265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.00390625, 0.015625, 0.0, 0.01171875, 0.001953125, 0.025390625, 0.0, 0.001953125, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.013671875, 0.154296875, 0.05859375, 0.01171875, 0.00390625, 0.00390625, 0.0, 0.01171875, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0546875, 0.0, 0.001953125, 0.0, 0.0, 0.013671875, 0.005859375, 0.0, 0.0, 0.017578125, 0.0, 0.021484375, 0.00390625, 0.0, 0.0078125, 0.44140625, 0.0390625, 0.0, 0.048828125, 0.001953125, 0.0, 0.001953125, 0.021484375, 0.005859375, 0.0, 0.025390625, 0.03515625, 0.00390625, 0.0, 0.00390625, 0.017578125, 0.23828125, 0.0, 0.0, 0.99609375, 0.0, 0.009765625, 0.0, 0.005859375, 0.0, 0.21875, 0.048828125, 0.0, 0.029296875, 0.0, 0.005859375, 0.01171875, 0.0, 0.037109375, 0.0, 0.0, 0.017578125, 0.029296875, 0.0, 0.00390625, 0.009765625, 0.0, 0.005859375, 0.0, 0.01953125, 0.001953125, 0.919921875, 0.0, 0.0, 0.013671875, 0.0, 0.025390625, 0.0, 0.021484375, 0.02734375, 0.0, 0.033203125, 0.02734375, 0.994140625, 0.00390625, 0.01171875, 0.005859375, 0.005859375, 0.0, 0.005859375, 0.0, 0.068359375, 0.001953125, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.01171875, 0.0, 0.0078125, 0.0, 0.009765625, 0.005859375, 0.029296875, 0.0, 0.0, 0.0, 0.015625, 0.021484375, 0.013671875, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.068359375, 0.03125, 0.0, 0.001953125, 0.0, 0.0, 0.037109375, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.001953125, 0.0, 0.0, 0.083984375, 0.005859375, 0.09765625, 0.005859375, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.009765625, 0.01171875, 0.0078125, 0.025390625, 0.0, 0.0, 0.224609375, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.013671875, 0.18359375, 0.0, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.025390625, 0.0390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.015625, 0.029296875, 0.0, 0.037109375, 0.0, 0.0, 0.0, 0.001953125, 0.029296875, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.0, 0.041015625, 0.009765625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0078125, 0.0, 0.00390625, 0.607421875, 0.0, 0.03125, 0.173828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.03125, 0.0234375, 0.0, 0.0, 0.0, 0.03125, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.005859375, 0.0, 0.001953125, 0.015625, 0.0, 0.013671875, 0.0, 0.013671875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.041015625, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.923828125, 0.0, 0.0, 0.001953125, 0.029296875, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.001953125, 0.060546875, 0.005859375, 0.01953125, 0.00390625, 0.013671875, 0.0, 0.0, 0.017578125, 0.015625, 0.0, 0.005859375, 0.0, 0.0, 0.033203125, 0.0078125, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.02734375, 0.01171875, 0.01171875, 0.99609375, 0.03515625, 0.0, 0.013671875, 0.0, 0.01953125, 0.013671875, 0.0, 0.0, 0.0, 0.017578125, 0.04296875, 0.0, 0.0, 0.015625, 0.0, 0.009765625, 0.986328125, 0.015625, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.001953125, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.009765625, 0.048828125, 0.0, 0.0, 0.0, 0.033203125, 0.0, 0.0, 0.001953125, 0.0078125, 0.0, 0.0, 0.021484375, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.005859375, 0.01953125, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.736328125, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.017578125, 0.0, 0.029296875, 0.01171875, 0.00390625, 0.0, 0.0, 0.01953125, 0.0, 0.03515625, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.04296875, 0.01171875, 0.0, 0.0, 0.03515625, 0.02734375, 0.03125, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.005859375, 0.0, 0.0, 0.01171875, 0.0, 0.025390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.060546875, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.01171875, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.017578125, 0.0, 0.01953125, 0.0, 0.037109375, 0.0234375, 0.044921875, 0.0, 0.005859375, 0.0, 0.029296875, 0.0234375, 0.25, 0.0, 0.01171875, 0.0, 0.01953125, 0.00390625, 0.01953125, 0.0, 0.021484375, 0.015625, 0.23046875, 0.001953125, 0.076171875, 0.03515625, 0.0, 0.017578125, 0.021484375, 0.029296875, 0.00390625, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.001953125, 0.009765625, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.62890625, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.009765625, 0.015625, 0.0, 0.02734375, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.01953125, 0.0, 0.01171875, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.048828125, 0.0, 0.0, 0.0, 0.00390625, 0.001953125, 0.0, 0.013671875, 0.001953125, 0.0, 0.033203125, 0.0, 0.009765625, 0.041015625, 0.03125, 0.0, 0.0, 0.0, 0.025390625, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.17578125, 0.005859375, 0.0, 0.009765625, 0.072265625, 0.03125, 0.00390625, 0.0234375, 0.0, 0.0, 0.013671875, 0.009765625, 0.013671875, 0.001953125, 0.0, 0.009765625, 0.0, 0.00390625, 0.0, 0.015625, 0.0, 0.06640625, 0.0, 0.03125, 0.0, 0.001953125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.021484375, 0.0, 0.0, 0.0, 0.005859375, 0.056640625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.03515625, 0.025390625, 0.0, 0.0, 0.00390625, 0.03515625, 0.03125, 0.0, 0.033203125, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.0, 0.015625, 0.0, 0.05859375, 0.001953125, 0.00390625, 0.0, 0.013671875, 0.005859375, 0.0, 0.5, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.986328125, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.00390625, 0.005859375, 0.0234375, 0.0, 0.0, 0.013671875, 0.048828125, 0.0390625, 0.0, 0.0, 0.029296875, 0.021484375, 0.041015625, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.001953125, 0.013671875, 0.0, 0.0, 0.0]

 sparsity of   [0.02587890625, 0.017578125, 0.03125, 0.03955078125, 0.0, 0.03173828125, 0.0126953125, 0.0458984375, 0.97021484375, 0.05224609375, 0.03271484375, 0.01171875, 0.02978515625, 0.0244140625, 0.0341796875, 0.00048828125, 0.01025390625, 0.0205078125, 0.02001953125, 0.0546875, 0.02685546875, 0.01513671875, 0.01708984375, 0.01611328125, 0.01806640625, 0.04443359375, 0.0400390625, 0.05078125, 0.095703125, 0.056640625, 0.01220703125, 0.03857421875, 0.01171875, 0.85400390625, 0.0146484375, 0.0, 0.0458984375, 0.041015625, 0.00830078125, 0.99951171875, 0.24169921875, 0.02880859375, 0.02880859375, 0.0390625, 0.05712890625, 0.029296875, 0.01953125, 0.03173828125, 0.0380859375, 0.0830078125, 0.01318359375, 0.00830078125, 0.03857421875, 0.033203125, 0.9140625, 0.0234375, 0.0, 0.03076171875, 0.03515625, 0.025390625, 0.12744140625, 0.01953125, 0.0341796875, 0.02197265625, 0.08154296875, 0.0439453125, 0.080078125, 0.0234375, 0.90478515625, 0.0107421875, 0.03564453125, 0.02294921875, 0.0244140625, 0.04443359375, 0.0234375, 0.0126953125, 0.017578125, 0.0517578125, 0.064453125, 0.001953125, 0.56787109375, 0.0595703125, 0.625, 0.0576171875, 0.02880859375, 0.009765625, 0.01513671875, 0.03857421875, 0.03466796875, 0.02490234375, 0.0458984375, 0.10009765625, 0.01611328125, 0.00439453125, 0.0, 0.02490234375, 0.013671875, 0.033203125, 0.02685546875, 0.02685546875, 0.03076171875, 0.02001953125, 0.0361328125, 0.0, 0.060546875, 0.02197265625, 0.02783203125, 0.021484375, 0.03173828125, 0.0, 0.0380859375, 0.0380859375, 0.04345703125, 0.0712890625, 0.0, 0.0458984375, 0.0, 0.0341796875, 0.03076171875, 0.01025390625, 0.1455078125, 0.046875, 0.04541015625, 0.04052734375, 0.00830078125, 0.00048828125, 0.0419921875, 0.02978515625, 0.0283203125, 0.03271484375, 0.03369140625, 0.02783203125, 0.013671875, 0.0205078125, 0.0400390625, 0.00927734375, 0.015625, 0.099609375, 0.041015625, 0.009765625, 0.03076171875, 0.02294921875, 0.0361328125, 0.015625, 0.01708984375, 0.0302734375, 0.0390625, 0.02880859375, 0.0419921875, 0.0244140625, 0.0419921875, 0.884765625, 0.02099609375, 0.02734375, 0.0927734375, 0.00048828125, 0.03076171875, 0.08984375, 0.0244140625, 0.0361328125, 0.01025390625, 0.0390625, 0.44482421875, 0.04736328125, 0.04931640625, 0.01123046875, 0.00244140625, 0.04150390625, 0.0283203125, 0.02685546875, 0.025390625, 0.01416015625, 0.03125, 0.099609375, 0.0029296875, 0.0205078125, 0.0224609375, 0.02880859375, 0.03564453125, 0.0595703125, 0.02099609375, 0.76611328125, 0.00927734375, 0.04345703125, 0.03369140625, 0.03662109375, 0.03125, 0.03076171875, 0.0263671875, 0.0888671875, 0.00146484375, 0.01806640625, 0.0380859375, 0.029296875, 0.3740234375, 0.00048828125, 0.4189453125, 0.0234375, 0.01220703125, 0.02685546875, 0.03857421875, 0.037109375, 0.04443359375, 0.01611328125, 0.01025390625, 0.0, 0.02685546875, 0.0380859375, 0.0224609375, 0.9990234375, 0.02880859375, 0.037109375, 0.07568359375, 0.0, 0.0322265625, 0.0009765625, 0.00048828125, 0.45263671875, 0.03173828125, 0.02587890625, 0.02978515625, 0.03564453125, 0.0595703125, 0.01513671875, 0.04736328125, 0.03564453125, 0.0146484375, 0.00146484375, 0.03564453125, 0.02880859375, 0.02783203125, 0.03125, 0.0908203125, 0.029296875, 0.03466796875, 0.017578125, 0.02392578125, 0.01513671875, 0.02392578125, 0.02587890625, 0.03857421875, 0.04638671875, 0.025390625, 0.07177734375, 0.0302734375, 0.025390625, 0.0224609375, 0.0400390625, 0.04638671875, 0.017578125, 0.03173828125, 0.009765625, 0.0146484375, 0.49462890625, 0.87353515625, 0.02978515625, 0.02490234375, 0.03515625, 0.91357421875, 0.05029296875, 0.07763671875, 0.03076171875, 0.029296875, 0.0732421875, 0.01513671875, 0.017578125, 0.099609375, 0.11328125, 0.04150390625, 0.0341796875, 0.033203125, 0.0048828125, 0.01513671875, 0.04345703125, 0.00048828125, 0.03369140625, 0.93603515625, 0.046875, 0.0107421875, 0.0390625, 0.04296875, 0.0048828125, 0.724609375, 0.02783203125, 0.0224609375, 0.0390625, 0.03466796875, 0.021484375, 0.0166015625, 0.01318359375, 0.0078125, 0.5146484375, 0.0283203125, 0.13037109375, 0.04931640625, 0.0205078125, 0.65673828125, 0.03173828125, 0.046875, 0.90087890625, 0.03759765625, 0.0, 0.06298828125, 0.0400390625, 0.0322265625, 0.04052734375, 0.033203125, 0.0859375, 0.02880859375, 0.02685546875, 0.94775390625, 0.02978515625, 0.0, 0.03271484375, 0.01611328125, 0.14013671875, 0.02001953125, 0.0302734375, 0.04443359375, 0.0009765625, 0.078125, 0.0322265625, 0.04296875, 0.04052734375, 0.03466796875, 0.02783203125, 0.02490234375, 0.0, 0.9990234375, 0.041015625, 0.00048828125, 0.0087890625, 0.4287109375, 0.02490234375, 0.0, 0.06201171875, 0.037109375, 0.04248046875, 0.021484375, 0.0146484375, 0.02587890625, 0.02734375, 0.04052734375, 0.0, 0.021484375, 0.00537109375, 0.01953125, 0.02001953125, 0.029296875, 0.02783203125, 0.01416015625, 0.02099609375, 0.03173828125, 0.0400390625, 0.55908203125, 0.77197265625, 0.01708984375, 0.0302734375, 0.07421875, 0.0390625, 0.025390625, 0.78369140625, 0.03173828125, 0.01806640625, 0.02001953125, 0.025390625, 0.103515625, 0.0615234375, 0.04345703125, 0.0380859375, 0.04296875, 0.02490234375, 0.57177734375, 0.4658203125, 0.0263671875, 0.76171875, 0.06396484375, 0.00830078125, 0.0380859375, 0.01513671875, 0.04541015625, 0.46240234375, 0.0, 0.02783203125, 0.03759765625, 0.03173828125, 0.66943359375, 0.04052734375, 0.03564453125, 0.06298828125, 0.021484375, 0.99951171875, 0.02490234375, 0.05908203125, 0.13818359375, 0.04931640625, 0.04638671875, 0.05615234375, 0.03515625, 0.02978515625, 0.03759765625, 0.03662109375, 0.0302734375, 0.01025390625, 0.0341796875, 0.248046875, 0.03076171875, 0.0107421875, 0.00927734375, 0.0390625, 0.04736328125, 0.0302734375, 0.03759765625, 0.0087890625, 0.015625, 0.00048828125, 0.021484375, 0.0185546875, 0.05859375, 0.0234375, 0.037109375, 0.01123046875, 0.009765625, 0.0205078125, 0.39501953125, 0.0751953125, 0.59765625, 0.0107421875, 0.02197265625, 0.02099609375, 0.00244140625, 0.03662109375, 0.01025390625, 0.0, 0.02197265625, 0.048828125, 0.04150390625, 0.05712890625, 0.048828125, 0.02734375, 0.03759765625, 0.041015625, 0.03466796875, 0.037109375, 0.017578125, 0.0205078125, 0.04638671875, 0.03515625, 0.51806640625, 0.03076171875, 0.86767578125, 0.03076171875, 0.01708984375, 0.01953125, 0.03955078125, 0.02880859375, 0.17431640625, 0.02392578125, 0.03466796875, 0.0400390625, 0.0078125, 0.01513671875, 0.02880859375, 0.0400390625, 0.01806640625, 0.0546875, 0.029296875, 0.02734375, 0.1328125, 0.01220703125, 0.0283203125, 0.0302734375, 0.65869140625, 0.05029296875, 0.0703125, 0.015625, 0.0, 0.0, 0.03466796875, 0.96923828125, 0.0322265625, 0.02197265625, 0.99609375, 0.0263671875, 0.06005859375, 0.00732421875, 0.048828125, 0.02392578125, 0.1025390625, 0.00146484375, 0.20849609375, 0.00048828125, 0.0, 0.03173828125, 0.03173828125, 0.0126953125, 0.01171875, 0.0283203125, 0.044921875, 0.00439453125, 0.2177734375, 0.03515625, 0.03125, 0.00048828125, 0.02392578125, 0.0361328125, 0.013671875, 0.02880859375, 0.021484375, 0.01416015625, 0.06396484375, 0.05810546875]

 sparsity of   [0.00434027798473835, 0.0, 0.0407986119389534, 0.0941840261220932, 0.0627170130610466, 0.00021701389050576836, 0.0, 0.0034722222480922937, 0.0, 0.0679253488779068, 0.0, 0.0, 0.0, 0.0, 0.0614149309694767, 0.0616319440305233, 0.0, 0.6959635615348816, 0.0, 0.0, 0.0, 0.0, 0.0327690988779068, 0.0, 0.0071614584885537624, 0.0030381944961845875, 0.00021701389050576836, 0.0, 0.078993059694767, 0.0243055559694767, 0.0, 0.0, 0.02973090298473835, 0.0336371548473835, 0.00021701389050576836, 0.0568576380610466, 0.0475260429084301, 0.0670572891831398, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.0, 0.009114583022892475, 0.0, 0.009114583022892475, 0.0384114570915699, 0.00021701389050576836, 0.00021701389050576836, 0.0034722222480922937, 0.0030381944961845875, 0.0225694440305233, 0.00021701389050576836, 0.0822482630610466, 0.0349392369389534, 0.0870225727558136, 0.0004340277810115367, 0.0, 0.00390625, 0.0, 0.013888888992369175, 0.0, 0.0264756940305233, 0.00021701389050576836, 0.0, 0.0609809048473835, 0.010416666977107525, 0.1037326380610466, 0.0, 0.0, 0.001953125, 0.00021701389050576836, 0.7348090410232544, 0.00021701389050576836, 0.0, 0.0032552082557231188, 0.0010850694961845875, 0.00021701389050576836, 0.0049913194961845875, 0.0045572915114462376, 0.0, 0.0026041667442768812, 0.0394965298473835, 0.013671875, 0.0, 0.02278645895421505, 0.0, 0.0, 0.0290798619389534, 0.008897569961845875, 0.0301649309694767, 0.0, 0.0802951380610466, 0.0477430559694767, 0.01714409701526165, 0.0, 0.0, 0.0, 0.005859375, 0.007378472480922937, 0.0, 0.0032552082557231188, 0.0418836809694767, 0.0405815988779068, 0.0030381944961845875, 0.00021701389050576836, 0.9997829794883728, 0.0533854179084301, 0.01019965298473835, 0.009548611007630825, 0.0, 0.0008680555620230734, 0.0, 0.02018229104578495, 0.0030381944961845875, 0.0460069440305233, 0.0, 0.02408854104578495, 0.004123263992369175, 0.0106336809694767, 0.0646701380610466, 0.0, 0.02734375, 0.00021701389050576836, 0.009114583022892475, 0.01909722201526165, 0.0232204869389534, 0.0980902761220932, 0.015407986007630825, 0.00021701389050576836, 0.1527777761220932, 0.0559895820915699, 0.00021701389050576836, 0.0336371548473835, 0.0164930559694767, 0.1069878488779068, 0.0234375, 0.0, 0.0329861119389534, 0.0, 0.0, 0.00021701389050576836, 0.0564236119389534, 0.012803819961845875, 0.00021701389050576836, 0.00021701389050576836, 0.01128472201526165, 0.0763888880610466, 0.012803819961845875, 0.0427517369389534, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.0863715261220932, 0.00021701389050576836, 0.0, 0.0444878488779068, 0.0067274305038154125, 0.00021701389050576836, 0.046875, 0.0032552082557231188, 0.0939670130610466, 0.0010850694961845875, 0.00021701389050576836, 0.013020833022892475, 0.1002604141831398, 0.0384114570915699, 0.011067708022892475, 0.0349392369389534, 0.0347222238779068, 0.0533854179084301, 0.1098090261220932, 0.00021701389050576836, 0.2990451455116272, 0.01801215298473835, 0.0, 0.02170138992369175, 0.0, 0.011501736007630825, 0.0353732630610466, 0.0358072929084301, 0.0049913194961845875, 0.00021701389050576836, 0.00021701389050576836, 0.037109375, 0.0, 0.0690104141831398, 0.029296875, 0.16015625, 0.0, 0.0, 0.009114583022892475, 0.001953125, 0.0364583320915699, 0.00021701389050576836, 0.00021701389050576836, 0.0004340277810115367, 0.0651041641831398, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.0349392369389534, 0.00933159701526165, 0.005425347480922937, 0.0, 0.0737847238779068, 0.0184461809694767, 0.098524309694767, 0.1807725727558136, 0.00021701389050576836, 0.0017361111240461469, 0.0, 0.0069444444961845875, 0.00021701389050576836, 0.8797743320465088, 0.063368059694767, 0.02994791604578495, 0.0067274305038154125, 0.0655381977558136, 0.0026041667442768812, 0.0, 0.0303819440305233, 0.00021701389050576836, 0.0184461809694767, 0.0596788190305233, 0.0, 0.0373263880610466, 0.2198350727558136, 0.0425347238779068, 0.01974826492369175, 0.0412326380610466, 0.02756076492369175, 0.0401475690305233, 0.00390625, 0.125, 0.02105034701526165, 0.0026041667442768812, 0.0004340277810115367, 0.8580729365348816, 0.0651041641831398, 0.0407986119389534, 0.0609809048473835, 0.00629340298473835, 0.5418837070465088, 0.0314670130610466, 0.00021701389050576836, 0.0373263880610466, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.00434027798473835, 0.00021701389050576836, 0.00021701389050576836, 0.01605902798473835, 0.00629340298473835, 0.0086805559694767, 0.0023871527519077063, 0.0028211805038154125, 0.00021701389050576836, 0.0703125, 0.0030381944961845875, 0.0004340277810115367, 0.0251736119389534, 0.00021701389050576836, 0.0, 0.014322916977107525, 0.0, 0.0023871527519077063, 0.0006510416860692203, 0.02300347201526165, 0.1013454869389534, 0.0462239570915699, 0.0045572915114462376, 0.0, 0.02170138992369175, 0.0, 0.0, 0.1169704869389534, 0.011935763992369175, 0.0017361111240461469, 0.0, 0.0284288190305233, 0.1623263955116272, 0.013020833022892475, 0.00021701389050576836, 0.00021701389050576836, 0.007595486007630825, 0.0863715261220932, 0.0, 0.0720486119389534, 0.008463541977107525, 0.00021701389050576836, 0.0, 0.0, 0.01996527798473835, 0.015625, 0.0407986119389534, 0.0, 0.02973090298473835, 0.0, 0.00021701389050576836, 0.0010850694961845875, 0.0004340277810115367, 0.0568576380610466, 0.1963975727558136, 0.0234375, 0.00021701389050576836, 0.119140625, 0.02669270895421505, 0.00021701389050576836, 0.02083333395421505, 0.0384114570915699, 0.0618489570915699, 0.0368923619389534, 0.0, 0.0, 0.0086805559694767, 0.9982638955116272, 0.00021701389050576836, 0.0, 0.015407986007630825, 0.0078125, 0.00021701389050576836, 0.0015190972480922937, 0.1061197891831398, 0.0377604179084301, 0.0557725690305233, 0.0065104165114462376, 0.0008680555620230734, 0.0661892369389534, 0.0, 0.0, 0.0301649309694767, 0.0, 0.0, 0.6733940839767456, 0.0004340277810115367, 0.0, 0.044921875, 0.0, 0.0642361119389534, 0.0, 0.0620659738779068, 0.0, 0.0, 0.6391059160232544, 0.1447482705116272, 0.0603298619389534, 0.0670572891831398, 0.00021701389050576836, 0.55078125, 0.0030381944961845875, 0.1484375, 0.013671875, 0.0303819440305233, 0.0, 0.0523003488779068, 0.0603298619389534, 0.00021701389050576836, 0.0941840261220932, 0.0004340277810115367, 0.0, 0.02560763992369175, 0.00021701389050576836, 0.02018229104578495, 0.0004340277810115367, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.0, 0.01128472201526165, 0.0414496548473835, 0.00021701389050576836, 0.0, 0.00021701389050576836, 0.0164930559694767, 0.0, 0.0, 0.013020833022892475, 0.0, 0.0, 0.0577256940305233, 0.0034722222480922937, 0.0, 0.0, 0.3111979067325592, 0.0340711809694767, 0.0004340277810115367, 0.0629340261220932, 0.0557725690305233, 0.0006510416860692203, 0.0271267369389534, 0.0473090298473835, 0.0837673619389534, 0.1421440988779068, 0.0303819440305233, 0.0, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.00434027798473835, 0.1134982630610466, 0.0010850694961845875, 0.0, 0.0321180559694767, 0.01019965298473835, 0.0696614608168602, 0.0, 0.0, 0.02191840298473835, 0.00824652798473835, 0.0184461809694767, 0.0034722222480922937, 0.0, 0.0631510391831398, 0.0086805559694767, 0.7289496660232544, 0.0225694440305233, 0.0004340277810115367, 0.0, 0.3070746660232544, 0.0, 0.0, 0.03081597201526165, 0.0, 0.0, 0.00629340298473835, 0.0418836809694767, 0.0540364570915699, 0.0008680555620230734, 0.00021701389050576836, 0.0, 0.00021701389050576836, 0.0559895820915699, 0.0, 0.0206163190305233, 0.0, 0.0078125, 0.0, 0.00021701389050576836, 0.1391059011220932, 0.0028211805038154125, 0.5726996660232544, 0.0173611119389534, 0.0006510416860692203, 0.007595486007630825, 0.0, 0.0336371548473835, 0.01996527798473835, 0.0944010391831398, 0.02083333395421505, 0.0301649309694767, 0.0184461809694767, 0.4329427182674408, 0.283203125, 0.0505642369389534, 0.0, 0.0, 0.0763888880610466, 0.00021701389050576836, 0.0698784738779068, 0.8821614384651184, 0.00021701389050576836, 0.0423177070915699, 0.0036892362404614687, 0.00021701389050576836, 0.0, 0.3036024272441864, 0.0, 0.00434027798473835, 0.00021701389050576836, 0.00021701389050576836, 0.0366753488779068, 0.01605902798473835, 0.0, 0.0, 0.3302951455116272, 0.0, 0.0069444444961845875, 0.0, 0.0, 0.0859375, 0.166015625, 0.0284288190305233, 0.0451388880610466, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.005859375, 0.0629340261220932, 0.0, 0.00021701389050576836, 0.02191840298473835, 0.01627604104578495, 0.0, 0.00021701389050576836, 0.0759548619389534, 0.00021701389050576836, 0.0, 0.0, 0.008463541977107525, 0.008029513992369175, 0.01519097201526165, 0.0431857630610466, 0.02170138992369175, 0.004123263992369175, 0.0, 0.00021701389050576836, 0.0, 0.0321180559694767, 0.02300347201526165]

 sparsity of   [0.0, 0.056640625, 0.091796875, 0.03515625, 0.056640625, 0.119140625, 0.54296875, 0.0078125, 0.994140625, 0.015625, 0.02734375, 0.00390625, 0.052734375, 0.0390625, 0.01953125, 0.005859375, 0.08984375, 0.99609375, 0.89453125, 0.009765625, 0.30859375, 0.0078125, 0.4609375, 0.01171875, 0.005859375, 0.052734375, 0.87890625, 0.0390625, 0.0625, 0.025390625, 0.033203125, 0.056640625, 0.99609375, 0.001953125, 0.86328125, 0.01953125, 0.015625, 0.328125, 0.4453125, 0.0546875, 0.01953125, 0.02734375, 0.0234375, 0.0859375, 0.01171875, 0.265625, 0.994140625, 0.14453125, 0.076171875, 0.046875, 0.083984375, 0.01171875, 0.025390625, 0.01171875, 0.099609375, 0.857421875, 0.044921875, 0.205078125, 0.99609375, 0.017578125, 0.046875, 0.763671875, 0.166015625, 0.888671875, 0.001953125, 0.046875, 0.033203125, 0.2109375, 0.0078125, 0.01953125, 0.0234375, 0.005859375, 0.064453125, 0.873046875, 0.013671875, 0.068359375, 0.00390625, 0.16796875, 0.064453125, 0.580078125, 0.033203125, 0.03515625, 0.00390625, 0.009765625, 0.0078125, 0.125, 0.025390625, 0.79296875, 0.904296875, 0.005859375, 0.015625, 0.00390625, 0.09765625, 0.046875, 0.2265625, 0.412109375, 0.10546875, 0.021484375, 0.154296875, 0.021484375, 0.994140625, 0.423828125, 0.296875, 0.994140625, 0.01171875, 0.03515625, 0.017578125, 0.00390625, 0.134765625, 0.701171875, 0.09765625, 0.013671875, 0.052734375, 0.044921875, 0.728515625, 0.140625, 0.037109375, 0.04296875, 0.103515625, 0.0390625, 0.88671875, 0.064453125, 0.0, 0.0234375, 0.1484375, 0.01171875, 0.1328125, 0.015625, 0.560546875, 0.025390625, 0.99609375, 0.611328125, 0.109375, 0.07421875, 0.046875, 0.0, 0.1171875, 0.02734375, 0.12890625, 0.009765625, 0.0078125, 0.80859375, 0.28515625, 0.076171875, 0.017578125, 0.0859375, 0.01953125, 0.0078125, 0.099609375, 0.73828125, 0.119140625, 0.068359375, 0.02734375, 0.083984375, 0.916015625, 0.013671875, 0.041015625, 0.537109375, 0.025390625, 0.009765625, 0.013671875, 0.171875, 0.02734375, 0.109375, 0.01171875, 0.11328125, 0.005859375, 0.07421875, 0.44921875, 0.0546875, 0.01953125, 0.787109375, 0.015625, 0.5234375, 0.046875, 0.0390625, 0.70703125, 0.0546875, 0.03125, 0.84765625, 0.01953125, 0.03515625, 0.021484375, 0.01953125, 0.01953125, 0.01953125, 0.005859375, 0.40234375, 0.041015625, 0.130859375, 0.85546875, 0.080078125, 0.994140625, 0.064453125, 0.037109375, 0.9140625, 0.041015625, 0.0, 0.033203125, 0.078125, 0.029296875, 0.041015625, 0.880859375, 0.021484375, 0.064453125, 0.04296875, 0.041015625, 0.125, 0.0703125, 0.16015625, 0.736328125, 0.029296875, 0.021484375, 0.73046875, 0.052734375, 0.03125, 0.005859375, 0.037109375, 0.42578125, 0.65234375, 0.21484375, 0.107421875, 0.994140625, 0.041015625, 0.0859375, 0.02734375, 0.005859375, 0.078125, 0.947265625, 0.033203125, 0.05078125, 0.0234375, 0.025390625, 0.998046875, 0.0390625, 0.005859375, 0.048828125, 0.0546875, 0.005859375, 0.63671875, 0.046875, 0.0234375, 0.0546875, 0.08984375, 0.009765625, 0.048828125, 0.037109375, 0.787109375, 0.033203125, 0.26953125, 0.005859375, 0.076171875, 0.080078125, 0.0234375, 0.66796875, 0.01171875, 0.0078125, 0.01953125, 0.033203125, 0.01953125, 0.072265625, 0.59375, 0.072265625, 0.078125, 0.05078125, 0.216796875, 0.052734375, 0.037109375, 0.029296875, 0.103515625, 0.03515625, 0.0078125, 0.01953125, 0.185546875, 0.271484375, 0.01171875, 0.017578125, 0.109375, 0.87890625, 0.005859375, 0.990234375, 0.09375, 0.126953125, 0.140625, 0.091796875, 0.166015625, 0.068359375, 0.529296875, 0.064453125, 0.056640625, 0.64453125, 0.005859375, 0.041015625, 0.044921875, 0.0234375, 0.04296875, 0.09765625, 0.95703125, 0.171875, 0.01953125, 0.11328125, 0.01953125, 0.037109375, 0.66015625, 0.0546875, 0.150390625, 0.0078125, 0.060546875, 0.029296875, 0.80859375, 0.04296875, 0.09375, 0.0546875, 0.005859375, 0.96484375, 0.064453125, 0.0546875, 0.0703125, 0.92578125, 0.025390625, 0.08984375, 0.076171875, 0.84375, 0.072265625, 0.021484375, 0.03125, 0.033203125, 0.015625, 0.18359375, 0.017578125, 0.099609375, 0.060546875, 0.0390625, 0.06640625, 0.01953125, 0.13671875, 0.029296875, 0.009765625, 0.015625, 0.796875, 0.052734375, 0.005859375, 0.00390625, 0.01171875, 0.85546875, 0.115234375, 0.046875, 0.130859375, 0.744140625, 0.04296875, 0.0078125, 0.185546875, 0.19140625, 0.037109375, 0.01171875, 0.099609375, 0.00390625, 0.0390625, 0.087890625, 0.013671875, 0.044921875, 0.01953125, 0.83984375, 0.060546875, 0.01953125, 0.025390625, 0.330078125, 0.689453125, 0.146484375, 0.0234375, 0.755859375, 0.005859375, 0.015625, 0.001953125, 0.140625, 0.07421875, 0.029296875, 0.01953125, 0.05859375, 0.064453125, 0.978515625, 0.013671875, 0.1015625, 0.0546875, 0.0078125, 0.19921875, 0.994140625, 0.017578125, 0.201171875, 0.03515625, 0.40625, 0.009765625, 0.029296875, 0.03125, 0.052734375, 0.001953125, 0.8203125, 0.109375, 0.0234375, 0.078125, 0.05859375, 0.111328125, 0.994140625, 0.0859375, 0.064453125, 0.486328125, 0.01953125, 0.13671875, 0.005859375, 0.998046875, 0.169921875, 0.013671875, 0.220703125, 0.076171875, 0.0, 0.298828125, 0.0234375, 0.146484375, 0.03125, 0.00390625, 0.044921875, 0.06640625, 0.03125, 0.10546875, 0.361328125, 0.037109375, 0.005859375, 0.009765625, 0.21484375, 0.029296875, 0.041015625, 0.046875, 0.00390625, 0.12890625, 0.0078125, 0.248046875, 0.01171875, 0.0390625, 0.033203125, 0.01171875, 0.05859375, 0.142578125, 0.078125, 0.02734375, 0.630859375, 0.994140625, 0.017578125, 0.0078125, 0.0234375, 0.025390625, 0.009765625, 0.15625, 0.021484375, 0.537109375, 0.904296875, 0.890625, 0.783203125, 0.021484375, 0.068359375, 0.02734375, 0.041015625, 0.029296875, 0.044921875, 0.142578125, 0.0546875, 0.013671875, 0.068359375, 0.0390625, 0.046875, 0.541015625, 0.05859375, 0.0390625, 0.23828125, 0.115234375, 0.0078125, 0.013671875, 0.03515625, 0.826171875, 0.056640625, 0.052734375, 0.1796875, 0.119140625, 0.0859375, 0.7578125, 0.009765625, 0.0078125, 0.021484375, 0.02734375, 0.044921875, 0.015625, 0.021484375, 0.029296875, 0.115234375, 0.087890625, 0.095703125, 0.115234375, 0.10546875, 0.103515625, 0.083984375, 0.1328125, 0.009765625, 0.029296875, 0.056640625, 0.998046875, 0.072265625, 0.10546875, 0.0703125, 0.072265625, 0.015625, 0.00390625, 0.03515625, 0.013671875, 0.859375, 0.091796875, 0.998046875, 0.05078125, 0.09375, 0.994140625, 0.013671875, 0.25390625, 0.03515625, 0.009765625, 0.0, 0.01171875, 0.15234375, 0.03515625, 0.03515625, 0.0234375, 0.083984375, 0.080078125, 0.068359375, 0.62109375, 0.052734375, 0.025390625, 0.03125, 0.009765625, 0.01171875, 0.03515625, 0.08203125, 0.048828125, 0.02734375, 0.0859375, 0.017578125, 0.04296875, 0.033203125, 0.08203125, 0.0, 0.015625, 0.044921875, 0.19140625, 0.0078125, 0.017578125, 0.078125, 0.01171875, 0.99609375, 0.19140625, 0.08984375, 0.119140625, 0.037109375, 0.060546875, 0.0546875, 0.033203125, 0.009765625, 0.05859375, 0.958984375, 0.6953125, 0.017578125, 0.13671875, 0.853515625, 0.123046875, 0.029296875, 0.943359375, 0.125, 0.046875, 0.02734375, 0.025390625, 0.013671875, 0.001953125, 0.0859375, 0.015625, 0.013671875, 0.0703125, 0.193359375, 0.0625, 0.02734375, 0.265625, 0.373046875, 0.1484375, 0.01953125, 0.001953125, 0.0546875, 0.30078125, 0.015625, 0.02734375, 0.0234375, 0.53125, 0.197265625, 0.154296875, 0.994140625, 0.994140625, 0.041015625, 0.994140625, 0.130859375, 0.052734375, 0.029296875, 0.037109375, 0.083984375, 0.044921875, 0.099609375, 0.03125, 0.025390625, 0.041015625, 0.947265625, 0.052734375, 0.08203125, 0.103515625, 0.994140625, 0.080078125, 0.994140625, 0.107421875, 0.01171875, 0.0234375, 0.0, 0.12890625, 0.626953125, 0.015625, 0.02734375, 0.0390625, 0.060546875, 0.044921875, 0.0078125, 0.12109375, 0.0625, 0.052734375, 0.03515625, 0.044921875, 0.013671875, 0.037109375, 0.033203125, 0.0234375, 0.01171875, 0.0234375, 0.658203125, 0.04296875, 0.552734375, 0.03125, 0.994140625, 0.0, 0.005859375, 0.009765625, 0.1171875, 0.0078125, 0.02734375, 0.166015625, 0.009765625, 0.890625, 0.029296875, 0.5078125, 0.03125, 0.044921875, 0.025390625, 0.109375, 0.09765625, 0.08203125, 0.1953125, 0.05859375, 0.017578125, 0.1015625, 0.029296875, 0.009765625, 0.08203125, 0.0, 0.130859375, 0.06640625, 0.76953125, 0.00390625, 0.79296875, 0.044921875, 0.115234375, 0.005859375, 0.080078125, 0.005859375, 0.060546875, 0.087890625, 0.048828125, 0.0078125, 0.25390625, 0.751953125, 0.005859375, 0.0078125, 0.041015625, 0.005859375, 0.79296875, 0.111328125, 0.099609375, 0.0625, 0.072265625, 0.064453125, 0.732421875, 0.0078125, 0.126953125, 0.125, 0.0234375, 0.2734375, 0.037109375, 0.0078125, 0.041015625, 0.15625, 0.041015625, 0.06640625, 0.736328125, 0.1015625, 0.0078125, 0.994140625, 0.291015625, 0.201171875, 0.337890625, 0.755859375, 0.00390625, 0.001953125, 0.01953125, 0.2421875, 0.01953125, 0.0703125, 0.033203125, 0.009765625, 0.248046875, 0.810546875, 0.78125, 0.05078125, 0.017578125, 0.625, 0.14453125, 0.0546875, 0.025390625, 0.048828125, 0.00390625, 0.005859375, 0.064453125, 0.103515625, 0.017578125, 0.111328125, 0.15625, 0.02734375, 0.0, 0.06640625, 0.037109375, 0.015625, 0.5546875, 0.0546875, 0.119140625, 0.037109375, 0.85546875, 0.0390625, 0.041015625, 0.005859375, 0.12890625, 0.013671875, 0.001953125, 0.021484375, 0.009765625, 0.0390625, 0.01953125, 0.181640625, 0.029296875, 0.99609375, 0.009765625, 0.02734375, 0.029296875, 0.0078125, 0.009765625, 0.771484375, 0.869140625, 0.01953125, 0.03125, 0.00390625, 0.029296875, 0.0234375, 0.166015625, 0.994140625, 0.064453125, 0.04296875, 0.037109375, 0.02734375, 0.994140625, 0.0234375, 0.169921875, 0.01953125, 0.228515625, 0.009765625, 0.0390625, 0.146484375, 0.10546875, 0.013671875, 0.263671875, 0.013671875, 0.994140625, 0.2109375, 0.908203125, 0.03515625, 0.099609375, 0.017578125, 0.994140625, 0.001953125, 0.017578125, 0.03515625, 0.90625, 0.064453125, 0.009765625, 0.046875, 0.0234375, 0.521484375, 0.078125, 0.009765625, 0.005859375, 0.0625, 0.052734375, 0.013671875, 0.0078125, 0.052734375, 0.314453125, 0.1171875, 0.99609375, 0.029296875, 0.623046875, 0.046875, 0.111328125, 0.048828125, 0.048828125, 0.091796875, 0.830078125, 0.8359375, 0.017578125, 0.021484375, 0.017578125, 0.021484375, 0.0, 0.189453125, 0.00390625, 0.015625, 0.00390625, 0.041015625, 0.19140625, 0.01953125, 0.267578125, 0.013671875, 0.021484375, 0.90625, 0.09375, 0.0234375, 0.05859375, 0.076171875, 0.994140625, 0.037109375, 0.0078125, 0.171875, 0.01171875, 0.021484375, 0.01953125, 0.021484375, 0.04296875, 0.001953125, 0.021484375, 0.005859375, 0.091796875, 0.0625, 0.138671875, 0.677734375, 0.52734375, 0.0234375, 0.470703125, 0.044921875, 0.994140625, 0.05078125, 0.033203125, 0.080078125, 0.10546875, 0.025390625, 0.208984375, 0.669921875, 0.02734375, 0.025390625, 0.041015625, 0.02734375, 0.001953125, 0.015625, 0.005859375, 0.015625, 0.0234375, 0.84765625, 0.015625, 0.099609375, 0.021484375, 0.001953125, 0.001953125, 0.9375, 0.892578125, 0.001953125, 0.173828125, 0.01171875, 0.103515625, 0.03125, 0.033203125, 0.068359375, 0.021484375, 0.07421875, 0.060546875, 0.208984375, 0.1875, 0.025390625, 0.01171875, 0.0703125, 0.060546875, 0.994140625, 0.125, 0.068359375, 0.021484375, 0.001953125, 0.1171875, 0.015625, 0.296875, 0.033203125, 0.09375, 0.025390625, 0.01171875, 0.005859375, 0.12109375, 0.021484375, 0.01953125, 0.8359375, 0.03125, 0.05078125, 0.046875, 0.087890625, 0.1953125, 0.0390625, 0.888671875, 0.0078125, 0.998046875, 0.03125, 0.037109375, 0.99609375, 0.1640625, 0.06640625, 0.009765625, 0.015625, 0.814453125, 0.09765625, 0.060546875, 0.046875, 0.123046875, 0.11328125, 0.052734375, 0.013671875, 0.0, 0.166015625, 0.04296875, 0.041015625, 0.0078125, 0.0859375, 0.044921875, 0.05859375, 0.853515625, 0.072265625, 0.0234375, 0.025390625, 0.025390625, 0.544921875, 0.0078125, 0.046875, 0.025390625, 0.04296875, 0.08984375, 0.099609375, 0.046875, 0.04296875, 0.130859375, 0.01953125, 0.0390625, 0.0078125, 0.025390625, 0.037109375, 0.06640625, 0.01171875, 0.994140625, 0.0078125, 0.29296875, 0.00390625, 0.998046875, 0.0234375, 0.04296875, 0.193359375, 0.15234375, 0.08203125, 0.1328125, 0.994140625, 0.052734375, 0.03125, 0.015625, 0.017578125, 0.03125, 0.013671875, 0.044921875, 0.0390625, 0.048828125, 0.033203125, 0.015625, 0.0390625, 0.02734375, 0.015625, 0.0078125, 0.923828125, 0.994140625, 0.904296875, 0.46484375, 0.15625, 0.998046875, 0.060546875, 0.017578125, 0.029296875, 0.017578125, 0.005859375, 0.029296875, 0.998046875, 0.05859375, 0.0078125, 0.111328125, 0.03125, 0.755859375, 0.111328125, 0.044921875, 0.703125, 0.005859375, 0.1640625, 0.05859375, 0.1015625, 0.013671875, 0.056640625, 0.03125, 0.03515625, 0.07421875, 0.380859375, 0.01953125, 0.591796875, 0.03125, 0.025390625, 0.01171875, 0.037109375, 0.04296875, 0.015625, 0.0625, 0.04296875, 0.01953125, 0.005859375, 0.634765625, 0.169921875, 0.10546875, 0.171875, 0.0, 0.279296875, 0.0859375, 0.28515625, 0.00390625, 0.75, 0.015625, 0.013671875, 0.0546875, 0.015625, 0.017578125, 0.029296875, 0.001953125, 0.93359375, 0.0625, 0.04296875, 0.0234375, 0.123046875, 0.134765625, 0.041015625, 0.763671875, 0.068359375, 0.02734375, 0.064453125, 0.955078125, 0.76171875, 0.01171875, 0.03515625, 0.029296875, 0.1484375, 0.0546875, 0.447265625, 0.044921875, 0.021484375, 0.529296875, 0.072265625, 0.005859375, 0.197265625, 0.033203125, 0.951171875, 0.994140625, 0.0546875, 0.0, 0.1484375, 0.033203125, 0.111328125, 0.044921875, 0.072265625, 0.01171875, 0.013671875, 0.06640625, 0.06640625, 0.00390625, 0.14453125, 0.966796875, 0.01171875, 0.01171875, 0.134765625, 0.99609375, 0.125, 0.041015625, 0.07421875, 0.080078125, 0.466796875, 0.01953125, 0.998046875, 0.013671875, 0.078125, 0.587890625, 0.01171875, 0.29296875, 0.02734375, 0.0078125, 0.09375, 0.04296875, 0.0078125, 0.046875, 0.138671875, 0.009765625, 0.048828125, 0.087890625, 0.0234375, 0.013671875, 0.103515625, 0.015625, 0.041015625, 0.013671875, 0.0546875, 0.046875, 0.998046875, 0.18359375, 0.1015625, 0.01171875, 0.03515625, 0.90234375, 0.013671875, 0.033203125, 0.60546875, 0.04296875, 0.078125, 0.076171875, 0.994140625, 0.017578125, 0.013671875, 0.048828125, 0.19921875, 0.01953125, 0.01953125, 0.021484375, 0.033203125, 0.087890625, 0.078125, 0.005859375, 0.00390625, 0.009765625, 0.361328125, 0.0625, 0.029296875, 0.0234375, 0.025390625, 0.06640625, 0.0625, 0.046875, 0.05859375, 0.005859375, 0.697265625, 0.029296875, 0.05859375, 0.634765625, 0.99609375, 0.994140625, 0.087890625, 0.064453125, 0.998046875, 0.037109375, 0.0234375, 0.23828125, 0.005859375, 0.728515625, 0.974609375, 0.115234375, 0.01953125, 0.021484375, 0.01171875, 0.142578125, 0.861328125, 0.0078125, 0.994140625, 0.0234375, 0.05078125, 0.01953125, 0.994140625, 0.025390625, 0.046875, 0.103515625, 0.015625, 0.92578125, 0.169921875, 0.091796875, 0.015625, 0.0234375, 0.99609375, 0.017578125, 0.02734375, 0.009765625, 0.01953125, 0.013671875, 0.181640625, 0.08984375, 0.029296875, 0.87890625, 0.029296875, 0.671875, 0.259765625, 0.044921875, 0.02734375, 0.310546875, 0.01171875, 0.333984375, 0.037109375, 0.0, 0.03515625, 0.01953125, 0.052734375, 0.015625, 0.03125, 0.009765625, 0.013671875, 0.02734375, 0.017578125, 0.091796875, 0.03515625, 0.046875, 0.080078125, 0.169921875, 0.0234375, 0.0703125, 0.01171875, 0.107421875, 0.0390625, 0.048828125, 0.013671875, 0.99609375, 0.009765625, 0.029296875, 0.05859375, 0.34375, 0.6953125, 0.041015625, 0.75, 0.13671875, 0.037109375, 0.02734375, 0.15625, 0.0390625, 0.005859375, 0.015625, 0.185546875, 0.01953125, 0.005859375, 0.083984375, 0.0546875, 0.064453125, 0.0390625, 0.04296875, 0.015625, 0.1171875, 0.09765625, 0.234375, 0.294921875, 0.220703125, 0.99609375, 0.1328125, 0.078125, 0.953125, 0.0078125, 0.642578125, 0.052734375, 0.0859375, 0.02734375, 0.017578125, 0.041015625, 0.03125, 0.037109375, 0.015625, 0.169921875, 0.0390625, 0.033203125, 0.041015625, 0.017578125, 0.951171875, 0.0, 0.0703125, 0.728515625, 0.064453125, 0.0234375, 0.01171875, 0.001953125, 0.046875, 0.947265625, 0.041015625, 0.025390625, 0.08984375, 0.0546875, 0.76171875, 0.0, 0.01953125, 0.6171875, 0.849609375, 0.046875, 0.037109375, 0.11328125, 0.955078125, 0.99609375, 0.07421875, 0.876953125, 0.041015625, 0.009765625, 0.078125, 0.005859375, 0.05078125, 0.03125, 0.119140625, 0.234375, 0.021484375, 0.0078125, 0.01953125, 0.009765625, 0.021484375, 0.03125, 0.02734375, 0.880859375, 0.994140625, 0.84375, 0.173828125, 0.025390625, 0.00390625, 0.068359375, 0.640625, 0.0625, 0.375, 0.0078125, 0.1328125, 0.064453125, 0.005859375, 0.01953125, 0.1171875, 0.0078125, 0.015625, 0.3984375, 0.015625, 0.234375, 0.052734375, 0.833984375, 0.046875, 0.009765625, 0.0546875, 0.56640625, 0.009765625, 0.00390625, 0.076171875, 0.03125, 0.046875, 0.03125, 0.08203125, 0.908203125, 0.005859375, 0.029296875, 0.017578125, 0.046875, 0.99609375, 0.07421875, 0.529296875, 0.99609375, 0.076171875, 0.0703125, 0.037109375, 0.046875, 0.033203125, 0.02734375, 0.025390625, 0.01953125, 0.0078125, 0.21875, 0.072265625, 0.005859375, 0.064453125, 0.083984375, 0.556640625, 0.0078125, 0.095703125, 0.15234375, 0.052734375, 0.009765625, 0.0078125, 0.015625, 0.02734375, 0.1328125, 0.021484375, 0.162109375, 0.0234375, 0.0390625, 0.033203125, 0.017578125, 0.044921875, 0.001953125, 0.005859375, 0.0234375, 0.580078125, 0.05859375, 0.03125, 0.046875, 0.06640625, 0.033203125, 0.021484375, 0.021484375, 0.103515625, 0.052734375, 0.134765625, 0.029296875, 0.080078125, 0.021484375, 0.015625, 0.01171875, 0.015625, 0.115234375, 0.439453125, 0.009765625, 0.021484375, 0.078125, 0.99609375, 0.044921875, 0.02734375, 0.140625, 0.0078125, 0.068359375, 0.0625, 0.0390625, 0.994140625, 0.16796875, 0.384765625, 0.00390625, 0.009765625, 0.087890625, 0.01953125, 0.994140625, 0.078125, 0.08984375, 0.072265625, 0.068359375, 0.2421875, 0.064453125, 0.021484375, 0.02734375, 0.05078125, 0.041015625, 0.095703125, 0.05859375, 0.994140625, 0.013671875, 0.001953125, 0.009765625, 0.0546875, 0.021484375, 0.037109375, 0.041015625, 0.029296875, 0.015625, 0.11328125, 0.10546875, 0.115234375, 0.705078125, 0.05078125, 0.01171875, 0.775390625, 0.07421875, 0.55859375, 0.8203125, 0.0234375, 0.2421875, 0.03515625, 0.669921875, 0.03515625, 0.029296875, 0.05078125, 0.294921875, 0.095703125, 0.548828125, 0.015625, 0.119140625, 0.01171875, 0.009765625, 0.01953125, 0.123046875, 0.9453125, 0.994140625, 0.85546875, 0.001953125, 0.017578125, 0.07421875, 0.857421875, 0.00390625, 0.021484375, 0.01953125, 0.03515625, 0.01953125, 0.994140625, 0.0390625, 0.314453125, 0.193359375, 0.119140625, 0.00390625, 0.02734375, 0.994140625, 0.275390625, 0.013671875, 0.0234375, 0.046875, 0.02734375, 0.0078125, 0.015625, 0.99609375, 0.041015625, 0.0078125, 0.158203125, 0.14453125, 0.052734375, 0.076171875, 0.046875, 0.09375, 0.037109375, 0.015625, 0.0234375, 0.09375, 0.017578125, 0.734375, 0.03515625, 0.052734375, 0.03515625, 0.0546875, 0.083984375, 0.052734375, 0.017578125, 0.021484375, 0.0390625, 0.154296875, 0.046875, 0.017578125, 0.11328125, 0.837890625, 0.134765625, 0.05859375, 0.02734375, 0.01171875, 0.037109375, 0.109375, 0.123046875, 0.072265625, 0.423828125, 0.013671875, 0.095703125, 0.9296875, 0.0390625, 0.017578125, 0.724609375, 0.345703125, 0.05078125, 0.021484375, 0.1484375, 0.029296875, 0.130859375, 0.5546875, 0.994140625, 0.01953125, 0.591796875, 0.00390625, 0.01171875, 0.033203125, 0.013671875, 0.07421875, 0.732421875, 0.0234375, 0.02734375, 0.224609375, 0.01953125, 0.03125, 0.421875, 0.095703125, 0.251953125, 0.201171875, 0.994140625, 0.03515625, 0.0078125, 0.06640625, 0.080078125, 0.509765625, 0.07421875, 0.208984375, 0.009765625, 0.99609375, 0.99609375, 0.99609375, 0.09375, 0.099609375, 0.50390625, 0.056640625, 0.00390625, 0.048828125, 0.041015625, 0.0625, 0.060546875, 0.06640625, 0.056640625, 0.236328125, 0.029296875, 0.62109375, 0.033203125, 0.046875, 0.013671875, 0.119140625, 0.01171875, 0.798828125, 0.01953125, 0.388671875, 0.060546875, 0.994140625, 0.029296875, 0.576171875, 0.0703125, 0.99609375, 0.005859375, 0.013671875, 0.013671875, 0.087890625, 0.0078125, 0.029296875, 0.01953125, 0.08984375, 0.107421875, 0.869140625, 0.017578125, 0.041015625, 0.083984375, 0.009765625, 0.021484375, 0.61328125, 0.994140625, 0.0546875, 0.994140625, 0.00390625, 0.01171875, 0.111328125, 0.033203125, 0.0078125, 0.8046875, 0.087890625, 0.021484375, 0.0859375, 0.083984375, 0.01953125, 0.029296875, 0.791015625, 0.0625, 0.005859375, 0.375, 0.064453125, 0.01171875, 0.041015625, 0.01953125, 0.396484375, 0.017578125, 0.107421875, 0.03515625, 0.994140625, 0.84765625, 0.00390625, 0.134765625, 0.033203125, 0.08203125, 0.03515625, 0.076171875, 0.212890625, 0.03515625, 0.041015625, 0.04296875, 0.994140625, 0.228515625, 0.044921875, 0.013671875, 0.015625, 0.021484375, 0.0234375, 0.0234375, 0.01953125, 0.0, 0.09375, 0.015625, 0.994140625, 0.03515625, 0.025390625, 0.130859375, 0.935546875, 0.076171875, 0.091796875, 0.005859375, 0.005859375, 0.037109375, 0.994140625, 0.0546875, 0.2109375, 0.04296875, 0.724609375, 0.994140625, 0.0234375, 0.017578125, 0.130859375, 0.0546875, 0.591796875, 0.021484375, 0.017578125, 0.0625, 0.1875, 0.005859375, 0.234375, 0.0078125, 0.0703125, 0.029296875, 0.107421875, 0.0, 0.041015625, 0.009765625, 0.01171875, 0.017578125, 0.029296875, 0.0859375, 0.650390625, 0.033203125, 0.009765625, 0.017578125, 0.845703125, 0.0078125, 0.060546875, 0.037109375, 0.015625, 0.095703125, 0.0234375, 0.28125, 0.068359375, 0.0234375, 0.138671875, 0.99609375, 0.041015625, 0.041015625, 0.095703125, 0.056640625, 0.013671875, 0.07421875, 0.01171875, 0.580078125, 0.05078125, 0.099609375, 0.90625, 0.408203125, 0.046875, 0.244140625, 0.779296875, 0.060546875, 0.0859375, 0.12109375, 0.994140625, 0.021484375, 0.056640625, 0.005859375, 0.037109375, 0.068359375, 0.021484375, 0.013671875, 0.021484375, 0.02734375, 0.580078125, 0.08203125, 0.080078125, 0.029296875, 0.783203125, 0.01171875, 0.28515625, 0.013671875, 0.021484375, 0.03515625, 0.828125, 0.0078125, 0.99609375, 0.013671875, 0.03125, 0.087890625, 0.30078125, 0.0, 0.001953125, 0.37890625, 0.05078125, 0.0703125, 0.51171875, 0.267578125, 0.05859375, 0.0, 0.01171875, 0.66015625, 0.0234375, 0.087890625, 0.095703125, 0.052734375, 0.958984375, 0.033203125, 0.02734375, 0.796875, 0.06640625, 0.009765625, 0.068359375, 0.015625, 0.02734375, 0.056640625, 0.015625, 0.130859375, 0.0234375, 0.015625, 0.111328125, 0.015625, 0.015625, 0.068359375, 0.015625, 0.0234375, 0.0546875, 0.85546875, 0.896484375, 0.08984375, 0.0, 0.0234375, 0.01171875, 0.07421875, 0.9375, 0.08984375, 0.00390625, 0.115234375, 0.025390625, 0.015625, 0.08203125, 0.09765625, 0.021484375, 0.015625, 0.015625, 0.1640625, 0.02734375, 0.77734375, 0.015625, 0.015625, 0.083984375, 0.017578125, 0.123046875, 0.060546875, 0.037109375, 0.033203125, 0.73046875, 0.0546875, 0.060546875, 0.033203125, 0.0859375, 0.998046875, 0.146484375, 0.048828125, 0.99609375, 0.048828125, 0.015625, 0.029296875, 0.017578125, 0.193359375, 0.021484375, 0.99609375, 0.9765625, 0.0859375, 0.037109375, 0.021484375, 0.013671875, 0.11328125, 0.01171875, 0.0625, 0.099609375, 0.037109375, 0.0390625, 0.076171875, 0.015625, 0.27734375, 0.17578125, 0.0234375, 0.068359375, 0.283203125, 0.994140625, 0.099609375, 0.037109375, 0.142578125, 0.607421875, 0.06640625, 0.181640625, 0.05859375, 0.015625, 0.0078125, 0.185546875, 0.80078125, 0.001953125, 0.01953125, 0.01953125, 0.115234375, 0.033203125, 0.0546875, 0.037109375, 0.041015625, 0.01171875, 0.01953125, 0.037109375, 0.01171875, 0.0078125, 0.048828125, 0.5546875, 0.76171875, 0.109375, 0.029296875, 0.0078125, 0.037109375, 0.193359375, 0.181640625, 0.0390625, 0.046875, 0.037109375, 0.01171875, 0.693359375, 0.119140625, 0.025390625, 0.017578125, 0.12890625, 0.01171875, 0.001953125, 0.013671875, 0.041015625, 0.080078125, 0.611328125, 0.99609375, 0.16796875, 0.068359375, 0.05078125, 0.033203125, 0.0234375, 0.91015625, 0.99609375, 0.0234375, 0.068359375, 0.115234375, 0.017578125, 0.095703125, 0.025390625, 0.033203125, 0.056640625, 0.025390625, 0.056640625, 0.791015625, 0.828125, 0.244140625, 0.001953125, 0.080078125, 0.193359375, 0.025390625, 0.017578125, 0.13671875, 0.134765625, 0.71875, 0.88671875, 0.015625, 0.091796875, 0.005859375, 0.99609375, 0.037109375, 0.826171875, 0.03125, 0.02734375, 0.015625, 0.041015625, 0.0078125, 0.095703125, 0.0, 0.005859375, 0.064453125, 0.017578125, 0.033203125, 0.0390625, 0.173828125, 0.017578125, 0.041015625, 0.021484375, 0.01953125, 0.021484375, 0.998046875, 0.12890625, 0.052734375, 0.05078125, 0.0234375, 0.0390625, 0.009765625, 0.02734375, 0.072265625, 0.982421875, 0.212890625, 0.220703125, 0.322265625, 0.201171875, 0.017578125, 0.51953125, 0.099609375, 0.076171875, 0.01171875, 0.291015625, 0.017578125, 0.021484375, 0.099609375, 0.044921875, 0.06640625, 0.080078125, 0.041015625, 0.013671875, 0.66015625, 0.01171875, 0.02734375, 0.064453125]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 6748730.024455793 (unstructured) 0 (structured)

max weight is  tensor([7.8862e-01, 3.1030e-09, 5.4430e-09, 3.2162e-09, 2.9702e-09, 3.2162e-09,
        1.3758e-09, 1.7273e-08, 1.3529e-01, 2.3159e-09, 7.5963e-09, 3.2161e-09,
        2.1539e-01, 1.9909e-01, 4.7166e-02, 1.1151e-01, 2.6163e-01, 3.1641e-01,
        2.3078e-01, 3.2162e-09, 1.2746e-02, 1.9234e-09, 2.2768e-01, 9.9654e-02,
        2.4389e-01, 1.2582e-01, 4.7510e-01, 4.0198e-01, 2.7871e-02, 2.1178e-01,
        1.0671e-01, 1.6271e-02, 3.6303e-01, 4.2894e-09, 3.5919e-09, 1.1932e-02,
        1.7825e-01, 9.8256e-09, 1.0206e-01, 4.9598e-09, 1.8739e-01, 6.8945e-03,
        4.8153e-09, 8.4630e-09, 3.2009e-01, 9.8781e-01, 1.8554e-01, 7.4405e-02,
        3.0873e-01, 1.5415e-01, 1.4806e-01, 6.3123e-01, 2.4743e-01, 1.9473e-01,
        1.3465e-09, 4.8837e-02, 2.4435e-01, 3.1822e-09, 7.2861e-02, 1.3758e-09,
        3.3933e-01, 2.7700e-02, 3.2162e-09, 3.3276e-09], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.3057e-02, 1.2375e-01, 2.1822e-02, 1.6918e-08, 9.0565e-02, 3.8342e-02,
        7.2923e-02, 1.2067e-01, 1.2613e-01, 1.0223e-01, 1.9498e-02, 3.6242e-02,
        1.6628e-01, 2.4841e-01, 6.3693e-02, 1.9722e-01, 1.3611e-01, 1.2239e-01,
        1.6506e-01, 8.4817e-08, 8.4817e-08, 2.4296e-02, 4.9514e-02, 8.0279e-02,
        5.8920e-02, 1.0550e-01, 8.5519e-02, 3.2044e-01, 2.3088e-01, 1.3034e-01,
        9.9417e-02, 1.7828e-01, 3.7172e-08, 2.4783e-01, 2.6729e-01, 9.9317e-09,
        1.6918e-08, 5.8184e-02, 1.9928e-01, 5.8209e-02, 1.5552e-01, 9.8687e-03,
        2.4967e-01, 3.5273e-02, 1.6578e-01, 9.4600e-02, 2.0599e-01, 1.3719e-01,
        1.4220e-01, 1.4313e-01, 1.5527e-08, 8.4525e-02, 7.8596e-02, 3.4173e-08,
        7.7604e-03, 8.8866e-02, 9.3272e-02, 1.2355e-01, 7.4624e-02, 1.7406e-08,
        2.2452e-01, 9.5772e-02, 3.6233e-02, 3.7626e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([9.2702e-02, 4.3598e-07, 1.0266e-06, 6.7380e-07, 6.7380e-07, 2.3367e-02,
        8.0380e-02, 2.9406e-01, 8.4115e-02, 1.0324e-07, 3.7322e-02, 4.6672e-07,
        1.8443e-07, 2.0429e-01, 1.6181e-07, 1.9937e-07, 9.3781e-02, 6.6073e-02,
        1.2239e-01, 1.2734e-07, 3.3146e-02, 2.4872e-07, 5.9850e-02, 5.3027e-02,
        4.7701e-02, 4.4502e-07, 4.6344e-02, 1.1686e-01, 1.6534e-01, 5.4103e-07,
        1.4867e-01, 2.0666e-07, 5.2392e-02, 1.0266e-06, 9.5050e-02, 6.7985e-07,
        9.2374e-02, 5.0202e-07, 2.0666e-07, 6.7991e-07, 3.1722e-07, 3.3182e-02,
        8.8809e-02, 1.3207e-06, 2.3005e-02, 5.4200e-07, 2.4864e-07, 6.0027e-02,
        5.4103e-07, 4.9773e-02, 2.4864e-07, 8.1366e-07, 5.8488e-02, 1.9937e-07,
        3.4583e-07, 5.4543e-02, 1.8443e-07, 5.3726e-02, 1.0738e-01, 7.3258e-02,
        6.8216e-02, 3.3293e-02, 1.0006e-06, 2.6650e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.3698e-02, 1.1310e-02, 2.5159e-02, 4.1511e-02, 1.7886e-02, 1.9776e-02,
        9.7141e-02, 5.9695e-02, 4.3073e-08, 2.2478e-02, 9.1813e-03, 1.7293e-01,
        3.0635e-02, 2.0712e-02, 3.3501e-08, 8.1345e-02, 1.6100e-01, 1.8478e-02,
        2.6088e-02, 8.6125e-02, 6.5231e-08, 3.8584e-08, 3.7767e-02, 7.7598e-02,
        2.1474e-01, 2.8324e-08, 8.5777e-08, 1.2046e-01, 9.4165e-09, 4.8662e-02,
        8.1243e-02, 3.2682e-08, 3.1087e-02, 3.7614e-02, 4.5997e-02, 2.4559e-08,
        2.3359e-02, 8.0643e-08, 2.8750e-02, 2.8544e-08, 8.0643e-08, 1.2792e-07,
        1.2263e-07, 5.8682e-02, 3.4549e-08, 6.0069e-08, 2.5919e-02, 2.3631e-02,
        1.0362e-02, 4.4470e-02, 7.1480e-02, 2.8726e-08, 2.0818e-01, 9.3250e-03,
        5.9730e-02, 4.4246e-03, 7.9562e-02, 7.2201e-02, 7.1613e-02, 2.4490e-02,
        4.7154e-08, 2.9675e-08, 2.3819e-02, 1.1093e-01, 1.7427e-01, 2.2289e-02,
        3.1370e-02, 2.8042e-08, 4.3199e-02, 2.2331e-02, 1.2666e-01, 7.9432e-02,
        1.6299e-02, 7.8528e-02, 4.4641e-08, 1.3115e-01, 2.7884e-02, 1.4241e-02,
        8.5882e-03, 4.4952e-02, 4.9511e-08, 6.2341e-02, 1.8160e-02, 3.5618e-03,
        1.0374e-01, 7.6602e-02, 8.3795e-02, 2.8042e-08, 7.9675e-02, 8.4116e-08,
        1.0778e-01, 8.9652e-08, 5.4770e-02, 6.6153e-02, 8.3380e-08, 6.2070e-02,
        2.4084e-02, 2.2339e-02, 1.8021e-01, 1.5525e-01, 7.5935e-02, 2.3477e-02,
        2.0370e-08, 1.2933e-07, 1.1773e-01, 1.0801e-01, 2.7090e-02, 1.5130e-01,
        1.1419e-02, 1.6938e-08, 2.9039e-02, 2.2809e-02, 1.8112e-01, 3.0697e-02,
        7.1624e-02, 3.6991e-08, 9.9825e-02, 2.1920e-02, 8.3464e-08, 2.5867e-08,
        8.8350e-02, 1.1226e-02, 3.1977e-08, 7.1309e-08, 1.6695e-02, 8.8952e-02,
        2.4051e-08, 1.5215e-01, 1.7121e-02, 5.5652e-08, 5.8716e-02, 4.4641e-08,
        2.4555e-02, 6.8440e-02, 2.2760e-01, 1.5693e-01, 1.0129e-02, 4.7883e-02,
        1.3474e-01, 1.3439e-02, 2.5497e-02, 2.8544e-08, 5.1544e-02, 2.5865e-02,
        2.0064e-01, 6.2629e-08, 1.9797e-01, 1.7126e-02, 8.0643e-08, 9.2843e-03,
        9.6828e-02, 4.0320e-02, 3.6419e-08, 1.4242e-02, 4.5783e-02, 3.2058e-02,
        3.6507e-08, 1.5085e-01, 2.2859e-02, 4.1799e-08, 8.9974e-02, 6.1711e-02,
        2.0390e-02, 5.3455e-08, 4.1896e-03, 2.8279e-02, 4.7136e-08, 1.8787e-02,
        4.8401e-02, 4.9268e-02, 1.7228e-01, 3.0850e-02, 1.7602e-01, 3.4551e-02,
        4.0702e-08, 1.8858e-02, 1.2381e-02, 3.5355e-02, 5.8016e-02, 1.5954e-01,
        4.4684e-02, 2.0108e-02, 5.4170e-02, 1.4643e-01, 3.4700e-02, 3.0590e-02,
        2.1692e-02, 2.3184e-02, 2.6780e-02, 4.0651e-08, 1.2735e-07, 6.2117e-02,
        7.0344e-02, 8.4116e-08, 4.9511e-08, 7.0503e-02, 6.0371e-02, 1.2302e-07,
        4.3073e-08, 2.9352e-02, 9.5478e-02, 2.8028e-02, 1.5058e-01, 8.9061e-03,
        1.0009e-01, 3.8702e-02, 6.2629e-08, 6.5231e-08, 3.8501e-02, 1.6043e-01,
        1.9966e-02, 9.0628e-02, 7.5403e-02, 2.1151e-02, 6.6924e-02, 2.4461e-02,
        3.5353e-02, 3.6824e-08, 7.1876e-08, 4.0880e-02, 6.1658e-02, 5.1868e-02,
        3.3623e-08, 9.7045e-02, 2.5990e-02, 2.3316e-02, 2.1487e-01, 5.3283e-02,
        1.6014e-02, 3.4989e-08, 4.7606e-08, 1.4769e-01, 8.3399e-08, 3.1093e-08,
        4.7136e-08, 2.6746e-02, 8.0541e-02, 2.7531e-02, 2.2306e-02, 7.0805e-08,
        2.8517e-08, 2.8426e-08, 8.7245e-09, 3.6507e-08, 4.0651e-08, 1.5123e-02,
        1.5435e-02, 7.5622e-02, 1.6966e-02, 6.2943e-08, 1.7968e-01, 4.0995e-02,
        8.0643e-08, 1.5423e-02, 1.6142e-02, 4.0609e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.4698e-08, 1.2260e-01, 6.2605e-02, 7.0132e-02, 1.3454e-01, 3.6342e-02,
        9.5556e-02, 2.2086e-01, 8.2174e-09, 1.8857e-01, 1.3612e-01, 1.4552e-02,
        1.8628e-02, 1.5787e-01, 4.6972e-08, 1.0200e-01, 1.2785e-01, 3.7520e-02,
        1.3699e-01, 7.7453e-02, 7.3324e-09, 2.3945e-08, 6.7021e-02, 9.1357e-02,
        2.4855e-01, 3.4713e-03, 9.4268e-09, 2.1883e-02, 4.6972e-08, 8.1117e-02,
        3.7966e-02, 4.6983e-08, 1.0809e-08, 2.7812e-02, 8.9555e-02, 2.6167e-08,
        6.9884e-02, 2.6842e-08, 1.6641e-02, 4.6971e-08, 1.8729e-08, 2.6167e-08,
        2.9170e-08, 6.7433e-02, 4.1518e-02, 2.9170e-08, 4.4834e-02, 6.0426e-02,
        6.3053e-02, 1.2869e-08, 2.0622e-02, 2.3942e-08, 1.2336e-01, 1.0359e-01,
        3.5302e-02, 8.1745e-02, 1.3224e-01, 2.2659e-02, 1.3010e-01, 3.9584e-02,
        3.5962e-08, 1.7412e-08, 1.4189e-01, 7.0579e-02, 1.1880e-01, 1.3446e-02,
        4.3307e-02, 4.6973e-08, 1.2475e-01, 1.0766e-01, 8.4852e-02, 1.7114e-02,
        1.1210e-02, 1.2108e-02, 3.5962e-08, 1.1578e-01, 1.3253e-01, 2.4728e-02,
        1.0841e-02, 2.8508e-02, 2.4816e-08, 1.4585e-02, 8.7322e-02, 2.1795e-02,
        6.0514e-02, 8.1126e-02, 9.4811e-02, 1.0184e-08, 9.5245e-02, 2.9170e-08,
        7.2294e-02, 1.0184e-08, 4.6101e-02, 4.2505e-02, 1.1064e-08, 4.5913e-02,
        6.9551e-02, 1.3118e-01, 1.3604e-01, 5.3755e-02, 9.1809e-02, 5.6987e-02,
        4.7449e-03, 1.0184e-08, 1.5642e-01, 1.0294e-01, 1.5261e-01, 1.3133e-01,
        9.4717e-09, 2.6167e-08, 7.9783e-02, 2.4707e-02, 1.9874e-02, 1.2614e-01,
        1.1027e-01, 2.9170e-08, 8.9375e-02, 1.3380e-01, 2.3942e-08, 2.6167e-08,
        1.8208e-01, 2.4281e-08, 1.6025e-08, 9.0398e-09, 1.9337e-01, 1.7283e-01,
        2.6842e-08, 3.2607e-02, 9.8017e-02, 3.0895e-08, 1.0237e-01, 2.6167e-08,
        2.4690e-02, 8.3271e-02, 2.5004e-02, 1.0861e-01, 7.8594e-02, 1.3328e-01,
        9.6605e-02, 1.6071e-01, 5.3181e-02, 4.6973e-08, 6.0757e-02, 7.2607e-09,
        1.0763e-01, 2.3942e-08, 4.7463e-02, 1.2743e-01, 9.4634e-09, 1.4296e-01,
        7.5373e-02, 1.3978e-02, 1.8729e-08, 1.8847e-01, 7.8149e-02, 4.0999e-02,
        1.6798e-08, 1.5061e-01, 1.3291e-01, 1.8729e-08, 7.9326e-02, 1.7981e-01,
        5.7190e-02, 2.9170e-08, 6.1724e-03, 2.3835e-02, 1.6025e-08, 1.4394e-01,
        1.6966e-01, 4.8104e-02, 1.6082e-01, 2.2396e-01, 1.2353e-01, 3.0266e-02,
        7.7419e-02, 1.5212e-01, 1.8960e-01, 5.7378e-02, 5.5995e-02, 9.9302e-02,
        1.1189e-01, 2.7179e-02, 9.9299e-02, 3.9209e-02, 8.2110e-02, 1.0387e-01,
        9.3662e-02, 2.1150e-01, 2.4145e-01, 2.9170e-08, 2.9170e-08, 1.3021e-01,
        5.8511e-02, 1.0184e-08, 1.5232e-08, 1.1225e-01, 3.1791e-02, 4.6972e-08,
        5.0968e-09, 3.2440e-02, 8.8317e-02, 1.2212e-08, 3.0992e-02, 1.3612e-01,
        9.6151e-02, 5.2886e-02, 1.0007e-08, 2.9170e-08, 1.4833e-02, 1.0379e-01,
        2.1905e-01, 9.0633e-02, 9.8030e-02, 1.0605e-01, 1.2359e-01, 1.7235e-01,
        5.5247e-02, 4.6973e-08, 3.0055e-08, 2.5304e-02, 1.0280e-02, 4.1701e-02,
        1.8729e-08, 5.7633e-02, 6.7100e-02, 5.8192e-02, 1.3785e-01, 8.5126e-02,
        1.4710e-01, 2.3942e-08, 1.6025e-08, 9.6073e-02, 5.5414e-02, 1.5438e-08,
        2.6842e-08, 1.6663e-01, 7.1009e-02, 7.9994e-09, 4.4206e-02, 4.3383e-02,
        2.3942e-08, 2.9170e-08, 2.6167e-08, 3.5951e-08, 2.9170e-08, 4.7559e-02,
        7.6876e-02, 8.4193e-02, 1.1167e-01, 2.9170e-08, 8.7211e-02, 7.0743e-02,
        2.9170e-08, 1.3469e-06, 6.4159e-02, 3.0640e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.7857e-02, 4.9365e-02, 6.1954e-02, 9.2577e-04, 1.1620e-07, 2.6838e-07,
        7.8177e-02, 3.9663e-07, 8.2677e-02, 1.5567e-07, 1.8008e-02, 3.4549e-02,
        1.2522e-07, 8.3920e-08, 2.0847e-07, 4.0514e-02, 1.1620e-07, 1.5899e-01,
        5.3052e-08, 2.6838e-07, 8.3920e-08, 5.4707e-02, 7.4335e-08, 8.2929e-02,
        3.9663e-07, 2.5453e-07, 8.3920e-08, 1.7369e-07, 1.2094e-07, 2.8338e-02,
        1.6970e-02, 2.4642e-07, 5.6884e-02, 5.3562e-07, 1.5427e-07, 3.6889e-02,
        1.0032e-07, 1.2132e-07, 3.5925e-02, 1.0214e-01, 2.5453e-07, 5.2325e-07,
        8.9896e-08, 8.8767e-08, 2.6838e-07, 3.2872e-02, 3.3892e-02, 7.2857e-08,
        3.7955e-02, 8.2990e-02, 9.7743e-02, 7.9256e-02, 5.5745e-02, 4.5807e-07,
        3.2039e-02, 8.9896e-08, 3.6665e-02, 2.9948e-02, 3.5933e-02, 4.2264e-02,
        8.3920e-08, 3.8441e-02, 5.0421e-02, 4.0209e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([9.8361e-02, 1.4827e-07, 9.9944e-02, 2.9412e-07, 1.4827e-07, 8.3661e-02,
        8.4747e-02, 1.0907e-07, 6.8654e-02, 4.0969e-07, 8.7275e-02, 1.0912e-01,
        1.8074e-02, 3.5511e-07, 4.6585e-07, 2.3659e-02, 2.0649e-07, 7.6497e-02,
        5.5655e-07, 1.2615e-01, 1.5933e-07, 1.3729e-07, 2.2463e-07, 4.2470e-07,
        1.0729e-01, 1.0835e-01, 9.5383e-02, 6.6079e-02, 1.3729e-07, 8.1807e-02,
        5.9807e-07, 8.4382e-02, 2.5685e-07, 2.1409e-02, 9.8160e-08, 7.6777e-02,
        1.6777e-07, 2.2116e-02, 2.3302e-07, 6.0601e-07, 8.6448e-02, 4.6336e-07,
        8.3152e-02, 3.5511e-07, 8.1835e-07, 9.6907e-07, 8.4549e-07, 1.4827e-07,
        1.3729e-07, 2.9313e-02, 9.0008e-02, 1.3729e-07, 3.7238e-07, 9.5100e-02,
        1.0907e-07, 8.5529e-02, 1.5933e-07, 3.8148e-02, 1.6777e-07, 1.2602e-01,
        4.2470e-07, 9.6907e-07, 7.4501e-02, 5.3399e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.2012e-02, 1.2257e-02, 1.9916e-02, 3.4983e-03, 1.5223e-02, 2.6275e-02,
        8.6794e-02, 4.7671e-02, 7.2090e-03, 1.2404e-02, 1.2060e-02, 1.0723e-02,
        2.9734e-02, 1.3289e-02, 1.5369e-08, 1.1669e-02, 1.8023e-02, 1.9742e-08,
        2.4898e-02, 5.6366e-03, 2.2761e-02, 1.1360e-07, 6.3728e-03, 1.1784e-02,
        2.5450e-01, 6.7112e-08, 6.3470e-08, 3.5871e-02, 4.1324e-02, 1.5467e-02,
        3.8951e-02, 1.1324e-07, 6.4685e-08, 9.6098e-02, 2.3288e-02, 8.2364e-09,
        4.8898e-08, 8.7921e-08, 2.4779e-08, 7.4988e-02, 9.3420e-02, 5.5006e-02,
        9.0770e-02, 5.0024e-02, 9.3333e-08, 9.3497e-02, 4.1527e-02, 3.2258e-08,
        3.0402e-02, 3.9599e-02, 1.2621e-01, 4.1504e-02, 2.8813e-08, 1.0213e-02,
        2.1794e-02, 2.9418e-02, 9.4240e-03, 4.7927e-08, 1.6273e-02, 5.1723e-08,
        6.0441e-02, 3.6739e-08, 9.8569e-03, 1.1685e-02, 2.5334e-02, 2.5094e-08,
        1.6659e-02, 5.4949e-08, 7.2526e-03, 2.8562e-02, 3.9406e-02, 3.1376e-02,
        2.9563e-08, 2.6324e-02, 5.0232e-08, 2.2471e-02, 1.2054e-02, 4.2839e-08,
        2.1321e-02, 2.6627e-02, 5.4520e-08, 3.8439e-02, 5.6402e-03, 3.2181e-02,
        1.3966e-02, 3.6446e-02, 3.9181e-02, 7.8001e-02, 9.3044e-02, 7.8742e-08,
        1.1812e-02, 8.1432e-03, 2.3575e-02, 3.2558e-08, 9.0345e-02, 6.8380e-02,
        5.4543e-08, 4.6810e-02, 8.9481e-03, 1.1679e-02, 8.9389e-03, 6.0306e-08,
        5.3298e-02, 8.2517e-08, 1.1611e-02, 5.2513e-02, 1.4496e-02, 1.6769e-02,
        4.0934e-08, 1.1450e-01, 9.1080e-03, 2.2463e-02, 1.5791e-02, 1.1436e-02,
        4.6258e-02, 1.1324e-07, 5.3616e-02, 1.8997e-02, 5.4949e-08, 8.2488e-02,
        2.1182e-01, 3.6987e-08, 2.4515e-08, 9.2530e-02, 1.7294e-02, 1.0513e-02,
        1.1438e-07, 2.2693e-02, 2.3334e-08, 6.3470e-08, 8.0716e-03, 5.0998e-02,
        4.1364e-02, 1.1116e-01, 1.3181e-02, 3.8096e-02, 2.3964e-02, 1.2813e-02,
        1.6247e-02, 1.4059e-02, 4.5921e-03, 9.4747e-02, 8.4561e-02, 2.9519e-08,
        7.7103e-08, 6.0850e-02, 1.5594e-02, 8.7415e-03, 8.2364e-09, 1.2363e-02,
        1.8080e-02, 1.7157e-02, 5.6739e-02, 1.6142e-02, 7.2545e-03, 4.3300e-08,
        6.6365e-02, 1.0704e-01, 9.3197e-03, 6.9851e-02, 3.7134e-02, 1.1952e-02,
        3.0415e-08, 5.2503e-08, 4.9993e-08, 3.3351e-02, 7.2228e-08, 5.5969e-03,
        1.8512e-02, 8.4217e-03, 1.3613e-02, 4.1190e-02, 5.7074e-03, 9.2573e-02,
        4.5330e-08, 1.7970e-02, 1.7173e-02, 3.7501e-02, 5.4925e-08, 3.6907e-02,
        1.3108e-02, 1.9959e-02, 7.2820e-02, 3.7339e-02, 1.0791e-02, 6.9321e-02,
        3.5554e-07, 1.9126e-02, 2.5025e-02, 4.9568e-02, 2.3761e-01, 7.0934e-02,
        1.5588e-01, 4.5645e-02, 6.3470e-08, 5.1392e-02, 4.0276e-02, 1.7342e-01,
        3.6697e-08, 1.2363e-02, 4.6424e-02, 4.5130e-08, 9.7019e-03, 4.2075e-02,
        1.2289e-02, 3.8921e-02, 8.2781e-02, 8.2517e-08, 6.3901e-08, 5.8371e-03,
        1.6829e-02, 2.6845e-02, 1.2174e-02, 9.0635e-03, 5.5547e-02, 2.3338e-02,
        3.9588e-03, 8.4901e-08, 1.3028e-01, 3.7900e-02, 7.0869e-02, 6.9286e-03,
        1.4937e-01, 1.0593e-02, 1.3201e-02, 1.1794e-02, 5.2236e-02, 6.2073e-03,
        1.8124e-02, 3.7981e-08, 5.4949e-08, 4.5970e-03, 5.4850e-08, 4.5134e-08,
        8.4921e-02, 1.0849e-02, 1.4236e-02, 2.6092e-08, 1.5667e-02, 2.5189e-08,
        2.0366e-01, 6.4831e-02, 1.0127e-02, 1.3295e-01, 3.8446e-08, 1.5949e-08,
        6.2629e-08, 7.2132e-03, 2.8081e-02, 1.4498e-01, 2.1230e-02, 1.2761e-02,
        1.5212e-07, 3.5421e-02, 7.4478e-08, 4.1553e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.4363e-07, 3.8621e-07, 9.2369e-02, 3.6265e-07, 1.8420e-07, 2.9525e-02,
        2.6892e-07, 4.7937e-02, 8.3352e-03, 8.4078e-03, 8.4042e-03, 3.1263e-07,
        2.8742e-07, 8.7112e-08, 3.2844e-07, 1.2451e-07, 1.1610e-01, 1.4473e-01,
        1.2913e-01, 2.4579e-07, 8.8772e-03, 3.8805e-07, 2.8249e-07, 1.0804e-01,
        1.8001e-07, 2.9371e-07, 9.2033e-02, 9.7877e-02, 2.0055e-07, 5.3348e-07,
        3.4363e-07, 8.5087e-03, 8.3734e-03, 1.2462e-01, 1.9729e-02, 7.8912e-03,
        3.4363e-07, 8.6425e-03, 2.9925e-07, 2.2353e-07, 1.9255e-07, 3.8805e-07,
        3.4363e-07, 3.5939e-07, 5.0800e-07, 1.1704e-01, 3.9128e-02, 1.5091e-07,
        3.3139e-07, 2.4379e-02, 3.4363e-07, 2.8309e-07, 4.2227e-02, 1.0634e-02,
        2.8742e-07, 2.8226e-02, 1.4622e-01, 8.4893e-03, 1.5805e-07, 8.3290e-03,
        4.0322e-02, 1.4530e-01, 8.7254e-03, 1.3118e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.5697e-07, 4.1139e-02, 5.2370e-07, 4.6767e-07, 5.1907e-03, 5.0691e-03,
        3.6289e-07, 3.6550e-02, 5.8856e-07, 3.7623e-07, 1.0937e-06, 1.6964e-06,
        5.4047e-03, 9.2578e-07, 6.4070e-07, 4.1969e-02, 3.5295e-02, 1.9292e-06,
        4.3107e-02, 6.9999e-07, 3.6724e-02, 1.0925e-06, 4.2882e-02, 3.8768e-02,
        6.9123e-07, 1.0150e-01, 3.4423e-03, 3.6397e-02, 5.3601e-07, 9.7407e-07,
        4.6490e-02, 3.7623e-07, 5.3601e-07, 5.8836e-03, 1.3704e-06, 6.1041e-02,
        3.2043e-02, 5.8622e-03, 2.0989e-06, 1.2017e-07, 3.1871e-07, 4.6325e-02,
        3.8138e-02, 1.2376e-01, 3.3616e-07, 9.4259e-02, 6.8072e-03, 4.9824e-02,
        1.1594e-06, 3.4332e-02, 3.4939e-02, 4.1725e-02, 7.1204e-02, 6.7997e-07,
        1.0086e-01, 1.1430e-01, 2.6586e-07, 8.1668e-07, 6.5698e-03, 4.0978e-02,
        2.1629e-07, 5.5391e-02, 7.2563e-07, 5.4891e-03], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.9941e-02, 1.0988e-02, 5.9785e-08, 7.1971e-08, 1.1738e-02, 7.0368e-08,
        4.5724e-02, 3.2764e-02, 1.6765e-02, 1.1572e-02, 6.5627e-03, 1.3187e-02,
        4.2119e-02, 1.0334e-02, 5.1925e-08, 9.5511e-03, 1.1109e-01, 8.9250e-08,
        6.9556e-08, 1.4427e-02, 1.2707e-01, 1.2581e-07, 6.7457e-03, 9.6021e-03,
        7.0185e-02, 1.8177e-07, 5.3525e-08, 3.3473e-08, 4.7346e-03, 1.0371e-02,
        8.6614e-08, 7.9537e-02, 9.2981e-08, 7.6932e-02, 1.6533e-02, 7.7608e-02,
        3.5348e-02, 9.1964e-02, 1.1292e-07, 2.2143e-02, 1.5285e-01, 2.9754e-02,
        5.5303e-02, 3.3325e-02, 7.2401e-08, 5.3113e-02, 1.6722e-02, 9.8996e-08,
        5.0183e-08, 2.3903e-02, 2.6475e-02, 6.0641e-03, 1.7592e-02, 7.5015e-03,
        6.9809e-02, 6.1775e-03, 9.0079e-03, 6.0004e-08, 1.1491e-02, 7.8392e-03,
        7.2000e-02, 1.2019e-01, 6.8101e-03, 8.2944e-03, 1.7364e-02, 6.9455e-08,
        1.3863e-03, 7.0118e-02, 2.7511e-02, 3.0575e-03, 1.0029e-02, 1.5139e-02,
        4.2880e-02, 2.3413e-03, 1.5026e-01, 1.2335e-02, 9.2518e-03, 2.9326e-08,
        2.7576e-08, 3.4460e-02, 1.9929e-01, 8.0862e-04, 1.3444e-02, 1.1656e-07,
        6.3579e-03, 9.8017e-03, 2.8265e-02, 1.9101e-01, 1.2040e-02, 1.2440e-01,
        1.0286e-01, 6.4930e-02, 1.1578e-07, 6.2290e-03, 5.6116e-02, 4.8882e-02,
        7.0571e-08, 9.2630e-03, 5.6205e-03, 1.0130e-02, 1.0920e-02, 1.0525e-02,
        4.4522e-08, 8.6142e-02, 1.0809e-02, 1.0267e-02, 1.5362e-02, 4.1008e-03,
        4.8718e-08, 2.3706e-01, 4.2438e-08, 1.4266e-02, 1.5110e-02, 8.5221e-03,
        2.6214e-02, 5.1881e-02, 1.8428e-02, 1.2377e-02, 1.0117e-07, 4.1171e-03,
        5.7213e-02, 2.2814e-08, 1.9626e-07, 1.8066e-02, 1.3094e-02, 1.0942e-02,
        1.5340e-07, 1.7796e-02, 9.3883e-08, 6.1965e-02, 1.1276e-02, 6.6170e-02,
        1.0591e-07, 3.5923e-02, 1.4060e-02, 3.2384e-02, 1.8403e-02, 7.6414e-03,
        1.2722e-02, 1.0852e-02, 1.1536e-07, 1.0966e-01, 5.3757e-03, 6.7675e-08,
        1.6418e-03, 2.6917e-02, 1.0305e-02, 2.0766e-02, 1.6370e-01, 8.7729e-03,
        1.5438e-02, 1.4683e-02, 2.6931e-02, 9.5693e-03, 7.9229e-03, 8.5508e-08,
        1.1846e-02, 6.2958e-03, 9.6390e-03, 6.1615e-02, 3.3272e-08, 7.0923e-03,
        2.2666e-02, 1.2951e-07, 2.2518e-07, 4.5341e-03, 3.5369e-02, 1.9276e-02,
        1.5094e-02, 9.5750e-08, 1.2352e-01, 2.1528e-02, 9.2085e-03, 6.0842e-02,
        2.2911e-08, 1.1603e-02, 1.1339e-02, 3.1773e-02, 4.8949e-02, 1.1678e-02,
        1.2142e-02, 1.1842e-02, 7.4420e-08, 5.9698e-02, 7.1751e-03, 7.7795e-03,
        1.7293e-02, 1.6452e-02, 2.1731e-02, 6.1757e-04, 3.4572e-02, 8.0062e-03,
        2.7641e-02, 4.6555e-02, 1.8609e-01, 5.3683e-02, 8.2900e-03, 9.0454e-02,
        9.3711e-02, 1.0613e-07, 1.5658e-02, 4.6789e-08, 9.2537e-03, 7.6601e-03,
        8.2932e-03, 2.4035e-02, 9.5666e-02, 2.6759e-02, 4.7458e-08, 1.8250e-02,
        1.6784e-02, 1.2195e-02, 1.0031e-02, 1.4450e-02, 4.3275e-02, 1.5512e-02,
        3.6116e-02, 7.7420e-02, 5.5896e-02, 1.5503e-02, 4.0710e-08, 3.3997e-02,
        1.0994e-01, 1.3308e-02, 4.9337e-03, 5.4101e-08, 7.6885e-03, 8.8049e-03,
        1.6973e-02, 1.5391e-01, 3.8874e-08, 4.5500e-03, 8.0842e-08, 4.1634e-08,
        5.5919e-03, 2.0079e-02, 9.3206e-03, 1.2223e-07, 9.8330e-08, 8.0140e-08,
        1.4359e-02, 4.8448e-02, 9.9091e-08, 9.2482e-03, 3.6147e-02, 8.6328e-08,
        3.0456e-03, 8.1097e-03, 5.6488e-03, 9.6689e-03, 1.0062e-07, 1.0374e-02,
        5.3562e-02, 1.2929e-02, 4.3379e-02, 1.7308e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0934e-02, 1.3434e-07, 8.5697e-08, 1.5590e-08, 7.2606e-02, 6.8340e-04,
        2.5182e-07, 1.2210e-01, 4.8613e-02, 5.8682e-02, 4.0816e-02, 1.3692e-01,
        6.4281e-08, 7.7253e-02, 7.4116e-02, 2.8631e-07, 1.1026e-01, 1.8238e-07,
        1.8709e-07, 3.5250e-02, 1.6235e-07, 1.7326e-01, 3.0581e-07, 1.1997e-07,
        1.7006e-07, 6.2122e-02, 6.5060e-02, 5.6929e-07, 2.1100e-07, 2.1100e-07,
        1.4794e-01, 1.6235e-07, 2.2568e-07, 2.5963e-02, 1.1868e-07, 3.0783e-02,
        2.9760e-07, 4.9072e-08, 1.2860e-01, 1.6644e-07, 3.5256e-07, 6.0260e-07,
        1.8231e-07, 1.4962e-07, 1.8842e-02, 2.9232e-02, 1.9629e-07, 1.4188e-07,
        3.4622e-02, 4.6009e-02, 3.1766e-02, 6.0052e-07, 4.3966e-02, 1.3792e-01,
        1.2621e-07, 4.7048e-02, 7.8481e-02, 1.2701e-07, 2.5203e-02, 1.1997e-07,
        1.5213e-01, 1.9041e-01, 1.1162e-07, 1.8238e-07, 2.1427e-07, 6.6042e-02,
        3.1067e-07, 3.0364e-01, 3.3259e-07, 5.2791e-02, 4.8703e-08, 5.2651e-02,
        6.0017e-07, 4.0890e-07, 2.2646e-01, 4.1012e-02, 3.7095e-07, 2.0357e-07,
        3.2928e-07, 3.8596e-07, 1.4307e-07, 1.3823e-01, 2.7124e-02, 3.3294e-03,
        5.8059e-02, 2.8633e-07, 1.3008e-01, 2.1553e-07, 7.3214e-02, 1.2824e-01,
        1.0965e-07, 1.1997e-07, 4.6915e-02, 1.0814e-02, 4.9072e-08, 1.1004e-07,
        7.7549e-08, 2.9760e-07, 5.8607e-02, 1.6212e-07, 2.3065e-07, 6.5480e-08,
        4.9072e-08, 4.5395e-02, 7.7549e-08, 6.3379e-06, 1.2498e-07, 4.1777e-02,
        1.3371e-07, 5.9560e-07, 4.1285e-07, 3.4358e-02, 1.1935e-01, 1.6798e-01,
        6.1716e-02, 1.2976e-07, 3.2064e-07, 4.4981e-07, 2.1537e-07, 3.8598e-02,
        7.4468e-02, 1.6235e-07, 1.7270e-07, 1.2846e-01, 2.3144e-07, 1.9134e-07,
        1.5657e-01, 4.7872e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.4725e-06, 2.5972e-06, 1.7368e-06, 7.6373e-07, 3.7650e-02, 7.0345e-07,
        2.0900e-06, 1.4274e-06, 8.8543e-07, 1.5755e-06, 4.3911e-07, 1.8776e-06,
        2.5104e-06, 1.2362e-06, 2.7781e-06, 2.7781e-06, 2.3569e-02, 1.6872e-06,
        2.3447e-06, 6.7891e-07, 3.0196e-02, 1.7195e-06, 2.3619e-06, 1.0006e-01,
        1.3641e-06, 9.8765e-02, 9.6536e-02, 3.9156e-02, 1.8776e-06, 1.0201e-06,
        2.3619e-06, 9.2529e-02, 1.3310e-07, 1.1486e-06, 2.4759e-02, 1.5942e-06,
        1.8408e-06, 4.0515e-07, 1.1936e-06, 1.6075e-06, 4.0466e-02, 2.4077e-06,
        4.3906e-02, 8.1377e-07, 1.2831e-06, 9.4673e-07, 9.3862e-03, 1.3996e-06,
        1.9571e-02, 1.7558e-06, 2.4077e-06, 1.1136e-06, 3.9423e-02, 1.7558e-06,
        1.1938e-06, 6.6993e-07, 1.7195e-06, 1.3841e-02, 6.6880e-07, 2.5363e-06,
        2.5972e-06, 9.1233e-07, 9.6084e-02, 3.4222e-02, 3.7779e-02, 1.3972e-06,
        1.2394e-06, 2.5789e-06, 1.1775e-06, 9.1233e-07, 1.6075e-06, 4.6296e-02,
        9.4253e-02, 4.4671e-02, 2.3619e-06, 3.1771e-06, 2.0311e-02, 1.0201e-06,
        1.2142e-06, 2.3447e-06, 4.2734e-07, 2.3447e-06, 1.2831e-06, 3.8704e-02,
        1.3350e-06, 3.4800e-02, 6.8175e-07, 4.4019e-02, 6.9675e-07, 8.7290e-02,
        4.3488e-02, 1.3432e-02, 8.4459e-07, 4.0875e-02, 2.4077e-06, 7.8999e-07,
        1.8182e-06, 6.6993e-07, 1.4890e-07, 1.8776e-06, 4.4206e-02, 6.0967e-07,
        3.1840e-02, 3.4540e-02, 2.0618e-06, 4.7233e-08, 1.2831e-06, 4.0484e-02,
        9.2818e-02, 9.9464e-02, 1.4217e-06, 2.4061e-02, 1.7269e-06, 1.1136e-06,
        8.3841e-02, 1.6070e-06, 1.0201e-06, 9.8637e-02, 3.2780e-02, 6.9675e-07,
        7.1234e-07, 1.6075e-06, 1.4549e-06, 1.2142e-06, 1.9505e-06, 9.4864e-07,
        3.6949e-07, 1.1936e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1472e-01, 2.2476e-02, 1.1103e-02, 4.7857e-02, 7.7484e-02, 3.9830e-02,
        5.0325e-08, 1.7076e-07, 6.3294e-03, 3.8632e-02, 8.1640e-03, 5.4067e-02,
        5.3004e-08, 2.5386e-02, 3.4928e-02, 8.1445e-03, 7.1926e-08, 1.6390e-07,
        7.2811e-02, 4.3997e-02, 5.3455e-02, 5.7944e-02, 5.6288e-02, 1.3665e-02,
        1.6988e-07, 1.1512e-07, 3.6721e-02, 1.0891e-07, 6.2090e-02, 3.4544e-02,
        1.1643e-07, 1.6415e-01, 1.3581e-02, 8.1825e-08, 6.2311e-08, 6.0801e-08,
        1.2640e-07, 7.4471e-08, 3.4132e-08, 6.0117e-08, 2.9492e-02, 1.3477e-07,
        4.2998e-02, 6.0618e-02, 1.5518e-07, 1.0843e-07, 6.5931e-02, 4.6793e-02,
        7.2234e-02, 9.3998e-08, 4.7199e-02, 4.9223e-02, 1.0991e-01, 4.0682e-02,
        6.0117e-08, 6.1247e-02, 1.5592e-02, 7.8911e-02, 2.1537e-02, 6.8302e-02,
        7.6986e-02, 1.1331e-01, 1.2622e-07, 8.3202e-08, 8.6850e-02, 3.6291e-02,
        1.0170e-02, 8.3202e-08, 3.5766e-08, 2.4741e-07, 2.9702e-02, 6.0315e-02,
        6.3728e-02, 8.5952e-02, 1.2622e-07, 2.8478e-02, 5.8984e-02, 1.0517e-07,
        3.4132e-08, 7.4419e-08, 1.5131e-07, 3.3553e-02, 9.5477e-03, 1.5539e-02,
        1.1330e-02, 3.9445e-02, 5.8201e-02, 5.2667e-08, 1.0955e-01, 6.3389e-02,
        3.5024e-02, 5.1780e-02, 6.0412e-08, 3.8269e-08, 4.5198e-02, 1.2385e-07,
        4.3158e-02, 2.4805e-02, 5.0325e-08, 3.1118e-02, 3.5281e-02, 9.5290e-03,
        6.8073e-08, 4.9006e-02, 8.1825e-08, 5.0157e-08, 7.7064e-08, 9.2453e-02,
        8.2708e-02, 3.2007e-02, 7.1295e-02, 6.3683e-02, 4.5827e-02, 7.1729e-02,
        1.5918e-07, 1.3146e-07, 4.4037e-02, 3.5549e-02, 1.2331e-02, 1.2981e-07,
        1.6773e-02, 5.8525e-02, 1.5918e-07, 3.8681e-02, 1.1416e-07, 8.7462e-02,
        4.8910e-02, 6.0801e-08, 7.3325e-03, 1.8994e-02, 1.1302e-03, 6.5461e-02,
        2.9389e-02, 4.4386e-02, 3.5766e-08, 2.8941e-02, 1.5050e-02, 1.7076e-07,
        1.7904e-01, 1.5955e-07, 1.2480e-07, 6.0274e-02, 2.8256e-02, 1.2399e-07,
        1.7076e-07, 5.6305e-08, 6.0117e-08, 2.1847e-02, 2.1728e-02, 2.9366e-02,
        5.4489e-08, 2.1367e-07, 4.5271e-02, 1.3082e-07, 6.6769e-02, 1.2873e-01,
        1.0891e-07, 9.7592e-08, 5.2310e-02, 2.7675e-02, 1.0517e-07, 3.9651e-02,
        1.5238e-07, 3.4624e-02, 8.1149e-03, 1.1735e-02, 2.1301e-07, 6.8608e-08,
        2.6641e-02, 4.8764e-02, 8.4788e-08, 1.5488e-02, 4.5309e-02, 4.5698e-02,
        6.0801e-08, 4.2165e-02, 8.7003e-08, 9.2060e-02, 1.5248e-02, 9.6442e-08,
        1.0842e-07, 6.5033e-02, 7.4618e-08, 1.6570e-01, 8.5325e-02, 1.1418e-07,
        5.5346e-08, 5.2788e-02, 3.0981e-02, 1.5518e-07, 5.1780e-02, 2.3857e-02,
        8.0123e-02, 3.5741e-02, 3.0042e-02, 3.4132e-08, 7.6473e-03, 3.2190e-08,
        5.4294e-08, 5.2075e-08, 6.9864e-02, 6.0481e-02, 7.7064e-08, 2.7836e-07,
        1.6157e-02, 1.5518e-07, 1.7525e-02, 4.3681e-02, 5.3554e-02, 3.7593e-02,
        1.2604e-02, 1.1418e-07, 8.2688e-03, 3.3850e-02, 8.0922e-02, 5.0325e-08,
        2.6727e-02, 8.1605e-02, 1.9419e-02, 2.7502e-02, 7.3742e-02, 6.0117e-08,
        4.3528e-02, 1.4654e-01, 1.0921e-02, 1.1643e-07, 5.5084e-02, 3.3483e-02,
        4.9021e-02, 8.4126e-08, 2.7659e-02, 1.4903e-01, 1.1417e-07, 2.4259e-06,
        5.6058e-02, 6.5939e-08, 4.1849e-08, 6.4799e-02, 2.8165e-02, 1.0690e-01,
        3.2380e-02, 8.2027e-02, 5.1939e-02, 5.5823e-02, 5.1924e-02, 4.5372e-02,
        3.1340e-02, 9.1567e-02, 6.6806e-02, 8.3202e-08, 1.3687e-02, 3.3548e-02,
        5.4242e-02, 6.3491e-02, 4.5710e-02, 1.5315e-02, 4.1129e-02, 5.2075e-08,
        7.1487e-02, 9.7521e-02, 9.3998e-08, 3.2804e-08, 1.2961e-01, 1.7076e-07,
        2.6695e-02, 5.4668e-02, 8.8449e-02, 5.4294e-08, 1.0771e-01, 3.2544e-08,
        5.1396e-08, 6.8104e-02, 8.4934e-08, 5.3004e-08, 5.3004e-08, 1.5918e-07,
        1.1422e-01, 3.7475e-08, 4.3148e-02, 2.7108e-02, 5.0325e-08, 8.2856e-03,
        5.1902e-02, 7.7903e-02, 6.0064e-02, 5.2144e-02, 2.0924e-02, 1.5918e-07,
        6.9128e-02, 5.6299e-02, 3.4249e-02, 1.7076e-07, 7.3950e-02, 1.8692e-02,
        1.5518e-07, 4.2234e-02, 6.3037e-02, 3.8786e-02, 4.2412e-02, 7.8980e-02,
        7.4419e-08, 2.1744e-08, 3.4527e-02, 2.5027e-02, 1.2971e-02, 7.2706e-02,
        1.0843e-07, 1.7076e-07, 7.5779e-08, 5.7226e-02, 4.3612e-02, 3.1931e-02,
        6.6416e-02, 2.7836e-07, 1.5918e-07, 2.3385e-02, 1.9309e-07, 9.7375e-03,
        5.0325e-08, 4.4359e-02, 7.0002e-08, 5.6230e-02, 3.3078e-02, 1.4236e-07,
        7.6638e-08, 1.0158e-02, 1.2060e-02, 4.4455e-02, 6.0721e-08, 2.3836e-02,
        1.7076e-07, 7.6827e-02, 1.7881e-02, 5.4061e-02, 4.5155e-08, 5.6800e-02,
        1.9758e-02, 2.3540e-02, 7.8299e-08, 3.7475e-08, 1.3248e-07, 8.0831e-02,
        1.3298e-07, 7.7142e-03, 1.6234e-01, 1.9857e-07, 1.2171e-01, 5.0325e-08,
        1.8721e-01, 1.3248e-07, 1.9857e-07, 6.0428e-08, 3.6952e-02, 2.4629e-02,
        8.4934e-08, 1.8927e-02, 1.3248e-07, 3.2448e-02, 6.0117e-08, 9.4441e-03,
        2.9675e-02, 1.8941e-02, 7.3037e-08, 6.9393e-03, 7.3907e-02, 1.3954e-07,
        7.7064e-08, 4.5155e-08, 5.3915e-02, 3.1559e-02, 1.0511e-02, 3.4284e-02,
        9.8105e-02, 6.0117e-08, 3.4132e-08, 5.0325e-08, 2.1932e-02, 4.5155e-08,
        1.9857e-07, 7.9493e-08, 5.7435e-02, 2.1666e-07, 5.3879e-02, 3.3694e-02,
        1.0524e-02, 4.4261e-02, 2.7467e-02, 3.2719e-02, 6.3469e-02, 6.5683e-02,
        7.4197e-02, 2.5254e-02, 3.1240e-02, 9.4432e-03, 1.5518e-07, 8.5961e-02,
        4.3534e-02, 4.4850e-02, 6.5805e-02, 1.0843e-07, 6.0428e-08, 1.2052e-02,
        2.1133e-02, 1.6390e-07, 3.4844e-02, 2.5104e-02, 1.5918e-07, 1.2143e-02,
        5.0325e-08, 8.9660e-02, 5.6454e-03, 7.6151e-08, 2.8060e-02, 5.0836e-02,
        6.3852e-02, 5.3606e-02, 2.3280e-02, 7.3890e-02, 8.2037e-02, 3.0055e-02,
        1.2637e-02, 3.3470e-02, 1.0891e-07, 1.7076e-07, 8.1825e-08, 4.2643e-08,
        4.7789e-02, 8.4126e-08, 1.8165e-01, 2.7836e-07, 1.1418e-07, 1.0239e-02,
        6.9454e-08, 6.0315e-06, 1.3248e-07, 4.1849e-08, 2.6498e-02, 9.4555e-08,
        3.4096e-02, 1.5986e-02, 2.4233e-02, 5.6359e-02, 4.1986e-08, 6.1118e-02,
        1.5518e-07, 4.9289e-02, 8.0280e-08, 5.5463e-02, 5.7711e-03, 8.1825e-08,
        3.4005e-02, 7.4419e-08, 1.1712e-02, 1.9386e-02, 3.9112e-02, 4.1444e-02,
        3.7070e-02, 7.9822e-02, 4.7577e-02, 3.4765e-02, 8.6383e-02, 5.4106e-02,
        7.7064e-08, 1.3298e-07, 2.7836e-07, 1.7076e-07, 5.9384e-02, 3.8051e-02,
        3.2409e-02, 1.2804e-02, 1.0843e-07, 1.0843e-07, 1.5918e-07, 8.4561e-02,
        1.4834e-01, 3.4985e-07, 2.5867e-02, 2.1488e-02, 3.6545e-02, 7.9806e-08,
        5.8066e-02, 6.1364e-02, 7.2180e-04, 3.2039e-07, 1.1643e-07, 5.3813e-02,
        3.8733e-02, 4.6075e-02, 1.4610e-01, 4.0158e-02, 1.1418e-07, 3.4801e-02,
        5.3004e-08, 5.6985e-02, 2.4745e-02, 5.0325e-08, 6.2861e-02, 7.0086e-02,
        3.6098e-02, 1.9516e-07, 7.4419e-08, 1.8878e-02, 7.6151e-08, 1.6673e-01,
        3.5775e-02, 5.3247e-02, 2.4355e-07, 4.2385e-02, 7.8341e-02, 6.5866e-02,
        2.0143e-02, 2.7836e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.2399e-02, 8.6147e-02, 5.7525e-02, 3.9940e-02, 3.7059e-02, 5.8360e-02,
        6.6290e-08, 7.5822e-08, 5.9991e-02, 7.6214e-02, 1.9608e-02, 4.7646e-02,
        8.8321e-08, 2.6044e-02, 5.1054e-02, 6.3047e-02, 1.8045e-07, 2.1628e-07,
        5.8697e-02, 4.6813e-02, 8.2758e-02, 4.3877e-02, 8.2145e-02, 1.0305e-07,
        2.6381e-02, 2.2514e-02, 5.6471e-02, 7.5822e-08, 5.8364e-03, 1.0942e-07,
        1.2387e-07, 9.0246e-03, 9.1980e-02, 2.6738e-07, 3.5126e-02, 1.3836e-07,
        2.9382e-02, 3.7548e-02, 9.0644e-08, 6.0307e-08, 6.4337e-02, 2.9392e-07,
        5.2216e-02, 6.8881e-02, 1.1381e-07, 7.5822e-08, 7.1118e-02, 5.1046e-02,
        4.2834e-02, 3.8194e-07, 4.9681e-02, 1.8259e-07, 3.8209e-02, 1.2514e-02,
        2.6738e-07, 7.6381e-02, 6.4987e-02, 5.5826e-02, 5.1388e-02, 6.9199e-02,
        1.8810e-02, 1.1716e-02, 6.6290e-08, 7.5822e-08, 4.8582e-02, 7.5363e-03,
        8.0705e-02, 6.6290e-08, 2.6738e-07, 4.7011e-07, 4.8543e-02, 5.9858e-03,
        7.4879e-02, 6.6157e-03, 5.2617e-08, 3.3516e-02, 6.6517e-02, 7.1581e-08,
        2.2758e-07, 2.6835e-07, 1.2387e-07, 7.7198e-02, 7.4968e-02, 1.7360e-02,
        7.7617e-02, 6.4325e-02, 4.5380e-02, 1.4101e-02, 1.9851e-02, 5.6474e-02,
        7.0919e-02, 5.7101e-02, 7.5822e-08, 6.1969e-03, 9.5462e-02, 7.5822e-08,
        3.9252e-02, 3.8255e-02, 5.2617e-08, 3.8181e-02, 6.4265e-02, 6.9237e-08,
        2.6738e-07, 4.6110e-02, 6.6290e-08, 3.6079e-07, 2.6738e-07, 6.1269e-02,
        5.3925e-02, 4.2329e-03, 3.2292e-02, 3.7963e-02, 7.9907e-02, 4.6657e-02,
        7.5822e-08, 1.2387e-07, 8.5555e-02, 1.0043e-01, 4.7998e-02, 2.0651e-02,
        3.1907e-02, 7.9033e-03, 1.2387e-07, 8.1674e-02, 2.6738e-07, 4.6811e-02,
        3.7128e-02, 4.7011e-07, 5.4523e-02, 8.6980e-02, 1.7024e-07, 4.0241e-02,
        8.6993e-02, 4.3433e-02, 5.2617e-08, 8.7605e-02, 7.7140e-02, 1.4448e-07,
        1.0181e-02, 2.9252e-02, 6.6290e-08, 4.3813e-02, 2.7473e-02, 1.1570e-07,
        9.8342e-08, 1.0119e-08, 8.8321e-08, 7.4963e-02, 9.1567e-02, 5.1402e-02,
        1.3883e-07, 4.4261e-02, 4.4590e-02, 6.4314e-02, 2.1766e-02, 6.8873e-03,
        2.6835e-07, 1.3836e-07, 3.9062e-02, 1.9761e-02, 6.6290e-08, 1.1050e-01,
        2.2758e-07, 4.6747e-02, 5.4548e-02, 9.1113e-02, 2.2758e-07, 2.9730e-02,
        2.8977e-02, 6.9027e-02, 7.5822e-08, 9.9829e-02, 3.3722e-02, 2.4838e-02,
        2.6738e-07, 1.0919e-01, 2.5286e-02, 1.1506e-02, 7.7113e-02, 5.2631e-08,
        1.8281e-07, 4.2572e-02, 1.4448e-07, 4.1312e-02, 6.9649e-02, 2.6738e-07,
        8.8321e-08, 3.9833e-02, 5.6126e-02, 3.5241e-07, 3.7205e-03, 8.5861e-02,
        4.6379e-02, 9.0412e-02, 2.0217e-02, 1.0119e-08, 7.4093e-02, 3.5093e-08,
        8.8321e-08, 6.0307e-08, 1.1253e-01, 3.1222e-02, 5.2617e-08, 2.4875e-07,
        5.5225e-02, 2.2758e-07, 4.9271e-02, 5.7236e-02, 5.1472e-02, 3.4971e-02,
        2.0303e-02, 2.2758e-07, 7.1122e-02, 6.1717e-02, 5.0022e-02, 6.6290e-08,
        3.8790e-02, 6.7584e-02, 1.0344e-01, 4.8789e-02, 5.7998e-02, 2.6738e-07,
        4.6853e-02, 3.2604e-02, 6.2504e-08, 3.5241e-07, 4.0128e-02, 5.5659e-02,
        4.2609e-02, 2.6738e-07, 2.1063e-02, 3.9993e-03, 7.5822e-08, 4.0842e-07,
        2.3171e-03, 6.6290e-08, 1.5035e-07, 6.1208e-02, 3.9938e-02, 4.6438e-03,
        3.9767e-03, 4.0612e-02, 6.8398e-02, 6.5571e-02, 4.2908e-02, 4.7145e-02,
        4.0152e-02, 6.2242e-02, 5.1682e-02, 2.2285e-07, 9.4758e-02, 6.7839e-02,
        2.8699e-02, 4.8885e-02, 7.8284e-02, 9.0121e-02, 4.8249e-02, 3.9494e-07,
        2.9723e-02, 5.5028e-02, 8.8321e-08, 5.6897e-02, 7.9659e-03, 1.2387e-07,
        1.3132e-02, 6.3241e-02, 4.0650e-02, 6.0307e-08, 5.7171e-02, 7.5822e-08,
        2.0884e-02, 3.7440e-02, 9.1560e-08, 6.0307e-08, 2.2758e-07, 3.5093e-08,
        5.3149e-02, 2.6738e-07, 6.2022e-02, 4.4708e-02, 7.5530e-08, 7.9829e-02,
        7.5881e-02, 3.4156e-02, 4.5575e-02, 4.6329e-02, 7.2070e-02, 8.8321e-08,
        7.5374e-02, 5.1834e-02, 5.3314e-02, 6.0307e-08, 8.3415e-02, 7.0563e-08,
        1.2387e-07, 4.9163e-02, 5.0079e-03, 1.1189e-01, 6.6300e-02, 2.4756e-02,
        8.8321e-08, 2.6300e-07, 8.2259e-02, 5.5903e-02, 2.2614e-02, 6.2068e-02,
        6.6290e-08, 4.7978e-07, 9.0011e-03, 5.2089e-02, 4.2849e-02, 8.7759e-02,
        4.9275e-02, 3.5240e-07, 6.6290e-08, 6.4341e-02, 1.4448e-07, 1.1634e-01,
        6.6290e-08, 7.8404e-02, 2.0886e-07, 5.7988e-02, 6.6264e-02, 6.6290e-08,
        8.8321e-08, 7.5431e-02, 2.4962e-02, 5.2812e-02, 7.5822e-08, 9.4559e-02,
        2.6835e-07, 6.4109e-02, 5.1315e-02, 5.2199e-02, 2.4875e-07, 5.3413e-02,
        2.5770e-02, 5.5514e-02, 5.2177e-02, 4.7978e-07, 8.8321e-08, 3.9621e-02,
        5.2617e-08, 6.5158e-02, 3.1741e-03, 6.6290e-08, 5.4444e-02, 6.6290e-08,
        4.6923e-02, 2.6738e-07, 2.6738e-07, 2.5554e-07, 6.5139e-02, 1.1414e-01,
        3.5093e-08, 1.4313e-07, 1.8045e-07, 5.0948e-02, 6.6290e-08, 1.0475e-01,
        3.5712e-02, 2.3243e-02, 3.1747e-02, 8.0652e-02, 4.4383e-02, 6.6290e-08,
        6.0307e-08, 8.0590e-08, 4.5846e-03, 1.1374e-07, 2.5709e-02, 6.1413e-02,
        4.4372e-02, 3.5093e-08, 6.0307e-08, 7.5822e-08, 2.6907e-02, 2.2758e-07,
        1.2387e-07, 2.5501e-07, 3.8237e-02, 1.9423e-02, 6.0944e-02, 6.4995e-02,
        7.7048e-02, 4.9682e-02, 8.0623e-02, 3.6269e-02, 5.3335e-02, 4.8544e-02,
        5.2298e-02, 3.7745e-02, 3.5863e-02, 8.0127e-02, 3.6182e-07, 2.3035e-03,
        5.9937e-02, 3.0974e-02, 4.7145e-02, 3.6079e-07, 6.6290e-08, 6.1272e-02,
        7.5518e-02, 3.5580e-07, 4.8989e-02, 7.8239e-02, 8.8321e-08, 1.1101e-01,
        2.0525e-07, 4.1507e-03, 7.3548e-02, 1.8045e-07, 3.9151e-02, 7.7499e-02,
        5.0044e-02, 5.6981e-03, 5.3370e-02, 5.6878e-02, 4.1824e-02, 8.0382e-02,
        3.7244e-02, 6.1885e-02, 2.2108e-07, 4.0655e-08, 3.7067e-07, 4.3062e-02,
        4.3485e-02, 6.6290e-08, 4.0627e-03, 1.1387e-07, 2.6738e-07, 3.9614e-02,
        1.3296e-02, 6.6254e-08, 6.6290e-08, 3.7069e-07, 5.5347e-02, 6.6290e-08,
        8.6028e-02, 3.9799e-02, 5.5887e-02, 4.1308e-02, 3.3709e-08, 5.3353e-02,
        2.2758e-07, 2.9022e-02, 4.4281e-02, 3.0980e-02, 4.7431e-02, 7.5822e-08,
        7.8839e-02, 2.0525e-07, 5.1302e-02, 7.5327e-02, 8.5012e-02, 2.7929e-02,
        3.3785e-02, 7.0389e-02, 5.6771e-02, 2.6735e-02, 4.6451e-02, 2.4272e-02,
        9.1560e-08, 7.5822e-08, 1.2387e-07, 4.7978e-07, 5.6200e-02, 5.6675e-02,
        9.4929e-08, 6.5425e-02, 6.6290e-08, 2.2758e-07, 8.8321e-08, 1.9966e-02,
        2.5002e-03, 2.6738e-07, 5.4094e-02, 2.1325e-02, 7.5674e-02, 5.8975e-02,
        5.2972e-03, 4.6531e-03, 3.7183e-02, 6.6290e-08, 7.5822e-08, 5.8669e-02,
        1.5176e-07, 5.8545e-02, 4.8094e-02, 9.2669e-08, 3.1651e-07, 3.3686e-02,
        2.6738e-07, 4.4835e-02, 6.4047e-02, 2.1628e-07, 4.0181e-03, 6.9264e-02,
        4.4052e-02, 1.0785e-07, 5.2617e-08, 5.9824e-02, 9.9616e-08, 4.0246e-02,
        3.2163e-02, 1.1570e-02, 5.2617e-08, 7.1579e-02, 3.8939e-03, 3.7892e-02,
        9.2967e-02, 2.0886e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.8750e-07, 1.6749e-01, 1.0523e-01, 4.5406e-02, 4.4620e-02, 6.1801e-02,
        3.2519e-07, 4.9621e-02, 4.8823e-07, 5.3753e-02, 5.6828e-02, 8.6697e-08,
        6.1713e-07, 6.1713e-07, 5.6411e-07, 2.3384e-07, 4.1805e-02, 6.3123e-02,
        5.6411e-07, 3.3305e-02, 6.1611e-07, 1.7467e-07, 5.5047e-02, 6.9761e-02,
        3.9320e-07, 5.6411e-07, 3.7353e-07, 5.3672e-07, 1.0031e-01, 5.4788e-02,
        2.4109e-07, 7.8318e-02, 5.6624e-02, 3.0577e-07, 2.6614e-02, 1.7467e-07,
        2.4109e-07, 6.6151e-02, 5.4225e-07, 4.7623e-02, 5.4059e-02, 4.9726e-02,
        1.8750e-07, 3.9320e-07, 5.3672e-07, 7.0426e-02, 6.7945e-02, 8.5292e-07,
        2.3384e-07, 2.6257e-07, 3.6140e-07, 6.1447e-02, 7.8599e-02, 7.9017e-02,
        5.4225e-07, 4.8396e-02, 2.0320e-02, 1.8991e-07, 8.6697e-08, 4.4949e-02,
        4.5372e-02, 6.8079e-02, 3.7393e-02, 5.3106e-02, 1.0892e-07, 3.9646e-07,
        4.4215e-02, 4.9683e-02, 3.6797e-07, 3.4380e-02, 2.0879e-07, 3.6253e-07,
        5.0665e-02, 5.7391e-02, 3.2519e-07, 5.5681e-02, 7.3352e-02, 5.4130e-02,
        6.1204e-02, 1.1079e-07, 2.4109e-07, 8.0320e-02, 6.0432e-02, 5.2895e-02,
        6.6479e-02, 4.9907e-02, 3.9320e-07, 5.5104e-02, 6.7100e-02, 4.2164e-07,
        5.0784e-02, 1.0225e-06, 4.8823e-07, 6.6940e-02, 3.9320e-07, 6.4361e-02,
        6.0489e-02, 5.1455e-02, 3.7641e-02, 4.8670e-02, 5.5864e-02, 5.7892e-08,
        5.3940e-02, 5.2950e-02, 4.1228e-02, 1.1080e-07, 5.6411e-07, 9.5703e-07,
        5.0269e-02, 3.3010e-02, 3.0577e-07, 1.8750e-07, 4.6399e-02, 3.0577e-07,
        5.0005e-02, 5.9108e-02, 5.3195e-02, 4.7507e-02, 5.6786e-02, 1.8750e-07,
        2.3384e-07, 7.0742e-02, 3.7353e-07, 7.5976e-02, 4.2164e-07, 5.8628e-02,
        4.5537e-02, 5.4476e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1815e-01, 4.8778e-07, 1.5524e-01, 1.4380e-06, 1.2684e-01, 4.4894e-07,
        1.1779e-01, 1.4320e-06, 1.5602e-06, 4.8927e-07, 6.4625e-02, 2.8223e-07,
        1.0587e-01, 2.6133e-07, 1.8653e-07, 1.2610e-01, 1.3054e-01, 6.6655e-02,
        2.5626e-07, 2.6133e-07, 1.5292e-01, 1.0288e-01, 8.5556e-02, 1.1464e-01,
        8.8354e-07, 1.2375e-01, 1.8653e-07, 1.0999e-01, 8.8354e-07, 1.0039e-07,
        1.1179e-01, 7.0004e-07, 2.6133e-07, 7.0004e-07, 1.2323e-01, 1.2877e-01,
        2.8223e-07, 2.0772e-07, 1.8509e-06, 2.6133e-07, 1.5602e-06, 2.6133e-07,
        7.2405e-07, 3.9321e-07, 1.1514e-01, 7.6805e-02, 1.2227e-01, 2.6133e-07,
        1.0930e-01, 2.6133e-07, 1.0169e-01, 1.0358e-01, 1.3828e-01, 1.0487e-01,
        8.8354e-07, 1.4380e-06, 6.8067e-07, 1.0984e-01, 3.9638e-07, 1.5428e-01,
        1.1319e-01, 1.2279e-01, 1.4312e-06, 1.0251e-01, 4.2371e-07, 1.1898e-01,
        2.6133e-07, 1.0781e-01, 4.4894e-07, 8.0815e-08, 1.4312e-01, 1.3337e-01,
        2.6133e-07, 5.0672e-07, 1.3625e-01, 1.0797e-01, 4.2371e-07, 1.0570e-06,
        9.6091e-02, 2.9433e-07, 2.6133e-07, 1.0039e-07, 3.0558e-07, 1.0233e-01,
        8.8308e-02, 1.3121e-01, 1.3110e-01, 1.2270e-01, 1.0933e-01, 6.0188e-07,
        3.2977e-07, 1.1897e-01, 1.0881e-01, 1.0070e-01, 9.6518e-02, 1.1430e-01,
        2.8223e-07, 1.1898e-01, 9.4930e-02, 1.8653e-07, 1.9112e-06, 1.4380e-01,
        7.8967e-07, 6.5085e-07, 9.4703e-02, 1.3862e-01, 4.8778e-07, 1.5602e-06,
        1.3285e-01, 1.0960e-06, 8.8354e-07, 1.0910e-01, 1.5602e-06, 3.9181e-07,
        8.2528e-02, 1.1804e-01, 3.9181e-07, 2.6133e-07, 6.6452e-07, 1.4354e-01,
        1.1697e-01, 3.0793e-07, 9.7515e-02, 5.7759e-07, 9.9062e-02, 1.3777e-01,
        5.7759e-07, 1.4491e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.6898e-02, 7.8696e-02, 1.9098e-01, 5.2796e-02, 7.1728e-02, 7.5126e-02,
        2.2667e-02, 5.0765e-02, 9.0053e-02, 5.0673e-02, 3.5450e-02, 8.0829e-02,
        5.3771e-02, 6.3445e-02, 7.3162e-02, 4.9515e-02, 1.5755e-01, 1.4514e-01,
        4.0624e-02, 5.7893e-02, 4.0977e-02, 4.0610e-02, 6.4429e-02, 2.1293e-07,
        4.6093e-02, 1.9158e-02, 7.6130e-02, 1.3694e-01, 9.2623e-03, 2.6471e-03,
        1.4554e-07, 5.3691e-02, 5.5485e-02, 3.5321e-08, 3.4588e-02, 5.8690e-02,
        1.3068e-02, 1.4003e-07, 1.0655e-07, 8.1606e-08, 3.8303e-02, 7.5135e-08,
        5.7914e-02, 4.6328e-02, 1.3744e-07, 1.7165e-01, 1.2506e-02, 7.1053e-02,
        8.4085e-02, 5.5501e-08, 4.3930e-02, 1.9412e-02, 1.8162e-02, 2.5491e-02,
        1.7470e-07, 4.2150e-02, 4.9911e-02, 4.8905e-02, 3.6606e-02, 1.9522e-02,
        2.6026e-02, 1.4365e-01, 1.7470e-07, 1.0478e-07, 6.6837e-02, 6.2019e-02,
        7.1590e-02, 6.8193e-08, 1.1742e-07, 1.7470e-07, 5.5692e-02, 5.0829e-02,
        7.0504e-02, 4.9436e-02, 5.0383e-02, 7.9680e-02, 5.8188e-02, 1.6775e-02,
        1.2998e-02, 6.0048e-08, 6.1484e-02, 1.5411e-02, 5.7728e-02, 1.1254e-02,
        4.3159e-02, 5.1467e-02, 1.4761e-02, 5.2267e-03, 2.0111e-02, 6.3451e-02,
        6.1892e-02, 7.8359e-02, 7.8037e-02, 3.2738e-02, 5.5692e-02, 1.1641e-07,
        3.5038e-02, 7.3239e-02, 7.8011e-08, 5.9818e-02, 5.5320e-02, 1.9120e-07,
        7.8619e-02, 8.1854e-02, 5.5534e-08, 1.7014e-01, 7.5094e-08, 1.3158e-02,
        6.6037e-02, 6.6793e-02, 5.9806e-02, 3.2898e-02, 4.8324e-02, 3.5910e-02,
        9.0332e-08, 4.2164e-08, 2.6007e-02, 3.5182e-02, 2.4334e-02, 2.9479e-02,
        1.0927e-07, 4.3880e-02, 1.5268e-07, 4.5898e-02, 1.9978e-07, 6.6598e-02,
        6.1819e-02, 6.8192e-08, 6.9064e-02, 6.0692e-02, 1.1941e-07, 7.7590e-02,
        5.5788e-02, 6.4624e-02, 1.7790e-01, 8.0830e-02, 7.6985e-02, 2.6055e-02,
        1.8942e-02, 1.9575e-02, 3.4487e-02, 4.1638e-02, 1.7390e-02, 4.3288e-03,
        5.5501e-08, 3.9619e-08, 8.1607e-08, 5.5345e-02, 3.8981e-02, 8.5171e-02,
        6.8193e-08, 3.9507e-02, 8.9350e-02, 2.1291e-02, 2.5517e-02, 3.9065e-02,
        9.5621e-02, 3.9889e-02, 3.2446e-02, 6.3512e-03, 1.0478e-07, 5.1079e-02,
        1.1742e-07, 4.8375e-02, 2.5689e-02, 5.3784e-02, 1.9912e-07, 4.8150e-02,
        3.3229e-02, 2.7677e-02, 2.5464e-02, 7.8527e-02, 6.0556e-02, 5.0237e-02,
        1.4453e-01, 7.1188e-02, 7.2673e-02, 5.0409e-02, 6.9261e-02, 3.8169e-08,
        1.1641e-07, 5.6674e-02, 1.7792e-01, 2.0711e-02, 5.8788e-02, 7.5094e-08,
        5.0969e-02, 1.0553e-02, 7.1042e-02, 8.5304e-08, 5.8158e-02, 2.0113e-02,
        3.3313e-02, 3.7678e-02, 5.9222e-02, 5.9370e-08, 5.1129e-02, 2.6867e-07,
        1.4555e-01, 1.5047e-07, 5.3830e-02, 9.3101e-02, 4.3984e-02, 7.5094e-08,
        3.8957e-02, 7.7843e-08, 7.2220e-02, 3.9610e-02, 9.5348e-02, 5.3628e-02,
        2.1438e-02, 1.0478e-07, 3.6163e-02, 6.0614e-02, 3.1915e-02, 5.1220e-02,
        6.1818e-02, 2.2165e-02, 3.1530e-02, 5.6160e-02, 5.7468e-02, 3.8492e-08,
        3.3504e-02, 7.4692e-02, 1.2409e-02, 5.5457e-02, 6.2499e-02, 9.6856e-03,
        1.2593e-07, 1.9912e-07, 7.1181e-02, 3.8291e-02, 9.3899e-08, 1.7606e-07,
        7.5270e-02, 6.4963e-02, 2.1859e-07, 4.3658e-02, 1.5776e-01, 3.4636e-02,
        5.8116e-02, 4.5645e-02, 2.2808e-02, 2.1474e-02, 7.1413e-02, 7.2167e-02,
        1.3136e-02, 3.6234e-02, 2.1417e-02, 6.0036e-02, 5.8082e-02, 5.3704e-02,
        7.3326e-02, 2.2184e-02, 8.6843e-02, 7.2474e-02, 8.6431e-02, 3.8492e-08,
        6.4137e-02, 4.4602e-02, 4.8803e-02, 1.1891e-07, 4.2232e-02, 1.1742e-07,
        1.2801e-07, 4.6511e-02, 4.1516e-02, 1.1742e-07, 1.5833e-01, 1.3132e-01,
        3.3216e-02, 4.8709e-02, 3.5344e-09, 1.8872e-07, 7.1320e-02, 3.0954e-08,
        1.9578e-02, 1.1364e-07, 6.1394e-02, 1.1042e-02, 6.6461e-02, 3.4901e-02,
        4.0150e-02, 3.4517e-02, 1.4030e-01, 5.1625e-02, 7.5705e-02, 4.2739e-02,
        3.7357e-02, 3.8866e-02, 6.7425e-02, 7.5094e-08, 2.3362e-02, 4.8810e-02,
        5.5501e-08, 3.0852e-02, 6.2928e-02, 5.8210e-02, 6.8780e-02, 4.3304e-02,
        1.9912e-07, 5.6227e-08, 4.4388e-02, 6.1907e-02, 2.9743e-02, 6.3265e-02,
        7.7762e-02, 1.7429e-07, 2.9061e-02, 6.6139e-02, 5.3577e-02, 2.9826e-02,
        4.5808e-02, 1.2748e-07, 1.3245e-07, 1.3472e-02, 1.6821e-01, 7.3502e-02,
        1.4554e-07, 4.3836e-02, 3.3393e-02, 8.1421e-02, 6.2576e-02, 5.5246e-02,
        5.0192e-02, 1.0390e-02, 1.2808e-02, 5.0782e-02, 2.6867e-07, 5.4249e-02,
        1.1641e-07, 5.8419e-02, 8.8997e-02, 4.8674e-02, 1.9978e-07, 6.1198e-02,
        5.8331e-02, 5.3587e-02, 4.6417e-02, 6.0183e-08, 7.5094e-08, 3.0221e-02,
        1.0488e-07, 3.4603e-02, 2.9361e-02, 7.5094e-08, 1.6826e-01, 6.7286e-08,
        4.0762e-02, 1.0654e-07, 6.6312e-08, 7.2529e-02, 9.3216e-02, 5.1062e-02,
        7.5094e-08, 1.2983e-07, 4.8195e-02, 3.2903e-02, 7.8011e-08, 6.2058e-02,
        6.5390e-02, 7.3973e-02, 3.2584e-02, 6.7415e-02, 5.1869e-02, 5.5516e-02,
        7.3943e-02, 5.9226e-02, 2.4128e-02, 1.3916e-07, 2.3155e-02, 8.3139e-02,
        4.4114e-02, 4.3524e-02, 6.1430e-02, 6.2116e-02, 1.4136e-07, 2.6867e-07,
        1.0655e-07, 6.8193e-08, 2.9607e-02, 2.6845e-02, 3.8511e-02, 7.4820e-02,
        6.2301e-02, 3.4866e-02, 7.3349e-02, 7.5196e-08, 8.3089e-02, 5.3690e-02,
        3.3786e-02, 8.2635e-02, 9.2363e-02, 7.9530e-02, 7.1360e-02, 7.3170e-03,
        5.4187e-02, 2.9222e-02, 2.4552e-02, 1.2644e-07, 9.3899e-08, 5.8181e-02,
        2.1868e-02, 3.8492e-08, 5.8716e-02, 2.0975e-02, 1.7470e-07, 5.8523e-02,
        1.7470e-07, 1.0468e-02, 4.3123e-02, 3.5223e-07, 4.9833e-02, 5.9128e-02,
        4.9935e-02, 4.3713e-02, 4.8216e-02, 6.2012e-02, 4.2260e-02, 3.8101e-02,
        1.5978e-01, 6.9409e-02, 3.7180e-08, 4.6732e-08, 1.6464e-07, 5.8347e-02,
        2.5797e-02, 5.9045e-02, 3.7562e-02, 5.0042e-08, 1.4732e-07, 7.4336e-02,
        4.3387e-02, 3.8435e-08, 8.5330e-08, 5.5501e-08, 7.4738e-02, 2.7455e-07,
        6.8936e-02, 5.3647e-02, 4.7358e-02, 7.5406e-02, 1.8872e-07, 3.1376e-02,
        3.7867e-02, 4.5959e-02, 6.6145e-02, 1.6238e-02, 4.8994e-02, 1.9978e-07,
        6.0341e-02, 6.4814e-02, 5.6765e-03, 7.5152e-02, 4.0206e-02, 6.3934e-02,
        3.0990e-02, 8.8735e-02, 3.5938e-02, 5.7865e-02, 5.3206e-02, 3.5529e-02,
        1.7770e-07, 2.3143e-08, 9.5184e-03, 4.7803e-03, 6.4449e-02, 2.8241e-02,
        5.1194e-08, 3.4770e-02, 1.7770e-07, 3.5240e-07, 8.1001e-02, 7.9532e-02,
        3.6564e-02, 6.7283e-08, 1.6467e-02, 3.0719e-02, 4.8106e-02, 6.5090e-02,
        7.6948e-02, 1.5933e-01, 2.1951e-01, 1.1641e-07, 3.7180e-08, 2.7729e-02,
        5.5339e-08, 5.6864e-02, 8.8128e-02, 3.7576e-02, 7.2667e-02, 1.7715e-07,
        2.6867e-07, 4.5626e-02, 5.2694e-02, 6.8834e-02, 6.6145e-02, 1.2679e-01,
        3.4456e-02, 1.2644e-07, 4.0701e-02, 2.0557e-02, 6.4914e-08, 1.7293e-02,
        6.9372e-02, 5.1921e-02, 1.7155e-07, 4.7458e-02, 3.1515e-02, 6.8565e-02,
        5.9661e-02, 1.4732e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.9674e-07, 2.9058e-08, 9.1511e-07, 1.0460e-02, 4.9847e-07, 1.9479e-07,
        9.1860e-03, 1.3336e-06, 3.4235e-02, 5.0546e-02, 3.7470e-02, 3.5558e-02,
        8.6813e-07, 3.8772e-07, 6.6302e-07, 4.0599e-02, 6.7473e-07, 2.9346e-02,
        4.0929e-07, 3.1347e-02, 1.9783e-02, 2.6495e-07, 2.5885e-03, 6.1371e-07,
        9.3885e-07, 4.6425e-07, 1.6793e-02, 1.7614e-06, 3.5287e-02, 3.6809e-07,
        2.5800e-07, 8.1642e-07, 3.1681e-07, 1.3409e-06, 6.7299e-07, 3.6809e-07,
        1.5250e-06, 2.4968e-02, 2.9058e-08, 5.8800e-07, 1.1952e-01, 3.0509e-02,
        9.5227e-08, 4.1504e-02, 3.0500e-07, 6.0321e-07, 2.9058e-08, 5.8800e-07,
        3.4153e-02, 3.3838e-02, 3.6199e-03, 2.6502e-02, 2.8602e-07, 6.2168e-08,
        3.6870e-07, 5.5209e-07, 2.4525e-02, 3.9673e-07, 1.6077e-02, 9.1511e-07,
        3.6809e-07, 3.4696e-02, 2.3101e-02, 1.7614e-06, 3.5546e-02, 7.6841e-07,
        2.0939e-07, 1.1359e-02, 2.1953e-02, 9.8450e-07, 8.4236e-07, 4.1201e-02,
        1.6558e-07, 2.0779e-03, 3.9673e-07, 5.0027e-07, 1.8685e-07, 3.2629e-02,
        2.8138e-07, 2.0677e-02, 3.4588e-02, 2.9364e-02, 4.4179e-07, 5.0027e-07,
        4.3073e-04, 6.6855e-07, 5.5528e-07, 3.4066e-07, 1.0203e-06, 5.4278e-07,
        7.2927e-07, 3.3952e-07, 2.4051e-07, 4.2010e-07, 3.7074e-02, 2.8025e-02,
        3.6809e-07, 4.9350e-03, 1.1125e-07, 1.4434e-01, 1.6099e-02, 5.5209e-07,
        1.1419e-01, 3.0551e-02, 3.3085e-02, 3.0500e-07, 1.0062e-06, 2.8281e-07,
        9.2183e-07, 3.9694e-02, 2.9194e-07, 1.7969e-07, 9.6638e-07, 2.1804e-02,
        2.4456e-02, 1.4500e-06, 2.6495e-07, 9.6638e-07, 1.0912e-06, 1.6018e-02,
        4.9359e-07, 1.0203e-06, 1.5736e-02, 1.7702e-07, 4.9359e-07, 2.5118e-02,
        3.5880e-02, 9.1511e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1367e-06, 1.3958e-06, 1.1628e-06, 1.7885e-06, 5.0367e-06, 6.3976e-07,
        5.5551e-07, 1.1972e-06, 3.9520e-08, 2.8967e-06, 1.4307e-06, 9.1541e-07,
        2.4877e-06, 1.5346e-06, 1.7342e-06, 8.5669e-02, 2.0674e-06, 1.0481e-01,
        1.3154e-06, 1.0766e-01, 3.2573e-06, 2.9765e-06, 1.4448e-06, 3.4829e-02,
        1.4307e-06, 1.2523e-06, 1.3958e-06, 1.2273e-06, 2.8248e-06, 2.0695e-06,
        7.3907e-07, 1.4638e-06, 6.0488e-07, 7.1814e-03, 1.0211e-01, 2.4150e-06,
        3.9580e-06, 3.0211e-06, 5.3618e-07, 8.3314e-07, 1.2192e-06, 1.1467e-01,
        3.0211e-06, 1.4052e-06, 1.0808e-06, 8.4983e-07, 2.8773e-06, 8.3314e-07,
        1.0669e-07, 1.7821e-02, 5.9285e-07, 1.4145e-06, 4.0047e-06, 1.1202e-06,
        3.7290e-06, 2.2753e-02, 2.1372e-02, 1.4307e-06, 1.0127e-03, 2.6998e-06,
        2.8692e-06, 3.3163e-02, 9.2241e-07, 2.8415e-06, 1.7005e-06, 1.5582e-01,
        1.0065e-06, 3.6971e-06, 3.0211e-06, 2.2266e-06, 2.0913e-06, 1.9195e-06,
        1.3264e-06, 1.5762e-06, 7.6004e-07, 1.5315e-06, 1.3141e-06, 1.0065e-06,
        1.5369e-06, 1.1064e-01, 1.1517e-06, 8.9075e-07, 9.8939e-07, 1.5955e-06,
        6.3578e-07, 2.3383e-06, 3.5796e-02, 1.2794e-06, 2.3192e-06, 1.2350e-06,
        3.3068e-06, 3.0967e-06, 1.1517e-06, 6.7042e-07, 1.6971e-02, 7.9076e-07,
        1.8273e-06, 1.8266e-06, 1.3137e-06, 1.7711e-06, 1.9257e-06, 1.1458e-01,
        2.2266e-06, 8.8738e-04, 1.0328e-01, 3.7871e-06, 1.8333e-06, 1.2350e-06,
        1.2350e-06, 8.6285e-07, 1.0729e-01, 8.7989e-02, 2.0943e-06, 3.0211e-06,
        4.1363e-06, 8.9075e-07, 2.0956e-06, 1.0808e-06, 9.2237e-02, 2.6123e-02,
        8.5334e-02, 1.2407e-06, 7.4372e-07, 3.9520e-08, 3.1602e-06, 1.7111e-02,
        1.7057e-06, 2.5959e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5069e-02, 1.1059e-02, 3.4233e-02, 5.1227e-02, 1.2525e-01, 1.4848e-02,
        9.0439e-08, 1.7746e-07, 3.2528e-02, 1.0415e-02, 1.1298e-07, 1.2471e-02,
        4.0092e-02, 5.6276e-08, 7.4317e-02, 6.2362e-08, 1.4261e-02, 2.3671e-02,
        3.0160e-02, 9.1867e-03, 1.0778e-02, 7.1025e-08, 3.8924e-02, 4.0300e-08,
        1.3707e-07, 1.3394e-02, 3.7538e-02, 1.5665e-02, 3.8230e-08, 2.8789e-07,
        5.9697e-03, 4.7213e-03, 3.9140e-02, 5.2129e-08, 2.3006e-07, 3.2570e-03,
        4.1355e-02, 9.9615e-08, 1.1401e-01, 7.0048e-08, 3.5346e-02, 1.3298e-07,
        2.6020e-02, 2.6940e-02, 2.6711e-07, 4.1215e-02, 1.1541e-02, 1.8200e-02,
        1.2600e-07, 1.5632e-07, 4.1564e-03, 3.2586e-07, 1.2138e-02, 1.9947e-02,
        8.5587e-02, 2.5435e-02, 2.2978e-02, 1.4573e-07, 4.6870e-02, 7.7208e-03,
        2.5267e-02, 1.4179e-02, 4.7128e-02, 1.8452e-01, 1.2395e-02, 1.3086e-01,
        3.1618e-02, 4.5939e-08, 1.4291e-01, 8.9461e-08, 2.3502e-03, 4.6662e-02,
        1.1644e-02, 2.4129e-02, 1.4542e-02, 1.6161e-02, 2.4600e-02, 3.0361e-08,
        9.4546e-03, 2.4387e-07, 1.0537e-07, 8.6873e-03, 2.1585e-02, 2.6535e-07,
        7.9329e-03, 3.0659e-02, 1.5944e-02, 8.1533e-08, 4.4630e-02, 1.3083e-02,
        1.2960e-02, 2.6149e-07, 1.9004e-01, 3.1574e-08, 3.7963e-02, 2.4387e-07,
        1.6391e-02, 3.6657e-02, 2.0896e-07, 2.1466e-02, 3.4106e-02, 2.4687e-08,
        6.9318e-03, 2.6995e-02, 1.0748e-02, 4.7877e-02, 1.2808e-01, 9.9836e-03,
        8.0017e-03, 2.0241e-02, 6.0813e-02, 6.1402e-02, 3.2864e-02, 1.1166e-02,
        2.4387e-07, 1.0508e-07, 1.3575e-02, 2.7134e-02, 2.9939e-02, 1.8471e-07,
        1.7539e-02, 1.0260e-07, 6.2764e-02, 9.6288e-03, 9.8392e-08, 2.1718e-02,
        4.7269e-03, 6.4000e-03, 2.3014e-02, 3.0423e-02, 6.4512e-02, 2.5912e-02,
        1.9293e-02, 6.5277e-02, 7.1367e-03, 8.7369e-03, 1.7791e-07, 1.1379e-07,
        1.0446e-02, 8.0270e-02, 1.2821e-07, 9.1513e-08, 4.4538e-02, 8.0518e-08,
        7.3142e-02, 4.1292e-08, 1.5021e-01, 1.4826e-02, 1.4181e-07, 4.1975e-08,
        6.6275e-08, 2.6658e-07, 3.0913e-02, 7.8958e-08, 6.2311e-08, 7.3409e-03,
        2.1746e-07, 3.2664e-03, 3.9148e-02, 2.7670e-02, 1.5639e-07, 2.4368e-02,
        6.2453e-02, 6.2728e-08, 1.5243e-02, 2.0877e-02, 4.0338e-02, 1.2185e-07,
        1.7106e-02, 1.9347e-02, 6.4854e-02, 3.2103e-02, 6.7513e-02, 3.2708e-02,
        8.3076e-03, 1.4950e-02, 2.7335e-02, 4.5116e-02, 1.0538e-02, 6.8398e-08,
        2.9473e-08, 5.6288e-03, 1.4165e-02, 6.6776e-02, 6.8136e-03, 1.3168e-07,
        7.7861e-08, 8.2249e-08, 8.9840e-03, 1.5639e-07, 1.9897e-02, 2.2770e-02,
        9.2045e-02, 2.0465e-02, 5.4413e-03, 1.4389e-01, 2.2623e-02, 1.7414e-01,
        1.4682e-02, 1.1075e-03, 1.0008e-02, 2.9069e-02, 5.1461e-08, 1.8024e-07,
        6.9180e-08, 5.7241e-08, 1.3428e-02, 4.7493e-08, 3.1245e-02, 6.1032e-02,
        1.7513e-01, 7.4627e-06, 2.3726e-02, 2.0633e-02, 1.1276e-02, 2.1626e-02,
        1.6115e-02, 1.9512e-02, 1.6194e-02, 6.1452e-08, 3.6262e-02, 1.1253e-07,
        1.0790e-02, 1.2422e-01, 1.3443e-07, 9.2366e-02, 2.0166e-07, 3.2732e-02,
        1.6393e-02, 1.8329e-07, 1.7572e-02, 1.2529e-02, 2.8708e-07, 6.2600e-08,
        1.8984e-02, 7.1348e-08, 6.0331e-08, 5.0865e-02, 1.0418e-02, 1.6367e-02,
        1.2056e-07, 1.0761e-07, 1.1320e-07, 1.7498e-02, 4.2567e-02, 4.6638e-02,
        2.6650e-07, 1.7623e-07, 1.6388e-02, 1.1007e-07, 1.1576e-02, 4.4059e-02,
        8.2692e-02, 2.2466e-02, 1.4752e-02, 4.9023e-03, 3.3034e-02, 6.4689e-08,
        4.6817e-02, 3.7272e-02, 3.8070e-02, 6.0676e-08, 3.2791e-02, 2.1747e-07,
        8.3876e-08, 1.8699e-02, 2.7547e-02, 1.8551e-07, 1.0546e-02, 1.2727e-02,
        4.0636e-02, 2.1017e-02, 8.1601e-08, 8.7757e-02, 7.5243e-02, 1.6871e-07,
        8.6732e-03, 2.1752e-07, 2.4530e-02, 4.8970e-08, 9.3817e-03, 5.2824e-03,
        1.0142e-02, 6.4935e-02, 6.0501e-02, 8.4804e-08, 2.1794e-02, 6.7842e-08,
        5.0211e-03, 2.0075e-07, 2.8192e-02, 1.8024e-07, 1.4018e-01, 2.6985e-02,
        7.1903e-07, 7.6591e-08, 6.2409e-02, 1.5716e-02, 3.4473e-02, 8.9628e-08,
        9.8392e-08, 2.7355e-07, 4.0971e-03, 3.0795e-02, 5.2061e-08, 1.2867e-01,
        5.8914e-02, 5.8632e-02, 1.6503e-02, 3.9942e-03, 4.9853e-02, 1.3923e-02,
        4.1080e-02, 5.7785e-08, 6.6492e-02, 1.6704e-02, 4.2809e-02, 3.5711e-02,
        7.2407e-02, 3.0328e-08, 1.7393e-07, 1.3379e-07, 6.5754e-03, 5.9219e-02,
        9.1270e-08, 1.0482e-02, 2.9449e-02, 3.4943e-02, 7.1945e-08, 1.5855e-02,
        1.8024e-07, 1.3138e-02, 3.8520e-02, 2.2776e-02, 1.8551e-07, 2.0383e-02,
        6.5111e-02, 6.8152e-03, 2.1266e-07, 2.8698e-07, 8.9194e-08, 3.6825e-08,
        1.8247e-02, 3.0496e-02, 1.0619e-07, 7.8193e-08, 1.3699e-02, 7.8193e-08,
        1.9683e-02, 1.0183e-07, 5.2129e-08, 2.8783e-07, 1.2011e-02, 1.3343e-02,
        4.6170e-08, 1.7026e-07, 3.0191e-02, 2.8694e-02, 3.3648e-02, 4.1648e-02,
        1.7845e-07, 1.2301e-07, 1.1790e-07, 3.8565e-02, 5.6531e-02, 2.1926e-02,
        1.5349e-02, 1.0336e-07, 6.2130e-08, 2.9506e-07, 1.0294e-02, 2.0535e-07,
        2.8020e-02, 9.0000e-08, 4.8986e-03, 1.5885e-07, 6.2205e-08, 8.7049e-03,
        8.9986e-08, 1.4029e-01, 1.7105e-02, 1.3403e-07, 3.8553e-03, 5.1703e-02,
        8.5861e-03, 1.7964e-02, 3.7451e-08, 8.5065e-08, 9.3217e-03, 3.9030e-02,
        9.1831e-03, 1.5327e-07, 2.4343e-02, 9.2627e-03, 9.6708e-03, 9.7504e-08,
        3.4182e-02, 1.3753e-01, 1.4779e-07, 4.8606e-02, 1.2682e-07, 1.1209e-02,
        7.8667e-03, 8.9360e-08, 2.2387e-02, 2.3140e-02, 1.8822e-01, 3.3063e-02,
        1.3521e-01, 7.2142e-08, 2.1934e-02, 1.5407e-01, 7.4436e-03, 1.2782e-02,
        7.2159e-02, 1.4147e-02, 8.8874e-08, 3.2249e-02, 8.0466e-08, 8.1598e-03,
        4.8450e-02, 2.5468e-02, 8.8032e-08, 6.0710e-07, 1.4152e-02, 2.2866e-02,
        4.5421e-03, 1.5631e-07, 1.1651e-02, 1.3802e-07, 6.7449e-02, 3.7255e-02,
        2.2757e-07, 1.0855e-07, 1.7062e-07, 5.6276e-08, 1.8415e-07, 9.3888e-08,
        1.9528e-02, 1.9516e-01, 1.3468e-07, 1.9501e-02, 1.5632e-07, 5.1771e-08,
        1.0446e-02, 3.6129e-02, 9.8918e-08, 5.2556e-02, 1.7077e-02, 2.6618e-02,
        2.5304e-02, 1.5944e-02, 1.4664e-02, 5.5532e-03, 6.2092e-08, 9.8265e-08,
        6.4580e-02, 1.9581e-02, 2.0226e-02, 1.8519e-07, 2.3545e-02, 3.7698e-02,
        6.2061e-08, 5.3216e-02, 1.4729e-07, 7.7821e-08, 2.6349e-02, 1.9006e-02,
        1.8763e-07, 2.2232e-02, 4.4539e-08, 9.5305e-02, 9.2932e-08, 1.1126e-07,
        2.8629e-02, 1.3360e-02, 4.3141e-02, 1.8261e-07, 2.7440e-02, 3.1295e-02,
        5.5587e-08, 1.2195e-07, 4.2932e-02, 1.1317e-07, 1.0047e-07, 1.2464e-02,
        7.9184e-08, 1.4561e-07, 2.2708e-02, 2.0068e-02, 1.0371e-02, 1.7417e-02,
        9.1558e-08, 3.9350e-02, 9.2776e-03, 2.5611e-02, 7.5865e-08, 1.3886e-02,
        2.1684e-07, 6.6671e-02, 1.4423e-07, 7.7610e-08, 6.4689e-08, 5.2784e-02,
        8.2270e-02, 5.6068e-02, 4.6170e-08, 1.2308e-07, 2.1652e-02, 1.3619e-07,
        5.5855e-02, 7.4702e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.8069e-07, 5.3303e-02, 8.9258e-07, 5.3883e-02, 6.4883e-07, 6.1643e-07,
        1.0879e-06, 4.2177e-02, 1.0074e-01, 9.3298e-02, 3.2815e-02, 1.2023e-01,
        3.1196e-07, 5.9642e-02, 1.2574e-07, 4.2191e-07, 6.8023e-07, 1.2555e-06,
        4.2093e-02, 1.9059e-07, 9.0929e-07, 6.6367e-08, 1.5435e-06, 1.3285e-01,
        4.9657e-07, 5.2568e-07, 9.9186e-07, 3.2850e-02, 4.6652e-02, 7.1662e-07,
        5.4144e-07, 5.3227e-07, 1.4928e-06, 3.9613e-02, 8.1382e-07, 7.4328e-07,
        2.7739e-02, 6.3659e-07, 4.8376e-07, 4.6522e-02, 9.9871e-02, 5.1577e-02,
        4.7799e-07, 8.0981e-07, 7.0415e-07, 5.0680e-07, 5.3066e-02, 5.9538e-07,
        8.0207e-07, 3.8675e-02, 3.6147e-02, 4.4711e-07, 3.7000e-02, 8.8095e-07,
        4.6456e-02, 9.0844e-02, 4.3367e-02, 8.3737e-07, 6.5097e-07, 1.5348e-06,
        4.1261e-02, 6.0834e-07, 1.0959e-01, 2.9963e-07, 4.0686e-07, 2.7145e-02,
        8.3737e-07, 1.2688e-01, 6.8023e-07, 1.5007e-01, 5.2595e-02, 1.9059e-07,
        6.4883e-07, 3.0403e-07, 3.4154e-02, 4.6397e-02, 3.1765e-07, 4.7348e-07,
        4.6193e-02, 6.4883e-07, 5.5538e-08, 3.9979e-07, 1.3283e-01, 3.1224e-07,
        6.4417e-07, 3.5898e-07, 1.1707e-01, 3.3903e-02, 7.7634e-07, 6.2311e-07,
        1.1350e-02, 2.2963e-07, 1.1724e-01, 4.6685e-02, 3.3431e-07, 6.1347e-07,
        7.8316e-07, 5.2490e-07, 5.6239e-02, 1.1583e-06, 4.2861e-02, 9.3447e-02,
        4.6867e-07, 9.2404e-02, 7.1147e-02, 1.2627e-06, 4.7237e-02, 6.4883e-07,
        9.4703e-02, 5.2490e-07, 1.5850e-02, 4.6469e-02, 3.1180e-07, 3.0403e-07,
        3.9577e-07, 1.6555e-01, 1.3730e-01, 9.8748e-02, 3.8472e-02, 7.6328e-07,
        1.5082e-01, 4.8439e-07, 4.1685e-02, 6.2311e-07, 4.2908e-02, 6.0834e-07,
        3.0403e-07, 7.7634e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.6469e-06, 3.8075e-06, 2.9872e-06, 3.5284e-06, 6.2837e-07, 1.0255e-01,
        3.4852e-02, 3.6469e-06, 1.5135e-06, 5.2533e-07, 1.2768e-06, 1.0704e-06,
        2.1422e-06, 1.0142e-01, 4.5945e-02, 2.8176e-06, 8.6976e-02, 1.3509e-06,
        9.9987e-07, 9.5983e-02, 2.3403e-06, 1.8589e-06, 1.1688e-01, 1.5681e-03,
        6.0994e-07, 2.5869e-06, 1.4029e-06, 1.2552e-01, 6.3504e-04, 3.4124e-06,
        2.7208e-06, 2.2012e-06, 1.0478e-06, 3.6261e-06, 9.6007e-07, 1.7427e-07,
        1.8285e-06, 1.4751e-06, 6.5876e-07, 9.8185e-02, 1.4848e-01, 1.0737e-06,
        1.9770e-06, 8.3790e-02, 1.7958e-06, 9.1601e-02, 1.3775e-06, 2.3626e-03,
        3.3942e-06, 1.2403e-06, 1.6899e-06, 1.7288e-06, 2.3194e-06, 2.8536e-06,
        1.2593e-06, 1.0088e-01, 5.1823e-07, 1.0566e-06, 1.2530e-06, 4.1320e-02,
        5.2533e-07, 7.3188e-07, 1.1644e-01, 1.3391e-06, 1.6641e-06, 5.9403e-07,
        1.3509e-06, 8.5541e-07, 1.2772e-01, 2.5323e-06, 1.4600e-06, 9.6800e-02,
        4.4885e-02, 1.1341e-01, 6.7949e-07, 1.0403e-01, 1.1185e-01, 2.2820e-06,
        1.0737e-06, 8.0656e-07, 1.0250e-01, 1.0784e-06, 4.5030e-07, 2.9872e-06,
        1.1369e-01, 7.8872e-07, 9.7153e-07, 2.1422e-06, 2.1683e-06, 1.3587e-01,
        9.5464e-02, 2.7161e-03, 1.9421e-03, 6.7263e-07, 5.1061e-06, 8.2199e-07,
        1.7080e-01, 3.4618e-06, 8.7000e-02, 2.9872e-06, 1.4008e-06, 2.1683e-06,
        2.2943e-06, 1.5027e-06, 5.1132e-06, 6.7263e-07, 2.3403e-06, 6.4195e-07,
        1.3509e-06, 2.8536e-06, 1.0784e-06, 7.7194e-07, 1.3674e-06, 1.2565e-06,
        1.4699e-01, 1.4008e-06, 8.0656e-07, 3.9465e-06, 2.6047e-06, 9.8373e-02,
        2.3618e-06, 1.0566e-06, 2.5157e-06, 1.0354e-01, 1.0737e-06, 1.2010e-01,
        5.1381e-07, 1.4113e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.7359e-02, 1.4672e-02, 3.2456e-02, 4.3280e-02, 2.4735e-02, 1.3934e-02,
        6.1265e-08, 4.0572e-02, 4.7246e-02, 2.7574e-02, 6.3398e-02, 6.4832e-02,
        4.5789e-02, 1.8038e-02, 4.8766e-02, 5.7292e-02, 1.7727e-02, 7.6817e-03,
        8.5291e-03, 2.6449e-02, 3.2543e-02, 2.4975e-02, 2.6778e-02, 4.4589e-08,
        7.4523e-02, 1.0261e-02, 4.4411e-02, 1.7190e-02, 2.6713e-02, 1.1483e-07,
        2.7874e-07, 2.1915e-02, 3.3381e-02, 9.0460e-02, 9.8522e-03, 6.3433e-03,
        1.8041e-02, 1.6776e-07, 1.1057e-02, 5.7782e-02, 7.4009e-03, 4.1324e-08,
        5.2942e-02, 2.0345e-02, 1.8104e-07, 5.8166e-02, 3.6921e-02, 3.7399e-02,
        4.1769e-02, 6.0574e-08, 3.2175e-02, 1.9014e-02, 1.3298e-02, 1.8592e-07,
        2.8280e-02, 1.9009e-02, 1.3506e-01, 2.5383e-02, 5.3514e-02, 4.5395e-02,
        6.9734e-02, 3.4739e-02, 8.4146e-03, 1.2838e-02, 9.2742e-03, 1.7240e-02,
        3.5341e-02, 5.6866e-08, 1.5423e-02, 6.5992e-02, 3.1166e-02, 9.4130e-08,
        3.0093e-02, 3.3376e-02, 9.1825e-03, 4.3218e-02, 4.2225e-02, 1.5669e-07,
        1.4187e-07, 2.0080e-01, 2.0857e-02, 9.4754e-03, 2.9478e-02, 1.0558e-07,
        9.9572e-03, 2.4750e-02, 1.4404e-02, 2.2356e-01, 2.8421e-02, 5.6446e-02,
        3.2895e-02, 2.1713e-02, 4.9521e-02, 3.6031e-02, 2.8499e-02, 6.7252e-08,
        2.5949e-02, 7.0617e-02, 5.3755e-08, 1.3836e-02, 2.8694e-02, 6.7370e-08,
        1.2682e-02, 5.7021e-02, 9.7190e-08, 1.4596e-07, 1.6426e-02, 1.3395e-02,
        2.4030e-02, 3.3308e-02, 1.0323e-02, 6.9077e-02, 4.7401e-02, 3.3162e-02,
        1.1726e-07, 1.0678e-07, 8.3101e-03, 3.1566e-02, 1.6029e-07, 9.0321e-08,
        1.4156e-07, 9.9411e-02, 1.2928e-07, 2.6473e-02, 2.7195e-07, 3.0794e-02,
        5.3815e-02, 1.7982e-07, 4.3584e-02, 1.4524e-02, 4.2416e-03, 4.2875e-02,
        4.3374e-02, 3.1423e-02, 9.7959e-03, 5.5306e-02, 3.5864e-02, 5.5445e-02,
        3.1476e-02, 5.6403e-02, 4.8727e-08, 1.6629e-02, 4.7662e-08, 1.0553e-07,
        1.6650e-02, 6.1030e-02, 1.3816e-02, 1.7283e-02, 2.0595e-07, 2.4034e-02,
        2.7898e-02, 6.2757e-02, 3.8831e-02, 4.1978e-02, 1.4469e-01, 1.7695e-01,
        2.3666e-02, 1.5204e-07, 4.4176e-02, 7.0339e-02, 1.3343e-07, 2.1847e-02,
        4.7346e-02, 2.9351e-07, 2.1885e-02, 2.0319e-02, 9.4630e-08, 2.9946e-07,
        5.5488e-02, 2.0129e-02, 6.0921e-08, 5.6942e-02, 3.6857e-02, 4.8743e-02,
        1.1540e-02, 3.3632e-02, 4.7770e-02, 3.3009e-02, 7.4833e-03, 1.0557e-07,
        8.6427e-08, 1.6099e-01, 5.4189e-02, 2.5702e-02, 1.5643e-02, 6.7252e-08,
        1.2794e-07, 7.5778e-08, 4.3774e-02, 1.4613e-07, 3.6767e-02, 1.6416e-02,
        1.1607e-02, 2.5313e-02, 2.1217e-02, 1.4513e-02, 2.7332e-02, 1.9566e-02,
        6.6163e-08, 7.5968e-08, 1.2782e-02, 5.3404e-03, 4.3416e-02, 6.7252e-08,
        6.4495e-08, 5.7369e-08, 6.3099e-02, 1.9298e-02, 6.4463e-03, 1.4933e-02,
        1.3649e-02, 1.0558e-07, 1.0096e-07, 5.9843e-02, 2.2256e-02, 1.1254e-02,
        6.8385e-03, 2.1595e-02, 3.2022e-02, 1.2805e-07, 1.4706e-02, 1.0951e-07,
        3.7173e-02, 3.5215e-02, 2.1124e-07, 1.3953e-02, 6.4387e-03, 2.0739e-02,
        8.7043e-08, 4.2390e-02, 9.9997e-03, 5.7182e-08, 3.8700e-08, 2.5022e-07,
        1.7889e-07, 5.5329e-08, 9.1397e-02, 4.6708e-02, 5.2807e-02, 1.8099e-01,
        2.3212e-08, 3.6783e-02, 1.7408e-07, 3.7381e-02, 6.0350e-02, 1.3187e-01,
        6.0304e-02, 3.4600e-02, 8.9180e-03, 5.4464e-02, 2.2656e-02, 3.0742e-02,
        2.3491e-02, 2.0088e-02, 2.6294e-02, 1.5459e-02, 4.6694e-02, 2.3809e-07,
        4.7359e-03, 1.6924e-02, 1.7654e-01, 8.7976e-08, 2.4402e-02, 9.4707e-08,
        2.7103e-07, 2.0901e-02, 1.9674e-02, 9.8667e-02, 4.3052e-02, 1.0392e-01,
        2.5393e-02, 5.2755e-02, 1.0951e-07, 1.1898e-02, 1.4074e-02, 7.5648e-08,
        5.9286e-02, 9.4707e-08, 2.4203e-02, 8.6869e-08, 9.6624e-03, 1.1985e-02,
        1.5033e-02, 6.3958e-02, 1.7887e-02, 3.3364e-03, 2.2084e-02, 5.2691e-02,
        1.3230e-02, 2.4027e-02, 1.3363e-01, 6.6203e-08, 1.4586e-02, 1.3859e-07,
        4.6664e-08, 6.8984e-02, 5.1247e-02, 1.3496e-02, 4.0739e-02, 4.1505e-02,
        7.9062e-02, 2.0140e-01, 1.5178e-02, 1.8966e-02, 4.2198e-02, 4.2929e-02,
        4.1016e-02, 1.8844e-02, 6.6011e-02, 1.0802e-02, 2.5129e-07, 1.8098e-02,
        6.8264e-02, 1.7830e-01, 8.8474e-03, 1.6654e-02, 3.0984e-02, 4.9186e-02,
        1.0720e-02, 1.6971e-02, 1.2555e-07, 3.7549e-02, 1.5523e-02, 1.9350e-07,
        9.4719e-08, 4.5606e-02, 4.6255e-02, 5.2519e-08, 5.8335e-02, 1.4139e-02,
        1.0951e-07, 3.4385e-02, 3.0253e-02, 1.8529e-02, 6.5127e-02, 2.0589e-02,
        3.8745e-02, 6.3851e-02, 1.1295e-07, 3.3318e-02, 3.5066e-08, 2.6379e-08,
        2.1117e-07, 1.5270e-07, 2.0187e-02, 5.1334e-09, 4.8359e-02, 2.7068e-07,
        6.6549e-03, 1.9505e-02, 5.8703e-02, 1.6939e-07, 5.2038e-02, 2.7861e-02,
        1.3342e-07, 2.1433e-03, 4.8248e-02, 3.3134e-02, 4.2446e-08, 2.1313e-02,
        1.2977e-01, 9.4084e-08, 5.1265e-03, 1.5316e-02, 5.5106e-08, 1.2822e-07,
        1.8952e-02, 5.1457e-08, 2.5366e-02, 5.9825e-08, 5.5975e-02, 4.9850e-02,
        3.4115e-02, 7.5014e-02, 8.4212e-02, 7.8543e-08, 2.2009e-07, 1.4835e-07,
        5.8289e-08, 1.4884e-02, 1.0555e-07, 6.2187e-02, 5.3518e-02, 5.9733e-03,
        3.2233e-02, 1.9306e-02, 2.2843e-02, 1.1454e-07, 4.4998e-02, 3.0819e-02,
        3.0241e-02, 7.2614e-02, 4.5721e-02, 2.4794e-02, 1.2004e-02, 1.7861e-02,
        2.9476e-02, 9.6302e-03, 1.7306e-02, 7.6372e-03, 9.9965e-02, 6.5480e-03,
        7.6093e-03, 1.0951e-07, 1.6314e-01, 1.0366e-02, 7.7691e-03, 1.1342e-02,
        3.8210e-02, 5.6124e-02, 1.8436e-02, 2.2857e-02, 7.5464e-02, 1.9747e-02,
        2.8344e-02, 1.8250e-02, 1.4860e-02, 5.6253e-02, 3.6519e-02, 2.5866e-02,
        5.3456e-02, 4.1535e-02, 6.2418e-02, 6.7639e-08, 1.9676e-07, 8.8441e-03,
        2.1193e-02, 1.4768e-07, 1.4639e-02, 8.6023e-08, 8.5606e-03, 3.9723e-02,
        4.9952e-08, 7.6646e-08, 1.7910e-07, 1.7910e-07, 2.8536e-02, 1.7910e-07,
        4.7756e-02, 1.7927e-02, 4.0969e-02, 6.6642e-08, 2.2658e-01, 1.0552e-07,
        5.5700e-02, 2.8109e-02, 1.3194e-02, 1.9730e-02, 7.7182e-08, 7.5608e-08,
        2.7109e-02, 3.2563e-02, 1.0535e-02, 5.6507e-02, 3.2689e-02, 4.6958e-02,
        5.3194e-02, 3.1300e-02, 5.1075e-08, 2.0113e-02, 4.3350e-02, 2.9379e-07,
        1.7950e-01, 2.6523e-02, 1.7894e-07, 7.7277e-09, 5.0982e-02, 7.4481e-02,
        1.8024e-07, 2.0780e-02, 6.0815e-02, 8.5931e-03, 2.6502e-02, 2.2087e-02,
        1.7011e-02, 1.7601e-07, 2.9317e-03, 1.9146e-01, 1.5030e-02, 5.8063e-02,
        1.2101e-07, 2.4975e-02, 6.9767e-02, 9.0977e-08, 1.7910e-07, 1.1752e-02,
        1.9010e-07, 1.1363e-02, 7.3189e-02, 1.9141e-07, 6.6070e-02, 4.5852e-02,
        2.2018e-07, 4.7203e-02, 1.7628e-02, 5.0402e-02, 2.9174e-02, 5.7510e-02,
        1.4896e-07, 7.9641e-02, 6.3893e-02, 9.3662e-08, 2.4350e-01, 4.9713e-08,
        1.7949e-02, 5.3573e-02, 1.5215e-07, 8.2815e-08, 1.0516e-07, 1.3208e-07,
        4.6558e-02, 3.8093e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.7525e-02, 4.5530e-07, 7.5471e-02, 7.6398e-02, 2.4781e-07, 1.9786e-01,
        9.8736e-02, 1.3262e-07, 7.8084e-02, 1.1144e-01, 7.4107e-07, 3.3470e-07,
        1.6122e-07, 1.5168e-07, 1.6122e-07, 1.7670e-07, 1.5168e-07, 8.3184e-02,
        9.8602e-02, 1.0347e-01, 1.4837e-07, 7.4138e-07, 9.7156e-02, 1.0202e-01,
        7.0321e-08, 1.0526e-07, 3.1068e-07, 8.2856e-02, 9.4489e-02, 3.0127e-07,
        5.2612e-07, 9.9647e-02, 4.8773e-02, 9.3588e-02, 3.3646e-07, 1.0287e-01,
        1.0586e-01, 5.2612e-07, 1.1077e-01, 5.3489e-07, 8.7037e-02, 7.4107e-07,
        7.6752e-02, 9.4078e-07, 1.3262e-07, 3.0128e-07, 8.1228e-02, 9.0532e-02,
        4.1773e-07, 1.0526e-07, 1.1245e-01, 5.2612e-07, 4.5531e-07, 9.0561e-02,
        1.1423e-01, 1.5168e-07, 5.2612e-07, 9.6595e-02, 9.4344e-08, 1.1839e-01,
        1.3262e-07, 3.5580e-07, 9.7032e-02, 1.1238e-01, 7.9991e-02, 1.0526e-07,
        1.1157e-01, 7.2439e-07, 1.1831e-01, 1.6706e-07, 1.0267e-01, 2.8280e-07,
        1.0713e-07, 4.9761e-07, 9.9418e-02, 9.3344e-02, 9.9401e-02, 7.4107e-07,
        1.7670e-07, 8.3917e-02, 1.0310e-01, 1.0364e-01, 7.4108e-07, 1.1994e-01,
        6.0227e-08, 9.4344e-08, 5.3489e-07, 9.2523e-02, 8.4630e-02, 1.1825e-01,
        9.2700e-02, 9.6780e-02, 9.2034e-02, 1.5168e-07, 9.6300e-02, 1.1471e-01,
        1.2254e-01, 1.5168e-07, 1.0671e-01, 9.3030e-02, 1.5168e-07, 4.5531e-07,
        5.7515e-07, 3.0132e-07, 7.0611e-07, 1.3262e-07, 9.8782e-02, 1.1629e-01,
        8.1873e-02, 1.0078e-01, 9.6933e-02, 5.3489e-07, 9.8594e-02, 4.9761e-07,
        9.7577e-02, 9.4308e-08, 5.3489e-07, 9.2709e-02, 8.5768e-02, 9.0235e-02,
        1.6122e-07, 5.3489e-07, 1.0714e-07, 5.2612e-07, 6.6893e-02, 2.4781e-07,
        9.6949e-02, 8.8604e-02, 8.5022e-02, 1.0526e-07, 1.6122e-07, 2.4781e-07,
        7.0321e-08, 9.6676e-02, 9.4307e-08, 1.1559e-01, 1.0526e-07, 3.1068e-07,
        3.4094e-07, 1.0400e-01, 9.5983e-07, 1.7670e-07, 1.0760e-01, 5.7515e-07,
        2.7773e-07, 7.0610e-07, 9.5921e-02, 9.9865e-02, 1.6122e-07, 7.0321e-08,
        1.1331e-01, 9.9042e-02, 1.1360e-01, 1.3262e-07, 1.7670e-07, 9.2919e-02,
        1.0722e-01, 9.6228e-02, 1.0604e-01, 3.6100e-07, 1.2106e-01, 9.8582e-02,
        9.0807e-02, 1.7670e-07, 9.8246e-02, 5.6294e-02, 1.3262e-07, 1.4837e-07,
        8.9545e-02, 7.5316e-02, 9.9471e-02, 1.2517e-01, 5.3489e-07, 1.7670e-07,
        1.0837e-01, 9.0944e-02, 9.1714e-02, 4.9761e-07, 9.6718e-02, 5.3489e-07,
        9.5983e-07, 7.9011e-07, 1.0713e-07, 7.0321e-08, 1.3262e-07, 1.3155e-07,
        5.3489e-07, 1.0206e-01, 1.0512e-01, 5.1023e-07, 1.0345e-01, 7.0321e-08,
        9.1150e-02, 1.1324e-01, 1.9819e-07, 1.3262e-07, 1.0687e-01, 9.1305e-02,
        4.5531e-07, 3.9032e-07, 3.1068e-07, 9.5713e-02, 5.2612e-07, 1.3262e-07,
        1.0265e-01, 1.0380e-01, 1.0315e-01, 9.5317e-02, 5.3489e-07, 9.0027e-02,
        9.3056e-02, 1.0631e-01, 7.4942e-02, 4.5530e-07, 1.3586e-01, 1.8133e-07,
        4.5531e-07, 1.0373e-01, 1.3262e-07, 1.0020e-01, 4.5531e-07, 1.1865e-01,
        1.5168e-07, 9.8609e-02, 1.2065e-07, 5.3489e-07, 1.2065e-07, 9.6539e-02,
        1.0458e-01, 7.0983e-07, 1.2056e-01, 9.3987e-02, 6.8141e-02, 8.6522e-02,
        9.1018e-02, 2.2037e-07, 7.0321e-08, 1.3262e-07, 1.0075e-01, 9.6235e-02,
        7.4108e-07, 1.0517e-01, 1.0245e-01, 1.3262e-07, 1.5168e-07, 1.5139e-07,
        1.0887e-01, 4.5531e-07, 3.6100e-07, 9.6655e-02, 1.7670e-07, 9.1424e-02,
        1.3262e-07, 9.8366e-02, 9.0767e-02, 4.9761e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.5533e-07, 2.3308e-06, 9.1257e-02, 1.1755e-06, 1.6435e-06, 1.5668e-06,
        4.0758e-06, 7.3707e-02, 8.5539e-02, 7.3427e-02, 1.3291e-06, 1.9071e-06,
        2.2071e-06, 1.0294e-06, 2.7671e-06, 1.0294e-06, 2.6359e-06, 1.0079e-06,
        2.2071e-06, 4.1136e-06, 7.5376e-07, 2.2071e-06, 1.9071e-06, 1.2812e-06,
        2.2192e-06, 2.6359e-06, 9.8886e-08, 1.1755e-06, 7.5819e-02, 8.4720e-07,
        7.5688e-02, 2.2192e-06, 7.8819e-02, 2.6359e-06, 8.1993e-02, 7.7782e-02,
        1.9071e-06, 2.2192e-06, 1.3291e-06, 4.6460e-06, 8.1359e-02, 8.4838e-02,
        8.6261e-02, 2.3308e-06, 8.5508e-02, 7.2761e-02, 1.2812e-06, 1.0087e-06,
        7.2183e-02, 7.0277e-02, 2.2071e-06, 8.7197e-02, 3.2976e-06, 7.6120e-02,
        1.4145e-06, 7.6373e-02, 7.0223e-02, 3.2976e-06, 7.9642e-02, 4.1004e-06,
        9.7773e-02, 7.5376e-07, 7.4653e-02, 8.9654e-02, 2.2192e-06, 1.4145e-06,
        7.2768e-02, 2.7671e-06, 7.5376e-07, 6.5533e-07, 7.5376e-07, 1.9071e-06,
        4.0758e-06, 7.4408e-02, 2.2071e-06, 8.0366e-02, 6.3482e-02, 8.4073e-02,
        2.5261e-06, 1.3291e-06, 3.2976e-06, 7.2675e-02, 7.5376e-07, 1.9222e-06,
        7.5553e-02, 1.3291e-06, 1.3291e-06, 1.2812e-06, 2.2192e-06, 6.5533e-07,
        2.2192e-06, 2.2192e-06, 9.5629e-02, 1.9071e-06, 8.5333e-02, 1.9071e-06,
        1.3291e-06, 2.6646e-06, 1.1755e-06, 7.0587e-02, 2.0001e-06, 1.9071e-06,
        5.3090e-06, 8.5507e-07, 1.9071e-06, 4.2244e-07, 2.2192e-06, 2.2071e-06,
        2.1051e-06, 2.6646e-06, 7.5376e-07, 1.0294e-06, 8.1678e-02, 7.1704e-02,
        8.1473e-02, 9.1219e-02, 4.6460e-06, 7.5376e-07, 7.8293e-02, 1.0480e-06,
        1.0087e-06, 2.2071e-06, 7.4739e-02, 4.2196e-06, 2.6359e-06, 7.5376e-07,
        7.6196e-02, 2.6359e-06, 4.1040e-06, 6.5533e-07, 1.8208e-06, 8.5049e-02,
        7.0390e-02, 7.1323e-07, 4.1132e-06, 7.4872e-02, 2.2071e-06, 1.3291e-06,
        4.2196e-06, 2.2192e-06, 9.0802e-07, 8.7748e-02, 1.3291e-06, 7.5376e-07,
        4.2244e-07, 2.6152e-06, 1.9071e-06, 1.5668e-06, 1.5784e-06, 2.0523e-06,
        1.3291e-06, 1.1755e-06, 2.0001e-06, 1.1755e-06, 2.2071e-06, 1.3291e-06,
        8.4850e-02, 7.5376e-07, 8.4347e-07, 2.8581e-06, 2.7671e-06, 8.5506e-07,
        1.3291e-06, 1.0294e-06, 4.6460e-06, 2.0001e-06, 1.5784e-06, 7.0451e-02,
        1.4051e-06, 6.5275e-02, 2.6646e-06, 2.2192e-06, 8.7704e-07, 1.0087e-06,
        1.2571e-06, 1.3690e-06, 7.7392e-02, 1.7921e-06, 5.3134e-06, 2.6359e-06,
        8.0406e-02, 6.9927e-02, 7.5376e-07, 8.4347e-07, 1.0294e-06, 4.1032e-06,
        1.1196e-06, 3.6260e-02, 2.8581e-06, 7.5394e-02, 4.6460e-06, 7.5376e-07,
        2.2071e-06, 1.5668e-06, 4.2196e-06, 4.2196e-06, 1.9071e-06, 6.9178e-02,
        8.4347e-07, 2.2192e-06, 7.5376e-07, 2.7671e-06, 9.1096e-07, 4.2196e-06,
        1.7921e-06, 2.7671e-06, 8.4221e-02, 2.5261e-06, 8.2155e-02, 7.5929e-02,
        7.5376e-07, 4.1106e-06, 7.5376e-07, 2.7671e-06, 1.3291e-06, 2.7671e-06,
        5.2490e-02, 4.1018e-06, 7.4610e-02, 7.7259e-02, 6.5533e-07, 6.5533e-07,
        2.6359e-06, 1.0079e-06, 1.3291e-06, 4.2196e-06, 7.9045e-02, 3.2976e-06,
        1.9071e-06, 1.3690e-06, 7.1699e-02, 7.6450e-02, 7.5376e-07, 1.9071e-06,
        4.6460e-06, 1.2812e-06, 1.8038e-06, 6.5533e-07, 7.4645e-02, 1.2088e-06,
        1.1755e-06, 1.4051e-06, 1.9071e-06, 6.5533e-07, 1.8208e-06, 7.0620e-02,
        8.2838e-02, 6.2383e-02, 7.5376e-07, 5.3134e-06, 7.2688e-02, 5.3144e-06,
        7.5376e-07, 8.6510e-02, 1.0087e-06, 2.5261e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.2037e-08, 1.0813e-07, 2.0093e-02,  ..., 1.3359e-07, 4.4839e-07,
        2.9424e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.7198e-07, 7.8851e-07, 5.4214e-02,  ..., 1.1363e-06, 8.6235e-07,
        3.0603e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.9811e-06, 6.1727e-07, 9.3986e-07, 1.9812e-06, 4.7259e-02, 7.1326e-07,
        9.0466e-07, 1.3232e-06, 5.8891e-07, 3.5086e-02, 2.6805e-02, 6.1491e-07,
        8.6171e-07, 1.7103e-06, 3.4255e-07, 8.6120e-07, 4.1305e-02, 1.8843e-06,
        4.0559e-02, 4.6456e-02, 2.6129e-02, 1.8640e-06, 1.6126e-06, 1.4831e-06,
        4.8102e-07, 1.8959e-06, 1.7103e-06, 2.5449e-06, 4.0242e-07, 1.0262e-06,
        5.7763e-07, 3.6270e-02, 3.4776e-02, 5.3242e-07, 9.4707e-07, 6.1727e-07,
        6.1491e-07, 1.8219e-06, 5.3085e-07, 1.0989e-06, 6.6658e-07, 6.6658e-07,
        3.7165e-02, 3.9708e-02, 3.7980e-02, 1.2683e-06, 1.7210e-06, 2.6776e-06,
        1.6126e-06, 1.0989e-06, 6.3733e-07, 1.6126e-06, 2.4759e-07, 2.2491e-07,
        1.2683e-06, 1.3962e-06, 4.7246e-07, 4.0503e-02, 1.7836e-06, 2.6997e-06,
        2.8884e-02, 7.1980e-07, 1.7103e-06, 1.0313e-06, 2.6802e-06, 4.2746e-02,
        3.7505e-02, 9.1662e-07, 9.7386e-07, 4.0837e-02, 2.9649e-02, 2.8439e-07,
        3.0408e-07, 2.7115e-02, 3.8078e-02, 7.2070e-07, 3.7764e-02, 2.4274e-06,
        3.7018e-02, 3.9452e-02, 7.2070e-07, 3.3537e-02, 4.0451e-02, 8.6171e-07,
        8.6171e-07, 1.7322e-06, 3.6189e-02, 7.1980e-07, 1.2683e-06, 2.7905e-02,
        2.8815e-02, 8.4353e-07, 1.7322e-06, 1.4724e-01, 6.1491e-07, 3.4255e-07,
        1.4384e-06, 5.1364e-07, 1.3232e-06, 1.7322e-06, 3.2519e-02, 1.7319e-06,
        1.8843e-06, 4.0727e-02, 1.6126e-06, 1.5835e-06, 8.6929e-07, 1.5027e-06,
        3.6718e-02, 6.4607e-07, 1.7322e-06, 9.1822e-07, 2.4759e-07, 3.6349e-02,
        8.9058e-07, 2.4759e-07, 3.6719e-02, 4.4834e-07, 4.4740e-02, 3.5705e-02,
        8.9058e-07, 3.4984e-02, 6.1491e-07, 5.3242e-07, 4.3384e-02, 3.6158e-02,
        8.1882e-07, 1.7322e-06, 1.8843e-06, 4.7622e-07, 1.7210e-06, 5.8628e-07,
        8.6171e-07, 2.4759e-07, 2.4680e-07, 1.0072e-06, 4.8096e-07, 1.6126e-06,
        2.6340e-06, 1.9812e-06, 2.9785e-02, 1.4276e-06, 5.3242e-07, 1.3962e-06,
        5.1550e-02, 3.6512e-02, 6.1491e-07, 9.7386e-07, 4.1818e-02, 1.8219e-06,
        3.8984e-02, 1.3091e-06, 1.9812e-06, 2.5088e-06, 5.1357e-07, 8.3066e-07,
        8.0546e-07, 3.9463e-02, 1.8959e-06, 4.3339e-02, 5.3937e-07, 3.9516e-02,
        6.6144e-07, 2.4759e-07, 6.3371e-07, 3.4255e-07, 2.2130e-07, 5.5513e-07,
        9.5948e-07, 2.8439e-07, 6.8969e-07, 1.7210e-06, 1.8219e-06, 9.5779e-07,
        2.5174e-02, 1.1837e-06, 1.0253e-06, 4.0422e-02, 1.7322e-06, 1.7710e-06,
        1.8241e-06, 7.1980e-07, 1.3962e-06, 1.8839e-06, 1.5027e-06, 1.1096e-06,
        2.3729e-03, 1.5027e-06, 6.8969e-07, 8.2235e-07, 6.1727e-07, 5.1928e-07,
        1.3232e-06, 2.7140e-06, 2.4274e-06, 1.7210e-06, 7.1980e-07, 2.4274e-06,
        7.1980e-07, 1.7210e-06, 1.9646e-06, 8.9058e-07, 4.6537e-02, 6.0392e-07,
        7.2070e-07, 4.0016e-02, 1.5858e-06, 9.9652e-07, 4.0710e-02, 1.7210e-06,
        6.0392e-07, 2.5578e-06, 1.7210e-06, 1.7103e-06, 6.1491e-07, 1.0082e-06,
        3.6956e-02, 4.1414e-02, 9.0845e-07, 2.9148e-02, 3.2470e-02, 1.8219e-06,
        1.2275e-06, 2.8439e-07, 3.4502e-02, 1.3199e-06, 1.9812e-06, 9.7386e-07,
        5.7572e-07, 2.6874e-06, 1.7210e-06, 1.7210e-06, 1.5292e-06, 6.0392e-07,
        5.7572e-07, 1.4384e-06, 2.5088e-06, 1.3703e-06, 2.4759e-07, 1.7836e-06,
        5.3592e-07, 1.0577e-06, 1.8843e-06, 7.1980e-07, 9.4707e-07, 4.9335e-07,
        1.2252e-06, 1.6126e-06, 2.6776e-06, 1.2275e-06, 1.8219e-06, 3.8583e-07,
        1.9812e-06, 2.3692e-02, 1.6126e-06, 1.8219e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.6460e-06, 3.0324e-06, 8.5667e-07, 2.0036e-06, 8.6607e-07, 1.5516e-06,
        2.5904e-06, 5.5174e-02, 1.5526e-06, 6.9412e-02, 1.9776e-06, 1.0697e-06,
        1.9939e-06, 5.0630e-02, 3.7644e-02, 1.0822e-06, 6.0122e-02, 5.6622e-02,
        5.4634e-02, 2.2575e-06, 1.4536e-06, 7.0625e-07, 6.6692e-02, 2.5718e-06,
        1.0110e-06, 2.5904e-06, 1.7153e-06, 1.1781e-06, 6.4778e-02, 1.0384e-06,
        1.3276e-06, 1.0110e-06, 1.2121e-06, 5.7248e-02, 2.9613e-06, 1.0278e-06,
        1.0697e-06, 1.1781e-06, 3.1592e-06, 9.2664e-07, 3.0324e-06, 8.5667e-07,
        2.1653e-06, 5.3029e-02, 4.7300e-06, 1.1526e-06, 3.2316e-06, 1.7847e-06,
        4.6822e-02, 1.0289e-06, 4.9772e-02, 3.1592e-06, 1.0697e-06, 1.6092e-06,
        5.2796e-02, 1.0355e-06, 6.7838e-07, 7.3729e-07, 7.3729e-07, 5.0189e-07,
        6.7838e-07, 4.3360e-02, 2.1653e-06, 3.2316e-06, 2.9073e-06, 2.3514e-06,
        4.7300e-06, 4.5807e-06, 1.1781e-06, 2.5718e-06, 1.6998e-06, 7.3729e-07,
        1.1781e-06, 1.0289e-06, 7.3729e-07, 1.0822e-06, 8.5667e-07, 7.3729e-07,
        2.5718e-06, 4.5914e-06, 1.3592e-06, 1.6986e-06, 5.9330e-02, 7.2851e-07,
        2.0036e-06, 3.2316e-06, 4.6763e-06, 1.4654e-06, 7.3729e-07, 6.7400e-02,
        1.0251e-06, 3.6019e-07, 7.2852e-07, 3.0324e-06, 5.8851e-10, 3.2316e-06,
        2.0036e-06, 3.8624e-02, 3.8205e-02, 1.4536e-06, 2.2575e-06, 8.5667e-07,
        4.6773e-06, 2.9613e-06, 2.0036e-06, 2.5885e-06, 6.7838e-07, 2.0036e-06,
        8.0080e-07, 1.0697e-06, 2.2689e-06, 6.2794e-02, 2.4931e-06, 5.4946e-02,
        1.5516e-06, 9.3080e-07, 3.2316e-06, 6.1078e-02, 9.3080e-07, 2.5904e-06,
        1.4654e-06, 2.3514e-06, 2.9073e-06, 4.7300e-06, 4.2829e-06, 6.7838e-07,
        7.2852e-07, 1.5067e-06, 2.2689e-06, 1.3334e-06, 1.9776e-06, 2.6484e-07,
        2.9070e-06, 7.2852e-07, 1.6986e-06, 1.3334e-06, 6.0653e-02, 1.4654e-06,
        1.6986e-06, 6.3955e-02, 1.3334e-06, 4.8084e-02, 1.3276e-06, 1.6092e-06,
        7.3729e-07, 1.0822e-06, 1.6092e-06, 1.0110e-06, 4.6779e-06, 1.0087e-06,
        4.7300e-06, 5.2876e-02, 1.3161e-06, 5.7151e-02, 1.0697e-06, 1.7127e-06,
        2.5718e-06, 1.5516e-06, 2.5718e-06, 3.0065e-06, 6.2050e-02, 1.2862e-06,
        6.0208e-06, 7.2851e-07, 2.0036e-06, 6.3703e-02, 5.1946e-07, 3.0065e-06,
        1.0697e-06, 1.1781e-06, 1.1781e-06, 1.1526e-06, 5.7344e-02, 7.3517e-02,
        3.2316e-06, 3.2316e-06, 3.1592e-06, 1.2862e-06, 4.3079e-02, 5.0189e-07,
        2.2689e-06, 7.3729e-07, 9.3080e-07, 5.1317e-02, 1.0110e-06, 1.0251e-06,
        1.4654e-06, 4.7300e-06, 2.9073e-06, 5.7363e-02, 2.3514e-06, 1.4654e-06,
        5.6906e-02, 1.3161e-06, 4.8394e-06, 3.3030e-06, 2.0036e-06, 2.0036e-06,
        6.0386e-02, 1.3276e-06, 1.5526e-06, 1.0110e-06, 1.6092e-06, 2.3931e-06,
        1.0355e-06, 2.2689e-06, 1.4654e-06, 8.1621e-07, 1.6092e-06, 3.1592e-06,
        3.2316e-06, 5.7760e-02, 5.5040e-02, 2.0036e-06, 2.3514e-06, 6.2414e-02,
        4.6771e-06, 3.1592e-06, 5.9292e-02, 4.6783e-06, 1.1781e-06, 3.3531e-07,
        5.6844e-02, 6.5482e-07, 1.0289e-06, 2.1653e-06, 8.5667e-07, 1.8772e-06,
        3.2316e-06, 6.7262e-07, 6.0760e-06, 6.3311e-02, 7.2852e-07, 1.1781e-06,
        5.6901e-02, 2.2079e-06, 1.2121e-06, 1.2862e-06, 2.1653e-06, 2.3514e-06,
        3.5220e-08, 2.2689e-06, 6.2502e-02, 1.3334e-06, 1.6986e-06, 3.3530e-07,
        1.2071e-06, 2.2689e-06, 1.0697e-06, 6.0751e-02, 5.7101e-02, 2.3514e-06,
        1.0697e-06, 5.3775e-02, 2.2575e-06, 2.5718e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.5641e-08, 1.8800e-07, 4.7397e-02,  ..., 6.8214e-08, 1.3022e-07,
        1.1372e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.4805e-02, 6.3897e-02, 1.7006e-06, 2.4750e-06, 1.9015e-06, 8.4300e-02,
        6.0056e-02, 3.1731e-07, 5.7444e-02, 7.5596e-07, 6.0706e-07, 6.7432e-07,
        6.9885e-07, 2.4516e-06, 4.4984e-07, 2.4591e-06, 5.0578e-02, 1.1882e-06,
        6.7432e-07, 5.7502e-02, 6.2427e-07, 1.2290e-01, 3.1452e-07, 5.8892e-07,
        4.9742e-07, 3.6820e-07, 4.6358e-02, 7.7366e-07, 8.8845e-07, 6.2231e-07,
        1.3890e-06, 4.3670e-02, 4.7322e-02, 5.3042e-07, 1.1832e-06, 5.2669e-02,
        1.1940e-06, 5.6608e-02, 7.4130e-07, 5.1280e-02, 4.9156e-02, 1.1632e-06,
        2.4025e-06, 1.6043e-06, 1.0321e-06, 4.9996e-02, 1.1961e-01, 5.8916e-02,
        1.6043e-06, 1.5079e-06, 2.0630e-07, 5.4839e-07, 4.6856e-02, 4.4984e-07,
        5.5177e-07, 5.1502e-07, 1.1882e-06, 4.2908e-02, 1.0321e-06, 6.7432e-07,
        1.0711e-01, 7.7384e-07, 1.1616e-06, 2.6058e-06, 1.0607e-06, 1.3065e-06,
        5.8892e-07, 3.2992e-07, 4.4984e-07, 6.0387e-07, 5.6653e-02, 4.4984e-07,
        5.5806e-07, 1.1389e-06, 1.2368e-06, 1.8231e-06, 7.8938e-07, 1.2609e-06,
        4.6892e-02, 5.0378e-07, 5.4839e-07, 4.9547e-02, 8.4578e-07, 1.2567e-06,
        9.4192e-07, 5.0948e-02, 1.3006e-01, 5.2350e-02, 1.2368e-06, 1.1767e-06,
        5.7105e-02, 4.4613e-02, 1.1458e-06, 5.6420e-02, 3.2097e-06, 1.6059e-06,
        1.6059e-06, 4.9204e-02, 6.7432e-07, 5.4756e-02, 6.1483e-02, 2.9805e-07,
        1.6059e-06, 9.6598e-02, 1.6126e-06, 1.3876e-06, 8.9657e-07, 6.7432e-07,
        2.7821e-07, 2.4516e-06, 5.3607e-02, 6.1889e-02, 5.8892e-07, 5.1160e-07,
        6.1652e-02, 7.5596e-07, 5.2662e-02, 6.7432e-07, 2.2647e-06, 1.6862e-01,
        1.6126e-06, 6.7255e-07, 7.5805e-07, 5.6711e-02, 7.1725e-07, 1.2292e-01,
        1.1767e-06, 2.0967e-06, 2.4543e-06, 1.1458e-06, 7.0873e-07, 1.2368e-06,
        2.4516e-06, 5.2579e-02, 8.0620e-07, 8.7685e-07, 5.5806e-07, 1.7066e-07,
        8.9177e-07, 5.2294e-02, 6.7432e-07, 7.8938e-07, 4.8563e-07, 5.2026e-02,
        1.1610e-01, 4.9742e-07, 1.2368e-06, 4.4984e-07, 9.6583e-07, 2.7383e-02,
        1.2368e-06, 6.0387e-07, 7.8938e-07, 2.0967e-06, 5.0456e-02, 9.7549e-07,
        8.9619e-07, 4.7749e-07, 1.9102e-06, 4.8462e-02, 2.4209e-06, 1.6132e-06,
        5.0859e-02, 5.2848e-02, 5.0695e-07, 2.0967e-06, 4.1237e-07, 5.3884e-07,
        5.6791e-02, 7.7383e-07, 3.9775e-02, 8.1558e-07, 1.1940e-06, 3.8674e-02,
        5.2282e-02, 8.2068e-02, 8.9177e-07, 4.9161e-02, 4.5708e-02, 1.3065e-06,
        1.2866e-06, 8.4134e-07, 5.3884e-07, 1.6126e-06, 5.9146e-02, 4.1153e-07,
        5.0446e-02, 4.1348e-02, 2.4591e-06, 5.2086e-02, 5.6179e-02, 4.7355e-02,
        8.5662e-07, 2.3635e-06, 5.6102e-02, 4.4885e-02, 4.0056e-02, 4.1360e-07,
        2.4630e-06, 1.1616e-06, 3.1731e-07, 9.8856e-07, 5.3884e-07, 9.7549e-07,
        7.8938e-07, 5.3087e-02, 1.2368e-06, 4.7929e-02, 5.8892e-07, 4.2579e-02,
        5.1160e-07, 3.8882e-02, 6.6960e-07, 1.2368e-06, 5.5067e-07, 6.9917e-07,
        4.4976e-02, 4.8187e-02, 1.2321e-06, 2.4209e-06, 5.6007e-02, 4.8236e-02,
        1.1458e-06, 1.6126e-06, 5.5459e-02, 7.5805e-07, 4.6894e-02, 5.1502e-07,
        6.7432e-07, 8.7685e-07, 1.1499e-06, 5.4011e-02, 1.7006e-06, 8.7218e-07,
        5.6832e-02, 4.4984e-07, 6.7432e-07, 1.6043e-06, 8.0362e-07, 8.4578e-07,
        1.2368e-06, 5.0234e-02, 2.0967e-06, 2.7821e-07, 5.1177e-02, 1.1940e-06,
        7.4537e-07, 1.0882e-01, 8.5662e-07, 1.9102e-06, 1.2981e-06, 5.0946e-02,
        4.7749e-07, 4.3929e-07, 5.4839e-07, 5.9154e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.3793e-06, 4.2216e-06, 3.5381e-06, 8.5549e-06, 1.2470e-05, 2.2991e-02,
        1.8053e-06, 2.1359e-06, 3.2866e-06, 6.4676e-06, 2.1925e-02, 3.1295e-06,
        2.9899e-06, 5.9001e-06, 7.4541e-02, 9.6410e-02, 7.5263e-02, 2.5077e-02,
        2.7510e-02, 1.5460e-06, 2.4308e-02, 2.4194e-06, 7.2899e-02, 1.1679e-06,
        7.3884e-06, 2.6007e-06, 1.7255e-06, 4.0545e-06, 1.4636e-06, 8.9372e-06,
        5.9968e-06, 2.0495e-02, 4.2238e-02, 1.7251e-07, 2.2066e-02, 8.7396e-06,
        3.0514e-02, 3.2490e-06, 4.1763e-06, 5.3914e-06, 5.8693e-06, 4.1864e-02,
        2.5087e-02, 6.4760e-06, 2.3819e-02, 1.9363e-06, 5.2652e-06, 2.3308e-06,
        4.7374e-06, 1.0642e-05, 4.7556e-06, 2.7493e-06, 1.5805e-02, 1.5905e-06,
        3.0079e-06, 1.7251e-07, 4.1893e-06, 2.5820e-06, 9.4426e-07, 2.6767e-06,
        4.5478e-06, 2.2569e-06, 3.6799e-07, 2.6605e-06, 2.0585e-06, 2.7319e-02,
        2.3501e-02, 1.9191e-06, 1.7423e-06, 4.7462e-06, 2.3055e-06, 3.0597e-06,
        1.3079e-06, 3.2490e-06, 1.2425e-06, 2.6394e-02, 1.4365e-06, 6.2168e-06,
        1.3416e-06, 3.0068e-06, 7.6487e-02, 4.3318e-06, 6.2168e-06, 5.8308e-06,
        2.3825e-06, 2.7765e-06, 6.8403e-02, 6.2197e-06, 8.3180e-06, 2.2651e-07,
        2.4213e-02, 9.2713e-06, 6.2168e-06, 4.9511e-06, 5.7030e-06, 2.4291e-06,
        6.9475e-07, 6.3256e-06, 2.0585e-06, 7.3884e-06, 5.5676e-06, 7.3373e-02,
        5.7897e-06, 6.2709e-06, 2.2742e-06, 1.7298e-06, 5.3867e-06, 3.3713e-07,
        3.6191e-06, 6.0589e-06, 4.7705e-02, 1.7250e-07, 2.3953e-02, 3.6127e-06,
        1.1274e-05, 3.4690e-06, 6.2197e-06, 1.8614e-06, 6.4274e-06, 2.0864e-06,
        1.4636e-06, 1.9590e-02, 2.3793e-06, 1.2898e-06, 4.0031e-06, 3.8140e-06,
        6.5208e-06, 3.2103e-06, 3.3290e-06, 2.2598e-02, 3.2490e-06, 5.1954e-06,
        3.0068e-06, 2.9552e-06, 3.0833e-06, 2.5786e-06, 7.7314e-02, 1.6173e-06,
        1.5442e-06, 2.4311e-06, 3.8337e-06, 3.7455e-06, 2.5672e-02, 1.7584e-06,
        1.4365e-06, 3.2340e-02, 3.5205e-06, 3.0447e-06, 7.2653e-02, 1.5905e-06,
        9.1895e-06, 3.5744e-06, 8.7908e-06, 8.1973e-02, 2.2569e-06, 1.5905e-06,
        1.7298e-06, 3.9976e-06, 1.1121e-06, 1.6271e-06, 1.3416e-06, 3.4978e-06,
        7.3947e-02, 6.3885e-06, 2.7476e-02, 1.3480e-06, 2.0585e-06, 7.6810e-06,
        6.9934e-06, 1.4551e-06, 8.9794e-02, 5.3684e-06, 5.3008e-06, 5.5676e-06,
        3.5966e-06, 2.2569e-06, 4.9942e-06, 7.5774e-02, 5.3150e-06, 5.3090e-06,
        3.0447e-06, 4.3499e-06, 7.2265e-02, 2.3558e-02, 6.1785e-06, 5.7401e-06,
        3.4978e-06, 7.2284e-02, 1.8614e-06, 3.4978e-06, 9.2366e-02, 2.3781e-06,
        2.4221e-02, 1.2995e-06, 2.6767e-06, 1.7584e-06, 3.7018e-06, 1.5762e-06,
        3.3990e-06, 3.0620e-06, 2.6007e-06, 1.7322e-06, 1.1796e-05, 2.3350e-02,
        1.8517e-02, 3.8337e-06, 1.8675e-02, 3.3026e-02, 1.1849e-01, 5.4070e-06,
        3.8344e-06, 7.5037e-06, 3.3415e-06, 2.4593e-06, 7.8954e-06, 5.8271e-06,
        6.9475e-07, 1.8053e-06, 6.1785e-06, 4.0743e-06, 3.4978e-06, 1.5442e-06,
        6.9934e-06, 6.0759e-06, 3.6506e-06, 2.9142e-06, 2.1358e-06, 3.6798e-07,
        5.3090e-06, 3.0204e-02, 3.0148e-06, 9.4048e-06, 1.7140e-06, 6.9934e-06,
        3.3555e-06, 1.3924e-06, 1.8920e-02, 3.2941e-06, 2.2478e-02, 8.5760e-02,
        2.7221e-02, 8.4158e-02, 2.7765e-06, 6.8394e-06, 1.1121e-06, 1.3085e-06,
        9.3283e-06, 8.3655e-06, 4.0938e-06, 5.8084e-06, 3.6506e-06, 2.3793e-06,
        1.5155e-06, 2.3433e-02, 1.5762e-06, 4.0782e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.3829e-07, 7.0519e-08, 4.6547e-02,  ..., 6.2736e-07, 3.2817e-07,
        4.4201e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.9344e-06, 1.8211e-06, 2.7589e-06, 6.6581e-07, 3.7673e-06, 1.3509e-06,
        2.4013e-06, 1.4527e-06, 9.7346e-07, 7.9937e-07, 9.9429e-07, 5.6695e-07,
        1.0187e-06, 2.5854e-06, 2.2486e-06, 1.0187e-06, 2.8070e-06, 1.8010e-06,
        7.8992e-07, 6.2965e-07, 3.0172e-06, 8.9371e-07, 2.9344e-06, 3.4632e-07,
        2.9344e-06, 7.6504e-07, 3.7673e-06, 1.1535e-06, 1.0554e-06, 4.1291e-07,
        2.6801e-06, 9.9429e-07, 1.2352e-06, 6.6145e-07, 8.3496e-07, 1.8068e-06,
        7.8296e-07, 3.3199e-06, 1.2890e-06, 1.6174e-06, 1.8300e-06, 1.5199e-06,
        2.4985e-06, 9.4319e-07, 7.4635e-07, 4.2532e-06, 8.1338e-07, 3.5288e-06,
        8.8356e-07, 9.4017e-08, 3.0088e-07, 8.9937e-07, 3.0847e-06, 1.5852e-06,
        1.8864e-06, 2.0705e-06, 1.0376e-06, 1.8585e-06, 3.7893e-06, 8.8496e-07,
        1.8068e-06, 1.8864e-06, 1.8010e-06, 9.9948e-07, 1.1714e-06, 4.2465e-07,
        3.1474e-06, 2.3708e-06, 4.7897e-07, 1.1125e-06, 1.2867e-06, 7.7599e-07,
        9.4017e-08, 2.3825e-06, 1.3014e-06, 2.0705e-06, 1.2742e-06, 8.0319e-07,
        8.1644e-07, 2.3211e-06, 1.0146e-06, 2.6958e-06, 2.2519e-06, 2.6419e-06,
        4.8596e-06, 2.2895e-06, 1.6878e-06, 1.2576e-06, 4.5389e-06, 1.1201e-06,
        1.1714e-06, 2.3297e-06, 1.0146e-06, 5.8826e-07, 2.2288e-06, 1.9445e-06,
        1.0274e-06, 1.0986e-06, 2.8851e-06, 1.6217e-06, 1.1125e-06, 8.1338e-07,
        1.4527e-06, 1.0376e-06, 2.7037e-06, 8.8496e-07, 4.8295e-07, 1.6535e-06,
        3.4376e-06, 2.2694e-06, 1.0376e-06, 3.0134e-06, 1.9139e-06, 9.7346e-07,
        1.6174e-06, 2.0676e-06, 7.4404e-07, 5.8665e-07, 2.0871e-06, 8.8496e-07,
        2.6958e-06, 1.7193e-06, 4.8596e-06, 8.6513e-07, 9.2109e-07, 2.2694e-06,
        7.4635e-07, 6.0287e-07, 7.5076e-07, 2.0166e-06, 1.0249e-06, 1.1125e-06,
        3.0200e-06, 1.5727e-06, 5.4594e-07, 3.0134e-06, 1.0986e-06, 1.1714e-06,
        5.1626e-07, 2.0675e-06, 2.2694e-06, 1.0963e-06, 3.4632e-07, 1.4552e-06,
        1.6535e-06, 4.6419e-06, 2.0141e-06, 3.2846e-06, 3.4632e-07, 1.4009e-06,
        2.2389e-06, 2.7037e-06, 2.9344e-06, 4.8295e-07, 2.4013e-06, 8.0319e-07,
        1.4167e-06, 9.7346e-07, 3.0088e-07, 1.9485e-06, 5.5038e-07, 1.5578e-06,
        4.2349e-07, 1.0376e-06, 2.2857e-06, 6.6581e-07, 2.5955e-06, 2.8766e-06,
        1.4739e-06, 1.9187e-06, 3.3218e-06, 4.4883e-07, 3.3157e-06, 9.4992e-07,
        1.8607e-06, 2.4027e-06, 2.1745e-06, 1.1667e-06, 1.6462e-06, 3.5871e-06,
        2.4194e-06, 8.0250e-07, 1.0392e-06, 7.5958e-07, 1.3853e-06, 1.6012e-06,
        9.7346e-07, 3.0134e-06, 1.1204e-06, 2.3748e-06, 1.0376e-06, 1.8760e-06,
        9.9429e-07, 3.7559e-06, 3.7673e-06, 7.9875e-07, 3.0000e-06, 1.7429e-06,
        7.5558e-07, 2.2547e-06, 3.1474e-06, 2.2694e-06, 2.2694e-06, 1.2890e-06,
        1.4266e-06, 2.8449e-07, 2.0959e-06, 2.2930e-06, 3.7673e-06, 2.4985e-06,
        8.8496e-07, 2.2694e-06, 2.9735e-06, 4.7897e-07, 7.9022e-07, 3.6082e-06,
        2.2694e-06, 5.1626e-07, 2.4069e-07, 1.1525e-06, 1.0587e-06, 6.6581e-07,
        1.0376e-06, 1.0587e-06, 3.4153e-07, 1.3096e-06, 1.3014e-06, 9.8409e-07,
        2.6042e-06, 5.8665e-07, 4.9528e-06, 1.0717e-06, 7.9875e-07, 8.8776e-07,
        1.3042e-06, 1.3014e-06, 2.8006e-06, 2.4143e-06, 2.6816e-06, 1.7398e-06,
        1.8994e-06, 1.4613e-06, 5.1626e-07, 2.0558e-06, 9.1400e-07, 5.3069e-07,
        7.0450e-07, 2.9864e-06, 1.6112e-06, 1.7953e-06, 1.2948e-06, 1.1547e-06,
        3.4632e-07, 9.3352e-07, 7.5076e-07, 2.7037e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.9501e-06, 4.0457e-06, 6.0774e-06, 5.0366e-06, 2.7965e-06, 6.1822e-06,
        3.2026e-06, 5.2355e-06, 6.5665e-06, 3.3018e-06, 4.8449e-06, 3.5553e-06,
        4.0255e-06, 6.7909e-06, 7.3650e-06, 4.9217e-06, 7.2143e-06, 1.5138e-05,
        1.3767e-05, 5.2355e-06, 4.4017e-06, 2.3987e-06, 3.9980e-06, 4.3502e-06,
        1.0268e-05, 5.2354e-06, 2.3411e-06, 3.4339e-06, 3.2122e-06, 4.1410e-06,
        3.5334e-06, 6.5022e-06, 1.5932e-06, 7.7865e-06, 7.4400e-06, 6.5383e-06,
        2.9596e-06, 2.9437e-06, 6.3871e-06, 2.9437e-06, 5.0705e-06, 1.7642e-06,
        6.7909e-06, 4.3394e-06, 3.2150e-06, 2.9437e-06, 3.5584e-06, 4.3218e-06,
        4.8449e-06, 4.5987e-06, 4.2015e-06, 6.6788e-06, 4.2891e-06, 2.2344e-06,
        5.8352e-06, 7.0943e-06, 4.5671e-06, 3.1956e-06, 4.4750e-06, 3.7173e-06,
        3.6432e-06, 7.1609e-06, 5.0442e-06, 5.9509e-06, 3.8062e-06, 3.5893e-06,
        3.4760e-06, 2.6266e-06, 7.2143e-06, 1.0057e-05, 2.4604e-06, 4.3838e-06,
        4.1283e-06, 5.9447e-06, 6.1942e-06, 5.3161e-06, 4.2001e-06, 3.8663e-06,
        2.2344e-06, 1.8172e-06, 1.0950e-06, 7.4400e-06, 7.1733e-06, 4.7436e-06,
        5.3055e-06, 1.0299e-05, 3.2408e-06, 3.1941e-06, 3.8812e-06, 3.4055e-06,
        9.3020e-06, 4.8449e-06, 3.7361e-06, 5.9752e-06, 4.2107e-06, 7.4338e-06,
        4.8449e-06, 4.4017e-06, 3.0962e-07, 8.7674e-06, 1.0197e-05, 2.6260e-06,
        8.1686e-06, 3.4711e-06, 5.0879e-06, 4.0790e-06, 1.6996e-06, 6.5610e-06,
        5.0442e-06, 7.3849e-06, 1.1810e-05, 2.9996e-06, 7.5276e-06, 5.8929e-06,
        8.7674e-06, 5.2355e-06, 8.1085e-06, 7.7401e-06, 6.6788e-06, 4.9638e-06,
        8.7722e-06, 4.1360e-06, 4.4750e-06, 6.1822e-06, 5.2545e-06, 6.6788e-06,
        3.1713e-06, 5.3428e-06, 6.1822e-06, 2.7871e-06, 4.8993e-06, 4.4743e-06,
        7.3555e-06, 4.3573e-06, 6.4518e-06, 5.9019e-06, 7.2143e-06, 3.5677e-06,
        7.0943e-06, 2.2690e-06, 4.8449e-06, 3.7614e-06, 4.5671e-06, 4.0941e-06,
        1.1345e-06, 4.0390e-06, 5.9818e-06, 3.6432e-06, 3.9025e-07, 2.3788e-06,
        6.3976e-06, 6.3797e-06, 7.1092e-06, 6.4425e-06, 3.7851e-06, 5.3161e-06,
        1.8152e-06, 4.5395e-06, 5.2355e-06, 5.2355e-06, 3.5626e-06, 5.6410e-06,
        7.3555e-06, 4.1402e-06, 3.3562e-06, 5.9100e-06, 8.6389e-06, 4.8449e-06,
        1.0212e-05, 3.8333e-06, 2.6266e-06, 5.3085e-06, 5.8929e-06, 2.9482e-06,
        3.0391e-06, 4.1627e-06, 1.2146e-06, 5.1837e-06, 6.6788e-06, 7.2143e-06,
        4.0700e-06, 5.1967e-06, 4.8449e-06, 3.8812e-06, 3.2142e-06, 5.9538e-06,
        3.4339e-06, 3.1713e-06, 5.9019e-06, 2.1507e-06, 1.1345e-06, 5.2550e-06,
        7.2143e-06, 8.5819e-06, 3.1782e-06, 4.1402e-06, 1.0846e-05, 2.7187e-06,
        2.1373e-06, 7.2143e-06, 3.2990e-06, 3.5090e-06, 5.7842e-06, 4.1918e-06,
        2.7187e-06, 5.6150e-06, 1.7174e-06, 3.2026e-06, 1.7174e-06, 3.7881e-06,
        7.4338e-06, 5.7057e-06, 2.4382e-06, 7.2143e-06, 6.2692e-06, 6.5022e-06,
        5.4091e-06, 3.7361e-06, 4.9104e-06, 6.6788e-06, 4.8449e-06, 5.2355e-06,
        4.4017e-06, 3.7074e-06, 5.8929e-06, 5.7057e-06, 5.0879e-06, 7.2143e-06,
        3.4753e-06, 7.7724e-06, 7.7865e-06, 6.1822e-06, 6.4807e-06, 4.2350e-06,
        1.4681e-06, 5.2355e-06, 6.6788e-06, 3.7523e-06, 7.9528e-06, 2.1071e-06,
        9.9820e-06, 4.3502e-06, 4.2015e-06, 7.0943e-06, 3.0391e-06, 8.2589e-07,
        1.8648e-06, 5.2551e-06, 4.0385e-06, 8.9918e-06, 4.8449e-06, 7.8199e-06,
        8.2948e-06, 6.0910e-07, 4.0065e-06, 8.7722e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3146e-07, 4.4667e-07, 1.6171e-07,  ..., 5.6952e-07, 2.6248e-07,
        1.5656e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5969e-06, 1.1991e-06, 6.5290e-07, 1.0707e-06, 2.6537e-07, 3.2371e-06,
        4.5137e-07, 1.8634e-06, 1.2832e-06, 1.9247e-06, 1.4711e-06, 6.8630e-07,
        6.5581e-07, 2.6515e-06, 8.5221e-07, 8.8756e-07, 4.6630e-07, 1.1991e-06,
        1.4750e-01, 3.4774e-06, 7.5731e-07, 1.5969e-06, 1.9247e-06, 7.1588e-07,
        1.2227e-06, 4.9886e-07, 1.1003e-06, 1.9015e-06, 9.5307e-07, 3.2186e-06,
        7.0244e-07, 1.9247e-06, 6.8273e-07, 2.8444e-06, 7.5731e-07, 2.0966e-06,
        1.1552e-06, 3.1319e-06, 3.0729e-06, 3.0127e-06, 1.3265e-06, 4.3077e-07,
        2.2330e-06, 1.9614e-06, 2.6515e-06, 1.1116e-06, 2.5334e-06, 3.2371e-06,
        6.8772e-07, 1.4178e-06, 1.0707e-06, 4.6630e-07, 3.4014e-06, 6.8772e-07,
        1.2564e-01, 1.1931e-06, 1.1992e-06, 1.5355e-06, 4.9494e-07, 3.3426e-06,
        2.3798e-06, 1.2480e-06, 1.3339e-06, 4.6927e-07, 1.5001e-06, 1.5598e-06,
        1.1150e-06, 7.8908e-07, 6.1975e-07, 2.7047e-06, 1.1336e-06, 1.3094e-06,
        2.7784e-06, 3.2371e-06, 2.2990e-06, 6.8772e-07, 1.9434e-06, 2.6515e-06,
        1.0743e-06, 1.2753e-06, 7.5731e-07, 3.0127e-06, 1.9614e-06, 1.9247e-06,
        2.0154e-06, 2.5923e-06, 1.6158e-06, 1.9445e-06, 7.0012e-07, 9.5721e-07,
        5.9706e-07, 8.5221e-07, 3.3797e-06, 1.2649e-06, 9.5307e-07, 1.9247e-06,
        5.6983e-07, 3.2186e-06, 3.0020e-06, 1.3491e-06, 1.9015e-06, 1.2904e-06,
        1.9483e-06, 1.1335e-06, 1.3728e-06, 2.7762e-06, 3.2186e-06, 1.5179e-06,
        1.3491e-06, 1.9555e-06, 8.8756e-07, 1.0743e-06, 1.2453e-06, 1.3728e-06,
        7.5731e-07, 8.4229e-07, 1.3339e-06, 1.9675e-06, 1.6803e-06, 1.8402e-07,
        3.0028e-06, 1.2227e-06, 1.2480e-06, 2.3798e-06, 5.8471e-07, 2.6515e-06,
        2.2478e-06, 9.5308e-07, 1.5001e-06, 3.2371e-06, 1.9614e-06, 1.6999e-06,
        1.2453e-06, 2.1750e-06, 4.9886e-07, 7.5731e-07, 8.8703e-07, 3.2371e-06,
        3.2371e-06, 9.5721e-07, 1.1991e-06, 1.3060e-06, 6.8273e-07, 2.2914e-06,
        7.7214e-07, 1.1887e-06, 7.5731e-07, 1.5919e-06, 9.5721e-07, 4.5137e-07,
        9.5461e-07, 1.3391e-06, 1.1552e-06, 1.0223e-06, 1.3491e-06, 9.5308e-07,
        4.3077e-07, 1.2227e-06, 1.9887e-06, 5.4142e-07, 1.3339e-06, 3.0522e-06,
        1.6004e-06, 1.0743e-06, 1.9614e-06, 3.3426e-06, 1.5751e-06, 1.1931e-06,
        3.9970e-07, 3.0127e-06, 1.6244e-06, 1.5001e-06, 4.4108e-07, 1.9015e-06,
        1.0149e-06, 1.3258e-01, 1.3058e-06, 3.0522e-06, 6.1975e-07, 1.9247e-06,
        7.8908e-07, 2.6515e-06, 2.0501e-06, 7.8908e-07, 1.3728e-06, 1.0587e-06,
        1.3728e-06, 1.0587e-06, 2.2532e-06, 1.9015e-06, 7.6642e-07, 1.6803e-06,
        1.8698e-06, 3.0728e-06, 8.2913e-07, 1.7571e-06, 3.4774e-06, 1.8454e-06,
        5.4142e-07, 1.3740e-06, 1.7737e-06, 5.6387e-07, 2.3583e-06, 1.1992e-06,
        3.9204e-06, 1.6066e-06, 1.1116e-06, 1.3060e-06, 3.2186e-06, 1.3522e-06,
        8.5221e-07, 1.3339e-06, 2.5010e-06, 2.6824e-06, 1.6500e-06, 1.9015e-06,
        1.6158e-06, 6.8772e-07, 8.9402e-07, 1.1991e-06, 1.0743e-06, 7.8908e-07,
        2.2330e-06, 9.5650e-07, 2.6515e-06, 9.1749e-07, 9.5721e-07, 2.0296e-06,
        1.6231e-06, 2.2478e-06, 3.4422e-06, 9.5307e-07, 4.5805e-07, 1.1782e-06,
        1.1011e-06, 3.2186e-06, 5.9706e-07, 2.3120e-06, 2.5923e-06, 3.0153e-06,
        2.4293e-06, 1.3058e-06, 7.5731e-07, 1.1335e-06, 3.5264e-06, 6.5290e-07,
        8.8706e-08, 9.5897e-07, 2.2422e-06, 9.5307e-07, 4.8904e-07, 2.6515e-06,
        1.2613e-06, 7.5731e-07, 2.2330e-06, 3.3797e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.6016e-06, 7.5901e-06, 1.0560e-05, 4.9691e-06, 1.7218e-06, 6.3959e-06,
        3.2663e-06, 2.9450e-06, 1.3177e-06, 6.0114e-06, 7.9169e-06, 2.0465e-06,
        6.5593e-06, 3.4044e-06, 2.3861e-06, 4.1906e-06, 3.4489e-06, 4.9071e-06,
        5.4683e-06, 2.1754e-06, 4.7187e-06, 4.1211e-06, 7.2530e-06, 6.9532e-06,
        2.1627e-06, 2.6612e-06, 5.0307e-06, 8.0382e-06, 2.7016e-06, 1.7318e-06,
        5.9614e-06, 4.7573e-06, 9.2727e-06, 1.4551e-06, 2.9972e-06, 7.4597e-06,
        5.1136e-06, 4.4227e-06, 4.5259e-06, 1.1945e-06, 1.5389e-06, 4.3066e-06,
        3.3123e-06, 1.0579e-05, 1.4610e-06, 5.5578e-07, 2.6612e-06, 2.2438e-06,
        2.6912e-06, 9.8023e-06, 4.4708e-06, 5.8657e-06, 2.2215e-07, 5.9690e-06,
        2.3622e-06, 1.8855e-06, 4.3483e-06, 2.0938e-06, 1.8504e-06, 6.2136e-06,
        2.9002e-06, 2.9720e-06, 5.1653e-06, 7.1493e-06, 2.2189e-06, 1.3398e-05,
        4.4913e-06, 3.0002e-06, 6.2136e-06, 1.4307e-06, 3.2659e-06, 4.7187e-06,
        3.4676e-06, 3.1796e-06, 4.5167e-06, 2.8537e-06, 2.2189e-06, 2.5724e-06,
        6.0290e-06, 1.7318e-06, 6.9645e-06, 2.3809e-06, 5.4683e-06, 1.8039e-06,
        5.4683e-06, 2.3196e-06, 8.6879e-06, 3.5100e-07, 1.1884e-06, 2.2438e-06,
        1.7761e-06, 3.4676e-06, 3.2299e-06, 5.4683e-06, 3.5410e-06, 8.6879e-06,
        2.5337e-06, 3.6294e-06, 6.0603e-06, 6.4293e-06, 3.0002e-06, 2.0465e-06,
        2.5840e-06, 3.2747e-06, 4.2428e-06, 2.2885e-06, 1.9036e-06, 3.9579e-06,
        5.4683e-06, 4.7919e-06, 4.3066e-06, 2.3531e-06, 4.3711e-06, 6.1422e-06,
        6.9531e-06, 4.7373e-06, 1.6450e-06, 3.6280e-06, 6.2340e-06, 6.2136e-06,
        4.3066e-06, 6.5593e-06, 1.4977e-01, 5.2536e-07, 2.6567e-06, 1.8686e-06,
        9.5009e-07, 3.3048e-06, 1.3407e-06, 2.2438e-06, 4.1650e-06, 3.5916e-06,
        7.1205e-06, 2.1234e-06, 6.4827e-06, 4.6825e-06, 5.4683e-06, 2.3166e-06,
        2.2438e-06, 3.4055e-06, 2.2438e-06, 2.5840e-06, 8.7387e-06, 6.4827e-06,
        3.4358e-06, 1.7464e-06, 1.7037e-06, 3.0213e-06, 6.5681e-06, 1.5245e-06,
        6.1801e-06, 5.9690e-06, 7.1836e-06, 3.2040e-06, 7.3434e-06, 3.4385e-06,
        3.2299e-06, 3.4676e-06, 2.4860e-06, 3.6294e-06, 3.9030e-06, 2.0465e-06,
        3.6256e-06, 7.2539e-06, 3.5916e-06, 4.5213e-06, 7.5667e-06, 3.6256e-06,
        2.5853e-06, 2.2438e-06, 8.1334e-06, 2.9002e-06, 5.1503e-06, 5.4655e-06,
        6.6077e-06, 3.1290e-06, 6.9531e-06, 3.1871e-06, 4.5213e-06, 4.6218e-06,
        2.2438e-06, 6.5593e-06, 5.6694e-06, 3.3954e-06, 4.7733e-06, 6.4293e-06,
        1.5674e-06, 1.5389e-06, 3.5026e-06, 2.3674e-06, 2.5155e-06, 5.5142e-06,
        6.7999e-06, 5.4683e-06, 2.2438e-06, 3.5916e-06, 5.9914e-06, 2.3926e-06,
        4.1305e-06, 2.7065e-06, 4.8770e-06, 2.1627e-06, 3.7919e-06, 3.4676e-06,
        4.7733e-06, 2.5840e-06, 6.9660e-06, 6.1422e-06, 7.2530e-06, 5.2697e-06,
        1.4035e-06, 2.9368e-06, 2.3926e-06, 3.9749e-06, 3.9749e-06, 5.3748e-06,
        4.7276e-06, 2.5111e-06, 3.5026e-06, 5.6961e-06, 2.5840e-06, 2.1235e-06,
        3.2207e-06, 3.8908e-06, 3.4676e-06, 1.8504e-06, 2.0882e-06, 2.3622e-06,
        3.5100e-07, 1.7208e-06, 7.2530e-06, 2.5648e-06, 7.9742e-06, 3.8113e-06,
        4.7733e-06, 3.0443e-06, 7.2530e-06, 7.3988e-06, 3.4770e-06, 6.2340e-06,
        4.4708e-06, 9.3409e-06, 2.7318e-06, 1.8447e-06, 3.2659e-06, 2.0465e-06,
        2.5648e-06, 3.1113e-06, 2.8642e-06, 5.6034e-06, 1.7154e-06, 6.8820e-06,
        8.6879e-06, 4.7276e-06, 4.5833e-06, 5.4683e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.9173e-07, 4.4682e-07, 3.6352e-07,  ..., 5.7896e-07, 6.1695e-08,
        1.0272e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.1815e-07, 1.5423e-06, 1.1144e-06, 1.0833e-06, 8.2811e-07, 1.7601e-06,
        1.6365e-06, 1.7430e-06, 2.0580e-06, 9.6974e-07, 8.2874e-07, 1.0846e-06,
        2.6266e-06, 1.0038e-06, 1.1144e-06, 1.4586e-06, 1.1065e-06, 1.1144e-06,
        2.2633e-06, 1.1399e-06, 1.3266e-06, 6.2356e-07, 2.9401e-06, 2.5214e-06,
        1.4102e-06, 5.7982e-07, 8.6887e-07, 8.3188e-07, 1.1399e-06, 1.6088e-06,
        8.6408e-07, 4.0477e-07, 9.1627e-07, 1.1175e-06, 6.8611e-07, 1.7727e-06,
        1.7430e-06, 1.9811e-01, 5.7982e-07, 1.7212e-06, 1.6744e-06, 2.4400e-06,
        2.6254e-06, 6.6170e-07, 1.5856e-06, 7.6982e-07, 6.7514e-07, 6.2356e-07,
        1.1343e-06, 2.0929e-06, 1.6499e-06, 7.9168e-07, 1.1175e-06, 7.2715e-07,
        8.2799e-07, 1.0556e-06, 1.5241e-06, 1.3878e-06, 1.2227e-06, 1.1592e-06,
        1.9593e-06, 3.8119e-07, 5.6803e-07, 7.9168e-07, 2.8061e-06, 1.4315e-06,
        1.3188e-06, 1.9593e-06, 1.7832e-06, 9.1627e-07, 1.4586e-06, 7.6982e-07,
        1.0556e-06, 1.4748e-06, 6.9652e-07, 2.6266e-06, 2.1021e-06, 1.5241e-06,
        7.2715e-07, 5.4801e-07, 1.0417e-06, 4.7040e-07, 4.9489e-07, 2.2633e-06,
        2.3871e-06, 4.9489e-07, 2.1047e-06, 2.1047e-06, 9.6974e-07, 1.1005e-06,
        1.2819e-06, 1.3188e-06, 1.6744e-06, 6.8611e-07, 7.6982e-07, 4.0496e-06,
        9.2522e-07, 9.6974e-07, 8.2874e-07, 1.6296e-06, 6.2356e-07, 1.3792e-06,
        2.2870e-06, 2.7893e-06, 1.3878e-06, 6.6608e-07, 6.6170e-07, 2.3782e-06,
        9.3084e-07, 1.2743e-06, 1.3860e-06, 2.1351e-07, 3.9173e-07, 7.2435e-07,
        3.6250e-07, 1.1144e-06, 8.7678e-07, 1.3266e-06, 7.4145e-07, 1.0417e-06,
        5.9547e-07, 1.6789e-06, 5.9547e-07, 1.1809e-06, 1.9593e-06, 5.4884e-07,
        8.2811e-07, 1.3285e-07, 1.3188e-06, 1.1176e-06, 6.8611e-07, 9.5601e-07,
        6.2356e-07, 3.5389e-07, 1.1175e-06, 1.1809e-06, 1.9593e-06, 1.3266e-06,
        1.0301e-06, 2.3150e-07, 1.3878e-06, 1.0556e-06, 8.2874e-07, 5.9547e-07,
        8.2799e-07, 1.1144e-06, 1.1399e-06, 2.6266e-06, 7.6982e-07, 2.3150e-07,
        1.2819e-06, 1.6838e-06, 2.7893e-06, 9.1501e-07, 1.1592e-06, 1.4102e-06,
        1.3285e-07, 1.1343e-06, 6.2356e-07, 1.0846e-06, 2.8049e-06, 2.0009e-06,
        1.7304e-06, 1.3285e-07, 1.1144e-06, 7.6930e-07, 2.1760e-06, 6.7514e-07,
        1.7601e-06, 6.2356e-07, 2.1351e-07, 5.9083e-07, 6.7514e-07, 4.9489e-07,
        7.6930e-07, 1.6744e-06, 5.7982e-07, 8.2874e-07, 2.2633e-06, 1.1592e-06,
        9.5601e-07, 2.3150e-07, 1.1176e-06, 6.5755e-07, 1.8264e-06, 2.2870e-06,
        1.2902e-06, 7.2715e-07, 2.8061e-06, 4.7738e-07, 9.1627e-07, 1.8264e-06,
        1.1925e-06, 1.1144e-06, 7.4145e-07, 8.8222e-07, 1.1144e-06, 2.7104e-06,
        7.6930e-07, 6.7514e-07, 1.0095e-06, 7.2715e-07, 1.1144e-06, 7.7204e-07,
        2.2633e-06, 2.8049e-06, 7.1187e-07, 9.1627e-07, 8.2874e-07, 1.4748e-06,
        6.8611e-07, 1.4102e-06, 6.6171e-07, 8.2811e-07, 1.5856e-06, 5.1186e-07,
        3.5389e-07, 6.6170e-07, 5.7940e-07, 9.6974e-07, 1.0417e-06, 1.1175e-06,
        1.1144e-06, 1.7601e-06, 2.6717e-06, 1.2819e-06, 1.4748e-06, 2.8061e-06,
        2.2633e-06, 6.0955e-07, 1.5241e-06, 1.1809e-06, 5.1186e-07, 2.1047e-06,
        6.8611e-07, 1.1809e-06, 1.0556e-06, 2.8049e-06, 1.1399e-06, 4.9184e-07,
        9.6974e-07, 9.6974e-07, 7.9168e-07, 1.3188e-06, 1.1144e-06, 5.4801e-07,
        1.4340e-06, 4.9611e-07, 2.7893e-06, 1.1399e-06, 9.0271e-07, 1.0236e-06,
        5.9547e-07, 7.2184e-07, 1.1144e-06, 2.3606e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.6091e-06, 2.1464e-06, 6.0524e-06, 1.2072e-06, 2.4850e-06, 5.5688e-06,
        3.2629e-06, 3.3208e-06, 2.4376e-06, 1.6412e-06, 4.5437e-06, 2.3090e-06,
        1.2503e-06, 7.4721e-06, 3.3550e-06, 5.8456e-06, 1.8083e-06, 5.0065e-06,
        6.8846e-06, 5.2074e-06, 4.7702e-06, 4.5596e-06, 1.6655e-06, 8.6236e-06,
        1.1829e-01, 9.4527e-06, 7.7163e-06, 4.3555e-06, 5.7825e-06, 4.8092e-06,
        3.8925e-06, 3.7429e-06, 3.1917e-06, 3.3340e-06, 2.2271e-06, 9.1181e-06,
        1.6948e-06, 2.8259e-06, 5.1946e-06, 4.5175e-07, 5.1118e-06, 7.6385e-06,
        4.2170e-06, 4.2115e-06, 5.0475e-06, 1.2420e-06, 6.1515e-06, 7.7163e-06,
        2.7300e-06, 1.9074e-06, 2.7900e-06, 2.6901e-06, 3.0161e-06, 2.3713e-06,
        3.6222e-06, 4.4385e-06, 1.4924e-06, 2.9135e-06, 8.6236e-06, 4.9474e-06,
        4.6660e-06, 5.9054e-06, 9.8419e-06, 6.2801e-06, 5.5359e-06, 5.8456e-06,
        1.9416e-06, 3.1427e-06, 1.7131e-06, 1.6744e-06, 5.2799e-06, 3.6711e-06,
        4.7913e-06, 2.3359e-06, 4.0175e-06, 3.7675e-06, 4.6890e-06, 3.6899e-06,
        2.9135e-06, 4.3399e-06, 9.7563e-06, 3.8968e-06, 2.7865e-06, 5.3891e-06,
        4.2170e-06, 1.9425e-06, 1.9416e-06, 5.5671e-06, 3.5361e-06, 4.0438e-06,
        3.4464e-06, 4.5568e-06, 2.6043e-06, 3.2559e-07, 5.4825e-06, 9.1181e-06,
        1.6412e-06, 6.1486e-06, 3.8344e-06, 2.5032e-06, 4.6132e-06, 3.6711e-06,
        2.0406e-06, 1.3277e-06, 6.0381e-06, 2.9135e-06, 4.7421e-06, 5.5235e-06,
        4.2170e-06, 5.1158e-06, 2.4376e-06, 2.8305e-06, 3.0438e-06, 2.5523e-06,
        4.9597e-06, 2.8259e-06, 2.9002e-06, 8.6236e-06, 4.6481e-06, 3.3930e-06,
        4.1393e-06, 3.9141e-06, 4.5359e-06, 5.4169e-06, 3.9765e-06, 3.1715e-06,
        6.5736e-06, 3.2020e-06, 6.4875e-06, 1.6655e-06, 2.9695e-06, 6.5059e-06,
        7.6666e-06, 2.3907e-06, 6.2562e-06, 1.6412e-06, 3.3600e-06, 2.1767e-06,
        6.7127e-06, 3.8843e-06, 1.2072e-06, 4.6685e-06, 7.4721e-06, 5.2028e-06,
        4.6481e-06, 6.2830e-06, 2.4037e-06, 2.5076e-06, 1.2072e-06, 3.1917e-06,
        4.9474e-06, 7.9104e-06, 1.6655e-06, 3.9709e-06, 2.3917e-06, 4.2342e-06,
        3.2952e-06, 3.3884e-06, 3.3339e-06, 4.4769e-06, 4.7143e-06, 8.1331e-06,
        2.2271e-06, 4.1174e-06, 2.3359e-06, 5.7825e-06, 1.7480e-06, 1.6710e-06,
        3.6222e-06, 4.9817e-06, 6.4291e-06, 3.4800e-06, 2.0985e-06, 3.0161e-06,
        7.6746e-06, 3.1658e-06, 4.6132e-06, 2.5032e-06, 3.1433e-06, 3.2888e-06,
        6.7994e-06, 1.0598e-06, 4.9817e-06, 2.4037e-06, 4.7095e-06, 3.0820e-06,
        2.4890e-06, 9.7429e-06, 1.2072e-06, 5.6050e-06, 7.6023e-06, 8.6298e-06,
        4.4503e-06, 1.6948e-06, 1.4868e-06, 3.4582e-06, 6.4441e-06, 6.2562e-06,
        5.5400e-06, 5.6243e-06, 5.3369e-06, 6.3141e-06, 4.7730e-06, 7.7616e-06,
        7.3585e-06, 3.1467e-06, 5.0309e-06, 3.4191e-06, 6.2562e-06, 1.1988e-05,
        2.1478e-06, 3.1154e-06, 5.6050e-06, 3.6253e-06, 8.9181e-06, 1.5577e-06,
        3.3997e-06, 7.4335e-06, 9.4831e-06, 5.8455e-06, 4.0808e-06, 1.9735e-06,
        2.1464e-06, 5.5280e-06, 2.7900e-06, 2.3528e-06, 2.5107e-06, 2.2838e-06,
        5.7705e-06, 6.8846e-06, 7.2828e-06, 7.2883e-06, 2.9135e-06, 6.2562e-06,
        1.3000e-05, 3.0161e-06, 4.7373e-06, 4.6660e-06, 1.6412e-06, 5.5671e-06,
        1.2830e-06, 1.6434e-07, 8.6298e-06, 3.5253e-06, 1.6152e-06, 6.5103e-06,
        1.9770e-06, 1.1325e-06, 2.4898e-06, 6.7127e-06, 8.3683e-07, 6.0381e-06,
        2.6901e-06, 7.0930e-06, 3.3884e-06, 2.9002e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.1376e-07, 1.9293e-07, 2.6609e-07,  ..., 1.1490e-07, 6.3090e-07,
        6.3036e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.6135e-07, 4.7537e-07, 8.5239e-07, 5.3440e-07, 5.3440e-07, 2.2469e-06,
        2.2469e-06, 5.1472e-07, 4.8833e-07, 2.2469e-06, 9.4676e-07, 8.8407e-07,
        1.5355e-06, 2.5473e-07, 6.1076e-07, 1.5707e-07, 4.8833e-07, 5.3440e-07,
        1.1364e-06, 1.9398e-07, 1.4919e-06, 1.3892e-06, 1.3892e-06, 1.3892e-06,
        4.6107e-07, 1.9398e-07, 1.6038e-06, 1.1182e-06, 1.0244e-06, 4.6203e-07,
        1.1182e-06, 8.0681e-07, 6.1076e-07, 9.4676e-07, 1.0285e-06, 2.1812e-06,
        5.3440e-07, 3.6609e-07, 7.6497e-07, 1.3919e-06, 1.3892e-06, 4.8541e-07,
        1.4469e-06, 2.1805e-07, 2.2470e-06, 5.6135e-07, 5.3440e-07, 7.3419e-07,
        7.0816e-07, 8.8407e-07, 4.7539e-07, 3.6609e-07, 4.5149e-07, 1.1182e-06,
        9.4676e-07, 6.1076e-07, 1.3892e-06, 2.9004e-07, 1.4607e-06, 4.6308e-07,
        1.0500e-06, 1.9398e-07, 2.2164e-06, 2.2174e-06, 4.0708e-07, 4.8833e-07,
        1.0244e-06, 2.0083e-02, 4.0708e-07, 1.4469e-06, 1.1182e-06, 2.2470e-06,
        7.3419e-07, 2.2159e-06, 4.6203e-07, 1.2286e-06, 8.0681e-07, 2.2470e-06,
        2.2615e-06, 6.0027e-07, 8.8407e-07, 3.9789e-07, 1.3919e-06, 2.1784e-06,
        8.8407e-07, 1.2238e-06, 1.4607e-06, 7.4549e-07, 1.5355e-06, 1.3362e-06,
        8.5239e-07, 1.1182e-06, 4.7539e-07, 4.5725e-07, 8.8045e-07, 2.2470e-06,
        3.6609e-07, 1.0500e-06, 4.7537e-07, 9.4676e-07, 4.5138e-07, 2.2165e-06,
        4.8833e-07, 4.8541e-07, 4.5138e-07, 8.0681e-07, 1.7775e-06, 4.6107e-07,
        8.8407e-07, 4.5149e-07, 1.2238e-06, 4.7536e-07, 1.9398e-07, 2.2469e-06,
        3.6609e-07, 1.7775e-06, 6.1076e-07, 4.0708e-07, 6.0110e-07, 9.4676e-07,
        8.0919e-07, 4.8824e-07, 2.1049e-06, 8.5239e-07, 1.3362e-06, 2.9004e-07,
        2.2469e-06, 5.6135e-07, 2.2467e-06, 1.5355e-06, 8.8044e-07, 8.8407e-07,
        1.3892e-06, 1.3892e-06, 4.7539e-07, 6.3704e-07, 1.5707e-07, 4.2698e-07,
        5.0396e-07, 1.5707e-07, 1.2566e-06, 1.5707e-07, 8.5239e-07, 1.2286e-06,
        6.1076e-07, 7.3419e-07, 5.6135e-07, 8.0681e-07, 1.0285e-06, 5.3440e-07,
        1.7037e-07, 6.8677e-07, 1.3892e-06, 2.2615e-06, 1.3892e-06, 4.8541e-07,
        1.5355e-06, 8.5239e-07, 2.2615e-06, 1.9222e-06, 1.9398e-07, 1.2566e-06,
        4.6107e-07, 1.5355e-06, 6.1076e-07, 1.4607e-06, 2.1857e-06, 1.1902e-06,
        1.0440e-06, 2.2999e-07, 4.8541e-07, 4.8541e-07, 1.4607e-06, 1.9398e-07,
        7.6577e-07, 8.8407e-07, 4.0708e-07, 4.5138e-07, 1.0285e-06, 4.7539e-07,
        1.4469e-06, 2.2469e-06, 1.2286e-06, 1.9398e-07, 2.2966e-07, 1.6015e-06,
        8.5239e-07, 4.8833e-07, 1.9222e-06, 4.5138e-07, 7.3419e-07, 8.0681e-07,
        1.2238e-06, 2.2615e-06, 1.9222e-06, 2.2498e-06, 1.0500e-06, 4.6203e-07,
        7.6497e-07, 4.6308e-07, 4.8833e-07, 5.3440e-07, 7.6497e-07, 6.9807e-07,
        1.6262e-06, 4.9567e-07, 4.8833e-07, 4.8541e-07, 1.0776e-06, 1.0417e-06,
        3.2774e-07, 6.1076e-07, 2.1049e-06, 4.8541e-07, 5.4720e-07, 4.6308e-07,
        2.2615e-06, 7.6497e-07, 5.4720e-07, 4.8833e-07, 1.4469e-06, 2.2469e-06,
        6.1076e-07, 2.9004e-07, 8.5239e-07, 5.3440e-07, 1.5355e-06, 6.4318e-07,
        7.3419e-07, 5.3440e-07, 6.1076e-07, 2.2469e-06, 6.1076e-07, 6.2664e-07,
        1.5355e-06, 6.2965e-07, 5.6135e-07, 1.4469e-06, 2.2164e-06, 6.9807e-07,
        6.1076e-07, 1.4469e-06, 3.2776e-07, 2.1805e-07, 8.4764e-07, 2.2469e-06,
        1.3892e-06, 1.9398e-07, 4.2698e-07, 1.6263e-06, 9.4676e-07, 1.4469e-06,
        2.1805e-07, 2.2467e-06, 1.3919e-06, 2.1805e-07, 3.4245e-07, 2.2469e-06,
        1.4571e-06, 1.9398e-07, 2.1805e-07, 2.2469e-06, 1.4607e-06, 6.1076e-07,
        2.2469e-06, 1.7775e-06, 1.3892e-06, 2.2162e-06, 5.6135e-07, 8.5239e-07,
        9.4676e-07, 1.5707e-07, 4.7539e-07, 3.8716e-07, 8.0681e-07, 8.5239e-07,
        4.7537e-07, 7.8583e-07, 4.7225e-07, 3.6609e-07, 2.1857e-06, 3.6849e-07,
        5.6135e-07, 1.6262e-06, 3.6609e-07, 4.8541e-07, 2.2469e-06, 7.1522e-07,
        2.2469e-06, 4.6107e-07, 2.9004e-07, 5.3440e-07, 5.7543e-07, 5.6135e-07,
        5.3440e-07, 1.7767e-06, 1.7775e-06, 9.4676e-07, 6.3308e-07, 5.4720e-07,
        9.4676e-07, 4.8541e-07, 4.9567e-07, 6.3308e-07, 1.6015e-06, 4.7535e-07,
        2.2470e-06, 2.2999e-07, 7.3792e-07, 1.4919e-06, 6.1076e-07, 5.3440e-07,
        2.3997e-07, 1.4190e-06, 1.3362e-06, 2.2469e-06, 1.4607e-06, 1.9398e-07,
        2.1805e-07, 6.3571e-07, 1.4469e-06, 6.0027e-07, 1.1364e-06, 9.4676e-07,
        1.4607e-06, 2.1766e-06, 4.6308e-07, 6.3704e-07, 3.6849e-07, 6.9515e-07,
        1.4469e-06, 1.4469e-06, 7.8594e-07, 7.3419e-07, 5.4720e-07, 8.5239e-07,
        8.8407e-07, 7.6497e-07, 7.3419e-07, 3.3843e-03, 1.3892e-06, 7.3419e-07,
        1.9222e-06, 7.6497e-07, 1.4469e-06, 1.5707e-07, 1.4584e-06, 1.0500e-06,
        1.5707e-07, 5.5220e-07, 4.2698e-07, 2.1857e-06, 4.9000e-07, 1.5707e-07,
        1.3362e-06, 8.8045e-07, 8.8044e-07, 4.8833e-07, 2.2469e-06, 1.3892e-06,
        7.3419e-07, 1.2483e-06, 1.3892e-06, 1.4469e-06, 2.1805e-07, 5.7543e-07,
        2.2500e-06, 8.5239e-07, 1.5355e-06, 7.3419e-07, 4.8833e-07, 7.3419e-07,
        1.0440e-06, 1.0831e-06, 5.3440e-07, 2.1805e-07, 7.3419e-07, 1.6262e-06,
        5.8318e-07, 2.2470e-06, 1.5355e-06, 7.1522e-07, 5.6135e-07, 4.8833e-07,
        1.4607e-06, 8.5239e-07, 1.7775e-06, 4.8833e-07, 1.4469e-06, 2.9004e-07,
        2.2615e-06, 8.8044e-07, 1.3892e-06, 2.2173e-06, 2.2469e-06, 1.3892e-06,
        1.4469e-06, 2.2469e-06, 6.1076e-07, 3.6609e-07, 4.7535e-07, 2.6533e-02,
        3.6849e-07, 2.1051e-06, 5.6135e-07, 2.8161e-07, 1.1621e-06, 1.4469e-06,
        1.1182e-06, 1.4607e-06, 1.3892e-06, 1.3892e-06, 1.6263e-06, 1.9222e-06,
        1.6015e-06, 2.2615e-06, 1.0500e-06, 5.6135e-07, 4.5138e-07, 1.0500e-06,
        4.8833e-07, 3.1301e-07, 7.7071e-07, 8.8044e-07, 9.3337e-07, 3.6609e-07,
        6.4318e-07, 5.3440e-07, 2.2469e-06, 1.0500e-06, 8.5239e-07, 1.3919e-06,
        8.8407e-07, 4.5041e-02, 2.1766e-06, 9.4676e-07, 5.1472e-07, 1.6262e-06,
        4.8833e-07, 5.6135e-07, 7.3792e-07, 1.5707e-07, 5.3937e-07, 1.9398e-07,
        1.5355e-06, 3.6609e-07, 2.2469e-06, 1.7775e-06, 1.4469e-06, 6.1076e-07,
        1.7775e-06, 5.6135e-07, 1.5355e-06, 5.4720e-07, 1.9398e-07, 5.3440e-07,
        1.9398e-07, 4.6308e-07, 7.3419e-07, 4.6107e-07, 4.8833e-07, 1.9398e-07,
        8.5239e-07, 2.2469e-06, 6.3704e-07, 3.4661e-07, 3.6849e-07, 2.1805e-07,
        2.2615e-06, 5.6135e-07, 5.3440e-07, 1.3892e-06, 1.3892e-06, 1.7613e-06,
        6.3704e-07, 5.3440e-07, 2.1857e-06, 7.3257e-07, 8.8407e-07, 1.7775e-06,
        2.2469e-06, 8.0681e-07, 5.3440e-07, 9.4676e-07, 1.0831e-06, 1.3892e-06,
        1.5004e-01, 4.5149e-07, 1.0831e-06, 1.2286e-06, 9.0639e-07, 8.5239e-07,
        7.3419e-07, 8.8407e-07, 1.0285e-06, 1.2769e-06, 4.9567e-07, 1.2238e-06,
        4.7539e-07, 1.1182e-06, 5.6135e-07, 6.9515e-07, 5.3908e-07, 2.6569e-07,
        5.3440e-07, 5.6135e-07, 6.4318e-07, 7.6497e-07, 1.4607e-06, 2.2999e-07,
        2.8894e-06, 4.8541e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.1366e-06, 9.6298e-06, 4.8274e-06, 1.8170e-06, 2.2636e-06, 1.2762e-05,
        5.1880e-06, 1.6488e-06, 9.6298e-06, 3.0259e-06, 5.0993e-06, 1.0344e-05,
        2.8531e-06, 7.3458e-06, 9.0110e-06, 7.3458e-06, 5.2158e-06, 5.4998e-06,
        9.6298e-06, 6.9711e-06, 1.3595e-05, 7.0863e-06, 6.9711e-06, 1.2403e-05,
        5.0993e-06, 4.0347e-06, 6.5262e-06, 3.0596e-06, 1.0344e-05, 5.9841e-06,
        6.4410e-06, 9.0905e-06, 3.9502e-06, 8.7235e-06, 4.7643e-06, 1.2762e-05,
        5.9841e-06, 2.5009e-06, 5.9975e-06, 1.0750e-05, 7.5792e-07, 3.3359e-06,
        2.8531e-06, 5.1094e-06, 2.2636e-06, 9.6298e-06, 5.7984e-06, 2.6740e-06,
        1.4659e-06, 1.8515e-06, 9.3886e-06, 5.0993e-06, 6.9711e-06, 4.7643e-06,
        3.5193e-06, 5.6440e-06, 1.3445e-05, 1.4659e-06, 4.4347e-06, 3.7900e-06,
        6.5262e-06, 5.9975e-06, 7.4516e-06, 3.5193e-06, 9.7514e-06, 1.1269e-05,
        9.0681e-07, 2.7020e-06, 2.8422e-06, 3.0259e-06, 4.7967e-06, 1.4659e-06,
        4.7643e-06, 1.2403e-05, 5.0993e-06, 6.5589e-06, 5.3014e-06, 4.1929e-06,
        1.0750e-05, 1.0344e-05, 4.9568e-06, 2.7240e-06, 1.2012e-05, 8.1570e-06,
        5.9841e-06, 1.4659e-06, 7.4516e-06, 3.0077e-06, 2.2636e-06, 1.2833e-05,
        8.9571e-06, 4.4347e-06, 2.4397e-07, 7.4516e-06, 3.1377e-06, 6.4498e-06,
        2.2636e-06, 3.7893e-06, 6.9711e-06, 9.7514e-06, 3.5193e-06, 1.0573e-06,
        1.6600e-06, 1.0001e-02, 7.4516e-06, 5.3423e-06, 8.9571e-06, 4.8274e-06,
        4.7967e-06, 1.0573e-06, 1.0573e-06, 2.2636e-06, 3.7892e-06, 6.3399e-06,
        5.8762e-06, 2.2636e-06, 1.8170e-06, 7.5792e-07, 3.5193e-06, 1.4552e-05,
        6.5262e-06, 8.9571e-06, 3.5193e-06, 6.0612e-06, 4.4347e-06, 3.5501e-06,
        2.5319e-06, 3.7893e-06, 5.0993e-06, 5.6440e-06, 9.6298e-06, 1.0573e-06,
        1.0344e-05, 8.9571e-06, 5.3033e-06, 3.5199e-06, 4.1929e-06, 3.1377e-06,
        6.5119e-06, 4.8275e-06, 2.0027e-06, 1.0344e-05, 3.0077e-06, 5.3014e-06,
        6.9711e-06, 4.7967e-06, 6.2012e-06, 5.9841e-06, 4.7643e-06, 3.1377e-06,
        7.7994e-06, 1.4659e-06, 5.4998e-06, 7.7994e-06, 1.0344e-05, 5.1094e-06,
        5.6440e-06, 3.7900e-06, 8.8499e-06, 5.6574e-06, 1.4659e-06, 5.0993e-06,
        1.0470e-05, 6.5262e-06, 7.9133e-06, 5.9841e-06, 8.4258e-06, 3.6197e-06,
        5.4197e-06, 6.0612e-06, 9.7688e-06, 6.0612e-06, 3.3905e-06, 4.1929e-06,
        9.1699e-06, 7.4516e-06, 9.6298e-06, 6.5519e-06, 3.0892e-06, 4.0347e-06,
        5.3014e-06, 1.0573e-06, 5.2104e-06, 4.9550e-06, 7.5792e-07, 1.2833e-05,
        6.7466e-06, 5.1094e-06, 9.1699e-06, 9.6298e-06, 4.3764e-06, 3.1377e-06,
        8.9571e-06, 3.9502e-06, 2.7240e-06, 3.5193e-06, 1.8519e-05, 7.7994e-06,
        5.6440e-06, 5.2104e-06, 3.3006e-06, 5.8619e-06, 2.2636e-06, 3.7893e-06,
        1.2833e-05, 5.3014e-06, 8.8499e-06, 1.8170e-06, 1.7872e-06, 5.9110e-06,
        9.6298e-06, 3.1062e-06, 2.7240e-06, 6.5262e-06, 6.3088e-06, 1.1307e-05,
        3.3905e-06, 5.4998e-06, 5.1094e-06, 4.8275e-06, 1.0470e-05, 8.9571e-06,
        3.3247e-06, 4.8541e-06, 3.7893e-06, 9.7514e-06, 2.7240e-06, 1.0573e-06,
        9.3006e-06, 7.6578e-06, 5.1094e-06, 1.4074e-05, 6.3088e-06, 5.2104e-06,
        1.0344e-05, 4.7014e-06, 1.0344e-05, 6.9711e-06, 4.7443e-06, 9.1699e-06,
        5.7500e-06, 5.7582e-06, 5.4998e-06, 4.8275e-06, 4.9568e-06, 9.3886e-06,
        5.3803e-06, 2.1343e-06, 4.0422e-06, 1.0790e-05, 1.2833e-05, 3.6197e-06,
        3.6440e-06, 5.4998e-06, 5.0993e-06, 3.4405e-06, 1.2012e-05, 4.0084e-06,
        4.7643e-06, 3.0259e-06, 3.7893e-06, 1.0573e-06, 4.1929e-06, 8.1497e-06,
        1.2403e-05, 5.4998e-06, 6.7466e-06, 3.5193e-06, 3.1377e-06, 3.6197e-06,
        6.0612e-06, 4.7643e-06, 3.7951e-06, 6.5589e-06, 7.5792e-07, 1.6488e-06,
        1.3182e-05, 1.0573e-06, 5.0325e-06, 4.7967e-06, 4.7643e-06, 3.2963e-06,
        1.1307e-05, 4.0347e-06, 1.7871e-06, 2.2636e-06, 1.2762e-05, 1.4659e-06,
        3.0259e-06, 8.7583e-06, 7.4516e-06, 3.3247e-06, 2.7240e-06, 8.3288e-06,
        7.6578e-06, 1.0344e-05, 2.1044e-06, 5.4020e-06, 1.3324e-05, 7.3399e-06,
        5.4998e-06, 1.2403e-05, 2.1552e-06, 6.4498e-06, 6.6444e-06, 3.2565e-06,
        6.0612e-06, 3.3096e-06, 6.4410e-06, 1.4659e-06, 1.0014e-06, 3.0259e-06,
        1.1969e-06, 5.9841e-06, 4.1929e-06, 2.8531e-06, 1.0573e-06, 6.5589e-06,
        3.5193e-06, 1.2012e-05, 1.1674e-05, 7.3399e-06, 5.4998e-06, 4.1929e-06,
        5.0993e-06, 3.5193e-06, 5.4707e-06, 3.5193e-06, 7.6578e-06, 9.1699e-06,
        4.9568e-06, 7.6578e-06, 5.1094e-06, 1.0470e-05, 2.1044e-06, 1.8170e-06,
        6.0612e-06, 3.1100e-06, 5.2104e-06, 3.3247e-06, 5.0993e-06, 5.1094e-06,
        5.1054e-06, 3.2565e-06, 7.6578e-06, 6.5119e-06, 3.3905e-06, 6.5119e-06,
        1.4659e-06, 6.3822e-06, 3.3905e-06, 5.0993e-06, 5.1094e-06, 4.7967e-06,
        5.3892e-06, 3.8195e-06, 9.7514e-06, 4.7014e-06, 1.4522e-05, 4.1929e-06,
        4.1929e-06, 3.6364e-06, 7.6539e-06, 4.4101e-06, 6.0612e-06, 3.7900e-06,
        7.6578e-06, 3.7893e-06, 6.5262e-06, 9.3961e-06, 1.0344e-05, 9.6298e-06,
        4.7967e-06, 6.5262e-06, 6.9711e-06, 5.8742e-06, 1.0949e-06, 7.5792e-07,
        1.8170e-06, 7.5792e-07, 6.9711e-06, 6.3399e-06, 4.4101e-06, 4.4347e-06,
        6.5262e-06, 5.1094e-06, 6.7466e-06, 5.9975e-06, 6.5589e-06, 5.1880e-06,
        3.3247e-06, 3.6364e-06, 2.2636e-06, 3.7951e-06, 6.0612e-06, 3.3247e-06,
        2.5009e-06, 1.0344e-05, 7.5792e-07, 3.5193e-06, 3.3247e-06, 5.1094e-06,
        7.4924e-06, 5.3014e-06, 1.3541e-05, 3.1062e-06, 1.0573e-06, 1.0573e-06,
        5.1094e-06, 5.5531e-06, 1.0573e-06, 3.7900e-06, 5.4998e-06, 3.3905e-06,
        2.4655e-06, 4.0347e-06, 2.4655e-06, 4.7643e-06, 3.7893e-06, 6.9711e-06,
        5.0993e-06, 1.0573e-06, 3.3905e-06, 3.0259e-06, 1.2012e-05, 3.1062e-06,
        3.5199e-06, 3.5193e-06, 5.2104e-06, 5.4707e-06, 5.3014e-06, 3.3905e-06,
        5.0993e-06, 4.7967e-06, 1.2012e-05, 2.2636e-06, 3.3905e-06, 4.4101e-06,
        3.1062e-06, 3.7893e-06, 3.5193e-06, 5.3014e-06, 5.9841e-06, 5.0618e-06,
        4.1929e-06, 5.1880e-06, 3.7907e-06, 2.7240e-06, 6.9711e-06, 3.5193e-06,
        3.7893e-06, 6.9711e-06, 3.2565e-06, 6.9711e-06, 2.7240e-06, 1.8170e-06,
        4.9636e-06, 1.2762e-05, 1.2833e-05, 3.1377e-06, 3.6740e-06, 3.9097e-06,
        4.7643e-06, 7.3355e-06, 7.3399e-06, 7.4516e-06, 5.3758e-06, 2.2636e-06,
        3.3905e-06, 4.8274e-06, 9.6298e-06, 4.1929e-06, 4.1929e-06, 5.0325e-06,
        9.6298e-06, 1.0470e-05, 3.5193e-06, 7.3458e-06, 3.9502e-06, 5.3014e-06,
        3.6197e-06, 3.7900e-06, 5.1094e-06, 2.7240e-06, 4.4101e-06, 1.0344e-05,
        1.0344e-05, 7.4516e-06, 1.8170e-06, 1.0344e-05, 4.8541e-06, 1.0344e-05,
        5.1094e-06, 5.0618e-06, 4.9568e-06, 4.7643e-06, 6.3399e-06, 6.9711e-06,
        7.6578e-06, 9.0905e-06, 8.9571e-06, 6.5262e-06, 3.3247e-06, 4.7967e-06,
        9.6298e-06, 5.9975e-06, 3.7900e-06, 1.2762e-05, 1.2012e-05, 5.1094e-06,
        3.3247e-06, 3.7893e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2425e-06, 9.4264e-07, 7.0137e-07,  ..., 6.3647e-07, 4.7673e-07,
        3.8092e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0034e-06, 7.4289e-07, 2.1823e-06,  ..., 2.6231e-06, 7.2800e-07,
        1.6625e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5461e-06, 1.2947e-06, 1.9296e-06, 2.1761e-06, 2.4370e-06, 3.6438e-06,
        3.3453e-06, 5.1526e-06, 1.9296e-06, 9.6910e-07, 5.3941e-06, 3.7029e-06,
        2.2041e-06, 3.6388e-06, 1.8692e-06, 5.3147e-06, 3.6438e-06, 2.7619e-06,
        1.9296e-06, 9.6910e-07, 2.5547e-06, 2.0080e-06, 3.5552e-06, 5.1386e-06,
        2.7619e-06, 2.7948e-06, 7.0673e-06, 7.4662e-06, 2.5505e-06, 4.0641e-06,
        4.4571e-06, 2.4630e-06, 6.8632e-07, 2.2051e-06, 2.7344e-06, 6.2393e-06,
        1.1627e-06, 2.0022e-06, 8.4311e-06, 1.9296e-06, 1.5523e-06, 2.0360e-06,
        4.8674e-06, 8.4311e-06, 2.5796e-06, 5.5554e-06, 2.4748e-06, 6.8632e-07,
        3.4385e-06, 5.9812e-06, 6.6650e-06, 2.2051e-06, 2.1009e-06, 3.4204e-06,
        1.0935e-06, 5.1920e-06, 5.1526e-06, 1.7597e-06, 4.4612e-06, 1.1963e-06,
        6.4485e-06, 2.0564e-06, 2.0564e-06, 4.4988e-06, 4.5831e-06, 2.0564e-06,
        2.2803e-06, 4.5831e-06, 2.0360e-06, 3.0131e-06, 1.8022e-06, 1.5523e-06,
        2.8333e-06, 2.4630e-06, 3.0131e-06, 6.4485e-06, 5.3147e-06, 1.8844e-06,
        8.3138e-06, 3.7029e-06, 1.8611e-06, 3.0237e-06, 4.1007e-06, 2.2052e-06,
        2.2041e-06, 5.7182e-06, 4.9675e-07, 1.2587e-06, 3.1844e-06, 1.9158e-06,
        2.4748e-06, 2.2203e-06, 1.8022e-06, 5.7182e-06, 3.8504e-06, 5.3941e-06,
        1.1704e-06, 8.0974e-06, 6.1555e-06, 6.2393e-06, 1.9708e-06, 1.0237e-06,
        2.3850e-06, 1.9296e-06, 1.9296e-06, 3.8227e-06, 2.3119e-06, 2.0360e-06,
        3.7578e-06, 3.9646e-06, 5.1230e-06, 2.5497e-06, 4.2834e-06, 8.9313e-06,
        5.1230e-06, 1.9296e-06, 9.5732e-07, 5.1810e-06, 3.1105e-06, 3.4675e-06,
        5.1014e-06, 9.5732e-07, 3.0462e-06, 2.0360e-06, 1.6470e-06, 8.4311e-06,
        5.6366e-06, 8.9313e-06, 5.9812e-06, 7.5128e-08, 1.5905e-06, 2.3621e-06,
        3.4204e-06, 2.0450e-06, 3.1105e-06, 7.0673e-06, 6.2397e-06, 1.5523e-06,
        7.5128e-08, 1.8692e-06, 2.0657e-06, 5.1233e-06, 3.3376e-06, 2.6586e-01,
        9.5732e-07, 3.7775e-06, 3.5813e-06, 1.9296e-06, 1.3699e-06, 3.8874e-06,
        2.4737e-01, 1.0164e-06, 3.8227e-06, 1.2514e-06, 3.2776e-06, 4.4165e-06,
        3.5552e-06, 1.7663e-06, 5.4072e-06, 7.0673e-06, 4.0027e-06, 1.6636e-06,
        5.9812e-06, 1.9296e-06, 1.7597e-06, 3.7393e-06, 2.5256e-06, 3.7775e-06,
        6.7618e-06, 1.3196e-06, 2.3855e-06, 1.4877e-06, 4.5831e-06, 3.3013e-06,
        6.8632e-07, 2.0327e-06, 5.9718e-07, 2.3621e-06, 3.7578e-06, 2.5812e-06,
        2.4641e-06, 3.6438e-06, 3.8624e-06, 3.9395e-06, 3.3453e-06, 7.8293e-07,
        2.2052e-06, 2.0360e-06, 1.6234e-06, 2.0360e-06, 4.5730e-06, 5.7182e-06,
        4.2295e-06, 5.3079e-06, 7.4662e-06, 1.3660e-06, 2.0450e-06, 3.3453e-06,
        3.7069e-06, 2.2203e-06, 5.3941e-06, 3.2776e-06, 1.2514e-06, 1.2104e-06,
        2.0564e-06, 1.9296e-06, 1.4789e-06, 3.4176e-06, 8.4040e-07, 2.2367e-06,
        1.5743e-06, 3.9646e-06, 1.9708e-06, 5.7182e-06, 9.5731e-07, 1.9296e-06,
        1.9708e-06, 3.1493e-06, 2.2051e-06, 7.4662e-06, 1.9296e-06, 4.0446e-06,
        2.0564e-06, 1.5936e-06, 3.4483e-06, 1.7597e-06, 1.8549e-01, 3.8620e-06,
        4.1409e-06, 3.5559e-06, 3.0131e-06, 1.5461e-06, 9.0823e-06, 2.7619e-06,
        2.1009e-06, 1.1729e-05, 3.2785e-06, 9.0367e-06, 1.3196e-06, 2.8580e-06,
        4.5304e-06, 2.4924e-06, 2.0360e-06, 2.0360e-06, 2.0730e-06, 3.4176e-06,
        1.5920e-06, 7.4662e-06, 6.2393e-06, 2.5796e-06, 3.0877e-06, 4.6477e-06,
        1.9296e-06, 3.7775e-06, 2.3435e-06, 4.9480e-06, 4.7411e-06, 2.0360e-06,
        3.7053e-06, 9.9175e-07, 2.0360e-06, 3.0877e-06, 4.0047e-06, 6.7618e-06,
        4.5457e-06, 2.8793e-06, 3.7029e-06, 9.5732e-07, 6.2397e-06, 1.4789e-06,
        3.3013e-06, 3.7029e-06, 2.8125e-06, 3.0361e-06, 4.5740e-06, 8.4311e-06,
        3.4613e-06, 2.0360e-06, 9.0367e-06, 3.1748e-06, 1.9296e-06, 4.7411e-06,
        3.2776e-06, 5.3147e-06, 6.6439e-06, 3.6951e-06, 5.3585e-06, 7.4662e-06,
        4.4184e-06, 4.5111e-06, 3.8504e-06, 3.3013e-06, 1.7824e-06, 3.7393e-06,
        2.2051e-06, 2.0564e-06, 3.5813e-06, 7.0673e-06, 5.1013e-06, 5.3118e-06,
        2.0762e-06, 2.9192e-06, 2.0145e-06, 5.9812e-06, 2.9192e-06, 5.3585e-06,
        5.4167e-06, 2.0360e-06, 9.9175e-07, 5.9812e-06, 7.1100e-06, 3.1105e-06,
        2.2367e-06, 3.4204e-06, 2.0143e-07, 2.7401e-06, 5.8220e-06, 4.8277e-06,
        2.2198e-06, 5.1233e-06, 3.1105e-06, 6.8143e-06, 1.9296e-06, 4.8848e-06,
        3.6951e-06, 2.0360e-06, 5.3970e-06, 5.9717e-07, 1.8692e-06, 1.4747e-06,
        3.0131e-06, 4.6017e-06, 4.3293e-06, 2.4924e-06, 9.9175e-07, 3.4204e-06,
        1.9296e-06, 4.4988e-06, 3.2611e-06, 3.3279e-06, 5.1014e-06, 1.9296e-06,
        5.9812e-06, 3.3013e-06, 3.6951e-06, 3.3453e-06, 8.8940e-07, 3.8031e-06,
        2.0564e-06, 5.0666e-07, 2.8460e-06, 3.2611e-06, 5.1230e-06, 3.5490e-06,
        2.0564e-06, 3.4204e-06, 7.4662e-06, 9.5732e-07, 3.5621e-06, 1.9296e-06,
        1.4789e-06, 2.8548e-01, 4.1040e-06, 9.0367e-06, 6.3984e-06, 8.1563e-06,
        9.0823e-06, 1.8021e-06, 3.9646e-06, 3.7029e-06, 1.2514e-06, 1.9626e-06,
        2.3850e-06, 5.1014e-06, 3.7029e-06, 2.0614e-06, 1.2753e-06, 2.7460e-06,
        2.5796e-06, 2.9312e-06, 3.2227e-06, 1.9626e-06, 3.7393e-06, 3.1105e-06,
        2.6139e-06, 3.7635e-06, 5.0435e-06, 6.2393e-06, 1.9166e-06, 5.2497e-06,
        3.8468e-06, 1.1627e-06, 2.0360e-06, 1.6628e-06, 3.3013e-06, 5.3941e-06,
        2.4748e-06, 2.3435e-06, 9.8003e-07, 5.1233e-06, 1.9296e-06, 1.8692e-06,
        5.9812e-06, 3.4467e-06, 2.7619e-06, 3.1105e-06, 3.4204e-06, 3.7393e-06,
        5.8220e-06, 2.2122e-06, 2.3621e-06, 3.1105e-06, 7.4662e-06, 1.5523e-06,
        3.8227e-06, 3.7635e-06, 2.4923e-06, 3.2611e-06, 6.7618e-06, 2.2051e-06,
        1.1963e-06, 5.3118e-06, 3.5621e-06, 5.6366e-06, 3.5621e-06, 1.1627e-06,
        9.6307e-06, 1.9708e-06, 3.4382e-06, 1.8656e-06, 1.5935e-06, 3.2776e-06,
        3.7180e-06, 7.4681e-06, 1.0804e-06, 1.2807e-06, 5.3941e-06, 1.3196e-06,
        1.4492e-06, 3.6951e-06, 2.4924e-06, 2.4200e-06, 6.9008e-06, 1.2788e-05,
        1.7597e-06, 3.8031e-06, 1.6470e-06, 1.9516e-06, 3.8410e-06, 1.4766e-06,
        3.1843e-06, 2.7619e-06, 4.1394e-06, 4.5304e-06, 9.5732e-07, 3.5621e-06,
        2.4923e-06, 1.5133e-06, 5.1230e-06, 2.0327e-06, 2.0564e-06, 2.6778e-06,
        3.1105e-06, 2.2367e-06, 2.5547e-06, 1.1963e-06, 9.6559e-07, 7.1428e-06,
        3.7029e-06, 1.9296e-06, 2.6986e-06, 3.8031e-06, 1.4877e-06, 3.1813e-06,
        5.3585e-06, 2.2041e-06, 1.9296e-06, 5.9758e-07, 1.9296e-06, 9.4900e-07,
        3.6951e-06, 3.3465e-06, 1.9296e-06, 5.0737e-06, 3.0361e-06, 1.2267e-06,
        2.7833e-06, 2.7401e-06, 1.0804e-06, 1.9296e-06, 4.1413e-06, 1.7071e-06,
        5.6366e-06, 2.5796e-06, 4.3948e-06, 2.7619e-06, 1.8611e-06, 3.1105e-06,
        4.7411e-06, 1.3096e-06, 3.8227e-06, 2.6984e-06, 2.0564e-06, 2.9555e-06,
        1.9296e-06, 6.7148e-06, 3.2148e-06, 1.7845e-06, 3.6951e-06, 3.1748e-06,
        3.7193e-06, 8.3168e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.7683e-06, 1.9743e-05, 7.0925e-06, 9.7683e-06, 1.4202e-05, 1.9751e-05,
        1.3480e-05, 1.9137e-05, 2.6875e-05, 2.2224e-05, 1.4993e-05, 1.9025e-05,
        9.9342e-06, 1.0095e-05, 1.1685e-05, 5.2016e-06, 9.7683e-06, 1.0382e-05,
        1.4822e-05, 9.5265e-06, 1.6037e-05, 1.5063e-05, 9.3216e-06, 1.5902e-05,
        1.7727e-05, 4.8128e-06, 1.0946e-05, 1.0818e-05, 1.2540e-05, 5.7371e-06,
        1.4193e-05, 4.7676e-06, 1.4020e-05, 6.6452e-06, 1.6052e-06, 1.9423e-05,
        4.6100e-06, 4.7259e-06, 2.4638e-05, 5.0907e-06, 1.9206e-05, 2.0367e-05,
        1.6250e-05, 1.0358e-01, 1.0393e-05, 1.3203e-05, 1.0687e-05, 7.1851e-06,
        6.2142e-06, 1.3920e-05, 2.1113e-05, 1.6096e-05, 1.3224e-05, 9.2793e-06,
        9.0783e-06, 5.6227e-05, 1.1413e-05, 1.1352e-05, 1.0521e-05, 7.9093e-06,
        8.7626e-06, 9.7683e-06, 9.6700e-02, 2.0473e-05, 5.7371e-06, 7.7065e-06,
        7.6009e-06, 8.8541e-06, 1.1391e-05, 1.4691e-05, 2.0342e-05, 1.1705e-05,
        2.4730e-05, 1.6078e-05, 2.7234e-05, 2.3571e-05, 6.8932e-06, 7.8413e-06,
        1.0416e-05, 9.8107e-06, 2.3779e-05, 2.3266e-05, 1.3212e-05, 1.1077e-05,
        1.5715e-05, 4.7394e-06, 1.8362e-05, 2.3202e-05, 1.6163e-05, 2.0504e-05,
        9.7683e-06, 1.6173e-05, 2.2977e-05, 6.8702e-06, 1.5342e-05, 6.3616e-06,
        1.1741e-05, 1.3495e-05, 1.0818e-05, 8.2928e-06, 9.9677e-02, 8.3214e-06,
        6.0888e-06, 1.2590e-05, 9.6791e-06, 1.7182e-05, 6.6616e-06, 1.7769e-05,
        1.1284e-05, 1.0687e-05, 1.6854e-05, 7.7486e-06, 9.8707e-06, 1.2826e-05,
        5.0907e-06, 6.3674e-06, 7.7607e-06, 9.2793e-06, 1.5998e-05, 9.9753e-06,
        4.7284e-06, 7.8413e-06, 9.8854e-06, 4.9328e-06, 6.6616e-06, 1.5435e-05,
        1.0637e-05, 9.9753e-06, 9.0859e-06, 4.9749e-06, 4.1343e-06, 1.1707e-05,
        1.4148e-05, 5.0907e-06, 1.2166e-05, 2.1455e-05, 5.8391e-06, 1.3711e-05,
        2.0398e-05, 1.3254e-05, 1.1164e-05, 1.4822e-05, 1.2943e-05, 1.5675e-05,
        8.6635e-06, 8.3517e-06, 1.0382e-05, 2.1708e-05, 2.0686e-05, 1.5435e-05,
        5.0907e-06, 1.5215e-05, 8.3540e-06, 1.7756e-05, 4.2756e-06, 3.9790e-06,
        7.8165e-06, 7.6497e-06, 4.2756e-06, 1.0634e-05, 1.1196e-05, 3.4719e-06,
        9.6481e-06, 4.7394e-06, 1.5248e-05, 1.7500e-05, 1.6156e-05, 1.4086e-05,
        1.4086e-05, 1.6854e-05, 9.1856e-06, 1.0532e-05, 1.1183e-05, 1.4086e-05,
        1.0672e-05, 1.5163e-05, 1.4129e-05, 3.9886e-06, 5.2269e-06, 4.9749e-06,
        6.6616e-06, 1.3593e-05, 1.0829e-05, 1.9775e-05, 1.2287e-05, 7.8165e-06,
        9.2020e-06, 1.4636e-05, 3.7220e-06, 7.3008e-06, 2.3779e-05, 5.3101e-06,
        8.2334e-06, 6.6218e-02, 7.1464e-06, 2.2552e-05, 1.1891e-05, 2.7225e-05,
        1.9205e-05, 1.0963e-05, 2.3779e-05, 7.8843e-06, 2.0431e-05, 7.3305e-06,
        8.5838e-06, 9.0898e-06, 3.5326e-06, 1.0934e-05, 1.0481e-05, 8.8500e-06,
        1.0372e-05, 1.7647e-05, 1.2654e-05, 5.3230e-06, 5.4892e-06, 1.7365e-05,
        1.0762e-05, 4.3960e-06, 1.1676e-05, 1.4218e-05, 1.5376e-05, 1.0101e-05,
        1.3593e-05, 6.3168e-06, 7.7998e-06, 1.2545e-05, 2.0184e-05, 1.3469e-05,
        8.2406e-06, 1.3612e-05, 1.1324e-05, 2.1686e-05, 2.1534e-05, 1.4671e-05,
        1.9641e-05, 7.4571e-06, 2.3202e-05, 1.6116e-05, 2.8581e-05, 1.6581e-05,
        1.1705e-05, 3.7692e-06, 1.1951e-05, 1.1618e-05, 1.4073e-05, 1.5512e-05,
        9.2365e-06, 9.3861e-06, 1.3121e-05, 8.8541e-06, 1.4202e-05, 4.9328e-06,
        1.9861e-05, 1.6508e-05, 7.3109e-07, 9.3355e-06, 1.4086e-05, 1.3415e-05,
        2.1222e-05, 2.0667e-05, 1.5533e-05, 1.2518e-05, 1.1913e-05, 1.0946e-05,
        1.5373e-05, 1.2545e-05, 6.1457e-06, 2.2173e-05, 4.6831e-06, 1.2444e-05,
        7.5611e-06, 2.3202e-05, 1.1266e-05, 1.0672e-05, 7.9405e-06, 8.3503e-06,
        9.8107e-06, 1.9205e-05, 9.3299e-06, 1.4086e-05, 6.9085e-06, 1.9565e-05,
        1.3203e-05, 7.8165e-06, 1.2943e-05, 1.5435e-05, 5.0340e-06, 1.5591e-05,
        1.2462e-05, 1.7832e-05, 1.7365e-05, 7.4571e-06, 1.4160e-05, 1.2462e-05,
        8.5186e-06, 7.7998e-06, 2.1470e-05, 1.8137e-05, 7.8165e-06, 1.7769e-05,
        1.3193e-05, 4.4659e-06, 1.0973e-05, 1.6670e-05, 1.2275e-05, 9.3861e-06,
        5.0907e-06, 1.3804e-05, 1.0818e-05, 1.4707e-05, 7.1737e-06, 4.1761e-06,
        1.3480e-05, 1.6163e-05, 2.1113e-05, 2.4151e-05, 1.8949e-05, 9.5265e-06,
        5.0535e-06, 1.9281e-05, 1.2866e-05, 1.7802e-05, 2.0335e-05, 1.0529e-05,
        1.3822e-05, 2.3620e-05, 1.0818e-05, 1.5290e-05, 9.8224e-06, 1.2316e-01,
        1.1196e-05, 5.7303e-06, 9.7073e-06, 9.8854e-06, 1.9626e-05, 1.8157e-05,
        1.1705e-05, 1.7378e-05, 7.1592e-06, 4.7635e-06, 1.4418e-05, 7.1312e-06,
        2.9813e-06, 8.7627e-06, 2.3631e-05, 1.3298e-05, 1.6762e-05, 1.2844e-05,
        1.5435e-05, 1.8362e-05, 5.5965e-06, 1.2554e-05, 2.0664e-05, 1.7170e-05,
        5.8581e-06, 1.1891e-05, 2.6928e-05, 1.3683e-05, 2.0764e-05, 2.3266e-05,
        7.1047e-02, 1.8157e-05, 8.1601e-06, 1.1070e-05, 1.0435e-01, 3.7601e-06,
        1.4244e-05, 1.4409e-05, 2.0116e-05, 2.3739e-05, 1.6854e-05, 2.3266e-05,
        6.6616e-06, 3.1478e-06, 1.2010e-05, 1.3593e-05, 9.5797e-06, 1.3398e-05,
        7.5056e-06, 1.1228e-05, 2.0764e-05, 8.1537e-06, 7.7227e-06, 1.9205e-05,
        9.2793e-06, 1.4086e-05, 2.9129e-05, 1.1339e-05, 1.1883e-05, 1.7779e-05,
        6.1936e-06, 2.5207e-05, 1.5347e-05, 1.5453e-05, 1.0519e-05, 6.1048e-06,
        9.1856e-06, 9.4118e-06, 1.3870e-05, 1.7312e-05, 1.8128e-05, 6.1191e-06,
        4.8128e-06, 4.6409e-06, 2.0926e-05, 7.1920e-06, 3.9402e-06, 8.5274e-06,
        1.2795e-05, 1.0818e-05, 1.3920e-05, 6.7842e-06, 6.7640e-06, 9.2365e-06,
        1.6037e-05, 2.4077e-05, 1.1705e-05, 4.9317e-06, 1.3495e-05, 1.2462e-05,
        3.6192e-06, 5.0907e-06, 9.2685e-06, 8.9231e-06, 1.5435e-05, 1.4308e-05,
        1.3804e-05, 1.3091e-01, 2.2422e-05, 8.6392e-06, 1.3804e-05, 2.5996e-05,
        1.2448e-05, 1.9502e-05, 1.3533e-05, 2.4960e-05, 1.0818e-05, 1.7136e-05,
        1.5580e-06, 7.2309e-06, 9.2365e-06, 4.5581e-06, 8.4410e-06, 6.1191e-06,
        1.2866e-05, 1.4808e-05, 1.1391e-05, 1.1675e-05, 1.5960e-05, 5.6563e-06,
        1.1066e-05, 2.7701e-06, 1.9508e-05, 2.1222e-05, 1.4636e-05, 6.1191e-06,
        1.5001e-05, 1.5376e-05, 1.3480e-05, 2.4027e-05, 9.2793e-06, 9.6481e-06,
        2.3649e-05, 6.3631e-06, 3.5671e-06, 7.8772e-06, 1.9464e-05, 9.7002e-06,
        1.1707e-05, 1.0532e-05, 1.6882e-05, 6.3615e-06, 1.4240e-05, 7.8165e-06,
        7.7310e-06, 5.5463e-06, 2.0184e-05, 8.9249e-06, 1.5285e-05, 2.0473e-05,
        1.1947e-05, 4.5766e-06, 7.5533e-06, 2.5790e-05, 8.3517e-06, 4.2756e-06,
        5.9472e-06, 1.3920e-05, 1.3920e-05, 1.0687e-05, 4.4323e-06, 1.0399e-05,
        1.8880e-05, 7.7607e-06, 5.9137e-06, 1.1966e-05, 8.9014e-06, 1.3771e-05,
        5.0907e-06, 1.0672e-05, 7.8030e-06, 1.0333e-05, 2.3202e-05, 1.1996e-05,
        7.8772e-06, 2.3571e-05, 1.7802e-05, 1.0393e-05, 3.0327e-05, 1.2565e-05,
        2.5137e-05, 1.1505e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.6667e-06, 2.2225e-06, 2.0820e-06,  ..., 1.0480e-06, 2.6649e-06,
        5.4524e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.4836e-06, 3.4836e-06, 4.0290e-06, 8.7680e-06, 4.8685e-06, 2.1670e-06,
        2.5243e-06, 4.5453e-06, 4.5731e-06, 2.5454e-06, 8.2692e-07, 2.3423e-06,
        8.5378e-06, 7.1795e-06, 4.6773e-06, 7.8404e-06, 2.5454e-06, 4.3210e-06,
        6.2339e-06, 5.2409e-06, 7.7197e-06, 6.7561e-06, 4.3038e-06, 8.9239e-06,
        3.3102e-06, 4.5798e-06, 1.6477e-06, 3.7126e-06, 4.1016e-06, 4.0913e-06,
        2.9703e-06, 1.7095e-06, 4.2941e-06, 6.6893e-06, 4.3876e-06, 8.2164e-06,
        5.3426e-06, 4.3484e-06, 4.0603e-06, 1.1528e-06, 7.2484e-06, 6.7542e-06,
        6.8122e-06, 4.3484e-06, 1.0052e-05, 3.9710e-06, 1.4388e-06, 3.5580e-06,
        4.9774e-06, 4.7321e-06, 3.3101e-06, 4.5453e-06, 3.7670e-06, 4.5453e-06,
        9.4381e-06, 4.8475e-06, 7.1795e-06, 4.6828e-06, 2.9962e-06, 3.0211e-06,
        3.1928e-06, 3.5662e-06, 1.8280e-06, 8.2446e-06, 7.8132e-06, 3.5404e-06,
        2.3273e-06, 6.2340e-06, 3.3314e-06, 3.4016e-06, 4.1085e-06, 5.3602e-07,
        5.5182e-06, 8.6367e-06, 5.1585e-06, 2.6457e-06, 5.5455e-06, 2.1906e-06,
        4.2097e-06, 9.0865e-07, 3.3546e-07, 4.9600e-06, 3.9208e-06, 6.4479e-06,
        4.8685e-06, 1.0197e-05, 5.2005e-06, 7.0414e-07, 2.5494e-06, 2.9096e-06,
        8.0380e-06, 4.0622e-06, 3.2402e-06, 4.0570e-06, 1.4789e-06, 1.3127e-06,
        4.5453e-06, 2.5494e-06, 9.2254e-06, 1.8002e-06, 3.7341e-06, 7.1377e-06,
        6.7607e-06, 3.1959e-06, 2.2403e-06, 4.4887e-06, 5.0688e-06, 3.4836e-06,
        4.1993e-06, 4.8685e-06, 7.0414e-07, 7.5778e-06, 8.1868e-07, 1.8285e-06,
        2.7627e-06, 4.4077e-06, 2.4662e-06, 7.1275e-06, 3.6397e-06, 1.8102e-06,
        2.0646e-06, 2.2734e-06, 6.2703e-06, 4.4575e-06, 7.3075e-06, 6.7669e-06,
        5.9482e-06, 9.3208e-06, 2.9233e-06, 1.9463e-06, 7.1795e-06, 3.1971e-06,
        5.0710e-06, 4.1539e-06, 4.5453e-06, 7.2883e-06, 3.4247e-06, 5.4943e-06,
        4.9600e-06, 4.4887e-06, 1.7095e-06, 1.5842e-06, 2.4289e-06, 4.4887e-06,
        8.0720e-06, 1.1071e-05, 1.7095e-06, 1.8896e-06, 1.3174e-06, 2.3712e-06,
        3.5335e-06, 6.5843e-06, 5.1629e-06, 4.6398e-06, 5.1150e-06, 4.8939e-06,
        2.0860e-06, 7.8035e-06, 2.7417e-06, 5.9759e-06, 7.7197e-06, 3.0656e-06,
        2.4103e-06, 6.2355e-06, 3.1281e-06, 3.5699e-06, 3.7698e-06, 2.6457e-06,
        4.7268e-06, 7.3023e-06, 8.4375e-06, 6.5768e-06, 2.5494e-06, 4.0622e-06,
        4.1159e-06, 4.9927e-06, 7.2332e-06, 3.1362e-06, 4.5453e-06, 7.0944e-06,
        3.5654e-06, 1.2174e-05, 4.1688e-06, 1.5051e-06, 8.1296e-07, 9.2254e-06,
        4.0234e-06, 1.0116e-05, 3.8930e-06, 4.3571e-06, 1.4614e-06, 4.0290e-06,
        6.8573e-06, 4.2941e-06, 2.4848e-06, 7.7795e-06, 2.4848e-06, 3.1929e-06,
        1.7095e-06, 6.6450e-06, 6.7224e-06, 6.1885e-06, 8.8560e-06, 4.5252e-06,
        6.8692e-06, 2.1606e-06, 1.6874e-06, 5.5920e-06, 3.7126e-06, 8.8308e-06,
        7.2283e-06, 5.7725e-06, 4.2117e-06, 2.1390e-06, 6.0021e-06, 5.4966e-06,
        2.2106e-06, 3.7452e-06, 5.2409e-06, 3.4836e-06, 1.1152e-05, 5.6920e-06,
        9.6046e-06, 5.9831e-06, 2.1389e-06, 6.5033e-06, 6.0147e-06, 4.6648e-06,
        2.1683e-06, 1.7420e-06, 7.2332e-06, 7.2772e-06, 6.6763e-06, 6.0355e-06,
        6.3100e-06, 4.4784e-06, 3.5763e-06, 4.9529e-06, 6.1266e-06, 5.0688e-06,
        6.8880e-06, 3.2852e-06, 3.4836e-06, 6.4240e-06, 3.5439e-06, 4.7829e-06,
        4.3571e-06, 2.3783e-06, 3.4647e-06, 2.7238e-06, 3.7062e-06, 6.2355e-06,
        2.9630e-06, 1.6489e-06, 2.4289e-06, 3.9710e-06, 6.7646e-06, 1.6389e-06,
        2.6752e-06, 8.0788e-06, 1.6736e-06, 5.6776e-06, 1.0499e-05, 6.7893e-06,
        1.0581e-05, 8.2131e-06, 7.1275e-06, 9.6097e-06, 3.8930e-06, 2.4911e-06,
        3.4128e-06, 6.4255e-06, 2.7664e-06, 6.2340e-06, 1.0351e-05, 3.5674e-06,
        2.7972e-06, 2.8542e-06, 1.3319e-05, 6.8477e-06, 5.8982e-06, 5.4649e-06,
        6.1053e-06, 8.3544e-06, 4.6366e-06, 3.3391e-06, 2.2802e-06, 2.4720e-06,
        4.1085e-06, 4.5448e-06, 6.0690e-06, 3.1959e-06, 6.2339e-06, 3.1959e-06,
        5.0439e-06, 4.5410e-06, 8.8308e-06, 3.0211e-06, 4.5515e-06, 6.1854e-06,
        6.2810e-06, 8.2344e-06, 2.7932e-06, 3.3556e-06, 4.0240e-06, 2.7932e-06,
        6.5033e-06, 9.1007e-06, 5.6105e-06, 4.9774e-06, 4.6366e-06, 6.7930e-06,
        2.3089e-06, 7.2371e-06, 9.5950e-07, 4.4887e-06, 5.6793e-06, 4.4878e-06,
        4.2941e-06, 1.0311e-05, 5.8982e-06, 1.0305e-05, 5.5903e-06, 1.5842e-06,
        6.1227e-06, 4.1432e-06, 4.5715e-06, 5.6217e-06, 6.3572e-06, 2.3345e-06,
        5.9880e-06, 6.0414e-06, 2.4848e-06, 7.8794e-06, 4.3484e-06, 1.1506e-06,
        6.0923e-06, 4.7038e-06, 8.3897e-06, 1.7866e-06, 3.9551e-06, 6.4573e-06,
        2.8542e-06, 1.0311e-05, 1.6533e-06, 6.1854e-06, 4.7466e-06, 6.5768e-06,
        5.5418e-06, 6.9230e-06, 9.6097e-06, 6.5768e-06, 1.7095e-06, 7.7795e-06,
        6.0008e-06, 2.4103e-06, 5.1655e-06, 9.5012e-06, 3.7400e-06, 1.3127e-06,
        8.3062e-06, 4.5731e-06, 1.9500e-06, 4.9966e-06, 1.8536e-06, 4.0240e-06,
        2.9891e-06, 2.6962e-06, 2.2403e-06, 4.3651e-06, 7.2776e-06, 3.4836e-06,
        6.0008e-06, 3.9796e-06, 1.1868e-05, 1.1803e-06, 1.0052e-05, 4.7218e-06,
        5.3745e-06, 2.0860e-06, 9.5645e-06, 1.5691e-06, 6.7893e-06, 4.3405e-06,
        3.8021e-06, 3.6562e-06, 2.6457e-06, 6.5645e-06, 3.3866e-06, 2.3089e-06,
        1.8280e-06, 5.4674e-06, 6.3100e-06, 1.0311e-05, 5.5887e-06, 3.7670e-06,
        2.6251e-06, 1.1154e-05, 9.5440e-06, 1.0305e-05, 1.1154e-05, 4.9966e-06,
        9.6183e-06, 6.2355e-06, 5.2482e-06, 1.6589e-06, 2.0583e-06, 1.3319e-05,
        2.7300e-06, 2.6533e-06, 7.0944e-06, 3.7552e-06, 3.7381e-06, 2.1191e-06,
        4.9529e-06, 4.8685e-06, 8.4375e-06, 4.7480e-06, 8.6725e-06, 4.6624e-06,
        5.1798e-06, 3.5580e-06, 4.9213e-06, 5.7235e-06, 7.9824e-06, 3.4836e-06,
        5.5531e-06, 4.3361e-07, 2.8542e-06, 5.5918e-06, 2.3746e-06, 2.4720e-06,
        3.7126e-06, 7.9309e-06, 7.8012e-06, 4.8475e-06, 4.1380e-06, 2.1606e-06,
        2.4720e-06, 2.2403e-06, 4.5453e-06, 3.6904e-06, 5.7350e-06, 2.7417e-06,
        3.6904e-06, 1.7228e-06, 7.9360e-06, 6.8098e-06, 4.5691e-06, 1.0277e-05,
        7.1149e-06, 4.5453e-06, 4.3876e-06, 7.6646e-06, 4.9240e-06, 7.7869e-06,
        2.0646e-06, 6.3100e-06, 1.0305e-05, 1.0751e-05, 5.6242e-06, 1.4296e-06,
        6.2703e-06, 4.1041e-06, 7.1690e-06, 5.1585e-06, 6.7737e-06, 7.8123e-06,
        5.1629e-06, 9.4186e-06, 4.8685e-06, 2.0860e-06, 1.5842e-06, 3.7958e-06,
        6.3253e-06, 6.2340e-06, 3.1959e-06, 1.2319e-05, 4.8541e-06, 3.2643e-06,
        4.0744e-06, 5.8446e-06, 7.6646e-06, 6.2810e-06, 6.0008e-06, 2.6788e-06,
        7.0066e-06, 4.0290e-06, 2.0887e-06, 4.7545e-06, 4.0229e-06, 7.5895e-06,
        1.2174e-05, 4.3133e-06, 2.7238e-06, 5.9880e-06, 5.6242e-06, 5.6545e-06,
        4.6750e-06, 1.6601e-06, 2.2269e-06, 5.3560e-06, 3.4016e-06, 2.9086e-06,
        2.5454e-06, 4.0229e-06, 3.4659e-06, 3.4836e-06, 4.0154e-06, 2.5445e-06,
        4.0290e-06, 2.4720e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.7970e-06, 9.8192e-06, 4.7548e-06, 1.0480e-05, 8.7242e-06, 1.2413e-05,
        2.0152e-05, 1.4159e-05, 1.1657e-05, 1.0453e-05, 1.0822e-05, 4.8138e-06,
        1.2277e-05, 1.1632e-05, 1.5049e-05, 5.8754e-06, 4.2383e-06, 4.8759e-06,
        9.8178e-06, 1.2016e-05, 6.6316e-06, 1.1679e-05, 8.0407e-06, 4.9922e-06,
        1.6983e-05, 2.5838e-05, 1.2996e-05, 1.5752e-05, 1.5176e-05, 1.6757e-05,
        1.2876e-05, 6.9773e-06, 1.9032e-05, 1.2101e-05, 6.2981e-06, 1.0452e-05,
        1.5049e-05, 1.0452e-05, 5.1250e-06, 1.0544e-05, 1.3082e-05, 1.6913e-05,
        9.0433e-06, 1.7079e-05, 4.6339e-06, 6.4018e-06, 8.9642e-06, 1.1743e-05,
        4.2383e-06, 1.7962e-05, 1.0044e-05, 4.5496e-06, 1.6412e-05, 1.6757e-05,
        1.1018e-05, 1.5914e-05, 5.3558e-06, 1.6316e-05, 1.4445e-05, 2.1442e-05,
        1.4607e-05, 9.8020e-06, 6.4018e-06, 3.4699e-06, 1.0016e-05, 5.1706e-06,
        1.2165e-05, 9.3711e-06, 1.3360e-05, 1.7890e-05, 9.3450e-06, 9.5854e-06,
        8.9642e-06, 1.4964e-05, 1.2457e-05, 1.0924e-05, 6.9645e-06, 7.0343e-06,
        5.1706e-06, 4.4147e-06, 9.0433e-06, 1.8274e-05, 9.5561e-06, 5.6011e-06,
        1.1892e-05, 4.8982e-06, 5.9866e-06, 1.6554e-05, 1.8910e-05, 1.1513e-05,
        1.2708e-05, 8.4369e-06, 1.6825e-05, 5.8214e-06, 5.4980e-06, 1.0503e-05,
        3.1876e-06, 1.0924e-05, 1.0480e-05, 5.9496e-06, 1.0452e-05, 6.4105e-06,
        1.6672e-05, 2.4049e-05, 1.2842e-05, 1.5204e-05, 1.5570e-05, 7.9417e-06,
        1.0483e-05, 5.0450e-06, 1.7798e-06, 2.1442e-05, 7.3324e-06, 7.0344e-06,
        1.7334e-05, 1.3796e-05, 2.4475e-05, 3.1120e-05, 1.5614e-05, 1.7624e-05,
        3.4699e-06, 1.0458e-05, 5.5575e-06, 1.6412e-05, 1.6161e-05, 5.3891e-06,
        1.4098e-05, 1.3360e-05, 2.0604e-05, 1.1495e-05, 2.3402e-06, 2.6828e-05,
        2.0152e-05, 6.3700e-06, 9.2666e-06, 1.2400e-05, 5.5057e-06, 5.8485e-06,
        1.5471e-05, 6.6108e-06, 8.9642e-06, 1.4729e-05, 7.8300e-06, 4.3325e-06,
        3.0745e-06, 1.9032e-05, 2.5357e-05, 3.4699e-06, 7.5574e-06, 9.3753e-06,
        6.3592e-06, 9.5382e-06, 2.3402e-06, 9.2583e-06, 7.1551e-06, 9.4037e-06,
        6.8604e-06, 3.1701e-05, 1.9166e-05, 1.0452e-05, 6.4018e-06, 9.2583e-06,
        1.5614e-05, 9.0524e-06, 8.2487e-06, 1.0670e-05, 1.0670e-05, 6.3522e-06,
        1.5798e-05, 9.2873e-06, 1.1892e-05, 2.0604e-05, 1.2648e-05, 1.2376e-05,
        1.0670e-05, 2.6026e-05, 7.4614e-06, 8.7648e-06, 1.8180e-05, 1.5001e-05,
        2.8322e-05, 8.7242e-06, 1.5324e-05, 7.4220e-06, 1.0261e-05, 6.1748e-06,
        6.1748e-06, 8.4369e-06, 1.2277e-05, 1.0832e-05, 5.2205e-06, 1.5543e-05,
        6.1748e-06, 2.0522e-05, 7.5977e-06, 1.4276e-05, 8.2487e-06, 1.0146e-05,
        9.0433e-06, 3.0808e-05, 5.7099e-06, 1.2708e-05, 2.4049e-05, 4.5496e-06,
        1.0483e-05, 9.8273e-06, 9.8020e-06, 5.5200e-06, 1.1743e-05, 2.4735e-05,
        1.5176e-05, 4.6339e-06, 2.7328e-06, 7.9393e-06, 8.3052e-06, 1.2910e-05,
        6.6607e-06, 7.0846e-06, 1.0625e-05, 1.2048e-05, 5.8214e-06, 8.9642e-06,
        1.3467e-05, 1.1459e-05, 4.2990e-06, 9.4259e-06, 1.4435e-05, 1.0016e-05,
        9.8115e-06, 1.5570e-05, 9.7884e-06, 2.1442e-05, 1.0030e-05, 1.0968e-05,
        4.5496e-06, 1.2678e-05, 1.1015e-05, 1.1472e-05, 1.3082e-05, 1.6485e-06,
        1.7450e-05, 1.2678e-05, 1.2020e-05, 7.5574e-06, 1.2996e-05, 8.5194e-06,
        7.4588e-06, 6.5267e-06, 5.7494e-06, 7.6106e-06, 1.0924e-05, 2.2703e-05,
        9.6675e-06, 1.1742e-05, 9.8292e-06, 2.0152e-05, 1.4650e-05, 2.7030e-05,
        1.6534e-05, 1.4379e-05, 1.0452e-05, 1.8157e-05, 1.1015e-05, 9.0433e-06,
        1.7393e-05, 1.4114e-05, 1.0822e-05, 1.0241e-05, 4.3370e-06, 6.2290e-06,
        1.8976e-06, 6.1748e-06, 1.0924e-05, 2.0159e-05, 1.5566e-05, 5.9863e-06,
        4.9922e-06, 1.2580e-05, 9.9804e-06, 5.2374e-06, 2.4049e-05, 1.3418e-05,
        1.5570e-05, 1.3843e-05, 6.8692e-06, 2.5357e-05, 1.6412e-05, 1.5049e-05,
        1.8076e-05, 4.5221e-06, 7.2500e-06, 2.6828e-05, 7.8300e-06, 4.2933e-06,
        5.2205e-06, 4.9922e-06, 6.1748e-06, 5.1250e-06, 2.0152e-05, 9.8652e-06,
        2.6828e-05, 7.9018e-06, 1.2255e-05, 1.7624e-05, 6.4559e-06, 1.9166e-05,
        2.1046e-05, 2.4866e-05, 5.7564e-06, 1.1743e-05, 1.4828e-05, 1.0023e-05,
        1.5049e-05, 6.1046e-06, 7.0298e-06, 1.8342e-05, 9.9709e-06, 1.8208e-05,
        6.8604e-06, 9.3628e-06, 4.7241e-06, 1.2457e-05, 3.6255e-06, 6.8604e-06,
        7.4588e-06, 9.4259e-06, 2.3402e-06, 4.5444e-06, 1.2678e-05, 7.4837e-06,
        6.5151e-06, 1.0453e-05, 1.3467e-05, 1.1519e-05, 1.4076e-05, 1.5349e-05,
        1.5692e-05, 1.2403e-06, 1.2850e-05, 6.8692e-06, 1.5338e-05, 4.2990e-06,
        2.3402e-06, 5.8602e-06, 9.8652e-06, 7.4837e-06, 6.1748e-06, 1.6385e-05,
        1.0242e-05, 8.6320e-06, 2.5894e-07, 7.7192e-06, 8.2756e-06, 8.7242e-06,
        1.5614e-05, 1.2228e-05, 1.1470e-05, 1.3360e-05, 1.5768e-05, 5.4195e-06,
        3.8750e-06, 1.3360e-05, 1.1791e-05, 6.8604e-06, 1.0605e-05, 1.5543e-05,
        1.1728e-05, 2.4956e-05, 6.8604e-06, 1.2962e-05, 1.1627e-05, 8.2566e-06,
        9.4037e-06, 7.9417e-06, 2.4529e-05, 9.4259e-06, 5.8052e-06, 1.3796e-05,
        1.3034e-05, 5.9496e-06, 4.4441e-06, 1.3360e-05, 2.5357e-05, 1.2299e-05,
        2.1442e-05, 5.5153e-06, 1.0993e-05, 2.4322e-05, 2.6004e-06, 3.5597e-06,
        2.4049e-05, 8.9157e-06, 2.0152e-05, 7.4220e-06, 1.3481e-05, 6.5151e-06,
        6.1046e-06, 1.6626e-05, 1.6532e-05, 3.1701e-05, 1.0083e-05, 8.8713e-06,
        5.2253e-06, 1.9693e-05, 1.3360e-05, 8.7269e-06, 4.4157e-06, 1.2016e-05,
        1.5647e-05, 1.0605e-05, 2.0152e-05, 1.3294e-05, 1.0016e-05, 2.1848e-05,
        1.4447e-05, 2.1015e-05, 9.4259e-06, 4.5221e-06, 1.0529e-05, 3.8251e-06,
        6.1295e-06, 6.6607e-06, 8.0131e-06, 1.1892e-05, 6.8604e-06, 7.2515e-06,
        8.6661e-06, 1.4159e-05, 1.2973e-05, 6.8604e-06, 3.1204e-06, 2.6026e-05,
        8.7242e-06, 1.7450e-05, 1.4200e-05, 9.4214e-06, 1.7520e-05, 1.7536e-05,
        5.1792e-06, 1.1549e-05, 2.9780e-06, 4.5543e-06, 1.3566e-05, 3.1876e-06,
        1.5798e-05, 2.9815e-06, 1.4523e-05, 1.3207e-05, 7.4539e-06, 8.8713e-06,
        4.1406e-06, 1.0670e-05, 1.5614e-05, 1.8955e-06, 2.5357e-05, 6.2154e-06,
        1.2562e-05, 1.7893e-05, 7.6980e-06, 7.5574e-06, 1.5049e-05, 6.5209e-06,
        2.6828e-05, 1.1479e-05, 3.9852e-06, 1.8284e-05, 1.1016e-05, 5.5153e-06,
        3.3635e-06, 8.7242e-06, 3.4486e-06, 2.0604e-05, 9.3628e-06, 1.2146e-05,
        2.6026e-05, 4.7548e-06, 1.3064e-05, 7.7191e-06, 1.3034e-05, 1.8247e-05,
        8.6660e-06, 1.0453e-05, 5.3066e-06, 3.4699e-06, 2.0590e-05, 1.2377e-05,
        6.9649e-06, 8.9642e-06, 1.3301e-05, 8.7347e-06, 4.5221e-06, 7.4588e-06,
        9.5382e-06, 9.4259e-06, 2.0895e-06, 1.6604e-05, 9.0011e-06, 8.8713e-06,
        6.8604e-06, 8.7242e-06, 1.0185e-05, 1.6665e-05, 2.4049e-05, 1.2930e-05,
        1.2190e-05, 5.3354e-06, 9.2313e-06, 4.5543e-06, 5.6011e-06, 1.2266e-05,
        1.0605e-05, 9.4046e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.5102e-07, 7.4799e-07, 1.2051e-06,  ..., 4.4106e-07, 7.6661e-07,
        1.2206e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2210, 1.8333, 1.1690, 1.4468, 1.3326, 1.0458, 1.3757, 1.1975, 1.4671,
        1.9208, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218,
        0.0218], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.03703703731298447, 0.0, 1.0, 0.0, 1.0, 0.0, 0.03703703731298447, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.03703703731298447, 0.03703703731298447, 1.0, 0.03703703731298447, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.34375, 0.34375, 0.34375, 1.0, 0.359375, 0.359375, 0.359375, 0.34375, 0.359375, 0.359375, 0.375, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 1.0, 1.0, 0.421875, 0.359375, 0.359375, 0.34375, 0.34375, 0.359375, 0.359375, 0.359375, 0.359375, 0.34375, 0.34375, 1.0, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.34375, 0.34375, 0.359375, 0.390625, 0.34375, 0.375, 0.34375, 0.359375, 0.359375, 0.34375, 0.34375, 0.359375, 1.0, 0.375, 0.34375, 1.0, 0.40625, 0.34375, 0.359375, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 0.390625, 0.34375]

 sparsity of   [0.1388888955116272, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.1302083283662796, 0.1302083283662796, 0.1336805522441864, 1.0, 0.1354166716337204, 1.0, 1.0, 0.1284722238779068, 1.0, 1.0, 0.1354166716337204, 0.1354166716337204, 0.1336805522441864, 1.0, 0.1388888955116272, 1.0, 0.142361119389534, 0.1371527761220932, 0.140625, 1.0, 0.1302083283662796, 0.1319444477558136, 0.1319444477558136, 1.0, 0.1336805522441864, 1.0, 0.1336805522441864, 1.0, 0.1371527761220932, 1.0, 0.1302083283662796, 1.0, 1.0, 1.0, 1.0, 0.1527777761220932, 0.1354166716337204, 1.0, 0.1458333283662796, 1.0, 1.0, 0.1319444477558136, 1.0, 0.1319444477558136, 1.0, 1.0, 0.1354166716337204, 1.0, 1.0, 0.1527777761220932, 1.0, 0.1458333283662796, 0.1388888955116272, 0.1354166716337204, 0.1336805522441864, 0.15625, 1.0, 0.1475694477558136]

 sparsity of   [0.453125, 0.484375, 0.453125, 0.46875, 0.5, 0.46875, 0.46875, 0.4375, 1.0, 0.484375, 0.46875, 0.453125, 0.46875, 0.46875, 1.0, 0.4375, 0.4375, 0.46875, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.46875, 0.4375, 1.0, 1.0, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 0.46875, 0.453125, 0.453125, 1.0, 0.484375, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 0.453125, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.453125, 0.453125, 1.0, 0.4375, 0.46875, 0.453125, 0.484375, 0.46875, 0.453125, 0.4375, 0.46875, 1.0, 1.0, 0.484375, 0.453125, 0.4375, 0.46875, 0.46875, 1.0, 0.484375, 0.46875, 0.4375, 0.46875, 0.46875, 0.4375, 1.0, 0.4375, 0.453125, 0.46875, 0.484375, 0.46875, 1.0, 0.453125, 0.5, 0.46875, 0.453125, 0.4375, 0.4375, 1.0, 0.4375, 1.0, 0.4375, 1.0, 0.4375, 0.4375, 1.0, 0.453125, 0.46875, 0.46875, 0.46875, 0.4375, 0.4375, 0.453125, 1.0, 1.0, 0.4375, 0.4375, 0.46875, 0.453125, 0.453125, 1.0, 0.46875, 0.484375, 0.46875, 0.4375, 0.46875, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.4375, 0.46875, 1.0, 1.0, 0.46875, 0.453125, 1.0, 0.453125, 0.46875, 1.0, 0.453125, 1.0, 0.453125, 0.453125, 0.453125, 0.4375, 0.46875, 0.4375, 0.4375, 0.46875, 0.484375, 1.0, 0.4375, 0.453125, 0.4375, 1.0, 0.453125, 0.453125, 1.0, 0.46875, 0.453125, 0.46875, 1.0, 0.453125, 0.4375, 0.453125, 1.0, 0.453125, 0.453125, 1.0, 0.4375, 0.453125, 0.453125, 1.0, 0.46875, 0.46875, 1.0, 0.46875, 0.453125, 0.46875, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.453125, 0.46875, 0.453125, 0.453125, 0.4375, 0.453125, 0.46875, 0.46875, 0.453125, 0.4375, 0.46875, 0.46875, 0.46875, 0.484375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.4375, 0.46875, 0.453125, 0.453125, 1.0, 1.0, 0.46875, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.46875, 1.0, 1.0, 0.453125, 0.453125, 0.453125, 1.0, 0.453125, 0.4375, 0.46875, 0.46875, 0.4375, 0.453125, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4375, 0.4375, 0.46875, 0.46875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.453125, 1.0, 0.4375, 0.46875, 1.0, 0.4375, 0.46875, 1.0]

 sparsity of   [1.0, 0.34375, 0.390625, 0.359375, 0.34375, 0.359375, 0.375, 0.359375, 1.0, 0.375, 0.34375, 0.40625, 0.375, 0.34375, 1.0, 0.34375, 0.34375, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.515625, 1.0, 0.40625, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.34375, 1.0, 0.390625, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.390625, 0.359375, 1.0, 0.359375, 1.0, 0.375, 0.34375, 0.375, 0.34375, 0.359375, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.390625, 0.34375, 1.0, 0.34375, 0.34375, 0.359375, 0.375, 0.375, 0.390625, 1.0, 0.359375, 0.34375, 0.421875, 0.40625, 0.421875, 1.0, 0.375, 0.359375, 0.453125, 0.34375, 0.34375, 0.359375, 1.0, 0.359375, 1.0, 0.375, 1.0, 0.359375, 0.375, 1.0, 0.359375, 0.375, 0.359375, 0.34375, 0.359375, 0.375, 0.359375, 0.453125, 1.0, 0.34375, 0.359375, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.359375, 0.375, 0.359375, 0.359375, 1.0, 0.359375, 0.34375, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.34375, 0.34375, 1.0, 0.375, 0.359375, 1.0, 0.359375, 1.0, 0.359375, 0.375, 0.390625, 0.390625, 0.375, 0.34375, 0.390625, 0.34375, 0.34375, 1.0, 0.359375, 1.0, 0.359375, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.34375, 0.40625, 1.0, 0.375, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 0.375, 1.0, 0.4375, 0.375, 1.0, 0.34375, 0.359375, 0.359375, 0.34375, 0.359375, 0.34375, 0.375, 0.359375, 0.34375, 0.359375, 0.34375, 0.359375, 0.359375, 0.359375, 0.375, 0.359375, 0.359375, 0.375, 0.359375, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 0.375, 0.359375, 0.34375, 0.375, 1.0, 1.0, 0.359375, 0.34375, 0.34375, 0.34375, 0.34375, 0.359375, 0.359375, 0.34375, 0.375, 1.0, 1.0, 0.375, 0.421875, 0.359375, 1.0, 0.359375, 0.375, 0.34375, 0.359375, 0.34375, 0.34375, 1.0, 1.0, 0.40625, 0.359375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 0.375, 0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 0.34375, 0.359375, 0.359375, 1.0, 0.4375, 0.359375, 1.0, 1.0, 0.375, 0.359375]

 sparsity of   [0.25390625, 0.2421875, 0.23828125, 0.46484375, 1.0, 1.0, 0.23046875, 1.0, 0.23828125, 1.0, 0.265625, 0.25, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.234375, 1.0, 1.0, 1.0, 0.23828125, 1.0, 0.234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 0.2578125, 1.0, 0.23828125, 1.0, 1.0, 0.25390625, 1.0, 1.0, 0.2421875, 0.2265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2578125, 0.2578125, 1.0, 0.25, 0.23046875, 0.234375, 0.234375, 0.234375, 1.0, 0.25390625, 1.0, 0.25, 0.2578125, 0.25, 0.26171875, 1.0, 0.26171875, 0.25390625, 0.25]

 sparsity of   [0.4826388955116272, 1.0, 0.4791666567325592, 1.0, 1.0, 0.4809027910232544, 0.4809027910232544, 1.0, 0.487847238779068, 1.0, 0.4826388955116272, 0.4791666567325592, 0.5, 1.0, 1.0, 0.4895833432674408, 1.0, 0.487847238779068, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 0.472222238779068, 0.4861111044883728, 0.4826388955116272, 0.4861111044883728, 1.0, 0.484375, 1.0, 0.4826388955116272, 1.0, 0.4895833432674408, 1.0, 0.4791666567325592, 1.0, 0.4947916567325592, 1.0, 1.0, 0.4861111044883728, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4895833432674408, 0.484375, 1.0, 1.0, 0.4809027910232544, 1.0, 0.4826388955116272, 1.0, 0.4965277910232544, 1.0, 0.4791666567325592, 1.0, 1.0, 0.487847238779068, 0.487847238779068]

 sparsity of   [0.53125, 0.515625, 0.515625, 0.53125, 0.5, 0.53125, 0.484375, 0.515625, 0.53125, 0.53125, 0.5625, 0.53125, 0.515625, 0.5, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.546875, 0.484375, 1.0, 0.515625, 0.53125, 0.5, 1.0, 1.0, 0.484375, 0.5, 0.515625, 0.515625, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.515625, 0.484375, 0.515625, 0.5, 0.484375, 1.0, 0.5, 0.515625, 1.0, 0.5, 0.5, 0.453125, 0.5, 1.0, 0.515625, 0.515625, 0.5, 0.53125, 1.0, 0.53125, 1.0, 0.5, 1.0, 0.515625, 0.515625, 0.5, 1.0, 0.5, 1.0, 0.53125, 0.5, 0.5, 0.515625, 1.0, 0.5, 1.0, 0.515625, 0.515625, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.53125, 0.484375, 0.515625, 0.515625, 0.5, 0.484375, 0.46875, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.484375, 0.5, 1.0, 0.515625, 0.515625, 0.515625, 0.53125, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.53125, 0.5, 1.0, 0.484375, 0.53125, 0.515625, 0.5, 0.53125, 0.515625, 1.0, 0.5, 0.515625, 1.0, 0.46875, 0.46875, 1.0, 1.0, 0.46875, 0.546875, 0.515625, 1.0, 0.53125, 1.0, 1.0, 0.53125, 0.5, 0.484375, 0.484375, 0.546875, 0.5, 0.515625, 0.53125, 0.515625, 0.53125, 0.515625, 0.5, 0.5, 1.0, 1.0, 0.46875, 0.515625, 0.515625, 1.0, 0.515625, 0.515625, 0.546875, 0.484375, 0.515625, 0.53125, 1.0, 0.5, 0.484375, 0.515625, 0.484375, 0.484375, 0.515625, 1.0, 1.0, 1.0, 0.515625, 1.0, 0.546875, 0.515625, 0.515625, 0.515625, 0.515625, 0.5625, 0.484375, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.5, 0.53125, 0.515625, 0.5, 0.484375, 0.515625, 0.484375, 1.0, 0.515625, 0.515625, 0.5, 0.484375, 0.46875, 0.5, 0.515625, 1.0, 0.5, 0.5, 0.515625, 1.0, 0.53125, 0.5, 1.0, 0.53125, 0.5, 0.546875, 0.515625, 0.5, 1.0, 1.0, 0.53125, 0.5, 0.5, 0.546875, 0.5, 0.484375, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.484375, 0.53125, 0.5, 0.53125, 0.515625, 0.515625, 0.484375, 0.515625, 0.515625, 1.0, 1.0, 0.5625, 1.0, 1.0, 0.484375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 0.46875, 0.5, 0.515625, 0.5, 1.0, 1.0, 1.0, 0.515625, 0.515625, 0.5, 0.484375, 0.53125, 1.0, 0.5, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.1171875, 1.0, 1.0, 0.11328125, 1.0, 0.10546875, 0.13671875, 0.14453125, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.10546875, 0.10546875, 1.0, 0.12890625, 1.0, 1.0, 0.1015625, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 1.0, 0.140625, 0.13671875, 0.1171875, 0.12890625, 0.12890625, 1.0, 0.13671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.109375, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.11328125, 0.12890625, 1.0, 0.12890625, 0.1015625, 0.1171875, 1.0, 0.15234375, 0.10546875, 0.109375, 0.12890625, 0.109375]

 sparsity of   [1.0, 0.5121527910232544, 1.0, 1.0, 0.5121527910232544, 0.5121527910232544, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.5121527910232544, 0.5208333134651184, 1.0, 0.5104166865348816, 1.0, 0.5, 1.0, 0.5121527910232544, 0.5173611044883728, 1.0, 0.5, 0.6579861044883728, 0.5034722089767456, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 0.5190972089767456, 1.0, 0.5086805820465088, 0.5, 0.5173611044883728, 1.0, 1.0, 1.0, 0.5121527910232544, 0.5034722089767456, 0.5138888955116272, 1.0, 0.5052083134651184, 0.5104166865348816, 0.515625, 1.0, 0.5086805820465088, 0.5034722089767456, 0.5208333134651184, 0.5052083134651184, 1.0, 0.5017361044883728, 0.5052083134651184, 1.0, 1.0, 0.5017361044883728, 0.5277777910232544, 1.0, 0.4982638955116272, 1.0, 0.5190972089767456]

 sparsity of   [0.4375, 0.46875, 1.0, 1.0, 0.453125, 1.0, 0.4375, 0.4375, 0.453125, 0.484375, 0.4375, 0.4375, 0.4375, 0.453125, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 1.0, 1.0, 1.0, 0.453125, 0.4375, 1.0, 0.453125, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.453125, 0.4375, 0.46875, 0.4375, 0.46875, 0.453125, 0.453125, 0.46875, 1.0, 0.46875, 0.453125, 0.4375, 0.4375, 0.453125, 0.46875, 0.453125, 1.0, 0.46875, 0.4375, 0.4375, 0.453125, 0.46875, 0.453125, 0.4375, 0.484375, 0.46875, 0.453125, 0.453125, 1.0, 1.0, 0.4375, 0.4375, 0.5, 0.453125, 1.0, 0.5, 0.453125, 0.4375, 0.4375, 0.46875, 0.4375, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 1.0, 0.453125, 0.453125, 0.453125, 0.453125, 0.46875, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 0.484375, 1.0, 0.4375, 1.0, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.453125, 1.0, 0.46875, 0.4375, 1.0, 1.0, 0.453125, 0.453125, 0.4375, 1.0, 0.453125, 1.0, 0.453125, 0.453125, 0.4375, 1.0, 0.4375, 0.453125, 0.4375, 0.453125, 0.453125, 0.453125, 0.4375, 1.0, 0.4375, 0.46875, 1.0, 0.453125, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.46875, 0.453125, 0.453125, 0.453125, 1.0, 0.4375, 0.453125, 0.46875, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.4375, 0.5, 0.4375, 0.4375, 0.4375, 0.453125, 0.46875, 1.0, 0.4375, 0.453125, 0.453125, 0.5, 0.484375, 0.4375, 0.46875, 0.4375, 0.5, 0.453125, 0.4375, 0.4375, 0.4375, 0.453125, 0.4375, 0.4375, 1.0, 0.453125, 1.0, 0.46875, 0.453125, 0.453125, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.453125, 0.484375, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.453125, 0.4375, 0.4375, 1.0, 0.46875, 1.0, 1.0, 0.453125, 0.4375, 0.484375, 1.0, 1.0, 1.0, 0.46875, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 0.453125, 0.453125, 0.453125, 0.453125, 1.0, 0.453125, 0.4375, 0.4375, 0.4375, 1.0]

 sparsity of   [0.0625, 1.0, 1.0, 1.0, 0.02734375, 0.25, 1.0, 0.0390625, 0.03515625, 0.0390625, 0.046875, 0.0234375, 1.0, 0.0234375, 0.03125, 1.0, 0.01953125, 1.0, 1.0, 0.05078125, 1.0, 0.03125, 1.0, 1.0, 1.0, 0.03515625, 0.02734375, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.0390625, 1.0, 1.0, 0.0390625, 0.04296875, 0.03515625, 1.0, 0.03125, 0.0390625, 1.0, 0.04296875, 0.04296875, 1.0, 0.04296875, 1.0, 0.03125, 0.0390625, 1.0, 1.0, 1.0, 0.0234375, 1.0, 0.03125, 1.0, 0.02734375, 1.0, 0.03125, 1.0, 1.0, 0.03515625, 0.03125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.0390625, 0.09375, 0.03515625, 1.0, 0.02734375, 1.0, 0.04296875, 0.04296875, 1.0, 1.0, 0.03125, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 0.0390625, 0.03125, 0.04296875, 0.02734375, 1.0, 1.0, 1.0, 1.0, 0.03515625, 0.03125, 1.0, 1.0, 0.03515625, 1.0, 1.0, 0.03515625, 0.03125]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.5251736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5329861044883728, 1.0, 1.0, 1.0, 0.5034722089767456, 1.0, 1.0, 0.480034738779068, 1.0, 0.5026041865348816, 0.5043402910232544, 0.506944477558136, 1.0, 1.0, 1.0, 0.5043402910232544, 1.0, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5121527910232544, 1.0, 0.5121527910232544, 1.0, 1.0, 1.0, 0.5581597089767456, 1.0, 0.5208333134651184, 1.0, 1.0, 1.0, 0.5112847089767456, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 1.0, 1.0, 1.0, 1.0, 0.5043402910232544, 0.5190972089767456, 0.5121527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 0.5026041865348816, 0.5, 1.0, 1.0, 0.5277777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5078125, 1.0, 0.53125, 1.0, 0.4939236044883728, 1.0, 0.4904513955116272, 0.5, 0.5407986044883728, 1.0, 0.5086805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5017361044883728, 1.0, 0.5208333134651184, 0.5321180820465088, 1.0, 1.0, 1.0, 0.5286458134651184, 0.4991319477558136, 0.484375, 1.0, 0.5251736044883728, 1.0, 1.0, 0.4921875, 1.0, 1.0, 0.4982638955116272, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.671875, 0.6875, 0.7109375, 0.6796875, 0.6875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6796875, 0.6875, 0.671875, 1.0, 0.6875, 0.6796875, 0.6953125, 1.0, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6875, 0.671875, 0.6953125, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.6875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6953125, 0.6796875, 1.0, 0.671875, 0.6796875, 0.671875, 0.6953125, 1.0, 0.671875, 0.6796875, 0.6796875, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 1.0, 0.671875, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6796875, 0.6953125, 0.6796875, 0.671875, 1.0, 0.6796875, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.703125, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 0.6796875, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 0.6796875, 0.6875, 1.0, 0.6953125, 0.6796875, 1.0, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6796875, 0.6953125, 1.0, 0.671875, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6796875, 1.0, 1.0, 0.671875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 1.0, 0.6875, 0.703125, 0.6875, 1.0, 1.0, 0.6875, 0.671875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.671875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 0.671875, 0.6796875, 0.671875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.671875, 0.671875, 0.7109375, 1.0, 0.6875, 0.671875, 0.671875, 1.0, 0.703125, 0.6796875, 0.6875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.671875, 0.6953125, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.671875, 0.6875, 0.671875, 0.6875, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.6875, 0.6796875, 0.671875, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6875, 0.671875, 0.6875, 0.671875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.671875, 0.6953125, 1.0, 0.6875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.671875, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.671875, 0.6953125, 0.6953125, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 0.703125, 0.6875, 1.0, 0.6875, 1.0, 0.671875, 0.6875, 0.6796875, 1.0, 0.671875, 0.6796875, 0.6953125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6953125, 0.6796875, 1.0, 0.671875, 1.0, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.703125, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.6875, 0.6796875, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.671875, 0.6796875, 0.7109375, 0.6796875, 0.6796875, 0.671875, 0.6875, 0.671875, 0.671875, 0.6875, 0.6875, 0.703125, 1.0, 0.671875, 0.6796875, 0.671875, 0.6953125, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.671875, 0.6796875, 1.0, 0.6953125, 1.0, 0.6796875, 0.6953125, 1.0, 0.6953125, 0.6796875, 0.671875, 0.671875, 0.6875, 0.671875, 0.6796875, 0.6796875, 0.703125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6796875, 0.6953125, 0.671875, 1.0, 0.671875, 1.0, 0.671875, 1.0, 0.6875, 0.6953125, 1.0, 0.671875, 1.0, 0.703125, 0.671875, 0.6953125, 0.6875, 0.671875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.671875, 0.6875, 0.671875, 1.0, 0.6796875, 0.6953125, 0.7109375, 1.0, 1.0, 0.6875, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 0.671875, 1.0, 0.671875, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.671875, 0.671875, 1.0, 0.671875, 0.671875, 0.6796875, 0.6875, 1.0]

 sparsity of   [0.03125, 0.03125, 0.0390625, 0.04296875, 0.04296875, 0.04296875, 1.0, 1.0, 0.03515625, 0.0390625, 0.0625, 0.0390625, 1.0, 0.0625, 0.04296875, 0.03125, 1.0, 1.0, 0.046875, 0.04296875, 0.03125, 0.03515625, 0.03515625, 1.0, 0.046875, 0.0546875, 0.04296875, 1.0, 0.0546875, 1.0, 1.0, 0.05859375, 0.03515625, 1.0, 0.04296875, 1.0, 0.05859375, 0.03515625, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 0.0390625, 1.0, 1.0, 0.04296875, 0.046875, 0.03515625, 1.0, 0.04296875, 1.0, 0.03515625, 0.0703125, 1.0, 0.046875, 0.03515625, 0.03515625, 0.03515625, 0.03515625, 0.046875, 0.078125, 1.0, 1.0, 0.046875, 0.08203125, 0.0390625, 1.0, 1.0, 1.0, 0.0390625, 0.08203125, 0.03515625, 0.07421875, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 0.04296875, 0.03515625, 0.03515625, 0.046875, 0.05859375, 0.05859375, 0.03515625, 0.0390625, 0.0390625, 1.0, 0.05859375, 0.046875, 1.0, 0.046875, 0.0546875, 1.0, 0.0546875, 0.0390625, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 0.03515625, 0.04296875, 0.09375, 0.03515625, 0.04296875, 0.04296875, 0.04296875, 1.0, 1.0, 0.03125, 0.046875, 0.046875, 0.046875, 0.046875, 0.0859375, 1.0, 0.04296875, 1.0, 0.03515625, 0.0546875, 1.0, 0.03515625, 0.03515625, 1.0, 0.0390625, 0.03125, 0.046875, 1.0, 0.0390625, 0.05078125, 1.0, 0.078125, 0.0390625, 1.0, 0.03515625, 0.03515625, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.0390625, 0.03515625, 1.0, 0.03515625, 0.046875, 0.04296875, 0.04296875, 0.09375, 1.0, 1.0, 0.03515625, 0.06640625, 1.0, 0.04296875, 1.0, 0.0390625, 0.04296875, 0.02734375, 1.0, 0.046875, 0.04296875, 0.03515625, 1.0, 0.046875, 0.05078125, 0.04296875, 1.0, 0.0390625, 0.0546875, 0.05859375, 0.03125, 1.0, 1.0, 0.0546875, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 0.0390625, 0.03515625, 1.0, 0.1015625, 0.03515625, 0.046875, 0.04296875, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.03515625, 0.046875, 1.0, 1.0, 0.04296875, 1.0, 0.04296875, 0.0390625, 0.04296875, 0.05078125, 0.05078125, 1.0, 0.046875, 0.0390625, 0.04296875, 1.0, 0.04296875, 0.04296875, 0.03515625, 0.04296875, 0.0390625, 1.0, 0.0546875, 0.046875, 1.0, 1.0, 0.0546875, 0.03515625, 0.04296875, 1.0, 0.0546875, 0.125, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.0390625, 0.05859375, 0.09765625, 0.12890625, 0.04296875, 0.04296875, 0.03515625, 0.0390625, 0.0390625, 0.04296875, 0.0390625, 0.0546875, 1.0, 0.03125, 0.03125, 0.0625, 0.05078125, 0.03515625, 0.046875, 0.04296875, 1.0, 0.0625, 0.04296875, 1.0, 0.03515625, 0.078125, 1.0, 0.078125, 0.0390625, 0.05859375, 1.0, 0.05078125, 1.0, 0.046875, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 0.03125, 0.04296875, 1.0, 0.0390625, 0.03125, 0.05078125, 0.03515625, 0.05078125, 0.0390625, 1.0, 0.0390625, 0.03515625, 0.04296875, 1.0, 0.04296875, 1.0, 1.0, 0.04296875, 0.08984375, 0.0234375, 0.0390625, 0.0390625, 1.0, 1.0, 0.03125, 0.0390625, 0.0546875, 0.04296875, 1.0, 1.0, 0.0625, 0.05078125, 0.05078125, 0.03125, 0.05078125, 1.0, 1.0, 0.03125, 1.0, 0.04296875, 1.0, 0.04296875, 1.0, 0.0390625, 0.04296875, 1.0, 1.0, 0.03125, 0.0390625, 0.03515625, 1.0, 0.03125, 1.0, 0.03125, 0.0390625, 0.046875, 1.0, 0.03515625, 0.0546875, 0.0390625, 0.0390625, 1.0, 1.0, 0.0546875, 1.0, 0.03515625, 0.15234375, 1.0, 0.0390625, 1.0, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.046875, 1.0, 1.0, 1.0, 0.03125, 1.0, 0.03125, 0.04296875, 0.0390625, 0.03515625, 0.03125, 0.0390625, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.0625, 0.046875, 0.046875, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.0390625, 0.05078125, 0.0390625, 0.03515625, 0.0390625, 0.0390625, 0.02734375, 0.05078125, 0.046875, 0.03515625, 0.04296875, 0.04296875, 0.0390625, 0.046875, 1.0, 0.13671875, 0.0390625, 0.05078125, 0.04296875, 1.0, 1.0, 0.046875, 0.03515625, 1.0, 0.0390625, 0.0390625, 1.0, 0.0234375, 1.0, 0.09765625, 0.0390625, 1.0, 0.04296875, 0.046875, 0.04296875, 0.078125, 0.05078125, 0.03515625, 0.05078125, 0.0390625, 0.0546875, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.046875, 1.0, 0.11328125, 1.0, 1.0, 0.0390625, 0.0625, 1.0, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 0.04296875, 0.05078125, 0.046875, 1.0, 0.04296875, 1.0, 0.04296875, 0.046875, 0.05078125, 0.05078125, 1.0, 0.03125, 1.0, 0.0390625, 0.0390625, 0.03515625, 0.04296875, 0.04296875, 0.03515625, 0.04296875, 0.03515625, 0.046875, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.03515625, 1.0, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 0.1640625, 1.0, 0.0390625, 0.04296875, 0.04296875, 0.04296875, 0.0859375, 0.09765625, 0.046875, 1.0, 1.0, 0.03515625, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 0.04296875, 1.0, 0.03515625, 0.03515625, 1.0, 0.12109375, 0.0390625, 0.06640625, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 0.046875, 0.05859375, 1.0, 0.03515625, 0.08203125, 0.04296875, 0.03515625, 1.0]

 sparsity of   [1.0, 0.30078125, 0.30078125, 0.30859375, 0.314453125, 0.3046875, 1.0, 0.30859375, 1.0, 0.310546875, 0.30859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.314453125, 0.30859375, 1.0, 0.318359375, 1.0, 1.0, 0.314453125, 0.310546875, 1.0, 1.0, 1.0, 1.0, 0.306640625, 0.306640625, 1.0, 0.306640625, 0.302734375, 1.0, 0.3203125, 1.0, 1.0, 0.30078125, 1.0, 0.318359375, 0.3046875, 0.31640625, 1.0, 1.0, 1.0, 0.30859375, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.310546875, 0.3125, 0.302734375, 1.0, 0.318359375, 0.31640625, 1.0, 1.0, 0.322265625, 0.3125, 0.306640625, 0.3125, 0.30859375, 1.0, 1.0, 0.318359375, 0.3046875, 1.0, 0.318359375, 1.0, 1.0, 0.30859375, 0.3125, 1.0, 0.30859375, 0.3125, 0.3125, 0.310546875, 1.0, 1.0, 0.306640625, 0.30859375, 0.3046875, 0.3046875, 0.306640625, 1.0, 0.30859375, 0.3046875, 1.0, 0.3125, 1.0, 1.0, 0.310546875, 1.0, 0.310546875, 0.306640625, 0.302734375, 0.310546875, 0.31640625, 0.306640625, 1.0, 0.302734375, 0.314453125, 0.31640625, 1.0, 1.0, 1.0, 0.310546875, 0.322265625, 1.0, 1.0, 0.310546875, 1.0, 0.306640625, 0.314453125, 0.302734375, 0.302734375, 0.3046875, 1.0, 1.0, 0.310546875, 1.0, 0.30859375, 1.0, 0.30859375, 0.30859375, 0.3125]

 sparsity of   [0.4149305522441864, 1.0, 0.409722238779068, 1.0, 0.4140625, 1.0, 0.4131944477558136, 1.0, 1.0, 1.0, 0.4192708432674408, 1.0, 0.4157986044883728, 1.0, 1.0, 0.4105902910232544, 0.4184027910232544, 0.4192708432674408, 1.0, 1.0, 0.4192708432674408, 0.4123263955116272, 0.4201388955116272, 0.4140625, 1.0, 0.4123263955116272, 1.0, 0.4157986044883728, 1.0, 1.0, 0.4149305522441864, 1.0, 1.0, 1.0, 0.4140625, 0.409722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4105902910232544, 0.4192708432674408, 0.4140625, 1.0, 0.4184027910232544, 1.0, 0.4140625, 0.4236111044883728, 0.409722238779068, 0.417534738779068, 1.0, 1.0, 1.0, 0.4149305522441864, 1.0, 0.4114583432674408, 0.4184027910232544, 0.4131944477558136, 1.0, 0.4149305522441864, 1.0, 0.4201388955116272, 1.0, 0.417534738779068, 1.0, 1.0, 0.4105902910232544, 0.4166666567325592, 1.0, 1.0, 0.4123263955116272, 0.417534738779068, 1.0, 1.0, 0.4210069477558136, 1.0, 1.0, 1.0, 1.0, 0.4123263955116272, 0.4227430522441864, 0.4131944477558136, 0.417534738779068, 0.4184027910232544, 0.4166666567325592, 1.0, 1.0, 0.417534738779068, 0.4131944477558136, 0.4140625, 0.417534738779068, 0.4149305522441864, 1.0, 0.4157986044883728, 0.4210069477558136, 1.0, 1.0, 0.4149305522441864, 1.0, 1.0, 0.4149305522441864, 0.4131944477558136, 1.0, 1.0, 0.4114583432674408, 1.0, 1.0, 0.417534738779068, 1.0, 1.0, 0.417534738779068, 0.4157986044883728, 1.0, 1.0, 1.0, 0.4140625, 0.4149305522441864, 1.0, 0.417534738779068, 1.0, 0.4140625, 0.4123263955116272, 1.0, 0.4123263955116272]

 sparsity of   [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 1.0, 0.5078125, 0.5, 0.5, 0.5078125, 0.5078125, 0.515625, 1.0, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.4921875, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 0.4921875, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 0.5, 1.0, 0.4921875, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.4921875, 0.4921875, 0.4921875, 1.0, 1.0, 1.0, 0.4921875, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5078125, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5078125, 0.5, 0.5, 0.5078125, 0.5, 0.4921875, 0.4921875, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.4921875, 0.4921875, 1.0, 0.5, 1.0, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.5078125, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5078125, 0.5078125, 1.0, 0.4921875, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5078125, 1.0, 1.0, 1.0, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.515625, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5078125, 1.0, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 1.0, 1.0, 0.4921875, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 0.4921875, 1.0, 0.5078125, 1.0, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5078125, 0.5, 0.5, 0.5078125, 1.0, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 1.0, 0.5, 0.5078125, 0.515625, 0.4921875, 0.5, 0.5, 1.0, 1.0, 0.4921875, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5078125, 0.4921875, 0.5078125, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5078125, 1.0, 1.0, 0.5, 1.0, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 0.4921875, 0.5, 0.515625, 0.4921875, 0.5, 0.5078125, 0.5, 0.5, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5078125, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.5, 1.0, 0.515625, 0.4921875, 0.5, 0.5078125, 0.5078125, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 0.515625, 0.5, 1.0, 0.4921875, 1.0, 0.5, 0.5078125, 0.5078125, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4921875, 0.5, 1.0, 1.0, 0.5, 0.5078125, 1.0, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.5078125, 0.5, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 0.4921875, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.4921875, 0.5, 0.4921875, 0.5078125, 0.515625, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5078125, 0.5, 1.0, 0.5078125, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5078125, 0.5, 0.4921875, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5078125, 0.5, 0.5, 0.5, 1.0, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.5078125, 0.5, 0.5078125, 0.5078125, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.515625, 0.5, 0.4921875, 0.4921875, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5078125, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5078125, 1.0, 0.5078125, 0.5, 1.0, 0.4921875, 0.4921875, 0.4921875, 1.0, 0.5078125, 0.5078125, 0.5, 0.5078125, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.220703125, 1.0, 1.0, 0.216796875, 1.0, 0.203125, 0.19921875, 0.197265625, 0.201171875, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.205078125, 1.0, 0.19921875, 0.21484375, 1.0, 0.2578125, 1.0, 1.0, 1.0, 0.2109375, 1.0, 0.203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20703125, 1.0, 1.0, 0.205078125, 0.20703125, 1.0, 0.201171875, 1.0, 1.0, 1.0, 1.0, 0.205078125, 0.203125, 0.251953125, 0.205078125, 1.0, 1.0, 1.0, 1.0, 0.205078125, 1.0, 0.212890625, 1.0, 1.0, 0.19921875, 0.201171875, 1.0, 0.201171875, 1.0, 1.0, 0.21484375, 0.201171875, 1.0, 1.0, 0.1953125, 1.0, 0.26171875, 1.0, 1.0, 1.0, 0.20703125, 1.0, 0.203125, 0.203125, 0.205078125, 1.0, 1.0, 0.4765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 0.205078125, 1.0, 0.23828125, 1.0, 0.1953125, 0.20703125, 1.0, 0.193359375, 0.19921875, 0.197265625, 1.0, 1.0, 1.0, 1.0, 0.1953125, 1.0, 1.0, 1.0, 0.21484375, 0.21875, 1.0, 1.0, 1.0, 1.0, 0.2109375, 1.0, 1.0, 0.208984375, 1.0, 1.0, 0.19921875, 0.201171875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 1.0, 0.5355902910232544, 1.0, 0.5486111044883728, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5911458134651184, 0.5486111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5842013955116272, 0.5842013955116272, 1.0, 0.6545138955116272, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5581597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5529513955116272, 1.0, 0.6701388955116272, 0.5425347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5347222089767456, 0.5486111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.5633680820465088, 0.5451388955116272, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 1.0, 1.0]

 sparsity of   [0.765625, 0.765625, 0.7578125, 0.75, 0.7578125, 0.7734375, 1.0, 1.0, 0.75, 0.7578125, 1.0, 0.765625, 0.75, 1.0, 0.75, 1.0, 0.765625, 0.765625, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 0.75, 0.7578125, 1.0, 1.0, 0.765625, 0.765625, 0.7578125, 1.0, 1.0, 0.7734375, 0.7578125, 1.0, 0.7578125, 1.0, 0.75, 1.0, 0.75, 0.765625, 1.0, 0.75, 0.765625, 0.7578125, 1.0, 1.0, 0.796875, 1.0, 0.765625, 0.765625, 0.7578125, 0.75, 0.7578125, 1.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.7578125, 0.7578125, 0.765625, 0.75, 0.765625, 1.0, 0.75, 1.0, 0.78125, 0.75, 0.7734375, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.765625, 0.78125, 1.0, 0.7734375, 0.7578125, 0.765625, 1.0, 0.7578125, 0.765625, 0.7734375, 1.0, 0.75, 1.0, 0.765625, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 0.7578125, 1.0, 0.765625, 0.75, 0.7734375, 0.75, 0.75, 0.765625, 0.765625, 0.75, 0.75, 0.7578125, 0.75, 0.765625, 1.0, 1.0, 0.7734375, 0.75, 0.75, 1.0, 0.765625, 1.0, 0.7578125, 0.765625, 1.0, 0.75, 0.765625, 0.765625, 0.7734375, 0.765625, 0.75, 0.75, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.75, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.7578125, 0.75, 0.7578125, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 0.765625, 0.765625, 1.0, 0.75, 0.765625, 0.7578125, 0.7578125, 0.765625, 0.75, 0.765625, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 1.0, 0.7734375, 0.765625, 0.75, 0.765625, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.765625, 0.7578125, 0.75, 0.7578125, 0.7578125, 0.75, 0.7578125, 0.75, 0.765625, 0.796875, 0.765625, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.765625, 0.765625, 0.765625, 0.78125, 0.78125, 1.0, 0.75, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 1.0, 0.75, 0.765625, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 1.0, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.765625, 0.7578125, 0.75, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.765625, 0.75, 1.0, 0.75, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.7734375, 0.7734375, 0.765625, 0.75, 1.0, 0.75, 0.75, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.765625, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.75, 1.0, 0.765625, 1.0, 0.7578125, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 0.75, 0.75, 0.75, 0.765625, 0.765625, 0.75, 0.765625, 0.75, 1.0, 0.7578125, 0.765625, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.7734375, 0.75, 0.75, 1.0, 0.7578125, 1.0, 0.765625, 0.7734375, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.765625, 1.0, 0.796875, 0.7578125, 0.765625, 0.765625, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 0.765625, 0.765625, 0.765625, 1.0, 0.7578125, 0.75, 1.0, 0.7578125, 1.0, 0.765625, 0.765625, 1.0, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.75, 1.0, 0.765625, 0.75, 0.7578125, 0.7578125, 0.75, 0.7578125, 1.0, 0.75, 1.0, 0.765625, 0.75, 0.765625, 1.0, 1.0, 0.765625, 0.7578125, 0.7734375, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.75, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.75, 0.765625, 0.765625, 0.75, 0.765625, 0.7578125, 0.75, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.75, 0.7578125, 1.0, 0.765625, 1.0, 0.75, 1.0, 1.0, 0.7578125, 0.765625, 0.75, 1.0, 0.75, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 0.7734375, 0.765625, 0.7734375, 1.0, 0.765625, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.75, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.75]

 sparsity of   [1.0, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.115234375, 0.11328125, 0.119140625, 0.126953125, 0.115234375, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.125, 0.125, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.12890625, 0.123046875, 0.125, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 0.123046875, 0.119140625, 1.0, 0.134765625, 1.0, 0.119140625, 0.1171875, 0.115234375, 1.0, 1.0, 1.0, 0.125, 1.0, 0.115234375, 1.0, 1.0, 0.126953125, 1.0, 0.115234375, 1.0, 0.123046875, 0.12109375, 1.0, 1.0, 1.0, 0.125, 0.123046875, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.119140625, 0.125, 1.0, 1.0, 0.142578125, 1.0, 0.12109375, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 0.119140625, 0.115234375, 1.0, 0.123046875, 0.125, 1.0, 0.123046875, 1.0, 0.12109375, 1.0, 0.138671875, 0.1171875, 1.0, 1.0, 1.0, 0.123046875, 0.115234375, 0.123046875, 0.12890625, 1.0, 0.123046875, 1.0, 0.12109375, 1.0, 0.12890625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.538194477558136, 1.0, 1.0, 0.5503472089767456, 0.600694477558136, 1.0, 1.0, 1.0, 0.538194477558136, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 0.5503472089767456, 1.0, 1.0, 0.5416666865348816, 1.0, 0.5329861044883728, 1.0, 0.585069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5338541865348816, 1.0, 1.0, 1.0, 0.5477430820465088, 1.0, 1.0, 0.5425347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 1.0, 1.0, 0.5434027910232544, 0.546006977558136, 0.5373263955116272, 1.0, 0.5390625, 0.5494791865348816, 1.0, 1.0, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 0.5416666865348816, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.5503472089767456, 0.5842013955116272, 0.5963541865348816, 1.0, 1.0, 1.0, 0.5321180820465088, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 0.5503472089767456, 1.0, 0.5407986044883728, 1.0, 0.5564236044883728]

 sparsity of   [0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.71875, 0.7109375, 0.703125, 0.703125, 0.703125, 0.71875, 0.703125, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.7265625, 0.703125, 1.0, 0.7109375, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.71875, 0.7109375, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 1.0, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.71875, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.71875, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.71875, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.734375, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.7265625, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 1.0, 0.7265625, 0.703125, 0.71875, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.7109375, 1.0, 0.7109375, 0.7109375, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.703125]

 sparsity of   [0.08203125, 1.0, 0.072265625, 0.078125, 1.0, 0.078125, 0.072265625, 1.0, 0.080078125, 0.072265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.07421875, 0.07421875, 1.0, 1.0, 0.076171875, 0.076171875, 1.0, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 0.07421875, 0.08984375, 0.07421875, 1.0, 0.080078125, 0.07421875, 1.0, 0.07421875, 1.0, 0.0703125, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.076171875, 0.078125, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.0859375, 0.076171875, 1.0, 1.0, 0.076171875, 1.0, 0.072265625, 1.0, 1.0, 0.080078125, 0.07421875, 0.078125, 1.0, 0.078125, 1.0, 0.068359375, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.08203125, 0.076171875, 0.07421875, 1.0, 1.0, 0.07421875, 0.072265625, 0.078125, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.080078125, 0.072265625, 0.080078125, 0.076171875, 0.080078125, 0.07421875, 1.0, 0.076171875, 0.068359375, 0.076171875, 1.0, 0.078125, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.076171875, 0.08203125, 0.072265625, 0.072265625, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 0.078125, 0.08203125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 0.07421875, 0.078125, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.068359375, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.076171875, 0.068359375, 1.0, 1.0, 0.072265625, 0.078125, 0.072265625, 1.0, 1.0, 0.080078125, 0.0703125, 0.080078125, 0.076171875, 1.0, 0.078125, 0.07421875, 0.083984375, 1.0, 0.08203125, 0.078125, 1.0, 1.0, 0.07421875, 0.080078125, 0.068359375, 0.076171875, 1.0, 1.0, 0.078125, 0.072265625, 0.07421875, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 1.0, 0.076171875, 1.0, 0.0703125, 0.072265625, 1.0, 1.0, 0.07421875, 0.076171875, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 0.078125, 0.080078125, 0.07421875, 0.078125, 1.0, 0.07421875, 0.072265625, 0.078125, 0.078125, 1.0, 0.072265625, 1.0, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 0.076171875, 1.0, 0.080078125, 1.0, 1.0, 1.0, 0.080078125, 0.068359375, 1.0, 0.083984375, 0.072265625, 0.076171875, 0.08203125, 0.08203125, 1.0, 1.0, 1.0, 0.076171875, 0.076171875, 1.0, 0.068359375, 0.078125, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 0.07421875, 1.0, 0.07421875, 1.0, 0.076171875, 0.072265625, 1.0]

 sparsity of   [1.0, 1.0, 0.4939236044883728, 1.0, 1.0, 1.0, 1.0, 0.4947916567325592, 0.4939236044883728, 0.4973958432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.495659738779068, 1.0, 0.4943576455116272, 1.0, 0.495659738779068, 1.0, 0.4934895932674408, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 0.4939236044883728, 0.49609375, 0.4947916567325592, 1.0, 0.4943576455116272, 0.4969618022441864, 1.0, 1.0, 0.4947916567325592, 0.4930555522441864, 1.0, 0.4943576455116272, 1.0, 0.4926215410232544, 1.0, 0.4934895932674408, 0.495659738779068, 1.0, 0.49609375, 1.0, 0.4934895932674408, 1.0, 0.4947916567325592, 0.4943576455116272, 1.0, 1.0, 0.495659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 0.4952256977558136, 0.4969618022441864, 0.4947916567325592, 1.0, 1.0, 1.0, 0.4978298544883728, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4934895932674408, 1.0, 0.495659738779068, 1.0, 1.0, 1.0, 1.0, 0.4947916567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.49609375, 0.49609375, 0.4921875, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.4965277910232544, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4930555522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4969618022441864, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.49609375, 1.0, 1.0, 1.0, 0.4943576455116272, 0.49609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4991319477558136, 1.0, 0.4969618022441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.49609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4978298544883728, 1.0, 0.4934895932674408, 0.4939236044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4982638955116272, 1.0, 0.49609375, 0.4965277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 0.4939236044883728, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.495659738779068, 0.49609375, 1.0, 1.0, 0.4939236044883728, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.73046875, 1.0, 0.72265625, 0.7265625, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.73046875, 0.72265625, 0.7265625, 0.73046875, 0.73046875, 0.73046875, 0.72265625, 1.0, 0.7421875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 0.734375, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 0.72265625, 0.72265625, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.734375, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 0.73046875, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 0.72265625, 0.72265625, 0.7265625, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.734375, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 0.72265625, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.73046875, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.734375, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.73046875, 0.734375, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.73046875, 0.73046875, 0.73046875, 0.73046875, 1.0, 1.0, 0.72265625, 0.7265625, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.73828125, 0.73046875, 0.73046875, 0.72265625, 0.73046875, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.72265625, 1.0, 0.734375, 0.7265625, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 1.0, 1.0, 0.734375, 0.72265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.72265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.73828125, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.73046875, 0.73046875, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0703125, 0.111328125, 1.0, 0.076171875, 1.0, 0.080078125, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 0.078125, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 0.08984375, 1.0, 1.0, 0.087890625, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.08203125, 1.0, 1.0, 1.0, 0.1015625, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.080078125, 0.1484375, 0.076171875, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 0.078125, 0.08203125, 0.134765625, 1.0, 0.078125, 0.083984375, 0.07421875, 1.0, 0.076171875, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.08203125, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 1.0, 1.0, 0.080078125, 1.0, 1.0, 0.08203125, 1.0, 0.080078125, 0.099609375, 1.0, 0.083984375, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.09375, 0.08203125, 0.076171875, 0.078125, 0.078125, 0.080078125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.0859375, 1.0, 0.08984375, 1.0, 0.080078125, 1.0, 1.0, 0.07421875, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.08203125, 0.08203125, 1.0, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 0.08203125, 1.0, 1.0, 0.078125, 0.08203125, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 0.083984375, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.09375, 1.0, 0.083984375, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.07421875, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1796875, 0.126953125, 0.09375, 1.0, 1.0, 0.0703125, 0.07421875, 1.0, 1.0, 1.0, 0.21484375, 0.087890625, 1.0, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.076171875, 1.0, 1.0, 0.072265625, 1.0, 1.0, 0.072265625, 0.083984375, 1.0, 0.087890625, 1.0, 1.0, 0.0859375, 0.08984375, 0.130859375, 0.08203125, 1.0, 0.09375, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.078125, 1.0, 0.091796875, 1.0, 1.0, 0.08203125, 0.076171875, 0.087890625, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 0.083984375, 1.0, 0.07421875, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.078125, 0.076171875, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.087890625, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.076171875, 0.072265625, 0.072265625, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 0.078125, 0.08203125, 0.078125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.080078125, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.087890625, 0.083984375, 0.078125, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.083984375, 1.0, 0.08203125, 1.0, 0.09375, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.0859375, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.083984375, 0.080078125, 0.080078125, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.087890625, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 0.08203125, 0.080078125, 1.0, 1.0, 0.087890625, 1.0, 0.078125, 0.07421875, 1.0, 0.09375, 0.080078125, 0.076171875, 0.078125, 0.080078125, 1.0, 0.078125, 0.08203125, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0859375, 1.0, 0.087890625, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.0859375, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 0.0859375, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.08203125, 1.0, 0.0859375, 1.0, 1.0, 0.083984375, 0.078125, 1.0, 0.076171875, 0.08203125, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.078125, 0.078125, 0.091796875, 0.076171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.072265625, 1.0, 1.0, 0.072265625, 1.0, 0.16015625, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 0.083984375, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.08984375, 0.07421875, 0.080078125, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.07421875, 0.09375, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.078125, 0.091796875, 1.0, 1.0, 0.07421875, 1.0, 0.083984375, 1.0, 1.0, 0.076171875, 0.08203125, 1.0, 0.083984375, 1.0, 1.0, 0.091796875, 0.080078125, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.080078125, 0.0859375, 1.0, 1.0, 0.087890625, 1.0, 1.0, 0.126953125, 0.095703125, 0.083984375, 0.08203125, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.087890625, 1.0, 0.091796875, 0.61328125, 1.0, 0.08984375, 1.0, 1.0, 0.078125, 0.080078125, 0.080078125, 0.087890625, 1.0, 0.08203125, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.078125, 0.083984375, 0.103515625, 1.0, 1.0, 1.0, 0.087890625, 1.0, 0.09375, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.0859375, 1.0, 1.0, 0.076171875, 0.115234375, 0.0859375, 1.0, 1.0, 1.0, 0.0859375, 0.083984375, 0.078125, 1.0, 1.0, 0.099609375, 0.08203125, 0.0703125, 0.07421875, 1.0, 1.0, 0.16796875, 0.0859375, 1.0, 0.080078125, 0.076171875, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.0859375, 0.091796875, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.08203125, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 0.087890625, 0.08203125, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.072265625, 0.07421875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.099609375, 0.091796875, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.087890625, 0.072265625, 1.0, 0.07421875, 0.07421875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 0.6044921875, 0.6103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 0.607421875, 0.603515625, 0.6103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.60546875, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6025390625, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.60546875, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.60546875, 0.603515625, 1.0, 1.0, 0.6044921875, 0.6064453125, 1.0, 1.0, 0.6083984375, 0.60546875, 1.0, 0.6103515625, 1.0, 0.607421875, 0.6025390625, 1.0, 0.607421875, 0.6064453125, 1.0, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.611328125, 0.609375, 1.0, 1.0, 0.6005859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.6044921875, 1.0, 0.6025390625, 0.603515625, 1.0, 0.6064453125, 1.0, 1.0, 0.6025390625, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.5947265625, 0.6044921875, 1.0, 1.0, 0.6025390625, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6044921875, 1.0, 0.603515625, 1.0, 0.607421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.599609375, 1.0, 1.0, 0.603515625, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6083984375, 1.0, 0.607421875, 0.609375, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7638888955116272, 1.0, 1.0, 1.0, 0.7699652910232544, 0.7703993320465088, 1.0, 0.7703993320465088, 0.765625, 0.7664930820465088, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7634548544883728, 1.0, 1.0, 1.0, 1.0, 0.7682291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7721354365348816, 0.7721354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76171875, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 0.7690972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 0.7651909589767456, 1.0, 0.7690972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7721354365348816, 1.0, 0.7638888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7703993320465088, 0.76171875, 1.0, 1.0, 1.0, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7682291865348816, 0.7625868320465088, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7638888955116272, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7586805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 0.7643229365348816, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8125, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.82421875, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.82421875, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.8125, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.82421875, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82421875, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.8125, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8203125, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.82421875, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.82421875, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8125, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.8203125, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8203125, 1.0, 0.82421875, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.5869140625, 0.583984375, 1.0, 1.0, 1.0, 0.5849609375, 0.5791015625, 1.0, 0.5849609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 0.5869140625, 1.0, 0.583984375, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 0.5849609375, 1.0, 1.0, 0.5849609375, 1.0, 0.583984375, 1.0, 0.58984375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.587890625, 0.5859375, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5849609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 0.5869140625, 1.0, 1.0, 1.0, 0.5888671875, 0.5791015625, 0.591796875, 1.0, 1.0, 0.587890625, 0.5908203125, 1.0, 0.5869140625, 1.0, 1.0, 1.0, 0.587890625, 1.0, 0.583984375, 0.58203125, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.587890625, 0.5830078125, 1.0, 1.0, 0.5888671875, 1.0, 0.5859375, 1.0, 1.0, 0.587890625, 1.0, 1.0, 1.0, 0.591796875, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.58203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 0.5849609375, 0.5830078125, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 1.0, 0.587890625, 1.0, 1.0, 0.58203125, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5869140625, 1.0, 0.5927734375, 1.0, 1.0, 0.59375, 0.5888671875, 0.58203125, 1.0, 0.5849609375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 0.5859375, 0.5927734375, 1.0, 0.5849609375, 0.58203125, 0.591796875, 1.0, 1.0, 0.5849609375, 0.5908203125, 0.591796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.58984375, 1.0, 0.5888671875, 1.0, 0.591796875, 1.0, 0.5927734375, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.5888671875, 1.0, 1.0, 0.587890625, 0.5869140625, 1.0, 1.0, 0.587890625, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5869140625, 1.0, 1.0, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 0.587890625, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 0.5849609375]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 1.0, 1.0, 0.6744791865348816, 1.0, 1.0, 1.0, 0.6731770634651184, 0.6727430820465088, 0.6740451455116272, 0.6727430820465088, 0.674913227558136, 1.0, 0.674913227558136, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 0.6731770634651184, 1.0, 0.674913227558136, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 0.6731770634651184, 1.0, 0.6766493320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6740451455116272, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 0.6731770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 0.6731770634651184, 1.0, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6731770634651184, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6723090410232544, 1.0, 1.0, 1.0, 1.0, 0.6744791865348816, 0.6740451455116272, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 0.6723090410232544, 1.0, 0.6744791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 0.67578125, 1.0, 0.6779513955116272, 0.6740451455116272, 0.6731770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 0.6736111044883728, 0.6727430820465088, 0.6744791865348816, 0.6723090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.80078125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.7890625, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.78515625, 0.77734375, 1.0, 1.0, 1.0, 0.78125, 0.77734375, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78515625, 0.7890625, 1.0, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.78125, 1.0, 0.7890625, 0.79296875, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78515625, 0.7890625, 1.0, 0.77734375, 0.7890625, 1.0, 0.78125, 1.0, 1.0, 0.77734375, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78515625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7890625, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.77734375, 0.78515625, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.7890625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.78515625, 1.0, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.78125, 0.78515625, 0.79296875, 0.78125, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78125, 0.78515625, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.79296875, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 0.78125, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.78515625, 0.7890625, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.79296875, 1.0, 1.0, 0.78515625, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.7890625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.78515625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.79296875, 0.79296875, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.7890625, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.77734375, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.7890625, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.78125, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7890625, 1.0, 0.78125, 0.77734375, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9869791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.587890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9871962070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5693359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6689453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.576171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.978515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9794921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9924045205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9924045205116272, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.77783203125, 0.77783203125, 0.77734375, 0.77734375, 0.77783203125, 0.77734375, 0.77734375, 0.7783203125, 0.77734375, 0.77734375, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]

Total parameter pruned: 22728940.003513373 (unstructured) 21611437 (structured)

Test: [0/79]	Time 0.179 (0.179)	Loss 0.3081 (0.3081) ([0.186]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.850

 Total elapsed time  3:57:54.389911 
 FINETUNING


 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.03703703731298447, 0.0, 1.0, 0.0, 1.0, 0.0, 0.03703703731298447, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.03703703731298447, 0.03703703731298447, 1.0, 0.03703703731298447, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.34375, 0.34375, 0.34375, 1.0, 0.359375, 0.359375, 0.359375, 0.34375, 0.359375, 0.359375, 0.375, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 1.0, 1.0, 0.421875, 0.359375, 0.359375, 0.34375, 0.34375, 0.359375, 0.359375, 0.359375, 0.359375, 0.34375, 0.34375, 1.0, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.34375, 0.34375, 0.359375, 0.390625, 0.34375, 0.375, 0.34375, 0.359375, 0.359375, 0.34375, 0.34375, 0.359375, 1.0, 0.375, 0.34375, 1.0, 0.40625, 0.34375, 0.359375, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 0.390625, 0.34375]

 sparsity of   [0.1388888955116272, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.1302083283662796, 0.1302083283662796, 0.1336805522441864, 1.0, 0.1354166716337204, 1.0, 1.0, 0.1284722238779068, 1.0, 1.0, 0.1354166716337204, 0.1354166716337204, 0.1336805522441864, 1.0, 0.1388888955116272, 1.0, 0.142361119389534, 0.1371527761220932, 0.140625, 1.0, 0.1302083283662796, 0.1319444477558136, 0.1319444477558136, 1.0, 0.1336805522441864, 1.0, 0.1336805522441864, 1.0, 0.1371527761220932, 1.0, 0.1302083283662796, 1.0, 1.0, 1.0, 1.0, 0.1527777761220932, 0.1354166716337204, 1.0, 0.1458333283662796, 1.0, 1.0, 0.1319444477558136, 1.0, 0.1319444477558136, 1.0, 1.0, 0.1354166716337204, 1.0, 1.0, 0.1527777761220932, 1.0, 0.1458333283662796, 0.1388888955116272, 0.1354166716337204, 0.1336805522441864, 0.15625, 1.0, 0.1475694477558136]

 sparsity of   [0.453125, 0.484375, 0.453125, 0.46875, 0.5, 0.46875, 0.46875, 0.4375, 1.0, 0.484375, 0.46875, 0.453125, 0.46875, 0.46875, 1.0, 0.4375, 0.4375, 0.46875, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.46875, 0.4375, 1.0, 1.0, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 0.46875, 0.453125, 0.453125, 1.0, 0.484375, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 0.453125, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.453125, 0.453125, 1.0, 0.4375, 0.46875, 0.453125, 0.484375, 0.46875, 0.453125, 0.4375, 0.46875, 1.0, 1.0, 0.484375, 0.453125, 0.4375, 0.46875, 0.46875, 1.0, 0.484375, 0.46875, 0.4375, 0.46875, 0.46875, 0.4375, 1.0, 0.4375, 0.453125, 0.46875, 0.484375, 0.46875, 1.0, 0.453125, 0.5, 0.46875, 0.453125, 0.4375, 0.4375, 1.0, 0.4375, 1.0, 0.4375, 1.0, 0.4375, 0.4375, 1.0, 0.453125, 0.46875, 0.46875, 0.46875, 0.4375, 0.4375, 0.453125, 1.0, 1.0, 0.4375, 0.4375, 0.46875, 0.453125, 0.453125, 1.0, 0.46875, 0.484375, 0.46875, 0.4375, 0.46875, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.4375, 0.46875, 1.0, 1.0, 0.46875, 0.453125, 1.0, 0.453125, 0.46875, 1.0, 0.453125, 1.0, 0.453125, 0.453125, 0.453125, 0.4375, 0.46875, 0.4375, 0.4375, 0.46875, 0.484375, 1.0, 0.4375, 0.453125, 0.4375, 1.0, 0.453125, 0.453125, 1.0, 0.46875, 0.453125, 0.46875, 1.0, 0.453125, 0.4375, 0.453125, 1.0, 0.453125, 0.453125, 1.0, 0.4375, 0.453125, 0.453125, 1.0, 0.46875, 0.46875, 1.0, 0.46875, 0.453125, 0.46875, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.453125, 0.46875, 0.453125, 0.453125, 0.4375, 0.453125, 0.46875, 0.46875, 0.453125, 0.4375, 0.46875, 0.46875, 0.46875, 0.484375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.4375, 0.46875, 0.453125, 0.453125, 1.0, 1.0, 0.46875, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.453125, 0.46875, 1.0, 1.0, 0.453125, 0.453125, 0.453125, 1.0, 0.453125, 0.4375, 0.46875, 0.46875, 0.4375, 0.453125, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4375, 0.4375, 0.46875, 0.46875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46875, 0.453125, 0.46875, 0.453125, 1.0, 0.4375, 0.46875, 1.0, 0.4375, 0.46875, 1.0]

 sparsity of   [1.0, 0.34375, 0.390625, 0.359375, 0.34375, 0.359375, 0.375, 0.359375, 1.0, 0.375, 0.34375, 0.40625, 0.375, 0.34375, 1.0, 0.34375, 0.34375, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.515625, 1.0, 0.40625, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.34375, 1.0, 0.390625, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.390625, 0.359375, 1.0, 0.359375, 1.0, 0.375, 0.34375, 0.375, 0.34375, 0.359375, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.390625, 0.34375, 1.0, 0.34375, 0.34375, 0.359375, 0.375, 0.375, 0.390625, 1.0, 0.359375, 0.34375, 0.421875, 0.40625, 0.421875, 1.0, 0.375, 0.359375, 0.453125, 0.34375, 0.34375, 0.359375, 1.0, 0.359375, 1.0, 0.375, 1.0, 0.359375, 0.375, 1.0, 0.359375, 0.375, 0.359375, 0.34375, 0.359375, 0.375, 0.359375, 0.453125, 1.0, 0.34375, 0.359375, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.359375, 0.375, 0.359375, 0.359375, 1.0, 0.359375, 0.34375, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.34375, 0.34375, 1.0, 0.375, 0.359375, 1.0, 0.359375, 1.0, 0.359375, 0.375, 0.390625, 0.390625, 0.375, 0.34375, 0.390625, 0.34375, 0.34375, 1.0, 0.359375, 1.0, 0.359375, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.34375, 0.40625, 1.0, 0.375, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 1.0, 0.34375, 0.359375, 0.375, 1.0, 0.4375, 0.375, 1.0, 0.34375, 0.359375, 0.359375, 0.34375, 0.359375, 0.34375, 0.375, 0.359375, 0.34375, 0.359375, 0.34375, 0.359375, 0.359375, 0.359375, 0.375, 0.359375, 0.359375, 0.375, 0.359375, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 0.375, 0.359375, 0.34375, 0.375, 1.0, 1.0, 0.359375, 0.34375, 0.34375, 0.34375, 0.34375, 0.359375, 0.359375, 0.34375, 0.375, 1.0, 1.0, 0.375, 0.421875, 0.359375, 1.0, 0.359375, 0.375, 0.34375, 0.359375, 0.34375, 0.34375, 1.0, 1.0, 0.40625, 0.359375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 0.375, 0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 0.34375, 0.359375, 0.359375, 1.0, 0.4375, 0.359375, 1.0, 1.0, 0.375, 0.359375]

 sparsity of   [0.25390625, 0.2421875, 0.23828125, 0.46484375, 1.0, 1.0, 0.23046875, 1.0, 0.23828125, 1.0, 0.265625, 0.25, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.234375, 1.0, 1.0, 1.0, 0.23828125, 1.0, 0.234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 0.2578125, 1.0, 0.23828125, 1.0, 1.0, 0.25390625, 1.0, 1.0, 0.2421875, 0.2265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2578125, 0.2578125, 1.0, 0.25, 0.23046875, 0.234375, 0.234375, 0.234375, 1.0, 0.25390625, 1.0, 0.25, 0.2578125, 0.25, 0.26171875, 1.0, 0.26171875, 0.25390625, 0.25]

 sparsity of   [0.4826388955116272, 1.0, 0.4791666567325592, 1.0, 1.0, 0.4809027910232544, 0.4809027910232544, 1.0, 0.487847238779068, 1.0, 0.4826388955116272, 0.4791666567325592, 0.5, 1.0, 1.0, 0.4895833432674408, 1.0, 0.487847238779068, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 0.472222238779068, 0.4861111044883728, 0.4826388955116272, 0.4861111044883728, 1.0, 0.484375, 1.0, 0.4826388955116272, 1.0, 0.4895833432674408, 1.0, 0.4791666567325592, 1.0, 0.4947916567325592, 1.0, 1.0, 0.4861111044883728, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4895833432674408, 0.484375, 1.0, 1.0, 0.4809027910232544, 1.0, 0.4826388955116272, 1.0, 0.4965277910232544, 1.0, 0.4791666567325592, 1.0, 1.0, 0.487847238779068, 0.487847238779068]

 sparsity of   [0.53125, 0.515625, 0.515625, 0.53125, 0.5, 0.53125, 0.484375, 0.515625, 0.53125, 0.53125, 0.5625, 0.53125, 0.515625, 0.5, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.546875, 0.484375, 1.0, 0.515625, 0.53125, 0.5, 1.0, 1.0, 0.484375, 0.5, 0.515625, 0.515625, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.515625, 0.484375, 0.515625, 0.5, 0.484375, 1.0, 0.5, 0.515625, 1.0, 0.5, 0.5, 0.453125, 0.5, 1.0, 0.515625, 0.515625, 0.5, 0.53125, 1.0, 0.53125, 1.0, 0.5, 1.0, 0.515625, 0.515625, 0.5, 1.0, 0.5, 1.0, 0.53125, 0.5, 0.5, 0.515625, 1.0, 0.5, 1.0, 0.515625, 0.515625, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.53125, 0.484375, 0.515625, 0.515625, 0.5, 0.484375, 0.46875, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.484375, 0.5, 1.0, 0.515625, 0.515625, 0.515625, 0.53125, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.53125, 0.5, 1.0, 0.484375, 0.53125, 0.515625, 0.5, 0.53125, 0.515625, 1.0, 0.5, 0.515625, 1.0, 0.46875, 0.46875, 1.0, 1.0, 0.46875, 0.546875, 0.515625, 1.0, 0.53125, 1.0, 1.0, 0.53125, 0.5, 0.484375, 0.484375, 0.546875, 0.5, 0.515625, 0.53125, 0.515625, 0.53125, 0.515625, 0.5, 0.5, 1.0, 1.0, 0.46875, 0.515625, 0.515625, 1.0, 0.515625, 0.515625, 0.546875, 0.484375, 0.515625, 0.53125, 1.0, 0.5, 0.484375, 0.515625, 0.484375, 0.484375, 0.515625, 1.0, 1.0, 1.0, 0.515625, 1.0, 0.546875, 0.515625, 0.515625, 0.515625, 0.515625, 0.5625, 0.484375, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.5, 0.53125, 0.515625, 0.5, 0.484375, 0.515625, 0.484375, 1.0, 0.515625, 0.515625, 0.5, 0.484375, 0.46875, 0.5, 0.515625, 1.0, 0.5, 0.5, 0.515625, 1.0, 0.53125, 0.5, 1.0, 0.53125, 0.5, 0.546875, 0.515625, 0.5, 1.0, 1.0, 0.53125, 0.5, 0.5, 0.546875, 0.5, 0.484375, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.484375, 0.53125, 0.5, 0.53125, 0.515625, 0.515625, 0.484375, 0.515625, 0.515625, 1.0, 1.0, 0.5625, 1.0, 1.0, 0.484375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 0.46875, 0.5, 0.515625, 0.5, 1.0, 1.0, 1.0, 0.515625, 0.515625, 0.5, 0.484375, 0.53125, 1.0, 0.5, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.1171875, 1.0, 1.0, 0.11328125, 1.0, 0.10546875, 0.13671875, 0.14453125, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.10546875, 0.10546875, 1.0, 0.12890625, 1.0, 1.0, 0.1015625, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 1.0, 0.140625, 0.13671875, 0.1171875, 0.12890625, 0.12890625, 1.0, 0.13671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.109375, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.11328125, 0.12890625, 1.0, 0.12890625, 0.1015625, 0.1171875, 1.0, 0.15234375, 0.10546875, 0.109375, 0.12890625, 0.109375]

 sparsity of   [1.0, 0.5121527910232544, 1.0, 1.0, 0.5121527910232544, 0.5121527910232544, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.5121527910232544, 0.5208333134651184, 1.0, 0.5104166865348816, 1.0, 0.5, 1.0, 0.5121527910232544, 0.5173611044883728, 1.0, 0.5, 0.6579861044883728, 0.5034722089767456, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 0.5190972089767456, 1.0, 0.5086805820465088, 0.5, 0.5173611044883728, 1.0, 1.0, 1.0, 0.5121527910232544, 0.5034722089767456, 0.5138888955116272, 1.0, 0.5052083134651184, 0.5104166865348816, 0.515625, 1.0, 0.5086805820465088, 0.5034722089767456, 0.5208333134651184, 0.5052083134651184, 1.0, 0.5017361044883728, 0.5052083134651184, 1.0, 1.0, 0.5017361044883728, 0.5277777910232544, 1.0, 0.4982638955116272, 1.0, 0.5190972089767456]

 sparsity of   [0.4375, 0.46875, 1.0, 1.0, 0.453125, 1.0, 0.4375, 0.4375, 0.453125, 0.484375, 0.4375, 0.4375, 0.4375, 0.453125, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 1.0, 1.0, 1.0, 0.453125, 0.4375, 1.0, 0.453125, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.4375, 0.4375, 1.0, 1.0, 0.453125, 0.4375, 0.46875, 0.4375, 0.46875, 0.453125, 0.453125, 0.46875, 1.0, 0.46875, 0.453125, 0.4375, 0.4375, 0.453125, 0.46875, 0.453125, 1.0, 0.46875, 0.4375, 0.4375, 0.453125, 0.46875, 0.453125, 0.4375, 0.484375, 0.46875, 0.453125, 0.453125, 1.0, 1.0, 0.4375, 0.4375, 0.5, 0.453125, 1.0, 0.5, 0.453125, 0.4375, 0.4375, 0.46875, 0.4375, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 1.0, 0.453125, 0.453125, 0.453125, 0.453125, 0.46875, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 0.484375, 1.0, 0.4375, 1.0, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.453125, 1.0, 0.46875, 0.4375, 1.0, 1.0, 0.453125, 0.453125, 0.4375, 1.0, 0.453125, 1.0, 0.453125, 0.453125, 0.4375, 1.0, 0.4375, 0.453125, 0.4375, 0.453125, 0.453125, 0.453125, 0.4375, 1.0, 0.4375, 0.46875, 1.0, 0.453125, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.46875, 0.453125, 0.453125, 0.453125, 1.0, 0.4375, 0.453125, 0.46875, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.4375, 0.453125, 0.453125, 1.0, 0.4375, 0.5, 0.4375, 0.4375, 0.4375, 0.453125, 0.46875, 1.0, 0.4375, 0.453125, 0.453125, 0.5, 0.484375, 0.4375, 0.46875, 0.4375, 0.5, 0.453125, 0.4375, 0.4375, 0.4375, 0.453125, 0.4375, 0.4375, 1.0, 0.453125, 1.0, 0.46875, 0.453125, 0.453125, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.453125, 0.484375, 0.4375, 0.453125, 0.4375, 0.4375, 0.453125, 0.4375, 0.4375, 0.4375, 1.0, 0.453125, 0.4375, 0.4375, 0.453125, 1.0, 0.4375, 0.453125, 0.4375, 0.4375, 1.0, 0.46875, 1.0, 1.0, 0.453125, 0.4375, 0.484375, 1.0, 1.0, 1.0, 0.46875, 0.4375, 1.0, 0.453125, 0.4375, 1.0, 0.453125, 0.453125, 0.453125, 0.453125, 1.0, 0.453125, 0.4375, 0.4375, 0.4375, 1.0]

 sparsity of   [0.0625, 1.0, 1.0, 1.0, 0.02734375, 0.25, 1.0, 0.0390625, 0.03515625, 0.0390625, 0.046875, 0.0234375, 1.0, 0.0234375, 0.03125, 1.0, 0.01953125, 1.0, 1.0, 0.05078125, 1.0, 0.03125, 1.0, 1.0, 1.0, 0.03515625, 0.02734375, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.0390625, 1.0, 1.0, 0.0390625, 0.04296875, 0.03515625, 1.0, 0.03125, 0.0390625, 1.0, 0.04296875, 0.04296875, 1.0, 0.04296875, 1.0, 0.03125, 0.0390625, 1.0, 1.0, 1.0, 0.0234375, 1.0, 0.03125, 1.0, 0.02734375, 1.0, 0.03125, 1.0, 1.0, 0.03515625, 0.03125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.0390625, 0.09375, 0.03515625, 1.0, 0.02734375, 1.0, 0.04296875, 0.04296875, 1.0, 1.0, 0.03125, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 0.0390625, 0.03125, 0.04296875, 0.02734375, 1.0, 1.0, 1.0, 1.0, 0.03515625, 0.03125, 1.0, 1.0, 0.03515625, 1.0, 1.0, 0.03515625, 0.03125]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.5251736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5329861044883728, 1.0, 1.0, 1.0, 0.5034722089767456, 1.0, 1.0, 0.480034738779068, 1.0, 0.5026041865348816, 0.5043402910232544, 0.506944477558136, 1.0, 1.0, 1.0, 0.5043402910232544, 1.0, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5121527910232544, 1.0, 0.5121527910232544, 1.0, 1.0, 1.0, 0.5581597089767456, 1.0, 0.5208333134651184, 1.0, 1.0, 1.0, 0.5112847089767456, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 1.0, 1.0, 1.0, 1.0, 0.5043402910232544, 0.5190972089767456, 0.5121527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 0.5026041865348816, 0.5, 1.0, 1.0, 0.5277777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5078125, 1.0, 0.53125, 1.0, 0.4939236044883728, 1.0, 0.4904513955116272, 0.5, 0.5407986044883728, 1.0, 0.5086805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5017361044883728, 1.0, 0.5208333134651184, 0.5321180820465088, 1.0, 1.0, 1.0, 0.5286458134651184, 0.4991319477558136, 0.484375, 1.0, 0.5251736044883728, 1.0, 1.0, 0.4921875, 1.0, 1.0, 0.4982638955116272, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.671875, 0.6875, 0.7109375, 0.6796875, 0.6875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6796875, 0.6875, 0.671875, 1.0, 0.6875, 0.6796875, 0.6953125, 1.0, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6875, 0.671875, 0.6953125, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.6875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6953125, 0.6796875, 1.0, 0.671875, 0.6796875, 0.671875, 0.6953125, 1.0, 0.671875, 0.6796875, 0.6796875, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 1.0, 0.671875, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6796875, 0.6953125, 0.6796875, 0.671875, 1.0, 0.6796875, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.703125, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 0.6796875, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 0.6796875, 0.6875, 1.0, 0.6953125, 0.6796875, 1.0, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6796875, 0.6953125, 1.0, 0.671875, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6796875, 1.0, 1.0, 0.671875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 1.0, 0.6875, 0.703125, 0.6875, 1.0, 1.0, 0.6875, 0.671875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.671875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 0.671875, 0.6796875, 0.671875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.671875, 0.671875, 0.7109375, 1.0, 0.6875, 0.671875, 0.671875, 1.0, 0.703125, 0.6796875, 0.6875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.671875, 0.6953125, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.671875, 0.6875, 0.671875, 0.6875, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.6875, 0.6796875, 0.671875, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6875, 0.671875, 0.6875, 0.671875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.671875, 0.6953125, 1.0, 0.6875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.671875, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.671875, 0.6953125, 0.6953125, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 0.703125, 0.6875, 1.0, 0.6875, 1.0, 0.671875, 0.6875, 0.6796875, 1.0, 0.671875, 0.6796875, 0.6953125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6953125, 0.6796875, 1.0, 0.671875, 1.0, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.703125, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.6875, 0.6796875, 0.671875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.671875, 0.6796875, 0.7109375, 0.6796875, 0.6796875, 0.671875, 0.6875, 0.671875, 0.671875, 0.6875, 0.6875, 0.703125, 1.0, 0.671875, 0.6796875, 0.671875, 0.6953125, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.671875, 0.6796875, 1.0, 0.6953125, 1.0, 0.6796875, 0.6953125, 1.0, 0.6953125, 0.6796875, 0.671875, 0.671875, 0.6875, 0.671875, 0.6796875, 0.6796875, 0.703125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6796875, 0.6953125, 0.671875, 1.0, 0.671875, 1.0, 0.671875, 1.0, 0.6875, 0.6953125, 1.0, 0.671875, 1.0, 0.703125, 0.671875, 0.6953125, 0.6875, 0.671875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.671875, 0.6875, 0.671875, 1.0, 0.6796875, 0.6953125, 0.7109375, 1.0, 1.0, 0.6875, 0.6796875, 0.671875, 0.671875, 0.6796875, 1.0, 0.671875, 1.0, 0.671875, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.671875, 0.671875, 1.0, 0.671875, 0.671875, 0.6796875, 0.6875, 1.0]

 sparsity of   [0.03125, 0.03125, 0.0390625, 0.04296875, 0.04296875, 0.04296875, 1.0, 1.0, 0.03515625, 0.0390625, 0.0625, 0.0390625, 1.0, 0.0625, 0.04296875, 0.03125, 1.0, 1.0, 0.046875, 0.04296875, 0.03125, 0.03515625, 0.03515625, 1.0, 0.046875, 0.0546875, 0.04296875, 1.0, 0.0546875, 1.0, 1.0, 0.05859375, 0.03515625, 1.0, 0.04296875, 1.0, 0.05859375, 0.03515625, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 0.0390625, 1.0, 1.0, 0.04296875, 0.046875, 0.03515625, 1.0, 0.04296875, 1.0, 0.03515625, 0.0703125, 1.0, 0.046875, 0.03515625, 0.03515625, 0.03515625, 0.03515625, 0.046875, 0.078125, 1.0, 1.0, 0.046875, 0.08203125, 0.0390625, 1.0, 1.0, 1.0, 0.0390625, 0.08203125, 0.03515625, 0.07421875, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 0.04296875, 0.03515625, 0.03515625, 0.046875, 0.05859375, 0.05859375, 0.03515625, 0.0390625, 0.0390625, 1.0, 0.05859375, 0.046875, 1.0, 0.046875, 0.0546875, 1.0, 0.0546875, 0.0390625, 1.0, 1.0, 0.03125, 1.0, 1.0, 1.0, 0.03515625, 0.04296875, 0.09375, 0.03515625, 0.04296875, 0.04296875, 0.04296875, 1.0, 1.0, 0.03125, 0.046875, 0.046875, 0.046875, 0.046875, 0.0859375, 1.0, 0.04296875, 1.0, 0.03515625, 0.0546875, 1.0, 0.03515625, 0.03515625, 1.0, 0.0390625, 0.03125, 0.046875, 1.0, 0.0390625, 0.05078125, 1.0, 0.078125, 0.0390625, 1.0, 0.03515625, 0.03515625, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.0390625, 0.03515625, 1.0, 0.03515625, 0.046875, 0.04296875, 0.04296875, 0.09375, 1.0, 1.0, 0.03515625, 0.06640625, 1.0, 0.04296875, 1.0, 0.0390625, 0.04296875, 0.02734375, 1.0, 0.046875, 0.04296875, 0.03515625, 1.0, 0.046875, 0.05078125, 0.04296875, 1.0, 0.0390625, 0.0546875, 0.05859375, 0.03125, 1.0, 1.0, 0.0546875, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 0.0390625, 0.03515625, 1.0, 0.1015625, 0.03515625, 0.046875, 0.04296875, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.03515625, 0.046875, 1.0, 1.0, 0.04296875, 1.0, 0.04296875, 0.0390625, 0.04296875, 0.05078125, 0.05078125, 1.0, 0.046875, 0.0390625, 0.04296875, 1.0, 0.04296875, 0.04296875, 0.03515625, 0.04296875, 0.0390625, 1.0, 0.0546875, 0.046875, 1.0, 1.0, 0.0546875, 0.03515625, 0.04296875, 1.0, 0.0546875, 0.125, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.0390625, 0.05859375, 0.09765625, 0.12890625, 0.04296875, 0.04296875, 0.03515625, 0.0390625, 0.0390625, 0.04296875, 0.0390625, 0.0546875, 1.0, 0.03125, 0.03125, 0.0625, 0.05078125, 0.03515625, 0.046875, 0.04296875, 1.0, 0.0625, 0.04296875, 1.0, 0.03515625, 0.078125, 1.0, 0.078125, 0.0390625, 0.05859375, 1.0, 0.05078125, 1.0, 0.046875, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 0.03125, 0.04296875, 1.0, 0.0390625, 0.03125, 0.05078125, 0.03515625, 0.05078125, 0.0390625, 1.0, 0.0390625, 0.03515625, 0.04296875, 1.0, 0.04296875, 1.0, 1.0, 0.04296875, 0.08984375, 0.0234375, 0.0390625, 0.0390625, 1.0, 1.0, 0.03125, 0.0390625, 0.0546875, 0.04296875, 1.0, 1.0, 0.0625, 0.05078125, 0.05078125, 0.03125, 0.05078125, 1.0, 1.0, 0.03125, 1.0, 0.04296875, 1.0, 0.04296875, 1.0, 0.0390625, 0.04296875, 1.0, 1.0, 0.03125, 0.0390625, 0.03515625, 1.0, 0.03125, 1.0, 0.03125, 0.0390625, 0.046875, 1.0, 0.03515625, 0.0546875, 0.0390625, 0.0390625, 1.0, 1.0, 0.0546875, 1.0, 0.03515625, 0.15234375, 1.0, 0.0390625, 1.0, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.046875, 1.0, 1.0, 1.0, 0.03125, 1.0, 0.03125, 0.04296875, 0.0390625, 0.03515625, 0.03125, 0.0390625, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.0625, 0.046875, 0.046875, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.0390625, 0.05078125, 0.0390625, 0.03515625, 0.0390625, 0.0390625, 0.02734375, 0.05078125, 0.046875, 0.03515625, 0.04296875, 0.04296875, 0.0390625, 0.046875, 1.0, 0.13671875, 0.0390625, 0.05078125, 0.04296875, 1.0, 1.0, 0.046875, 0.03515625, 1.0, 0.0390625, 0.0390625, 1.0, 0.0234375, 1.0, 0.09765625, 0.0390625, 1.0, 0.04296875, 0.046875, 0.04296875, 0.078125, 0.05078125, 0.03515625, 0.05078125, 0.0390625, 0.0546875, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.046875, 1.0, 0.11328125, 1.0, 1.0, 0.0390625, 0.0625, 1.0, 1.0, 1.0, 0.0390625, 1.0, 0.04296875, 0.04296875, 0.05078125, 0.046875, 1.0, 0.04296875, 1.0, 0.04296875, 0.046875, 0.05078125, 0.05078125, 1.0, 0.03125, 1.0, 0.0390625, 0.0390625, 0.03515625, 0.04296875, 0.04296875, 0.03515625, 0.04296875, 0.03515625, 0.046875, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.03515625, 1.0, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 0.1640625, 1.0, 0.0390625, 0.04296875, 0.04296875, 0.04296875, 0.0859375, 0.09765625, 0.046875, 1.0, 1.0, 0.03515625, 1.0, 0.04296875, 0.03125, 1.0, 1.0, 0.04296875, 1.0, 0.03515625, 0.03515625, 1.0, 0.12109375, 0.0390625, 0.06640625, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 0.046875, 0.05859375, 1.0, 0.03515625, 0.08203125, 0.04296875, 0.03515625, 1.0]

 sparsity of   [1.0, 0.30078125, 0.30078125, 0.30859375, 0.314453125, 0.3046875, 1.0, 0.30859375, 1.0, 0.310546875, 0.30859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.314453125, 0.30859375, 1.0, 0.318359375, 1.0, 1.0, 0.314453125, 0.310546875, 1.0, 1.0, 1.0, 1.0, 0.306640625, 0.306640625, 1.0, 0.306640625, 0.302734375, 1.0, 0.3203125, 1.0, 1.0, 0.30078125, 1.0, 0.318359375, 0.3046875, 0.31640625, 1.0, 1.0, 1.0, 0.30859375, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.310546875, 0.3125, 0.302734375, 1.0, 0.318359375, 0.31640625, 1.0, 1.0, 0.322265625, 0.3125, 0.306640625, 0.3125, 0.30859375, 1.0, 1.0, 0.318359375, 0.3046875, 1.0, 0.318359375, 1.0, 1.0, 0.30859375, 0.3125, 1.0, 0.30859375, 0.3125, 0.3125, 0.310546875, 1.0, 1.0, 0.306640625, 0.30859375, 0.3046875, 0.3046875, 0.306640625, 1.0, 0.30859375, 0.3046875, 1.0, 0.3125, 1.0, 1.0, 0.310546875, 1.0, 0.310546875, 0.306640625, 0.302734375, 0.310546875, 0.31640625, 0.306640625, 1.0, 0.302734375, 0.314453125, 0.31640625, 1.0, 1.0, 1.0, 0.310546875, 0.322265625, 1.0, 1.0, 0.310546875, 1.0, 0.306640625, 0.314453125, 0.302734375, 0.302734375, 0.3046875, 1.0, 1.0, 0.310546875, 1.0, 0.30859375, 1.0, 0.30859375, 0.30859375, 0.3125]

 sparsity of   [0.4149305522441864, 1.0, 0.409722238779068, 1.0, 0.4140625, 1.0, 0.4131944477558136, 1.0, 1.0, 1.0, 0.4192708432674408, 1.0, 0.4157986044883728, 1.0, 1.0, 0.4105902910232544, 0.4184027910232544, 0.4192708432674408, 1.0, 1.0, 0.4192708432674408, 0.4123263955116272, 0.4201388955116272, 0.4140625, 1.0, 0.4123263955116272, 1.0, 0.4157986044883728, 1.0, 1.0, 0.4149305522441864, 1.0, 1.0, 1.0, 0.4140625, 0.409722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4105902910232544, 0.4192708432674408, 0.4140625, 1.0, 0.4184027910232544, 1.0, 0.4140625, 0.4236111044883728, 0.409722238779068, 0.417534738779068, 1.0, 1.0, 1.0, 0.4149305522441864, 1.0, 0.4114583432674408, 0.4184027910232544, 0.4131944477558136, 1.0, 0.4149305522441864, 1.0, 0.4201388955116272, 1.0, 0.417534738779068, 1.0, 1.0, 0.4105902910232544, 0.4166666567325592, 1.0, 1.0, 0.4123263955116272, 0.417534738779068, 1.0, 1.0, 0.4210069477558136, 1.0, 1.0, 1.0, 1.0, 0.4123263955116272, 0.4227430522441864, 0.4131944477558136, 0.417534738779068, 0.4184027910232544, 0.4166666567325592, 1.0, 1.0, 0.417534738779068, 0.4131944477558136, 0.4140625, 0.417534738779068, 0.4149305522441864, 1.0, 0.4157986044883728, 0.4210069477558136, 1.0, 1.0, 0.4149305522441864, 1.0, 1.0, 0.4149305522441864, 0.4131944477558136, 1.0, 1.0, 0.4114583432674408, 1.0, 1.0, 0.417534738779068, 1.0, 1.0, 0.417534738779068, 0.4157986044883728, 1.0, 1.0, 1.0, 0.4140625, 0.4149305522441864, 1.0, 0.417534738779068, 1.0, 0.4140625, 0.4123263955116272, 1.0, 0.4123263955116272]

 sparsity of   [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 1.0, 0.5078125, 0.5, 0.5, 0.5078125, 0.5078125, 0.515625, 1.0, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.4921875, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 0.4921875, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 0.5, 1.0, 0.4921875, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.4921875, 0.4921875, 0.4921875, 1.0, 1.0, 1.0, 0.4921875, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5078125, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5078125, 0.5, 0.5, 0.5078125, 0.5, 0.4921875, 0.4921875, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.4921875, 0.4921875, 1.0, 0.5, 1.0, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.5078125, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5078125, 0.5078125, 1.0, 0.4921875, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5078125, 1.0, 1.0, 1.0, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.515625, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5078125, 1.0, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 1.0, 1.0, 0.4921875, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 0.4921875, 1.0, 0.5078125, 1.0, 0.4921875, 1.0, 0.5, 0.4921875, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5078125, 0.5, 0.5, 0.5078125, 1.0, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 1.0, 0.5, 0.5078125, 0.515625, 0.4921875, 0.5, 0.5, 1.0, 1.0, 0.4921875, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5078125, 0.4921875, 0.5078125, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5078125, 1.0, 1.0, 0.5, 1.0, 0.4921875, 1.0, 0.5, 0.5078125, 0.4921875, 0.4921875, 0.5, 0.515625, 0.4921875, 0.5, 0.5078125, 0.5, 0.5, 0.5, 0.5, 1.0, 0.4921875, 0.5, 1.0, 0.5, 0.5, 0.5078125, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.5, 1.0, 0.515625, 0.4921875, 0.5, 0.5078125, 0.5078125, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.4921875, 0.515625, 0.5, 1.0, 0.4921875, 1.0, 0.5, 0.5078125, 0.5078125, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4921875, 0.5, 1.0, 1.0, 0.5, 0.5078125, 1.0, 0.4921875, 0.5, 0.5, 0.5, 0.4921875, 0.5, 0.5, 0.4921875, 0.5078125, 0.5, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 0.4921875, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5, 0.4921875, 0.5, 0.4921875, 0.5078125, 0.515625, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5078125, 0.5, 1.0, 0.5078125, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5078125, 0.5, 0.4921875, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.4921875, 0.5, 0.5078125, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5078125, 0.5, 0.5, 0.5, 1.0, 0.5, 0.4921875, 0.4921875, 0.5, 0.5, 0.4921875, 1.0, 0.5, 0.4921875, 0.5078125, 0.5, 0.5078125, 0.5078125, 0.5, 0.4921875, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.515625, 0.5, 0.4921875, 0.4921875, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5078125, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.4921875, 0.4921875, 0.5078125, 1.0, 0.5078125, 0.5, 1.0, 0.4921875, 0.4921875, 0.4921875, 1.0, 0.5078125, 0.5078125, 0.5, 0.5078125, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.220703125, 1.0, 1.0, 0.216796875, 1.0, 0.203125, 0.19921875, 0.197265625, 0.201171875, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.205078125, 1.0, 0.19921875, 0.21484375, 1.0, 0.2578125, 1.0, 1.0, 1.0, 0.2109375, 1.0, 0.203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20703125, 1.0, 1.0, 0.205078125, 0.20703125, 1.0, 0.201171875, 1.0, 1.0, 1.0, 1.0, 0.205078125, 0.203125, 0.251953125, 0.205078125, 1.0, 1.0, 1.0, 1.0, 0.205078125, 1.0, 0.212890625, 1.0, 1.0, 0.19921875, 0.201171875, 1.0, 0.201171875, 1.0, 1.0, 0.21484375, 0.201171875, 1.0, 1.0, 0.1953125, 1.0, 0.26171875, 1.0, 1.0, 1.0, 0.20703125, 1.0, 0.203125, 0.203125, 0.205078125, 1.0, 1.0, 0.4765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 0.205078125, 1.0, 0.23828125, 1.0, 0.1953125, 0.20703125, 1.0, 0.193359375, 0.19921875, 0.197265625, 1.0, 1.0, 1.0, 1.0, 0.1953125, 1.0, 1.0, 1.0, 0.21484375, 0.21875, 1.0, 1.0, 1.0, 1.0, 0.2109375, 1.0, 1.0, 0.208984375, 1.0, 1.0, 0.19921875, 0.201171875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 1.0, 0.5355902910232544, 1.0, 0.5486111044883728, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5911458134651184, 0.5486111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5842013955116272, 0.5842013955116272, 1.0, 0.6545138955116272, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5581597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5529513955116272, 1.0, 0.6701388955116272, 0.5425347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5347222089767456, 0.5486111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 0.5633680820465088, 0.5451388955116272, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 1.0, 1.0]

 sparsity of   [0.765625, 0.765625, 0.7578125, 0.75, 0.7578125, 0.7734375, 1.0, 1.0, 0.75, 0.7578125, 1.0, 0.765625, 0.75, 1.0, 0.75, 1.0, 0.765625, 0.765625, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 0.75, 0.7578125, 1.0, 1.0, 0.765625, 0.765625, 0.7578125, 1.0, 1.0, 0.7734375, 0.7578125, 1.0, 0.7578125, 1.0, 0.75, 1.0, 0.75, 0.765625, 1.0, 0.75, 0.765625, 0.7578125, 1.0, 1.0, 0.796875, 1.0, 0.765625, 0.765625, 0.7578125, 0.75, 0.7578125, 1.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.7578125, 0.7578125, 0.765625, 0.75, 0.765625, 1.0, 0.75, 1.0, 0.78125, 0.75, 0.7734375, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.765625, 0.78125, 1.0, 0.7734375, 0.7578125, 0.765625, 1.0, 0.7578125, 0.765625, 0.7734375, 1.0, 0.75, 1.0, 0.765625, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 0.7578125, 1.0, 0.765625, 0.75, 0.7734375, 0.75, 0.75, 0.765625, 0.765625, 0.75, 0.75, 0.7578125, 0.75, 0.765625, 1.0, 1.0, 0.7734375, 0.75, 0.75, 1.0, 0.765625, 1.0, 0.7578125, 0.765625, 1.0, 0.75, 0.765625, 0.765625, 0.7734375, 0.765625, 0.75, 0.75, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.75, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.7578125, 0.75, 0.7578125, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 0.765625, 0.765625, 1.0, 0.75, 0.765625, 0.7578125, 0.7578125, 0.765625, 0.75, 0.765625, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 1.0, 0.7734375, 0.765625, 0.75, 0.765625, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.765625, 0.7578125, 0.75, 0.7578125, 0.7578125, 0.75, 0.7578125, 0.75, 0.765625, 0.796875, 0.765625, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.765625, 0.765625, 0.765625, 0.78125, 0.78125, 1.0, 0.75, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 1.0, 0.75, 0.765625, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 1.0, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.765625, 0.7578125, 0.75, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.765625, 0.75, 1.0, 0.75, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.7734375, 0.7734375, 0.765625, 0.75, 1.0, 0.75, 0.75, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.765625, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.75, 1.0, 0.765625, 1.0, 0.7578125, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 0.75, 0.75, 0.75, 0.765625, 0.765625, 0.75, 0.765625, 0.75, 1.0, 0.7578125, 0.765625, 0.75, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.7734375, 0.75, 0.75, 1.0, 0.7578125, 1.0, 0.765625, 0.7734375, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 0.765625, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.765625, 1.0, 0.796875, 0.7578125, 0.765625, 0.765625, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 0.765625, 0.765625, 0.765625, 1.0, 0.7578125, 0.75, 1.0, 0.7578125, 1.0, 0.765625, 0.765625, 1.0, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.75, 1.0, 0.765625, 0.75, 0.7578125, 0.7578125, 0.75, 0.7578125, 1.0, 0.75, 1.0, 0.765625, 0.75, 0.765625, 1.0, 1.0, 0.765625, 0.7578125, 0.7734375, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.75, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 0.75, 1.0, 0.75, 0.765625, 0.765625, 0.75, 0.765625, 0.7578125, 0.75, 1.0, 1.0, 0.7578125, 0.75, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.75, 0.7578125, 1.0, 0.765625, 1.0, 0.75, 1.0, 1.0, 0.7578125, 0.765625, 0.75, 1.0, 0.75, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 0.7734375, 0.765625, 0.7734375, 1.0, 0.765625, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.75, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.75]

 sparsity of   [1.0, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.115234375, 0.11328125, 0.119140625, 0.126953125, 0.115234375, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.125, 0.125, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.12890625, 0.123046875, 0.125, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 0.123046875, 0.119140625, 1.0, 0.134765625, 1.0, 0.119140625, 0.1171875, 0.115234375, 1.0, 1.0, 1.0, 0.125, 1.0, 0.115234375, 1.0, 1.0, 0.126953125, 1.0, 0.115234375, 1.0, 0.123046875, 0.12109375, 1.0, 1.0, 1.0, 0.125, 0.123046875, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.119140625, 0.125, 1.0, 1.0, 0.142578125, 1.0, 0.12109375, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 0.119140625, 0.115234375, 1.0, 0.123046875, 0.125, 1.0, 0.123046875, 1.0, 0.12109375, 1.0, 0.138671875, 0.1171875, 1.0, 1.0, 1.0, 0.123046875, 0.115234375, 0.123046875, 0.12890625, 1.0, 0.123046875, 1.0, 0.12109375, 1.0, 0.12890625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.538194477558136, 1.0, 1.0, 0.5503472089767456, 0.600694477558136, 1.0, 1.0, 1.0, 0.538194477558136, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 0.5503472089767456, 1.0, 1.0, 0.5416666865348816, 1.0, 0.5329861044883728, 1.0, 0.585069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5338541865348816, 1.0, 1.0, 1.0, 0.5477430820465088, 1.0, 1.0, 0.5425347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 1.0, 1.0, 0.5434027910232544, 0.546006977558136, 0.5373263955116272, 1.0, 0.5390625, 0.5494791865348816, 1.0, 1.0, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 0.5416666865348816, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.5503472089767456, 0.5842013955116272, 0.5963541865348816, 1.0, 1.0, 1.0, 0.5321180820465088, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546006977558136, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 1.0, 1.0, 1.0, 0.5503472089767456, 1.0, 0.5407986044883728, 1.0, 0.5564236044883728]

 sparsity of   [0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.71875, 0.7109375, 0.703125, 0.703125, 0.703125, 0.71875, 0.703125, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.7265625, 0.703125, 1.0, 0.7109375, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.71875, 0.7109375, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 1.0, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.71875, 0.7109375, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.71875, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.71875, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.734375, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 1.0, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 1.0, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.7265625, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.703125, 0.703125, 0.7109375, 0.703125, 1.0, 1.0, 0.703125, 0.7109375, 0.7109375, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.7109375, 0.703125, 0.703125, 1.0, 0.7265625, 0.703125, 0.71875, 0.703125, 1.0, 0.703125, 0.703125, 1.0, 1.0, 0.7109375, 1.0, 0.703125, 0.7109375, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.703125, 0.703125, 1.0, 0.7109375, 0.703125, 1.0, 0.7109375, 1.0, 0.7109375, 0.7109375, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.703125]

 sparsity of   [0.08203125, 1.0, 0.072265625, 0.078125, 1.0, 0.078125, 0.072265625, 1.0, 0.080078125, 0.072265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.07421875, 0.07421875, 1.0, 1.0, 0.076171875, 0.076171875, 1.0, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 0.07421875, 0.08984375, 0.07421875, 1.0, 0.080078125, 0.07421875, 1.0, 0.07421875, 1.0, 0.0703125, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.076171875, 0.078125, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.0859375, 0.076171875, 1.0, 1.0, 0.076171875, 1.0, 0.072265625, 1.0, 1.0, 0.080078125, 0.07421875, 0.078125, 1.0, 0.078125, 1.0, 0.068359375, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.08203125, 0.076171875, 0.07421875, 1.0, 1.0, 0.07421875, 0.072265625, 0.078125, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.080078125, 0.072265625, 0.080078125, 0.076171875, 0.080078125, 0.07421875, 1.0, 0.076171875, 0.068359375, 0.076171875, 1.0, 0.078125, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.076171875, 0.08203125, 0.072265625, 0.072265625, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 0.078125, 0.08203125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 0.07421875, 0.078125, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.068359375, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 0.072265625, 1.0, 1.0, 1.0, 0.076171875, 0.068359375, 1.0, 1.0, 0.072265625, 0.078125, 0.072265625, 1.0, 1.0, 0.080078125, 0.0703125, 0.080078125, 0.076171875, 1.0, 0.078125, 0.07421875, 0.083984375, 1.0, 0.08203125, 0.078125, 1.0, 1.0, 0.07421875, 0.080078125, 0.068359375, 0.076171875, 1.0, 1.0, 0.078125, 0.072265625, 0.07421875, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 1.0, 0.076171875, 1.0, 0.0703125, 0.072265625, 1.0, 1.0, 0.07421875, 0.076171875, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 0.078125, 0.080078125, 0.07421875, 0.078125, 1.0, 0.07421875, 0.072265625, 0.078125, 0.078125, 1.0, 0.072265625, 1.0, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 0.076171875, 1.0, 0.080078125, 1.0, 1.0, 1.0, 0.080078125, 0.068359375, 1.0, 0.083984375, 0.072265625, 0.076171875, 0.08203125, 0.08203125, 1.0, 1.0, 1.0, 0.076171875, 0.076171875, 1.0, 0.068359375, 0.078125, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 0.07421875, 1.0, 0.07421875, 1.0, 0.076171875, 0.072265625, 1.0]

 sparsity of   [1.0, 1.0, 0.4939236044883728, 1.0, 1.0, 1.0, 1.0, 0.4947916567325592, 0.4939236044883728, 0.4973958432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.495659738779068, 1.0, 0.4943576455116272, 1.0, 0.495659738779068, 1.0, 0.4934895932674408, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 0.4939236044883728, 0.49609375, 0.4947916567325592, 1.0, 0.4943576455116272, 0.4969618022441864, 1.0, 1.0, 0.4947916567325592, 0.4930555522441864, 1.0, 0.4943576455116272, 1.0, 0.4926215410232544, 1.0, 0.4934895932674408, 0.495659738779068, 1.0, 0.49609375, 1.0, 0.4934895932674408, 1.0, 0.4947916567325592, 0.4943576455116272, 1.0, 1.0, 0.495659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 0.4952256977558136, 0.4969618022441864, 0.4947916567325592, 1.0, 1.0, 1.0, 0.4978298544883728, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4934895932674408, 1.0, 0.495659738779068, 1.0, 1.0, 1.0, 1.0, 0.4947916567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.49609375, 0.49609375, 0.4921875, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.4965277910232544, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4930555522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4969618022441864, 1.0, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.49609375, 1.0, 1.0, 1.0, 0.4943576455116272, 0.49609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4991319477558136, 1.0, 0.4969618022441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.49609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4978298544883728, 1.0, 0.4934895932674408, 0.4939236044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4982638955116272, 1.0, 0.49609375, 0.4965277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 0.4939236044883728, 0.4943576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4952256977558136, 0.495659738779068, 0.49609375, 1.0, 1.0, 0.4939236044883728, 1.0, 1.0, 0.4934895932674408, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.73046875, 1.0, 0.72265625, 0.7265625, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.73046875, 0.72265625, 0.7265625, 0.73046875, 0.73046875, 0.73046875, 0.72265625, 1.0, 0.7421875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 0.734375, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 0.72265625, 0.72265625, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.734375, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 0.73046875, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 0.72265625, 1.0, 0.72265625, 0.72265625, 0.7265625, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.734375, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 0.73046875, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.73046875, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 0.72265625, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.73046875, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.734375, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 0.73046875, 0.734375, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.72265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 1.0, 0.73046875, 0.73046875, 0.73046875, 0.73046875, 1.0, 1.0, 0.72265625, 0.7265625, 0.72265625, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 0.73046875, 0.73828125, 0.73046875, 0.73046875, 0.72265625, 0.73046875, 1.0, 0.72265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.7265625, 0.72265625, 1.0, 0.734375, 0.7265625, 1.0, 1.0, 0.73046875, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 0.72265625, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.7265625, 0.7265625, 1.0, 0.72265625, 1.0, 0.7265625, 0.72265625, 0.72265625, 1.0, 1.0, 0.734375, 0.72265625, 0.72265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 0.7265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.72265625, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.73046875, 0.7265625, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7265625, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 0.73828125, 1.0, 0.7265625, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 0.7265625, 0.73046875, 0.73046875, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.7265625, 0.7265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73046875, 0.7265625, 0.72265625, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.72265625, 1.0, 0.72265625, 0.72265625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0703125, 0.111328125, 1.0, 0.076171875, 1.0, 0.080078125, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 0.078125, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 0.08984375, 1.0, 1.0, 0.087890625, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.08203125, 1.0, 1.0, 1.0, 0.1015625, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.080078125, 0.1484375, 0.076171875, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 0.078125, 0.08203125, 0.134765625, 1.0, 0.078125, 0.083984375, 0.07421875, 1.0, 0.076171875, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.08203125, 1.0, 1.0, 1.0, 0.080078125, 0.080078125, 1.0, 1.0, 0.080078125, 1.0, 1.0, 0.08203125, 1.0, 0.080078125, 0.099609375, 1.0, 0.083984375, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.07421875, 0.09375, 0.08203125, 0.076171875, 0.078125, 0.078125, 0.080078125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.0859375, 1.0, 0.08984375, 1.0, 0.080078125, 1.0, 1.0, 0.07421875, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.08203125, 0.08203125, 1.0, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 0.08203125, 1.0, 1.0, 0.078125, 0.08203125, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 0.083984375, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.09375, 1.0, 0.083984375, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.07421875, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1796875, 0.126953125, 0.09375, 1.0, 1.0, 0.0703125, 0.07421875, 1.0, 1.0, 1.0, 0.21484375, 0.087890625, 1.0, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.076171875, 1.0, 1.0, 0.072265625, 1.0, 1.0, 0.072265625, 0.083984375, 1.0, 0.087890625, 1.0, 1.0, 0.0859375, 0.08984375, 0.130859375, 0.08203125, 1.0, 0.09375, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.078125, 1.0, 0.091796875, 1.0, 1.0, 0.08203125, 0.076171875, 0.087890625, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 0.083984375, 1.0, 0.07421875, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.078125, 0.076171875, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.087890625, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.076171875, 0.072265625, 0.072265625, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 0.078125, 0.08203125, 0.078125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.080078125, 1.0, 0.076171875, 1.0, 1.0, 1.0, 0.087890625, 0.083984375, 0.078125, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.076171875, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.083984375, 1.0, 0.08203125, 1.0, 0.09375, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.0859375, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.083984375, 0.080078125, 0.080078125, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.087890625, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 0.091796875, 1.0, 1.0, 1.0, 0.08203125, 0.080078125, 1.0, 1.0, 0.087890625, 1.0, 0.078125, 0.07421875, 1.0, 0.09375, 0.080078125, 0.076171875, 0.078125, 0.080078125, 1.0, 0.078125, 0.08203125, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.08203125, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0859375, 1.0, 0.087890625, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.078125, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 0.0859375, 1.0, 0.08203125, 1.0, 1.0, 0.078125, 0.0859375, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.08203125, 1.0, 0.0859375, 1.0, 1.0, 0.083984375, 0.078125, 1.0, 0.076171875, 0.08203125, 0.087890625, 1.0, 1.0, 1.0, 1.0, 0.078125, 0.078125, 0.091796875, 0.076171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.072265625, 1.0, 1.0, 0.072265625, 1.0, 0.16015625, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 0.076171875, 0.083984375, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.08984375, 0.07421875, 0.080078125, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08203125, 1.0, 1.0, 0.07421875, 0.09375, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 0.083984375, 1.0, 1.0, 0.078125, 0.091796875, 1.0, 1.0, 0.07421875, 1.0, 0.083984375, 1.0, 1.0, 0.076171875, 0.08203125, 1.0, 0.083984375, 1.0, 1.0, 0.091796875, 0.080078125, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.080078125, 0.0859375, 1.0, 1.0, 0.087890625, 1.0, 1.0, 0.126953125, 0.095703125, 0.083984375, 0.08203125, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 0.087890625, 1.0, 0.091796875, 0.61328125, 1.0, 0.08984375, 1.0, 1.0, 0.078125, 0.080078125, 0.080078125, 0.087890625, 1.0, 0.08203125, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.078125, 0.083984375, 0.103515625, 1.0, 1.0, 1.0, 0.087890625, 1.0, 0.09375, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.0859375, 1.0, 1.0, 0.076171875, 0.115234375, 0.0859375, 1.0, 1.0, 1.0, 0.0859375, 0.083984375, 0.078125, 1.0, 1.0, 0.099609375, 0.08203125, 0.0703125, 0.07421875, 1.0, 1.0, 0.16796875, 0.0859375, 1.0, 0.080078125, 0.076171875, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.0859375, 0.091796875, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 0.078125, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.08203125, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 0.080078125, 0.08203125, 0.087890625, 0.08203125, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.072265625, 0.07421875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.099609375, 0.091796875, 0.080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.087890625, 0.072265625, 1.0, 0.07421875, 0.07421875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 0.6044921875, 0.6103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 0.607421875, 0.603515625, 0.6103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.60546875, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6025390625, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.60546875, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.60546875, 0.603515625, 1.0, 1.0, 0.6044921875, 0.6064453125, 1.0, 1.0, 0.6083984375, 0.60546875, 1.0, 0.6103515625, 1.0, 0.607421875, 0.6025390625, 1.0, 0.607421875, 0.6064453125, 1.0, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.611328125, 0.609375, 1.0, 1.0, 0.6005859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 0.6044921875, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.6044921875, 1.0, 0.6025390625, 0.603515625, 1.0, 0.6064453125, 1.0, 1.0, 0.6025390625, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.5947265625, 0.6044921875, 1.0, 1.0, 0.6025390625, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6044921875, 1.0, 0.603515625, 1.0, 0.607421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.599609375, 1.0, 1.0, 0.603515625, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6083984375, 1.0, 0.607421875, 0.609375, 1.0, 1.0, 1.0, 0.607421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7638888955116272, 1.0, 1.0, 1.0, 0.7699652910232544, 0.7703993320465088, 1.0, 0.7703993320465088, 0.765625, 0.7664930820465088, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7634548544883728, 1.0, 1.0, 1.0, 1.0, 0.7682291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7721354365348816, 0.7721354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76171875, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 0.7690972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 0.7651909589767456, 1.0, 0.7690972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7721354365348816, 1.0, 0.7638888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7703993320465088, 0.76171875, 1.0, 1.0, 1.0, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7717013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7682291865348816, 0.7625868320465088, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7638888955116272, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7586805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7708333134651184, 0.7643229365348816, 1.0, 1.0, 0.7699652910232544, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8125, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.82421875, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.82421875, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.8125, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.82421875, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82421875, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.8125, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8203125, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.82421875, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.82421875, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8125, 1.0, 1.0, 0.81640625, 0.8203125, 0.81640625, 0.8203125, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.8203125, 1.0, 0.82421875, 0.81640625, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.81640625, 0.81640625, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 0.8203125, 0.81640625, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 0.81640625, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8203125, 1.0, 0.81640625, 0.81640625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.5869140625, 0.583984375, 1.0, 1.0, 1.0, 0.5849609375, 0.5791015625, 1.0, 0.5849609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 0.5869140625, 1.0, 0.583984375, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 0.5849609375, 1.0, 1.0, 0.5849609375, 1.0, 0.583984375, 1.0, 0.58984375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.587890625, 0.5859375, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5849609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 0.5869140625, 1.0, 1.0, 1.0, 0.5888671875, 0.5791015625, 0.591796875, 1.0, 1.0, 0.587890625, 0.5908203125, 1.0, 0.5869140625, 1.0, 1.0, 1.0, 0.587890625, 1.0, 0.583984375, 0.58203125, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.587890625, 0.5830078125, 1.0, 1.0, 0.5888671875, 1.0, 0.5859375, 1.0, 1.0, 0.587890625, 1.0, 1.0, 1.0, 0.591796875, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.58203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 0.5849609375, 0.5830078125, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 1.0, 0.587890625, 1.0, 1.0, 0.58203125, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5869140625, 1.0, 0.5927734375, 1.0, 1.0, 0.59375, 0.5888671875, 0.58203125, 1.0, 0.5849609375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 0.5859375, 0.5927734375, 1.0, 0.5849609375, 0.58203125, 0.591796875, 1.0, 1.0, 0.5849609375, 0.5908203125, 0.591796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.58984375, 1.0, 0.5888671875, 1.0, 0.591796875, 1.0, 0.5927734375, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.5888671875, 1.0, 1.0, 0.587890625, 0.5869140625, 1.0, 1.0, 0.587890625, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 0.5869140625, 1.0, 1.0, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5927734375, 1.0, 1.0, 0.587890625, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 0.5849609375]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 1.0, 1.0, 0.6744791865348816, 1.0, 1.0, 1.0, 0.6731770634651184, 0.6727430820465088, 0.6740451455116272, 0.6727430820465088, 0.674913227558136, 1.0, 0.674913227558136, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 0.6731770634651184, 1.0, 0.674913227558136, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 0.6731770634651184, 1.0, 0.6766493320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6740451455116272, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 0.6731770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 0.6731770634651184, 1.0, 1.0, 0.6740451455116272, 1.0, 1.0, 1.0, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6731770634651184, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6723090410232544, 1.0, 1.0, 1.0, 1.0, 0.6744791865348816, 0.6740451455116272, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 0.6723090410232544, 1.0, 0.6744791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 0.67578125, 1.0, 0.6779513955116272, 0.6740451455116272, 0.6731770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.674913227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 0.6736111044883728, 0.6727430820465088, 0.6744791865348816, 0.6723090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.80078125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.7890625, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.78515625, 0.77734375, 1.0, 1.0, 1.0, 0.78125, 0.77734375, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78515625, 0.7890625, 1.0, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.78125, 1.0, 0.7890625, 0.79296875, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78515625, 0.7890625, 1.0, 0.77734375, 0.7890625, 1.0, 0.78125, 1.0, 1.0, 0.77734375, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78515625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7890625, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.77734375, 0.78515625, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 0.78515625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.7890625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.78515625, 1.0, 0.7890625, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.78125, 0.78515625, 0.79296875, 0.78125, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 0.78125, 0.78515625, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 0.79296875, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 0.78125, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.78515625, 0.7890625, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.79296875, 1.0, 1.0, 0.78515625, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.7890625, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 0.78515625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.79296875, 0.79296875, 0.78515625, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 0.7890625, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 0.77734375, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.7890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7890625, 1.0, 0.7890625, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.78515625, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79296875, 0.78125, 1.0, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7890625, 1.0, 0.78125, 0.77734375, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9869791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.587890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5908203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9871962070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5693359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6689453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 0.5888671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.580078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.576171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5810546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.978515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9794921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9924045205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9924045205116272, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.77783203125, 0.77783203125, 0.77734375, 0.77734375, 0.77783203125, 0.77734375, 0.77734375, 0.7783203125, 0.77734375, 0.77734375, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]

Total parameter pruned: 22728940.003513373 (unstructured) 21611437 (structured)

Test: [0/79]	Time 0.178 (0.178)	Loss 0.3081 (0.3081) ([0.186]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.850
current lr 1.00000e-03
Grad=  tensor(0.3095, device='cuda:0')
Epoch: [300][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [300][100/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0114 (0.0069) ([0.011]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [300][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0122 (0.0070) ([0.012]+[0.000])	Prec@1 100.000 (99.903)
Epoch: [300][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0070) ([0.004]+[0.000])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1849 (0.1849) ([0.185]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(0.2281, device='cuda:0')
Epoch: [301][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0095 (0.0067) ([0.009]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [301][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0069) ([0.008]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [301][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0070) ([0.006]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.2064 (0.2064) ([0.206]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.5558, device='cuda:0')
Epoch: [302][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0055 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [302][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0183 (0.0067) ([0.018]+[0.000])	Prec@1 99.219 (99.918)
Epoch: [302][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0067) ([0.005]+[0.000])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.2004 (0.2004) ([0.200]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.1472, device='cuda:0')
Epoch: [303][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0086 (0.0070) ([0.009]+[0.000])	Prec@1 99.219 (99.915)
Epoch: [303][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0111 (0.0073) ([0.011]+[0.000])	Prec@1 99.219 (99.907)
Epoch: [303][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0073) ([0.005]+[0.000])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1843 (0.1843) ([0.184]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.4039, device='cuda:0')
Epoch: [304][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0064) ([0.005]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [304][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0138 (0.0066) ([0.014]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [304][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0066) ([0.004]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1859 (0.1859) ([0.186]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(1.6903, device='cuda:0')
Epoch: [305][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0159 (0.0159) ([0.016]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [305][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0055 (0.0079) ([0.006]+[0.000])	Prec@1 100.000 (99.853)
Epoch: [305][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0070) ([0.002]+[0.000])	Prec@1 100.000 (99.883)
Epoch: [305][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0069) ([0.005]+[0.000])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1767 (0.1767) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.870
current lr 1.00000e-03
Grad=  tensor(0.1096, device='cuda:0')
Epoch: [306][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0067) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [306][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0066) ([0.005]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [306][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0069) ([0.006]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1690 (0.1690) ([0.169]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.1290, device='cuda:0')
Epoch: [307][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0069) ([0.004]+[0.000])	Prec@1 100.000 (99.869)
Epoch: [307][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0068) ([0.004]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [307][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0068) ([0.003]+[0.000])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1622 (0.1622) ([0.162]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.1322, device='cuda:0')
Epoch: [308][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0079 (0.0064) ([0.008]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [308][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0061) ([0.004]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [308][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0120 (0.0061) ([0.012]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1861 (0.1861) ([0.186]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(0.1395, device='cuda:0')
Epoch: [309][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0062) ([0.005]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [309][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0063) ([0.005]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [309][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1988 (0.1988) ([0.199]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.1440, device='cuda:0')
Epoch: [310][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0078) ([0.003]+[0.000])	Prec@1 100.000 (99.861)
Epoch: [310][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0074) ([0.005]+[0.000])	Prec@1 100.000 (99.883)
Epoch: [310][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0070) ([0.004]+[0.000])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1915 (0.1915) ([0.192]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.6340, device='cuda:0')
Epoch: [311][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0060 (0.0060) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0092 (0.0062) ([0.009]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [311][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0065) ([0.003]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [311][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0063) ([0.002]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1694 (0.1694) ([0.169]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.0622, device='cuda:0')
Epoch: [312][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0061) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [312][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0246 (0.0064) ([0.025]+[0.000])	Prec@1 99.219 (99.911)
Epoch: [312][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0063) ([0.012]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1803 (0.1803) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.0977, device='cuda:0')
Epoch: [313][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0062) ([0.005]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [313][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0095 (0.0060) ([0.010]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [313][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0063) ([0.003]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.2015 (0.2015) ([0.202]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.5384, device='cuda:0')
Epoch: [314][0/391]	Time 0.205 (0.205)	Data 0.133 (0.133)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0065 (0.0068) ([0.007]+[0.000])	Prec@1 100.000 (99.892)
Epoch: [314][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0062) ([0.002]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [314][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0063) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1753 (0.1753) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(1.8695, device='cuda:0')
Epoch: [315][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0111 (0.0111) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [315][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0059) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [315][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0061) ([0.005]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [315][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0060) ([0.005]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1699 (0.1699) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.1776, device='cuda:0')
Epoch: [316][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0104 (0.0060) ([0.010]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [316][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [316][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1702 (0.1702) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.2396, device='cuda:0')
Epoch: [317][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0110 (0.0057) ([0.011]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [317][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [317][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0061 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1891 (0.1891) ([0.189]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(0.1830, device='cuda:0')
Epoch: [318][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0060) ([0.007]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [318][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0077 (0.0061) ([0.008]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [318][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0113 (0.0061) ([0.011]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1970 (0.1970) ([0.197]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.1694, device='cuda:0')
Epoch: [319][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [319][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [319][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [319][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0199 (0.0056) ([0.020]+[0.000])	Prec@1 99.219 (99.943)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1785 (0.1785) ([0.179]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.2385, device='cuda:0')
Epoch: [320][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [320][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [320][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0055) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.2026 (0.2026) ([0.203]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(1.4613, device='cuda:0')
Epoch: [321][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0095 (0.0095) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [321][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [321][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0054) ([0.006]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1815 (0.1815) ([0.181]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.680
current lr 1.00000e-03
Grad=  tensor(0.5499, device='cuda:0')
Epoch: [322][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0096 (0.0056) ([0.010]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [322][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0058) ([0.001]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [322][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0059) ([0.003]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1957 (0.1957) ([0.196]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.3018, device='cuda:0')
Epoch: [323][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [323][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [323][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0128 (0.0055) ([0.013]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [323][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0054) ([0.009]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2034 (0.2034) ([0.203]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.0185, device='cuda:0')
Epoch: [324][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0089 (0.0059) ([0.009]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [324][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0058) ([0.003]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [324][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.2013 (0.2013) ([0.201]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(1.5013, device='cuda:0')
Epoch: [325][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0108 (0.0108) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [325][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [325][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1785 (0.1785) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.0545, device='cuda:0')
Epoch: [326][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [326][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [326][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0057) ([0.008]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.2164 (0.2164) ([0.216]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(0.9701, device='cuda:0')
Epoch: [327][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0050 (0.0056) ([0.005]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [327][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [327][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.2183 (0.2183) ([0.218]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.4702, device='cuda:0')
Epoch: [328][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0087 (0.0087) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0071 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [328][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [328][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.2241 (0.2241) ([0.224]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.700
current lr 1.00000e-03
Grad=  tensor(0.0232, device='cuda:0')
Epoch: [329][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [329][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [329][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0093 (0.0050) ([0.009]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.2142 (0.2142) ([0.214]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.870
current lr 1.00000e-03
Grad=  tensor(1.2981, device='cuda:0')
Epoch: [330][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0064 (0.0064) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [330][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [330][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1852 (0.1852) ([0.185]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.1249, device='cuda:0')
Epoch: [331][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [331][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0056) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [331][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1975 (0.1975) ([0.197]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.8330, device='cuda:0')
Epoch: [332][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0116 (0.0048) ([0.012]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [332][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [332][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0051) ([0.006]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1768 (0.1768) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(1.0781, device='cuda:0')
Epoch: [333][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0093 (0.0093) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [333][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [333][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0212 (0.0056) ([0.021]+[0.000])	Prec@1 99.219 (99.945)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.2153 (0.2153) ([0.215]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(0.6933, device='cuda:0')
Epoch: [334][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0085 (0.0085) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [334][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0053) ([0.010]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [334][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.2307 (0.2307) ([0.231]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-03
Grad=  tensor(0.4091, device='cuda:0')
Epoch: [335][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0075 (0.0075) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [335][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0060) ([0.002]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [335][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0057) ([0.002]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [335][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0055) ([0.002]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.2077 (0.2077) ([0.208]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(0.0220, device='cuda:0')
Epoch: [336][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0059 (0.0053) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [336][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0054) ([0.006]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [336][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.2179 (0.2179) ([0.218]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.3006, device='cuda:0')
Epoch: [337][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [337][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [337][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1986 (0.1986) ([0.199]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.0156, device='cuda:0')
Epoch: [338][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0093 (0.0052) ([0.009]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [338][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [338][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1874 (0.1874) ([0.187]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.0584, device='cuda:0')
Epoch: [339][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0082 (0.0051) ([0.008]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [339][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [339][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1987 (0.1987) ([0.199]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(0.4857, device='cuda:0')
Epoch: [340][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [340][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0086 (0.0046) ([0.009]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [340][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0146 (0.0048) ([0.015]+[0.000])	Prec@1 99.219 (99.974)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1910 (0.1910) ([0.191]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(1.5940, device='cuda:0')
Epoch: [341][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0085 (0.0085) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [341][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [341][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.2030 (0.2030) ([0.203]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.590
current lr 1.00000e-03
Grad=  tensor(0.1653, device='cuda:0')
Epoch: [342][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0104 (0.0053) ([0.010]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [342][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [342][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0091 (0.0051) ([0.009]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1826 (0.1826) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.940
current lr 1.00000e-03
Grad=  tensor(0.0739, device='cuda:0')
Epoch: [343][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0070 (0.0053) ([0.007]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [343][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0054) ([0.007]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [343][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.2015 (0.2015) ([0.201]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.1455, device='cuda:0')
Epoch: [344][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0072 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [344][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [344][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1900 (0.1900) ([0.190]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.4042, device='cuda:0')
Epoch: [345][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0068 (0.0046) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [345][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [345][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1903 (0.1903) ([0.190]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.0226, device='cuda:0')
Epoch: [346][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [346][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [346][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1659 (0.1659) ([0.166]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.0269, device='cuda:0')
Epoch: [347][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [347][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [347][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0061 (0.0053) ([0.006]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1738 (0.1738) ([0.174]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.720
current lr 1.00000e-03
Grad=  tensor(0.1182, device='cuda:0')
Epoch: [348][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0056 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [348][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [348][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.150 (0.150)	Loss 0.1683 (0.1683) ([0.168]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.0148, device='cuda:0')
Epoch: [349][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0075 (0.0046) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [349][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0046) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [349][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1739 (0.1739) ([0.174]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0386, device='cuda:0')
Epoch: [350][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0050) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [350][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0049) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [350][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1765 (0.1765) ([0.177]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.740
current lr 1.00000e-04
Grad=  tensor(0.1066, device='cuda:0')
Epoch: [351][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0046 (0.0042) ([0.005]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [351][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [351][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0042) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1838 (0.1838) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.0550, device='cuda:0')
Epoch: [352][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [352][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [352][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1720 (0.1720) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-04
Grad=  tensor(0.3478, device='cuda:0')
Epoch: [353][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0100 (0.0039) ([0.010]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [353][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [353][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0137 (0.0043) ([0.014]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1791 (0.1791) ([0.179]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(4.2911, device='cuda:0')
Epoch: [354][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0065 (0.0065) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0189 (0.0045) ([0.019]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [354][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [354][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0044) ([0.012]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1676 (0.1676) ([0.168]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-04
Grad=  tensor(0.4686, device='cuda:0')
Epoch: [355][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0058 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0083 (0.0043) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [355][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [355][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1656 (0.1656) ([0.166]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.860
current lr 1.00000e-04
Grad=  tensor(0.0796, device='cuda:0')
Epoch: [356][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [356][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [356][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1716 (0.1716) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-04
Grad=  tensor(0.3213, device='cuda:0')
Epoch: [357][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [357][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [357][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1706 (0.1706) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0312, device='cuda:0')
Epoch: [358][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0051 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [358][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [358][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1810 (0.1810) ([0.181]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.6017, device='cuda:0')
Epoch: [359][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0088 (0.0042) ([0.009]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [359][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [359][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1804 (0.1804) ([0.180]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.2117, device='cuda:0')
Epoch: [360][0/391]	Time 0.211 (0.211)	Data 0.141 (0.141)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [360][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [360][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0042) ([0.006]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1749 (0.1749) ([0.175]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0557, device='cuda:0')
Epoch: [361][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0078 (0.0038) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [361][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0106 (0.0038) ([0.011]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [361][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1619 (0.1619) ([0.162]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.0533, device='cuda:0')
Epoch: [362][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [362][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [362][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1685 (0.1685) ([0.169]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.970
current lr 1.00000e-04
Grad=  tensor(0.1691, device='cuda:0')
Epoch: [363][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0080 (0.0046) ([0.008]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [363][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0084 (0.0043) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [363][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1647 (0.1647) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-04
Grad=  tensor(0.2067, device='cuda:0')
Epoch: [364][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0044) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [364][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0092 (0.0044) ([0.009]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [364][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1657 (0.1657) ([0.166]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.830
current lr 1.00000e-04
Grad=  tensor(0.0517, device='cuda:0')
Epoch: [365][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0042) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [365][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [365][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1710 (0.1710) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-04
Grad=  tensor(0.5355, device='cuda:0')
Epoch: [366][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [366][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0044) ([0.007]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [366][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1771 (0.1771) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.0463, device='cuda:0')
Epoch: [367][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0078 (0.0039) ([0.008]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [367][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [367][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0088 (0.0042) ([0.009]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1741 (0.1741) ([0.174]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(1.4186, device='cuda:0')
Epoch: [368][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [368][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0043) ([0.006]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [368][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1707 (0.1707) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.020
current lr 1.00000e-04
Grad=  tensor(4.0897, device='cuda:0')
Epoch: [369][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0172 (0.0172) ([0.017]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [369][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [369][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [369][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1728 (0.1728) ([0.173]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(0.3095, device='cuda:0')
Epoch: [370][0/391]	Time 0.208 (0.208)	Data 0.138 (0.138)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [370][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [370][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0041) ([0.008]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1725 (0.1725) ([0.173]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.880
current lr 1.00000e-04
Grad=  tensor(0.0410, device='cuda:0')
Epoch: [371][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [371][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [371][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1770 (0.1770) ([0.177]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(0.0204, device='cuda:0')
Epoch: [372][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [372][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0042) ([0.006]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [372][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1751 (0.1751) ([0.175]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(0.5847, device='cuda:0')
Epoch: [373][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [373][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [373][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1694 (0.1694) ([0.169]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.050
current lr 1.00000e-04
Grad=  tensor(0.0596, device='cuda:0')
Epoch: [374][0/391]	Time 0.208 (0.208)	Data 0.136 (0.136)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0176 (0.0043) ([0.018]+[0.000])	Prec@1 99.219 (99.938)
Epoch: [374][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [374][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1658 (0.1658) ([0.166]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(0.0426, device='cuda:0')
Epoch: [375][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [375][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [375][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1737 (0.1737) ([0.174]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.0482, device='cuda:0')
Epoch: [376][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [376][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0008 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [376][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1739 (0.1739) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.2073, device='cuda:0')
Epoch: [377][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [377][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [377][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1721 (0.1721) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(0.1786, device='cuda:0')
Epoch: [378][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [378][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [378][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0104 (0.0040) ([0.010]+[0.000])	Prec@1 99.219 (99.966)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1780 (0.1780) ([0.178]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.2648, device='cuda:0')
Epoch: [379][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0064 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [379][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [379][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1647 (0.1647) ([0.165]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(0.0816, device='cuda:0')
Epoch: [380][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0044) ([0.005]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [380][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [380][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1642 (0.1642) ([0.164]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(0.0876, device='cuda:0')
Epoch: [381][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0065 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [381][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [381][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1664 (0.1664) ([0.166]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.010
current lr 1.00000e-04
Grad=  tensor(0.0303, device='cuda:0')
Epoch: [382][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [382][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [382][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.870
current lr 1.00000e-04
Grad=  tensor(0.0647, device='cuda:0')
Epoch: [383][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0071 (0.0034) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [383][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [383][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1673 (0.1673) ([0.167]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.980
current lr 1.00000e-04
Grad=  tensor(0.0442, device='cuda:0')
Epoch: [384][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0072 (0.0039) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [384][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [384][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0107 (0.0036) ([0.011]+[0.000])	Prec@1 99.219 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1768 (0.1768) ([0.177]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.030
current lr 1.00000e-04
Grad=  tensor(1.0041, device='cuda:0')
Epoch: [385][0/391]	Time 0.208 (0.208)	Data 0.136 (0.136)	Loss 0.0080 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0082 (0.0037) ([0.008]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [385][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0070 (0.0038) ([0.007]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [385][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1641 (0.1641) ([0.164]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.860
current lr 1.00000e-04
Grad=  tensor(0.0201, device='cuda:0')
Epoch: [386][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [386][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [386][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1721 (0.1721) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.990
current lr 1.00000e-04
Grad=  tensor(6.1624, device='cuda:0')
Epoch: [387][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0161 (0.0161) ([0.016]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [387][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [387][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [387][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1704 (0.1704) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(0.4234, device='cuda:0')
Epoch: [388][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0057 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [388][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [388][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1777 (0.1777) ([0.178]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.0572, device='cuda:0')
Epoch: [389][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0262 (0.0038) ([0.026]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [389][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [389][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1703 (0.1703) ([0.170]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.010
current lr 1.00000e-04
Grad=  tensor(0.1079, device='cuda:0')
Epoch: [390][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [390][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [390][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1628 (0.1628) ([0.163]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.5909, device='cuda:0')
Epoch: [391][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [391][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [391][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1655 (0.1655) ([0.165]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.980
current lr 1.00000e-04
Grad=  tensor(0.0722, device='cuda:0')
Epoch: [392][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [392][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [392][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1850 (0.1850) ([0.185]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(0.7215, device='cuda:0')
Epoch: [393][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [393][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [393][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1885 (0.1885) ([0.189]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.080
current lr 1.00000e-04
Grad=  tensor(0.0833, device='cuda:0')
Epoch: [394][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0060 (0.0036) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [394][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [394][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1984 (0.1984) ([0.198]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.0704, device='cuda:0')
Epoch: [395][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [395][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [395][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1805 (0.1805) ([0.181]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.0047, device='cuda:0')
Epoch: [396][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [396][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [396][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1687 (0.1687) ([0.169]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(2.2838, device='cuda:0')
Epoch: [397][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0080 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [397][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [397][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1856 (0.1856) ([0.186]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.810
current lr 1.00000e-04
Grad=  tensor(0.1097, device='cuda:0')
Epoch: [398][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [398][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [398][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1803 (0.1803) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.060
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [399][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0086 (0.0040) ([0.009]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [399][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [399][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0010 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1722 (0.1722) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0728, device='cuda:0')
Epoch: [400][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [400][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [400][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.0750, device='cuda:0')
Epoch: [401][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [401][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [401][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0070 (0.0038) ([0.007]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1700 (0.1700) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.000
current lr 1.00000e-05
Grad=  tensor(0.4148, device='cuda:0')
Epoch: [402][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0088 (0.0037) ([0.009]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [402][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [402][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1772 (0.1772) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.990
current lr 1.00000e-05
Grad=  tensor(0.0524, device='cuda:0')
Epoch: [403][0/391]	Time 0.255 (0.255)	Data 0.184 (0.184)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0013 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [403][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [403][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.1623, device='cuda:0')
Epoch: [404][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [404][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1720 (0.1720) ([0.172]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-05
Grad=  tensor(0.2162, device='cuda:0')
Epoch: [405][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [405][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0075 (0.0040) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [405][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1785 (0.1785) ([0.178]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.870
current lr 1.00000e-05
Grad=  tensor(0.1025, device='cuda:0')
Epoch: [406][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [406][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [406][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1782 (0.1782) ([0.178]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.1333, device='cuda:0')
Epoch: [407][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0036) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [407][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [407][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1772 (0.1772) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [408][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0060 (0.0034) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [408][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [408][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1765 (0.1765) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.0565, device='cuda:0')
Epoch: [409][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [409][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [409][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1764 (0.1764) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.2103, device='cuda:0')
Epoch: [410][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [410][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [410][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1755 (0.1755) ([0.176]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.3699, device='cuda:0')
Epoch: [411][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [411][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [411][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0104 (0.0036) ([0.010]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1735 (0.1735) ([0.174]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.920
current lr 1.00000e-05
Grad=  tensor(1.4359, device='cuda:0')
Epoch: [412][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0100 (0.0100) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0039) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [412][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [412][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1695 (0.1695) ([0.169]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(2.6459, device='cuda:0')
Epoch: [413][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.065 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [413][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [413][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1796 (0.1796) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.0374, device='cuda:0')
Epoch: [414][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [414][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [414][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1753 (0.1753) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.000
current lr 1.00000e-05
Grad=  tensor(0.1285, device='cuda:0')
Epoch: [415][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [415][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [415][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1804 (0.1804) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.1053, device='cuda:0')
Epoch: [416][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0014 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [416][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [416][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0095 (0.0038) ([0.009]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1666 (0.1666) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0429, device='cuda:0')
Epoch: [417][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0057 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [417][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0008 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [417][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1803 (0.1803) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(3.0080, device='cuda:0')
Epoch: [418][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0092 (0.0092) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [418][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [418][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1768 (0.1768) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.0635, device='cuda:0')
Epoch: [419][0/391]	Time 0.214 (0.214)	Data 0.142 (0.142)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [419][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [419][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1733 (0.1733) ([0.173]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.780
current lr 1.00000e-05
Grad=  tensor(0.0240, device='cuda:0')
Epoch: [420][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [420][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [420][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1734 (0.1734) ([0.173]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-05
Grad=  tensor(0.0480, device='cuda:0')
Epoch: [421][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [421][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [421][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1775 (0.1775) ([0.178]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.870
current lr 1.00000e-05
Grad=  tensor(0.0720, device='cuda:0')
Epoch: [422][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [422][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [422][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1911 (0.1911) ([0.191]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.1078, device='cuda:0')
Epoch: [423][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [423][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [423][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1703 (0.1703) ([0.170]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.0399, device='cuda:0')
Epoch: [424][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [424][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [424][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1789 (0.1789) ([0.179]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(0.1031, device='cuda:0')
Epoch: [425][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [425][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [425][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1850 (0.1850) ([0.185]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.0830, device='cuda:0')
Epoch: [426][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [426][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0071 (0.0034) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [426][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1645 (0.1645) ([0.165]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.900
current lr 1.00000e-05
Grad=  tensor(1.0020, device='cuda:0')
Epoch: [427][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [427][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [427][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1761 (0.1761) ([0.176]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.7847, device='cuda:0')
Epoch: [428][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [428][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [428][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1719 (0.1719) ([0.172]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0317, device='cuda:0')
Epoch: [429][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0087 (0.0033) ([0.009]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [429][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [429][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0035) ([0.007]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1641 (0.1641) ([0.164]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.1041, device='cuda:0')
Epoch: [430][0/391]	Time 0.213 (0.213)	Data 0.141 (0.141)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [430][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [430][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1771 (0.1771) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.960
current lr 1.00000e-05
Grad=  tensor(0.3335, device='cuda:0')
Epoch: [431][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [431][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [431][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1749 (0.1749) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.840
current lr 1.00000e-05
Grad=  tensor(0.0372, device='cuda:0')
Epoch: [432][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [432][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [432][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1698 (0.1698) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.1026, device='cuda:0')
Epoch: [433][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [433][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [433][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1740 (0.1740) ([0.174]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.0503, device='cuda:0')
Epoch: [434][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [434][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1691 (0.1691) ([0.169]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(0.0301, device='cuda:0')
Epoch: [435][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0060 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [435][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [435][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1624 (0.1624) ([0.162]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-05
Grad=  tensor(0.1042, device='cuda:0')
Epoch: [436][0/391]	Time 0.213 (0.213)	Data 0.143 (0.143)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0045 (0.0042) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [436][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [436][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1750 (0.1750) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.2696, device='cuda:0')
Epoch: [437][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0084 (0.0041) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [437][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [437][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1672 (0.1672) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.0038, device='cuda:0')
Epoch: [438][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [438][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [438][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0036) ([0.007]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1731 (0.1731) ([0.173]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.000
current lr 1.00000e-05
Grad=  tensor(0.0176, device='cuda:0')
Epoch: [439][0/391]	Time 0.211 (0.211)	Data 0.141 (0.141)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [439][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [439][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1763 (0.1763) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.000
current lr 1.00000e-05
Grad=  tensor(0.0110, device='cuda:0')
Epoch: [440][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [440][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [440][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0103 (0.0038) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1649 (0.1649) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.900
current lr 1.00000e-05
Grad=  tensor(0.2519, device='cuda:0')
Epoch: [441][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [441][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0064 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [441][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0130 (0.0038) ([0.013]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1762 (0.1762) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(0.0424, device='cuda:0')
Epoch: [442][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [442][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [442][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1617 (0.1617) ([0.162]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.0744, device='cuda:0')
Epoch: [443][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [443][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [443][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1817 (0.1817) ([0.182]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-05
Grad=  tensor(0.6301, device='cuda:0')
Epoch: [444][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [444][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [444][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1635 (0.1635) ([0.164]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [445][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [445][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0040) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [445][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1800 (0.1800) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.3236, device='cuda:0')
Epoch: [446][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [446][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [446][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.137 (0.137)	Loss 0.1811 (0.1811) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.1287, device='cuda:0')
Epoch: [447][0/391]	Time 0.212 (0.212)	Data 0.142 (0.142)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0012 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [447][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [447][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0035) ([0.006]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1920 (0.1920) ([0.192]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.7530, device='cuda:0')
Epoch: [448][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0068 (0.0068) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [448][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [448][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1757 (0.1757) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.900
current lr 1.00000e-05
Grad=  tensor(0.1001, device='cuda:0')
Epoch: [449][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [449][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [449][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1745 (0.1745) ([0.174]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.0894, device='cuda:0')
Epoch: [450][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [450][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [450][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1723 (0.1723) ([0.172]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.900
current lr 1.00000e-06
Grad=  tensor(0.1086, device='cuda:0')
Epoch: [451][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0012 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [451][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [451][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1714 (0.1714) ([0.171]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.1506, device='cuda:0')
Epoch: [452][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [452][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [452][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1747 (0.1747) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-06
Grad=  tensor(0.0040, device='cuda:0')
Epoch: [453][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [453][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [453][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1655 (0.1655) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.770
current lr 1.00000e-06
Grad=  tensor(0.0065, device='cuda:0')
Epoch: [454][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [454][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [454][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1695 (0.1695) ([0.169]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.010
current lr 1.00000e-06
Grad=  tensor(0.4523, device='cuda:0')
Epoch: [455][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0090 (0.0038) ([0.009]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [455][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [455][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1719 (0.1719) ([0.172]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.020
current lr 1.00000e-06
Grad=  tensor(2.0377, device='cuda:0')
Epoch: [456][0/391]	Time 0.215 (0.215)	Data 0.144 (0.144)	Loss 0.0094 (0.0094) ([0.009]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [456][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [456][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [456][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1713 (0.1713) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.980
current lr 1.00000e-06
Grad=  tensor(0.1683, device='cuda:0')
Epoch: [457][0/391]	Time 0.214 (0.214)	Data 0.142 (0.142)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0037) ([0.006]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [457][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1757 (0.1757) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.2508, device='cuda:0')
Epoch: [458][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [458][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [458][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1784 (0.1784) ([0.178]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.980
current lr 1.00000e-06
Grad=  tensor(0.1888, device='cuda:0')
Epoch: [459][0/391]	Time 0.212 (0.212)	Data 0.142 (0.142)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [459][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1633 (0.1633) ([0.163]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.970
current lr 1.00000e-06
Grad=  tensor(0.0112, device='cuda:0')
Epoch: [460][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [460][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [460][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.6897, device='cuda:0')
Epoch: [461][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [461][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [461][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0091 (0.0037) ([0.009]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1767 (0.1767) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.780
current lr 1.00000e-06
Grad=  tensor(0.0124, device='cuda:0')
Epoch: [462][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [462][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [462][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1633 (0.1633) ([0.163]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.5727, device='cuda:0')
Epoch: [463][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [463][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [463][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1736 (0.1736) ([0.174]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(1.3536, device='cuda:0')
Epoch: [464][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [464][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [464][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1653 (0.1653) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.040
current lr 1.00000e-06
Grad=  tensor(0.4061, device='cuda:0')
Epoch: [465][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [465][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [465][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1840 (0.1840) ([0.184]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-06
Grad=  tensor(0.0695, device='cuda:0')
Epoch: [466][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [466][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [466][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0130 (0.0037) ([0.013]+[0.000])	Prec@1 99.219 (99.969)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1797 (0.1797) ([0.180]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.030
current lr 1.00000e-06
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [467][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [467][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [467][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1777 (0.1777) ([0.178]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.040
current lr 1.00000e-06
Grad=  tensor(8.5993, device='cuda:0')
Epoch: [468][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0087 (0.0087) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [468][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [468][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.138 (0.138)	Loss 0.1772 (0.1772) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.900
current lr 1.00000e-06
Grad=  tensor(0.0407, device='cuda:0')
Epoch: [469][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [469][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [469][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1734 (0.1734) ([0.173]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.960
current lr 1.00000e-06
Grad=  tensor(0.5724, device='cuda:0')
Epoch: [470][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [470][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [470][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1655 (0.1655) ([0.166]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.0049, device='cuda:0')
Epoch: [471][0/391]	Time 0.209 (0.209)	Data 0.140 (0.140)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0076 (0.0034) ([0.008]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [471][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [471][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1767 (0.1767) ([0.177]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.0291, device='cuda:0')
Epoch: [472][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [472][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [472][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.138 (0.138)	Loss 0.1639 (0.1639) ([0.164]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.830
current lr 1.00000e-06
Grad=  tensor(0.6502, device='cuda:0')
Epoch: [473][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [473][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [473][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1822 (0.1822) ([0.182]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.990
current lr 1.00000e-06
Grad=  tensor(1.0483, device='cuda:0')
Epoch: [474][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [474][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [474][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1800 (0.1800) ([0.180]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.0309, device='cuda:0')
Epoch: [475][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0082 (0.0039) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [475][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [475][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1710 (0.1710) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.0316, device='cuda:0')
Epoch: [476][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [476][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [476][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1681 (0.1681) ([0.168]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.010
current lr 1.00000e-06
Grad=  tensor(0.0579, device='cuda:0')
Epoch: [477][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [477][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [477][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0035) ([0.006]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1574 (0.1574) ([0.157]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.810
current lr 1.00000e-06
Grad=  tensor(0.0025, device='cuda:0')
Epoch: [478][0/391]	Time 0.206 (0.206)	Data 0.134 (0.134)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [478][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [478][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1714 (0.1714) ([0.171]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.1000, device='cuda:0')
Epoch: [479][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [479][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [479][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1653 (0.1653) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.0297, device='cuda:0')
Epoch: [480][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [480][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0095 (0.0040) ([0.009]+[0.000])	Prec@1 99.219 (99.953)
Epoch: [480][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1702 (0.1702) ([0.170]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.3298, device='cuda:0')
Epoch: [481][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [481][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [481][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1823 (0.1823) ([0.182]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.050
current lr 1.00000e-06
Grad=  tensor(0.2562, device='cuda:0')
Epoch: [482][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [482][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [482][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0217 (0.0040) ([0.022]+[0.000])	Prec@1 99.219 (99.951)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.920
current lr 1.00000e-06
Grad=  tensor(0.0512, device='cuda:0')
Epoch: [483][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [483][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [483][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1710 (0.1710) ([0.171]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.990
current lr 1.00000e-06
Grad=  tensor(0.0358, device='cuda:0')
Epoch: [484][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [484][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [484][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1839 (0.1839) ([0.184]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.130
current lr 1.00000e-06
Grad=  tensor(6.4203, device='cuda:0')
Epoch: [485][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0107 (0.0107) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [485][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [485][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [485][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1665 (0.1665) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.920
current lr 1.00000e-06
Grad=  tensor(0.1020, device='cuda:0')
Epoch: [486][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [486][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [486][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1692 (0.1692) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.810
current lr 1.00000e-06
Grad=  tensor(0.3020, device='cuda:0')
Epoch: [487][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [487][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [487][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1648 (0.1648) ([0.165]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.900
current lr 1.00000e-06
Grad=  tensor(0.0984, device='cuda:0')
Epoch: [488][0/391]	Time 0.256 (0.256)	Data 0.185 (0.185)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0051 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [488][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [488][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.1673 (0.1673) ([0.167]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.7332, device='cuda:0')
Epoch: [489][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [489][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [489][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0008 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1713 (0.1713) ([0.171]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 93.990
current lr 1.00000e-06
Grad=  tensor(2.8439, device='cuda:0')
Epoch: [490][0/391]	Time 0.205 (0.205)	Data 0.133 (0.133)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [490][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [490][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1728 (0.1728) ([0.173]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.2239, device='cuda:0')
Epoch: [491][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0050 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [491][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0218 (0.0038) ([0.022]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [491][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1748 (0.1748) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.990
current lr 1.00000e-06
Grad=  tensor(0.1652, device='cuda:0')
Epoch: [492][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [492][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [492][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0075 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1680 (0.1680) ([0.168]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.110
current lr 1.00000e-06
Grad=  tensor(0.3380, device='cuda:0')
Epoch: [493][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [493][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0064 (0.0034) ([0.006]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [493][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1779 (0.1779) ([0.178]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.910
current lr 1.00000e-06
Grad=  tensor(0.0159, device='cuda:0')
Epoch: [494][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [494][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [494][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0091 (0.0038) ([0.009]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1764 (0.1764) ([0.176]+[0.000])	Prec@1 96.875 (96.875)
 * Prec@1 94.020
current lr 1.00000e-06
Grad=  tensor(0.0345, device='cuda:0')
Epoch: [495][0/391]	Time 0.211 (0.211)	Data 0.139 (0.139)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [495][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [495][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1678 (0.1678) ([0.168]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.920
current lr 1.00000e-06
Grad=  tensor(0.0070, device='cuda:0')
Epoch: [496][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0006 (0.0006) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0051 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [496][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [496][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1746 (0.1746) ([0.175]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(1.6585, device='cuda:0')
Epoch: [497][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [497][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [497][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0129 (0.0040) ([0.013]+[0.000])	Prec@1 99.219 (99.964)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1788 (0.1788) ([0.179]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-06
Grad=  tensor(0.0345, device='cuda:0')
Epoch: [498][0/391]	Time 0.208 (0.208)	Data 0.136 (0.136)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [498][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [498][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0036) ([0.008]+[0.000])	Prec@1 99.219 (99.974)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1764 (0.1764) ([0.176]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.0348, device='cuda:0')
Epoch: [499][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [499][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0071 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [499][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0161 (0.0037) ([0.016]+[0.000])	Prec@1 99.219 (99.979)
Test: [0/79]	Time 0.150 (0.150)	Loss 0.1693 (0.1693) ([0.169]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.940

 Elapsed time for training  5:26:57.568579

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5625, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6796875, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6796875, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6484375, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.60546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1025390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1025390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2072482705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.642578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.77783203125, 0.77783203125, 0.77734375, 0.77734375, 0.77783203125, 0.77734375, 0.77734375, 0.7783203125, 0.77734375, 0.77734375, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]
Total parameter pruned: 21803362.000030518 (unstructured) 21611437 (structured)
Test: [0/79]	Time 0.155 (0.155)	Loss 0.1693 (0.1693) ([0.169]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.940
Best accuracy:  94.13
