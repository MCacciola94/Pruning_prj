V0.0.1-_resnet18_Cifar10_lr0.01_l0.2_a1.6_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp4_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5487061738967896, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.3523094356060028, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21628862619400024, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.18689115345478058, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.14327552914619446, Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.12737567722797394, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11029236763715744, Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.2971794605255127, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09336047619581223, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11640232801437378, Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.1180206686258316, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09721729904413223, Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.1253737211227417, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11488018184900284, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08681126683950424, Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08893948793411255, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0640970766544342, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.08375772833824158, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06052006036043167, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.017058581113815308, Linear(in_features=512, out_features=100, bias=True): 0.39575180411338806}
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [0][0/391]	Time 0.205 (0.205)	Data 0.160 (0.160)	Loss nan (nan) ([4.652]+[nan])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.893)
Epoch: [0][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.927)
Epoch: [0][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.980)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [1][0/391]	Time 0.213 (0.213)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [1][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.886)
Epoch: [1][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.203)
Epoch: [1][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.115)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [2][0/391]	Time 0.190 (0.190)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [2][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.816)
Epoch: [2][200/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.919)
Epoch: [2][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.076)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [3][0/391]	Time 0.231 (0.231)	Data 0.188 (0.188)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [3][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.769)
Epoch: [3][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.892)
Epoch: [3][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.013)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [4][0/391]	Time 0.194 (0.194)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [4][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.638)
Epoch: [4][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.063)
Epoch: [4][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.985)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [5][0/391]	Time 0.211 (0.211)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [5][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.963)
Epoch: [5][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.180)
Epoch: [5][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.047)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [6][0/391]	Time 0.207 (0.207)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [6][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.404)
Epoch: [6][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.234)
Epoch: [6][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.008)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [7][0/391]	Time 0.189 (0.189)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [7][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.164)
Epoch: [7][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.133)
Epoch: [7][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.990)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [8][0/391]	Time 0.203 (0.203)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [8][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.947)
Epoch: [8][200/391]	Time 0.040 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.841)
Epoch: [8][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.928)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [9][0/391]	Time 0.211 (0.211)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [9][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.329)
Epoch: [9][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.927)
Epoch: [9][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.998)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [10][0/391]	Time 0.211 (0.211)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [10][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.855)
Epoch: [10][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.826)
Epoch: [10][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.884)
Test: [0/79]	Time 0.140 (0.140)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [11][0/391]	Time 0.213 (0.213)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [11][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.653)
Epoch: [11][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.923)
Epoch: [11][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.949)
Test: [0/79]	Time 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [12][0/391]	Time 0.211 (0.211)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [12][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.893)
Epoch: [12][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.935)
Epoch: [12][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.837)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [13][0/391]	Time 0.222 (0.222)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [13][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.094)
Epoch: [13][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.250)
Epoch: [13][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.071)
Test: [0/79]	Time 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [14][0/391]	Time 0.219 (0.219)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [14][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.125)
Epoch: [14][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.098)
Epoch: [14][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.946)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [15][0/391]	Time 0.209 (0.209)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [15][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.188 (10.381)
Epoch: [15][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.939)
Epoch: [15][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.925)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [16][0/391]	Time 0.218 (0.218)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [16][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.986)
Epoch: [16][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.904)
Epoch: [16][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.985)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [17][0/391]	Time 0.194 (0.194)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [17][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.739)
Epoch: [17][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.927)
Epoch: [17][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.951)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [18][0/391]	Time 0.191 (0.191)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [18][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.754)
Epoch: [18][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.152)
Epoch: [18][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.969)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [19][0/391]	Time 0.196 (0.196)	Data 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [19][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.994)
Epoch: [19][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.853)
Epoch: [19][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.917)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [20][0/391]	Time 0.209 (0.209)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [20][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.708)
Epoch: [20][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.861)
Epoch: [20][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.993)
Test: [0/79]	Time 0.182 (0.182)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [21][0/391]	Time 0.198 (0.198)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [21][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.916)
Epoch: [21][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (10.055)
Epoch: [21][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.949)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [22][0/391]	Time 0.218 (0.218)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [22][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.808)
Epoch: [22][200/391]	Time 0.039 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.873)
Epoch: [22][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.837)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [23][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [23][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.071)
Epoch: [23][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.888)
Epoch: [23][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.962)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [24][0/391]	Time 0.198 (0.198)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [24][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.071)
Epoch: [24][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.896)
Epoch: [24][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.026)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [25][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [25][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.048)
Epoch: [25][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.036)
Epoch: [25][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.063)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [26][0/391]	Time 0.212 (0.212)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [26][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.303)
Epoch: [26][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.044)
Epoch: [26][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.120)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [27][0/391]	Time 0.204 (0.204)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [27][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.932)
Epoch: [27][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.810)
Epoch: [27][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.933)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [28][0/391]	Time 0.192 (0.192)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (15.625)
Epoch: [28][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.125)
Epoch: [28][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.067)
Epoch: [28][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.021)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [29][0/391]	Time 0.204 (0.204)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [29][100/391]	Time 0.038 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.071)
Epoch: [29][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.160)
Epoch: [29][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.024)
Test: [0/79]	Time 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [30][0/391]	Time 0.190 (0.190)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [30][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.599)
Epoch: [30][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.028)
Epoch: [30][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.993)
Test: [0/79]	Time 0.184 (0.184)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [31][0/391]	Time 0.213 (0.213)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [31][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (10.203)
Epoch: [31][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.032)
Epoch: [31][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.949)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [32][0/391]	Time 0.191 (0.191)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [32][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.971)
Epoch: [32][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.904)
Epoch: [32][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.892)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [33][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [33][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.156)
Epoch: [33][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.188 (10.125)
Epoch: [33][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.107)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [34][0/391]	Time 0.191 (0.191)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [34][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.916)
Epoch: [34][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.075)
Epoch: [34][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.019)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [35][0/391]	Time 0.189 (0.189)	Data 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [35][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.319)
Epoch: [35][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.059)
Epoch: [35][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.047)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [36][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [36][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.017)
Epoch: [36][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.768)
Epoch: [36][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.863)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [37][0/391]	Time 0.199 (0.199)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [37][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.646)
Epoch: [37][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.896)
Epoch: [37][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.925)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [38][0/391]	Time 0.191 (0.191)	Data 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [38][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.326)
Epoch: [38][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.316)
Epoch: [38][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.133)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [39][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [39][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.893)
Epoch: [39][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.012)
Epoch: [39][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.920)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [40][0/391]	Time 0.213 (0.213)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [40][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.700)
Epoch: [40][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.977)
Epoch: [40][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.897)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [41][0/391]	Time 0.192 (0.192)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [41][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.623)
Epoch: [41][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.865)
Epoch: [41][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.933)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [42][0/391]	Time 0.215 (0.215)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [42][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.110)
Epoch: [42][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.977)
Epoch: [42][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.962)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [43][0/391]	Time 0.199 (0.199)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [43][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.769)
Epoch: [43][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.900)
Epoch: [43][300/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.003)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [44][0/391]	Time 0.208 (0.208)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [44][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.916)
Epoch: [44][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.923)
Epoch: [44][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.873)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [45][0/391]	Time 0.193 (0.193)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [45][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.102)
Epoch: [45][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.106)
Epoch: [45][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.068)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [46][0/391]	Time 0.216 (0.216)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [46][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.241)
Epoch: [46][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.098)
Epoch: [46][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.071)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [47][0/391]	Time 0.215 (0.215)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [47][100/391]	Time 0.039 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.862)
Epoch: [47][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.911)
Epoch: [47][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.917)
Test: [0/79]	Time 0.181 (0.181)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [48][0/391]	Time 0.197 (0.197)	Data 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (14.844)
Epoch: [48][100/391]	Time 0.036 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.094)
Epoch: [48][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.141)
Epoch: [48][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.032)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [49][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [49][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.063)
Epoch: [49][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.024)
Epoch: [49][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.920)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [50][0/391]	Time 0.225 (0.225)	Data 0.182 (0.182)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [50][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.040)
Epoch: [50][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.016)
Epoch: [50][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.977)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [51][0/391]	Time 0.192 (0.192)	Data 0.140 (0.140)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [51][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.141)
Epoch: [51][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.012)
Epoch: [51][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.972)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [52][0/391]	Time 0.204 (0.204)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [52][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.442)
Epoch: [52][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.172)
Epoch: [52][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.089)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [53][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [53][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.342)
Epoch: [53][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.148)
Epoch: [53][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.084)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [54][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [54][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.025)
Epoch: [54][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.768)
Epoch: [54][300/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.949)
Test: [0/79]	Time 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [55][0/391]	Time 0.186 (0.186)	Data 0.142 (0.142)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [55][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.179)
Epoch: [55][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.036)
Epoch: [55][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.969)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [56][0/391]	Time 0.191 (0.191)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [56][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.924)
Epoch: [56][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.869)
Epoch: [56][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.962)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [57][0/391]	Time 0.195 (0.195)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [57][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.777)
Epoch: [57][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.904)
Epoch: [57][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.052)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [58][0/391]	Time 0.202 (0.202)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [58][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.514)
Epoch: [58][200/391]	Time 0.044 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.709)
Epoch: [58][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.904)
Test: [0/79]	Time 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [59][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [59][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.164)
Epoch: [59][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.148)
Epoch: [59][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (10.141)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [60][0/391]	Time 0.196 (0.196)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [60][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.855)
Epoch: [60][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (9.892)
Epoch: [60][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.938)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [61][0/391]	Time 0.193 (0.193)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [61][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.661)
Epoch: [61][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.919)
Epoch: [61][300/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.006)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [62][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [62][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.615)
Epoch: [62][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.698)
Epoch: [62][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.845)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [63][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [63][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.777)
Epoch: [63][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.028)
Epoch: [63][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.946)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [64][0/391]	Time 0.193 (0.193)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [64][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.357)
Epoch: [64][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.242)
Epoch: [64][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.128)
Test: [0/79]	Time 0.181 (0.181)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [65][0/391]	Time 0.191 (0.191)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [65][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.769)
Epoch: [65][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.997)
Epoch: [65][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.050)
Test: [0/79]	Time 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [66][0/391]	Time 0.190 (0.190)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [66][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.886)
Epoch: [66][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.009)
Epoch: [66][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.975)
Test: [0/79]	Time 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [67][0/391]	Time 0.188 (0.188)	Data 0.142 (0.142)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [67][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.141)
Epoch: [67][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.188 (9.838)
Epoch: [67][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.985)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [68][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [68][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.947)
Epoch: [68][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.199)
Epoch: [68][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.021)
Test: [0/79]	Time 0.184 (0.184)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [69][0/391]	Time 0.222 (0.222)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [69][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.677)
Epoch: [69][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.659)
Epoch: [69][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.889)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [70][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (4.688)
Epoch: [70][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.870)
Epoch: [70][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.001)
Epoch: [70][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.956)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [71][0/391]	Time 0.199 (0.199)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [71][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.288)
Epoch: [71][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.152)
Epoch: [71][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.084)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [72][0/391]	Time 0.196 (0.196)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [72][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.793)
Epoch: [72][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.892)
Epoch: [72][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.972)
Test: [0/79]	Time 0.185 (0.185)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [73][0/391]	Time 0.211 (0.211)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [73][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.265)
Epoch: [73][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.164)
Epoch: [73][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.982)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [74][0/391]	Time 0.218 (0.218)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [74][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.063)
Epoch: [74][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.989)
Epoch: [74][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.029)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [75][0/391]	Time 0.191 (0.191)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [75][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [75][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.810)
Epoch: [75][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.915)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [76][0/391]	Time 0.194 (0.194)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [76][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.002)
Epoch: [76][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.148)
Epoch: [76][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.019)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [77][0/391]	Time 0.195 (0.195)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [77][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.543)
Epoch: [77][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.106)
Epoch: [77][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.019)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [78][0/391]	Time 0.190 (0.190)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [78][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.079)
Epoch: [78][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.020)
Epoch: [78][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.086)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [79][0/391]	Time 0.232 (0.232)	Data 0.182 (0.182)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [79][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.855)
Epoch: [79][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.985)
Epoch: [79][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.956)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [80][0/391]	Time 0.193 (0.193)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [80][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.272)
Epoch: [80][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.059)
Epoch: [80][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.081)
Test: [0/79]	Time 0.177 (0.177)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [81][0/391]	Time 0.222 (0.222)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [81][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.040)
Epoch: [81][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.129)
Epoch: [81][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (9.943)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [82][0/391]	Time 0.197 (0.197)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [82][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.226)
Epoch: [82][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.207)
Epoch: [82][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.065)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [83][0/391]	Time 0.199 (0.199)	Data 0.154 (0.154)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [83][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.272)
Epoch: [83][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.012)
Epoch: [83][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.073)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [84][0/391]	Time 0.189 (0.189)	Data 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [84][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.862)
Epoch: [84][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.985)
Epoch: [84][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.008)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [85][0/391]	Time 0.191 (0.191)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [85][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.188 (10.063)
Epoch: [85][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.168)
Epoch: [85][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.120)
Test: [0/79]	Time 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [86][0/391]	Time 0.224 (0.224)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [86][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.087)
Epoch: [86][200/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.001)
Epoch: [86][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.915)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [87][0/391]	Time 0.195 (0.195)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [87][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.932)
Epoch: [87][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.024)
Epoch: [87][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.920)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [88][0/391]	Time 0.221 (0.221)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [88][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.063)
Epoch: [88][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.853)
Epoch: [88][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.975)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [89][0/391]	Time 0.197 (0.197)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [89][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.056)
Epoch: [89][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.044)
Epoch: [89][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.001)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [90][0/391]	Time 0.198 (0.198)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [90][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.769)
Epoch: [90][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.036)
Epoch: [90][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.972)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [91][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [91][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.435)
Epoch: [91][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.935)
Epoch: [91][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.003)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [92][0/391]	Time 0.195 (0.195)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [92][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.125)
Epoch: [92][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.001)
Epoch: [92][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.936)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [93][0/391]	Time 0.190 (0.190)	Data 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [93][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.056)
Epoch: [93][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.133)
Epoch: [93][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.011)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [94][0/391]	Time 0.192 (0.192)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [94][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.762)
Epoch: [94][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.632)
Epoch: [94][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.840)
Test: [0/79]	Time 0.191 (0.191)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [95][0/391]	Time 0.218 (0.218)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [95][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.017)
Epoch: [95][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.199)
Epoch: [95][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.993)
Test: [0/79]	Time 0.177 (0.177)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [96][0/391]	Time 0.193 (0.193)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [96][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.149)
Epoch: [96][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.125)
Epoch: [96][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.045)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [97][0/391]	Time 0.195 (0.195)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [97][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.009)
Epoch: [97][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.180)
Epoch: [97][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.029)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [98][0/391]	Time 0.192 (0.192)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [98][100/391]	Time 0.039 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.450)
Epoch: [98][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.113)
Epoch: [98][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.938)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [99][0/391]	Time 0.191 (0.191)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [99][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.079)
Epoch: [99][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.036)
Epoch: [99][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.946)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [100][0/391]	Time 0.224 (0.224)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [100][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.025)
Epoch: [100][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.028)
Epoch: [100][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.115)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [101][0/391]	Time 0.210 (0.210)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [101][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.924)
Epoch: [101][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.993)
Epoch: [101][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.941)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [102][0/391]	Time 0.194 (0.194)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [102][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.824)
Epoch: [102][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.810)
Epoch: [102][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.047)
Test: [0/79]	Time 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [103][0/391]	Time 0.191 (0.191)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [103][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.878)
Epoch: [103][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.993)
Epoch: [103][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.972)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [104][0/391]	Time 0.189 (0.189)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [104][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.040)
Epoch: [104][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.935)
Epoch: [104][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.946)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [105][0/391]	Time 0.206 (0.206)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (16.406)
Epoch: [105][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.893)
Epoch: [105][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.974)
Epoch: [105][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.063)
Test: [0/79]	Time 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [106][0/391]	Time 0.190 (0.190)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [106][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.909)
Epoch: [106][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.113)
Epoch: [106][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.985)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [107][0/391]	Time 0.203 (0.203)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [107][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.040)
Epoch: [107][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.113)
Epoch: [107][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.998)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [108][0/391]	Time 0.200 (0.200)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [108][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.808)
Epoch: [108][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.919)
Epoch: [108][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.982)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [109][0/391]	Time 0.199 (0.199)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [109][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.141)
Epoch: [109][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.834)
Epoch: [109][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.972)
Test: [0/79]	Time 0.181 (0.181)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [110][0/391]	Time 0.193 (0.193)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [110][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.909)
Epoch: [110][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.993)
Epoch: [110][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.034)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [111][0/391]	Time 0.195 (0.195)	Data 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [111][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.195)
Epoch: [111][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.036)
Epoch: [111][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.042)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [112][0/391]	Time 0.191 (0.191)	Data 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [112][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.017)
Epoch: [112][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.834)
Epoch: [112][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.969)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [113][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [113][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.862)
Epoch: [113][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.787)
Epoch: [113][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.814)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [114][0/391]	Time 0.211 (0.211)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [114][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.218)
Epoch: [114][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.129)
Epoch: [114][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.138)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [115][0/391]	Time 0.182 (0.182)	Data 0.138 (0.138)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [115][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.141)
Epoch: [115][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.160)
Epoch: [115][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.011)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [116][0/391]	Time 0.219 (0.219)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [116][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.032)
Epoch: [116][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.082)
Epoch: [116][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.969 (10.019)
Test: [0/79]	Time 0.184 (0.184)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [117][0/391]	Time 0.197 (0.197)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [117][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.785)
Epoch: [117][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.810)
Epoch: [117][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.889)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [118][0/391]	Time 0.201 (0.201)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (15.625)
Epoch: [118][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.971)
Epoch: [118][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.222)
Epoch: [118][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.938)
Test: [0/79]	Time 0.147 (0.147)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [119][0/391]	Time 0.187 (0.187)	Data 0.142 (0.142)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [119][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.226)
Epoch: [119][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.164)
Epoch: [119][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.024)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [120][0/391]	Time 0.188 (0.188)	Data 0.142 (0.142)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [120][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.110)
Epoch: [120][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.032)
Epoch: [120][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.190)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [121][0/391]	Time 0.205 (0.205)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [121][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.940)
Epoch: [121][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.977)
Epoch: [121][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.972)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [122][0/391]	Time 0.228 (0.228)	Data 0.184 (0.184)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [122][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.195)
Epoch: [122][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.094)
Epoch: [122][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.133)
Test: [0/79]	Time 0.182 (0.182)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [123][0/391]	Time 0.222 (0.222)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [123][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.932)
Epoch: [123][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.962)
Epoch: [123][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.990)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [124][0/391]	Time 0.198 (0.198)	Data 0.153 (0.153)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [124][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.187)
Epoch: [124][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.892)
Epoch: [124][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.827)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [125][0/391]	Time 0.189 (0.189)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [125][100/391]	Time 0.039 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.754)
Epoch: [125][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.830)
Epoch: [125][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.832)
Test: [0/79]	Time 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [126][0/391]	Time 0.221 (0.221)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [126][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.708)
Epoch: [126][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.036)
Epoch: [126][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.021)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [127][0/391]	Time 0.194 (0.194)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [127][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.311)
Epoch: [127][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.234)
Epoch: [127][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.011)
Test: [0/79]	Time 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [128][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [128][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.017)
Epoch: [128][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.981)
Epoch: [128][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.980)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [129][0/391]	Time 0.216 (0.216)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [129][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.048)
Epoch: [129][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.857)
Epoch: [129][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.943)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [130][0/391]	Time 0.189 (0.189)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [130][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.916)
Epoch: [130][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.040)
Epoch: [130][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.094)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [131][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [131][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.149)
Epoch: [131][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.974)
Epoch: [131][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.998)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [132][0/391]	Time 0.194 (0.194)	Data 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [132][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.909)
Epoch: [132][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.977)
Epoch: [132][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (9.988)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [133][0/391]	Time 0.190 (0.190)	Data 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [133][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.971)
Epoch: [133][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.997)
Epoch: [133][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (10.086)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [134][0/391]	Time 0.204 (0.204)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [134][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.947)
Epoch: [134][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.024)
Epoch: [134][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.951)
Test: [0/79]	Time 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [135][0/391]	Time 0.218 (0.218)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [135][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.777)
Epoch: [135][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.012)
Epoch: [135][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.985)
Test: [0/79]	Time 0.145 (0.145)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [136][0/391]	Time 0.189 (0.189)	Data 0.144 (0.144)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [136][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.295)
Epoch: [136][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (10.044)
Epoch: [136][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.985)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [137][0/391]	Time 0.186 (0.186)	Data 0.141 (0.141)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [137][100/391]	Time 0.037 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.172)
Epoch: [137][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.028)
Epoch: [137][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.011)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [138][0/391]	Time 0.218 (0.218)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [138][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.986)
Epoch: [138][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.145)
Epoch: [138][300/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.097)
Test: [0/79]	Time 0.181 (0.181)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [139][0/391]	Time 0.193 (0.193)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [139][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.831)
Epoch: [139][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.873)
Epoch: [139][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.003)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [140][0/391]	Time 0.195 (0.195)	Data 0.150 (0.150)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [140][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.032)
Epoch: [140][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.915)
Epoch: [140][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.863)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [141][0/391]	Time 0.193 (0.193)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [141][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.862)
Epoch: [141][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.156)
Epoch: [141][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.200)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [142][0/391]	Time 0.194 (0.194)	Data 0.148 (0.148)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [142][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.909)
Epoch: [142][200/391]	Time 0.040 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.082)
Epoch: [142][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.871)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [143][0/391]	Time 0.191 (0.191)	Data 0.146 (0.146)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [143][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.777)
Epoch: [143][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.892)
Epoch: [143][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.938)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [144][0/391]	Time 0.188 (0.188)	Data 0.143 (0.143)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [144][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.102)
Epoch: [144][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.374)
Epoch: [144][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.029)
Test: [0/79]	Time 0.198 (0.198)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [145][0/391]	Time 0.217 (0.217)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [145][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.187)
Epoch: [145][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.791)
Epoch: [145][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.871)
Test: [0/79]	Time 0.191 (0.191)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [146][0/391]	Time 0.212 (0.212)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [146][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.048)
Epoch: [146][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.849)
Epoch: [146][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.881)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [147][0/391]	Time 0.217 (0.217)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [147][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.040)
Epoch: [147][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.911)
Epoch: [147][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.959)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [148][0/391]	Time 0.211 (0.211)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [148][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.187)
Epoch: [148][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.020)
Epoch: [148][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.047)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [149][0/391]	Time 0.222 (0.222)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [149][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.009)
Epoch: [149][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.067)
Epoch: [149][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.936)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [150][0/391]	Time 0.209 (0.209)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [150][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.017)
Epoch: [150][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.117)
Epoch: [150][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.135)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [151][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [151][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.986)
Epoch: [151][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (9.974)
Epoch: [151][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.008)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [152][0/391]	Time 0.215 (0.215)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [152][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.164)
Epoch: [152][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.129)
Epoch: [152][300/391]	Time 0.039 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.076)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [153][0/391]	Time 0.212 (0.212)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [153][100/391]	Time 0.039 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.179)
Epoch: [153][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.950)
Epoch: [153][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.008)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [154][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [154][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.412)
Epoch: [154][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.281)
Epoch: [154][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.078)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [155][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [155][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.133)
Epoch: [155][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.040)
Epoch: [155][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.086)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [156][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [156][100/391]	Time 0.036 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.334)
Epoch: [156][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.106)
Epoch: [156][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.972)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [157][0/391]	Time 0.207 (0.207)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [157][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.365)
Epoch: [157][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.981)
Epoch: [157][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.065)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [158][0/391]	Time 0.207 (0.207)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [158][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.553)
Epoch: [158][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.974)
Epoch: [158][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.003)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [159][0/391]	Time 0.205 (0.205)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [159][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.878)
Epoch: [159][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.051)
Epoch: [159][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.972)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [160][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [160][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.319)
Epoch: [160][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.086)
Epoch: [160][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.052)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [161][0/391]	Time 0.207 (0.207)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [161][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.210)
Epoch: [161][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (10.055)
Epoch: [161][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.962)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [162][0/391]	Time 0.209 (0.209)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [162][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.924)
Epoch: [162][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.176)
Epoch: [162][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.138)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [163][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [163][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.056)
Epoch: [163][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.071)
Epoch: [163][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.138)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [164][0/391]	Time 0.213 (0.213)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [164][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.071)
Epoch: [164][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.997)
Epoch: [164][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.021)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [165][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [165][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.056)
Epoch: [165][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.016)
Epoch: [165][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.969 (9.954)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [166][0/391]	Time 0.204 (0.204)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [166][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.994)
Epoch: [166][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.977)
Epoch: [166][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.920)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [167][0/391]	Time 0.207 (0.207)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (14.844)
Epoch: [167][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.708)
Epoch: [167][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.818)
Epoch: [167][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.949)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [168][0/391]	Time 0.211 (0.211)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [168][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.102)
Epoch: [168][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.876)
Epoch: [168][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.019)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [169][0/391]	Time 0.208 (0.208)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [169][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.847)
Epoch: [169][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.900)
Epoch: [169][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.904)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [170][0/391]	Time 0.208 (0.208)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [170][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.909)
Epoch: [170][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.152)
Epoch: [170][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.936)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [171][0/391]	Time 0.205 (0.205)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [171][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.063)
Epoch: [171][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.195)
Epoch: [171][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.257)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [172][0/391]	Time 0.209 (0.209)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [172][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.971)
Epoch: [172][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.215)
Epoch: [172][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.063)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [173][0/391]	Time 0.210 (0.210)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [173][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.280)
Epoch: [173][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.125)
Epoch: [173][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.086)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [174][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [174][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.646)
Epoch: [174][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.962)
Epoch: [174][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.110)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [175][0/391]	Time 0.206 (0.206)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [175][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.901)
Epoch: [175][200/391]	Time 0.039 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.977)
Epoch: [175][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.814)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [176][0/391]	Time 0.214 (0.214)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [176][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.646)
Epoch: [176][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.865)
Epoch: [176][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.967)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [177][0/391]	Time 0.213 (0.213)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [177][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.537)
Epoch: [177][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.845)
Epoch: [177][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.892)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [178][0/391]	Time 0.210 (0.210)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [178][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.133)
Epoch: [178][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.152)
Epoch: [178][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.016)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [179][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [179][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.265)
Epoch: [179][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.203)
Epoch: [179][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.078)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [180][0/391]	Time 0.211 (0.211)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [180][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.210)
Epoch: [180][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.117)
Epoch: [180][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (10.076)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [181][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [181][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.994)
Epoch: [181][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.032)
Epoch: [181][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.021)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [182][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [182][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.886)
Epoch: [182][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.857)
Epoch: [182][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.889)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [183][0/391]	Time 0.211 (0.211)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [183][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.141)
Epoch: [183][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.129)
Epoch: [183][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.135)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [184][0/391]	Time 0.203 (0.203)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [184][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.118)
Epoch: [184][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.977)
Epoch: [184][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.972)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [185][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (15.625)
Epoch: [185][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.442)
Epoch: [185][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.164)
Epoch: [185][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.110)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [186][0/391]	Time 0.207 (0.207)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [186][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.025)
Epoch: [186][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.218)
Epoch: [186][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.026)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [187][0/391]	Time 0.205 (0.205)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [187][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.234)
Epoch: [187][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.969 (10.079)
Epoch: [187][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.159)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [188][0/391]	Time 0.211 (0.211)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [188][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (9.994)
Epoch: [188][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.803)
Epoch: [188][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.995)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [189][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [189][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.017)
Epoch: [189][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.261)
Epoch: [189][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.081)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [190][0/391]	Time 0.212 (0.212)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [190][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.978)
Epoch: [190][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.203)
Epoch: [190][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.063)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [191][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [191][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.870)
Epoch: [191][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.977)
Epoch: [191][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.084)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [192][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [192][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.025)
Epoch: [192][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.012)
Epoch: [192][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.977)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [193][0/391]	Time 0.209 (0.209)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [193][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.234)
Epoch: [193][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.195)
Epoch: [193][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.172)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [194][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [194][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.777)
Epoch: [194][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.946)
Epoch: [194][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.938)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [195][0/391]	Time 0.206 (0.206)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [195][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.125)
Epoch: [195][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.977)
Epoch: [195][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.008)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [196][0/391]	Time 0.203 (0.203)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [196][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.615)
Epoch: [196][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.900)
Epoch: [196][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 3.906 (9.928)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [197][0/391]	Time 0.204 (0.204)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (16.406)
Epoch: [197][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.079)
Epoch: [197][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.113)
Epoch: [197][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.052)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [198][0/391]	Time 0.206 (0.206)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [198][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.241)
Epoch: [198][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.172)
Epoch: [198][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.016)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-02
Grad=  tensor(nan, device='cuda:0')
Epoch: [199][0/391]	Time 0.210 (0.210)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [199][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.396)
Epoch: [199][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.238)
Epoch: [199][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.026)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [200][0/391]	Time 0.206 (0.206)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [200][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.087)
Epoch: [200][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.822)
Epoch: [200][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.860)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [201][0/391]	Time 0.202 (0.202)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [201][100/391]	Time 0.037 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.777)
Epoch: [201][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.900)
Epoch: [201][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.956)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [202][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [202][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.226)
Epoch: [202][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.300)
Epoch: [202][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.102)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [203][0/391]	Time 0.213 (0.213)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [203][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.947)
Epoch: [203][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.818)
Epoch: [203][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.951)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [204][0/391]	Time 0.209 (0.209)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [204][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.785)
Epoch: [204][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.931)
Epoch: [204][300/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.990)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [205][0/391]	Time 0.208 (0.208)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [205][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (10.342)
Epoch: [205][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.265)
Epoch: [205][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.130)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [206][0/391]	Time 0.207 (0.207)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [206][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.048)
Epoch: [206][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.055)
Epoch: [206][300/391]	Time 0.035 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.975)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [207][0/391]	Time 0.208 (0.208)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [207][100/391]	Time 0.036 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.429)
Epoch: [207][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.670)
Epoch: [207][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.780)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [208][0/391]	Time 0.202 (0.202)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [208][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.739)
Epoch: [208][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.962)
Epoch: [208][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.052)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [209][0/391]	Time 0.205 (0.205)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [209][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.203)
Epoch: [209][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.191)
Epoch: [209][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.016)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [210][0/391]	Time 0.205 (0.205)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [210][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.319)
Epoch: [210][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.195)
Epoch: [210][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.094)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [211][0/391]	Time 0.207 (0.207)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [211][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.079)
Epoch: [211][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.981)
Epoch: [211][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.037)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [212][0/391]	Time 0.207 (0.207)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [212][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.048)
Epoch: [212][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.989)
Epoch: [212][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.990)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [213][0/391]	Time 0.204 (0.204)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [213][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.179)
Epoch: [213][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.028)
Epoch: [213][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.102)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [214][0/391]	Time 0.201 (0.201)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [214][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.009)
Epoch: [214][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.098)
Epoch: [214][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.985)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [215][0/391]	Time 0.202 (0.202)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [215][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.754)
Epoch: [215][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.915)
Epoch: [215][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.034)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [216][0/391]	Time 0.205 (0.205)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [216][100/391]	Time 0.036 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.311)
Epoch: [216][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.051)
Epoch: [216][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.052)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [217][0/391]	Time 0.206 (0.206)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [217][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.893)
Epoch: [217][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.869)
Epoch: [217][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.956)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [218][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [218][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.156)
Epoch: [218][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.086)
Epoch: [218][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.998)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [219][0/391]	Time 0.209 (0.209)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (16.406)
Epoch: [219][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.723)
Epoch: [219][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.071)
Epoch: [219][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.951)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [220][0/391]	Time 0.203 (0.203)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [220][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.017)
Epoch: [220][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.822)
Epoch: [220][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.998)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [221][0/391]	Time 0.204 (0.204)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [221][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.125)
Epoch: [221][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.970)
Epoch: [221][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.977)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [222][0/391]	Time 0.208 (0.208)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [222][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.334)
Epoch: [222][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.218)
Epoch: [222][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.102)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [223][0/391]	Time 0.212 (0.212)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [223][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.731)
Epoch: [223][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.989)
Epoch: [223][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.920)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [224][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [224][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.916)
Epoch: [224][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.981)
Epoch: [224][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.039)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [225][0/391]	Time 0.204 (0.204)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [225][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.909)
Epoch: [225][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.962)
Epoch: [225][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.003)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [226][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [226][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.909)
Epoch: [226][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.133)
Epoch: [226][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.021)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [227][0/391]	Time 0.204 (0.204)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [227][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.110)
Epoch: [227][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.148)
Epoch: [227][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.058)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [228][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [228][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.955)
Epoch: [228][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.884)
Epoch: [228][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.980)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [229][0/391]	Time 0.208 (0.208)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [229][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.916)
Epoch: [229][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.834)
Epoch: [229][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.920)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [230][0/391]	Time 0.207 (0.207)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (14.062)
Epoch: [230][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.986)
Epoch: [230][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.176)
Epoch: [230][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.951)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [231][0/391]	Time 0.204 (0.204)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [231][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.824)
Epoch: [231][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.001)
Epoch: [231][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.042)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [232][0/391]	Time 0.208 (0.208)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [232][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 3.125 (9.646)
Epoch: [232][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.830)
Epoch: [232][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.029)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [233][0/391]	Time 0.207 (0.207)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [233][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.940)
Epoch: [233][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.946)
Epoch: [233][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.917)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [234][0/391]	Time 0.203 (0.203)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [234][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.986)
Epoch: [234][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.810)
Epoch: [234][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.832)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [235][0/391]	Time 0.203 (0.203)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [235][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.017)
Epoch: [235][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.086)
Epoch: [235][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.034)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [236][0/391]	Time 0.204 (0.204)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [236][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.179)
Epoch: [236][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.082)
Epoch: [236][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.873)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [237][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [237][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.754)
Epoch: [237][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.001)
Epoch: [237][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (9.990)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [238][0/391]	Time 0.200 (0.200)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [238][100/391]	Time 0.043 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.048)
Epoch: [238][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.993)
Epoch: [238][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.993)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [239][0/391]	Time 0.207 (0.207)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [239][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.870)
Epoch: [239][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.117)
Epoch: [239][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.954)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [240][0/391]	Time 0.208 (0.208)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [240][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.816)
Epoch: [240][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.985)
Epoch: [240][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.071)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [241][0/391]	Time 0.204 (0.204)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [241][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.870)
Epoch: [241][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.923)
Epoch: [241][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.990)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [242][0/391]	Time 0.207 (0.207)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [242][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.870)
Epoch: [242][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.993)
Epoch: [242][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.115)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [243][0/391]	Time 0.205 (0.205)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [243][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.677)
Epoch: [243][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 3.906 (9.900)
Epoch: [243][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.912)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [244][0/391]	Time 0.206 (0.206)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [244][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.241)
Epoch: [244][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.067)
Epoch: [244][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.034)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [245][0/391]	Time 0.206 (0.206)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [245][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.669)
Epoch: [245][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (10.071)
Epoch: [245][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.055)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [246][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [246][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.210)
Epoch: [246][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.102)
Epoch: [246][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.990)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [247][0/391]	Time 0.203 (0.203)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [247][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.079)
Epoch: [247][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.787)
Epoch: [247][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.938)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [248][0/391]	Time 0.221 (0.221)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.844 (14.844)
Epoch: [248][100/391]	Time 0.038 (0.039)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.025)
Epoch: [248][200/391]	Time 0.039 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.001)
Epoch: [248][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.016)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-03
Grad=  tensor(nan, device='cuda:0')
Epoch: [249][0/391]	Time 0.210 (0.210)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [249][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.025)
Epoch: [249][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.187)
Epoch: [249][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.052)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [250][0/391]	Time 0.204 (0.204)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [250][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.071)
Epoch: [250][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.958)
Epoch: [250][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.969)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [251][0/391]	Time 0.203 (0.203)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [251][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (9.855)
Epoch: [251][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.884)
Epoch: [251][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.889)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [252][0/391]	Time 0.199 (0.199)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [252][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.808)
Epoch: [252][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.939)
Epoch: [252][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.995)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [253][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [253][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.932)
Epoch: [253][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.865)
Epoch: [253][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.917)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [254][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [254][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.512)
Epoch: [254][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.195)
Epoch: [254][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.120)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [255][0/391]	Time 0.207 (0.207)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [255][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.955)
Epoch: [255][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.911)
Epoch: [255][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.089)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [256][0/391]	Time 0.203 (0.203)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [256][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.210)
Epoch: [256][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.129)
Epoch: [256][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.086)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [257][0/391]	Time 0.205 (0.205)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [257][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.102)
Epoch: [257][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.129)
Epoch: [257][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.045)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [258][0/391]	Time 0.201 (0.201)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [258][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.179)
Epoch: [258][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.075)
Epoch: [258][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 2.344 (10.148)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [259][0/391]	Time 0.201 (0.201)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [259][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.002)
Epoch: [259][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.020)
Epoch: [259][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.982)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [260][0/391]	Time 0.203 (0.203)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [260][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.265)
Epoch: [260][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.059)
Epoch: [260][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.980)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [261][0/391]	Time 0.213 (0.213)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [261][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.545)
Epoch: [261][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.075)
Epoch: [261][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.938)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [262][0/391]	Time 0.212 (0.212)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [262][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.978)
Epoch: [262][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.067)
Epoch: [262][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.016)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [263][0/391]	Time 0.202 (0.202)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [263][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.940)
Epoch: [263][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.787)
Epoch: [263][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.003)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [264][0/391]	Time 0.203 (0.203)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [264][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.677)
Epoch: [264][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.020)
Epoch: [264][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.037)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [265][0/391]	Time 0.208 (0.208)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (16.406)
Epoch: [265][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.350)
Epoch: [265][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.133)
Epoch: [265][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.988)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [266][0/391]	Time 0.206 (0.206)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [266][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.365)
Epoch: [266][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.335)
Epoch: [266][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.133)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [267][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [267][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.329)
Epoch: [267][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.845)
Epoch: [267][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.886)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [268][0/391]	Time 0.206 (0.206)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [268][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.653)
Epoch: [268][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.079)
Epoch: [268][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.110)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [269][0/391]	Time 0.205 (0.205)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [269][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.878)
Epoch: [269][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.954)
Epoch: [269][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.982)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [270][0/391]	Time 0.204 (0.204)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (6.250)
Epoch: [270][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (9.886)
Epoch: [270][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.927)
Epoch: [270][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.977)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [271][0/391]	Time 0.205 (0.205)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [271][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.172)
Epoch: [271][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.207)
Epoch: [271][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.006)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [272][0/391]	Time 0.202 (0.202)	Data 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (12.500)
Epoch: [272][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.110)
Epoch: [272][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.071)
Epoch: [272][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.977)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [273][0/391]	Time 0.198 (0.198)	Data 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [273][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.265)
Epoch: [273][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.028)
Epoch: [273][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.039)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [274][0/391]	Time 0.198 (0.198)	Data 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [274][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.450)
Epoch: [274][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.374)
Epoch: [274][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.174)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [275][0/391]	Time 0.196 (0.196)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [275][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.002)
Epoch: [275][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.102)
Epoch: [275][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.029)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [276][0/391]	Time 0.202 (0.202)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [276][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.071)
Epoch: [276][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (10.082)
Epoch: [276][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 16.406 (10.117)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [277][0/391]	Time 0.202 (0.202)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [277][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.226)
Epoch: [277][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.397)
Epoch: [277][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.112)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [278][0/391]	Time 0.199 (0.199)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (7.812)
Epoch: [278][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.723)
Epoch: [278][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.884)
Epoch: [278][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.936)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [279][0/391]	Time 0.197 (0.197)	Data 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (11.719)
Epoch: [279][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (9.739)
Epoch: [279][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.935)
Epoch: [279][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.001)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [280][0/391]	Time 0.196 (0.196)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [280][100/391]	Time 0.036 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.955)
Epoch: [280][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.939)
Epoch: [280][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.938)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [281][0/391]	Time 0.200 (0.200)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 3.906 (3.906)
Epoch: [281][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (10.613)
Epoch: [281][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (10.191)
Epoch: [281][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.133)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [282][0/391]	Time 0.201 (0.201)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [282][100/391]	Time 0.036 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.172)
Epoch: [282][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.188 (10.183)
Epoch: [282][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.073)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [283][0/391]	Time 0.198 (0.198)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (5.469)
Epoch: [283][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.808)
Epoch: [283][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 5.469 (9.822)
Epoch: [283][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.892)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [284][0/391]	Time 0.197 (0.197)	Data 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [284][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.839)
Epoch: [284][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.892)
Epoch: [284][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (9.956)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [285][0/391]	Time 0.197 (0.197)	Data 0.156 (0.156)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [285][100/391]	Time 0.035 (0.037)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.032)
Epoch: [285][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (9.981)
Epoch: [285][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.990)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [286][0/391]	Time 0.200 (0.200)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [286][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.048)
Epoch: [286][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.079)
Epoch: [286][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 4.688 (10.091)
Test: [0/79]	Time 0.163 (0.163)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [287][0/391]	Time 0.206 (0.206)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [287][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.164)
Epoch: [287][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 15.625 (10.028)
Epoch: [287][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.089)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [288][0/391]	Time 0.209 (0.209)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [288][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 2.344 (9.831)
Epoch: [288][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.768)
Epoch: [288][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.892)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [289][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [289][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.886)
Epoch: [289][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.020)
Epoch: [289][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.055)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [290][0/391]	Time 0.202 (0.202)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 17.969 (17.969)
Epoch: [290][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.048)
Epoch: [290][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.183)
Epoch: [290][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.190)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [291][0/391]	Time 0.200 (0.200)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (8.594)
Epoch: [291][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.118)
Epoch: [291][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.094)
Epoch: [291][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (9.946)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [292][0/391]	Time 0.204 (0.204)	Data 0.155 (0.155)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [292][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (9.940)
Epoch: [292][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (10.036)
Epoch: [292][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.917)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [293][0/391]	Time 0.201 (0.201)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [293][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (9.978)
Epoch: [293][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (9.974)
Epoch: [293][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.026)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [294][0/391]	Time 0.203 (0.203)	Data 0.154 (0.154)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.375)
Epoch: [294][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.002)
Epoch: [294][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (9.962)
Epoch: [294][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.812 (10.029)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [295][0/391]	Time 0.201 (0.201)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [295][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (9.793)
Epoch: [295][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.040)
Epoch: [295][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.013)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [296][0/391]	Time 0.200 (0.200)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (13.281)
Epoch: [296][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 9.375 (10.265)
Epoch: [296][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 8.594 (10.001)
Epoch: [296][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 11.719 (10.016)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [297][0/391]	Time 0.199 (0.199)	Data 0.157 (0.157)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (7.031)
Epoch: [297][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.032)
Epoch: [297][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.125)
Epoch: [297][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 14.062 (10.128)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [298][0/391]	Time 0.203 (0.203)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.938)
Epoch: [298][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 13.281 (9.769)
Epoch: [298][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 12.500 (9.830)
Epoch: [298][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.938 (10.135)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [299][0/391]	Time 0.201 (0.201)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
Epoch: [299][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[nan])	Prec@1 6.250 (10.295)
Epoch: [299][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 7.031 (10.024)
Epoch: [299][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.016)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000

 Elapsed time for training  1:19:54.408313

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 0.0 (unstructured) 0 (structured)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        nan, nan, nan, nan], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Total parameter pruned: 0.0 (unstructured) 0 (structured)

Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000

 Total elapsed time  1:19:56.506576 
 FINETUNING


 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

Total parameter pruned: 0.0 (unstructured) 0 (structured)

Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[nan])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [300][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [300][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.195)
Epoch: [300][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.012)
Epoch: [300][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.967)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [301][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 16.406 (16.406)
Epoch: [301][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.568)
Epoch: [301][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.865)
Epoch: [301][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.868)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [302][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 17.188 (17.188)
Epoch: [302][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.924)
Epoch: [302][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.869)
Epoch: [302][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (10.094)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [303][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [303][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.831)
Epoch: [303][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.005)
Epoch: [303][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.886)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [304][0/391]	Time 0.192 (0.192)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [304][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.909)
Epoch: [304][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.040)
Epoch: [304][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.001)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [305][0/391]	Time 0.192 (0.192)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [305][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 18.750 (9.878)
Epoch: [305][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.047)
Epoch: [305][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.962)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [306][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [306][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.241)
Epoch: [306][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.024)
Epoch: [306][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.037)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [307][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [307][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.955)
Epoch: [307][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.059)
Epoch: [307][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.112)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [308][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [308][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.886)
Epoch: [308][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.884)
Epoch: [308][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.001)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [309][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [309][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.210)
Epoch: [309][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.168)
Epoch: [309][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.016)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [310][0/391]	Time 0.239 (0.239)	Data 0.215 (0.215)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [310][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.226)
Epoch: [310][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.032)
Epoch: [310][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.993)
Test: [0/79]	Time 0.197 (0.197)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [311][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [311][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.172)
Epoch: [311][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.059)
Epoch: [311][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.949)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [312][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [312][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.342)
Epoch: [312][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.207)
Epoch: [312][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.006)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [313][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [313][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.295)
Epoch: [313][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.016)
Epoch: [313][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.964)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [314][0/391]	Time 0.195 (0.195)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [314][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.367)
Epoch: [314][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.970)
Epoch: [314][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.003)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [315][0/391]	Time 0.194 (0.194)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [315][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.514)
Epoch: [315][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.725)
Epoch: [315][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 17.188 (9.881)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [316][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [316][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.048)
Epoch: [316][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.001)
Epoch: [316][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.071)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [317][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [317][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.986)
Epoch: [317][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.942)
Epoch: [317][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.094)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [318][0/391]	Time 0.195 (0.195)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [318][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.932)
Epoch: [318][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.152)
Epoch: [318][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.078)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [319][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [319][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.326)
Epoch: [319][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.939)
Epoch: [319][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.985)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [320][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [320][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.032)
Epoch: [320][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.989)
Epoch: [320][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.951)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [321][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [321][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.326)
Epoch: [321][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.009)
Epoch: [321][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.058)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [322][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [322][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.715)
Epoch: [322][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.841)
Epoch: [322][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.969)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [323][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [323][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.125)
Epoch: [323][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.935)
Epoch: [323][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.006)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [324][0/391]	Time 0.188 (0.188)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [324][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.909)
Epoch: [324][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.931)
Epoch: [324][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.006)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [325][0/391]	Time 0.205 (0.205)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [325][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.025)
Epoch: [325][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.985)
Epoch: [325][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.065)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [326][0/391]	Time 0.187 (0.187)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [326][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.427)
Epoch: [326][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.285)
Epoch: [326][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.993)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [327][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [327][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.599)
Epoch: [327][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.005)
Epoch: [327][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.089)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [328][0/391]	Time 0.186 (0.186)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [328][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.599)
Epoch: [328][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.869)
Epoch: [328][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.964)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [329][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [329][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.584)
Epoch: [329][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.954)
Epoch: [329][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.029)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [330][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [330][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.295)
Epoch: [330][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.152)
Epoch: [330][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.099)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [331][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [331][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.295)
Epoch: [331][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.137)
Epoch: [331][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.063)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [332][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [332][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.203)
Epoch: [332][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.966)
Epoch: [332][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.936)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [333][0/391]	Time 0.186 (0.186)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [333][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.924)
Epoch: [333][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.818)
Epoch: [333][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.956)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [334][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [334][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.978)
Epoch: [334][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.915)
Epoch: [334][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.977)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [335][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [335][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.388)
Epoch: [335][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.234)
Epoch: [335][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.112)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [336][0/391]	Time 0.192 (0.192)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [336][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.118)
Epoch: [336][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.075)
Epoch: [336][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.055)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [337][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [337][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.017)
Epoch: [337][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.106)
Epoch: [337][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.013)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [338][0/391]	Time 0.187 (0.187)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [338][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.071)
Epoch: [338][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.981)
Epoch: [338][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.050)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [339][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [339][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.793)
Epoch: [339][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.888)
Epoch: [339][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.886)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [340][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [340][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.839)
Epoch: [340][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (9.849)
Epoch: [340][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.091)
Test: [0/79]	Time 0.150 (0.150)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [341][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [341][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.265)
Epoch: [341][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.191)
Epoch: [341][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.097)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [342][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [342][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.839)
Epoch: [342][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.861)
Epoch: [342][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.975)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [343][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [343][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.234)
Epoch: [343][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.113)
Epoch: [343][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.933)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [344][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [344][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.257)
Epoch: [344][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.160)
Epoch: [344][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.011)
Test: [0/79]	Time 0.149 (0.149)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [345][0/391]	Time 0.188 (0.188)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [345][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.551)
Epoch: [345][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.413)
Epoch: [345][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.232)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [346][0/391]	Time 0.185 (0.185)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [346][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.700)
Epoch: [346][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.876)
Epoch: [346][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.998)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [347][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [347][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 21.094 (9.909)
Epoch: [347][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.958)
Epoch: [347][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.001)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [348][0/391]	Time 0.193 (0.193)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [348][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.715)
Epoch: [348][200/391]	Time 0.018 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (9.896)
Epoch: [348][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.972)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(nan, device='cuda:0')
Epoch: [349][0/391]	Time 0.187 (0.187)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [349][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.886)
Epoch: [349][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.040)
Epoch: [349][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.943)
Test: [0/79]	Time 0.151 (0.151)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [350][0/391]	Time 0.187 (0.187)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [350][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.234)
Epoch: [350][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.005)
Epoch: [350][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.946)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [351][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [351][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.692)
Epoch: [351][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.853)
Epoch: [351][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.933)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [352][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [352][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.412)
Epoch: [352][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.277)
Epoch: [352][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.032)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [353][0/391]	Time 0.188 (0.188)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [353][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.932)
Epoch: [353][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.012)
Epoch: [353][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.019)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [354][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [354][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.924)
Epoch: [354][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.795)
Epoch: [354][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.972)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [355][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [355][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (9.669)
Epoch: [355][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.795)
Epoch: [355][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.759)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [356][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [356][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.125)
Epoch: [356][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.981)
Epoch: [356][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.998)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [357][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [357][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.978)
Epoch: [357][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.012)
Epoch: [357][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.032)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [358][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [358][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.731)
Epoch: [358][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.810)
Epoch: [358][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.881)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [359][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [359][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.955)
Epoch: [359][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.927)
Epoch: [359][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (9.972)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [360][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [360][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.808)
Epoch: [360][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.040)
Epoch: [360][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.021)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [361][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 3.906 (3.906)
Epoch: [361][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.450)
Epoch: [361][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.028)
Epoch: [361][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.003)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [362][0/391]	Time 0.187 (0.187)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [362][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.754)
Epoch: [362][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.915)
Epoch: [362][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.930)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [363][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [363][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.646)
Epoch: [363][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.981)
Epoch: [363][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.943)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [364][0/391]	Time 0.190 (0.190)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [364][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.708)
Epoch: [364][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.931)
Epoch: [364][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.024)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [365][0/391]	Time 0.188 (0.188)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [365][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.994)
Epoch: [365][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.218)
Epoch: [365][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.001)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [366][0/391]	Time 0.194 (0.194)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [366][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.102)
Epoch: [366][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.094)
Epoch: [366][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.071)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [367][0/391]	Time 0.191 (0.191)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [367][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.195)
Epoch: [367][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.199)
Epoch: [367][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.029)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [368][0/391]	Time 0.189 (0.189)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [368][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.311)
Epoch: [368][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.946)
Epoch: [368][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.042)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [369][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [369][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.839)
Epoch: [369][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.129)
Epoch: [369][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.086)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [370][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [370][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.978)
Epoch: [370][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.989)
Epoch: [370][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.993)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [371][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [371][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.862)
Epoch: [371][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.981)
Epoch: [371][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.990)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [372][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [372][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.017)
Epoch: [372][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.009)
Epoch: [372][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.941)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [373][0/391]	Time 0.195 (0.195)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [373][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.280)
Epoch: [373][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.970)
Epoch: [373][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.946)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [374][0/391]	Time 0.197 (0.197)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [374][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.094)
Epoch: [374][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.082)
Epoch: [374][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.146)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [375][0/391]	Time 0.197 (0.197)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [375][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.334)
Epoch: [375][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.067)
Epoch: [375][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.112)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [376][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [376][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.056)
Epoch: [376][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.102)
Epoch: [376][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.941)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [377][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [377][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.187)
Epoch: [377][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.218)
Epoch: [377][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.047)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [378][0/391]	Time 0.192 (0.192)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [378][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.009)
Epoch: [378][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.900)
Epoch: [378][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.990)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [379][0/391]	Time 0.200 (0.200)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [379][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.272)
Epoch: [379][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.168)
Epoch: [379][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.058)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [380][0/391]	Time 0.197 (0.197)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [380][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.599)
Epoch: [380][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (9.861)
Epoch: [380][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 3.125 (10.003)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [381][0/391]	Time 0.199 (0.199)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [381][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.156)
Epoch: [381][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.129)
Epoch: [381][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.037)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [382][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [382][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.963)
Epoch: [382][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.935)
Epoch: [382][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.972)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [383][0/391]	Time 0.191 (0.191)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [383][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.373)
Epoch: [383][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.904)
Epoch: [383][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.055)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [384][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [384][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.149)
Epoch: [384][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.012)
Epoch: [384][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.065)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [385][0/391]	Time 0.190 (0.190)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [385][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.063)
Epoch: [385][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.954)
Epoch: [385][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.889)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [386][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [386][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.210)
Epoch: [386][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.974)
Epoch: [386][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.912)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [387][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [387][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.350)
Epoch: [387][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.137)
Epoch: [387][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.050)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [388][0/391]	Time 0.191 (0.191)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [388][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.909)
Epoch: [388][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.892)
Epoch: [388][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.860)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [389][0/391]	Time 0.191 (0.191)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [389][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.226)
Epoch: [389][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.001)
Epoch: [389][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.019)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [390][0/391]	Time 0.190 (0.190)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [390][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.210)
Epoch: [390][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.032)
Epoch: [390][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.938)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [391][0/391]	Time 0.194 (0.194)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [391][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.319)
Epoch: [391][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.180)
Epoch: [391][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.065)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [392][0/391]	Time 0.190 (0.190)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [392][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.870)
Epoch: [392][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.946)
Epoch: [392][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.133)
Test: [0/79]	Time 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [393][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [393][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.063)
Epoch: [393][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.312)
Epoch: [393][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.102)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [394][0/391]	Time 0.195 (0.195)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [394][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.442)
Epoch: [394][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.152)
Epoch: [394][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.125)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [395][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [395][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.754)
Epoch: [395][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.919)
Epoch: [395][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.899)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [396][0/391]	Time 0.192 (0.192)	Data 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [396][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.172)
Epoch: [396][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.137)
Epoch: [396][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.081)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [397][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [397][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.079)
Epoch: [397][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.954)
Epoch: [397][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.967)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [398][0/391]	Time 0.196 (0.196)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [398][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.901)
Epoch: [398][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.861)
Epoch: [398][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.910)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-05
Grad=  tensor(nan, device='cuda:0')
Epoch: [399][0/391]	Time 0.187 (0.187)	Data 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [399][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.133)
Epoch: [399][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.861)
Epoch: [399][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.969)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [400][0/391]	Time 0.188 (0.188)	Data 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [400][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (9.785)
Epoch: [400][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.985)
Epoch: [400][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.068)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [401][0/391]	Time 0.192 (0.192)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [401][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.094)
Epoch: [401][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.970)
Epoch: [401][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.032)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [402][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [402][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.056)
Epoch: [402][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.970)
Epoch: [402][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.039)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [403][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [403][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.056)
Epoch: [403][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.168)
Epoch: [403][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.034)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [404][0/391]	Time 0.191 (0.191)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [404][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.133)
Epoch: [404][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.133)
Epoch: [404][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.011)
Test: [0/79]	Time 0.152 (0.152)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [405][0/391]	Time 0.196 (0.196)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [405][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.179)
Epoch: [405][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.055)
Epoch: [405][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (10.047)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [406][0/391]	Time 0.198 (0.198)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [406][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.032)
Epoch: [406][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.082)
Epoch: [406][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.029)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [407][0/391]	Time 0.195 (0.195)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [407][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.971)
Epoch: [407][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.853)
Epoch: [407][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.013)
Test: [0/79]	Time 0.153 (0.153)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [408][0/391]	Time 0.201 (0.201)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [408][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.497)
Epoch: [408][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.164)
Epoch: [408][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.117)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [409][0/391]	Time 0.195 (0.195)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [409][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.901)
Epoch: [409][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.946)
Epoch: [409][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.897)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [410][0/391]	Time 0.194 (0.194)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [410][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.009)
Epoch: [410][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.036)
Epoch: [410][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.915)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [411][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [411][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.940)
Epoch: [411][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.997)
Epoch: [411][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.972)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [412][0/391]	Time 0.193 (0.193)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [412][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.118)
Epoch: [412][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.985)
Epoch: [412][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.011)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [413][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [413][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.094)
Epoch: [413][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (10.047)
Epoch: [413][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.065)
Test: [0/79]	Time 0.157 (0.157)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [414][0/391]	Time 0.200 (0.200)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [414][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.893)
Epoch: [414][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.896)
Epoch: [414][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.858)
Test: [0/79]	Time 0.154 (0.154)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [415][0/391]	Time 0.193 (0.193)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [415][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.723)
Epoch: [415][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.958)
Epoch: [415][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.990)
Test: [0/79]	Time 0.155 (0.155)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [416][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [416][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.986)
Epoch: [416][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.981)
Epoch: [416][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.008)
Test: [0/79]	Time 0.160 (0.160)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [417][0/391]	Time 0.191 (0.191)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [417][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.118)
Epoch: [417][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.203)
Epoch: [417][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.117)
Test: [0/79]	Time 0.156 (0.156)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [418][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [418][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.739)
Epoch: [418][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.756)
Epoch: [418][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.972)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [419][0/391]	Time 0.203 (0.203)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [419][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.955)
Epoch: [419][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.977)
Epoch: [419][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.982)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [420][0/391]	Time 0.201 (0.201)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [420][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.698)
Epoch: [420][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.269)
Epoch: [420][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.050)
Test: [0/79]	Time 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [421][0/391]	Time 0.200 (0.200)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [421][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.924)
Epoch: [421][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.071)
Epoch: [421][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.055)
Test: [0/79]	Time 0.161 (0.161)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [422][0/391]	Time 0.202 (0.202)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [422][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.893)
Epoch: [422][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.977)
Epoch: [422][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.985)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [423][0/391]	Time 0.204 (0.204)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [423][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.079)
Epoch: [423][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.141)
Epoch: [423][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.063)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [424][0/391]	Time 0.202 (0.202)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [424][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.870)
Epoch: [424][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.803)
Epoch: [424][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.019)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [425][0/391]	Time 0.201 (0.201)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [425][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.592)
Epoch: [425][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.911)
Epoch: [425][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.990)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [426][0/391]	Time 0.194 (0.194)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [426][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.940)
Epoch: [426][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.094)
Epoch: [426][300/391]	Time 0.020 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.034)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [427][0/391]	Time 0.195 (0.195)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [427][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.141)
Epoch: [427][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.040)
Epoch: [427][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.060)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [428][0/391]	Time 0.204 (0.204)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [428][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.777)
Epoch: [428][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.985)
Epoch: [428][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.798)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [429][0/391]	Time 0.200 (0.200)	Data 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [429][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.947)
Epoch: [429][200/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.296)
Epoch: [429][300/391]	Time 0.018 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.063)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [430][0/391]	Time 0.198 (0.198)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [430][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.141)
Epoch: [430][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.172)
Epoch: [430][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.161)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [431][0/391]	Time 0.208 (0.208)	Data 0.183 (0.183)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [431][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.419)
Epoch: [431][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.098)
Epoch: [431][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.110)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [432][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [432][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.009)
Epoch: [432][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.942)
Epoch: [432][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.037)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [433][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [433][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.195)
Epoch: [433][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.845)
Epoch: [433][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.845)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [434][0/391]	Time 0.199 (0.199)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [434][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.947)
Epoch: [434][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.919)
Epoch: [434][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.990)
Test: [0/79]	Time 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [435][0/391]	Time 0.205 (0.205)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [435][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.172)
Epoch: [435][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.148)
Epoch: [435][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.112)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [436][0/391]	Time 0.196 (0.196)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [436][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.032)
Epoch: [436][200/391]	Time 0.022 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (10.106)
Epoch: [436][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.042)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [437][0/391]	Time 0.198 (0.198)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [437][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.102)
Epoch: [437][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.939)
Epoch: [437][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.980)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [438][0/391]	Time 0.190 (0.190)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [438][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.839)
Epoch: [438][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.876)
Epoch: [438][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.055)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [439][0/391]	Time 0.184 (0.184)	Data 0.158 (0.158)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [439][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.947)
Epoch: [439][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (9.814)
Epoch: [439][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.988)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [440][0/391]	Time 0.196 (0.196)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [440][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.210)
Epoch: [440][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.172)
Epoch: [440][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.026)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [441][0/391]	Time 0.203 (0.203)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [441][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.017)
Epoch: [441][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.931)
Epoch: [441][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.076)
Test: [0/79]	Time 0.181 (0.181)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [442][0/391]	Time 0.197 (0.197)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [442][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.731)
Epoch: [442][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.841)
Epoch: [442][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.855)
Test: [0/79]	Time 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [443][0/391]	Time 0.212 (0.212)	Data 0.185 (0.185)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [443][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.700)
Epoch: [443][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.876)
Epoch: [443][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.837)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [444][0/391]	Time 0.206 (0.206)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [444][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.234)
Epoch: [444][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.164)
Epoch: [444][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.021)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [445][0/391]	Time 0.207 (0.207)	Data 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [445][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.978)
Epoch: [445][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.016)
Epoch: [445][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.019)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [446][0/391]	Time 0.203 (0.203)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [446][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.940)
Epoch: [446][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.927)
Epoch: [446][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.943)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [447][0/391]	Time 0.204 (0.204)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [447][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.847)
Epoch: [447][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.028)
Epoch: [447][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (10.060)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [448][0/391]	Time 0.212 (0.212)	Data 0.186 (0.186)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [448][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.886)
Epoch: [448][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.958)
Epoch: [448][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.930)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-06
Grad=  tensor(nan, device='cuda:0')
Epoch: [449][0/391]	Time 0.219 (0.219)	Data 0.192 (0.192)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [449][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.545)
Epoch: [449][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.880)
Epoch: [449][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.985)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [450][0/391]	Time 0.204 (0.204)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [450][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.048)
Epoch: [450][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.873)
Epoch: [450][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.001)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [451][0/391]	Time 0.203 (0.203)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [451][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.793)
Epoch: [451][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.857)
Epoch: [451][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.886)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [452][0/391]	Time 0.199 (0.199)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [452][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.257)
Epoch: [452][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.040)
Epoch: [452][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.169)
Test: [0/79]	Time 0.166 (0.166)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [453][0/391]	Time 0.198 (0.198)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [453][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.017)
Epoch: [453][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.892)
Epoch: [453][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.091)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [454][0/391]	Time 0.197 (0.197)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [454][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.978)
Epoch: [454][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.125)
Epoch: [454][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.050)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [455][0/391]	Time 0.194 (0.194)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [455][100/391]	Time 0.018 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.971)
Epoch: [455][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.016)
Epoch: [455][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.042)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [456][0/391]	Time 0.200 (0.200)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [456][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.886)
Epoch: [456][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.939)
Epoch: [456][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.998)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [457][0/391]	Time 0.189 (0.189)	Data 0.163 (0.163)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [457][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (10.118)
Epoch: [457][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.222)
Epoch: [457][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.995)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [458][0/391]	Time 0.203 (0.203)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [458][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.824)
Epoch: [458][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.950)
Epoch: [458][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.045)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [459][0/391]	Time 0.201 (0.201)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [459][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.094)
Epoch: [459][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.748)
Epoch: [459][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.980)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [460][0/391]	Time 0.203 (0.203)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [460][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.847)
Epoch: [460][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.818)
Epoch: [460][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.876)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [461][0/391]	Time 0.211 (0.211)	Data 0.186 (0.186)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [461][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.793)
Epoch: [461][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.966)
Epoch: [461][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.021)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [462][0/391]	Time 0.189 (0.189)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [462][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.932)
Epoch: [462][200/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.977)
Epoch: [462][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.013)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [463][0/391]	Time 0.194 (0.194)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [463][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.785)
Epoch: [463][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.818)
Epoch: [463][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.969)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [464][0/391]	Time 0.195 (0.195)	Data 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [464][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.831)
Epoch: [464][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 15.625 (10.001)
Epoch: [464][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.988)
Test: [0/79]	Time 0.162 (0.162)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [465][0/391]	Time 0.191 (0.191)	Data 0.164 (0.164)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [465][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.203)
Epoch: [465][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.981)
Epoch: [465][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.045)
Test: [0/79]	Time 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [466][0/391]	Time 0.186 (0.186)	Data 0.161 (0.161)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [466][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.793)
Epoch: [466][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.931)
Epoch: [466][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.954)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [467][0/391]	Time 0.193 (0.193)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [467][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.955)
Epoch: [467][200/391]	Time 0.020 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.803)
Epoch: [467][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.925)
Test: [0/79]	Time 0.174 (0.174)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [468][0/391]	Time 0.203 (0.203)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [468][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.777)
Epoch: [468][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.760)
Epoch: [468][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.001)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [469][0/391]	Time 0.200 (0.200)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [469][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.653)
Epoch: [469][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.102)
Epoch: [469][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 5.469 (10.013)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [470][0/391]	Time 0.206 (0.206)	Data 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [470][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.249)
Epoch: [470][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.974)
Epoch: [470][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.990)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [471][0/391]	Time 0.211 (0.211)	Data 0.185 (0.185)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [471][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.576)
Epoch: [471][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.888)
Epoch: [471][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (9.967)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [472][0/391]	Time 0.202 (0.202)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [472][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.978)
Epoch: [472][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.141)
Epoch: [472][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.008)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [473][0/391]	Time 0.194 (0.194)	Data 0.167 (0.167)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [473][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.924)
Epoch: [473][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (9.857)
Epoch: [473][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.902)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [474][0/391]	Time 0.217 (0.217)	Data 0.190 (0.190)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [474][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (9.878)
Epoch: [474][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.865)
Epoch: [474][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.011)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [475][0/391]	Time 0.198 (0.198)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [475][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.156)
Epoch: [475][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.024)
Epoch: [475][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.964)
Test: [0/79]	Time 0.168 (0.168)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [476][0/391]	Time 0.206 (0.206)	Data 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [476][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.659)
Epoch: [476][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.382)
Epoch: [476][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.102)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [477][0/391]	Time 0.205 (0.205)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [477][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (10.326)
Epoch: [477][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (9.981)
Epoch: [477][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.972)
Test: [0/79]	Time 0.159 (0.159)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [478][0/391]	Time 0.190 (0.190)	Data 0.165 (0.165)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [478][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.932)
Epoch: [478][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.806)
Epoch: [478][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.943)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [479][0/391]	Time 0.203 (0.203)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [479][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.491)
Epoch: [479][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.771)
Epoch: [479][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.936)
Test: [0/79]	Time 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [480][0/391]	Time 0.198 (0.198)	Data 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [480][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.226)
Epoch: [480][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.927)
Epoch: [480][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.011)
Test: [0/79]	Time 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [481][0/391]	Time 0.201 (0.201)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [481][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.731)
Epoch: [481][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.016)
Epoch: [481][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.039)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [482][0/391]	Time 0.210 (0.210)	Data 0.182 (0.182)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [482][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.878)
Epoch: [482][200/391]	Time 0.020 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.822)
Epoch: [482][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (9.933)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [483][0/391]	Time 0.212 (0.212)	Data 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [483][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.855)
Epoch: [483][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.094)
Epoch: [483][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.969)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [484][0/391]	Time 0.196 (0.196)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 17.188 (17.188)
Epoch: [484][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.203)
Epoch: [484][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.974)
Epoch: [484][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.001)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [485][0/391]	Time 0.198 (0.198)	Data 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [485][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.164)
Epoch: [485][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.001)
Epoch: [485][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.988)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [486][0/391]	Time 0.196 (0.196)	Data 0.170 (0.170)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [486][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.491)
Epoch: [486][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.822)
Epoch: [486][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.941)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [487][0/391]	Time 0.216 (0.216)	Data 0.189 (0.189)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [487][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.257)
Epoch: [487][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (10.028)
Epoch: [487][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.995)
Test: [0/79]	Time 0.183 (0.183)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [488][0/391]	Time 0.199 (0.199)	Data 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [488][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.071)
Epoch: [488][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.849)
Epoch: [488][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.897)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [489][0/391]	Time 0.207 (0.207)	Data 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [489][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.062 (9.661)
Epoch: [489][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (9.904)
Epoch: [489][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.998)
Test: [0/79]	Time 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [490][0/391]	Time 0.201 (0.201)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [490][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (10.048)
Epoch: [490][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.946)
Epoch: [490][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.019)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [491][0/391]	Time 0.204 (0.204)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [491][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.249)
Epoch: [491][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.226)
Epoch: [491][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 14.844 (9.959)
Test: [0/79]	Time 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [492][0/391]	Time 0.195 (0.195)	Data 0.169 (0.169)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [492][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 8.594 (10.249)
Epoch: [492][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.203)
Epoch: [492][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.050)
Test: [0/79]	Time 0.182 (0.182)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [493][0/391]	Time 0.203 (0.203)	Data 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [493][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.056)
Epoch: [493][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.191)
Epoch: [493][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.954)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [494][0/391]	Time 0.208 (0.208)	Data 0.181 (0.181)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [494][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (9.638)
Epoch: [494][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (9.694)
Epoch: [494][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.814)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [495][0/391]	Time 0.201 (0.201)	Data 0.175 (0.175)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [495][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 6.250 (10.056)
Epoch: [495][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.020)
Epoch: [495][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 13.281 (10.011)
Test: [0/79]	Time 0.172 (0.172)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [496][0/391]	Time 0.204 (0.204)	Data 0.177 (0.177)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [496][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 9.375 (10.156)
Epoch: [496][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (10.020)
Epoch: [496][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.042)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [497][0/391]	Time 0.205 (0.205)	Data 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [497][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 4.688 (10.025)
Epoch: [497][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 12.500 (10.063)
Epoch: [497][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.073)
Test: [0/79]	Time 0.179 (0.179)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [498][0/391]	Time 0.206 (0.206)	Data 0.180 (0.180)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [498][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 11.719 (9.855)
Epoch: [498][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 3.906 (10.016)
Epoch: [498][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (9.982)
Test: [0/79]	Time 0.178 (0.178)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-07
Grad=  tensor(nan, device='cuda:0')
Epoch: [499][0/391]	Time 0.213 (0.213)	Data 0.186 (0.186)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [499][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.938 (10.311)
Epoch: [499][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 7.812 (10.296)
Epoch: [499][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss nan (nan) ([nan]+[0.000])	Prec@1 3.906 (10.097)
Test: [0/79]	Time 0.176 (0.176)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000

 Elapsed time for training  1:47:48.732621

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 0.0 (unstructured) 0 (structured)
Test: [0/79]	Time 0.171 (0.171)	Loss nan (nan) ([nan]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
Best accuracy:  10.0
