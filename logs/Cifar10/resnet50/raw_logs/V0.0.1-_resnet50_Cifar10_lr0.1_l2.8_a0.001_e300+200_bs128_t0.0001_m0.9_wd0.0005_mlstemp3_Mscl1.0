V0.0.1-_resnet50_Cifar10_lr0.1_l2.8_a0.001_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5254763960838318, Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.24196940660476685, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.15759095549583435, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.13501641154289246, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.3461485505104065, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.17473100125789642, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21617008745670319, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.14946585893630981, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09060623496770859, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08498729020357132, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11497705429792404, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11804789304733276, Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08379501849412918, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1424030363559723, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.19753389060497284, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.16684924066066742, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.22829987108707428, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12801074981689453, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09603530913591385, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06901206821203232, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12272872775793076, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0835055485367775, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.07954221963882446, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12703275680541992, Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.19747452437877655, Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.15407174825668335, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1602816879749298, Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.10645194351673126, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10600411146879196, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.13483507931232452, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.21709460020065308, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11353497207164764, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0660422295331955, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10686782747507095, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06808818876743317, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.05323619768023491, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.08759226649999619, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07965513318777084, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06643471866846085, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12164679169654846, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09185265004634857, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06330689787864685, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1110600158572197, Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12582343816757202, Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.09035182744264603, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07410024106502533, Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.07018566876649857, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0687103122472763, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.04065759852528572, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.03755198046565056, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.04725675657391548, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.045549724251031876, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06192586570978165, Linear(in_features=2048, out_features=100, bias=True): 0.44340774416923523}
current lr 1.00000e-01
Grad=  tensor(7976.4058, device='cuda:0')
Epoch: [0][0/391]	Time 0.325 (0.325)	Data 0.189 (0.189)	Loss 7.9480 (7.9480) ([4.586]+[3.362])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 9.6365 (10.8615) ([2.387]+[7.249])	Prec@1 15.625 (10.636)
Epoch: [0][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 8.4535 (9.9090) ([2.136]+[6.317])	Prec@1 24.219 (14.191)
Epoch: [0][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 7.5560 (9.2706) ([2.082]+[5.474])	Prec@1 26.562 (16.263)
Test: [0/79]	Time 0.226 (0.226)	Loss 6.7140 (6.7140) ([1.932]+[4.782])	Prec@1 29.688 (29.688)
 * Prec@1 28.550
current lr 1.00000e-01
Grad=  tensor(0.6321, device='cuda:0')
Epoch: [1][0/391]	Time 0.319 (0.319)	Data 0.200 (0.200)	Loss 6.7583 (6.7583) ([1.977]+[4.782])	Prec@1 21.094 (21.094)
Epoch: [1][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 6.0544 (6.3834) ([1.938]+[4.117])	Prec@1 22.656 (26.168)
Epoch: [1][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 5.4778 (6.0569) ([1.883]+[3.595])	Prec@1 34.375 (27.721)
Epoch: [1][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 4.9051 (5.7830) ([1.730]+[3.175])	Prec@1 35.156 (28.610)
Test: [0/79]	Time 0.220 (0.220)	Loss 4.5503 (4.5503) ([1.732]+[2.819])	Prec@1 32.812 (32.812)
 * Prec@1 34.990
current lr 1.00000e-01
Grad=  tensor(1.5432, device='cuda:0')
Epoch: [2][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 4.6055 (4.6055) ([1.787]+[2.819])	Prec@1 34.375 (34.375)
Epoch: [2][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 4.1661 (4.3802) ([1.684]+[2.482])	Prec@1 37.500 (36.386)
Epoch: [2][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 3.6341 (4.2067) ([1.442]+[2.192])	Prec@1 46.875 (36.260)
Epoch: [2][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 3.5586 (4.0409) ([1.630]+[1.928])	Prec@1 37.500 (37.160)
Test: [0/79]	Time 0.214 (0.214)	Loss 3.4230 (3.4230) ([1.691]+[1.732])	Prec@1 36.719 (36.719)
 * Prec@1 40.560
current lr 1.00000e-01
Grad=  tensor(1.2166, device='cuda:0')
Epoch: [3][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 3.1762 (3.1762) ([1.445]+[1.732])	Prec@1 46.094 (46.094)
Epoch: [3][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 3.1262 (3.2150) ([1.580]+[1.546])	Prec@1 43.750 (42.110)
Epoch: [3][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 2.9995 (3.1361) ([1.486]+[1.513])	Prec@1 49.219 (42.572)
Epoch: [3][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 2.9185 (3.0712) ([1.577]+[1.342])	Prec@1 44.531 (43.005)
Test: [0/79]	Time 0.221 (0.221)	Loss 2.8064 (2.8064) ([1.592]+[1.214])	Prec@1 41.406 (41.406)
 * Prec@1 42.760
current lr 1.00000e-01
Grad=  tensor(1.4490, device='cuda:0')
Epoch: [4][0/391]	Time 0.311 (0.311)	Data 0.190 (0.190)	Loss 2.7058 (2.7058) ([1.492]+[1.214])	Prec@1 46.875 (46.875)
Epoch: [4][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 2.7288 (2.5935) ([1.615]+[1.114])	Prec@1 40.625 (47.679)
Epoch: [4][200/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 2.4693 (2.5282) ([1.451]+[1.018])	Prec@1 46.875 (48.414)
Epoch: [4][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 2.2344 (2.4582) ([1.291]+[0.943])	Prec@1 52.344 (49.452)
Test: [0/79]	Time 0.223 (0.223)	Loss 2.3129 (2.3129) ([1.433]+[0.880])	Prec@1 49.219 (49.219)
 * Prec@1 49.770
current lr 1.00000e-01
Grad=  tensor(1.4662, device='cuda:0')
Epoch: [5][0/391]	Time 0.324 (0.324)	Data 0.204 (0.204)	Loss 2.0911 (2.0911) ([1.211]+[0.880])	Prec@1 57.031 (57.031)
Epoch: [5][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 2.0609 (2.0808) ([1.236]+[0.825])	Prec@1 53.125 (55.685)
Epoch: [5][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.9145 (2.0267) ([1.141]+[0.773])	Prec@1 55.469 (56.744)
Epoch: [5][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 2.0153 (1.9890) ([1.271]+[0.744])	Prec@1 56.250 (57.392)
Test: [0/79]	Time 0.220 (0.220)	Loss 2.1011 (2.1011) ([1.408]+[0.693])	Prec@1 52.344 (52.344)
 * Prec@1 50.470
current lr 1.00000e-01
Grad=  tensor(1.3167, device='cuda:0')
Epoch: [6][0/391]	Time 0.312 (0.312)	Data 0.192 (0.192)	Loss 1.6298 (1.6298) ([0.937]+[0.693])	Prec@1 64.062 (64.062)
Epoch: [6][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.6679 (1.7691) ([1.012]+[0.656])	Prec@1 57.812 (60.775)
Epoch: [6][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.5907 (1.7398) ([0.966]+[0.625])	Prec@1 65.625 (61.132)
Epoch: [6][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.5027 (1.7269) ([0.902]+[0.600])	Prec@1 66.406 (61.148)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.7779 (1.7779) ([1.185]+[0.593])	Prec@1 54.688 (54.688)
 * Prec@1 55.390
current lr 1.00000e-01
Grad=  tensor(2.0438, device='cuda:0')
Epoch: [7][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 1.5336 (1.5336) ([0.941]+[0.593])	Prec@1 66.406 (66.406)
Epoch: [7][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 1.7083 (1.5732) ([1.154]+[0.555])	Prec@1 54.688 (64.434)
Epoch: [7][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.5735 (1.5537) ([1.037]+[0.536])	Prec@1 64.062 (64.883)
Epoch: [7][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.3665 (1.5349) ([0.845]+[0.521])	Prec@1 71.875 (65.358)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.6947 (1.6947) ([1.180]+[0.515])	Prec@1 55.469 (55.469)
 * Prec@1 59.890
current lr 1.00000e-01
Grad=  tensor(2.3689, device='cuda:0')
Epoch: [8][0/391]	Time 0.309 (0.309)	Data 0.188 (0.188)	Loss 1.5424 (1.5424) ([1.028]+[0.515])	Prec@1 61.719 (61.719)
Epoch: [8][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.5460 (1.4461) ([1.049]+[0.497])	Prec@1 57.812 (66.808)
Epoch: [8][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 1.3498 (1.4255) ([0.862]+[0.488])	Prec@1 71.875 (67.533)
Epoch: [8][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.4422 (1.4162) ([0.962]+[0.481])	Prec@1 65.625 (67.842)
Test: [0/79]	Time 0.231 (0.231)	Loss 1.5047 (1.5047) ([1.030]+[0.474])	Prec@1 59.375 (59.375)
 * Prec@1 61.420
current lr 1.00000e-01
Grad=  tensor(1.5342, device='cuda:0')
Epoch: [9][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 1.2333 (1.2333) ([0.759]+[0.474])	Prec@1 68.750 (68.750)
Epoch: [9][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.3214 (1.3278) ([0.856]+[0.465])	Prec@1 71.875 (69.941)
Epoch: [9][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.3394 (1.3217) ([0.889]+[0.451])	Prec@1 73.438 (70.114)
Epoch: [9][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 1.3370 (1.3089) ([0.879]+[0.458])	Prec@1 72.656 (70.318)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.8709 (1.8709) ([1.425]+[0.446])	Prec@1 53.906 (53.906)
 * Prec@1 61.030
current lr 1.00000e-01
Grad=  tensor(1.7038, device='cuda:0')
Epoch: [10][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 1.2271 (1.2271) ([0.781]+[0.446])	Prec@1 71.875 (71.875)
Epoch: [10][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.1386 (1.2453) ([0.702]+[0.437])	Prec@1 73.438 (71.759)
Epoch: [10][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0925 (1.2320) ([0.664]+[0.429])	Prec@1 78.125 (71.883)
Epoch: [10][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.3086 (1.2308) ([0.884]+[0.425])	Prec@1 70.312 (72.036)
Test: [0/79]	Time 0.217 (0.217)	Loss 1.1301 (1.1301) ([0.712]+[0.418])	Prec@1 74.219 (74.219)
 * Prec@1 72.150
current lr 1.00000e-01
Grad=  tensor(1.5816, device='cuda:0')
Epoch: [11][0/391]	Time 0.309 (0.309)	Data 0.190 (0.190)	Loss 1.1319 (1.1319) ([0.714]+[0.418])	Prec@1 77.344 (77.344)
Epoch: [11][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.2570 (1.2010) ([0.842]+[0.415])	Prec@1 68.750 (72.687)
Epoch: [11][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.2503 (1.1935) ([0.844]+[0.406])	Prec@1 67.188 (72.738)
Epoch: [11][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.3179 (1.1877) ([0.907]+[0.411])	Prec@1 70.312 (73.061)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.4819 (1.4819) ([1.087]+[0.395])	Prec@1 64.062 (64.062)
 * Prec@1 64.310
current lr 1.00000e-01
Grad=  tensor(1.9368, device='cuda:0')
Epoch: [12][0/391]	Time 0.303 (0.303)	Data 0.183 (0.183)	Loss 1.2756 (1.2756) ([0.880]+[0.395])	Prec@1 73.438 (73.438)
Epoch: [12][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.3212 (1.1594) ([0.928]+[0.393])	Prec@1 71.094 (73.167)
Epoch: [12][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.2392 (1.1610) ([0.846]+[0.393])	Prec@1 75.781 (73.239)
Epoch: [12][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 1.1106 (1.1511) ([0.725]+[0.386])	Prec@1 74.219 (73.640)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.3043 (1.3043) ([0.928]+[0.376])	Prec@1 67.969 (67.969)
 * Prec@1 64.330
current lr 1.00000e-01
Grad=  tensor(2.8360, device='cuda:0')
Epoch: [13][0/391]	Time 0.315 (0.315)	Data 0.195 (0.195)	Loss 1.2352 (1.2352) ([0.859]+[0.376])	Prec@1 73.438 (73.438)
Epoch: [13][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.1418 (1.1066) ([0.768]+[0.373])	Prec@1 73.438 (74.892)
Epoch: [13][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9143 (1.1024) ([0.539]+[0.375])	Prec@1 84.375 (74.708)
Epoch: [13][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 1.1010 (1.1052) ([0.728]+[0.373])	Prec@1 75.000 (74.587)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.3733 (1.3733) ([1.005]+[0.368])	Prec@1 67.969 (67.969)
 * Prec@1 63.550
current lr 1.00000e-01
Grad=  tensor(1.9139, device='cuda:0')
Epoch: [14][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 1.0321 (1.0321) ([0.664]+[0.368])	Prec@1 75.781 (75.781)
Epoch: [14][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 1.1066 (1.0793) ([0.744]+[0.362])	Prec@1 74.219 (75.286)
Epoch: [14][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0381 (1.0753) ([0.682]+[0.356])	Prec@1 77.344 (75.532)
Epoch: [14][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9902 (1.0725) ([0.637]+[0.353])	Prec@1 80.469 (75.470)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.3116 (1.3116) ([0.965]+[0.347])	Prec@1 69.531 (69.531)
 * Prec@1 69.590
current lr 1.00000e-01
Grad=  tensor(1.5995, device='cuda:0')
Epoch: [15][0/391]	Time 0.306 (0.306)	Data 0.185 (0.185)	Loss 0.9008 (0.9008) ([0.554]+[0.347])	Prec@1 79.688 (79.688)
Epoch: [15][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.1175 (1.0313) ([0.774]+[0.343])	Prec@1 75.781 (76.276)
Epoch: [15][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9190 (1.0340) ([0.577]+[0.342])	Prec@1 82.031 (76.081)
Epoch: [15][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.0221 (1.0309) ([0.681]+[0.341])	Prec@1 75.000 (76.184)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.0277 (1.0277) ([0.688]+[0.340])	Prec@1 71.094 (71.094)
 * Prec@1 73.940
current lr 1.00000e-01
Grad=  tensor(1.9754, device='cuda:0')
Epoch: [16][0/391]	Time 0.314 (0.314)	Data 0.189 (0.189)	Loss 1.0216 (1.0216) ([0.682]+[0.340])	Prec@1 78.125 (78.125)
Epoch: [16][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.9318 (1.0189) ([0.594]+[0.338])	Prec@1 82.031 (76.586)
Epoch: [16][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.1792 (1.0101) ([0.844]+[0.335])	Prec@1 67.188 (77.048)
Epoch: [16][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9925 (1.0086) ([0.659]+[0.333])	Prec@1 78.906 (76.967)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.1028 (1.1028) ([0.770]+[0.332])	Prec@1 72.656 (72.656)
 * Prec@1 71.930
current lr 1.00000e-01
Grad=  tensor(2.1000, device='cuda:0')
Epoch: [17][0/391]	Time 0.316 (0.316)	Data 0.191 (0.191)	Loss 0.9474 (0.9474) ([0.615]+[0.332])	Prec@1 81.250 (81.250)
Epoch: [17][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.0264 (1.0031) ([0.694]+[0.332])	Prec@1 75.000 (76.949)
Epoch: [17][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9253 (0.9931) ([0.594]+[0.331])	Prec@1 81.250 (77.208)
Epoch: [17][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8989 (0.9954) ([0.569]+[0.330])	Prec@1 78.906 (77.183)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.2487 (1.2487) ([0.919]+[0.329])	Prec@1 69.531 (69.531)
 * Prec@1 69.710
current lr 1.00000e-01
Grad=  tensor(1.8686, device='cuda:0')
Epoch: [18][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.8340 (0.8340) ([0.505]+[0.329])	Prec@1 81.250 (81.250)
Epoch: [18][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8624 (0.9582) ([0.535]+[0.327])	Prec@1 83.594 (78.318)
Epoch: [18][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8587 (0.9682) ([0.532]+[0.327])	Prec@1 85.938 (78.024)
Epoch: [18][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 1.1171 (0.9723) ([0.790]+[0.327])	Prec@1 72.656 (77.697)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.9519 (0.9519) ([0.626]+[0.326])	Prec@1 78.906 (78.906)
 * Prec@1 73.820
current lr 1.00000e-01
Grad=  tensor(1.6535, device='cuda:0')
Epoch: [19][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.8858 (0.8858) ([0.560]+[0.326])	Prec@1 82.031 (82.031)
Epoch: [19][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8772 (0.9552) ([0.552]+[0.325])	Prec@1 77.344 (78.581)
Epoch: [19][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.2810 (0.9605) ([0.957]+[0.324])	Prec@1 65.625 (78.315)
Epoch: [19][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8622 (0.9661) ([0.538]+[0.324])	Prec@1 82.812 (78.032)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.1494 (1.1494) ([0.826]+[0.323])	Prec@1 73.438 (73.438)
 * Prec@1 73.490
current lr 1.00000e-01
Grad=  tensor(2.0026, device='cuda:0')
Epoch: [20][0/391]	Time 0.300 (0.300)	Data 0.181 (0.181)	Loss 0.9991 (0.9991) ([0.676]+[0.323])	Prec@1 75.000 (75.000)
Epoch: [20][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.9601 (0.9543) ([0.637]+[0.323])	Prec@1 78.906 (78.148)
Epoch: [20][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9743 (0.9597) ([0.651]+[0.324])	Prec@1 77.344 (78.242)
Epoch: [20][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.9255 (0.9564) ([0.604]+[0.321])	Prec@1 80.469 (78.346)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.1114 (1.1114) ([0.790]+[0.322])	Prec@1 75.781 (75.781)
 * Prec@1 75.480
current lr 1.00000e-01
Grad=  tensor(1.6198, device='cuda:0')
Epoch: [21][0/391]	Time 0.321 (0.321)	Data 0.201 (0.201)	Loss 0.7821 (0.7821) ([0.461]+[0.322])	Prec@1 82.812 (82.812)
Epoch: [21][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8590 (0.9327) ([0.539]+[0.320])	Prec@1 81.250 (79.138)
Epoch: [21][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.0811 (0.9410) ([0.761]+[0.320])	Prec@1 75.000 (78.720)
Epoch: [21][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9425 (0.9442) ([0.622]+[0.320])	Prec@1 78.906 (78.654)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.7641 (1.7641) ([1.445]+[0.319])	Prec@1 56.250 (56.250)
 * Prec@1 59.050
current lr 1.00000e-01
Grad=  tensor(1.7590, device='cuda:0')
Epoch: [22][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.9111 (0.9111) ([0.592]+[0.319])	Prec@1 78.125 (78.125)
Epoch: [22][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.0671 (0.9223) ([0.749]+[0.318])	Prec@1 71.875 (79.324)
Epoch: [22][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.8958 (0.9287) ([0.577]+[0.319])	Prec@1 81.250 (79.011)
Epoch: [22][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8237 (0.9341) ([0.505]+[0.319])	Prec@1 80.469 (78.828)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.6467 (1.6467) ([1.328]+[0.319])	Prec@1 62.500 (62.500)
 * Prec@1 64.810
current lr 1.00000e-01
Grad=  tensor(1.5614, device='cuda:0')
Epoch: [23][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.8949 (0.8949) ([0.576]+[0.319])	Prec@1 82.812 (82.812)
Epoch: [23][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 1.0271 (0.9233) ([0.709]+[0.318])	Prec@1 75.781 (79.409)
Epoch: [23][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7767 (0.9317) ([0.459]+[0.317])	Prec@1 84.375 (79.023)
Epoch: [23][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.9751 (0.9328) ([0.658]+[0.317])	Prec@1 80.469 (79.057)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.2056 (1.2056) ([0.889]+[0.316])	Prec@1 68.750 (68.750)
 * Prec@1 74.310
current lr 1.00000e-01
Grad=  tensor(2.0707, device='cuda:0')
Epoch: [24][0/391]	Time 0.304 (0.304)	Data 0.186 (0.186)	Loss 0.8945 (0.8945) ([0.578]+[0.316])	Prec@1 81.250 (81.250)
Epoch: [24][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 1.0486 (0.9036) ([0.733]+[0.316])	Prec@1 73.438 (79.479)
Epoch: [24][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.0150 (0.9242) ([0.699]+[0.316])	Prec@1 75.781 (79.206)
Epoch: [24][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.9885 (0.9189) ([0.674]+[0.314])	Prec@1 74.219 (79.433)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.2786 (1.2786) ([0.963]+[0.316])	Prec@1 65.625 (65.625)
 * Prec@1 66.260
current lr 1.00000e-01
Grad=  tensor(2.4171, device='cuda:0')
Epoch: [25][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.9279 (0.9279) ([0.612]+[0.316])	Prec@1 79.688 (79.688)
Epoch: [25][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8995 (0.9110) ([0.583]+[0.316])	Prec@1 76.562 (79.873)
Epoch: [25][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 1.0675 (0.9222) ([0.752]+[0.316])	Prec@1 72.656 (79.400)
Epoch: [25][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.9896 (0.9250) ([0.674]+[0.315])	Prec@1 77.344 (79.168)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.1346 (1.1346) ([0.819]+[0.315])	Prec@1 76.562 (76.562)
 * Prec@1 72.250
current lr 1.00000e-01
Grad=  tensor(2.4471, device='cuda:0')
Epoch: [26][0/391]	Time 0.311 (0.311)	Data 0.193 (0.193)	Loss 0.9257 (0.9257) ([0.610]+[0.315])	Prec@1 78.906 (78.906)
Epoch: [26][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8698 (0.9099) ([0.554]+[0.316])	Prec@1 80.469 (79.602)
Epoch: [26][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9257 (0.9143) ([0.610]+[0.315])	Prec@1 78.906 (79.439)
Epoch: [26][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9480 (0.9134) ([0.634]+[0.314])	Prec@1 73.438 (79.415)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.0779 (1.0779) ([0.765]+[0.313])	Prec@1 73.438 (73.438)
 * Prec@1 73.780
current lr 1.00000e-01
Grad=  tensor(2.1495, device='cuda:0')
Epoch: [27][0/391]	Time 0.307 (0.307)	Data 0.183 (0.183)	Loss 0.8658 (0.8658) ([0.552]+[0.313])	Prec@1 79.688 (79.688)
Epoch: [27][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.0295 (0.8804) ([0.716]+[0.313])	Prec@1 78.125 (80.345)
Epoch: [27][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8083 (0.8955) ([0.495]+[0.313])	Prec@1 83.594 (80.037)
Epoch: [27][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0020 (0.9095) ([0.690]+[0.312])	Prec@1 71.875 (79.451)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.1329 (1.1329) ([0.821]+[0.312])	Prec@1 72.656 (72.656)
 * Prec@1 72.880
current lr 1.00000e-01
Grad=  tensor(1.8443, device='cuda:0')
Epoch: [28][0/391]	Time 0.314 (0.314)	Data 0.189 (0.189)	Loss 0.8333 (0.8333) ([0.522]+[0.312])	Prec@1 80.469 (80.469)
Epoch: [28][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8591 (0.8938) ([0.547]+[0.312])	Prec@1 79.688 (79.834)
Epoch: [28][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.8317 (0.9026) ([0.520]+[0.312])	Prec@1 85.938 (79.715)
Epoch: [28][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8480 (0.8981) ([0.538]+[0.310])	Prec@1 82.031 (79.864)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.9533 (0.9533) ([0.643]+[0.310])	Prec@1 78.125 (78.125)
 * Prec@1 76.540
current lr 1.00000e-01
Grad=  tensor(1.9394, device='cuda:0')
Epoch: [29][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.9307 (0.9307) ([0.620]+[0.310])	Prec@1 80.469 (80.469)
Epoch: [29][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.9422 (0.8860) ([0.631]+[0.311])	Prec@1 79.688 (80.353)
Epoch: [29][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.9845 (0.8964) ([0.673]+[0.311])	Prec@1 75.000 (79.909)
Epoch: [29][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.8570 (0.8966) ([0.548]+[0.309])	Prec@1 83.594 (79.994)
Test: [0/79]	Time 0.229 (0.229)	Loss 1.0370 (1.0370) ([0.726]+[0.311])	Prec@1 75.781 (75.781)
 * Prec@1 74.040
current lr 1.00000e-01
Grad=  tensor(2.0941, device='cuda:0')
Epoch: [30][0/391]	Time 0.309 (0.309)	Data 0.190 (0.190)	Loss 0.8528 (0.8528) ([0.542]+[0.311])	Prec@1 81.250 (81.250)
Epoch: [30][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.0495 (0.9178) ([0.738]+[0.311])	Prec@1 81.250 (78.953)
Epoch: [30][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7920 (0.9112) ([0.481]+[0.311])	Prec@1 82.812 (79.555)
Epoch: [30][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9168 (0.9041) ([0.606]+[0.311])	Prec@1 79.688 (79.768)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.0983 (1.0983) ([0.788]+[0.311])	Prec@1 75.000 (75.000)
 * Prec@1 73.410
current lr 1.00000e-01
Grad=  tensor(2.4852, device='cuda:0')
Epoch: [31][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.9561 (0.9561) ([0.646]+[0.311])	Prec@1 82.031 (82.031)
Epoch: [31][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9028 (0.8881) ([0.593]+[0.310])	Prec@1 77.344 (80.028)
Epoch: [31][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8006 (0.8957) ([0.491]+[0.310])	Prec@1 82.812 (79.944)
Epoch: [31][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8997 (0.8995) ([0.590]+[0.310])	Prec@1 80.469 (79.807)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.9739 (0.9739) ([0.664]+[0.310])	Prec@1 76.562 (76.562)
 * Prec@1 73.330
current lr 1.00000e-01
Grad=  tensor(1.9875, device='cuda:0')
Epoch: [32][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.8430 (0.8430) ([0.533]+[0.310])	Prec@1 82.812 (82.812)
Epoch: [32][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.9536 (0.8795) ([0.644]+[0.309])	Prec@1 78.906 (80.407)
Epoch: [32][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0219 (0.8858) ([0.713]+[0.309])	Prec@1 74.219 (80.154)
Epoch: [32][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.1567 (0.8883) ([0.849]+[0.307])	Prec@1 71.875 (80.207)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.9518 (0.9518) ([0.645]+[0.306])	Prec@1 78.906 (78.906)
 * Prec@1 77.480
current lr 1.00000e-01
Grad=  tensor(1.8204, device='cuda:0')
Epoch: [33][0/391]	Time 0.312 (0.312)	Data 0.193 (0.193)	Loss 0.9163 (0.9163) ([0.610]+[0.306])	Prec@1 80.469 (80.469)
Epoch: [33][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.0462 (0.8682) ([0.740]+[0.307])	Prec@1 77.344 (80.755)
Epoch: [33][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8300 (0.8844) ([0.521]+[0.309])	Prec@1 85.156 (80.166)
Epoch: [33][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8935 (0.8853) ([0.587]+[0.307])	Prec@1 79.688 (80.196)
Test: [0/79]	Time 0.223 (0.223)	Loss 1.2146 (1.2146) ([0.908]+[0.306])	Prec@1 71.094 (71.094)
 * Prec@1 71.750
current lr 1.00000e-01
Grad=  tensor(2.1830, device='cuda:0')
Epoch: [34][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.9955 (0.9955) ([0.689]+[0.306])	Prec@1 74.219 (74.219)
Epoch: [34][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8606 (0.8648) ([0.555]+[0.306])	Prec@1 85.156 (80.910)
Epoch: [34][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8163 (0.8722) ([0.510]+[0.306])	Prec@1 81.250 (80.550)
Epoch: [34][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.9537 (0.8787) ([0.647]+[0.307])	Prec@1 77.344 (80.344)
Test: [0/79]	Time 0.209 (0.209)	Loss 1.5064 (1.5064) ([1.199]+[0.307])	Prec@1 68.750 (68.750)
 * Prec@1 64.910
current lr 1.00000e-01
Grad=  tensor(1.5185, device='cuda:0')
Epoch: [35][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.7449 (0.7449) ([0.438]+[0.307])	Prec@1 84.375 (84.375)
Epoch: [35][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7411 (0.8683) ([0.434]+[0.307])	Prec@1 86.719 (80.917)
Epoch: [35][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9311 (0.8736) ([0.625]+[0.306])	Prec@1 78.906 (80.787)
Epoch: [35][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.9841 (0.8819) ([0.679]+[0.305])	Prec@1 75.000 (80.547)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.3575 (1.3575) ([1.053]+[0.305])	Prec@1 67.188 (67.188)
 * Prec@1 66.410
current lr 1.00000e-01
Grad=  tensor(2.0362, device='cuda:0')
Epoch: [36][0/391]	Time 0.320 (0.320)	Data 0.196 (0.196)	Loss 1.0154 (1.0154) ([0.711]+[0.305])	Prec@1 75.000 (75.000)
Epoch: [36][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.8814 (0.8697) ([0.578]+[0.304])	Prec@1 80.469 (80.941)
Epoch: [36][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.8509 (0.8765) ([0.546]+[0.305])	Prec@1 81.250 (80.624)
Epoch: [36][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8402 (0.8804) ([0.536]+[0.305])	Prec@1 82.812 (80.430)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.9931 (0.9931) ([0.687]+[0.306])	Prec@1 77.344 (77.344)
 * Prec@1 76.510
current lr 1.00000e-01
Grad=  tensor(2.3357, device='cuda:0')
Epoch: [37][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.8935 (0.8935) ([0.587]+[0.306])	Prec@1 79.688 (79.688)
Epoch: [37][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.1324 (0.8684) ([0.826]+[0.306])	Prec@1 74.219 (80.639)
Epoch: [37][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9016 (0.8661) ([0.596]+[0.305])	Prec@1 79.688 (80.834)
Epoch: [37][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8811 (0.8681) ([0.576]+[0.305])	Prec@1 75.000 (80.669)
Test: [0/79]	Time 0.221 (0.221)	Loss 1.1688 (1.1688) ([0.864]+[0.304])	Prec@1 72.656 (72.656)
 * Prec@1 73.160
current lr 1.00000e-01
Grad=  tensor(2.2146, device='cuda:0')
Epoch: [38][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.8722 (0.8722) ([0.568]+[0.304])	Prec@1 78.906 (78.906)
Epoch: [38][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.9665 (0.8792) ([0.662]+[0.305])	Prec@1 77.344 (80.345)
Epoch: [38][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 1.0882 (0.8816) ([0.784]+[0.304])	Prec@1 74.219 (80.368)
Epoch: [38][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.8513 (0.8777) ([0.547]+[0.304])	Prec@1 79.688 (80.396)
Test: [0/79]	Time 0.221 (0.221)	Loss 1.2631 (1.2631) ([0.959]+[0.304])	Prec@1 71.875 (71.875)
 * Prec@1 73.650
current lr 1.00000e-01
Grad=  tensor(1.6733, device='cuda:0')
Epoch: [39][0/391]	Time 0.305 (0.305)	Data 0.187 (0.187)	Loss 0.6834 (0.6834) ([0.380]+[0.304])	Prec@1 85.156 (85.156)
Epoch: [39][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.8949 (0.8699) ([0.590]+[0.304])	Prec@1 82.812 (80.709)
Epoch: [39][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.9345 (0.8749) ([0.631]+[0.304])	Prec@1 82.031 (80.484)
Epoch: [39][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.7522 (0.8754) ([0.449]+[0.304])	Prec@1 86.719 (80.617)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.9830 (0.9830) ([0.680]+[0.303])	Prec@1 76.562 (76.562)
 * Prec@1 73.720
current lr 1.00000e-01
Grad=  tensor(1.8359, device='cuda:0')
Epoch: [40][0/391]	Time 0.303 (0.303)	Data 0.186 (0.186)	Loss 0.8699 (0.8699) ([0.567]+[0.303])	Prec@1 77.344 (77.344)
Epoch: [40][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.9742 (0.8577) ([0.672]+[0.303])	Prec@1 72.656 (80.662)
Epoch: [40][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.8925 (0.8562) ([0.591]+[0.302])	Prec@1 79.688 (80.939)
Epoch: [40][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.9014 (0.8574) ([0.600]+[0.301])	Prec@1 80.469 (81.019)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.3868 (1.3868) ([1.087]+[0.300])	Prec@1 72.656 (72.656)
 * Prec@1 72.000
current lr 1.00000e-01
Grad=  tensor(2.4890, device='cuda:0')
Epoch: [41][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.8472 (0.8472) ([0.547]+[0.300])	Prec@1 78.906 (78.906)
Epoch: [41][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.7568 (0.8748) ([0.455]+[0.301])	Prec@1 87.500 (80.446)
Epoch: [41][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.7800 (0.8706) ([0.480]+[0.300])	Prec@1 82.031 (80.543)
Epoch: [41][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.9827 (0.8721) ([0.682]+[0.301])	Prec@1 77.344 (80.596)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.0080 (1.0080) ([0.707]+[0.301])	Prec@1 75.781 (75.781)
 * Prec@1 76.750
current lr 1.00000e-01
Grad=  tensor(1.5201, device='cuda:0')
Epoch: [42][0/391]	Time 0.306 (0.306)	Data 0.189 (0.189)	Loss 0.6778 (0.6778) ([0.376]+[0.301])	Prec@1 89.062 (89.062)
Epoch: [42][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.9060 (0.8495) ([0.605]+[0.301])	Prec@1 76.562 (81.072)
Epoch: [42][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.9398 (0.8576) ([0.638]+[0.302])	Prec@1 75.000 (81.025)
Epoch: [42][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.9758 (0.8603) ([0.674]+[0.302])	Prec@1 75.781 (80.887)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.8655 (0.8655) ([0.563]+[0.303])	Prec@1 82.031 (82.031)
 * Prec@1 77.810
current lr 1.00000e-01
Grad=  tensor(2.2790, device='cuda:0')
Epoch: [43][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.8577 (0.8577) ([0.555]+[0.303])	Prec@1 78.125 (78.125)
Epoch: [43][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9114 (0.8640) ([0.609]+[0.302])	Prec@1 82.031 (80.608)
Epoch: [43][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9377 (0.8673) ([0.635]+[0.302])	Prec@1 78.906 (80.749)
Epoch: [43][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6826 (0.8617) ([0.381]+[0.301])	Prec@1 88.281 (80.939)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.9927 (0.9927) ([0.692]+[0.301])	Prec@1 82.812 (82.812)
 * Prec@1 73.770
current lr 1.00000e-01
Grad=  tensor(1.9215, device='cuda:0')
Epoch: [44][0/391]	Time 0.300 (0.300)	Data 0.180 (0.180)	Loss 0.8949 (0.8949) ([0.594]+[0.301])	Prec@1 82.031 (82.031)
Epoch: [44][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7237 (0.8378) ([0.424]+[0.300])	Prec@1 88.281 (81.467)
Epoch: [44][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0824 (0.8592) ([0.783]+[0.300])	Prec@1 78.125 (80.850)
Epoch: [44][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8079 (0.8542) ([0.509]+[0.299])	Prec@1 84.375 (81.066)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.4677 (1.4677) ([1.169]+[0.299])	Prec@1 63.281 (63.281)
 * Prec@1 68.210
current lr 1.00000e-01
Grad=  tensor(2.1713, device='cuda:0')
Epoch: [45][0/391]	Time 0.299 (0.299)	Data 0.179 (0.179)	Loss 0.8021 (0.8021) ([0.504]+[0.299])	Prec@1 83.594 (83.594)
Epoch: [45][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.6924 (0.8524) ([0.393]+[0.300])	Prec@1 89.062 (81.033)
Epoch: [45][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7466 (0.8516) ([0.448]+[0.299])	Prec@1 85.156 (80.943)
Epoch: [45][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9064 (0.8531) ([0.607]+[0.300])	Prec@1 78.906 (81.102)
Test: [0/79]	Time 0.213 (0.213)	Loss 1.0028 (1.0028) ([0.703]+[0.300])	Prec@1 81.250 (81.250)
 * Prec@1 76.070
current lr 1.00000e-01
Grad=  tensor(1.8848, device='cuda:0')
Epoch: [46][0/391]	Time 0.298 (0.298)	Data 0.179 (0.179)	Loss 0.7951 (0.7951) ([0.495]+[0.300])	Prec@1 82.812 (82.812)
Epoch: [46][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8999 (0.8545) ([0.600]+[0.299])	Prec@1 81.250 (80.894)
Epoch: [46][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7450 (0.8429) ([0.447]+[0.298])	Prec@1 85.156 (81.324)
Epoch: [46][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0295 (0.8505) ([0.732]+[0.298])	Prec@1 77.344 (81.110)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.0163 (1.0163) ([0.719]+[0.298])	Prec@1 77.344 (77.344)
 * Prec@1 73.550
current lr 1.00000e-01
Grad=  tensor(2.2931, device='cuda:0')
Epoch: [47][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.9472 (0.9472) ([0.650]+[0.298])	Prec@1 78.125 (78.125)
Epoch: [47][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8139 (0.8525) ([0.516]+[0.298])	Prec@1 79.688 (81.041)
Epoch: [47][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8078 (0.8486) ([0.509]+[0.298])	Prec@1 78.906 (81.351)
Epoch: [47][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7141 (0.8533) ([0.416]+[0.298])	Prec@1 89.844 (81.040)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.9383 (0.9383) ([0.642]+[0.296])	Prec@1 76.562 (76.562)
 * Prec@1 76.490
current lr 1.00000e-01
Grad=  tensor(2.6660, device='cuda:0')
Epoch: [48][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.8440 (0.8440) ([0.548]+[0.296])	Prec@1 80.469 (80.469)
Epoch: [48][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7044 (0.8286) ([0.408]+[0.296])	Prec@1 87.500 (81.853)
Epoch: [48][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8180 (0.8418) ([0.521]+[0.297])	Prec@1 82.031 (81.425)
Epoch: [48][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6234 (0.8493) ([0.327]+[0.297])	Prec@1 92.969 (81.074)
Test: [0/79]	Time 0.219 (0.219)	Loss 2.0287 (2.0287) ([1.732]+[0.297])	Prec@1 55.469 (55.469)
 * Prec@1 60.690
current lr 1.00000e-01
Grad=  tensor(2.0777, device='cuda:0')
Epoch: [49][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.9105 (0.9105) ([0.614]+[0.297])	Prec@1 78.125 (78.125)
Epoch: [49][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9183 (0.8597) ([0.623]+[0.296])	Prec@1 78.125 (80.832)
Epoch: [49][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9498 (0.8484) ([0.654]+[0.295])	Prec@1 78.125 (81.091)
Epoch: [49][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8256 (0.8494) ([0.531]+[0.294])	Prec@1 77.344 (81.053)
Test: [0/79]	Time 0.217 (0.217)	Loss 1.1701 (1.1701) ([0.875]+[0.295])	Prec@1 70.312 (70.312)
 * Prec@1 70.050
current lr 1.00000e-01
Grad=  tensor(2.2829, device='cuda:0')
Epoch: [50][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.8949 (0.8949) ([0.600]+[0.295])	Prec@1 77.344 (77.344)
Epoch: [50][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9898 (0.8350) ([0.694]+[0.295])	Prec@1 71.875 (81.528)
Epoch: [50][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8329 (0.8455) ([0.537]+[0.296])	Prec@1 78.125 (81.211)
Epoch: [50][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.9837 (0.8488) ([0.690]+[0.294])	Prec@1 78.906 (81.162)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.0345 (1.0345) ([0.740]+[0.295])	Prec@1 78.906 (78.906)
 * Prec@1 74.160
current lr 1.00000e-01
Grad=  tensor(1.9092, device='cuda:0')
Epoch: [51][0/391]	Time 0.296 (0.296)	Data 0.177 (0.177)	Loss 0.8729 (0.8729) ([0.578]+[0.295])	Prec@1 78.125 (78.125)
Epoch: [51][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9846 (0.8303) ([0.690]+[0.294])	Prec@1 74.219 (81.815)
Epoch: [51][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8727 (0.8432) ([0.579]+[0.293])	Prec@1 82.031 (81.207)
Epoch: [51][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7750 (0.8470) ([0.482]+[0.294])	Prec@1 82.031 (81.092)
Test: [0/79]	Time 0.213 (0.213)	Loss 1.2436 (1.2436) ([0.951]+[0.292])	Prec@1 71.875 (71.875)
 * Prec@1 71.730
current lr 1.00000e-01
Grad=  tensor(1.5623, device='cuda:0')
Epoch: [52][0/391]	Time 0.298 (0.298)	Data 0.178 (0.178)	Loss 0.7256 (0.7256) ([0.433]+[0.292])	Prec@1 86.719 (86.719)
Epoch: [52][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.0510 (0.8242) ([0.760]+[0.291])	Prec@1 75.781 (82.054)
Epoch: [52][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7723 (0.8304) ([0.481]+[0.292])	Prec@1 82.031 (81.580)
Epoch: [52][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8590 (0.8334) ([0.568]+[0.291])	Prec@1 81.250 (81.416)
Test: [0/79]	Time 0.211 (0.211)	Loss 1.3356 (1.3356) ([1.044]+[0.292])	Prec@1 67.188 (67.188)
 * Prec@1 67.670
current lr 1.00000e-01
Grad=  tensor(2.4647, device='cuda:0')
Epoch: [53][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.9034 (0.9034) ([0.612]+[0.292])	Prec@1 78.125 (78.125)
Epoch: [53][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8660 (0.8388) ([0.573]+[0.293])	Prec@1 81.250 (81.033)
Epoch: [53][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8976 (0.8498) ([0.604]+[0.293])	Prec@1 78.906 (80.702)
Epoch: [53][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7508 (0.8453) ([0.459]+[0.292])	Prec@1 84.375 (80.892)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.9901 (0.9901) ([0.698]+[0.293])	Prec@1 76.562 (76.562)
 * Prec@1 77.850
current lr 1.00000e-01
Grad=  tensor(1.3317, device='cuda:0')
Epoch: [54][0/391]	Time 0.311 (0.311)	Data 0.187 (0.187)	Loss 0.7389 (0.7389) ([0.446]+[0.293])	Prec@1 85.156 (85.156)
Epoch: [54][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.9071 (0.8510) ([0.615]+[0.292])	Prec@1 75.000 (80.562)
Epoch: [54][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 1.0215 (0.8404) ([0.729]+[0.293])	Prec@1 75.781 (81.013)
Epoch: [54][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.8619 (0.8449) ([0.569]+[0.293])	Prec@1 79.688 (81.042)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.1927 (1.1927) ([0.903]+[0.290])	Prec@1 70.312 (70.312)
 * Prec@1 70.680
current lr 1.00000e-01
Grad=  tensor(2.0853, device='cuda:0')
Epoch: [55][0/391]	Time 0.305 (0.305)	Data 0.187 (0.187)	Loss 0.8901 (0.8901) ([0.600]+[0.290])	Prec@1 80.469 (80.469)
Epoch: [55][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.7976 (0.8232) ([0.508]+[0.290])	Prec@1 83.594 (81.559)
Epoch: [55][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.8259 (0.8417) ([0.535]+[0.291])	Prec@1 80.469 (81.079)
Epoch: [55][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.7409 (0.8432) ([0.450]+[0.291])	Prec@1 85.156 (81.022)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.4496 (1.4496) ([1.159]+[0.290])	Prec@1 61.719 (61.719)
 * Prec@1 71.010
current lr 1.00000e-01
Grad=  tensor(2.5693, device='cuda:0')
Epoch: [56][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.9315 (0.9315) ([0.641]+[0.290])	Prec@1 78.906 (78.906)
Epoch: [56][100/391]	Time 0.109 (0.110)	Data 0.000 (0.002)	Loss 0.7888 (0.8568) ([0.498]+[0.291])	Prec@1 82.031 (80.546)
Epoch: [56][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8551 (0.8371) ([0.566]+[0.290])	Prec@1 81.250 (81.219)
Epoch: [56][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.9866 (0.8428) ([0.698]+[0.289])	Prec@1 77.344 (81.092)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.8830 (0.8830) ([0.594]+[0.289])	Prec@1 81.250 (81.250)
 * Prec@1 79.270
current lr 1.00000e-01
Grad=  tensor(1.5310, device='cuda:0')
Epoch: [57][0/391]	Time 0.307 (0.307)	Data 0.189 (0.189)	Loss 0.7806 (0.7806) ([0.492]+[0.289])	Prec@1 80.469 (80.469)
Epoch: [57][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.7563 (0.8401) ([0.467]+[0.289])	Prec@1 84.375 (80.894)
Epoch: [57][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.8044 (0.8330) ([0.516]+[0.289])	Prec@1 83.594 (81.266)
Epoch: [57][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.9310 (0.8369) ([0.643]+[0.288])	Prec@1 78.125 (81.170)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.3005 (1.3005) ([1.012]+[0.288])	Prec@1 73.438 (73.438)
 * Prec@1 69.210
current lr 1.00000e-01
Grad=  tensor(1.9400, device='cuda:0')
Epoch: [58][0/391]	Time 0.300 (0.300)	Data 0.183 (0.183)	Loss 0.7704 (0.7704) ([0.482]+[0.288])	Prec@1 84.375 (84.375)
Epoch: [58][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.9352 (0.8144) ([0.646]+[0.289])	Prec@1 77.344 (81.900)
Epoch: [58][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7967 (0.8244) ([0.509]+[0.288])	Prec@1 82.812 (81.507)
Epoch: [58][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.7518 (0.8260) ([0.464]+[0.287])	Prec@1 80.469 (81.541)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.9956 (0.9956) ([0.709]+[0.287])	Prec@1 76.562 (76.562)
 * Prec@1 76.580
current lr 1.00000e-01
Grad=  tensor(2.3035, device='cuda:0')
Epoch: [59][0/391]	Time 0.310 (0.310)	Data 0.190 (0.190)	Loss 0.9051 (0.9051) ([0.619]+[0.287])	Prec@1 77.344 (77.344)
Epoch: [59][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8819 (0.8220) ([0.595]+[0.287])	Prec@1 78.906 (81.691)
Epoch: [59][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6918 (0.8317) ([0.404]+[0.288])	Prec@1 87.500 (81.269)
Epoch: [59][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8364 (0.8299) ([0.550]+[0.287])	Prec@1 82.812 (81.424)
Test: [0/79]	Time 0.223 (0.223)	Loss 1.1993 (1.1993) ([0.914]+[0.285])	Prec@1 65.625 (65.625)
 * Prec@1 70.790
current lr 1.00000e-01
Grad=  tensor(2.1750, device='cuda:0')
Epoch: [60][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.8093 (0.8093) ([0.524]+[0.285])	Prec@1 80.469 (80.469)
Epoch: [60][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.8787 (0.8083) ([0.593]+[0.286])	Prec@1 81.250 (82.132)
Epoch: [60][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7036 (0.8230) ([0.417]+[0.286])	Prec@1 86.719 (81.576)
Epoch: [60][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.6969 (0.8284) ([0.411]+[0.286])	Prec@1 86.719 (81.426)
Test: [0/79]	Time 0.223 (0.223)	Loss 1.1155 (1.1155) ([0.830]+[0.285])	Prec@1 73.438 (73.438)
 * Prec@1 75.380
current lr 1.00000e-01
Grad=  tensor(2.8148, device='cuda:0')
Epoch: [61][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.8735 (0.8735) ([0.588]+[0.285])	Prec@1 78.125 (78.125)
Epoch: [61][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.0200 (0.8092) ([0.734]+[0.286])	Prec@1 76.562 (81.853)
Epoch: [61][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8809 (0.8246) ([0.595]+[0.286])	Prec@1 80.469 (81.542)
Epoch: [61][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7139 (0.8284) ([0.428]+[0.286])	Prec@1 85.156 (81.328)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.8645 (0.8645) ([0.579]+[0.285])	Prec@1 80.469 (80.469)
 * Prec@1 79.360
current lr 1.00000e-01
Grad=  tensor(1.3494, device='cuda:0')
Epoch: [62][0/391]	Time 0.312 (0.312)	Data 0.193 (0.193)	Loss 0.6536 (0.6536) ([0.368]+[0.285])	Prec@1 89.062 (89.062)
Epoch: [62][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.7336 (0.8256) ([0.448]+[0.286])	Prec@1 82.031 (81.482)
Epoch: [62][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.7714 (0.8236) ([0.486]+[0.285])	Prec@1 85.156 (81.650)
Epoch: [62][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7169 (0.8249) ([0.433]+[0.284])	Prec@1 85.938 (81.541)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.0182 (1.0182) ([0.735]+[0.284])	Prec@1 73.438 (73.438)
 * Prec@1 74.980
current lr 1.00000e-01
Grad=  tensor(1.8776, device='cuda:0')
Epoch: [63][0/391]	Time 0.310 (0.310)	Data 0.190 (0.190)	Loss 0.8044 (0.8044) ([0.521]+[0.284])	Prec@1 82.812 (82.812)
Epoch: [63][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.7557 (0.8182) ([0.471]+[0.285])	Prec@1 82.812 (81.853)
Epoch: [63][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8393 (0.8223) ([0.555]+[0.284])	Prec@1 82.031 (81.728)
Epoch: [63][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7933 (0.8261) ([0.509]+[0.284])	Prec@1 81.250 (81.463)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.2378 (1.2378) ([0.955]+[0.283])	Prec@1 68.750 (68.750)
 * Prec@1 66.480
current lr 1.00000e-01
Grad=  tensor(2.0919, device='cuda:0')
Epoch: [64][0/391]	Time 0.310 (0.310)	Data 0.192 (0.192)	Loss 0.8992 (0.8992) ([0.616]+[0.283])	Prec@1 80.469 (80.469)
Epoch: [64][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7427 (0.8259) ([0.459]+[0.283])	Prec@1 82.812 (81.111)
Epoch: [64][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7922 (0.8309) ([0.509]+[0.283])	Prec@1 85.156 (81.258)
Epoch: [64][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7579 (0.8274) ([0.475]+[0.283])	Prec@1 83.594 (81.338)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.9473 (0.9473) ([0.665]+[0.282])	Prec@1 79.688 (79.688)
 * Prec@1 75.210
current lr 1.00000e-01
Grad=  tensor(2.2883, device='cuda:0')
Epoch: [65][0/391]	Time 0.299 (0.299)	Data 0.180 (0.180)	Loss 0.8156 (0.8156) ([0.533]+[0.282])	Prec@1 78.906 (78.906)
Epoch: [65][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8220 (0.8290) ([0.538]+[0.284])	Prec@1 82.812 (81.668)
Epoch: [65][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8355 (0.8279) ([0.552]+[0.283])	Prec@1 81.250 (81.689)
Epoch: [65][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8813 (0.8246) ([0.598]+[0.283])	Prec@1 79.688 (81.676)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.0674 (1.0674) ([0.784]+[0.283])	Prec@1 74.219 (74.219)
 * Prec@1 74.610
current lr 1.00000e-01
Grad=  tensor(1.9376, device='cuda:0')
Epoch: [66][0/391]	Time 0.296 (0.296)	Data 0.177 (0.177)	Loss 0.8240 (0.8240) ([0.541]+[0.283])	Prec@1 79.688 (79.688)
Epoch: [66][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7415 (0.8314) ([0.458]+[0.284])	Prec@1 80.469 (81.296)
Epoch: [66][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.6772 (0.8304) ([0.394]+[0.283])	Prec@1 85.938 (81.483)
Epoch: [66][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7797 (0.8265) ([0.498]+[0.282])	Prec@1 80.469 (81.595)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.3717 (1.3717) ([1.091]+[0.281])	Prec@1 65.625 (65.625)
 * Prec@1 66.730
current lr 1.00000e-01
Grad=  tensor(2.4292, device='cuda:0')
Epoch: [67][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.7838 (0.7838) ([0.503]+[0.281])	Prec@1 82.812 (82.812)
Epoch: [67][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8072 (0.8166) ([0.526]+[0.281])	Prec@1 82.812 (81.683)
Epoch: [67][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9353 (0.8158) ([0.654]+[0.281])	Prec@1 73.438 (81.580)
Epoch: [67][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9104 (0.8140) ([0.629]+[0.281])	Prec@1 78.906 (81.665)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.9676 (0.9676) ([0.686]+[0.281])	Prec@1 71.875 (71.875)
 * Prec@1 75.940
current lr 1.00000e-01
Grad=  tensor(1.6676, device='cuda:0')
Epoch: [68][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.7219 (0.7219) ([0.441]+[0.281])	Prec@1 85.938 (85.938)
Epoch: [68][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.9111 (0.8173) ([0.629]+[0.282])	Prec@1 78.906 (82.008)
Epoch: [68][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7914 (0.8204) ([0.509]+[0.282])	Prec@1 83.594 (81.650)
Epoch: [68][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.9231 (0.8158) ([0.642]+[0.281])	Prec@1 76.562 (81.808)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.8849 (0.8849) ([0.603]+[0.282])	Prec@1 79.688 (79.688)
 * Prec@1 76.830
current lr 1.00000e-01
Grad=  tensor(2.2634, device='cuda:0')
Epoch: [69][0/391]	Time 0.302 (0.302)	Data 0.182 (0.182)	Loss 0.8416 (0.8416) ([0.560]+[0.282])	Prec@1 73.438 (73.438)
Epoch: [69][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.7990 (0.8196) ([0.517]+[0.282])	Prec@1 82.031 (81.822)
Epoch: [69][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.7453 (0.8181) ([0.464]+[0.281])	Prec@1 87.500 (81.627)
Epoch: [69][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8553 (0.8278) ([0.574]+[0.281])	Prec@1 81.250 (81.234)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.2618 (1.2618) ([0.982]+[0.280])	Prec@1 66.406 (66.406)
 * Prec@1 70.440
current lr 1.00000e-01
Grad=  tensor(1.8899, device='cuda:0')
Epoch: [70][0/391]	Time 0.309 (0.309)	Data 0.189 (0.189)	Loss 0.8144 (0.8144) ([0.534]+[0.280])	Prec@1 82.812 (82.812)
Epoch: [70][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8225 (0.8186) ([0.542]+[0.280])	Prec@1 79.688 (81.907)
Epoch: [70][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7854 (0.8191) ([0.506]+[0.279])	Prec@1 82.812 (81.790)
Epoch: [70][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7587 (0.8197) ([0.479]+[0.279])	Prec@1 82.031 (81.717)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.3381 (1.3381) ([1.058]+[0.280])	Prec@1 61.719 (61.719)
 * Prec@1 68.720
current lr 1.00000e-01
Grad=  tensor(2.2290, device='cuda:0')
Epoch: [71][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.7858 (0.7858) ([0.505]+[0.280])	Prec@1 85.938 (85.938)
Epoch: [71][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8934 (0.8168) ([0.613]+[0.281])	Prec@1 78.906 (81.776)
Epoch: [71][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8118 (0.8202) ([0.531]+[0.280])	Prec@1 82.812 (81.464)
Epoch: [71][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8153 (0.8216) ([0.536]+[0.280])	Prec@1 79.688 (81.421)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.4216 (1.4216) ([1.142]+[0.279])	Prec@1 66.406 (66.406)
 * Prec@1 69.060
current lr 1.00000e-01
Grad=  tensor(1.6958, device='cuda:0')
Epoch: [72][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.7128 (0.7128) ([0.434]+[0.279])	Prec@1 82.812 (82.812)
Epoch: [72][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7488 (0.8000) ([0.469]+[0.280])	Prec@1 82.812 (81.815)
Epoch: [72][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0042 (0.8117) ([0.725]+[0.279])	Prec@1 75.000 (81.573)
Epoch: [72][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6906 (0.8157) ([0.411]+[0.279])	Prec@1 88.281 (81.525)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.1293 (1.1293) ([0.851]+[0.279])	Prec@1 73.438 (73.438)
 * Prec@1 74.570
current lr 1.00000e-01
Grad=  tensor(1.9997, device='cuda:0')
Epoch: [73][0/391]	Time 0.300 (0.300)	Data 0.180 (0.180)	Loss 0.7530 (0.7530) ([0.474]+[0.279])	Prec@1 83.594 (83.594)
Epoch: [73][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8694 (0.8362) ([0.590]+[0.279])	Prec@1 81.250 (81.188)
Epoch: [73][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8175 (0.8285) ([0.539]+[0.279])	Prec@1 83.594 (81.409)
Epoch: [73][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8286 (0.8222) ([0.550]+[0.279])	Prec@1 82.812 (81.556)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.9737 (0.9737) ([0.695]+[0.279])	Prec@1 75.000 (75.000)
 * Prec@1 74.190
current lr 1.00000e-01
Grad=  tensor(2.0815, device='cuda:0')
Epoch: [74][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.7406 (0.7406) ([0.462]+[0.279])	Prec@1 84.375 (84.375)
Epoch: [74][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7633 (0.8106) ([0.485]+[0.278])	Prec@1 82.812 (81.722)
Epoch: [74][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9679 (0.8150) ([0.690]+[0.278])	Prec@1 77.344 (81.763)
Epoch: [74][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7727 (0.8205) ([0.494]+[0.278])	Prec@1 82.031 (81.510)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.0582 (1.0582) ([0.780]+[0.278])	Prec@1 76.562 (76.562)
 * Prec@1 75.880
current lr 1.00000e-01
Grad=  tensor(2.4996, device='cuda:0')
Epoch: [75][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.9372 (0.9372) ([0.659]+[0.278])	Prec@1 74.219 (74.219)
Epoch: [75][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.9986 (0.8120) ([0.721]+[0.278])	Prec@1 77.344 (82.078)
Epoch: [75][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.7570 (0.8111) ([0.479]+[0.278])	Prec@1 83.594 (82.035)
Epoch: [75][300/391]	Time 0.109 (0.108)	Data 0.000 (0.001)	Loss 0.8965 (0.8117) ([0.619]+[0.277])	Prec@1 80.469 (81.943)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.9799 (0.9799) ([0.702]+[0.277])	Prec@1 76.562 (76.562)
 * Prec@1 71.920
current lr 1.00000e-01
Grad=  tensor(1.4874, device='cuda:0')
Epoch: [76][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.7278 (0.7278) ([0.450]+[0.277])	Prec@1 88.281 (88.281)
Epoch: [76][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.6813 (0.8012) ([0.404]+[0.277])	Prec@1 84.375 (82.333)
Epoch: [76][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8732 (0.8151) ([0.597]+[0.276])	Prec@1 78.125 (81.705)
Epoch: [76][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8377 (0.8157) ([0.562]+[0.276])	Prec@1 78.125 (81.715)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.2046 (1.2046) ([0.928]+[0.277])	Prec@1 73.438 (73.438)
 * Prec@1 72.450
current lr 1.00000e-01
Grad=  tensor(1.4060, device='cuda:0')
Epoch: [77][0/391]	Time 0.304 (0.304)	Data 0.186 (0.186)	Loss 0.7481 (0.7481) ([0.471]+[0.277])	Prec@1 83.594 (83.594)
Epoch: [77][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.7923 (0.8099) ([0.516]+[0.276])	Prec@1 85.156 (81.699)
Epoch: [77][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.8002 (0.8141) ([0.524]+[0.276])	Prec@1 81.250 (81.650)
Epoch: [77][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.7601 (0.8138) ([0.483]+[0.277])	Prec@1 81.250 (81.712)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.8377 (0.8377) ([0.562]+[0.276])	Prec@1 80.469 (80.469)
 * Prec@1 78.280
current lr 1.00000e-01
Grad=  tensor(2.3993, device='cuda:0')
Epoch: [78][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.8841 (0.8841) ([0.608]+[0.276])	Prec@1 75.000 (75.000)
Epoch: [78][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8940 (0.8045) ([0.618]+[0.276])	Prec@1 79.688 (81.838)
Epoch: [78][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6997 (0.8082) ([0.424]+[0.276])	Prec@1 81.250 (81.771)
Epoch: [78][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7296 (0.8088) ([0.454]+[0.275])	Prec@1 84.375 (81.920)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.2417 (1.2417) ([0.965]+[0.277])	Prec@1 68.750 (68.750)
 * Prec@1 68.950
current lr 1.00000e-01
Grad=  tensor(1.9212, device='cuda:0')
Epoch: [79][0/391]	Time 0.300 (0.300)	Data 0.180 (0.180)	Loss 0.7158 (0.7158) ([0.439]+[0.277])	Prec@1 87.500 (87.500)
Epoch: [79][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.9060 (0.8123) ([0.631]+[0.276])	Prec@1 74.219 (81.660)
Epoch: [79][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7751 (0.8057) ([0.501]+[0.275])	Prec@1 79.688 (81.934)
Epoch: [79][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8766 (0.8083) ([0.602]+[0.275])	Prec@1 81.250 (81.899)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.3566 (1.3566) ([1.082]+[0.274])	Prec@1 65.625 (65.625)
 * Prec@1 69.090
current lr 1.00000e-01
Grad=  tensor(2.0484, device='cuda:0')
Epoch: [80][0/391]	Time 0.298 (0.298)	Data 0.178 (0.178)	Loss 0.7797 (0.7797) ([0.505]+[0.274])	Prec@1 82.812 (82.812)
Epoch: [80][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7144 (0.8046) ([0.440]+[0.274])	Prec@1 85.156 (81.730)
Epoch: [80][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7827 (0.8145) ([0.508]+[0.274])	Prec@1 82.031 (81.549)
Epoch: [80][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.6385 (0.8151) ([0.364]+[0.274])	Prec@1 89.844 (81.515)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.7513 (0.7513) ([0.477]+[0.274])	Prec@1 82.031 (82.031)
 * Prec@1 77.160
current lr 1.00000e-01
Grad=  tensor(1.5840, device='cuda:0')
Epoch: [81][0/391]	Time 0.297 (0.297)	Data 0.178 (0.178)	Loss 0.7530 (0.7530) ([0.479]+[0.274])	Prec@1 83.594 (83.594)
Epoch: [81][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7047 (0.7832) ([0.432]+[0.273])	Prec@1 85.156 (82.666)
Epoch: [81][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8922 (0.7908) ([0.619]+[0.273])	Prec@1 81.250 (82.575)
Epoch: [81][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7868 (0.8049) ([0.513]+[0.274])	Prec@1 82.031 (82.052)
Test: [0/79]	Time 0.217 (0.217)	Loss 1.0944 (1.0944) ([0.821]+[0.274])	Prec@1 68.750 (68.750)
 * Prec@1 70.750
current lr 1.00000e-01
Grad=  tensor(2.0065, device='cuda:0')
Epoch: [82][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.7290 (0.7290) ([0.455]+[0.274])	Prec@1 88.281 (88.281)
Epoch: [82][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.8173 (0.8040) ([0.543]+[0.275])	Prec@1 78.906 (81.668)
Epoch: [82][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6403 (0.8081) ([0.366]+[0.274])	Prec@1 88.281 (81.697)
Epoch: [82][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8248 (0.8067) ([0.551]+[0.274])	Prec@1 82.031 (81.824)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.3220 (1.3220) ([1.049]+[0.273])	Prec@1 63.281 (63.281)
 * Prec@1 68.230
current lr 1.00000e-01
Grad=  tensor(1.8435, device='cuda:0')
Epoch: [83][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.7344 (0.7344) ([0.461]+[0.273])	Prec@1 81.250 (81.250)
Epoch: [83][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7986 (0.8064) ([0.525]+[0.273])	Prec@1 78.125 (81.838)
Epoch: [83][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7316 (0.8035) ([0.459]+[0.273])	Prec@1 85.156 (81.852)
Epoch: [83][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7449 (0.8079) ([0.473]+[0.272])	Prec@1 83.594 (81.754)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.7617 (0.7617) ([0.490]+[0.272])	Prec@1 80.469 (80.469)
 * Prec@1 80.110
current lr 1.00000e-01
Grad=  tensor(1.5073, device='cuda:0')
Epoch: [84][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.6787 (0.6787) ([0.407]+[0.272])	Prec@1 85.938 (85.938)
Epoch: [84][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7381 (0.7957) ([0.466]+[0.272])	Prec@1 82.031 (82.287)
Epoch: [84][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8327 (0.8008) ([0.561]+[0.272])	Prec@1 81.250 (82.136)
Epoch: [84][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7695 (0.8063) ([0.498]+[0.272])	Prec@1 84.375 (81.878)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.9225 (0.9225) ([0.651]+[0.271])	Prec@1 79.688 (79.688)
 * Prec@1 76.070
current lr 1.00000e-01
Grad=  tensor(1.4100, device='cuda:0')
Epoch: [85][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.6810 (0.6810) ([0.410]+[0.271])	Prec@1 85.156 (85.156)
Epoch: [85][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7742 (0.8083) ([0.502]+[0.272])	Prec@1 83.594 (81.846)
Epoch: [85][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8656 (0.8000) ([0.594]+[0.271])	Prec@1 79.688 (81.973)
Epoch: [85][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9312 (0.8011) ([0.660]+[0.271])	Prec@1 75.000 (81.990)
Test: [0/79]	Time 0.219 (0.219)	Loss 1.0457 (1.0457) ([0.774]+[0.272])	Prec@1 78.125 (78.125)
 * Prec@1 75.050
current lr 1.00000e-01
Grad=  tensor(1.6836, device='cuda:0')
Epoch: [86][0/391]	Time 0.300 (0.300)	Data 0.181 (0.181)	Loss 0.7116 (0.7116) ([0.440]+[0.272])	Prec@1 79.688 (79.688)
Epoch: [86][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7097 (0.7923) ([0.437]+[0.272])	Prec@1 85.938 (82.186)
Epoch: [86][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9082 (0.8027) ([0.635]+[0.273])	Prec@1 81.250 (81.763)
Epoch: [86][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7722 (0.8102) ([0.498]+[0.274])	Prec@1 81.250 (81.567)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.7768 (0.7768) ([0.504]+[0.273])	Prec@1 80.469 (80.469)
 * Prec@1 78.500
current lr 1.00000e-01
Grad=  tensor(2.1384, device='cuda:0')
Epoch: [87][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.7862 (0.7862) ([0.514]+[0.273])	Prec@1 83.594 (83.594)
Epoch: [87][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7833 (0.8131) ([0.510]+[0.273])	Prec@1 76.562 (81.343)
Epoch: [87][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7450 (0.8110) ([0.473]+[0.272])	Prec@1 83.594 (81.530)
Epoch: [87][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7037 (0.8159) ([0.431]+[0.273])	Prec@1 87.500 (81.442)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.9954 (0.9954) ([0.723]+[0.272])	Prec@1 75.781 (75.781)
 * Prec@1 73.300
current lr 1.00000e-01
Grad=  tensor(2.6383, device='cuda:0')
Epoch: [88][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.9023 (0.9023) ([0.630]+[0.272])	Prec@1 76.562 (76.562)
Epoch: [88][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.6815 (0.8165) ([0.409]+[0.273])	Prec@1 85.938 (81.675)
Epoch: [88][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6662 (0.8114) ([0.394]+[0.273])	Prec@1 84.375 (81.596)
Epoch: [88][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7775 (0.8077) ([0.506]+[0.272])	Prec@1 81.250 (81.831)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.7486 (0.7486) ([0.477]+[0.272])	Prec@1 86.719 (86.719)
 * Prec@1 79.950
current lr 1.00000e-01
Grad=  tensor(1.5317, device='cuda:0')
Epoch: [89][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.6461 (0.6461) ([0.374]+[0.272])	Prec@1 87.500 (87.500)
Epoch: [89][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.8457 (0.7954) ([0.573]+[0.272])	Prec@1 81.250 (81.923)
Epoch: [89][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7382 (0.8004) ([0.466]+[0.272])	Prec@1 85.156 (81.817)
Epoch: [89][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.7971 (0.8099) ([0.525]+[0.272])	Prec@1 85.156 (81.595)
Test: [0/79]	Time 0.221 (0.221)	Loss 1.0254 (1.0254) ([0.753]+[0.272])	Prec@1 73.438 (73.438)
 * Prec@1 75.380
current lr 1.00000e-01
Grad=  tensor(2.4107, device='cuda:0')
Epoch: [90][0/391]	Time 0.309 (0.309)	Data 0.191 (0.191)	Loss 0.9117 (0.9117) ([0.640]+[0.272])	Prec@1 78.906 (78.906)
Epoch: [90][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.7775 (0.8034) ([0.506]+[0.272])	Prec@1 80.469 (81.776)
Epoch: [90][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.8022 (0.7965) ([0.531]+[0.271])	Prec@1 79.688 (82.020)
Epoch: [90][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.7904 (0.7966) ([0.520]+[0.271])	Prec@1 82.812 (82.081)
Test: [0/79]	Time 0.223 (0.223)	Loss 1.1347 (1.1347) ([0.864]+[0.271])	Prec@1 74.219 (74.219)
 * Prec@1 73.600
current lr 1.00000e-01
Grad=  tensor(1.6732, device='cuda:0')
Epoch: [91][0/391]	Time 0.308 (0.308)	Data 0.190 (0.190)	Loss 0.7475 (0.7475) ([0.477]+[0.271])	Prec@1 83.594 (83.594)
Epoch: [91][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 1.0079 (0.8060) ([0.738]+[0.270])	Prec@1 73.438 (81.552)
Epoch: [91][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7112 (0.8045) ([0.441]+[0.270])	Prec@1 85.938 (81.887)
Epoch: [91][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.6874 (0.8050) ([0.417]+[0.271])	Prec@1 85.156 (81.808)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.0689 (1.0689) ([0.799]+[0.270])	Prec@1 72.656 (72.656)
 * Prec@1 72.240
current lr 1.00000e-01
Grad=  tensor(1.8009, device='cuda:0')
Epoch: [92][0/391]	Time 0.305 (0.305)	Data 0.188 (0.188)	Loss 0.8085 (0.8085) ([0.538]+[0.270])	Prec@1 82.812 (82.812)
Epoch: [92][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.7144 (0.7948) ([0.444]+[0.271])	Prec@1 86.719 (81.892)
Epoch: [92][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.6539 (0.8028) ([0.383]+[0.271])	Prec@1 89.844 (81.670)
Epoch: [92][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.6868 (0.8013) ([0.417]+[0.269])	Prec@1 85.156 (81.824)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.0446 (1.0446) ([0.775]+[0.270])	Prec@1 74.219 (74.219)
 * Prec@1 75.460
current lr 1.00000e-01
Grad=  tensor(1.8734, device='cuda:0')
Epoch: [93][0/391]	Time 0.297 (0.297)	Data 0.179 (0.179)	Loss 0.7421 (0.7421) ([0.472]+[0.270])	Prec@1 81.250 (81.250)
Epoch: [93][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.8013 (0.7868) ([0.531]+[0.270])	Prec@1 80.469 (82.403)
Epoch: [93][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.7574 (0.7906) ([0.489]+[0.269])	Prec@1 81.250 (82.226)
Epoch: [93][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.7795 (0.7937) ([0.510]+[0.269])	Prec@1 82.031 (82.203)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.9853 (0.9853) ([0.717]+[0.269])	Prec@1 75.781 (75.781)
 * Prec@1 73.780
current lr 1.00000e-01
Grad=  tensor(2.2419, device='cuda:0')
Epoch: [94][0/391]	Time 0.303 (0.303)	Data 0.183 (0.183)	Loss 0.7739 (0.7739) ([0.505]+[0.269])	Prec@1 81.250 (81.250)
Epoch: [94][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7919 (0.7971) ([0.522]+[0.270])	Prec@1 84.375 (81.892)
Epoch: [94][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.9503 (0.8006) ([0.681]+[0.270])	Prec@1 82.812 (81.919)
Epoch: [94][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.9433 (0.7984) ([0.674]+[0.269])	Prec@1 75.000 (81.907)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.0047 (1.0047) ([0.735]+[0.269])	Prec@1 73.438 (73.438)
 * Prec@1 75.400
current lr 1.00000e-01
Grad=  tensor(1.6751, device='cuda:0')
Epoch: [95][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.6985 (0.6985) ([0.429]+[0.269])	Prec@1 85.156 (85.156)
Epoch: [95][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7094 (0.7744) ([0.440]+[0.269])	Prec@1 84.375 (82.774)
Epoch: [95][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8290 (0.7963) ([0.559]+[0.270])	Prec@1 79.688 (81.806)
Epoch: [95][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7366 (0.8017) ([0.467]+[0.270])	Prec@1 84.375 (81.707)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.0187 (1.0187) ([0.750]+[0.269])	Prec@1 77.344 (77.344)
 * Prec@1 74.100
current lr 1.00000e-01
Grad=  tensor(1.9475, device='cuda:0')
Epoch: [96][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.7652 (0.7652) ([0.496]+[0.269])	Prec@1 82.031 (82.031)
Epoch: [96][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8519 (0.7853) ([0.583]+[0.269])	Prec@1 82.031 (82.364)
Epoch: [96][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.8329 (0.8060) ([0.563]+[0.269])	Prec@1 82.031 (81.782)
Epoch: [96][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7216 (0.8046) ([0.452]+[0.270])	Prec@1 83.594 (81.764)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.8212 (0.8212) ([0.552]+[0.270])	Prec@1 78.125 (78.125)
 * Prec@1 74.450
current lr 1.00000e-01
Grad=  tensor(1.9413, device='cuda:0')
Epoch: [97][0/391]	Time 0.301 (0.301)	Data 0.181 (0.181)	Loss 0.8068 (0.8068) ([0.537]+[0.270])	Prec@1 82.812 (82.812)
Epoch: [97][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.7047 (0.7731) ([0.436]+[0.269])	Prec@1 86.719 (82.758)
Epoch: [97][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8203 (0.8024) ([0.551]+[0.269])	Prec@1 82.812 (81.934)
Epoch: [97][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8944 (0.8096) ([0.626]+[0.269])	Prec@1 80.469 (81.647)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.7737 (1.7737) ([1.506]+[0.268])	Prec@1 63.281 (63.281)
 * Prec@1 66.180
current lr 1.00000e-01
Grad=  tensor(2.9713, device='cuda:0')
Epoch: [98][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.8754 (0.8754) ([0.607]+[0.268])	Prec@1 75.781 (75.781)
Epoch: [98][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8540 (0.8108) ([0.585]+[0.269])	Prec@1 81.250 (81.621)
Epoch: [98][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.6791 (0.8075) ([0.410]+[0.269])	Prec@1 82.812 (81.514)
Epoch: [98][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7570 (0.8051) ([0.488]+[0.269])	Prec@1 83.594 (81.634)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.9656 (0.9656) ([0.697]+[0.269])	Prec@1 78.125 (78.125)
 * Prec@1 76.310
current lr 1.00000e-01
Grad=  tensor(1.8319, device='cuda:0')
Epoch: [99][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.7098 (0.7098) ([0.441]+[0.269])	Prec@1 85.938 (85.938)
Epoch: [99][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.8137 (0.7740) ([0.546]+[0.268])	Prec@1 78.906 (82.689)
Epoch: [99][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8549 (0.7876) ([0.587]+[0.268])	Prec@1 78.125 (82.183)
Epoch: [99][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7479 (0.7879) ([0.480]+[0.267])	Prec@1 82.812 (82.203)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.0735 (1.0735) ([0.806]+[0.267])	Prec@1 71.094 (71.094)
 * Prec@1 71.970
current lr 1.00000e-02
Grad=  tensor(2.0811, device='cuda:0')
Epoch: [100][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.7064 (0.7064) ([0.439]+[0.267])	Prec@1 83.594 (83.594)
Epoch: [100][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.5511 (0.6191) ([0.316]+[0.235])	Prec@1 87.500 (86.989)
Epoch: [100][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.5556 (0.5943) ([0.323]+[0.233])	Prec@1 89.062 (87.729)
Epoch: [100][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.5899 (0.5789) ([0.359]+[0.231])	Prec@1 87.500 (88.255)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.5281 (0.5281) ([0.299]+[0.229])	Prec@1 91.406 (91.406)
 * Prec@1 88.880
current lr 1.00000e-02
Grad=  tensor(1.5291, device='cuda:0')
Epoch: [101][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.5132 (0.5132) ([0.284]+[0.229])	Prec@1 89.062 (89.062)
Epoch: [101][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.5690 (0.5105) ([0.342]+[0.227])	Prec@1 90.625 (90.354)
Epoch: [101][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5616 (0.5128) ([0.336]+[0.226])	Prec@1 86.719 (90.162)
Epoch: [101][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4319 (0.5069) ([0.208]+[0.224])	Prec@1 92.969 (90.308)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4812 (0.4812) ([0.259]+[0.223])	Prec@1 90.625 (90.625)
 * Prec@1 90.130
current lr 1.00000e-02
Grad=  tensor(1.5433, device='cuda:0')
Epoch: [102][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.4834 (0.4834) ([0.261]+[0.223])	Prec@1 92.188 (92.188)
Epoch: [102][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.4869 (0.4685) ([0.266]+[0.221])	Prec@1 89.062 (91.484)
Epoch: [102][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4640 (0.4736) ([0.245]+[0.219])	Prec@1 89.844 (91.227)
Epoch: [102][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4028 (0.4730) ([0.185]+[0.218])	Prec@1 93.750 (91.206)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4700 (0.4700) ([0.253]+[0.217])	Prec@1 92.969 (92.969)
 * Prec@1 90.070
current lr 1.00000e-02
Grad=  tensor(1.4300, device='cuda:0')
Epoch: [103][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.4225 (0.4225) ([0.206]+[0.217])	Prec@1 92.969 (92.969)
Epoch: [103][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.4607 (0.4612) ([0.246]+[0.215])	Prec@1 92.188 (91.607)
Epoch: [103][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.5113 (0.4548) ([0.298]+[0.214])	Prec@1 87.500 (91.795)
Epoch: [103][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4098 (0.4566) ([0.197]+[0.212])	Prec@1 91.406 (91.705)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.4959 (0.4959) ([0.285]+[0.211])	Prec@1 92.969 (92.969)
 * Prec@1 90.740
current lr 1.00000e-02
Grad=  tensor(1.9224, device='cuda:0')
Epoch: [104][0/391]	Time 0.313 (0.313)	Data 0.194 (0.194)	Loss 0.4377 (0.4377) ([0.227]+[0.211])	Prec@1 92.188 (92.188)
Epoch: [104][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3650 (0.4415) ([0.155]+[0.210])	Prec@1 96.094 (91.979)
Epoch: [104][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3730 (0.4425) ([0.165]+[0.208])	Prec@1 96.094 (91.900)
Epoch: [104][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.5409 (0.4405) ([0.334]+[0.207])	Prec@1 85.938 (92.016)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4743 (0.4743) ([0.268]+[0.206])	Prec@1 92.969 (92.969)
 * Prec@1 90.490
current lr 1.00000e-02
Grad=  tensor(1.7301, device='cuda:0')
Epoch: [105][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.3723 (0.3723) ([0.166]+[0.206])	Prec@1 94.531 (94.531)
Epoch: [105][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.4028 (0.4199) ([0.198]+[0.205])	Prec@1 92.188 (92.659)
Epoch: [105][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4081 (0.4249) ([0.205]+[0.204])	Prec@1 92.969 (92.452)
Epoch: [105][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.3900 (0.4252) ([0.188]+[0.202])	Prec@1 92.188 (92.416)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4728 (0.4728) ([0.271]+[0.201])	Prec@1 89.844 (89.844)
 * Prec@1 90.060
current lr 1.00000e-02
Grad=  tensor(1.7371, device='cuda:0')
Epoch: [106][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3344 (0.3344) ([0.133]+[0.201])	Prec@1 94.531 (94.531)
Epoch: [106][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3259 (0.4009) ([0.126]+[0.200])	Prec@1 96.094 (93.000)
Epoch: [106][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4600 (0.4061) ([0.261]+[0.199])	Prec@1 93.750 (92.747)
Epoch: [106][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3948 (0.4088) ([0.197]+[0.198])	Prec@1 92.969 (92.735)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4753 (0.4753) ([0.278]+[0.197])	Prec@1 90.625 (90.625)
 * Prec@1 90.370
current lr 1.00000e-02
Grad=  tensor(1.5534, device='cuda:0')
Epoch: [107][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.2861 (0.2861) ([0.089]+[0.197])	Prec@1 97.656 (97.656)
Epoch: [107][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.4150 (0.3971) ([0.219]+[0.196])	Prec@1 91.406 (93.154)
Epoch: [107][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.4098 (0.4012) ([0.215]+[0.195])	Prec@1 90.625 (92.868)
Epoch: [107][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.4587 (0.4013) ([0.265]+[0.194])	Prec@1 89.844 (92.855)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4149 (0.4149) ([0.222]+[0.193])	Prec@1 93.750 (93.750)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(2.8711, device='cuda:0')
Epoch: [108][0/391]	Time 0.297 (0.297)	Data 0.180 (0.180)	Loss 0.3966 (0.3966) ([0.204]+[0.193])	Prec@1 93.750 (93.750)
Epoch: [108][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3689 (0.3797) ([0.177]+[0.192])	Prec@1 93.750 (93.750)
Epoch: [108][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.4237 (0.3859) ([0.233]+[0.191])	Prec@1 95.312 (93.458)
Epoch: [108][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.4100 (0.3898) ([0.220]+[0.190])	Prec@1 94.531 (93.252)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4392 (0.4392) ([0.250]+[0.189])	Prec@1 91.406 (91.406)
 * Prec@1 90.460
current lr 1.00000e-02
Grad=  tensor(2.9297, device='cuda:0')
Epoch: [109][0/391]	Time 0.298 (0.298)	Data 0.181 (0.181)	Loss 0.3665 (0.3665) ([0.177]+[0.189])	Prec@1 93.750 (93.750)
Epoch: [109][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.3502 (0.3719) ([0.162]+[0.188])	Prec@1 94.531 (93.688)
Epoch: [109][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.4136 (0.3796) ([0.226]+[0.188])	Prec@1 90.625 (93.424)
Epoch: [109][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3578 (0.3831) ([0.171]+[0.187])	Prec@1 94.531 (93.288)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.3635 (0.3635) ([0.178]+[0.186])	Prec@1 93.750 (93.750)
 * Prec@1 89.710
current lr 1.00000e-02
Grad=  tensor(4.9656, device='cuda:0')
Epoch: [110][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.4333 (0.4333) ([0.247]+[0.186])	Prec@1 89.844 (89.844)
Epoch: [110][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3469 (0.3750) ([0.162]+[0.185])	Prec@1 94.531 (93.634)
Epoch: [110][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.4760 (0.3771) ([0.292]+[0.184])	Prec@1 86.719 (93.532)
Epoch: [110][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.4079 (0.3774) ([0.224]+[0.184])	Prec@1 91.406 (93.444)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.5413 (0.5413) ([0.358]+[0.183])	Prec@1 91.406 (91.406)
 * Prec@1 90.150
current lr 1.00000e-02
Grad=  tensor(3.0393, device='cuda:0')
Epoch: [111][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.3551 (0.3551) ([0.172]+[0.183])	Prec@1 93.750 (93.750)
Epoch: [111][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3892 (0.3606) ([0.207]+[0.182])	Prec@1 92.969 (93.928)
Epoch: [111][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3267 (0.3647) ([0.145]+[0.181])	Prec@1 93.750 (93.804)
Epoch: [111][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3789 (0.3681) ([0.198]+[0.181])	Prec@1 91.406 (93.698)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4632 (0.4632) ([0.283]+[0.180])	Prec@1 92.969 (92.969)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(5.0773, device='cuda:0')
Epoch: [112][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.3902 (0.3902) ([0.210]+[0.180])	Prec@1 92.969 (92.969)
Epoch: [112][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3144 (0.3634) ([0.135]+[0.179])	Prec@1 96.875 (93.603)
Epoch: [112][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3511 (0.3648) ([0.172]+[0.179])	Prec@1 92.188 (93.738)
Epoch: [112][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3743 (0.3669) ([0.196]+[0.178])	Prec@1 94.531 (93.618)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4975 (0.4975) ([0.320]+[0.178])	Prec@1 91.406 (91.406)
 * Prec@1 90.640
current lr 1.00000e-02
Grad=  tensor(5.3427, device='cuda:0')
Epoch: [113][0/391]	Time 0.292 (0.292)	Data 0.174 (0.174)	Loss 0.4154 (0.4154) ([0.238]+[0.178])	Prec@1 93.750 (93.750)
Epoch: [113][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.4161 (0.3606) ([0.239]+[0.177])	Prec@1 91.406 (93.595)
Epoch: [113][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3844 (0.3626) ([0.208]+[0.176])	Prec@1 89.844 (93.544)
Epoch: [113][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2727 (0.3623) ([0.097]+[0.176])	Prec@1 97.656 (93.542)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4558 (0.4558) ([0.280]+[0.175])	Prec@1 92.188 (92.188)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(2.7258, device='cuda:0')
Epoch: [114][0/391]	Time 0.297 (0.297)	Data 0.179 (0.179)	Loss 0.3396 (0.3396) ([0.164]+[0.175])	Prec@1 96.094 (96.094)
Epoch: [114][100/391]	Time 0.109 (0.110)	Data 0.000 (0.002)	Loss 0.3331 (0.3441) ([0.158]+[0.175])	Prec@1 93.750 (94.276)
Epoch: [114][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.3786 (0.3505) ([0.204]+[0.174])	Prec@1 92.188 (93.991)
Epoch: [114][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.3523 (0.3554) ([0.178]+[0.174])	Prec@1 90.625 (93.823)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5170 (0.5170) ([0.344]+[0.173])	Prec@1 89.062 (89.062)
 * Prec@1 90.290
current lr 1.00000e-02
Grad=  tensor(6.1410, device='cuda:0')
Epoch: [115][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3842 (0.3842) ([0.211]+[0.173])	Prec@1 89.062 (89.062)
Epoch: [115][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2954 (0.3539) ([0.123]+[0.173])	Prec@1 96.094 (93.541)
Epoch: [115][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3240 (0.3543) ([0.152]+[0.172])	Prec@1 96.094 (93.610)
Epoch: [115][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3520 (0.3572) ([0.180]+[0.172])	Prec@1 92.969 (93.550)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4555 (0.4555) ([0.284]+[0.171])	Prec@1 91.406 (91.406)
 * Prec@1 89.540
current lr 1.00000e-02
Grad=  tensor(3.7956, device='cuda:0')
Epoch: [116][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.2903 (0.2903) ([0.119]+[0.171])	Prec@1 96.094 (96.094)
Epoch: [116][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3760 (0.3435) ([0.205]+[0.171])	Prec@1 92.969 (93.998)
Epoch: [116][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2694 (0.3441) ([0.099]+[0.171])	Prec@1 96.875 (94.057)
Epoch: [116][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3454 (0.3478) ([0.175]+[0.170])	Prec@1 92.188 (93.836)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5152 (0.5152) ([0.345]+[0.170])	Prec@1 92.188 (92.188)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(3.7265, device='cuda:0')
Epoch: [117][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3282 (0.3282) ([0.158]+[0.170])	Prec@1 96.875 (96.875)
Epoch: [117][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3310 (0.3479) ([0.162]+[0.169])	Prec@1 95.312 (94.044)
Epoch: [117][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3146 (0.3480) ([0.146]+[0.169])	Prec@1 95.312 (93.882)
Epoch: [117][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3613 (0.3511) ([0.193]+[0.169])	Prec@1 92.969 (93.727)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.3990 (0.3990) ([0.230]+[0.168])	Prec@1 92.969 (92.969)
 * Prec@1 89.080
current lr 1.00000e-02
Grad=  tensor(6.6662, device='cuda:0')
Epoch: [118][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.3844 (0.3844) ([0.216]+[0.168])	Prec@1 92.969 (92.969)
Epoch: [118][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2842 (0.3470) ([0.116]+[0.168])	Prec@1 96.875 (93.603)
Epoch: [118][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3653 (0.3472) ([0.198]+[0.168])	Prec@1 91.406 (93.703)
Epoch: [118][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3279 (0.3492) ([0.160]+[0.168])	Prec@1 92.188 (93.631)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4566 (0.4566) ([0.289]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 89.350
current lr 1.00000e-02
Grad=  tensor(5.3508, device='cuda:0')
Epoch: [119][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.3620 (0.3620) ([0.195]+[0.167])	Prec@1 92.969 (92.969)
Epoch: [119][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3104 (0.3410) ([0.143]+[0.167])	Prec@1 95.312 (94.059)
Epoch: [119][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4350 (0.3422) ([0.268]+[0.167])	Prec@1 91.406 (93.964)
Epoch: [119][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4865 (0.3453) ([0.320]+[0.166])	Prec@1 88.281 (93.841)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4104 (0.4104) ([0.244]+[0.166])	Prec@1 90.625 (90.625)
 * Prec@1 90.150
current lr 1.00000e-02
Grad=  tensor(5.5293, device='cuda:0')
Epoch: [120][0/391]	Time 0.312 (0.312)	Data 0.193 (0.193)	Loss 0.3309 (0.3309) ([0.165]+[0.166])	Prec@1 92.969 (92.969)
Epoch: [120][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2564 (0.3381) ([0.091]+[0.166])	Prec@1 98.438 (93.920)
Epoch: [120][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3686 (0.3430) ([0.203]+[0.166])	Prec@1 92.969 (93.727)
Epoch: [120][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2958 (0.3482) ([0.130]+[0.165])	Prec@1 96.094 (93.537)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.6160 (0.6160) ([0.451]+[0.165])	Prec@1 89.844 (89.844)
 * Prec@1 88.210
current lr 1.00000e-02
Grad=  tensor(5.0846, device='cuda:0')
Epoch: [121][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.3324 (0.3324) ([0.167]+[0.165])	Prec@1 94.531 (94.531)
Epoch: [121][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3708 (0.3391) ([0.206]+[0.165])	Prec@1 95.312 (94.090)
Epoch: [121][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3561 (0.3434) ([0.192]+[0.165])	Prec@1 94.531 (93.898)
Epoch: [121][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4210 (0.3463) ([0.257]+[0.165])	Prec@1 89.062 (93.820)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4600 (0.4600) ([0.296]+[0.164])	Prec@1 93.750 (93.750)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(8.7387, device='cuda:0')
Epoch: [122][0/391]	Time 0.314 (0.314)	Data 0.196 (0.196)	Loss 0.3930 (0.3930) ([0.229]+[0.164])	Prec@1 91.406 (91.406)
Epoch: [122][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2985 (0.3283) ([0.134]+[0.164])	Prec@1 93.750 (94.485)
Epoch: [122][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3923 (0.3316) ([0.228]+[0.164])	Prec@1 93.750 (94.255)
Epoch: [122][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.5219 (0.3381) ([0.358]+[0.164])	Prec@1 85.938 (94.028)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.5573 (0.5573) ([0.394]+[0.164])	Prec@1 89.062 (89.062)
 * Prec@1 89.070
current lr 1.00000e-02
Grad=  tensor(6.8181, device='cuda:0')
Epoch: [123][0/391]	Time 0.311 (0.311)	Data 0.191 (0.191)	Loss 0.3509 (0.3509) ([0.187]+[0.164])	Prec@1 93.750 (93.750)
Epoch: [123][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3315 (0.3330) ([0.168]+[0.163])	Prec@1 94.531 (94.183)
Epoch: [123][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3995 (0.3358) ([0.236]+[0.163])	Prec@1 91.406 (94.057)
Epoch: [123][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3510 (0.3423) ([0.188]+[0.163])	Prec@1 90.625 (93.919)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4940 (0.4940) ([0.331]+[0.163])	Prec@1 85.938 (85.938)
 * Prec@1 87.750
current lr 1.00000e-02
Grad=  tensor(6.5011, device='cuda:0')
Epoch: [124][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3947 (0.3947) ([0.232]+[0.163])	Prec@1 92.969 (92.969)
Epoch: [124][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2835 (0.3398) ([0.121]+[0.163])	Prec@1 96.094 (93.789)
Epoch: [124][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3383 (0.3395) ([0.176]+[0.163])	Prec@1 94.531 (93.913)
Epoch: [124][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3371 (0.3448) ([0.174]+[0.163])	Prec@1 92.188 (93.703)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.5449 (0.5449) ([0.382]+[0.163])	Prec@1 91.406 (91.406)
 * Prec@1 87.900
current lr 1.00000e-02
Grad=  tensor(7.6678, device='cuda:0')
Epoch: [125][0/391]	Time 0.305 (0.305)	Data 0.184 (0.184)	Loss 0.3354 (0.3354) ([0.173]+[0.163])	Prec@1 92.188 (92.188)
Epoch: [125][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3055 (0.3339) ([0.143]+[0.162])	Prec@1 96.875 (94.114)
Epoch: [125][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3269 (0.3448) ([0.165]+[0.162])	Prec@1 95.312 (93.641)
Epoch: [125][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3854 (0.3439) ([0.223]+[0.162])	Prec@1 94.531 (93.688)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4197 (0.4197) ([0.258]+[0.162])	Prec@1 92.188 (92.188)
 * Prec@1 88.810
current lr 1.00000e-02
Grad=  tensor(6.8799, device='cuda:0')
Epoch: [126][0/391]	Time 0.299 (0.299)	Data 0.180 (0.180)	Loss 0.3030 (0.3030) ([0.141]+[0.162])	Prec@1 93.750 (93.750)
Epoch: [126][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3963 (0.3463) ([0.234]+[0.162])	Prec@1 92.969 (93.332)
Epoch: [126][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3206 (0.3453) ([0.159]+[0.162])	Prec@1 92.969 (93.532)
Epoch: [126][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3497 (0.3459) ([0.188]+[0.162])	Prec@1 92.188 (93.506)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5060 (0.5060) ([0.344]+[0.162])	Prec@1 91.406 (91.406)
 * Prec@1 89.300
current lr 1.00000e-02
Grad=  tensor(6.7270, device='cuda:0')
Epoch: [127][0/391]	Time 0.311 (0.311)	Data 0.190 (0.190)	Loss 0.3199 (0.3199) ([0.158]+[0.162])	Prec@1 96.094 (96.094)
Epoch: [127][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2909 (0.3362) ([0.129]+[0.162])	Prec@1 94.531 (93.943)
Epoch: [127][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3349 (0.3415) ([0.173]+[0.162])	Prec@1 93.750 (93.777)
Epoch: [127][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3131 (0.3449) ([0.151]+[0.162])	Prec@1 95.312 (93.612)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.5679 (0.5679) ([0.406]+[0.162])	Prec@1 89.062 (89.062)
 * Prec@1 88.410
current lr 1.00000e-02
Grad=  tensor(9.7731, device='cuda:0')
Epoch: [128][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.4175 (0.4175) ([0.256]+[0.162])	Prec@1 91.406 (91.406)
Epoch: [128][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3025 (0.3323) ([0.141]+[0.161])	Prec@1 94.531 (94.268)
Epoch: [128][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2698 (0.3328) ([0.108]+[0.161])	Prec@1 96.875 (94.143)
Epoch: [128][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4314 (0.3394) ([0.270]+[0.161])	Prec@1 92.188 (93.963)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.4553 (0.4553) ([0.294]+[0.161])	Prec@1 90.625 (90.625)
 * Prec@1 89.050
current lr 1.00000e-02
Grad=  tensor(8.9325, device='cuda:0')
Epoch: [129][0/391]	Time 0.298 (0.298)	Data 0.179 (0.179)	Loss 0.3590 (0.3590) ([0.198]+[0.161])	Prec@1 92.188 (92.188)
Epoch: [129][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3385 (0.3464) ([0.177]+[0.161])	Prec@1 92.969 (93.789)
Epoch: [129][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3064 (0.3375) ([0.145]+[0.161])	Prec@1 94.531 (93.933)
Epoch: [129][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4075 (0.3456) ([0.246]+[0.161])	Prec@1 89.844 (93.646)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5241 (0.5241) ([0.363]+[0.161])	Prec@1 89.844 (89.844)
 * Prec@1 88.340
current lr 1.00000e-02
Grad=  tensor(3.4210, device='cuda:0')
Epoch: [130][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.3086 (0.3086) ([0.147]+[0.161])	Prec@1 96.094 (96.094)
Epoch: [130][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3608 (0.3380) ([0.200]+[0.161])	Prec@1 92.969 (93.928)
Epoch: [130][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3387 (0.3402) ([0.178]+[0.161])	Prec@1 92.969 (93.828)
Epoch: [130][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2979 (0.3441) ([0.137]+[0.161])	Prec@1 97.656 (93.657)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5331 (0.5331) ([0.372]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 88.390
current lr 1.00000e-02
Grad=  tensor(7.0933, device='cuda:0')
Epoch: [131][0/391]	Time 0.299 (0.299)	Data 0.180 (0.180)	Loss 0.3391 (0.3391) ([0.178]+[0.161])	Prec@1 92.969 (92.969)
Epoch: [131][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3352 (0.3301) ([0.174]+[0.161])	Prec@1 94.531 (94.377)
Epoch: [131][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3147 (0.3316) ([0.154]+[0.161])	Prec@1 92.969 (94.150)
Epoch: [131][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3811 (0.3401) ([0.220]+[0.161])	Prec@1 92.969 (93.875)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4961 (0.4961) ([0.335]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 89.000
current lr 1.00000e-02
Grad=  tensor(3.2474, device='cuda:0')
Epoch: [132][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.2678 (0.2678) ([0.107]+[0.161])	Prec@1 95.312 (95.312)
Epoch: [132][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3095 (0.3231) ([0.149]+[0.161])	Prec@1 96.094 (94.763)
Epoch: [132][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4323 (0.3347) ([0.272]+[0.161])	Prec@1 87.500 (94.193)
Epoch: [132][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2960 (0.3417) ([0.135]+[0.161])	Prec@1 94.531 (93.823)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.4458 (0.4458) ([0.285]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 88.320
current lr 1.00000e-02
Grad=  tensor(5.2212, device='cuda:0')
Epoch: [133][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.2881 (0.2881) ([0.127]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [133][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3444 (0.3324) ([0.184]+[0.160])	Prec@1 92.188 (94.361)
Epoch: [133][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3700 (0.3386) ([0.209]+[0.161])	Prec@1 93.750 (94.049)
Epoch: [133][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3704 (0.3438) ([0.210]+[0.161])	Prec@1 92.188 (93.753)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4661 (0.4661) ([0.305]+[0.161])	Prec@1 89.844 (89.844)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(5.9390, device='cuda:0')
Epoch: [134][0/391]	Time 0.300 (0.300)	Data 0.181 (0.181)	Loss 0.3054 (0.3054) ([0.145]+[0.161])	Prec@1 93.750 (93.750)
Epoch: [134][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3347 (0.3270) ([0.174]+[0.161])	Prec@1 92.969 (94.454)
Epoch: [134][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2458 (0.3327) ([0.085]+[0.161])	Prec@1 98.438 (94.154)
Epoch: [134][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2995 (0.3388) ([0.139]+[0.161])	Prec@1 96.094 (93.859)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4194 (0.4194) ([0.259]+[0.161])	Prec@1 91.406 (91.406)
 * Prec@1 88.490
current lr 1.00000e-02
Grad=  tensor(7.8404, device='cuda:0')
Epoch: [135][0/391]	Time 0.309 (0.309)	Data 0.189 (0.189)	Loss 0.3469 (0.3469) ([0.186]+[0.161])	Prec@1 92.969 (92.969)
Epoch: [135][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3397 (0.3291) ([0.179]+[0.161])	Prec@1 93.750 (94.175)
Epoch: [135][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2839 (0.3344) ([0.123]+[0.161])	Prec@1 94.531 (94.007)
Epoch: [135][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3006 (0.3389) ([0.140]+[0.161])	Prec@1 94.531 (93.882)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.3800 (0.3800) ([0.219]+[0.161])	Prec@1 93.750 (93.750)
 * Prec@1 88.720
current lr 1.00000e-02
Grad=  tensor(7.8395, device='cuda:0')
Epoch: [136][0/391]	Time 0.303 (0.303)	Data 0.183 (0.183)	Loss 0.3890 (0.3890) ([0.228]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [136][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3576 (0.3321) ([0.197]+[0.160])	Prec@1 92.969 (94.028)
Epoch: [136][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2505 (0.3376) ([0.090]+[0.161])	Prec@1 96.094 (93.867)
Epoch: [136][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3218 (0.3415) ([0.161]+[0.161])	Prec@1 94.531 (93.633)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.5125 (0.5125) ([0.352]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 88.490
current lr 1.00000e-02
Grad=  tensor(8.6310, device='cuda:0')
Epoch: [137][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.3451 (0.3451) ([0.185]+[0.161])	Prec@1 95.312 (95.312)
Epoch: [137][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.3085 (0.3283) ([0.148]+[0.160])	Prec@1 92.969 (94.315)
Epoch: [137][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.2832 (0.3347) ([0.123]+[0.160])	Prec@1 95.312 (94.003)
Epoch: [137][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3608 (0.3365) ([0.200]+[0.160])	Prec@1 92.188 (93.934)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.5039 (0.5039) ([0.343]+[0.160])	Prec@1 87.500 (87.500)
 * Prec@1 88.990
current lr 1.00000e-02
Grad=  tensor(3.8861, device='cuda:0')
Epoch: [138][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.2651 (0.2651) ([0.105]+[0.160])	Prec@1 97.656 (97.656)
Epoch: [138][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.4283 (0.3323) ([0.268]+[0.160])	Prec@1 91.406 (93.982)
Epoch: [138][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3110 (0.3267) ([0.151]+[0.160])	Prec@1 93.750 (94.310)
Epoch: [138][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3324 (0.3358) ([0.172]+[0.160])	Prec@1 94.531 (93.997)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4225 (0.4225) ([0.262]+[0.161])	Prec@1 91.406 (91.406)
 * Prec@1 88.640
current lr 1.00000e-02
Grad=  tensor(8.1841, device='cuda:0')
Epoch: [139][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.3761 (0.3761) ([0.215]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [139][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3443 (0.3282) ([0.184]+[0.161])	Prec@1 92.188 (94.245)
Epoch: [139][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4284 (0.3346) ([0.268]+[0.161])	Prec@1 89.062 (94.007)
Epoch: [139][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3239 (0.3336) ([0.163]+[0.161])	Prec@1 93.750 (94.015)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3612 (0.3612) ([0.201]+[0.161])	Prec@1 94.531 (94.531)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(4.2265, device='cuda:0')
Epoch: [140][0/391]	Time 0.306 (0.306)	Data 0.189 (0.189)	Loss 0.2772 (0.2772) ([0.117]+[0.161])	Prec@1 96.875 (96.875)
Epoch: [140][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3356 (0.3221) ([0.175]+[0.161])	Prec@1 93.750 (94.438)
Epoch: [140][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3507 (0.3301) ([0.190]+[0.161])	Prec@1 93.750 (94.073)
Epoch: [140][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3400 (0.3287) ([0.180]+[0.160])	Prec@1 92.969 (94.228)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5059 (0.5059) ([0.345]+[0.160])	Prec@1 88.281 (88.281)
 * Prec@1 89.340
current lr 1.00000e-02
Grad=  tensor(8.0475, device='cuda:0')
Epoch: [141][0/391]	Time 0.308 (0.308)	Data 0.183 (0.183)	Loss 0.3589 (0.3589) ([0.198]+[0.160])	Prec@1 93.750 (93.750)
Epoch: [141][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2673 (0.3316) ([0.107]+[0.161])	Prec@1 94.531 (94.114)
Epoch: [141][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3236 (0.3361) ([0.163]+[0.161])	Prec@1 94.531 (94.018)
Epoch: [141][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2817 (0.3384) ([0.121]+[0.161])	Prec@1 96.875 (93.911)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4669 (0.4669) ([0.306]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 87.490
current lr 1.00000e-02
Grad=  tensor(10.0814, device='cuda:0')
Epoch: [142][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3781 (0.3781) ([0.217]+[0.161])	Prec@1 90.625 (90.625)
Epoch: [142][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.3427 (0.3159) ([0.182]+[0.161])	Prec@1 95.312 (94.957)
Epoch: [142][200/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.3495 (0.3320) ([0.189]+[0.161])	Prec@1 89.844 (94.189)
Epoch: [142][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3803 (0.3402) ([0.219]+[0.161])	Prec@1 92.188 (93.812)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4952 (0.4952) ([0.334]+[0.161])	Prec@1 89.062 (89.062)
 * Prec@1 89.070
current lr 1.00000e-02
Grad=  tensor(9.1219, device='cuda:0')
Epoch: [143][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3433 (0.3433) ([0.182]+[0.161])	Prec@1 93.750 (93.750)
Epoch: [143][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3253 (0.3278) ([0.164]+[0.161])	Prec@1 93.750 (94.291)
Epoch: [143][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4375 (0.3317) ([0.277]+[0.161])	Prec@1 89.844 (94.170)
Epoch: [143][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2695 (0.3386) ([0.108]+[0.161])	Prec@1 96.875 (93.914)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.5141 (0.5141) ([0.353]+[0.161])	Prec@1 90.625 (90.625)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(11.0045, device='cuda:0')
Epoch: [144][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.3509 (0.3509) ([0.190]+[0.161])	Prec@1 92.188 (92.188)
Epoch: [144][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3331 (0.3246) ([0.172]+[0.161])	Prec@1 94.531 (94.307)
Epoch: [144][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3375 (0.3264) ([0.177]+[0.161])	Prec@1 94.531 (94.302)
Epoch: [144][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3122 (0.3341) ([0.151]+[0.161])	Prec@1 95.312 (94.012)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.5063 (0.5063) ([0.345]+[0.161])	Prec@1 87.500 (87.500)
 * Prec@1 87.960
current lr 1.00000e-02
Grad=  tensor(9.4863, device='cuda:0')
Epoch: [145][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.3268 (0.3268) ([0.166]+[0.161])	Prec@1 93.750 (93.750)
Epoch: [145][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3112 (0.3258) ([0.150]+[0.161])	Prec@1 95.312 (94.547)
Epoch: [145][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3035 (0.3270) ([0.143]+[0.161])	Prec@1 94.531 (94.333)
Epoch: [145][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4056 (0.3309) ([0.245]+[0.161])	Prec@1 91.406 (94.202)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.4775 (0.4775) ([0.316]+[0.161])	Prec@1 89.844 (89.844)
 * Prec@1 88.970
current lr 1.00000e-02
Grad=  tensor(6.5994, device='cuda:0')
Epoch: [146][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3104 (0.3104) ([0.149]+[0.161])	Prec@1 96.094 (96.094)
Epoch: [146][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2760 (0.3265) ([0.115]+[0.161])	Prec@1 96.094 (94.261)
Epoch: [146][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3560 (0.3314) ([0.195]+[0.161])	Prec@1 92.188 (94.100)
Epoch: [146][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3790 (0.3341) ([0.218]+[0.161])	Prec@1 91.406 (93.973)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4670 (0.4670) ([0.306]+[0.161])	Prec@1 91.406 (91.406)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(11.3272, device='cuda:0')
Epoch: [147][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3720 (0.3720) ([0.211]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [147][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2736 (0.3259) ([0.112]+[0.161])	Prec@1 96.875 (94.423)
Epoch: [147][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.4044 (0.3305) ([0.243]+[0.161])	Prec@1 91.406 (94.209)
Epoch: [147][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3370 (0.3339) ([0.176]+[0.161])	Prec@1 94.531 (94.023)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5184 (0.5184) ([0.357]+[0.161])	Prec@1 90.625 (90.625)
 * Prec@1 88.860
current lr 1.00000e-02
Grad=  tensor(6.4195, device='cuda:0')
Epoch: [148][0/391]	Time 0.304 (0.304)	Data 0.187 (0.187)	Loss 0.3158 (0.3158) ([0.154]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [148][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.3313 (0.3227) ([0.170]+[0.161])	Prec@1 94.531 (94.493)
Epoch: [148][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3389 (0.3251) ([0.178]+[0.161])	Prec@1 93.750 (94.419)
Epoch: [148][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2730 (0.3286) ([0.112]+[0.161])	Prec@1 95.312 (94.248)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.5369 (0.5369) ([0.375]+[0.161])	Prec@1 91.406 (91.406)
 * Prec@1 87.770
current lr 1.00000e-02
Grad=  tensor(6.0881, device='cuda:0')
Epoch: [149][0/391]	Time 0.309 (0.309)	Data 0.190 (0.190)	Loss 0.3268 (0.3268) ([0.165]+[0.161])	Prec@1 94.531 (94.531)
Epoch: [149][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3278 (0.3160) ([0.166]+[0.161])	Prec@1 93.750 (94.717)
Epoch: [149][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3051 (0.3242) ([0.144]+[0.162])	Prec@1 95.312 (94.337)
Epoch: [149][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4091 (0.3327) ([0.247]+[0.162])	Prec@1 89.844 (94.002)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4605 (0.4605) ([0.299]+[0.162])	Prec@1 92.969 (92.969)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(5.8938, device='cuda:0')
Epoch: [150][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 0.2652 (0.2652) ([0.104]+[0.162])	Prec@1 96.094 (96.094)
Epoch: [150][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.4154 (0.3141) ([0.254]+[0.162])	Prec@1 90.625 (94.787)
Epoch: [150][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3136 (0.3261) ([0.152]+[0.162])	Prec@1 95.312 (94.290)
Epoch: [150][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3447 (0.3305) ([0.183]+[0.162])	Prec@1 92.969 (94.207)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4337 (0.4337) ([0.272]+[0.162])	Prec@1 92.969 (92.969)
 * Prec@1 89.310
current lr 1.00000e-02
Grad=  tensor(6.8089, device='cuda:0')
Epoch: [151][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.3141 (0.3141) ([0.152]+[0.162])	Prec@1 96.094 (96.094)
Epoch: [151][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3286 (0.3323) ([0.167]+[0.162])	Prec@1 92.188 (93.866)
Epoch: [151][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4406 (0.3335) ([0.279]+[0.162])	Prec@1 92.969 (94.065)
Epoch: [151][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2890 (0.3360) ([0.127]+[0.162])	Prec@1 96.875 (93.989)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.4660 (0.4660) ([0.304]+[0.162])	Prec@1 91.406 (91.406)
 * Prec@1 88.240
current lr 1.00000e-02
Grad=  tensor(7.9856, device='cuda:0')
Epoch: [152][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3258 (0.3258) ([0.164]+[0.162])	Prec@1 95.312 (95.312)
Epoch: [152][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2948 (0.3196) ([0.133]+[0.162])	Prec@1 96.094 (94.678)
Epoch: [152][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3218 (0.3222) ([0.160]+[0.162])	Prec@1 92.969 (94.473)
Epoch: [152][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2965 (0.3281) ([0.135]+[0.162])	Prec@1 95.312 (94.194)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.5909 (0.5909) ([0.429]+[0.162])	Prec@1 87.500 (87.500)
 * Prec@1 86.680
current lr 1.00000e-02
Grad=  tensor(10.1087, device='cuda:0')
Epoch: [153][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3303 (0.3303) ([0.168]+[0.162])	Prec@1 95.312 (95.312)
Epoch: [153][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3221 (0.3211) ([0.160]+[0.162])	Prec@1 93.750 (94.516)
Epoch: [153][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3884 (0.3281) ([0.226]+[0.162])	Prec@1 92.188 (94.115)
Epoch: [153][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3163 (0.3339) ([0.154]+[0.162])	Prec@1 94.531 (93.955)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4882 (0.4882) ([0.326]+[0.162])	Prec@1 89.062 (89.062)
 * Prec@1 88.630
current lr 1.00000e-02
Grad=  tensor(6.8129, device='cuda:0')
Epoch: [154][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.3475 (0.3475) ([0.185]+[0.162])	Prec@1 94.531 (94.531)
Epoch: [154][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2561 (0.3157) ([0.094]+[0.162])	Prec@1 97.656 (94.872)
Epoch: [154][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3135 (0.3253) ([0.151]+[0.162])	Prec@1 93.750 (94.399)
Epoch: [154][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4204 (0.3314) ([0.258]+[0.162])	Prec@1 92.969 (94.238)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4492 (0.4492) ([0.287]+[0.162])	Prec@1 92.188 (92.188)
 * Prec@1 88.600
current lr 1.00000e-02
Grad=  tensor(3.1459, device='cuda:0')
Epoch: [155][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.2663 (0.2663) ([0.104]+[0.162])	Prec@1 96.094 (96.094)
Epoch: [155][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2700 (0.3255) ([0.108]+[0.162])	Prec@1 96.094 (94.516)
Epoch: [155][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3337 (0.3295) ([0.171]+[0.162])	Prec@1 93.750 (94.236)
Epoch: [155][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3777 (0.3358) ([0.215]+[0.162])	Prec@1 93.750 (93.984)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4664 (0.4664) ([0.304]+[0.162])	Prec@1 87.500 (87.500)
 * Prec@1 87.780
current lr 1.00000e-02
Grad=  tensor(8.8908, device='cuda:0')
Epoch: [156][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.3549 (0.3549) ([0.192]+[0.162])	Prec@1 92.188 (92.188)
Epoch: [156][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3509 (0.3142) ([0.189]+[0.162])	Prec@1 94.531 (94.941)
Epoch: [156][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2430 (0.3208) ([0.081]+[0.162])	Prec@1 98.438 (94.597)
Epoch: [156][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2616 (0.3237) ([0.099]+[0.162])	Prec@1 96.875 (94.430)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.5211 (0.5211) ([0.359]+[0.162])	Prec@1 91.406 (91.406)
 * Prec@1 87.730
current lr 1.00000e-02
Grad=  tensor(10.4535, device='cuda:0')
Epoch: [157][0/391]	Time 0.309 (0.309)	Data 0.189 (0.189)	Loss 0.3207 (0.3207) ([0.158]+[0.162])	Prec@1 95.312 (95.312)
Epoch: [157][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3267 (0.3184) ([0.164]+[0.162])	Prec@1 94.531 (94.732)
Epoch: [157][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3169 (0.3220) ([0.155]+[0.162])	Prec@1 95.312 (94.454)
Epoch: [157][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3039 (0.3258) ([0.142]+[0.162])	Prec@1 94.531 (94.292)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.5855 (0.5855) ([0.423]+[0.163])	Prec@1 89.062 (89.062)
 * Prec@1 88.420
current lr 1.00000e-02
Grad=  tensor(3.8339, device='cuda:0')
Epoch: [158][0/391]	Time 0.310 (0.310)	Data 0.190 (0.190)	Loss 0.2713 (0.2713) ([0.109]+[0.163])	Prec@1 96.875 (96.875)
Epoch: [158][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3086 (0.3175) ([0.146]+[0.162])	Prec@1 93.750 (94.585)
Epoch: [158][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3072 (0.3222) ([0.145]+[0.163])	Prec@1 93.750 (94.403)
Epoch: [158][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3288 (0.3256) ([0.166]+[0.162])	Prec@1 92.969 (94.267)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4351 (0.4351) ([0.272]+[0.163])	Prec@1 93.750 (93.750)
 * Prec@1 89.230
current lr 1.00000e-02
Grad=  tensor(8.4214, device='cuda:0')
Epoch: [159][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.3401 (0.3401) ([0.177]+[0.163])	Prec@1 94.531 (94.531)
Epoch: [159][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3453 (0.3327) ([0.182]+[0.163])	Prec@1 92.188 (94.114)
Epoch: [159][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3286 (0.3284) ([0.166]+[0.163])	Prec@1 94.531 (94.255)
Epoch: [159][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3844 (0.3342) ([0.222]+[0.163])	Prec@1 92.969 (94.043)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.4159 (0.4159) ([0.253]+[0.163])	Prec@1 91.406 (91.406)
 * Prec@1 89.810
current lr 1.00000e-02
Grad=  tensor(7.0705, device='cuda:0')
Epoch: [160][0/391]	Time 0.316 (0.316)	Data 0.196 (0.196)	Loss 0.3141 (0.3141) ([0.151]+[0.163])	Prec@1 95.312 (95.312)
Epoch: [160][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3333 (0.3218) ([0.170]+[0.163])	Prec@1 93.750 (94.462)
Epoch: [160][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2876 (0.3195) ([0.125]+[0.163])	Prec@1 95.312 (94.485)
Epoch: [160][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3926 (0.3262) ([0.230]+[0.163])	Prec@1 92.188 (94.248)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4250 (0.4250) ([0.262]+[0.163])	Prec@1 93.750 (93.750)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(11.6834, device='cuda:0')
Epoch: [161][0/391]	Time 0.314 (0.314)	Data 0.195 (0.195)	Loss 0.4034 (0.4034) ([0.241]+[0.163])	Prec@1 91.406 (91.406)
Epoch: [161][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3100 (0.3161) ([0.147]+[0.163])	Prec@1 95.312 (94.717)
Epoch: [161][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3456 (0.3205) ([0.183]+[0.163])	Prec@1 93.750 (94.648)
Epoch: [161][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3135 (0.3301) ([0.151]+[0.163])	Prec@1 96.094 (94.274)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.6274 (0.6274) ([0.465]+[0.163])	Prec@1 85.156 (85.156)
 * Prec@1 86.200
current lr 1.00000e-02
Grad=  tensor(4.2206, device='cuda:0')
Epoch: [162][0/391]	Time 0.320 (0.320)	Data 0.201 (0.201)	Loss 0.2518 (0.2518) ([0.089]+[0.163])	Prec@1 97.656 (97.656)
Epoch: [162][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3359 (0.3163) ([0.173]+[0.163])	Prec@1 96.094 (94.756)
Epoch: [162][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3739 (0.3253) ([0.211]+[0.163])	Prec@1 92.969 (94.430)
Epoch: [162][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3538 (0.3289) ([0.191]+[0.163])	Prec@1 94.531 (94.337)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3817 (0.3817) ([0.219]+[0.163])	Prec@1 91.406 (91.406)
 * Prec@1 89.640
current lr 1.00000e-02
Grad=  tensor(5.6138, device='cuda:0')
Epoch: [163][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.3072 (0.3072) ([0.144]+[0.163])	Prec@1 95.312 (95.312)
Epoch: [163][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3169 (0.3149) ([0.154]+[0.163])	Prec@1 95.312 (94.771)
Epoch: [163][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3487 (0.3240) ([0.186]+[0.163])	Prec@1 94.531 (94.485)
Epoch: [163][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3463 (0.3259) ([0.183]+[0.163])	Prec@1 92.969 (94.365)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.5516 (0.5516) ([0.388]+[0.163])	Prec@1 85.156 (85.156)
 * Prec@1 88.180
current lr 1.00000e-02
Grad=  tensor(5.8797, device='cuda:0')
Epoch: [164][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 0.2989 (0.2989) ([0.136]+[0.163])	Prec@1 96.875 (96.875)
Epoch: [164][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3338 (0.3225) ([0.171]+[0.163])	Prec@1 93.750 (94.423)
Epoch: [164][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3619 (0.3231) ([0.199]+[0.163])	Prec@1 92.969 (94.387)
Epoch: [164][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3218 (0.3268) ([0.159]+[0.163])	Prec@1 95.312 (94.220)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5559 (0.5559) ([0.393]+[0.163])	Prec@1 89.844 (89.844)
 * Prec@1 88.760
current lr 1.00000e-02
Grad=  tensor(9.4717, device='cuda:0')
Epoch: [165][0/391]	Time 0.302 (0.302)	Data 0.182 (0.182)	Loss 0.3541 (0.3541) ([0.191]+[0.163])	Prec@1 92.969 (92.969)
Epoch: [165][100/391]	Time 0.107 (0.111)	Data 0.000 (0.002)	Loss 0.2734 (0.3133) ([0.110]+[0.163])	Prec@1 96.094 (94.810)
Epoch: [165][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.3758 (0.3174) ([0.213]+[0.163])	Prec@1 92.969 (94.644)
Epoch: [165][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2638 (0.3204) ([0.101]+[0.163])	Prec@1 96.094 (94.583)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4957 (0.4957) ([0.333]+[0.163])	Prec@1 91.406 (91.406)
 * Prec@1 88.770
current lr 1.00000e-02
Grad=  tensor(4.2553, device='cuda:0')
Epoch: [166][0/391]	Time 0.301 (0.301)	Data 0.183 (0.183)	Loss 0.2623 (0.2623) ([0.099]+[0.163])	Prec@1 96.094 (96.094)
Epoch: [166][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3421 (0.3070) ([0.179]+[0.163])	Prec@1 93.750 (94.949)
Epoch: [166][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2787 (0.3187) ([0.116]+[0.163])	Prec@1 97.656 (94.656)
Epoch: [166][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2867 (0.3261) ([0.123]+[0.163])	Prec@1 94.531 (94.396)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4997 (0.4997) ([0.337]+[0.163])	Prec@1 89.062 (89.062)
 * Prec@1 87.450
current lr 1.00000e-02
Grad=  tensor(9.1795, device='cuda:0')
Epoch: [167][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.3136 (0.3136) ([0.150]+[0.163])	Prec@1 92.969 (92.969)
Epoch: [167][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3146 (0.3161) ([0.151]+[0.163])	Prec@1 94.531 (94.531)
Epoch: [167][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3376 (0.3196) ([0.175]+[0.163])	Prec@1 92.969 (94.555)
Epoch: [167][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3344 (0.3184) ([0.171]+[0.163])	Prec@1 95.312 (94.601)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4239 (0.4239) ([0.261]+[0.163])	Prec@1 90.625 (90.625)
 * Prec@1 89.260
current lr 1.00000e-02
Grad=  tensor(7.1784, device='cuda:0')
Epoch: [168][0/391]	Time 0.303 (0.303)	Data 0.186 (0.186)	Loss 0.3261 (0.3261) ([0.163]+[0.163])	Prec@1 95.312 (95.312)
Epoch: [168][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3687 (0.3124) ([0.206]+[0.163])	Prec@1 92.188 (94.872)
Epoch: [168][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3223 (0.3141) ([0.159]+[0.163])	Prec@1 93.750 (94.784)
Epoch: [168][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3068 (0.3215) ([0.144]+[0.163])	Prec@1 93.750 (94.526)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.5189 (0.5189) ([0.355]+[0.163])	Prec@1 89.844 (89.844)
 * Prec@1 88.120
current lr 1.00000e-02
Grad=  tensor(11.3859, device='cuda:0')
Epoch: [169][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.4133 (0.4133) ([0.250]+[0.163])	Prec@1 92.188 (92.188)
Epoch: [169][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3629 (0.3203) ([0.200]+[0.163])	Prec@1 93.750 (94.346)
Epoch: [169][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2832 (0.3184) ([0.120]+[0.163])	Prec@1 96.875 (94.496)
Epoch: [169][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2948 (0.3203) ([0.132]+[0.163])	Prec@1 96.875 (94.503)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4541 (0.4541) ([0.291]+[0.163])	Prec@1 89.844 (89.844)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(7.2589, device='cuda:0')
Epoch: [170][0/391]	Time 0.303 (0.303)	Data 0.185 (0.185)	Loss 0.2820 (0.2820) ([0.119]+[0.163])	Prec@1 96.875 (96.875)
Epoch: [170][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3759 (0.3159) ([0.213]+[0.163])	Prec@1 93.750 (94.748)
Epoch: [170][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3467 (0.3199) ([0.183]+[0.163])	Prec@1 92.969 (94.551)
Epoch: [170][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2469 (0.3208) ([0.084]+[0.163])	Prec@1 96.094 (94.557)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4165 (0.4165) ([0.253]+[0.164])	Prec@1 89.844 (89.844)
 * Prec@1 88.070
current lr 1.00000e-02
Grad=  tensor(5.4487, device='cuda:0')
Epoch: [171][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.2741 (0.2741) ([0.110]+[0.164])	Prec@1 97.656 (97.656)
Epoch: [171][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2482 (0.3200) ([0.085]+[0.164])	Prec@1 97.656 (94.810)
Epoch: [171][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2812 (0.3197) ([0.118]+[0.164])	Prec@1 93.750 (94.671)
Epoch: [171][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4197 (0.3232) ([0.256]+[0.164])	Prec@1 92.188 (94.464)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.5186 (0.5186) ([0.355]+[0.164])	Prec@1 89.062 (89.062)
 * Prec@1 89.260
current lr 1.00000e-02
Grad=  tensor(5.7443, device='cuda:0')
Epoch: [172][0/391]	Time 0.298 (0.298)	Data 0.178 (0.178)	Loss 0.2616 (0.2616) ([0.098]+[0.164])	Prec@1 96.875 (96.875)
Epoch: [172][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2890 (0.3066) ([0.126]+[0.163])	Prec@1 96.094 (95.073)
Epoch: [172][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2565 (0.3090) ([0.093]+[0.164])	Prec@1 96.875 (95.060)
Epoch: [172][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4074 (0.3142) ([0.244]+[0.164])	Prec@1 90.625 (94.902)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.6380 (0.6380) ([0.474]+[0.164])	Prec@1 86.719 (86.719)
 * Prec@1 88.730
current lr 1.00000e-02
Grad=  tensor(6.0153, device='cuda:0')
Epoch: [173][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.2987 (0.2987) ([0.135]+[0.164])	Prec@1 96.094 (96.094)
Epoch: [173][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2912 (0.3227) ([0.127]+[0.164])	Prec@1 94.531 (94.717)
Epoch: [173][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2769 (0.3244) ([0.113]+[0.164])	Prec@1 97.656 (94.547)
Epoch: [173][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3538 (0.3261) ([0.190]+[0.164])	Prec@1 94.531 (94.505)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.5772 (0.5772) ([0.413]+[0.164])	Prec@1 85.938 (85.938)
 * Prec@1 87.320
current lr 1.00000e-02
Grad=  tensor(16.0702, device='cuda:0')
Epoch: [174][0/391]	Time 0.297 (0.297)	Data 0.177 (0.177)	Loss 0.4646 (0.4646) ([0.301]+[0.164])	Prec@1 89.062 (89.062)
Epoch: [174][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3108 (0.3156) ([0.147]+[0.164])	Prec@1 93.750 (94.732)
Epoch: [174][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3348 (0.3192) ([0.171]+[0.164])	Prec@1 95.312 (94.551)
Epoch: [174][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3746 (0.3209) ([0.210]+[0.164])	Prec@1 92.969 (94.503)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4123 (0.4123) ([0.248]+[0.164])	Prec@1 92.969 (92.969)
 * Prec@1 88.470
current lr 1.00000e-02
Grad=  tensor(3.8135, device='cuda:0')
Epoch: [175][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.2622 (0.2622) ([0.098]+[0.164])	Prec@1 97.656 (97.656)
Epoch: [175][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3323 (0.3205) ([0.168]+[0.164])	Prec@1 93.750 (94.585)
Epoch: [175][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2608 (0.3220) ([0.096]+[0.164])	Prec@1 96.094 (94.558)
Epoch: [175][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2870 (0.3228) ([0.123]+[0.164])	Prec@1 96.094 (94.472)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4870 (0.4870) ([0.322]+[0.165])	Prec@1 89.062 (89.062)
 * Prec@1 88.940
current lr 1.00000e-02
Grad=  tensor(10.2079, device='cuda:0')
Epoch: [176][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3628 (0.3628) ([0.198]+[0.165])	Prec@1 93.750 (93.750)
Epoch: [176][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2975 (0.3211) ([0.133]+[0.164])	Prec@1 95.312 (94.516)
Epoch: [176][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2704 (0.3233) ([0.106]+[0.164])	Prec@1 95.312 (94.380)
Epoch: [176][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3438 (0.3242) ([0.179]+[0.164])	Prec@1 95.312 (94.370)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5031 (0.5031) ([0.339]+[0.164])	Prec@1 89.062 (89.062)
 * Prec@1 89.130
current lr 1.00000e-02
Grad=  tensor(6.4653, device='cuda:0')
Epoch: [177][0/391]	Time 0.298 (0.298)	Data 0.179 (0.179)	Loss 0.2616 (0.2616) ([0.097]+[0.164])	Prec@1 96.875 (96.875)
Epoch: [177][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2688 (0.3139) ([0.105]+[0.164])	Prec@1 97.656 (94.995)
Epoch: [177][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3110 (0.3177) ([0.147]+[0.164])	Prec@1 93.750 (94.784)
Epoch: [177][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4076 (0.3250) ([0.243]+[0.164])	Prec@1 93.750 (94.505)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4767 (0.4767) ([0.312]+[0.165])	Prec@1 90.625 (90.625)
 * Prec@1 88.780
current lr 1.00000e-02
Grad=  tensor(6.0163, device='cuda:0')
Epoch: [178][0/391]	Time 0.300 (0.300)	Data 0.181 (0.181)	Loss 0.2765 (0.2765) ([0.112]+[0.165])	Prec@1 96.875 (96.875)
Epoch: [178][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2698 (0.3175) ([0.105]+[0.165])	Prec@1 94.531 (94.879)
Epoch: [178][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3874 (0.3155) ([0.223]+[0.165])	Prec@1 94.531 (94.889)
Epoch: [178][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2983 (0.3194) ([0.134]+[0.165])	Prec@1 93.750 (94.723)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4415 (0.4415) ([0.277]+[0.165])	Prec@1 89.062 (89.062)
 * Prec@1 88.960
current lr 1.00000e-02
Grad=  tensor(8.5090, device='cuda:0')
Epoch: [179][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.3817 (0.3817) ([0.217]+[0.165])	Prec@1 95.312 (95.312)
Epoch: [179][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2744 (0.3160) ([0.110]+[0.165])	Prec@1 94.531 (94.841)
Epoch: [179][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4080 (0.3180) ([0.243]+[0.165])	Prec@1 92.969 (94.788)
Epoch: [179][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3036 (0.3230) ([0.139]+[0.165])	Prec@1 96.875 (94.609)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4485 (0.4485) ([0.284]+[0.165])	Prec@1 90.625 (90.625)
 * Prec@1 88.910
current lr 1.00000e-02
Grad=  tensor(6.8653, device='cuda:0')
Epoch: [180][0/391]	Time 0.297 (0.297)	Data 0.178 (0.178)	Loss 0.2652 (0.2652) ([0.101]+[0.165])	Prec@1 97.656 (97.656)
Epoch: [180][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.4601 (0.3167) ([0.295]+[0.165])	Prec@1 90.625 (94.663)
Epoch: [180][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3654 (0.3160) ([0.201]+[0.165])	Prec@1 93.750 (94.823)
Epoch: [180][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3472 (0.3205) ([0.183]+[0.165])	Prec@1 94.531 (94.562)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.5355 (0.5355) ([0.371]+[0.165])	Prec@1 89.062 (89.062)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(6.3842, device='cuda:0')
Epoch: [181][0/391]	Time 0.301 (0.301)	Data 0.183 (0.183)	Loss 0.2834 (0.2834) ([0.119]+[0.165])	Prec@1 95.312 (95.312)
Epoch: [181][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2554 (0.3071) ([0.091]+[0.164])	Prec@1 96.094 (95.088)
Epoch: [181][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3130 (0.3131) ([0.149]+[0.165])	Prec@1 95.312 (94.916)
Epoch: [181][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3184 (0.3162) ([0.154]+[0.165])	Prec@1 94.531 (94.838)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.4609 (0.4609) ([0.296]+[0.165])	Prec@1 92.188 (92.188)
 * Prec@1 89.200
current lr 1.00000e-02
Grad=  tensor(9.4856, device='cuda:0')
Epoch: [182][0/391]	Time 0.296 (0.296)	Data 0.181 (0.181)	Loss 0.3427 (0.3427) ([0.178]+[0.165])	Prec@1 94.531 (94.531)
Epoch: [182][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3455 (0.3149) ([0.181]+[0.165])	Prec@1 96.094 (94.841)
Epoch: [182][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3637 (0.3189) ([0.199]+[0.165])	Prec@1 91.406 (94.741)
Epoch: [182][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3578 (0.3157) ([0.193]+[0.165])	Prec@1 92.969 (94.863)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.5696 (0.5696) ([0.405]+[0.165])	Prec@1 88.281 (88.281)
 * Prec@1 88.810
current lr 1.00000e-02
Grad=  tensor(9.1251, device='cuda:0')
Epoch: [183][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.2961 (0.2961) ([0.131]+[0.165])	Prec@1 95.312 (95.312)
Epoch: [183][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3169 (0.2986) ([0.152]+[0.164])	Prec@1 96.875 (95.606)
Epoch: [183][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3698 (0.3091) ([0.205]+[0.164])	Prec@1 92.188 (95.044)
Epoch: [183][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2525 (0.3143) ([0.088]+[0.165])	Prec@1 98.438 (94.897)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.5426 (0.5426) ([0.378]+[0.165])	Prec@1 88.281 (88.281)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(7.9317, device='cuda:0')
Epoch: [184][0/391]	Time 0.302 (0.302)	Data 0.184 (0.184)	Loss 0.3247 (0.3247) ([0.160]+[0.165])	Prec@1 93.750 (93.750)
Epoch: [184][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.3183 (0.3090) ([0.154]+[0.165])	Prec@1 93.750 (95.050)
Epoch: [184][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3565 (0.3024) ([0.192]+[0.164])	Prec@1 93.750 (95.332)
Epoch: [184][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3542 (0.3134) ([0.189]+[0.165])	Prec@1 92.188 (94.908)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5069 (0.5069) ([0.342]+[0.165])	Prec@1 89.062 (89.062)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(4.9439, device='cuda:0')
Epoch: [185][0/391]	Time 0.299 (0.299)	Data 0.182 (0.182)	Loss 0.2384 (0.2384) ([0.074]+[0.165])	Prec@1 96.875 (96.875)
Epoch: [185][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2982 (0.3160) ([0.133]+[0.165])	Prec@1 96.094 (94.856)
Epoch: [185][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3124 (0.3182) ([0.148]+[0.165])	Prec@1 95.312 (94.710)
Epoch: [185][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3624 (0.3190) ([0.198]+[0.165])	Prec@1 92.969 (94.757)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.5113 (0.5113) ([0.346]+[0.165])	Prec@1 87.500 (87.500)
 * Prec@1 88.500
current lr 1.00000e-02
Grad=  tensor(8.5875, device='cuda:0')
Epoch: [186][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.2955 (0.2955) ([0.130]+[0.165])	Prec@1 95.312 (95.312)
Epoch: [186][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2633 (0.3054) ([0.098]+[0.165])	Prec@1 96.875 (95.204)
Epoch: [186][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.4286 (0.3119) ([0.264]+[0.165])	Prec@1 92.969 (95.114)
Epoch: [186][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3873 (0.3149) ([0.222]+[0.165])	Prec@1 92.969 (94.876)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4571 (0.4571) ([0.292]+[0.165])	Prec@1 90.625 (90.625)
 * Prec@1 89.700
current lr 1.00000e-02
Grad=  tensor(3.6905, device='cuda:0')
Epoch: [187][0/391]	Time 0.308 (0.308)	Data 0.190 (0.190)	Loss 0.2306 (0.2306) ([0.066]+[0.165])	Prec@1 98.438 (98.438)
Epoch: [187][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3004 (0.2971) ([0.136]+[0.165])	Prec@1 95.312 (95.529)
Epoch: [187][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3173 (0.3058) ([0.152]+[0.165])	Prec@1 97.656 (95.114)
Epoch: [187][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2736 (0.3105) ([0.109]+[0.165])	Prec@1 96.094 (94.949)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.5208 (0.5208) ([0.356]+[0.165])	Prec@1 90.625 (90.625)
 * Prec@1 89.660
current lr 1.00000e-02
Grad=  tensor(4.8027, device='cuda:0')
Epoch: [188][0/391]	Time 0.308 (0.308)	Data 0.190 (0.190)	Loss 0.2788 (0.2788) ([0.114]+[0.165])	Prec@1 96.875 (96.875)
Epoch: [188][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3024 (0.3060) ([0.138]+[0.165])	Prec@1 96.094 (95.127)
Epoch: [188][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3152 (0.3076) ([0.150]+[0.165])	Prec@1 94.531 (95.223)
Epoch: [188][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3253 (0.3128) ([0.160]+[0.165])	Prec@1 94.531 (95.017)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4668 (0.4668) ([0.302]+[0.165])	Prec@1 89.844 (89.844)
 * Prec@1 89.220
current lr 1.00000e-02
Grad=  tensor(11.6686, device='cuda:0')
Epoch: [189][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.3286 (0.3286) ([0.164]+[0.165])	Prec@1 93.750 (93.750)
Epoch: [189][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2929 (0.3174) ([0.128]+[0.165])	Prec@1 95.312 (94.717)
Epoch: [189][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2847 (0.3189) ([0.120]+[0.165])	Prec@1 96.875 (94.609)
Epoch: [189][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3014 (0.3232) ([0.136]+[0.165])	Prec@1 94.531 (94.521)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4547 (0.4547) ([0.289]+[0.166])	Prec@1 90.625 (90.625)
 * Prec@1 89.000
current lr 1.00000e-02
Grad=  tensor(5.1296, device='cuda:0')
Epoch: [190][0/391]	Time 0.308 (0.308)	Data 0.191 (0.191)	Loss 0.2723 (0.2723) ([0.107]+[0.166])	Prec@1 96.094 (96.094)
Epoch: [190][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.3278 (0.3020) ([0.163]+[0.165])	Prec@1 92.188 (95.258)
Epoch: [190][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2659 (0.3100) ([0.101]+[0.165])	Prec@1 96.094 (95.002)
Epoch: [190][300/391]	Time 0.109 (0.108)	Data 0.000 (0.001)	Loss 0.3453 (0.3152) ([0.180]+[0.165])	Prec@1 95.312 (94.749)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5616 (0.5616) ([0.396]+[0.166])	Prec@1 89.062 (89.062)
 * Prec@1 88.090
current lr 1.00000e-02
Grad=  tensor(9.8657, device='cuda:0')
Epoch: [191][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 0.3101 (0.3101) ([0.145]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [191][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2145 (0.3192) ([0.049]+[0.165])	Prec@1 99.219 (94.694)
Epoch: [191][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2759 (0.3138) ([0.110]+[0.165])	Prec@1 96.094 (94.854)
Epoch: [191][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3401 (0.3130) ([0.175]+[0.165])	Prec@1 92.188 (94.871)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.5130 (0.5130) ([0.348]+[0.165])	Prec@1 89.062 (89.062)
 * Prec@1 87.940
current lr 1.00000e-02
Grad=  tensor(13.1130, device='cuda:0')
Epoch: [192][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.3571 (0.3571) ([0.192]+[0.165])	Prec@1 90.625 (90.625)
Epoch: [192][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3996 (0.3124) ([0.234]+[0.165])	Prec@1 92.188 (95.142)
Epoch: [192][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2860 (0.3191) ([0.120]+[0.166])	Prec@1 95.312 (94.737)
Epoch: [192][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4223 (0.3217) ([0.257]+[0.166])	Prec@1 91.406 (94.744)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4881 (0.4881) ([0.323]+[0.166])	Prec@1 91.406 (91.406)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(8.9671, device='cuda:0')
Epoch: [193][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.3576 (0.3576) ([0.192]+[0.166])	Prec@1 92.969 (92.969)
Epoch: [193][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3087 (0.3022) ([0.143]+[0.165])	Prec@1 95.312 (95.429)
Epoch: [193][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2585 (0.3093) ([0.093]+[0.165])	Prec@1 97.656 (95.021)
Epoch: [193][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.4423 (0.3143) ([0.277]+[0.166])	Prec@1 91.406 (94.856)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4785 (0.4785) ([0.313]+[0.166])	Prec@1 90.625 (90.625)
 * Prec@1 89.020
current lr 1.00000e-02
Grad=  tensor(9.6788, device='cuda:0')
Epoch: [194][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.3614 (0.3614) ([0.196]+[0.166])	Prec@1 92.969 (92.969)
Epoch: [194][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3175 (0.3077) ([0.152]+[0.165])	Prec@1 95.312 (95.158)
Epoch: [194][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3645 (0.3049) ([0.199]+[0.165])	Prec@1 92.188 (95.340)
Epoch: [194][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3591 (0.3116) ([0.194]+[0.166])	Prec@1 91.406 (95.084)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4802 (0.4802) ([0.315]+[0.166])	Prec@1 88.281 (88.281)
 * Prec@1 89.290
current lr 1.00000e-02
Grad=  tensor(11.3072, device='cuda:0')
Epoch: [195][0/391]	Time 0.299 (0.299)	Data 0.180 (0.180)	Loss 0.3682 (0.3682) ([0.203]+[0.166])	Prec@1 92.969 (92.969)
Epoch: [195][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3899 (0.3007) ([0.224]+[0.165])	Prec@1 91.406 (95.297)
Epoch: [195][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2705 (0.3067) ([0.105]+[0.166])	Prec@1 95.312 (95.091)
Epoch: [195][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3823 (0.3149) ([0.217]+[0.166])	Prec@1 93.750 (94.876)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.5301 (0.5301) ([0.364]+[0.166])	Prec@1 90.625 (90.625)
 * Prec@1 89.050
current lr 1.00000e-02
Grad=  tensor(3.2982, device='cuda:0')
Epoch: [196][0/391]	Time 0.305 (0.305)	Data 0.187 (0.187)	Loss 0.2553 (0.2553) ([0.090]+[0.166])	Prec@1 98.438 (98.438)
Epoch: [196][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2775 (0.3068) ([0.112]+[0.166])	Prec@1 96.875 (94.995)
Epoch: [196][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3543 (0.3139) ([0.189]+[0.165])	Prec@1 93.750 (94.885)
Epoch: [196][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3533 (0.3145) ([0.188]+[0.166])	Prec@1 93.750 (94.874)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.6848 (0.6848) ([0.519]+[0.165])	Prec@1 86.719 (86.719)
 * Prec@1 87.360
current lr 1.00000e-02
Grad=  tensor(6.9367, device='cuda:0')
Epoch: [197][0/391]	Time 0.307 (0.307)	Data 0.190 (0.190)	Loss 0.2622 (0.2622) ([0.097]+[0.165])	Prec@1 96.875 (96.875)
Epoch: [197][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2646 (0.3076) ([0.099]+[0.165])	Prec@1 98.438 (95.142)
Epoch: [197][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3195 (0.3107) ([0.154]+[0.165])	Prec@1 95.312 (95.005)
Epoch: [197][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2416 (0.3124) ([0.076]+[0.165])	Prec@1 96.875 (95.009)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4811 (0.4811) ([0.316]+[0.165])	Prec@1 92.969 (92.969)
 * Prec@1 89.620
current lr 1.00000e-02
Grad=  tensor(11.8413, device='cuda:0')
Epoch: [198][0/391]	Time 0.305 (0.305)	Data 0.184 (0.184)	Loss 0.3390 (0.3390) ([0.174]+[0.165])	Prec@1 93.750 (93.750)
Epoch: [198][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3360 (0.3073) ([0.171]+[0.165])	Prec@1 93.750 (95.104)
Epoch: [198][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3653 (0.3080) ([0.200]+[0.165])	Prec@1 92.969 (95.079)
Epoch: [198][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2837 (0.3099) ([0.118]+[0.165])	Prec@1 95.312 (94.998)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5813 (0.5813) ([0.416]+[0.166])	Prec@1 88.281 (88.281)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(6.3817, device='cuda:0')
Epoch: [199][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.3141 (0.3141) ([0.149]+[0.166])	Prec@1 96.094 (96.094)
Epoch: [199][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3935 (0.3048) ([0.228]+[0.166])	Prec@1 92.188 (95.127)
Epoch: [199][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2711 (0.3127) ([0.105]+[0.166])	Prec@1 97.656 (94.908)
Epoch: [199][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3226 (0.3155) ([0.157]+[0.166])	Prec@1 94.531 (94.866)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4167 (0.4167) ([0.251]+[0.166])	Prec@1 92.188 (92.188)
 * Prec@1 88.040
current lr 1.00000e-02
Grad=  tensor(8.8702, device='cuda:0')
Epoch: [200][0/391]	Time 0.302 (0.302)	Data 0.183 (0.183)	Loss 0.3211 (0.3211) ([0.155]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [200][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.2855 (0.3053) ([0.120]+[0.166])	Prec@1 95.312 (95.104)
Epoch: [200][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.3075 (0.3158) ([0.142]+[0.166])	Prec@1 93.750 (94.718)
Epoch: [200][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3081 (0.3175) ([0.142]+[0.166])	Prec@1 94.531 (94.692)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4486 (0.4486) ([0.283]+[0.166])	Prec@1 89.062 (89.062)
 * Prec@1 89.200
current lr 1.00000e-02
Grad=  tensor(6.7484, device='cuda:0')
Epoch: [201][0/391]	Time 0.300 (0.300)	Data 0.183 (0.183)	Loss 0.2844 (0.2844) ([0.118]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [201][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3650 (0.3069) ([0.199]+[0.166])	Prec@1 92.969 (95.119)
Epoch: [201][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2583 (0.3068) ([0.093]+[0.166])	Prec@1 97.656 (95.254)
Epoch: [201][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3261 (0.3103) ([0.160]+[0.166])	Prec@1 91.406 (95.035)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.6767 (0.6767) ([0.511]+[0.166])	Prec@1 87.500 (87.500)
 * Prec@1 87.710
current lr 1.00000e-02
Grad=  tensor(3.8689, device='cuda:0')
Epoch: [202][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.2486 (0.2486) ([0.083]+[0.166])	Prec@1 98.438 (98.438)
Epoch: [202][100/391]	Time 0.110 (0.110)	Data 0.000 (0.002)	Loss 0.3231 (0.3137) ([0.157]+[0.166])	Prec@1 96.094 (94.771)
Epoch: [202][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3854 (0.3143) ([0.220]+[0.166])	Prec@1 93.750 (94.776)
Epoch: [202][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.3369 (0.3171) ([0.171]+[0.166])	Prec@1 92.969 (94.705)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.5247 (0.5247) ([0.359]+[0.166])	Prec@1 89.844 (89.844)
 * Prec@1 88.680
current lr 1.00000e-02
Grad=  tensor(8.9800, device='cuda:0')
Epoch: [203][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.3148 (0.3148) ([0.149]+[0.166])	Prec@1 93.750 (93.750)
Epoch: [203][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2600 (0.3025) ([0.094]+[0.166])	Prec@1 96.875 (95.251)
Epoch: [203][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2867 (0.3014) ([0.121]+[0.166])	Prec@1 96.094 (95.340)
Epoch: [203][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2951 (0.3060) ([0.129]+[0.166])	Prec@1 96.094 (95.178)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4857 (0.4857) ([0.320]+[0.166])	Prec@1 90.625 (90.625)
 * Prec@1 89.500
current lr 1.00000e-02
Grad=  tensor(5.2956, device='cuda:0')
Epoch: [204][0/391]	Time 0.310 (0.310)	Data 0.190 (0.190)	Loss 0.2662 (0.2662) ([0.100]+[0.166])	Prec@1 97.656 (97.656)
Epoch: [204][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3396 (0.3030) ([0.174]+[0.166])	Prec@1 94.531 (95.359)
Epoch: [204][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3565 (0.3092) ([0.191]+[0.166])	Prec@1 92.188 (95.087)
Epoch: [204][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3844 (0.3164) ([0.218]+[0.166])	Prec@1 93.750 (94.804)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5001 (0.5001) ([0.334]+[0.166])	Prec@1 92.188 (92.188)
 * Prec@1 89.640
current lr 1.00000e-02
Grad=  tensor(3.9504, device='cuda:0')
Epoch: [205][0/391]	Time 0.300 (0.300)	Data 0.180 (0.180)	Loss 0.2603 (0.2603) ([0.094]+[0.166])	Prec@1 96.875 (96.875)
Epoch: [205][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2740 (0.2925) ([0.108]+[0.166])	Prec@1 96.094 (95.676)
Epoch: [205][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2535 (0.3023) ([0.088]+[0.166])	Prec@1 97.656 (95.336)
Epoch: [205][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4377 (0.3081) ([0.272]+[0.166])	Prec@1 91.406 (95.120)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3909 (0.3909) ([0.225]+[0.166])	Prec@1 95.312 (95.312)
 * Prec@1 88.960
current lr 1.00000e-02
Grad=  tensor(7.9037, device='cuda:0')
Epoch: [206][0/391]	Time 0.299 (0.299)	Data 0.180 (0.180)	Loss 0.2797 (0.2797) ([0.114]+[0.166])	Prec@1 96.094 (96.094)
Epoch: [206][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.3178 (0.3031) ([0.152]+[0.166])	Prec@1 93.750 (95.467)
Epoch: [206][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.3317 (0.3127) ([0.166]+[0.166])	Prec@1 96.875 (95.060)
Epoch: [206][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.3497 (0.3165) ([0.184]+[0.166])	Prec@1 93.750 (94.882)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.6606 (0.6606) ([0.495]+[0.166])	Prec@1 89.062 (89.062)
 * Prec@1 86.980
current lr 1.00000e-02
Grad=  tensor(10.5394, device='cuda:0')
Epoch: [207][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.3421 (0.3421) ([0.176]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [207][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3842 (0.3093) ([0.218]+[0.166])	Prec@1 93.750 (95.026)
Epoch: [207][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3304 (0.3095) ([0.165]+[0.166])	Prec@1 93.750 (95.118)
Epoch: [207][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2930 (0.3082) ([0.127]+[0.166])	Prec@1 95.312 (95.139)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.4506 (0.4506) ([0.285]+[0.166])	Prec@1 92.969 (92.969)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(5.1901, device='cuda:0')
Epoch: [208][0/391]	Time 0.311 (0.311)	Data 0.190 (0.190)	Loss 0.2402 (0.2402) ([0.074]+[0.166])	Prec@1 98.438 (98.438)
Epoch: [208][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3118 (0.3072) ([0.146]+[0.166])	Prec@1 95.312 (95.266)
Epoch: [208][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3837 (0.3159) ([0.218]+[0.166])	Prec@1 90.625 (94.831)
Epoch: [208][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2525 (0.3123) ([0.087]+[0.166])	Prec@1 98.438 (94.962)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.5071 (0.5071) ([0.341]+[0.166])	Prec@1 91.406 (91.406)
 * Prec@1 89.330
current lr 1.00000e-02
Grad=  tensor(11.8088, device='cuda:0')
Epoch: [209][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.3583 (0.3583) ([0.192]+[0.166])	Prec@1 92.969 (92.969)
Epoch: [209][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.2736 (0.3085) ([0.108]+[0.166])	Prec@1 93.750 (95.034)
Epoch: [209][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2963 (0.3098) ([0.130]+[0.166])	Prec@1 96.094 (94.982)
Epoch: [209][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3059 (0.3156) ([0.140]+[0.166])	Prec@1 96.094 (94.762)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4890 (0.4890) ([0.323]+[0.166])	Prec@1 89.062 (89.062)
 * Prec@1 89.620
current lr 1.00000e-02
Grad=  tensor(12.0357, device='cuda:0')
Epoch: [210][0/391]	Time 0.306 (0.306)	Data 0.185 (0.185)	Loss 0.3351 (0.3351) ([0.169]+[0.166])	Prec@1 93.750 (93.750)
Epoch: [210][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2623 (0.3037) ([0.096]+[0.166])	Prec@1 96.094 (95.204)
Epoch: [210][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3801 (0.3117) ([0.214]+[0.166])	Prec@1 93.750 (95.103)
Epoch: [210][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2939 (0.3131) ([0.128]+[0.166])	Prec@1 96.875 (95.048)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4541 (0.4541) ([0.288]+[0.166])	Prec@1 91.406 (91.406)
 * Prec@1 88.120
current lr 1.00000e-02
Grad=  tensor(13.8598, device='cuda:0')
Epoch: [211][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.3664 (0.3664) ([0.200]+[0.166])	Prec@1 95.312 (95.312)
Epoch: [211][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2656 (0.3118) ([0.099]+[0.166])	Prec@1 96.875 (95.135)
Epoch: [211][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4295 (0.3091) ([0.263]+[0.166])	Prec@1 91.406 (95.200)
Epoch: [211][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2971 (0.3123) ([0.131]+[0.166])	Prec@1 95.312 (95.045)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4768 (0.4768) ([0.311]+[0.166])	Prec@1 93.750 (93.750)
 * Prec@1 89.440
current lr 1.00000e-02
Grad=  tensor(11.0191, device='cuda:0')
Epoch: [212][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.3254 (0.3254) ([0.159]+[0.166])	Prec@1 93.750 (93.750)
Epoch: [212][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3551 (0.3014) ([0.189]+[0.166])	Prec@1 95.312 (95.475)
Epoch: [212][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2971 (0.3130) ([0.131]+[0.166])	Prec@1 94.531 (94.842)
Epoch: [212][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2658 (0.3172) ([0.100]+[0.166])	Prec@1 98.438 (94.700)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4679 (0.4679) ([0.302]+[0.166])	Prec@1 92.188 (92.188)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(3.3412, device='cuda:0')
Epoch: [213][0/391]	Time 0.303 (0.303)	Data 0.183 (0.183)	Loss 0.2631 (0.2631) ([0.097]+[0.166])	Prec@1 98.438 (98.438)
Epoch: [213][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3403 (0.2939) ([0.174]+[0.166])	Prec@1 92.969 (95.630)
Epoch: [213][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.4602 (0.3068) ([0.294]+[0.166])	Prec@1 90.625 (95.227)
Epoch: [213][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3678 (0.3112) ([0.202]+[0.166])	Prec@1 93.750 (95.014)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.6828 (0.6828) ([0.516]+[0.166])	Prec@1 87.500 (87.500)
 * Prec@1 87.570
current lr 1.00000e-02
Grad=  tensor(7.8828, device='cuda:0')
Epoch: [214][0/391]	Time 0.313 (0.313)	Data 0.188 (0.188)	Loss 0.3316 (0.3316) ([0.165]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [214][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3199 (0.2924) ([0.154]+[0.166])	Prec@1 96.094 (95.777)
Epoch: [214][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2733 (0.2986) ([0.107]+[0.166])	Prec@1 96.094 (95.433)
Epoch: [214][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3266 (0.3063) ([0.160]+[0.166])	Prec@1 95.312 (95.152)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.5169 (0.5169) ([0.351]+[0.166])	Prec@1 87.500 (87.500)
 * Prec@1 89.600
current lr 1.00000e-02
Grad=  tensor(7.2850, device='cuda:0')
Epoch: [215][0/391]	Time 0.307 (0.307)	Data 0.186 (0.186)	Loss 0.3217 (0.3217) ([0.155]+[0.166])	Prec@1 96.094 (96.094)
Epoch: [215][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3683 (0.3193) ([0.202]+[0.166])	Prec@1 93.750 (94.709)
Epoch: [215][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3527 (0.3139) ([0.186]+[0.166])	Prec@1 92.188 (94.862)
Epoch: [215][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4166 (0.3177) ([0.250]+[0.167])	Prec@1 92.188 (94.713)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3779 (0.3779) ([0.211]+[0.166])	Prec@1 92.969 (92.969)
 * Prec@1 89.300
current lr 1.00000e-02
Grad=  tensor(8.0363, device='cuda:0')
Epoch: [216][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.2964 (0.2964) ([0.130]+[0.166])	Prec@1 95.312 (95.312)
Epoch: [216][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3284 (0.2980) ([0.162]+[0.166])	Prec@1 93.750 (95.320)
Epoch: [216][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2536 (0.3001) ([0.087]+[0.166])	Prec@1 97.656 (95.258)
Epoch: [216][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3388 (0.3034) ([0.172]+[0.166])	Prec@1 92.969 (95.159)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.5318 (0.5318) ([0.365]+[0.166])	Prec@1 89.844 (89.844)
 * Prec@1 88.740
current lr 1.00000e-02
Grad=  tensor(2.9824, device='cuda:0')
Epoch: [217][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.2161 (0.2161) ([0.050]+[0.166])	Prec@1 99.219 (99.219)
Epoch: [217][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2809 (0.3104) ([0.115]+[0.166])	Prec@1 96.875 (95.104)
Epoch: [217][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2862 (0.3087) ([0.120]+[0.166])	Prec@1 96.875 (95.099)
Epoch: [217][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2996 (0.3086) ([0.133]+[0.166])	Prec@1 93.750 (95.136)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.5026 (0.5026) ([0.336]+[0.166])	Prec@1 88.281 (88.281)
 * Prec@1 89.540
current lr 1.00000e-02
Grad=  tensor(6.5017, device='cuda:0')
Epoch: [218][0/391]	Time 0.316 (0.316)	Data 0.198 (0.198)	Loss 0.3214 (0.3214) ([0.155]+[0.166])	Prec@1 94.531 (94.531)
Epoch: [218][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.2814 (0.2969) ([0.115]+[0.166])	Prec@1 96.875 (95.575)
Epoch: [218][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2659 (0.3029) ([0.100]+[0.166])	Prec@1 96.875 (95.301)
Epoch: [218][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3201 (0.3090) ([0.154]+[0.166])	Prec@1 95.312 (95.084)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.3651 (0.3651) ([0.199]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 88.490
current lr 1.00000e-02
Grad=  tensor(5.5638, device='cuda:0')
Epoch: [219][0/391]	Time 0.319 (0.319)	Data 0.200 (0.200)	Loss 0.2748 (0.2748) ([0.108]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [219][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3735 (0.2979) ([0.207]+[0.167])	Prec@1 92.188 (95.436)
Epoch: [219][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.3618 (0.3036) ([0.195]+[0.166])	Prec@1 92.969 (95.320)
Epoch: [219][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2569 (0.3068) ([0.090]+[0.166])	Prec@1 96.875 (95.178)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.5007 (0.5007) ([0.334]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 88.230
current lr 1.00000e-02
Grad=  tensor(9.3088, device='cuda:0')
Epoch: [220][0/391]	Time 0.312 (0.312)	Data 0.194 (0.194)	Loss 0.3413 (0.3413) ([0.175]+[0.167])	Prec@1 92.969 (92.969)
Epoch: [220][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3555 (0.2932) ([0.189]+[0.166])	Prec@1 93.750 (95.599)
Epoch: [220][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3154 (0.3021) ([0.149]+[0.166])	Prec@1 96.094 (95.285)
Epoch: [220][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3732 (0.3079) ([0.207]+[0.167])	Prec@1 92.969 (95.071)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.5951 (0.5951) ([0.429]+[0.167])	Prec@1 84.375 (84.375)
 * Prec@1 85.860
current lr 1.00000e-02
Grad=  tensor(6.2621, device='cuda:0')
Epoch: [221][0/391]	Time 0.308 (0.308)	Data 0.190 (0.190)	Loss 0.3083 (0.3083) ([0.142]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [221][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2476 (0.3008) ([0.081]+[0.166])	Prec@1 98.438 (95.475)
Epoch: [221][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3693 (0.2964) ([0.203]+[0.166])	Prec@1 93.750 (95.573)
Epoch: [221][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3374 (0.3001) ([0.171]+[0.166])	Prec@1 94.531 (95.463)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4195 (0.4195) ([0.253]+[0.166])	Prec@1 92.188 (92.188)
 * Prec@1 88.780
current lr 1.00000e-02
Grad=  tensor(5.9633, device='cuda:0')
Epoch: [222][0/391]	Time 0.302 (0.302)	Data 0.184 (0.184)	Loss 0.2778 (0.2778) ([0.111]+[0.166])	Prec@1 96.094 (96.094)
Epoch: [222][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2753 (0.2980) ([0.109]+[0.166])	Prec@1 96.094 (95.537)
Epoch: [222][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2630 (0.3034) ([0.097]+[0.166])	Prec@1 95.312 (95.254)
Epoch: [222][300/391]	Time 0.145 (0.109)	Data 0.000 (0.001)	Loss 0.4206 (0.3045) ([0.254]+[0.166])	Prec@1 92.188 (95.162)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5807 (0.5807) ([0.414]+[0.167])	Prec@1 90.625 (90.625)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(10.1386, device='cuda:0')
Epoch: [223][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.3349 (0.3349) ([0.168]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [223][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3401 (0.3081) ([0.173]+[0.167])	Prec@1 94.531 (95.073)
Epoch: [223][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3219 (0.3046) ([0.155]+[0.167])	Prec@1 94.531 (95.246)
Epoch: [223][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2120 (0.3094) ([0.045]+[0.167])	Prec@1 99.219 (95.152)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4796 (0.4796) ([0.313]+[0.167])	Prec@1 88.281 (88.281)
 * Prec@1 89.620
current lr 1.00000e-02
Grad=  tensor(11.0980, device='cuda:0')
Epoch: [224][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.3647 (0.3647) ([0.198]+[0.167])	Prec@1 92.969 (92.969)
Epoch: [224][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3157 (0.2955) ([0.149]+[0.167])	Prec@1 95.312 (95.730)
Epoch: [224][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3401 (0.3024) ([0.173]+[0.167])	Prec@1 92.188 (95.328)
Epoch: [224][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2838 (0.3054) ([0.117]+[0.167])	Prec@1 96.094 (95.209)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.4645 (0.4645) ([0.298]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 88.850
current lr 1.00000e-02
Grad=  tensor(6.3459, device='cuda:0')
Epoch: [225][0/391]	Time 0.301 (0.301)	Data 0.183 (0.183)	Loss 0.3320 (0.3320) ([0.165]+[0.167])	Prec@1 97.656 (97.656)
Epoch: [225][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2865 (0.2934) ([0.120]+[0.167])	Prec@1 95.312 (95.413)
Epoch: [225][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3039 (0.2997) ([0.137]+[0.167])	Prec@1 93.750 (95.274)
Epoch: [225][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3390 (0.3041) ([0.172]+[0.167])	Prec@1 95.312 (95.198)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.5059 (0.5059) ([0.339]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 88.880
current lr 1.00000e-02
Grad=  tensor(9.5325, device='cuda:0')
Epoch: [226][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.2973 (0.2973) ([0.131]+[0.167])	Prec@1 95.312 (95.312)
Epoch: [226][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3972 (0.2997) ([0.231]+[0.166])	Prec@1 91.406 (95.405)
Epoch: [226][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3219 (0.3024) ([0.155]+[0.167])	Prec@1 94.531 (95.351)
Epoch: [226][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2654 (0.3059) ([0.099]+[0.167])	Prec@1 97.656 (95.193)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3882 (0.3882) ([0.221]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(8.5168, device='cuda:0')
Epoch: [227][0/391]	Time 0.303 (0.303)	Data 0.185 (0.185)	Loss 0.2983 (0.2983) ([0.132]+[0.167])	Prec@1 95.312 (95.312)
Epoch: [227][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.4145 (0.3025) ([0.248]+[0.167])	Prec@1 91.406 (95.336)
Epoch: [227][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3401 (0.3071) ([0.173]+[0.167])	Prec@1 95.312 (95.157)
Epoch: [227][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2804 (0.3091) ([0.114]+[0.167])	Prec@1 96.875 (95.162)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4361 (0.4361) ([0.269]+[0.167])	Prec@1 90.625 (90.625)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(5.6962, device='cuda:0')
Epoch: [228][0/391]	Time 0.301 (0.301)	Data 0.184 (0.184)	Loss 0.2418 (0.2418) ([0.075]+[0.167])	Prec@1 97.656 (97.656)
Epoch: [228][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2993 (0.3089) ([0.132]+[0.167])	Prec@1 95.312 (95.026)
Epoch: [228][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2799 (0.3109) ([0.113]+[0.167])	Prec@1 96.094 (94.986)
Epoch: [228][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3040 (0.3102) ([0.137]+[0.167])	Prec@1 96.094 (95.024)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5535 (0.5535) ([0.386]+[0.167])	Prec@1 87.500 (87.500)
 * Prec@1 87.760
current lr 1.00000e-02
Grad=  tensor(7.7690, device='cuda:0')
Epoch: [229][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.3020 (0.3020) ([0.135]+[0.167])	Prec@1 94.531 (94.531)
Epoch: [229][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3682 (0.3103) ([0.201]+[0.167])	Prec@1 93.750 (95.011)
Epoch: [229][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3909 (0.3141) ([0.224]+[0.167])	Prec@1 91.406 (94.924)
Epoch: [229][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3065 (0.3135) ([0.139]+[0.167])	Prec@1 95.312 (94.895)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4252 (0.4252) ([0.258]+[0.167])	Prec@1 92.188 (92.188)
 * Prec@1 88.610
current lr 1.00000e-02
Grad=  tensor(5.4126, device='cuda:0')
Epoch: [230][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.2678 (0.2678) ([0.100]+[0.167])	Prec@1 95.312 (95.312)
Epoch: [230][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2702 (0.3084) ([0.103]+[0.167])	Prec@1 96.875 (95.204)
Epoch: [230][200/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.2724 (0.3065) ([0.105]+[0.167])	Prec@1 96.094 (95.223)
Epoch: [230][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.2667 (0.3091) ([0.099]+[0.167])	Prec@1 97.656 (95.105)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5093 (0.5093) ([0.342]+[0.167])	Prec@1 89.062 (89.062)
 * Prec@1 88.770
current lr 1.00000e-02
Grad=  tensor(3.2847, device='cuda:0')
Epoch: [231][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.2458 (0.2458) ([0.078]+[0.167])	Prec@1 98.438 (98.438)
Epoch: [231][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2750 (0.2953) ([0.108]+[0.167])	Prec@1 95.312 (95.614)
Epoch: [231][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2905 (0.2972) ([0.124]+[0.167])	Prec@1 94.531 (95.577)
Epoch: [231][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3580 (0.2971) ([0.191]+[0.167])	Prec@1 92.188 (95.559)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.5448 (0.5448) ([0.378]+[0.167])	Prec@1 88.281 (88.281)
 * Prec@1 88.190
current lr 1.00000e-02
Grad=  tensor(7.3468, device='cuda:0')
Epoch: [232][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.2820 (0.2820) ([0.115]+[0.167])	Prec@1 96.875 (96.875)
Epoch: [232][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2845 (0.2947) ([0.118]+[0.167])	Prec@1 95.312 (95.560)
Epoch: [232][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3116 (0.2958) ([0.145]+[0.167])	Prec@1 94.531 (95.585)
Epoch: [232][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2559 (0.2995) ([0.089]+[0.167])	Prec@1 98.438 (95.403)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.6305 (0.6305) ([0.463]+[0.167])	Prec@1 84.375 (84.375)
 * Prec@1 87.550
current lr 1.00000e-02
Grad=  tensor(6.2691, device='cuda:0')
Epoch: [233][0/391]	Time 0.301 (0.301)	Data 0.181 (0.181)	Loss 0.2660 (0.2660) ([0.099]+[0.167])	Prec@1 98.438 (98.438)
Epoch: [233][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.4268 (0.3089) ([0.260]+[0.167])	Prec@1 89.844 (95.050)
Epoch: [233][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3066 (0.3084) ([0.139]+[0.167])	Prec@1 94.531 (95.075)
Epoch: [233][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3449 (0.3123) ([0.178]+[0.167])	Prec@1 93.750 (94.882)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4786 (0.4786) ([0.311]+[0.168])	Prec@1 89.844 (89.844)
 * Prec@1 89.410
current lr 1.00000e-02
Grad=  tensor(10.2247, device='cuda:0')
Epoch: [234][0/391]	Time 0.306 (0.306)	Data 0.185 (0.185)	Loss 0.3528 (0.3528) ([0.185]+[0.168])	Prec@1 95.312 (95.312)
Epoch: [234][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3175 (0.2911) ([0.150]+[0.167])	Prec@1 92.969 (95.738)
Epoch: [234][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3006 (0.2958) ([0.133]+[0.167])	Prec@1 95.312 (95.588)
Epoch: [234][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3549 (0.3014) ([0.188]+[0.167])	Prec@1 94.531 (95.375)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.4198 (0.4198) ([0.252]+[0.167])	Prec@1 89.062 (89.062)
 * Prec@1 88.900
current lr 1.00000e-02
Grad=  tensor(9.5961, device='cuda:0')
Epoch: [235][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.3111 (0.3111) ([0.144]+[0.167])	Prec@1 93.750 (93.750)
Epoch: [235][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3449 (0.2916) ([0.178]+[0.167])	Prec@1 94.531 (95.715)
Epoch: [235][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3404 (0.3004) ([0.173]+[0.167])	Prec@1 93.750 (95.390)
Epoch: [235][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3234 (0.3036) ([0.156]+[0.167])	Prec@1 94.531 (95.333)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4455 (0.4455) ([0.278]+[0.167])	Prec@1 90.625 (90.625)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(12.8965, device='cuda:0')
Epoch: [236][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.3470 (0.3470) ([0.180]+[0.167])	Prec@1 92.969 (92.969)
Epoch: [236][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2801 (0.2865) ([0.113]+[0.167])	Prec@1 96.094 (96.001)
Epoch: [236][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2919 (0.2946) ([0.125]+[0.167])	Prec@1 96.875 (95.635)
Epoch: [236][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3168 (0.2977) ([0.150]+[0.167])	Prec@1 93.750 (95.523)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4320 (0.4320) ([0.265]+[0.167])	Prec@1 90.625 (90.625)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(8.0221, device='cuda:0')
Epoch: [237][0/391]	Time 0.298 (0.298)	Data 0.180 (0.180)	Loss 0.3444 (0.3444) ([0.177]+[0.167])	Prec@1 95.312 (95.312)
Epoch: [237][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.2549 (0.2954) ([0.088]+[0.167])	Prec@1 96.875 (95.653)
Epoch: [237][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3511 (0.3014) ([0.184]+[0.167])	Prec@1 93.750 (95.445)
Epoch: [237][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3089 (0.3021) ([0.142]+[0.167])	Prec@1 94.531 (95.383)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3587 (0.3587) ([0.192]+[0.167])	Prec@1 93.750 (93.750)
 * Prec@1 90.050
current lr 1.00000e-02
Grad=  tensor(10.7464, device='cuda:0')
Epoch: [238][0/391]	Time 0.302 (0.302)	Data 0.185 (0.185)	Loss 0.3182 (0.3182) ([0.151]+[0.167])	Prec@1 94.531 (94.531)
Epoch: [238][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2608 (0.2940) ([0.094]+[0.167])	Prec@1 96.094 (95.614)
Epoch: [238][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3285 (0.2947) ([0.162]+[0.167])	Prec@1 92.188 (95.534)
Epoch: [238][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.3738 (0.3032) ([0.207]+[0.167])	Prec@1 93.750 (95.302)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4161 (0.4161) ([0.249]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 89.020
current lr 1.00000e-02
Grad=  tensor(6.5306, device='cuda:0')
Epoch: [239][0/391]	Time 0.303 (0.303)	Data 0.185 (0.185)	Loss 0.2650 (0.2650) ([0.098]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [239][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3292 (0.3134) ([0.162]+[0.167])	Prec@1 94.531 (95.042)
Epoch: [239][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2748 (0.3115) ([0.107]+[0.168])	Prec@1 96.094 (95.103)
Epoch: [239][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3121 (0.3132) ([0.145]+[0.168])	Prec@1 95.312 (95.040)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5117 (0.5117) ([0.344]+[0.168])	Prec@1 89.062 (89.062)
 * Prec@1 89.740
current lr 1.00000e-02
Grad=  tensor(12.9709, device='cuda:0')
Epoch: [240][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.3739 (0.3739) ([0.206]+[0.168])	Prec@1 91.406 (91.406)
Epoch: [240][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3233 (0.2977) ([0.156]+[0.168])	Prec@1 92.969 (95.614)
Epoch: [240][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2985 (0.3012) ([0.131]+[0.167])	Prec@1 96.094 (95.429)
Epoch: [240][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3344 (0.3059) ([0.167]+[0.167])	Prec@1 94.531 (95.242)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.5071 (0.5071) ([0.340]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 88.720
current lr 1.00000e-02
Grad=  tensor(8.2079, device='cuda:0')
Epoch: [241][0/391]	Time 0.303 (0.303)	Data 0.185 (0.185)	Loss 0.2872 (0.2872) ([0.120]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [241][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.4359 (0.2973) ([0.268]+[0.167])	Prec@1 91.406 (95.622)
Epoch: [241][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2820 (0.3022) ([0.114]+[0.167])	Prec@1 97.656 (95.379)
Epoch: [241][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3731 (0.3022) ([0.206]+[0.167])	Prec@1 90.625 (95.375)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.5013 (0.5013) ([0.334]+[0.167])	Prec@1 89.062 (89.062)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(4.2233, device='cuda:0')
Epoch: [242][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.2788 (0.2788) ([0.111]+[0.167])	Prec@1 96.875 (96.875)
Epoch: [242][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.3872 (0.3025) ([0.220]+[0.167])	Prec@1 94.531 (95.429)
Epoch: [242][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2495 (0.3023) ([0.082]+[0.167])	Prec@1 98.438 (95.363)
Epoch: [242][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3262 (0.3039) ([0.159]+[0.167])	Prec@1 96.094 (95.325)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4649 (0.4649) ([0.298]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 88.960
current lr 1.00000e-02
Grad=  tensor(9.1305, device='cuda:0')
Epoch: [243][0/391]	Time 0.305 (0.305)	Data 0.187 (0.187)	Loss 0.3586 (0.3586) ([0.191]+[0.167])	Prec@1 93.750 (93.750)
Epoch: [243][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2995 (0.3127) ([0.132]+[0.167])	Prec@1 95.312 (95.227)
Epoch: [243][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2703 (0.3144) ([0.103]+[0.167])	Prec@1 96.094 (95.009)
Epoch: [243][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2970 (0.3125) ([0.130]+[0.167])	Prec@1 96.094 (95.043)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.5003 (0.5003) ([0.333]+[0.167])	Prec@1 92.188 (92.188)
 * Prec@1 88.840
current lr 1.00000e-02
Grad=  tensor(7.3972, device='cuda:0')
Epoch: [244][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.3055 (0.3055) ([0.138]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [244][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2872 (0.3016) ([0.120]+[0.167])	Prec@1 95.312 (95.475)
Epoch: [244][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.2164 (0.3008) ([0.049]+[0.167])	Prec@1 98.438 (95.499)
Epoch: [244][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.2798 (0.3024) ([0.113]+[0.167])	Prec@1 95.312 (95.398)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.4697 (0.4697) ([0.302]+[0.167])	Prec@1 87.500 (87.500)
 * Prec@1 88.800
current lr 1.00000e-02
Grad=  tensor(6.8404, device='cuda:0')
Epoch: [245][0/391]	Time 0.316 (0.316)	Data 0.198 (0.198)	Loss 0.2681 (0.2681) ([0.101]+[0.167])	Prec@1 95.312 (95.312)
Epoch: [245][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.2442 (0.2961) ([0.077]+[0.167])	Prec@1 96.875 (95.684)
Epoch: [245][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3374 (0.2944) ([0.170]+[0.167])	Prec@1 94.531 (95.581)
Epoch: [245][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2594 (0.2948) ([0.092]+[0.167])	Prec@1 97.656 (95.538)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.5800 (0.5800) ([0.413]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 88.720
current lr 1.00000e-02
Grad=  tensor(8.6426, device='cuda:0')
Epoch: [246][0/391]	Time 0.311 (0.311)	Data 0.194 (0.194)	Loss 0.2760 (0.2760) ([0.109]+[0.167])	Prec@1 96.875 (96.875)
Epoch: [246][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2889 (0.3012) ([0.122]+[0.167])	Prec@1 93.750 (95.374)
Epoch: [246][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3042 (0.3039) ([0.137]+[0.167])	Prec@1 93.750 (95.266)
Epoch: [246][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3321 (0.3099) ([0.165]+[0.167])	Prec@1 95.312 (95.076)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4946 (0.4946) ([0.327]+[0.167])	Prec@1 91.406 (91.406)
 * Prec@1 89.490
current lr 1.00000e-02
Grad=  tensor(7.2486, device='cuda:0')
Epoch: [247][0/391]	Time 0.304 (0.304)	Data 0.186 (0.186)	Loss 0.2824 (0.2824) ([0.115]+[0.167])	Prec@1 96.875 (96.875)
Epoch: [247][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2631 (0.2942) ([0.096]+[0.167])	Prec@1 96.875 (95.653)
Epoch: [247][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2658 (0.3004) ([0.098]+[0.167])	Prec@1 96.094 (95.511)
Epoch: [247][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.3334 (0.2993) ([0.166]+[0.167])	Prec@1 94.531 (95.523)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.5403 (0.5403) ([0.373]+[0.167])	Prec@1 89.062 (89.062)
 * Prec@1 89.400
current lr 1.00000e-02
Grad=  tensor(13.9449, device='cuda:0')
Epoch: [248][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.4073 (0.4073) ([0.240]+[0.167])	Prec@1 92.188 (92.188)
Epoch: [248][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.2877 (0.2919) ([0.120]+[0.167])	Prec@1 95.312 (95.738)
Epoch: [248][200/391]	Time 0.109 (0.108)	Data 0.000 (0.001)	Loss 0.2503 (0.2930) ([0.083]+[0.167])	Prec@1 96.875 (95.623)
Epoch: [248][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.3125 (0.2983) ([0.145]+[0.167])	Prec@1 95.312 (95.427)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.5873 (0.5873) ([0.420]+[0.167])	Prec@1 85.938 (85.938)
 * Prec@1 87.960
current lr 1.00000e-02
Grad=  tensor(16.3851, device='cuda:0')
Epoch: [249][0/391]	Time 0.306 (0.306)	Data 0.182 (0.182)	Loss 0.3991 (0.3991) ([0.232]+[0.167])	Prec@1 92.969 (92.969)
Epoch: [249][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3335 (0.3075) ([0.166]+[0.168])	Prec@1 94.531 (95.065)
Epoch: [249][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2545 (0.3120) ([0.087]+[0.168])	Prec@1 96.094 (94.939)
Epoch: [249][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2548 (0.3087) ([0.087]+[0.167])	Prec@1 98.438 (95.074)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.6122 (0.6122) ([0.445]+[0.167])	Prec@1 88.281 (88.281)
 * Prec@1 89.000
current lr 1.00000e-03
Grad=  tensor(7.3061, device='cuda:0')
Epoch: [250][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.2943 (0.2943) ([0.127]+[0.167])	Prec@1 96.094 (96.094)
Epoch: [250][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2284 (0.2649) ([0.064]+[0.165])	Prec@1 99.219 (96.689)
Epoch: [250][200/391]	Time 0.107 (0.110)	Data 0.000 (0.001)	Loss 0.2620 (0.2516) ([0.098]+[0.164])	Prec@1 96.875 (97.225)
Epoch: [250][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2472 (0.2450) ([0.083]+[0.164])	Prec@1 97.656 (97.433)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3366 (0.3366) ([0.173]+[0.164])	Prec@1 94.531 (94.531)
 * Prec@1 92.570
current lr 1.00000e-03
Grad=  tensor(4.2378, device='cuda:0')
Epoch: [251][0/391]	Time 0.312 (0.312)	Data 0.194 (0.194)	Loss 0.2245 (0.2245) ([0.061]+[0.164])	Prec@1 98.438 (98.438)
Epoch: [251][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.2011 (0.2160) ([0.037]+[0.164])	Prec@1 100.000 (98.523)
Epoch: [251][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1819 (0.2141) ([0.018]+[0.164])	Prec@1 100.000 (98.558)
Epoch: [251][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2338 (0.2154) ([0.070]+[0.163])	Prec@1 98.438 (98.502)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3453 (0.3453) ([0.182]+[0.163])	Prec@1 94.531 (94.531)
 * Prec@1 92.690
current lr 1.00000e-03
Grad=  tensor(0.9704, device='cuda:0')
Epoch: [252][0/391]	Time 0.302 (0.302)	Data 0.185 (0.185)	Loss 0.1898 (0.1898) ([0.027]+[0.163])	Prec@1 99.219 (99.219)
Epoch: [252][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2096 (0.2078) ([0.047]+[0.163])	Prec@1 97.656 (98.801)
Epoch: [252][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1952 (0.2074) ([0.032]+[0.163])	Prec@1 99.219 (98.799)
Epoch: [252][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1936 (0.2077) ([0.031]+[0.163])	Prec@1 99.219 (98.778)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3450 (0.3450) ([0.182]+[0.163])	Prec@1 92.188 (92.188)
 * Prec@1 92.790
current lr 1.00000e-03
Grad=  tensor(1.4118, device='cuda:0')
Epoch: [253][0/391]	Time 0.304 (0.304)	Data 0.187 (0.187)	Loss 0.1928 (0.1928) ([0.030]+[0.163])	Prec@1 100.000 (100.000)
Epoch: [253][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1900 (0.2022) ([0.028]+[0.162])	Prec@1 98.438 (98.909)
Epoch: [253][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2052 (0.2013) ([0.043]+[0.162])	Prec@1 98.438 (98.958)
Epoch: [253][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1836 (0.2010) ([0.021]+[0.162])	Prec@1 100.000 (98.970)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3398 (0.3398) ([0.178]+[0.162])	Prec@1 93.750 (93.750)
 * Prec@1 93.070
current lr 1.00000e-03
Grad=  tensor(1.6741, device='cuda:0')
Epoch: [254][0/391]	Time 0.308 (0.308)	Data 0.190 (0.190)	Loss 0.1840 (0.1840) ([0.022]+[0.162])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.1857 (0.1984) ([0.024]+[0.162])	Prec@1 99.219 (99.095)
Epoch: [254][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2022 (0.1975) ([0.041]+[0.162])	Prec@1 99.219 (99.110)
Epoch: [254][300/391]	Time 0.109 (0.108)	Data 0.000 (0.001)	Loss 0.2235 (0.1982) ([0.062]+[0.162])	Prec@1 96.875 (99.089)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3353 (0.3353) ([0.174]+[0.161])	Prec@1 92.969 (92.969)
 * Prec@1 92.930
current lr 1.00000e-03
Grad=  tensor(0.7339, device='cuda:0')
Epoch: [255][0/391]	Time 0.299 (0.299)	Data 0.181 (0.181)	Loss 0.1804 (0.1804) ([0.019]+[0.161])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1757 (0.1929) ([0.014]+[0.161])	Prec@1 100.000 (99.157)
Epoch: [255][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1873 (0.1939) ([0.026]+[0.161])	Prec@1 99.219 (99.133)
Epoch: [255][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1777 (0.1939) ([0.017]+[0.161])	Prec@1 100.000 (99.136)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3262 (0.3262) ([0.165]+[0.161])	Prec@1 93.750 (93.750)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(2.9666, device='cuda:0')
Epoch: [256][0/391]	Time 0.305 (0.305)	Data 0.186 (0.186)	Loss 0.1951 (0.1951) ([0.034]+[0.161])	Prec@1 97.656 (97.656)
Epoch: [256][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1768 (0.1943) ([0.016]+[0.161])	Prec@1 100.000 (99.018)
Epoch: [256][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1912 (0.1929) ([0.031]+[0.161])	Prec@1 99.219 (99.075)
Epoch: [256][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1835 (0.1928) ([0.023]+[0.160])	Prec@1 100.000 (99.089)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3101 (0.3101) ([0.150]+[0.160])	Prec@1 94.531 (94.531)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(3.9257, device='cuda:0')
Epoch: [257][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.2016 (0.2016) ([0.041]+[0.160])	Prec@1 98.438 (98.438)
Epoch: [257][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1792 (0.1910) ([0.019]+[0.160])	Prec@1 100.000 (99.226)
Epoch: [257][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2076 (0.1895) ([0.048]+[0.160])	Prec@1 97.656 (99.219)
Epoch: [257][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1910 (0.1897) ([0.031]+[0.160])	Prec@1 100.000 (99.252)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.3424 (0.3424) ([0.183]+[0.160])	Prec@1 94.531 (94.531)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(3.0328, device='cuda:0')
Epoch: [258][0/391]	Time 0.297 (0.297)	Data 0.179 (0.179)	Loss 0.1867 (0.1867) ([0.027]+[0.160])	Prec@1 99.219 (99.219)
Epoch: [258][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.1896 (0.1853) ([0.030]+[0.160])	Prec@1 99.219 (99.482)
Epoch: [258][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2010 (0.1864) ([0.041]+[0.159])	Prec@1 99.219 (99.374)
Epoch: [258][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1844 (0.1855) ([0.025]+[0.159])	Prec@1 99.219 (99.406)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3428 (0.3428) ([0.184]+[0.159])	Prec@1 95.312 (95.312)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(1.7870, device='cuda:0')
Epoch: [259][0/391]	Time 0.297 (0.297)	Data 0.180 (0.180)	Loss 0.1811 (0.1811) ([0.022]+[0.159])	Prec@1 99.219 (99.219)
Epoch: [259][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2016 (0.1871) ([0.043]+[0.159])	Prec@1 97.656 (99.257)
Epoch: [259][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.1819 (0.1860) ([0.023]+[0.159])	Prec@1 100.000 (99.324)
Epoch: [259][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1759 (0.1846) ([0.017]+[0.159])	Prec@1 100.000 (99.390)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.3390 (0.3390) ([0.180]+[0.159])	Prec@1 92.969 (92.969)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(1.4531, device='cuda:0')
Epoch: [260][0/391]	Time 0.302 (0.302)	Data 0.184 (0.184)	Loss 0.1767 (0.1767) ([0.018]+[0.159])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.1883 (0.1829) ([0.030]+[0.159])	Prec@1 100.000 (99.459)
Epoch: [260][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.1763 (0.1829) ([0.018]+[0.158])	Prec@1 100.000 (99.456)
Epoch: [260][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1721 (0.1832) ([0.014]+[0.158])	Prec@1 100.000 (99.424)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3278 (0.3278) ([0.170]+[0.158])	Prec@1 94.531 (94.531)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.4494, device='cuda:0')
Epoch: [261][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 0.1690 (0.1690) ([0.011]+[0.158])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.1971 (0.1828) ([0.039]+[0.158])	Prec@1 98.438 (99.327)
Epoch: [261][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1841 (0.1822) ([0.026]+[0.158])	Prec@1 100.000 (99.390)
Epoch: [261][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.2026 (0.1825) ([0.045]+[0.158])	Prec@1 99.219 (99.380)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.3344 (0.3344) ([0.177]+[0.158])	Prec@1 95.312 (95.312)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(0.4178, device='cuda:0')
Epoch: [262][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.1659 (0.1659) ([0.008]+[0.158])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1806 (0.1796) ([0.023]+[0.157])	Prec@1 99.219 (99.520)
Epoch: [262][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1823 (0.1802) ([0.025]+[0.157])	Prec@1 100.000 (99.495)
Epoch: [262][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1983 (0.1802) ([0.041]+[0.157])	Prec@1 99.219 (99.471)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3615 (0.3615) ([0.204]+[0.157])	Prec@1 95.312 (95.312)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(2.6726, device='cuda:0')
Epoch: [263][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.1878 (0.1878) ([0.031]+[0.157])	Prec@1 98.438 (98.438)
Epoch: [263][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.1731 (0.1792) ([0.016]+[0.157])	Prec@1 99.219 (99.482)
Epoch: [263][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1898 (0.1783) ([0.033]+[0.157])	Prec@1 98.438 (99.502)
Epoch: [263][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1840 (0.1780) ([0.027]+[0.157])	Prec@1 100.000 (99.509)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.3433 (0.3433) ([0.187]+[0.157])	Prec@1 94.531 (94.531)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(1.7721, device='cuda:0')
Epoch: [264][0/391]	Time 0.298 (0.298)	Data 0.179 (0.179)	Loss 0.1769 (0.1769) ([0.020]+[0.157])	Prec@1 99.219 (99.219)
Epoch: [264][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1669 (0.1783) ([0.010]+[0.156])	Prec@1 99.219 (99.505)
Epoch: [264][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1735 (0.1781) ([0.017]+[0.156])	Prec@1 100.000 (99.502)
Epoch: [264][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.1951 (0.1780) ([0.039]+[0.156])	Prec@1 98.438 (99.522)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3645 (0.3645) ([0.208]+[0.156])	Prec@1 93.750 (93.750)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.7074, device='cuda:0')
Epoch: [265][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.1698 (0.1698) ([0.014]+[0.156])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.1664 (0.1765) ([0.010]+[0.156])	Prec@1 100.000 (99.544)
Epoch: [265][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2078 (0.1765) ([0.052]+[0.156])	Prec@1 98.438 (99.549)
Epoch: [265][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1632 (0.1760) ([0.008]+[0.156])	Prec@1 100.000 (99.561)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.3359 (0.3359) ([0.180]+[0.156])	Prec@1 94.531 (94.531)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.5529, device='cuda:0')
Epoch: [266][0/391]	Time 0.303 (0.303)	Data 0.185 (0.185)	Loss 0.1652 (0.1652) ([0.010]+[0.156])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.108 (0.109)	Data 0.000 (0.002)	Loss 0.1684 (0.1770) ([0.013]+[0.155])	Prec@1 100.000 (99.520)
Epoch: [266][200/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1777 (0.1773) ([0.022]+[0.155])	Prec@1 99.219 (99.530)
Epoch: [266][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.1700 (0.1768) ([0.015]+[0.155])	Prec@1 100.000 (99.525)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.3519 (0.3519) ([0.197]+[0.155])	Prec@1 93.750 (93.750)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(1.0373, device='cuda:0')
Epoch: [267][0/391]	Time 0.305 (0.305)	Data 0.188 (0.188)	Loss 0.1781 (0.1781) ([0.023]+[0.155])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.109 (0.110)	Data 0.000 (0.002)	Loss 0.1791 (0.1724) ([0.024]+[0.155])	Prec@1 98.438 (99.667)
Epoch: [267][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1860 (0.1732) ([0.031]+[0.155])	Prec@1 99.219 (99.627)
Epoch: [267][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2003 (0.1740) ([0.046]+[0.155])	Prec@1 98.438 (99.593)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3406 (0.3406) ([0.186]+[0.155])	Prec@1 95.312 (95.312)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(2.4136, device='cuda:0')
Epoch: [268][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.1813 (0.1813) ([0.027]+[0.155])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1692 (0.1732) ([0.015]+[0.154])	Prec@1 100.000 (99.621)
Epoch: [268][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1667 (0.1734) ([0.012]+[0.154])	Prec@1 100.000 (99.619)
Epoch: [268][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1891 (0.1733) ([0.035]+[0.154])	Prec@1 99.219 (99.626)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.3504 (0.3504) ([0.196]+[0.154])	Prec@1 94.531 (94.531)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.7896, device='cuda:0')
Epoch: [269][0/391]	Time 0.312 (0.312)	Data 0.192 (0.192)	Loss 0.1673 (0.1673) ([0.013]+[0.154])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1696 (0.1729) ([0.016]+[0.154])	Prec@1 99.219 (99.606)
Epoch: [269][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1593 (0.1722) ([0.005]+[0.154])	Prec@1 100.000 (99.623)
Epoch: [269][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1821 (0.1724) ([0.028]+[0.154])	Prec@1 99.219 (99.608)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.3612 (0.3612) ([0.208]+[0.154])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(0.5993, device='cuda:0')
Epoch: [270][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.1644 (0.1644) ([0.011]+[0.154])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1884 (0.1715) ([0.035]+[0.153])	Prec@1 98.438 (99.660)
Epoch: [270][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1689 (0.1701) ([0.016]+[0.153])	Prec@1 99.219 (99.689)
Epoch: [270][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1725 (0.1707) ([0.019]+[0.153])	Prec@1 99.219 (99.642)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.3683 (0.3683) ([0.215]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(7.5838, device='cuda:0')
Epoch: [271][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.1979 (0.1979) ([0.045]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [271][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1684 (0.1705) ([0.015]+[0.153])	Prec@1 100.000 (99.582)
Epoch: [271][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1615 (0.1697) ([0.009]+[0.153])	Prec@1 100.000 (99.635)
Epoch: [271][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1624 (0.1701) ([0.010]+[0.153])	Prec@1 100.000 (99.637)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3828 (0.3828) ([0.230]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 93.340
current lr 1.00000e-03
Grad=  tensor(0.3290, device='cuda:0')
Epoch: [272][0/391]	Time 0.318 (0.318)	Data 0.190 (0.190)	Loss 0.1576 (0.1576) ([0.005]+[0.153])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1740 (0.1697) ([0.022]+[0.152])	Prec@1 99.219 (99.652)
Epoch: [272][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1638 (0.1694) ([0.011]+[0.152])	Prec@1 100.000 (99.662)
Epoch: [272][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1626 (0.1697) ([0.010]+[0.152])	Prec@1 100.000 (99.660)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.3510 (0.3510) ([0.199]+[0.152])	Prec@1 95.312 (95.312)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(2.1740, device='cuda:0')
Epoch: [273][0/391]	Time 0.304 (0.304)	Data 0.185 (0.185)	Loss 0.1649 (0.1649) ([0.013]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [273][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1752 (0.1684) ([0.023]+[0.152])	Prec@1 99.219 (99.621)
Epoch: [273][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1641 (0.1678) ([0.012]+[0.152])	Prec@1 100.000 (99.685)
Epoch: [273][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1805 (0.1680) ([0.029]+[0.152])	Prec@1 99.219 (99.673)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3608 (0.3608) ([0.209]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(3.3641, device='cuda:0')
Epoch: [274][0/391]	Time 0.312 (0.312)	Data 0.193 (0.193)	Loss 0.1942 (0.1942) ([0.043]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [274][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1796 (0.1672) ([0.028]+[0.151])	Prec@1 99.219 (99.675)
Epoch: [274][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1656 (0.1672) ([0.014]+[0.151])	Prec@1 100.000 (99.685)
Epoch: [274][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1641 (0.1674) ([0.013]+[0.151])	Prec@1 100.000 (99.696)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.3547 (0.3547) ([0.204]+[0.151])	Prec@1 95.312 (95.312)
 * Prec@1 93.510
current lr 1.00000e-03
Grad=  tensor(0.7373, device='cuda:0')
Epoch: [275][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.1638 (0.1638) ([0.013]+[0.151])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1721 (0.1669) ([0.021]+[0.151])	Prec@1 99.219 (99.683)
Epoch: [275][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1742 (0.1664) ([0.023]+[0.151])	Prec@1 99.219 (99.697)
Epoch: [275][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1583 (0.1663) ([0.008]+[0.151])	Prec@1 100.000 (99.694)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.3444 (0.3444) ([0.194]+[0.151])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(0.5331, device='cuda:0')
Epoch: [276][0/391]	Time 0.312 (0.312)	Data 0.192 (0.192)	Loss 0.1594 (0.1594) ([0.009]+[0.151])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.113 (0.112)	Data 0.000 (0.002)	Loss 0.1832 (0.1664) ([0.033]+[0.150])	Prec@1 97.656 (99.652)
Epoch: [276][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1635 (0.1656) ([0.013]+[0.150])	Prec@1 100.000 (99.697)
Epoch: [276][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1618 (0.1659) ([0.012]+[0.150])	Prec@1 100.000 (99.699)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.3431 (0.3431) ([0.193]+[0.150])	Prec@1 95.312 (95.312)
 * Prec@1 93.350
current lr 1.00000e-03
Grad=  tensor(2.7453, device='cuda:0')
Epoch: [277][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.1669 (0.1669) ([0.017]+[0.150])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1902 (0.1650) ([0.040]+[0.150])	Prec@1 98.438 (99.698)
Epoch: [277][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1562 (0.1649) ([0.006]+[0.150])	Prec@1 100.000 (99.701)
Epoch: [277][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1574 (0.1646) ([0.008]+[0.150])	Prec@1 100.000 (99.720)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3444 (0.3444) ([0.195]+[0.150])	Prec@1 95.312 (95.312)
 * Prec@1 93.410
current lr 1.00000e-03
Grad=  tensor(2.8284, device='cuda:0')
Epoch: [278][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.1724 (0.1724) ([0.023]+[0.150])	Prec@1 99.219 (99.219)
Epoch: [278][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1565 (0.1626) ([0.007]+[0.150])	Prec@1 100.000 (99.768)
Epoch: [278][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1554 (0.1637) ([0.006]+[0.149])	Prec@1 100.000 (99.740)
Epoch: [278][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1677 (0.1639) ([0.018]+[0.149])	Prec@1 100.000 (99.740)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3553 (0.3553) ([0.206]+[0.149])	Prec@1 94.531 (94.531)
 * Prec@1 93.300
current lr 1.00000e-03
Grad=  tensor(2.1040, device='cuda:0')
Epoch: [279][0/391]	Time 0.309 (0.309)	Data 0.190 (0.190)	Loss 0.1625 (0.1625) ([0.013]+[0.149])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1595 (0.1635) ([0.010]+[0.149])	Prec@1 100.000 (99.729)
Epoch: [279][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1674 (0.1635) ([0.019]+[0.149])	Prec@1 100.000 (99.751)
Epoch: [279][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1669 (0.1630) ([0.018]+[0.149])	Prec@1 100.000 (99.759)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3752 (0.3752) ([0.226]+[0.149])	Prec@1 95.312 (95.312)
 * Prec@1 93.400
current lr 1.00000e-03
Grad=  tensor(0.9214, device='cuda:0')
Epoch: [280][0/391]	Time 0.310 (0.310)	Data 0.191 (0.191)	Loss 0.1603 (0.1603) ([0.012]+[0.149])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1598 (0.1626) ([0.011]+[0.149])	Prec@1 100.000 (99.752)
Epoch: [280][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1611 (0.1625) ([0.013]+[0.148])	Prec@1 100.000 (99.743)
Epoch: [280][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1556 (0.1624) ([0.007]+[0.148])	Prec@1 100.000 (99.746)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3624 (0.3624) ([0.214]+[0.148])	Prec@1 94.531 (94.531)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.4938, device='cuda:0')
Epoch: [281][0/391]	Time 0.311 (0.311)	Data 0.193 (0.193)	Loss 0.1543 (0.1543) ([0.006]+[0.148])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1842 (0.1635) ([0.036]+[0.148])	Prec@1 99.219 (99.675)
Epoch: [281][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1552 (0.1625) ([0.007]+[0.148])	Prec@1 100.000 (99.720)
Epoch: [281][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1550 (0.1622) ([0.007]+[0.148])	Prec@1 100.000 (99.712)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.3355 (0.3355) ([0.188]+[0.148])	Prec@1 96.094 (96.094)
 * Prec@1 93.400
current lr 1.00000e-03
Grad=  tensor(0.3470, device='cuda:0')
Epoch: [282][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.1523 (0.1523) ([0.004]+[0.148])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1549 (0.1603) ([0.007]+[0.148])	Prec@1 100.000 (99.791)
Epoch: [282][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1534 (0.1611) ([0.006]+[0.148])	Prec@1 100.000 (99.763)
Epoch: [282][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1611 (0.1612) ([0.014]+[0.147])	Prec@1 99.219 (99.751)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.3505 (0.3505) ([0.203]+[0.147])	Prec@1 94.531 (94.531)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(1.3131, device='cuda:0')
Epoch: [283][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.1589 (0.1589) ([0.012]+[0.147])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1655 (0.1590) ([0.018]+[0.147])	Prec@1 100.000 (99.822)
Epoch: [283][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1633 (0.1603) ([0.016]+[0.147])	Prec@1 100.000 (99.782)
Epoch: [283][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1667 (0.1607) ([0.020]+[0.147])	Prec@1 99.219 (99.735)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.3654 (0.3654) ([0.219]+[0.147])	Prec@1 94.531 (94.531)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(1.0602, device='cuda:0')
Epoch: [284][0/391]	Time 0.300 (0.300)	Data 0.181 (0.181)	Loss 0.1569 (0.1569) ([0.010]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [284][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1546 (0.1601) ([0.008]+[0.147])	Prec@1 100.000 (99.729)
Epoch: [284][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1671 (0.1598) ([0.020]+[0.147])	Prec@1 99.219 (99.740)
Epoch: [284][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1700 (0.1600) ([0.023]+[0.146])	Prec@1 99.219 (99.735)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3422 (0.3422) ([0.196]+[0.146])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(1.7401, device='cuda:0')
Epoch: [285][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.1658 (0.1658) ([0.019]+[0.146])	Prec@1 99.219 (99.219)
Epoch: [285][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1514 (0.1596) ([0.005]+[0.146])	Prec@1 100.000 (99.776)
Epoch: [285][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1635 (0.1592) ([0.017]+[0.146])	Prec@1 99.219 (99.767)
Epoch: [285][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1511 (0.1592) ([0.005]+[0.146])	Prec@1 100.000 (99.756)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3347 (0.3347) ([0.189]+[0.146])	Prec@1 96.094 (96.094)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(1.0920, device='cuda:0')
Epoch: [286][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.1580 (0.1580) ([0.012]+[0.146])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1604 (0.1572) ([0.015]+[0.146])	Prec@1 99.219 (99.799)
Epoch: [286][200/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.1555 (0.1574) ([0.010]+[0.146])	Prec@1 100.000 (99.825)
Epoch: [286][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.1596 (0.1580) ([0.014]+[0.146])	Prec@1 99.219 (99.798)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3325 (0.3325) ([0.187]+[0.145])	Prec@1 95.312 (95.312)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(1.6951, device='cuda:0')
Epoch: [287][0/391]	Time 0.300 (0.300)	Data 0.182 (0.182)	Loss 0.1558 (0.1558) ([0.010]+[0.145])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.1585 (0.1560) ([0.013]+[0.145])	Prec@1 100.000 (99.861)
Epoch: [287][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.1644 (0.1568) ([0.019]+[0.145])	Prec@1 99.219 (99.825)
Epoch: [287][300/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.1538 (0.1571) ([0.009]+[0.145])	Prec@1 100.000 (99.824)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3444 (0.3444) ([0.199]+[0.145])	Prec@1 95.312 (95.312)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.9721, device='cuda:0')
Epoch: [288][0/391]	Time 0.302 (0.302)	Data 0.185 (0.185)	Loss 0.1551 (0.1551) ([0.010]+[0.145])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.107 (0.109)	Data 0.000 (0.002)	Loss 0.1518 (0.1566) ([0.007]+[0.145])	Prec@1 100.000 (99.760)
Epoch: [288][200/391]	Time 0.107 (0.108)	Data 0.000 (0.001)	Loss 0.1827 (0.1568) ([0.038]+[0.145])	Prec@1 98.438 (99.786)
Epoch: [288][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.1607 (0.1568) ([0.016]+[0.145])	Prec@1 99.219 (99.795)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3408 (0.3408) ([0.196]+[0.145])	Prec@1 95.312 (95.312)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(3.0080, device='cuda:0')
Epoch: [289][0/391]	Time 0.308 (0.308)	Data 0.189 (0.189)	Loss 0.1661 (0.1661) ([0.022]+[0.145])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1481 (0.1561) ([0.004]+[0.144])	Prec@1 100.000 (99.807)
Epoch: [289][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1532 (0.1558) ([0.009]+[0.144])	Prec@1 100.000 (99.829)
Epoch: [289][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1486 (0.1558) ([0.004]+[0.144])	Prec@1 100.000 (99.816)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.3341 (0.3341) ([0.190]+[0.144])	Prec@1 95.312 (95.312)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.6014, device='cuda:0')
Epoch: [290][0/391]	Time 0.305 (0.305)	Data 0.185 (0.185)	Loss 0.1500 (0.1500) ([0.006]+[0.144])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1492 (0.1553) ([0.005]+[0.144])	Prec@1 100.000 (99.807)
Epoch: [290][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1475 (0.1556) ([0.004]+[0.144])	Prec@1 100.000 (99.810)
Epoch: [290][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1504 (0.1557) ([0.007]+[0.144])	Prec@1 100.000 (99.798)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.3376 (0.3376) ([0.194]+[0.144])	Prec@1 95.312 (95.312)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(2.1085, device='cuda:0')
Epoch: [291][0/391]	Time 0.303 (0.303)	Data 0.182 (0.182)	Loss 0.1538 (0.1538) ([0.010]+[0.144])	Prec@1 99.219 (99.219)
Epoch: [291][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1555 (0.1548) ([0.012]+[0.144])	Prec@1 99.219 (99.799)
Epoch: [291][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1556 (0.1548) ([0.012]+[0.143])	Prec@1 99.219 (99.794)
Epoch: [291][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1541 (0.1551) ([0.011]+[0.143])	Prec@1 100.000 (99.785)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3579 (0.3579) ([0.215]+[0.143])	Prec@1 95.312 (95.312)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.2953, device='cuda:0')
Epoch: [292][0/391]	Time 0.306 (0.306)	Data 0.185 (0.185)	Loss 0.1460 (0.1460) ([0.003]+[0.143])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1535 (0.1567) ([0.010]+[0.143])	Prec@1 100.000 (99.783)
Epoch: [292][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1475 (0.1553) ([0.005]+[0.143])	Prec@1 100.000 (99.802)
Epoch: [292][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1504 (0.1547) ([0.008]+[0.143])	Prec@1 100.000 (99.816)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.3585 (0.3585) ([0.216]+[0.143])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(7.3240, device='cuda:0')
Epoch: [293][0/391]	Time 0.302 (0.302)	Data 0.182 (0.182)	Loss 0.1713 (0.1713) ([0.029]+[0.143])	Prec@1 99.219 (99.219)
Epoch: [293][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1570 (0.1540) ([0.014]+[0.143])	Prec@1 99.219 (99.783)
Epoch: [293][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1482 (0.1540) ([0.006]+[0.143])	Prec@1 100.000 (99.802)
Epoch: [293][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1587 (0.1536) ([0.016]+[0.142])	Prec@1 100.000 (99.818)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.3726 (0.3726) ([0.230]+[0.142])	Prec@1 96.094 (96.094)
 * Prec@1 93.450
current lr 1.00000e-03
Grad=  tensor(4.0245, device='cuda:0')
Epoch: [294][0/391]	Time 0.298 (0.298)	Data 0.179 (0.179)	Loss 0.1624 (0.1624) ([0.020]+[0.142])	Prec@1 99.219 (99.219)
Epoch: [294][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1462 (0.1522) ([0.004]+[0.142])	Prec@1 100.000 (99.892)
Epoch: [294][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1587 (0.1525) ([0.017]+[0.142])	Prec@1 99.219 (99.880)
Epoch: [294][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1589 (0.1527) ([0.017]+[0.142])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.4030 (0.4030) ([0.261]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(0.5150, device='cuda:0')
Epoch: [295][0/391]	Time 0.300 (0.300)	Data 0.180 (0.180)	Loss 0.1486 (0.1486) ([0.007]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1466 (0.1517) ([0.005]+[0.142])	Prec@1 100.000 (99.869)
Epoch: [295][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1464 (0.1521) ([0.005]+[0.142])	Prec@1 100.000 (99.848)
Epoch: [295][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1450 (0.1520) ([0.004]+[0.142])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.3902 (0.3902) ([0.249]+[0.141])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(0.3795, device='cuda:0')
Epoch: [296][0/391]	Time 0.304 (0.304)	Data 0.184 (0.184)	Loss 0.1465 (0.1465) ([0.005]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1471 (0.1515) ([0.006]+[0.141])	Prec@1 100.000 (99.869)
Epoch: [296][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1517 (0.1519) ([0.011]+[0.141])	Prec@1 100.000 (99.852)
Epoch: [296][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1518 (0.1518) ([0.011]+[0.141])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4114 (0.4114) ([0.270]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(3.1908, device='cuda:0')
Epoch: [297][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.1577 (0.1577) ([0.017]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1515 (0.1521) ([0.011]+[0.141])	Prec@1 100.000 (99.722)
Epoch: [297][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1443 (0.1520) ([0.004]+[0.141])	Prec@1 100.000 (99.767)
Epoch: [297][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1517 (0.1519) ([0.011]+[0.141])	Prec@1 99.219 (99.782)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.4028 (0.4028) ([0.262]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 93.400
current lr 1.00000e-03
Grad=  tensor(0.7450, device='cuda:0')
Epoch: [298][0/391]	Time 0.314 (0.314)	Data 0.194 (0.194)	Loss 0.1497 (0.1497) ([0.009]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1452 (0.1499) ([0.005]+[0.140])	Prec@1 100.000 (99.923)
Epoch: [298][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1619 (0.1503) ([0.022]+[0.140])	Prec@1 99.219 (99.868)
Epoch: [298][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1450 (0.1505) ([0.005]+[0.140])	Prec@1 100.000 (99.852)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3860 (0.3860) ([0.246]+[0.140])	Prec@1 95.312 (95.312)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(0.8716, device='cuda:0')
Epoch: [299][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 0.1484 (0.1484) ([0.008]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1510 (0.1494) ([0.011]+[0.140])	Prec@1 100.000 (99.869)
Epoch: [299][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1556 (0.1503) ([0.016]+[0.140])	Prec@1 99.219 (99.821)
Epoch: [299][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1432 (0.1501) ([0.003]+[0.140])	Prec@1 100.000 (99.831)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3886 (0.3886) ([0.249]+[0.140])	Prec@1 96.094 (96.094)
 * Prec@1 93.160

 Elapsed time for training  3:50:35.798825

 sparsity of   [0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9259259104728699, 0.9629629850387573, 0.48148149251937866, 0.0, 0.9259259104728699, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.8148148059844971, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.8888888955116272, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.984375, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.984375, 0.0, 0.984375, 0.96875, 0.96875, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.984375, 0.96875, 0.984375, 0.484375, 0.96875, 0.984375, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.96875, 0.96875, 0.984375, 0.0]

 sparsity of   [0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.6875, 0.0, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.487847238779068, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9947916865348816, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9947916865348816, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.9947916865348816, 0.9982638955116272, 0.9965277910232544, 0.6875]

 sparsity of   [0.96875, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.734375, 0.0, 0.734375, 0.0, 0.0, 0.734375, 0.96875, 0.96875, 0.96875, 0.984375, 0.734375, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.984375, 0.734375, 0.0, 0.734375, 0.96875, 0.734375, 0.0, 0.984375, 0.0, 0.0, 0.734375, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.734375, 0.0, 0.96875, 0.34375, 0.0, 0.0, 0.484375, 0.96875, 0.734375, 0.984375, 0.0, 0.0, 0.96875, 0.734375, 0.0, 0.734375, 0.96875, 0.734375, 0.0, 0.96875, 0.0, 0.734375, 0.734375, 0.96875, 0.0, 0.984375, 0.734375, 0.0, 0.0, 0.96875, 0.734375, 0.0, 0.09375, 0.0, 0.265625, 0.734375, 0.375, 0.0, 0.953125, 0.96875, 0.0, 0.984375, 0.0, 0.984375, 0.96875, 0.734375, 0.078125, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.984375, 0.734375, 0.4375, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.03125, 0.0, 0.890625, 0.0, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.0, 0.734375, 0.0, 0.96875, 0.734375, 0.0, 0.734375, 0.234375, 0.96875, 0.0, 0.96875, 0.734375, 0.984375, 0.078125, 0.0, 0.734375, 0.0, 0.96875, 0.734375, 0.578125, 0.96875, 0.96875, 0.0, 0.734375, 0.0, 0.96875, 0.984375, 0.0, 0.734375, 0.0, 0.734375, 0.734375, 0.0, 0.71875, 0.0, 0.734375, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.15625, 0.0, 0.0, 0.734375, 0.609375, 0.96875, 0.0, 0.0, 0.078125, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.734375, 0.96875, 0.734375, 0.734375, 0.734375, 0.734375, 0.0, 0.734375, 0.71875, 0.09375, 0.0, 0.125, 0.0, 0.96875, 0.734375, 0.125, 0.0, 0.96875, 0.0, 0.734375, 0.96875, 0.96875, 0.0, 0.0, 0.09375, 0.96875, 0.96875, 0.96875, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.734375, 0.625, 0.0, 0.96875, 0.984375, 0.0, 0.953125, 0.96875, 0.96875, 0.0, 0.0, 0.734375, 0.0, 0.265625, 0.0, 0.734375, 0.96875, 0.734375, 0.0, 0.0, 0.0, 0.734375, 0.96875, 0.734375, 0.96875, 0.0, 0.96875]

 sparsity of   [0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.984375, 0.984375, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.984375, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.96875, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.953125, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.96875, 0.0, 0.96875]

 sparsity of   [0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.3125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.3125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.3125, 0.9921875, 0.9921875, 0.9921875, 0.3125, 0.9921875, 0.9921875, 0.3125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.3125, 0.3125, 0.9921875]

 sparsity of   [0.9965277910232544, 0.9982638955116272, 0.6875, 0.0, 0.0, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.6857638955116272, 0.6875, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.6857638955116272, 0.9965277910232544, 0.6875, 0.6875, 0.6857638955116272, 0.0, 0.6875, 0.9965277910232544, 0.6875, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.6875, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9965277910232544, 0.6875, 0.9965277910232544, 0.9965277910232544, 0.6875, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9982638955116272, 0.6875, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272]

 sparsity of   [0.9375, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.328125, 0.0, 0.96875, 0.0, 0.96875, 0.5, 0.0, 0.96875, 0.5, 0.984375, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.5, 0.5, 0.5, 0.96875, 0.5, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.984375, 0.0, 0.5, 0.96875, 0.0, 0.0, 0.96875, 0.5, 0.5, 0.5, 0.96875, 0.5, 0.5, 0.0, 0.015625, 0.5, 0.5, 0.0, 0.0, 0.96875, 0.5, 0.0, 0.0, 0.96875, 0.015625, 0.0, 0.5, 0.0, 0.984375, 0.5, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.5, 0.015625, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.96875, 0.5, 0.984375, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.484375, 0.96875, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.96875, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.96875, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.96875, 0.0, 0.5, 0.5, 0.09375, 0.984375, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.765625, 0.5, 0.0, 0.0, 0.0, 0.0, 0.171875, 0.484375, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.96875, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5, 0.96875, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0, 0.96875, 0.0, 0.5, 0.96875, 0.96875, 0.0, 0.5, 0.96875, 0.96875, 0.0, 0.40625, 0.5, 0.5, 0.0, 0.0, 0.5, 0.0, 0.5, 0.328125, 0.0, 0.5, 0.96875, 0.5, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.515625, 0.5, 0.96875, 0.0, 0.96875]

 sparsity of   [0.0, 0.1640625, 0.98828125, 0.0, 0.0, 0.00390625, 0.0, 0.1953125, 0.0, 0.046875, 0.4453125, 0.0, 0.0, 0.0078125, 0.140625, 0.0, 0.0, 0.0, 0.08984375, 0.0, 0.1171875, 0.99609375, 0.99609375, 0.0, 0.16015625, 0.0, 0.0, 0.0, 0.0, 0.72265625, 0.02734375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.13671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39453125, 0.06640625, 0.99609375, 0.0859375, 0.0, 0.0, 0.0, 0.42578125, 0.02734375, 0.98828125, 0.8671875, 0.31640625, 0.05078125, 0.0, 0.93359375, 0.0, 0.9921875, 0.0, 0.9921875]

 sparsity of   [0.9965277910232544, 0.0, 0.0347222238779068, 0.0, 0.010416666977107525, 0.0, 0.0, 0.9184027910232544, 0.0555555559694767, 0.0, 0.0572916679084301, 0.02083333395421505, 0.0, 0.046875, 0.0, 0.3506944477558136, 0.9982638955116272, 0.03125, 0.0347222238779068, 0.0416666679084301, 0.9010416865348816, 0.0, 0.0, 0.9982638955116272, 0.9045138955116272, 0.0, 0.0, 0.9270833134651184, 0.013888888992369175, 0.0416666679084301, 0.0, 0.0, 0.0920138880610466, 0.9982638955116272, 0.0, 0.0364583320915699, 0.0, 0.0763888880610466, 0.0, 0.0416666679084301, 0.02083333395421505, 0.9965277910232544, 0.0, 0.0989583358168602, 0.0, 0.0, 0.913194477558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0833333358168602, 0.203125, 0.0173611119389534, 0.0, 0.0, 0.02777777798473835, 0.0, 0.0, 0.0, 0.0434027798473835, 0.8888888955116272, 0.0885416641831398]

 sparsity of   [0.015625, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.453125, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.421875, 0.0, 0.484375, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.46875, 0.0625, 0.0, 0.0, 0.4375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.46875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.046875, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.46875, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.484375, 0.0, 0.046875, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.484375, 0.046875, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.453125, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.453125, 0.0, 0.484375, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.46875, 0.484375, 0.46875, 0.484375, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.234375, 0.0, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.0, 0.484375, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.10546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.04296875, 0.0, 0.19921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.41015625, 0.0, 0.0, 0.98828125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33203125, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.4296875, 0.0, 0.0, 0.44921875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9296875, 0.98828125, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55078125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.02951388992369175, 0.013888888992369175, 0.999131977558136, 0.0, 0.8993055820465088, 0.2473958283662796, 0.9140625, 0.02083333395421505, 0.010416666977107525, 0.0, 0.013888888992369175, 0.9105902910232544, 0.0, 0.0, 0.0, 0.999131977558136, 0.8654513955116272, 0.0, 0.01822916604578495, 0.0, 0.8854166865348816, 0.0607638880610466, 0.0, 0.0, 0.01909722201526165, 0.0581597238779068, 0.0, 0.0, 0.0, 0.0451388880610466, 0.0, 0.0, 0.7942708134651184, 0.0, 0.0243055559694767, 0.0, 0.9192708134651184, 0.9409722089767456, 0.01996527798473835, 0.02777777798473835, 0.0, 0.02604166604578495, 0.01909722201526165, 0.071180559694767, 0.0, 0.952256977558136, 0.0, 0.110243059694767, 0.0434027798473835, 0.0425347238779068, 0.118055559694767, 0.9487847089767456, 0.0, 0.0, 0.9973958134651184, 0.01215277798473835, 0.9982638955116272, 0.0225694440305233, 0.921875, 0.0, 0.0494791679084301, 0.0581597238779068, 0.7916666865348816, 0.0538194440305233, 0.0503472238779068, 0.1371527761220932, 0.0, 0.0, 0.02777777798473835, 0.0, 0.013888888992369175, 0.0, 0.0, 0.0, 0.0251736119389534, 0.0928819477558136, 0.0164930559694767, 0.9696180820465088, 0.9166666865348816, 0.0642361119389534, 0.0, 0.0729166641831398, 0.3203125, 0.0, 0.0, 0.0, 0.01996527798473835, 0.0, 0.02690972201526165, 0.0, 0.0460069440305233, 0.0, 0.9071180820465088, 0.0251736119389534, 0.0, 0.0, 0.0, 0.01909722201526165, 0.0564236119389534, 0.9982638955116272, 0.0, 0.944444477558136, 0.0, 0.0416666679084301, 0.0, 0.8880208134651184, 0.0434027798473835, 0.9973958134651184, 0.0, 0.03125, 0.1085069477558136, 0.0, 0.0, 0.1883680522441864, 0.02604166604578495, 0.0355902798473835, 0.9982638955116272, 0.0, 0.03125, 0.9496527910232544, 0.0, 0.013020833022892475, 0.8020833134651184, 0.1215277761220932, 0.0442708320915699, 0.3203125, 0.1163194477558136]

 sparsity of   [0.0, 0.984375, 0.0, 0.9765625, 0.984375, 0.0, 0.984375, 0.0546875, 0.0625, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9765625, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.9921875, 0.9765625, 0.0, 0.9921875, 0.078125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.046875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9765625, 0.5, 0.0, 0.0234375, 0.984375, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9765625, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.984375, 0.9921875, 0.9765625, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.984375, 0.0, 0.9921875, 0.9765625, 0.9921875, 0.0, 0.9921875, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.9765625, 0.9765625, 0.984375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.984375, 0.953125, 0.0, 0.9921875, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.0, 0.984375, 0.0390625, 0.984375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.9921875, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.9765625, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.9765625, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.9765625, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5859375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.96875, 0.0, 0.96875, 0.9921875, 0.9765625, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.5859375, 0.0, 0.9921875, 0.9921875, 0.984375, 0.03125, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9921875, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.34375, 0.0, 0.5859375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.03125, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.9921875, 0.078125, 0.0, 0.984375, 0.984375, 0.171875, 0.4609375, 0.9921875, 0.9921875, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.5859375, 0.9765625, 0.0, 0.03125, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.9765625, 0.0, 0.0390625, 0.984375, 0.984375, 0.0, 0.5859375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.9765625, 0.0234375, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.984375, 0.9765625, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.5859375, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0546875, 0.5859375, 0.0, 0.984375]

 sparsity of   [0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.3203125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875]

 sparsity of   [0.99609375, 0.0, 0.0, 0.0, 0.466796875, 0.466796875, 0.99609375, 0.46484375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.466796875, 0.466796875, 0.99609375, 0.0, 0.466796875, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.994140625, 0.466796875, 0.466796875, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.466796875, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.9921875, 0.99609375, 0.46484375, 0.0, 0.46484375, 0.46484375, 0.99609375, 0.466796875, 0.0, 0.99609375, 0.99609375, 0.46484375, 0.466796875, 0.466796875, 0.0, 0.994140625, 0.0, 0.99609375, 0.4609375, 0.466796875, 0.0, 0.466796875, 0.99609375, 0.99609375, 0.994140625, 0.0, 0.0, 0.458984375, 0.998046875, 0.0, 0.46484375, 0.466796875, 0.466796875, 0.0, 0.0, 0.0, 0.466796875, 0.466796875, 0.0, 0.0, 0.0, 0.0, 0.466796875, 0.0, 0.45703125, 0.46484375, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.466796875, 0.466796875, 0.46484375, 0.0, 0.99609375, 0.466796875, 0.99609375, 0.0, 0.0, 0.0, 0.466796875, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.999131977558136]

 sparsity of   [0.0, 0.984375, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.3046875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.3828125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.9921875, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.53125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.21875, 0.0, 0.0, 0.0, 0.0, 0.140625, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.5078125, 0.0, 0.0, 0.6953125, 0.0, 0.1640625, 0.640625, 0.3515625, 0.0, 0.1875, 0.0, 0.0, 0.0, 0.5, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6953125, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.53125, 0.53125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.53125, 0.53125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.53125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.140625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.9765625, 0.0, 0.984375, 0.0, 0.984375, 0.9765625, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.5078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.53125, 0.0, 0.53125, 0.0, 0.0, 0.984375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.53125, 0.5234375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.9921875, 0.0, 0.0, 0.53125, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.169921875, 0.0, 0.0, 0.0, 0.0, 0.037109375, 0.99609375, 0.0, 0.0, 0.021484375, 0.0, 0.193359375, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.99609375, 0.333984375, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4140625, 0.048828125, 0.0, 0.080078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.99609375, 0.771484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.130859375, 0.0, 0.0, 0.0, 0.03125, 0.998046875, 0.0390625, 0.05078125, 0.0, 0.0, 0.138671875, 0.0, 0.0, 0.861328125, 0.998046875, 0.064453125, 0.0, 0.0, 0.078125, 0.0, 0.185546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.9296875, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.255859375, 0.0, 0.0, 0.0, 0.0, 0.048828125, 0.0, 0.0, 0.99609375, 0.09375, 0.181640625, 0.064453125, 0.0, 0.99609375, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20703125, 0.0, 0.0]

 sparsity of   [0.0590277798473835, 0.0, 0.03125, 0.046875, 0.0, 0.2291666716337204, 0.0416666679084301, 0.0, 0.0, 0.0381944440305233, 0.0564236119389534, 0.9973958134651184, 0.01128472201526165, 0.0, 0.0225694440305233, 0.1163194477558136, 0.0, 0.0, 0.0, 0.1137152761220932, 0.0, 0.0, 0.1597222238779068, 0.0303819440305233, 0.0078125, 0.01822916604578495, 0.0, 0.0, 0.0, 0.0703125, 0.0, 0.02690972201526165, 0.0, 0.0, 0.02777777798473835, 0.0017361111240461469, 0.7743055820465088, 0.0, 0.0954861119389534, 0.02083333395421505, 0.0460069440305233, 0.0842013880610466, 0.8680555820465088, 0.9973958134651184, 0.078993059694767, 0.0963541641831398, 0.8185763955116272, 0.157986119389534, 0.0, 0.0, 0.02170138992369175, 0.9774305820465088, 0.0, 0.0572916679084301, 0.8984375, 0.0, 0.0503472238779068, 0.0, 0.0, 0.0, 0.013020833022892475, 0.0, 0.0, 0.7986111044883728, 0.4921875, 0.0, 0.046875, 0.01996527798473835, 0.0, 0.9392361044883728, 0.0, 0.0989583358168602, 0.0355902798473835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0651041641831398, 0.0, 0.0920138880610466, 0.0, 0.6970486044883728, 0.0, 0.0, 0.2196180522441864, 0.118055559694767, 0.9704861044883728, 0.0546875, 0.006076388992369175, 0.0, 0.0390625, 0.0, 0.013888888992369175, 0.0, 0.0355902798473835, 0.0, 0.0737847238779068, 0.0, 0.0, 0.9973958134651184, 0.8394097089767456, 0.0, 0.7899305820465088, 0.284722238779068, 0.01909722201526165, 0.0520833320915699, 0.9652777910232544, 0.1059027761220932, 0.0, 0.0, 0.3064236044883728, 0.7638888955116272, 0.0355902798473835, 0.0381944440305233, 0.7899305820465088, 0.0, 0.0243055559694767, 0.8923611044883728, 0.0, 0.7994791865348816, 0.0, 0.9461805820465088, 0.013020833022892475, 0.999131977558136, 0.0, 0.0, 0.9730902910232544]

 sparsity of   [0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.59375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.59375, 0.0, 0.59375, 0.0, 0.0, 0.59375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0625, 0.59375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.5859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.59375, 0.59375, 0.0, 0.0, 0.984375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6328125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0703125, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0859375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6171875, 0.0, 0.59375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7109375, 0.0, 0.0859375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.59375, 0.0, 0.0, 0.9765625, 0.59375, 0.9921875, 0.0, 0.0, 0.0, 0.8359375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0703125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.59375, 0.59375, 0.0, 0.0, 0.984375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2421875, 0.0, 0.59375, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.046875, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.1328125, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.59375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.9921875, 0.0, 0.0, 0.5859375, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.546875, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.59375, 0.0, 0.0, 0.2890625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.109375, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.5859375, 0.0, 0.4296875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.951171875, 0.072265625, 0.99609375, 0.0, 0.1015625, 0.015625, 0.0, 0.99609375, 0.0, 0.01953125, 0.810546875, 0.04296875, 0.009765625, 0.03125, 0.0859375, 0.0, 0.005859375, 0.115234375, 0.0, 0.99609375, 0.12109375, 0.0, 0.87890625, 0.943359375, 0.0, 0.0, 0.033203125, 0.8359375, 0.91796875, 0.0078125, 0.080078125, 0.0, 0.07421875, 0.01953125, 0.09765625, 0.099609375, 0.00390625, 0.080078125, 0.0, 0.02734375, 0.998046875, 0.0, 0.099609375, 0.021484375, 0.083984375, 0.0, 0.068359375, 0.103515625, 0.005859375, 0.0, 0.0, 0.03515625, 0.041015625, 0.99609375, 0.0, 0.041015625, 0.0, 0.0, 0.072265625, 0.009765625, 0.0, 0.0, 0.0, 0.001953125, 0.916015625, 0.017578125, 0.99609375, 0.05078125, 0.0, 0.1015625, 0.0, 0.0, 0.021484375, 0.091796875, 0.005859375, 0.0, 0.060546875, 0.0, 0.09765625, 0.0, 0.0, 0.08984375, 0.00390625, 0.041015625, 0.119140625, 0.111328125, 0.0, 0.09765625, 0.0, 0.015625, 0.009765625, 0.107421875, 0.0, 0.884765625, 0.0, 0.013671875, 0.0, 0.0, 0.994140625, 0.0, 0.99609375, 0.09765625, 0.029296875, 0.0, 0.005859375, 0.0, 0.0, 0.037109375, 0.080078125, 0.0, 0.001953125, 0.0234375, 0.009765625, 0.0234375, 0.0, 0.048828125, 0.0, 0.00390625, 0.009765625, 0.048828125, 0.0, 0.0390625, 0.99609375, 0.0]

 sparsity of   [0.9765625, 0.827256977558136, 0.0494791679084301, 0.0, 0.0416666679084301, 0.889756977558136, 0.0, 0.0989583358168602, 0.0, 0.02170138992369175, 0.0, 0.0, 0.01215277798473835, 0.0763888880610466, 0.0078125, 0.1189236119389534, 0.014756944961845875, 0.0, 0.0, 0.1171875, 0.0338541679084301, 0.8342013955116272, 0.0373263880610466, 0.0251736119389534, 0.3133680522441864, 0.0225694440305233, 0.0078125, 0.0, 0.09375, 0.010416666977107525, 0.0, 0.0894097238779068, 0.0, 0.02690972201526165, 0.0512152798473835, 0.0434027798473835, 0.2326388955116272, 0.9722222089767456, 0.02951388992369175, 0.8923611044883728, 0.0, 0.013888888992369175, 0.013020833022892475, 0.0364583320915699, 0.9114583134651184, 0.0, 0.1215277761220932, 0.01128472201526165, 0.0, 0.0, 0.9982638955116272, 0.0, 0.02864583395421505, 0.063368059694767, 0.014756944961845875, 0.0520833320915699, 0.0, 0.9253472089767456, 0.0173611119389534, 0.0329861119389534, 0.0434027798473835, 0.1050347238779068, 0.394097238779068, 0.1302083283662796, 0.0555555559694767, 0.0, 0.0, 0.0052083334885537624, 0.9973958134651184, 0.0486111119389534, 0.02951388992369175, 0.01822916604578495, 0.02604166604578495, 0.9982638955116272, 0.0251736119389534, 0.0364583320915699, 0.7734375, 0.0, 0.046875, 0.015625, 0.0, 0.9982638955116272, 0.0, 0.0, 0.01996527798473835, 0.0625, 0.0, 0.0, 0.0338541679084301, 0.0086805559694767, 0.0460069440305233, 0.1397569477558136, 0.9982638955116272, 0.1197916641831398, 0.9973958134651184, 0.9973958134651184, 0.02777777798473835, 0.8211805820465088, 0.8576388955116272, 0.009548611007630825, 0.0, 0.0, 0.5234375, 0.1336805522441864, 0.015625, 0.0355902798473835, 0.0, 0.0512152798473835, 0.2465277761220932, 0.9774305820465088, 0.9487847089767456, 0.9427083134651184, 0.9982638955116272, 0.01822916604578495, 0.9279513955116272, 0.0, 0.0347222238779068, 0.0, 0.0, 0.01215277798473835, 0.881944477558136, 0.0, 0.0, 0.0321180559694767, 0.9973958134651184, 0.01996527798473835, 0.9982638955116272, 0.3463541567325592]

 sparsity of   [0.7578125, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.5546875, 0.0, 0.7578125, 0.7578125, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.984375, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.5859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.984375, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.7578125, 0.7578125, 0.0625, 0.9921875, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.7578125, 0.7578125, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.671875, 0.7578125, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.578125, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.9921875, 0.7578125, 0.0, 0.0, 0.7578125, 0.9921875, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.75, 0.0, 0.0, 0.0, 0.9921875, 0.7578125, 0.75, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.75, 0.0, 0.7578125, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.9765625, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.4296875, 0.7578125, 0.7578125, 0.9921875, 0.0, 0.0, 0.7578125, 0.078125, 0.7578125, 0.7578125, 0.7578125, 0.0, 0.0, 0.984375, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.984375, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.3046875, 0.0, 0.7578125, 0.34375, 0.7578125, 0.0, 0.0, 0.0, 0.984375, 0.7578125, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.7890625, 0.0, 0.75, 0.109375, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.7578125, 0.5, 0.7578125, 0.7578125, 0.078125, 0.734375, 0.7578125, 0.0, 0.7578125, 0.984375, 0.7578125, 0.0, 0.0, 0.7578125, 0.7578125, 0.984375, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.703125, 0.0, 0.7578125, 0.0, 0.7578125, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.0, 0.7578125, 0.0, 0.0, 0.0, 0.7578125, 0.7578125, 0.7578125, 0.0]

 sparsity of   [0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.060546875, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.998046875, 0.0, 0.060546875, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.060546875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.060546875, 0.0, 0.0, 0.060546875, 0.0, 0.99609375, 0.060546875, 0.0, 0.99609375, 0.060546875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.060546875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.060546875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.060546875, 0.0, 0.0, 0.99609375, 0.998046875, 0.060546875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.060546875, 0.0, 0.060546875, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.060546875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.060546875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.060546875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.060546875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.060546875, 0.060546875, 0.99609375, 0.060546875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.060546875, 0.0, 0.060546875, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.060546875, 0.0, 0.060546875, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.060546875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375]

 sparsity of   [0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.41015625, 0.999131977558136, 0.4192708432674408, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4353298544883728, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0989583358168602, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.1271701455116272, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136]

 sparsity of   [0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.98828125, 0.9921875, 0.0, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.99609375, 0.09375, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.0, 0.984375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.7734375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.34375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.4296875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.16796875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.3828125, 0.99609375, 0.0, 0.9921875, 0.7734375, 0.21875, 0.99609375, 0.36328125, 0.0, 0.9921875, 0.9921875, 0.7734375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.390625, 0.0, 0.99609375, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.3984375, 0.9921875, 0.9921875, 0.0, 0.0, 0.7734375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.7734375, 0.0, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.98828125, 0.0, 0.0, 0.4296875, 0.7734375, 0.9921875, 0.7734375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.7734375, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.984375, 0.60546875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.23046875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.98046875, 0.7734375, 0.9921875, 0.98828125, 0.9921875, 0.546875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.7734375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.33203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.7734375, 0.9921875, 0.9921875, 0.48828125, 0.9921875, 0.9921875, 0.9921875, 0.7734375, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.171875, 0.0, 0.3671875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.7734375, 0.0, 0.5, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.7734375, 0.98828125, 0.0, 0.7734375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.4296875, 0.9921875, 0.99609375, 0.484375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.9921875, 0.984375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9765625, 0.9921875, 0.7734375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.7734375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.984375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.7734375, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.4375, 0.0, 0.984375, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.19140625, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.7734375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.7734375, 0.98828125, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.7734375, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.7734375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.98046875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.7734375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7734375, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.390625, 0.9921875, 0.9921875, 0.9921875, 0.85546875, 0.9921875, 0.0, 0.7734375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.4453125, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.26953125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875]

 sparsity of   [0.107421875, 0.341796875, 0.1875, 0.994140625, 0.998046875, 0.0, 0.044921875, 0.048828125, 0.0, 0.044921875, 0.10546875, 0.150390625, 0.130859375, 0.998046875, 0.0, 0.1796875, 0.0, 0.99609375, 0.99609375, 0.201171875, 0.08984375, 0.994140625, 0.998046875, 0.1171875, 0.126953125, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.16015625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.994140625, 0.994140625, 0.0, 0.064453125, 0.998046875, 0.0703125, 0.0, 0.060546875, 0.44140625, 0.173828125, 0.017578125, 0.0, 0.99609375, 0.99609375, 0.546875, 0.0859375, 0.103515625, 0.087890625, 0.0, 0.080078125, 0.99609375, 0.083984375, 0.0, 0.8203125, 0.18359375, 0.1953125, 0.892578125, 0.046875, 0.99609375, 0.794921875, 0.0, 0.056640625, 0.0, 0.99609375, 0.84375, 0.998046875, 0.99609375, 0.994140625, 0.03515625, 0.998046875, 0.99609375, 0.130859375, 0.0703125, 0.015625, 0.994140625, 0.994140625, 0.052734375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.07421875, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.052734375, 0.99609375, 0.998046875, 0.0, 0.115234375, 0.056640625, 0.03515625, 0.0, 0.99609375, 0.8671875, 0.0, 0.99609375, 0.1953125, 0.0, 0.99609375, 0.392578125, 0.99609375, 0.0, 0.82421875, 0.89453125, 0.0, 0.0625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.556640625, 0.1015625, 0.0, 0.0, 0.0, 0.046875, 0.052734375, 0.25, 0.13671875, 0.99609375, 0.68359375, 0.029296875, 0.0, 0.998046875, 0.166015625, 0.0, 0.994140625, 0.99609375, 0.998046875, 0.181640625, 0.09375, 0.0, 0.998046875, 0.0, 0.99609375, 0.060546875, 0.0, 0.078125, 0.0, 0.994140625, 0.57421875, 0.08203125, 0.4921875, 0.5859375, 0.99609375, 0.0, 0.0, 0.1328125, 0.998046875, 0.0546875, 0.0, 0.025390625, 0.0, 0.0, 0.056640625, 0.0, 0.330078125, 0.0, 0.0, 0.0, 0.275390625, 0.052734375, 0.0, 0.99609375, 0.02734375, 0.03125, 0.21484375, 0.99609375, 0.48828125, 0.5, 0.86328125, 0.0, 0.30859375, 0.99609375, 0.998046875, 0.791015625, 0.99609375, 0.1015625, 0.99609375, 0.0, 0.998046875, 0.48828125, 0.99609375, 0.0, 0.08203125, 0.99609375, 0.07421875, 0.0703125, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.083984375, 0.09375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.095703125, 0.0, 0.99609375, 0.806640625, 0.99609375, 0.99609375, 0.169921875, 0.296875, 0.99609375, 0.0, 0.091796875, 0.85546875, 0.060546875, 0.197265625, 0.00390625, 0.181640625, 0.25390625, 0.08203125, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.998046875, 0.37109375, 0.845703125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.083984375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.123046875, 0.0, 0.060546875, 0.998046875, 0.19921875, 0.994140625, 0.998046875, 0.0, 0.87109375, 0.8515625, 0.99609375, 0.236328125, 0.044921875, 0.99609375, 0.0, 0.00390625, 0.20703125, 0.08203125, 0.0, 0.03515625, 0.267578125, 0.05859375, 0.095703125, 0.0, 0.0, 0.435546875, 0.771484375, 0.99609375, 0.99609375, 0.015625, 0.03125, 0.99609375, 0.0, 0.0, 0.99609375, 0.529296875, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.869140625, 0.99609375, 0.0546875, 0.9921875, 0.9921875, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.53125, 0.0, 0.0, 0.0, 0.99609375, 0.1015625, 0.05078125, 0.072265625, 0.18359375, 0.099609375, 0.0, 0.00390625, 0.0234375, 0.998046875, 0.1015625, 0.259765625, 0.99609375, 0.013671875, 0.92578125, 0.994140625, 0.1015625, 0.994140625, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.23046875, 0.0, 0.998046875, 0.9921875, 0.0, 0.99609375, 0.212890625, 0.994140625, 0.853515625, 0.21875, 0.99609375, 0.99609375, 0.125, 0.0, 0.7734375, 0.359375, 0.85546875, 0.37890625, 0.2265625, 0.99609375, 0.0, 0.115234375, 0.99609375, 0.99609375, 0.044921875, 0.998046875, 0.0625, 0.0859375, 0.0703125, 0.99609375, 0.0, 0.0, 0.998046875, 0.03125, 0.99609375, 0.99609375, 0.0, 0.083984375, 0.0, 0.77734375, 0.595703125, 0.0, 0.07421875, 0.99609375, 0.9921875, 0.0, 0.005859375, 0.0, 0.0, 0.154296875, 0.91015625, 0.99609375, 0.849609375, 0.0, 0.0703125, 0.87109375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.046875, 0.171875, 0.09765625, 0.0, 0.99609375, 0.154296875, 0.0, 0.060546875, 0.107421875, 0.0, 0.0, 0.048828125, 0.009765625, 0.201171875, 0.05859375, 0.99609375, 0.201171875, 0.0703125, 0.994140625, 0.0, 0.0, 0.998046875, 0.0, 0.849609375, 0.99609375, 0.24609375, 0.78515625, 0.076171875, 0.0703125, 0.01953125, 0.0, 0.16796875, 0.13671875, 0.994140625, 0.994140625, 0.99609375, 0.99609375, 0.052734375, 0.99609375, 0.0, 0.99609375, 0.076171875, 0.0, 0.994140625, 0.99609375, 0.583984375, 0.0, 0.0, 0.087890625, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.259765625, 0.0, 0.310546875, 0.0, 0.181640625, 0.111328125, 0.998046875, 0.79296875, 0.99609375, 0.041015625, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.99609375, 0.998046875, 0.998046875, 0.0, 0.99609375, 0.15625, 0.0, 0.99609375, 0.060546875, 0.126953125, 0.103515625, 0.99609375, 0.0, 0.046875, 0.060546875, 0.99609375, 0.046875, 0.119140625, 0.1015625, 0.197265625, 0.16015625, 0.318359375, 0.99609375, 0.0, 0.0, 0.384765625, 0.0, 0.0, 0.998046875, 0.01171875, 0.0, 0.99609375, 0.0, 0.06640625, 0.19921875, 0.99609375, 0.0, 0.0, 0.076171875, 0.8203125, 0.87890625, 0.0, 0.119140625, 0.0, 0.150390625, 0.998046875, 0.0, 0.99609375, 0.21875, 0.99609375, 0.4453125, 0.0, 0.0, 0.0, 0.0, 0.048828125, 0.0, 0.99609375, 0.99609375, 0.0, 0.080078125, 0.99609375, 0.998046875, 0.0, 0.392578125, 0.306640625, 0.013671875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.994140625, 0.802734375, 0.99609375, 0.013671875, 0.05859375, 0.99609375, 0.0, 0.037109375, 0.0, 0.99609375, 0.99609375, 0.44140625, 0.833984375, 0.994140625, 0.06640625, 0.0078125, 0.99609375, 0.0, 0.060546875, 0.109375, 0.99609375, 0.173828125, 0.466796875, 0.0, 0.095703125, 0.99609375, 0.9296875, 0.42578125, 0.0, 0.0, 0.40234375, 0.025390625, 0.05859375, 0.056640625, 0.99609375, 0.064453125, 0.99609375, 0.216796875, 0.033203125, 0.99609375, 0.865234375, 0.552734375, 0.994140625, 0.0390625, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.072265625, 0.0, 0.99609375, 0.0, 0.869140625, 0.0, 0.099609375, 0.994140625, 0.99609375, 0.99609375, 0.00390625, 0.0, 0.0, 0.998046875, 0.060546875, 0.19921875, 0.99609375, 0.05078125, 0.99609375, 0.998046875, 0.005859375, 0.169921875, 0.0, 0.009765625, 0.99609375, 0.0, 0.0, 0.0, 0.83203125, 0.09375, 0.0, 0.060546875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.2578125, 0.0, 0.32421875, 0.9921875, 0.0, 0.0, 0.10546875, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.185546875, 0.0, 0.89453125, 0.99609375, 0.060546875, 0.2265625, 0.0, 0.998046875, 0.994140625, 0.0546875, 0.998046875, 0.388671875, 0.99609375, 0.150390625, 0.0, 0.99609375, 0.201171875, 0.99609375, 0.095703125, 0.03125, 0.18359375, 0.0, 0.0390625, 0.99609375, 0.0, 0.060546875, 0.11328125, 0.22265625, 0.0, 0.0, 0.99609375, 0.140625, 0.0, 0.0, 0.0, 0.994140625, 0.13671875, 0.123046875, 0.994140625, 0.998046875, 0.025390625, 0.060546875, 0.27734375, 0.0, 0.8984375, 0.056640625, 0.998046875, 0.052734375, 0.99609375, 0.99609375, 0.7734375, 0.998046875, 0.10546875, 0.0, 0.0859375, 0.0, 0.04296875, 0.99609375, 0.091796875, 0.0, 0.99609375, 0.994140625, 0.0, 0.0, 0.0546875, 0.99609375, 0.0, 0.3671875, 0.99609375, 0.0, 0.99609375, 0.439453125, 0.998046875, 0.044921875, 0.99609375, 0.0, 0.99609375, 0.15234375, 0.99609375, 0.994140625, 0.99609375, 0.005859375, 0.326171875, 0.0546875, 0.791015625, 0.0, 0.138671875, 0.0, 0.994140625, 0.99609375, 0.99609375, 0.0, 0.0859375, 0.099609375, 0.998046875, 0.029296875, 0.05078125, 0.998046875, 0.0, 0.005859375, 0.064453125, 0.998046875, 0.21484375, 0.072265625, 0.052734375, 0.99609375, 0.0, 0.998046875, 0.173828125, 0.0, 0.0, 0.0, 0.12890625, 0.189453125, 0.99609375, 0.126953125, 0.99609375, 0.0, 0.26171875, 0.0, 0.99609375, 0.0, 0.998046875, 0.166015625, 0.12109375, 0.044921875, 0.99609375, 0.048828125, 0.998046875, 0.99609375, 0.0, 0.146484375, 0.998046875, 0.107421875, 0.060546875, 0.994140625, 0.837890625, 0.25, 0.048828125, 0.0, 0.0, 0.0, 0.0, 0.521484375, 0.060546875, 0.21484375, 0.998046875, 0.998046875, 0.060546875, 0.04296875, 0.998046875, 0.0, 0.119140625, 0.998046875, 0.0, 0.06640625, 0.99609375, 0.009765625, 0.388671875, 0.064453125, 0.0, 0.005859375, 0.0, 0.998046875, 0.998046875, 0.73828125, 0.0, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.38671875, 0.0, 0.0, 0.06640625, 0.20703125, 0.130859375, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.068359375, 0.103515625, 0.40625, 0.16015625, 0.314453125, 0.99609375, 0.00390625, 0.20703125, 0.0, 0.0, 0.044921875, 0.900390625, 0.994140625, 0.99609375, 0.99609375, 0.107421875, 0.0, 0.19140625, 0.802734375, 0.99609375, 0.99609375, 0.140625, 0.04296875, 0.01171875, 0.025390625, 0.048828125, 0.99609375, 0.99609375, 0.849609375, 0.822265625, 0.998046875, 0.998046875, 0.064453125, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.275390625, 0.15625, 0.32421875, 0.859375, 0.052734375, 0.07421875, 0.0, 0.994140625, 0.99609375, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.181640625, 0.0, 0.119140625, 0.998046875, 0.0, 0.001953125, 0.0546875, 0.99609375, 0.123046875, 0.203125, 0.998046875, 0.08203125, 0.99609375, 0.40234375, 0.025390625, 0.189453125, 0.99609375, 0.83203125, 0.203125, 0.857421875, 0.0390625, 0.0, 0.048828125, 0.0625, 0.99609375, 0.193359375, 0.453125, 0.99609375, 0.99609375, 0.275390625, 0.03515625, 0.99609375, 0.025390625, 0.005859375, 0.0, 0.1796875, 0.0, 0.0, 0.99609375, 0.998046875, 0.845703125, 0.0, 0.0, 0.041015625, 0.99609375, 0.287109375, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.9921875, 0.0, 0.99609375, 0.06640625, 0.99609375, 0.05859375, 0.220703125, 0.7734375, 0.99609375, 0.99609375, 0.99609375, 0.087890625, 0.005859375, 0.998046875, 0.0, 0.99609375, 0.0, 0.79296875, 0.0, 0.8203125, 0.13671875, 0.0, 0.0078125, 0.05859375, 0.0, 0.0, 0.0703125, 0.099609375, 0.0, 0.99609375, 0.11328125, 0.162109375, 0.998046875, 0.99609375, 0.998046875, 0.72265625, 0.0, 0.00390625, 0.0, 0.244140625, 0.99609375, 0.255859375, 0.87890625, 0.826171875, 0.994140625, 0.99609375, 0.99609375, 0.12109375, 0.0, 0.99609375, 0.087890625, 0.0, 0.0625, 0.046875, 0.99609375, 0.853515625, 0.99609375, 0.0, 0.0546875, 0.869140625, 0.998046875, 0.99609375, 0.7734375, 0.0, 0.99609375, 0.998046875, 0.138671875, 0.0625, 0.48828125, 0.998046875, 0.22265625, 0.0, 0.2265625, 0.0, 0.994140625, 0.998046875, 0.099609375, 0.99609375, 0.998046875, 0.1015625, 0.033203125, 0.099609375, 0.998046875, 0.0390625, 0.99609375, 0.271484375, 0.087890625, 0.99609375, 0.212890625, 0.0, 0.193359375]

 sparsity of   [0.9990234375, 0.21484375, 0.7041015625, 0.0, 0.06640625, 0.0576171875, 0.1884765625, 0.0234375, 0.998046875, 0.02734375, 0.2109375, 0.0732421875, 0.15625, 0.0458984375, 0.998046875, 0.0966796875, 0.9990234375, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.7041015625, 0.7041015625, 0.0927734375, 0.119140625, 0.9931640625, 0.5966796875, 0.123046875, 0.7041015625, 0.1142578125, 0.998046875, 0.9970703125, 0.6005859375, 0.060546875, 0.109375, 0.998046875, 0.9990234375, 0.998046875, 0.0751953125, 0.12890625, 0.6904296875, 0.05859375, 0.125, 0.0, 0.998046875, 0.0556640625, 0.1884765625, 0.9970703125, 0.0849609375, 0.0869140625, 0.2998046875, 0.7041015625, 0.06640625, 0.1171875, 0.998046875, 0.9990234375, 0.2470703125, 0.0458984375, 0.078125, 0.0869140625, 0.0673828125, 0.0400390625, 0.9990234375, 0.8857421875, 0.1103515625, 0.869140625, 0.998046875, 0.7041015625, 0.08984375, 0.234375, 0.0908203125, 0.2119140625, 0.998046875, 0.1279296875, 0.0498046875, 0.0, 0.0546875, 0.7041015625, 0.998046875, 0.1298828125, 0.0458984375, 0.751953125, 0.2099609375, 0.0, 0.7041015625, 0.07421875, 0.9970703125, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.220703125, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.677734375, 0.19140625, 0.0, 0.109375, 0.0859375, 0.0, 0.4541015625, 0.638671875, 0.9990234375, 0.0966796875, 0.056640625, 0.1171875, 0.669921875, 0.99609375, 0.0, 0.15625, 0.236328125, 0.1005859375, 0.998046875, 0.056640625, 0.998046875, 0.2626953125, 0.7041015625, 0.998046875, 0.0, 0.0009765625, 0.7041015625, 0.078125, 0.7041015625, 0.998046875, 0.171875, 0.998046875, 0.0, 0.998046875, 0.2470703125, 0.0400390625, 0.9990234375, 0.0947265625, 0.998046875, 0.6650390625, 0.1064453125, 0.9990234375, 0.6748046875, 0.6767578125, 0.7041015625, 0.7021484375, 0.998046875, 0.998046875, 0.703125, 0.703125, 0.064453125, 0.998046875, 0.0, 0.017578125, 0.09375, 0.037109375, 0.150390625, 0.1572265625, 0.998046875, 0.9990234375, 0.220703125, 0.998046875, 0.0, 0.998046875, 0.0986328125, 0.998046875, 0.2744140625, 0.0, 0.0, 0.0458984375, 0.0625, 0.7041015625, 0.998046875, 0.0, 0.05078125, 0.0498046875, 0.998046875, 0.0361328125, 0.998046875, 0.998046875, 0.197265625, 0.9990234375, 0.998046875, 0.998046875, 0.208984375, 0.6650390625, 0.0, 0.2421875, 0.1220703125, 0.998046875, 0.0869140625, 0.1923828125, 0.107421875, 0.130859375, 0.0419921875, 0.0, 0.072265625, 0.67578125, 0.998046875, 0.9990234375, 0.03125, 0.396484375, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.0986328125, 0.2490234375, 0.9970703125, 0.9990234375, 0.125, 0.7236328125, 0.998046875, 0.1162109375, 0.0771484375, 0.998046875, 0.2001953125, 0.0, 0.767578125, 0.9970703125, 0.0, 0.0, 0.7041015625, 0.02734375, 0.998046875, 0.1708984375, 0.1630859375, 0.068359375, 0.9990234375, 0.2607421875, 0.1259765625, 0.0, 0.7041015625, 0.8125, 0.123046875, 0.12890625, 0.0, 0.9990234375, 0.9970703125, 0.1220703125, 0.4697265625, 0.703125, 0.1689453125, 0.1279296875, 0.150390625, 0.2373046875, 0.998046875, 0.998046875, 0.0, 0.7724609375, 0.7041015625, 0.0498046875, 0.9990234375, 0.111328125, 0.12890625, 0.0341796875, 0.9970703125, 0.3466796875]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.323784738779068, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.3216145932674408, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0815972238779068, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.3220486044883728, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.3602430522441864, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.3446180522441864, 0.3046875, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.3259548544883728, 0.0, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.1193576380610466, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0859375, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3268229067325592, 0.9986979365348816, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.9986979365348816, 0.12890625, 0.9995659589767456, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.319878488779068, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8385416865348816, 0.082899309694767, 0.999131977558136, 0.999131977558136, 0.1115451380610466, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.3268229067325592, 0.999131977558136, 0.12109375, 0.0872395858168602, 0.999131977558136, 0.999131977558136, 0.2717013955116272, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0]

 sparsity of   [0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.99609375, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9140625, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9140625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.99609375, 0.9921875, 0.0, 0.91796875, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.91796875, 0.91796875, 0.0, 0.99609375, 0.0, 0.99609375, 0.9140625, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.9140625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9140625, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.91796875, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.99609375, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.91796875, 0.98828125, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.91796875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.91796875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.0, 0.9140625, 0.9921875, 0.99609375, 0.9921875, 0.91796875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.99609375, 0.91796875, 0.91796875, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9140625, 0.9921875, 0.0, 0.99609375, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.91796875, 0.91796875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.91796875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.91796875, 0.99609375, 0.0, 0.91796875, 0.91796875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.0, 0.9140625, 0.0, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.91796875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.91796875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.91796875, 0.91796875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.91796875, 0.0, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9140625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.0, 0.99609375, 0.91796875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.91796875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.98828125, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.0, 0.0, 0.9921875, 0.91796875, 0.99609375, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.0, 0.0, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.91796875, 0.9140625, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.91796875, 0.91796875, 0.91796875, 0.99609375, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.99609375, 0.9921875, 0.91796875, 0.91796875, 0.91796875, 0.91796875, 0.0, 0.91796875, 0.9921875, 0.91796875, 0.9921875, 0.91796875, 0.0, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.0, 0.9921875, 0.91796875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.91796875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.99609375, 0.99609375, 0.91796875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.91796875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875]

 sparsity of   [0.9990234375, 0.0625, 0.2412109375, 0.181640625, 0.1494140625, 0.9970703125, 0.046875, 0.998046875, 0.2236328125, 0.998046875, 0.1298828125, 0.9990234375, 0.9970703125, 0.5947265625, 0.9970703125, 0.5947265625, 0.998046875, 0.0927734375, 0.0869140625, 0.9970703125, 0.998046875, 0.0771484375, 0.90234375, 0.5947265625, 0.0419921875, 0.3466796875, 0.998046875, 0.0, 0.0, 0.568359375, 0.9970703125, 0.0966796875, 0.0, 0.5947265625, 0.998046875, 0.998046875, 0.5947265625, 0.9990234375, 0.06640625, 0.16796875, 0.3515625, 0.03515625, 0.998046875, 0.9990234375, 0.0, 0.9970703125, 0.4853515625, 0.0, 0.998046875, 0.0, 0.328125, 0.9990234375, 0.9970703125, 0.9990234375, 0.9951171875, 0.56640625, 0.3369140625, 0.0, 0.1767578125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.57421875, 0.9970703125, 0.998046875, 0.5927734375, 0.9990234375, 0.0, 0.064453125, 0.0, 0.998046875, 0.0634765625, 0.4560546875, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.5947265625, 0.9990234375, 0.80078125, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.173828125, 0.998046875, 0.9990234375, 0.8349609375, 0.998046875, 0.185546875, 0.998046875, 0.0, 0.998046875, 0.0, 0.05078125, 0.9970703125, 0.3212890625, 0.9990234375, 0.4853515625, 0.0, 0.9990234375, 0.3828125, 0.998046875, 0.5947265625, 0.0, 0.0, 0.5732421875, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.0859375, 0.0888671875, 0.0, 0.14453125, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.5693359375, 0.9970703125, 0.0, 0.0, 0.0, 0.0576171875, 0.8359375, 0.9970703125, 0.59375, 0.998046875, 0.0166015625, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.05078125, 0.0517578125, 0.998046875, 0.07421875, 0.1025390625, 0.1015625, 0.0, 0.2099609375, 0.08203125, 0.087890625, 0.9970703125, 0.08984375, 0.14453125, 0.998046875, 0.0, 0.9970703125, 0.3349609375, 0.9970703125, 0.4990234375, 0.998046875, 0.0, 0.998046875, 0.9990234375, 0.0, 0.0625, 0.0, 0.5947265625, 0.998046875, 0.58984375, 0.083984375, 0.0, 0.9990234375, 0.060546875, 0.0, 0.568359375, 0.0986328125, 0.08984375, 0.9990234375, 0.0, 0.056640625, 0.0, 0.12890625, 0.0, 0.2998046875, 0.115234375, 0.0693359375, 0.998046875, 0.5947265625, 0.1689453125, 0.330078125, 0.0, 0.0, 0.9970703125, 0.2060546875, 0.0, 0.83984375, 0.08984375, 0.9970703125, 0.998046875, 0.0, 0.59375, 0.998046875, 0.0, 0.9970703125, 0.0712890625, 0.998046875, 0.9990234375, 0.998046875, 0.064453125, 0.9990234375, 0.9990234375, 0.0732421875, 0.9970703125, 0.09375, 0.181640625, 0.0771484375, 0.9970703125, 0.998046875, 0.9990234375, 0.5947265625, 0.998046875, 0.111328125, 0.9990234375, 0.8310546875, 0.9990234375, 0.998046875, 0.06640625, 0.0, 0.1083984375, 0.998046875, 0.9990234375, 0.0908203125, 0.0, 0.998046875, 0.0947265625, 0.5712890625, 0.0, 0.998046875, 0.3232421875, 0.080078125, 0.0, 0.9990234375, 0.998046875, 0.0, 0.0, 0.0, 0.083984375, 0.0, 0.998046875, 0.9970703125, 0.1123046875, 0.998046875]

 sparsity of   [0.02387152798473835, 0.114149309694767, 0.02387152798473835, 0.9986979365348816, 0.0, 0.0, 0.0876736119389534, 0.0173611119389534, 0.0564236119389534, 0.28515625, 0.0403645820915699, 0.999131977558136, 0.2669270932674408, 0.8307291865348816, 0.999131977558136, 0.009114583022892475, 0.0, 0.1158854141831398, 0.9735243320465088, 0.0581597238779068, 0.609375, 0.8046875, 0.9986979365348816, 0.0, 0.08984375, 0.02777777798473835, 0.0342881940305233, 0.6532118320465088, 0.6167534589767456, 0.0590277798473835, 0.0, 0.0, 0.01909722201526165, 0.0407986119389534, 0.6705729365348816, 0.0963541641831398, 0.0, 0.01171875, 0.013888888992369175, 0.7521701455116272, 0.1549479216337204, 0.114149309694767, 0.03515625, 0.0494791679084301, 0.0, 0.02300347201526165, 0.999131977558136, 0.0, 0.0716145858168602, 0.010416666977107525, 0.0290798619389534, 0.1271701455116272, 0.0503472238779068, 0.0, 0.00824652798473835, 0.0486111119389534, 0.0407986119389534, 0.0065104165114462376, 0.6310763955116272, 0.109375, 0.1918402761220932, 0.9986979365348816, 0.1888020783662796, 0.84765625, 0.0321180559694767, 0.9205729365348816, 0.013454861007630825, 0.02951388992369175, 0.02300347201526165, 0.009982638992369175, 0.03125, 0.0394965298473835, 0.01519097201526165, 0.12890625, 0.0173611119389534, 0.0212673619389534, 0.0, 0.0872395858168602, 0.6236979365348816, 0.2039930522441864, 0.0, 0.9995659589767456, 0.067274309694767, 0.01128472201526165, 0.02951388992369175, 0.0824652761220932, 0.0464409738779068, 0.0234375, 0.8619791865348816, 0.1484375, 0.0616319440305233, 0.02604166604578495, 0.1215277761220932, 0.0303819440305233, 0.1575520783662796, 0.0, 0.0164930559694767, 0.1206597238779068, 0.03125, 0.0, 0.84765625, 0.1258680522441864, 0.013020833022892475, 0.0607638880610466, 0.0529513880610466, 0.0594618059694767, 0.0494791679084301, 0.02387152798473835, 0.0, 0.1154513880610466, 0.067274309694767, 0.2005208283662796, 0.1922743022441864, 0.01779513992369175, 0.0598958320915699, 0.0, 0.5347222089767456, 0.1558159738779068, 0.0564236119389534, 0.12890625, 0.0325520820915699, 0.01909722201526165, 0.0, 0.1028645858168602, 0.0, 0.999131977558136, 0.0186631940305233, 0.0316840298473835, 0.0251736119389534, 0.0963541641831398, 0.96484375, 0.014322916977107525, 0.0, 0.343315988779068, 0.0065104165114462376, 0.0164930559694767, 0.8619791865348816, 0.0425347238779068, 0.999131977558136, 0.01996527798473835, 0.0577256940305233, 0.9986979365348816, 0.0329861119389534, 0.01822916604578495, 0.9995659589767456, 0.0125868059694767, 0.0, 0.9110243320465088, 0.1271701455116272, 0.0390625, 0.007378472480922937, 0.01909722201526165, 0.0390625, 0.01519097201526165, 0.014322916977107525, 0.706163227558136, 0.9986979365348816, 0.0290798619389534, 0.0329861119389534, 0.03515625, 0.0, 0.01996527798473835, 0.02951388992369175, 0.0251736119389534, 0.01779513992369175, 0.07421875, 0.0251736119389534, 0.0203993059694767, 0.0494791679084301, 0.0290798619389534, 0.0, 0.0481770820915699, 0.1792534738779068, 0.7057291865348816, 0.0364583320915699, 0.9557291865348816, 0.0251736119389534, 0.0386284738779068, 0.0, 0.0516493059694767, 0.0234375, 0.02951388992369175, 0.0512152798473835, 0.8671875, 0.6649305820465088, 0.6293402910232544, 0.999131977558136, 0.1827256977558136, 0.0572916679084301, 0.0499131940305233, 0.0, 0.0668402761220932, 0.014322916977107525, 0.1497395783662796, 0.7274305820465088, 0.0625, 0.02734375, 0.078125, 0.1796875, 0.0360243059694767, 0.0638020858168602, 0.0, 0.0581597238779068, 0.0325520820915699, 0.0694444477558136, 0.0282118059694767, 0.2239583283662796, 0.03125, 0.02083333395421505, 0.999131977558136, 0.013020833022892475, 0.0, 0.2352430522441864, 0.0447048619389534, 0.5724826455116272, 0.014322916977107525, 0.0694444477558136, 0.9040798544883728, 0.2330729216337204, 0.03081597201526165, 0.1779513955116272, 0.17578125, 0.5763888955116272, 0.1414930522441864, 0.0212673619389534, 0.02213541604578495, 0.02734375, 0.7261284589767456, 0.01605902798473835, 0.0577256940305233, 0.071180559694767, 0.0802951380610466, 0.0425347238779068, 0.01692708395421505, 0.01605902798473835, 0.014322916977107525, 0.110243059694767, 0.1940104216337204, 0.0381944440305233, 0.0681423619389534, 0.0282118059694767, 0.0442708320915699, 0.0416666679084301, 0.0720486119389534, 0.0, 0.0, 0.0, 0.0559895820915699, 0.9995659589767456, 0.03125, 0.0316840298473835, 0.1193576380610466, 0.71484375, 0.0434027798473835, 0.5290798544883728, 0.013454861007630825]

 sparsity of   [0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0234375, 0.875, 0.9921875, 0.9921875, 0.0, 0.15234375, 0.39453125, 0.99609375, 0.1015625, 0.18359375, 0.0, 0.9921875, 0.6796875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.77734375, 0.79296875, 0.99609375, 0.8046875, 0.0, 0.99609375, 0.796875, 0.9921875, 0.0, 0.83984375, 0.890625, 0.00390625, 0.875, 0.9921875, 0.0, 0.99609375, 0.8203125, 0.875, 0.9921875, 0.875, 0.046875, 0.83984375, 0.12890625, 0.07421875, 0.859375, 0.875, 0.0, 0.05859375, 0.09765625, 0.99609375, 0.8671875, 0.98828125, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.70703125, 0.875, 0.9921875, 0.19140625, 0.99609375, 0.796875, 0.9921875, 0.8359375, 0.91796875, 0.0, 0.99609375, 0.0, 0.9921875, 0.8046875, 0.8125, 0.05859375, 0.98828125, 0.83203125, 0.84375, 0.9921875, 0.6484375, 0.9921875, 0.99609375, 0.1015625, 0.9921875, 0.99609375, 0.0, 0.7734375, 0.875, 0.99609375, 0.0, 0.875, 0.9921875, 0.8125, 0.0, 0.1875, 0.9921875, 0.1328125, 0.828125, 0.76171875, 0.86328125, 0.875, 0.82421875, 0.8828125, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.0, 0.78515625, 0.828125, 0.0, 0.08203125, 0.9921875, 0.875, 0.84765625, 0.77734375, 0.9921875, 0.0, 0.78515625, 0.87109375, 0.9921875, 0.9921875, 0.1171875, 0.94140625, 0.12109375, 0.0, 0.0, 0.875, 0.9921875, 0.0546875, 0.9921875, 0.99609375, 0.68359375, 0.12109375, 0.16015625, 0.0, 0.0625, 0.796875, 0.0, 0.9921875, 0.05859375, 0.98828125, 0.9921875, 0.8125, 0.875, 0.9921875, 0.06640625, 0.9921875, 0.0, 0.875, 0.9921875, 0.0, 0.9921875, 0.78125, 0.99609375, 0.73046875, 0.9921875, 0.85546875, 0.875, 0.8359375, 0.9921875, 0.05859375, 0.12109375, 0.0, 0.86328125, 0.0, 0.0, 0.9921875, 0.0, 0.87890625, 0.0, 0.0, 0.0, 0.875, 0.8125, 0.0, 0.9921875, 0.875, 0.99609375, 0.03515625, 0.9921875, 0.58984375, 0.859375, 0.9921875, 0.0, 0.81640625, 0.99609375, 0.87109375, 0.8671875, 0.86328125, 0.90625, 0.99609375, 0.41796875, 0.09765625, 0.87109375, 0.40234375, 0.36328125, 0.8359375, 0.078125, 0.99609375, 0.99609375, 0.83984375, 0.83984375, 0.99609375, 0.125, 0.828125, 0.07421875, 0.0546875, 0.78125, 0.99609375, 0.9296875, 0.875, 0.22265625, 0.0, 0.8046875, 0.89453125, 0.9921875, 0.16015625, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.71484375, 0.9921875, 0.875, 0.7890625, 0.99609375, 0.9921875, 0.875, 0.9921875, 0.0, 0.9921875, 0.0546875, 0.875, 0.06640625, 0.99609375, 0.98828125, 0.76953125, 0.0, 0.9921875, 0.0, 0.0, 0.875, 0.79296875, 0.1015625, 0.8125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.859375, 0.26171875, 0.0, 0.8125, 0.77734375, 0.9921875, 0.0, 0.35546875, 0.78125, 0.9921875, 0.0625, 0.9921875, 0.99609375, 0.0, 0.80078125, 0.98828125, 0.9921875, 0.0, 0.8046875, 0.75390625, 0.76171875, 0.9921875, 0.0, 0.69140625, 0.0390625, 0.9921875, 0.78125, 0.99609375, 0.875, 0.7421875, 0.99609375, 0.828125, 0.0, 0.80078125, 0.8203125, 0.9921875, 0.9921875, 0.0, 0.921875, 0.0, 0.0, 0.0, 0.92578125, 0.0703125, 0.9921875, 0.98828125, 0.9921875, 0.62890625, 0.9921875, 0.9921875, 0.0, 0.06640625, 0.90625, 0.06640625, 0.0, 0.0, 0.9921875, 0.16015625, 0.9921875, 0.1875, 0.859375, 0.87109375, 0.0, 0.875, 0.875, 0.7109375, 0.9921875, 0.9921875, 0.80078125, 0.9921875, 0.75, 0.99609375, 0.1328125, 0.8359375, 0.953125, 0.9921875, 0.86328125, 0.0, 0.10546875, 0.0, 0.0234375, 0.81640625, 0.8828125, 0.07421875, 0.05078125, 0.8046875, 0.0, 0.875, 0.9921875, 0.0, 0.9921875, 0.17578125, 0.12890625, 0.8671875, 0.87109375, 0.9921875, 0.9921875, 0.375, 0.08984375, 0.77734375, 0.83984375, 0.9921875, 0.83984375, 0.9921875, 0.875, 0.875, 0.76953125, 0.8671875, 0.05859375, 0.9921875, 0.7578125, 0.9921875, 0.9921875, 0.99609375, 0.82421875, 0.875, 0.875, 0.1328125, 0.81640625, 0.98828125, 0.99609375, 0.08984375, 0.9921875, 0.0, 0.9921875, 0.875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.82421875, 0.765625, 0.875, 0.78515625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.79296875, 0.78125, 0.0, 0.0, 0.0, 0.0, 0.875, 0.9921875, 0.9296875, 0.09375, 0.0, 0.8046875, 0.84375, 0.0, 0.9921875, 0.9921875, 0.0, 0.875, 0.8515625, 0.83203125, 0.0078125, 0.59375, 0.875, 0.9921875, 0.9921875, 0.99609375, 0.83984375, 0.875, 0.98828125, 0.875, 0.9921875, 0.9921875, 0.9140625, 0.828125, 0.0390625, 0.80078125, 0.875, 0.875, 0.26953125, 0.109375, 0.80078125, 0.87109375, 0.86328125, 0.875, 0.9921875, 0.99609375, 0.0, 0.1640625, 0.9921875, 0.02734375, 0.9921875, 0.765625, 0.77734375, 0.0, 0.0, 0.87109375, 0.01953125, 0.9921875, 0.703125, 0.0, 0.80859375, 0.73828125, 0.9921875, 0.875, 0.0625, 0.78515625, 0.8359375, 0.9921875, 0.85546875, 0.9921875, 0.72265625, 0.9921875, 0.67578125, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.70703125, 0.9921875, 0.765625, 0.0, 0.12109375, 0.6796875, 0.2109375, 0.8125, 0.9921875, 0.8671875, 0.15234375, 0.85546875, 0.78125, 0.8671875, 0.9921875, 0.09765625, 0.79296875, 0.99609375, 0.9921875, 0.99609375, 0.82421875, 0.5859375, 0.8203125, 0.0, 0.66796875, 0.875, 0.875, 0.0, 0.9921875, 0.875, 0.99609375, 0.73828125, 0.98828125, 0.05859375, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.796875, 0.765625, 0.91015625, 0.9921875, 0.83203125, 0.80078125, 0.99609375, 0.78125, 0.84375, 0.0, 0.875, 0.84375, 0.0, 0.98828125, 0.0, 0.8515625, 0.98828125, 0.76953125, 0.875, 0.80078125, 0.71484375, 0.875, 0.9921875, 0.875, 0.796875, 0.0, 0.8203125, 0.28515625, 0.99609375, 0.87109375, 0.0, 0.8515625, 0.875, 0.0, 0.0, 0.8203125, 0.82421875, 0.10546875, 0.875, 0.99609375, 0.9921875, 0.3984375, 0.68359375, 0.8359375, 0.99609375, 0.9921875, 0.85546875, 0.98828125, 0.9921875, 0.875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.796875, 0.9921875, 0.1953125, 0.99609375, 0.875, 0.875, 0.96484375, 0.875, 0.84765625, 0.875, 0.84765625, 0.83203125, 0.875, 0.8046875, 0.19921875, 0.9921875, 0.9921875, 0.8046875, 0.99609375, 0.9921875, 0.9921875, 0.80859375, 0.82421875, 0.98828125, 0.98828125, 0.0, 0.0234375, 0.0, 0.80078125, 0.9921875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.23046875, 0.0, 0.8203125, 0.74609375, 0.99609375, 0.98828125, 0.875, 0.875, 0.07421875, 0.83203125, 0.875, 0.9921875, 0.99609375, 0.078125, 0.9921875, 0.875, 0.9921875, 0.80859375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.03515625, 0.0, 0.8203125, 0.82421875, 0.98828125, 0.0234375, 0.9921875, 0.8984375, 0.07421875, 0.9921875, 0.9921875, 0.3984375, 0.0, 0.9921875, 0.078125, 0.41796875, 0.875, 0.9140625, 0.9921875, 0.84375, 0.0234375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.05078125, 0.99609375, 0.8203125, 0.828125, 0.03515625, 0.9921875, 0.0, 0.8671875, 0.8125, 0.7734375, 0.984375, 0.875, 0.9921875, 0.0, 0.0703125, 0.99609375, 0.0, 0.9375, 0.85546875, 0.72265625, 0.875, 0.0, 0.05078125, 0.14453125, 0.84375, 0.0, 0.875, 0.875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.23046875, 0.09765625, 0.38671875, 0.0, 0.98828125, 0.9921875, 0.828125, 0.91015625, 0.8203125, 0.99609375, 0.9921875, 0.875, 0.8203125, 0.3046875, 0.03515625, 0.0, 0.9921875, 0.1796875, 0.921875, 0.0, 0.99609375, 0.984375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.93359375, 0.0, 0.0, 0.9921875, 0.859375, 0.9921875, 0.14453125, 0.7734375, 0.58984375, 0.99609375, 0.078125, 0.7734375, 0.9453125, 0.8515625, 0.99609375, 0.99609375, 0.04296875, 0.828125, 0.85546875, 0.8671875, 0.0, 0.80859375, 0.875, 0.87109375, 0.0, 0.73046875, 0.9921875, 0.99609375, 0.96875, 0.98828125, 0.0, 0.0, 0.9921875, 0.21875, 0.2734375, 0.8125, 0.0703125, 0.9921875, 0.98828125, 0.0, 0.34765625, 0.9921875, 0.0703125, 0.0, 0.875, 0.80078125, 0.0859375, 0.73828125, 0.8359375, 0.09375, 0.0, 0.99609375, 0.0, 0.06640625, 0.875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.8671875, 0.6796875, 0.0, 0.8828125, 0.9921875, 0.99609375, 0.84375, 0.9921875, 0.99609375, 0.10546875, 0.08984375, 0.875, 0.875, 0.0, 0.0, 0.98828125, 0.19921875, 0.9921875, 0.12109375, 0.99609375, 0.79296875, 0.2421875, 0.9921875, 0.875, 0.765625, 0.99609375, 0.875, 0.8671875, 0.9921875, 0.0546875, 0.80078125, 0.74609375, 0.0, 0.66796875, 0.0, 0.0703125, 0.9921875, 0.875, 0.0, 0.9921875, 0.8359375, 0.859375, 0.83984375, 0.98828125, 0.77734375, 0.0, 0.046875, 0.01171875, 0.12890625, 0.7265625, 0.0, 0.828125, 0.0, 0.84375, 0.203125, 0.9921875, 0.76171875, 0.875, 0.7890625, 0.0, 0.78125, 0.77734375, 0.81640625, 0.99609375, 0.9921875, 0.83203125, 0.875, 0.69921875, 0.09765625, 0.42578125, 0.85546875, 0.0, 0.72265625, 0.74609375, 0.85546875, 0.875, 0.9921875, 0.8515625, 0.0, 0.9921875, 0.04296875, 0.7734375, 0.98828125, 0.9921875, 0.9921875, 0.87109375, 0.8359375, 0.875, 0.78515625, 0.98828125, 0.859375, 0.80859375, 0.76171875, 0.9921875, 0.84375, 0.9921875, 0.0859375, 0.9921875, 0.875, 0.9921875, 0.9921875, 0.07421875, 0.8671875, 0.2734375, 0.9921875, 0.98828125, 0.84765625, 0.77734375, 0.81640625, 0.9921875, 0.875, 0.875, 0.0, 0.0, 0.98828125, 0.859375, 0.0, 0.796875, 0.03515625, 0.0, 0.0, 0.9921875, 0.15625, 0.9921875, 0.99609375, 0.15625, 0.9921875, 0.83203125, 0.83984375, 0.875, 0.859375, 0.78515625, 0.9921875, 0.109375, 0.98828125, 0.2578125, 0.875, 0.0, 0.83203125, 0.73828125, 0.1484375, 0.9921875, 0.02734375, 0.84375, 0.1796875, 0.7734375, 0.9921875, 0.7890625, 0.9921875, 0.875, 0.9921875, 0.875, 0.0, 0.7421875, 0.7890625, 0.9921875, 0.875, 0.07421875, 0.86328125, 0.9921875, 0.9921875, 0.99609375, 0.875, 0.828125, 0.81640625, 0.75, 0.0, 0.84765625, 0.9921875, 0.875, 0.87109375, 0.9921875, 0.0703125, 0.95703125, 0.7421875, 0.875, 0.99609375, 0.875, 0.09765625, 0.0, 0.9921875, 0.875, 0.9921875, 0.0, 0.99609375, 0.98828125, 0.875, 0.875, 0.9921875, 0.0, 0.125, 0.99609375, 0.99609375, 0.875, 0.09765625, 0.9921875, 0.875, 0.9921875, 0.9921875, 0.8203125, 0.828125, 0.875, 0.99609375, 0.875, 0.98828125, 0.875, 0.99609375, 0.9921875, 0.83203125, 0.9921875, 0.74609375, 0.81640625, 0.8125, 0.0, 0.875, 0.9921875, 0.875, 0.9921875, 0.90234375, 0.04296875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0703125, 0.73046875, 0.79296875, 0.8671875, 0.875, 0.99609375, 0.99609375, 0.875, 0.79296875, 0.9921875, 0.99609375, 0.74609375, 0.0, 0.8125, 0.0, 0.99609375, 0.75, 0.875, 0.83203125, 0.8046875, 0.78515625, 0.14453125, 0.82421875, 0.8671875, 0.9921875, 0.75390625, 0.99609375, 0.82421875, 0.828125, 0.875, 0.0, 0.82421875]

 sparsity of   [0.0107421875, 0.0576171875, 0.826171875, 0.1689453125, 0.048828125, 0.0, 0.998046875, 0.0166015625, 0.041015625, 0.0537109375, 0.0830078125, 0.130859375, 0.0, 0.09375, 0.0224609375, 0.017578125, 0.0341796875, 0.0830078125, 0.0146484375, 0.1064453125, 0.10546875, 0.0419921875, 0.015625, 0.109375, 0.029296875, 0.041015625, 0.9970703125, 0.0, 0.0126953125, 0.0, 0.0, 0.046875, 0.033203125, 0.04296875, 0.0615234375, 0.095703125, 0.1474609375, 0.0, 0.4296875, 0.021484375, 0.0859375, 0.0, 0.0, 0.1611328125, 0.0263671875, 0.0107421875, 0.775390625, 0.0390625, 0.03125, 0.69921875, 0.29296875, 0.0126953125, 0.0068359375, 0.9970703125, 0.0, 0.025390625, 0.9990234375, 0.0, 0.0654296875, 0.14453125, 0.021484375, 0.0, 0.0, 0.689453125, 0.015625, 0.056640625, 0.005859375, 0.068359375, 0.0, 0.0390625, 0.0, 0.09375, 0.08984375, 0.017578125, 0.833984375, 0.0185546875, 0.8095703125, 0.091796875, 0.0166015625, 0.07421875, 0.0400390625, 0.017578125, 0.8642578125, 0.146484375, 0.0, 0.0, 0.822265625, 0.0859375, 0.7900390625, 0.0, 0.6455078125, 0.1787109375, 0.0, 0.9990234375, 0.0126953125, 0.021484375, 0.1259765625, 0.048828125, 0.4560546875, 0.392578125, 0.2041015625, 0.01953125, 0.0, 0.759765625, 0.0400390625, 0.0, 0.1591796875, 0.15234375, 0.0, 0.162109375, 0.0, 0.0302734375, 0.0185546875, 0.01953125, 0.0, 0.0029296875, 0.9990234375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0224609375, 0.017578125, 0.9990234375, 0.013671875, 0.1025390625, 0.0537109375, 0.1455078125, 0.169921875, 0.0419921875, 0.583984375, 0.0078125, 0.0, 0.0625, 0.0224609375, 0.0234375, 0.9970703125, 0.01953125, 0.021484375, 0.0166015625, 0.0458984375, 0.2275390625, 0.9970703125, 0.046875, 0.0625, 0.0, 0.0302734375, 0.0732421875, 0.021484375, 0.0498046875, 0.0361328125, 0.0576171875, 0.7392578125, 0.0, 0.5283203125, 0.0, 0.0107421875, 0.0283203125, 0.0185546875, 0.9970703125, 0.0302734375, 0.0, 0.021484375, 0.1513671875, 0.89453125, 0.0, 0.0537109375, 0.01171875, 0.205078125, 0.0234375, 0.0400390625, 0.1611328125, 0.0009765625, 0.6806640625, 0.0, 0.09765625, 0.017578125, 0.0205078125, 0.017578125, 0.0, 0.013671875, 0.6650390625, 0.0, 0.0244140625, 0.072265625, 0.01953125, 0.02734375, 0.021484375, 0.0732421875, 0.0, 0.1826171875, 0.0, 0.9228515625, 0.0185546875, 0.7724609375, 0.0234375, 0.1640625, 0.02734375, 0.0595703125, 0.171875, 0.0361328125, 0.0107421875, 0.892578125, 0.0517578125, 0.123046875, 0.025390625, 0.9990234375, 0.0, 0.021484375, 0.009765625, 0.236328125, 0.05859375, 0.046875, 0.138671875, 0.04296875, 0.0263671875, 0.2021484375, 0.01171875, 0.0625, 0.0498046875, 0.0244140625, 0.033203125, 0.119140625, 0.03125, 0.1865234375, 0.01953125, 0.0, 0.03125, 0.01171875, 0.0107421875, 0.1201171875, 0.06640625, 0.0234375, 0.078125, 0.0751953125, 0.0, 0.0380859375, 0.126953125, 0.095703125, 0.009765625, 0.009765625, 0.0966796875, 0.015625, 0.0673828125, 0.0576171875, 0.7236328125, 0.845703125, 0.0390625, 0.0283203125, 0.0, 0.0, 0.0390625, 0.02734375, 0.7109375, 0.125]

 sparsity of   [0.1319444477558136, 0.05078125, 0.04296875, 0.0086805559694767, 0.0625, 0.2378472238779068, 0.0251736119389534, 0.0, 0.03125, 0.1341145783662796, 0.0486111119389534, 0.0659722238779068, 0.1961805522441864, 0.0364583320915699, 0.0, 0.3407118022441864, 0.0386284738779068, 0.2517361044883728, 0.03515625, 0.0078125, 0.102430559694767, 0.3012152910232544, 0.01909722201526165, 0.0768229141831398, 0.9995659589767456, 0.0368923619389534, 0.8732638955116272, 0.0, 0.0, 0.1623263955116272, 0.0959201380610466, 0.3845486044883728, 0.0967881977558136, 0.02387152798473835, 0.0, 0.08984375, 0.090711809694767, 0.0, 0.03515625, 0.01909722201526165, 0.0, 0.0460069440305233, 0.1328125, 0.0394965298473835, 0.0, 0.0512152798473835, 0.0859375, 0.1072048619389534, 0.02690972201526165, 0.8359375, 0.013888888992369175, 0.0442708320915699, 0.0264756940305233, 0.007378472480922937, 0.010416666977107525, 0.3519965410232544, 0.0434027798473835, 0.0355902798473835, 0.02994791604578495, 0.1610243022441864, 0.0490451380610466, 0.078125, 0.0334201380610466, 0.1176215261220932, 0.014322916977107525, 0.0546875, 0.0772569477558136, 0.0737847238779068, 0.009114583022892475, 0.6901041865348816, 0.0, 0.0, 0.04296875, 0.9348958134651184, 0.0, 0.02213541604578495, 0.02604166604578495, 0.02473958395421505, 0.04296875, 0.04296875, 0.0611979179084301, 0.0815972238779068, 0.0, 0.0303819440305233, 0.1315104216337204, 0.0243055559694767, 0.006076388992369175, 0.54296875, 0.8016493320465088, 0.9995659589767456, 0.8250868320465088, 0.0373263880610466, 0.24609375, 0.1263020783662796, 0.1041666641831398, 0.01779513992369175, 0.0, 0.999131977558136, 0.0321180559694767, 0.0329861119389534, 0.121961809694767, 0.09765625, 0.3450520932674408, 0.0065104165114462376, 0.0243055559694767, 0.09375, 0.02690972201526165, 0.013454861007630825, 0.2131076455116272, 0.8776041865348816, 0.8125, 0.0520833320915699, 0.0013020833721384406, 0.02690972201526165, 0.0065104165114462376, 0.02604166604578495, 0.0802951380610466, 0.0, 0.0434027798473835, 0.0338541679084301, 0.03515625, 0.0707465261220932, 0.13671875, 0.0451388880610466, 0.2586805522441864, 0.0, 0.0447048619389534, 0.0794270858168602, 0.1158854141831398, 0.0707465261220932, 0.0407986119389534, 0.1362847238779068, 0.0850694477558136, 0.0425347238779068, 0.0416666679084301, 0.0729166641831398, 0.0334201380610466, 0.0282118059694767, 0.0733506977558136, 0.0434027798473835, 0.1232638880610466, 0.01519097201526165, 0.0564236119389534, 0.0698784738779068, 0.02864583395421505, 0.0551215298473835, 0.013454861007630825, 0.0442708320915699, 0.0, 0.01822916604578495, 0.0, 0.0, 0.1163194477558136, 0.00434027798473835, 0.094618059694767, 0.0, 0.0412326380610466, 0.1167534738779068, 0.0542534738779068, 0.1202256977558136, 0.253472238779068, 0.1206597238779068, 0.0243055559694767, 0.0616319440305233, 0.0625, 0.0607638880610466, 0.0, 0.0772569477558136, 0.0, 0.3641493022441864, 0.00824652798473835, 0.0720486119389534, 0.0329861119389534, 0.0551215298473835, 0.0598958320915699, 0.4418402910232544, 0.0, 0.3424479067325592, 0.0772569477558136, 0.1163194477558136, 0.7478298544883728, 0.0355902798473835, 0.9114583134651184, 0.08984375, 0.0316840298473835, 0.01692708395421505, 0.0677083358168602, 0.01953125, 0.0442708320915699, 0.0125868059694767, 0.0, 0.0538194440305233, 0.01128472201526165, 0.014756944961845875, 0.02170138992369175, 0.0481770820915699, 0.0, 0.5277777910232544, 0.0, 0.16015625, 0.0381944440305233, 0.0668402761220932, 0.01519097201526165, 0.0577256940305233, 0.02387152798473835, 0.0399305559694767, 0.4353298544883728, 0.071180559694767, 0.0703125, 0.9995659589767456, 0.0364583320915699, 0.0, 0.02690972201526165, 0.007378472480922937, 0.0447048619389534, 0.06640625, 0.0407986119389534, 0.1098090261220932, 0.0203993059694767, 0.0203993059694767, 0.1575520783662796, 0.0481770820915699, 0.075086809694767, 0.007378472480922937, 0.1497395783662796, 0.0, 0.0425347238779068, 0.0703125, 0.0390625, 0.8736979365348816, 0.3715277910232544, 0.0677083358168602, 0.04296875, 0.0377604179084301, 0.9995659589767456, 0.0, 0.0724826380610466, 0.0, 0.0590277798473835, 0.01909722201526165, 0.0499131940305233, 0.0928819477558136, 0.0, 0.0924479141831398, 0.7964409589767456, 0.02387152798473835, 0.0598958320915699, 0.0611979179084301, 0.0503472238779068, 0.0533854179084301, 0.0473090298473835, 0.02473958395421505, 0.1653645783662796, 0.01692708395421505, 0.8250868320465088, 0.0386284738779068]

 sparsity of   [0.0078125, 0.0, 0.078125, 0.91796875, 0.109375, 0.05859375, 0.671875, 0.2265625, 0.04296875, 0.0234375, 0.5703125, 0.11328125, 0.9921875, 0.9921875, 0.07421875, 0.73828125, 0.875, 0.99609375, 0.0234375, 0.046875, 0.9921875, 0.0859375, 0.484375, 0.65625, 0.59375, 0.171875, 0.078125, 0.5859375, 0.53515625, 0.02734375, 0.0, 0.578125, 0.62890625, 0.55078125, 0.55078125, 0.99609375, 0.0, 0.46484375, 0.8203125, 0.0, 0.03515625, 0.09765625, 0.9921875, 0.87109375, 0.9921875, 0.67578125, 0.5859375, 0.58984375, 0.08203125, 0.01953125, 0.08203125, 0.47265625, 0.875, 0.0234375, 0.05859375, 0.0, 0.02734375, 0.60546875, 0.03125, 0.3828125, 0.890625, 0.06640625, 0.75, 0.5078125, 0.12109375, 0.5625, 0.6171875, 0.0234375, 0.9921875, 0.0, 0.4921875, 0.4921875, 0.53515625, 0.26953125, 0.8984375, 0.5859375, 0.75390625, 0.61328125, 0.47265625, 0.0625, 0.875, 0.47265625, 0.5625, 0.0390625, 0.2734375, 0.80859375, 0.671875, 0.05859375, 0.875, 0.0, 0.98828125, 0.109375, 0.05859375, 0.9921875, 0.0234375, 0.828125, 0.609375, 0.3125, 0.7109375, 0.62109375, 0.6796875, 0.67578125, 0.09375, 0.3984375, 0.88671875, 0.0078125, 0.52734375, 0.9921875, 0.5078125, 0.0, 0.0546875, 0.6015625, 0.57421875, 0.5078125, 0.90625, 0.98828125, 0.0, 0.50390625, 0.546875, 0.91796875, 0.109375, 0.7421875, 0.5859375, 0.01953125, 0.0625, 0.0, 0.734375, 0.5390625, 0.53515625, 0.0703125, 0.078125, 0.48046875, 0.17578125, 0.05078125, 0.0234375, 0.07421875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.875, 0.6640625, 0.9921875, 0.0078125, 0.06640625, 0.51171875, 0.55859375, 0.6796875, 0.51953125, 0.03125, 0.875, 0.0390625, 0.6484375, 0.94921875, 0.0703125, 0.9921875, 0.71484375, 0.50390625, 0.87109375, 0.07421875, 0.875, 0.03125, 0.0, 0.56640625, 0.41796875, 0.0, 0.40234375, 0.77734375, 0.7734375, 0.87109375, 0.0, 0.0, 0.01171875, 0.54296875, 0.0, 0.0234375, 0.6640625, 0.0546875, 0.05078125, 0.4921875, 0.1640625, 0.640625, 0.04296875, 0.0, 0.48828125, 0.98828125, 0.30859375, 0.41015625, 0.55078125, 0.609375, 0.0, 0.19921875, 0.0, 0.15625, 0.0859375, 0.0, 0.390625, 0.75, 0.55078125, 0.05859375, 0.5546875, 0.2890625, 0.0234375, 0.73828125, 0.875, 0.5078125, 0.9921875, 0.0234375, 0.171875, 0.578125, 0.8515625, 0.109375, 0.48828125, 0.26953125, 0.5078125, 0.0078125, 0.93359375, 0.6015625, 0.03125, 0.40625, 0.65234375, 0.9921875, 0.13671875, 0.66015625, 0.0390625, 0.0546875, 0.08203125, 0.875, 0.07421875, 0.09765625, 0.62109375, 0.08203125, 0.57421875, 0.68359375, 0.625, 0.0859375, 0.92578125, 0.0, 0.6171875, 0.7109375, 0.484375, 0.46875, 0.46875, 0.9921875, 0.6171875, 0.546875, 0.00390625, 0.5546875, 0.015625, 0.0, 0.53515625, 0.0, 0.875, 0.55859375, 0.59765625, 0.0, 0.47265625, 0.01171875, 0.01171875, 0.0, 0.8984375, 0.49609375, 0.015625, 0.01171875, 0.578125, 0.4921875, 0.57421875, 0.0625, 0.99609375, 0.52734375, 0.4296875, 0.5, 0.54296875, 0.5078125, 0.98828125, 0.0, 0.5234375, 0.0234375, 0.0234375, 0.51953125, 0.0625, 0.53515625, 0.49609375, 0.05078125, 0.55078125, 0.0, 0.55859375, 0.65625, 0.875, 0.99609375, 0.0, 0.63671875, 0.0, 0.0, 0.0, 0.16015625, 0.91796875, 0.04296875, 0.0078125, 0.98828125, 0.6328125, 0.07421875, 0.05078125, 0.0, 0.0, 0.1953125, 0.12109375, 0.0, 0.0, 0.9921875, 0.4453125, 0.49609375, 0.99609375, 0.83203125, 0.45703125, 0.61328125, 0.7578125, 0.6875, 0.46484375, 0.02734375, 0.046875, 0.98828125, 0.046875, 0.12109375, 0.9921875, 0.9765625, 0.5, 0.48046875, 0.046875, 0.578125, 0.484375, 0.875, 0.0, 0.875, 0.67578125, 0.875, 0.48828125, 0.99609375, 0.67578125, 0.0, 0.4609375, 0.9921875, 0.0, 0.203125, 0.0390625, 0.9921875, 0.875, 0.1171875, 0.23046875, 0.13671875, 0.9921875, 0.0, 0.93359375, 0.5625, 0.9921875, 0.62890625, 0.62109375, 0.69921875, 0.10546875, 0.69140625, 0.609375, 0.0234375, 0.63671875, 0.9921875, 0.41015625, 0.8984375, 0.9921875, 0.69140625, 0.53125, 0.72265625, 0.5234375, 0.5546875, 0.1015625, 0.5859375, 0.875, 0.890625, 0.546875, 0.03125, 0.63671875, 0.73046875, 0.9921875, 0.02734375, 0.03125, 0.0, 0.54296875, 0.5078125, 0.59375, 0.71875, 0.5078125, 0.078125, 0.99609375, 0.0, 0.0859375, 0.12890625, 0.55078125, 0.30078125, 0.46875, 0.0, 0.6015625, 0.51953125, 0.875, 0.6015625, 0.54296875, 0.82421875, 0.51171875, 0.01171875, 0.859375, 0.0625, 0.0, 0.015625, 0.9921875, 0.55078125, 0.484375, 0.51171875, 0.53125, 0.66015625, 0.64453125, 0.54296875, 0.6640625, 0.6171875, 0.609375, 0.875, 0.05859375, 0.65234375, 0.62109375, 0.5390625, 0.4375, 0.55859375, 0.08984375, 0.875, 0.4609375, 0.48046875, 0.5, 0.68359375, 0.53515625, 0.640625, 0.03125, 0.125, 0.671875, 0.4609375, 0.09375, 0.48046875, 0.50390625, 0.3046875, 0.77734375, 0.875, 0.44140625, 0.71484375, 0.1640625, 0.92578125, 0.48828125, 0.0, 0.484375, 0.0, 0.9921875, 0.08984375, 0.51953125, 0.55859375, 0.6796875, 0.06640625, 0.58203125, 0.48828125, 0.05859375, 0.98828125, 0.046875, 0.51953125, 0.515625, 0.8984375, 0.546875, 0.87109375, 0.9921875, 0.0234375, 0.453125, 0.0546875, 0.51171875, 0.48828125, 0.6015625, 0.9921875, 0.63671875, 0.01953125, 0.01953125, 0.03125, 0.28515625, 0.59765625, 0.13671875, 0.51953125, 0.65234375, 0.13671875, 0.91015625, 0.625, 0.61328125, 0.875, 0.671875, 0.875, 0.77734375, 0.6328125, 0.0, 0.4140625, 0.53515625, 0.05859375, 0.02734375, 0.0390625, 0.0, 0.0, 0.5, 0.9921875, 0.8671875, 0.69140625, 0.54296875, 0.6484375, 0.52734375, 0.98828125, 0.75, 0.54296875, 0.44921875, 0.98828125, 0.69140625, 0.0, 0.875, 0.73046875, 0.0, 0.55859375, 0.875, 0.50390625, 0.66015625, 0.4765625, 0.9921875, 0.9921875, 0.48828125, 0.51171875, 0.6875, 0.625, 0.49609375, 0.0, 0.89453125, 0.01953125, 0.015625, 0.05859375, 0.0, 0.765625, 0.54296875, 0.54296875, 0.0, 0.71875, 0.0703125, 0.41796875, 0.59765625, 0.3984375, 0.00390625, 0.875, 0.48046875, 0.60546875, 0.02734375, 0.109375, 0.578125, 0.875, 0.046875, 0.109375, 0.578125, 0.5234375, 0.03125, 0.98828125, 0.49609375, 0.48828125, 0.0703125, 0.6328125, 0.6015625, 0.078125, 0.1875, 0.4921875, 0.5703125, 0.58203125, 0.59375, 0.875, 0.69921875, 0.546875, 0.8203125, 0.01953125, 0.9921875, 0.0859375, 0.0390625, 0.875, 0.07421875, 0.59375, 0.6328125, 0.9921875, 0.296875, 0.0859375, 0.62109375, 0.0, 0.109375, 0.01171875, 0.05859375, 0.9921875, 0.58203125, 0.48046875, 0.55859375, 0.95703125, 0.875, 0.1484375, 0.640625, 0.9921875, 0.01953125, 0.0, 0.609375, 0.07421875, 0.9921875, 0.59375, 0.828125, 0.90625, 0.04296875, 0.6328125, 0.8203125, 0.9921875, 0.9921875, 0.15234375, 0.63671875, 0.046875, 0.5703125, 0.0, 0.8203125, 0.98828125, 0.64453125, 0.546875, 0.74609375, 0.82421875, 0.16015625, 0.5859375, 0.9296875, 0.61328125, 0.625, 0.08984375, 0.05859375, 0.75, 0.875, 0.9921875, 0.8828125, 0.9921875, 0.1328125, 0.6328125, 0.53515625, 0.68359375, 0.9921875, 0.5859375, 0.8984375, 0.91015625, 0.06640625, 0.91015625, 0.67578125, 0.59375, 0.71875, 0.58203125, 0.7421875, 0.15625, 0.00390625, 0.6640625, 0.67578125, 0.61328125, 0.890625, 0.53515625, 0.578125, 0.0, 0.00390625, 0.8671875, 0.0390625, 0.5625, 0.54296875, 0.875, 0.875, 0.3828125, 0.86328125, 0.66015625, 0.75390625, 0.0, 0.546875, 0.046875, 0.625, 0.6640625, 0.046875, 0.9921875, 0.51953125, 0.0703125, 0.9921875, 0.1484375, 0.4765625, 0.52734375, 0.875, 0.05859375, 0.609375, 0.55078125, 0.64453125, 0.875, 0.45703125, 0.0, 0.22265625, 0.0, 0.078125, 0.9921875, 0.58984375, 0.0, 0.08203125, 0.01171875, 0.00390625, 0.41015625, 0.02734375, 0.38671875, 0.453125, 0.6796875, 0.07421875, 0.875, 0.9921875, 0.671875, 0.9921875, 0.91015625, 0.453125, 0.875, 0.5, 0.9921875, 0.46484375, 0.9921875, 0.61328125, 0.80859375, 0.44140625, 0.9921875, 0.5859375, 0.56640625, 0.7109375, 0.875, 0.9921875, 0.51953125, 0.50390625, 0.0, 0.02734375, 0.59765625, 0.9921875, 0.9921875, 0.3828125, 0.3984375, 0.0, 0.0859375, 0.0625, 0.5, 0.671875, 0.84375, 0.57421875, 0.41796875, 0.0, 0.58203125, 0.84765625, 0.875, 0.19921875, 0.6484375, 0.04296875, 0.8203125, 0.484375, 0.8125, 0.875, 0.09375, 0.875, 0.0, 0.7578125, 0.875, 0.0546875, 0.9921875, 0.66015625, 0.1171875, 0.59375, 0.140625, 0.67578125, 0.57421875, 0.34375, 0.9921875, 0.0625, 0.58203125, 0.61328125, 0.0859375, 0.7109375, 0.390625, 0.9921875, 0.5625, 0.69921875, 0.0, 0.0, 0.03515625, 0.625, 0.51171875, 0.27734375, 0.03515625, 0.65234375, 0.87109375, 0.05859375, 0.60546875, 0.99609375, 0.03515625, 0.57421875, 0.046875, 0.99609375, 0.9140625, 0.03125, 0.51171875, 0.0234375, 0.33984375, 0.47265625, 0.07421875, 0.21875, 0.49609375, 0.05078125, 0.02734375, 0.6875, 0.9921875, 0.99609375, 0.61328125, 0.05859375, 0.0, 0.9921875, 0.0, 0.90625, 0.796875, 0.41015625, 0.72265625, 0.6796875, 0.875, 0.05078125, 0.0546875, 0.03125, 0.58203125, 0.578125, 0.5078125, 0.54296875, 0.1171875, 0.4921875, 0.71484375, 0.6484375, 0.9921875, 0.53515625, 0.9453125, 0.609375, 0.640625, 0.5859375, 0.30859375, 0.56640625, 0.58203125, 0.6015625, 0.59765625, 0.171875, 0.875, 0.875, 0.5859375, 0.67578125, 0.9921875, 0.9921875, 0.078125, 0.05859375, 0.20703125, 0.53125, 0.765625, 0.4375, 0.046875, 0.73828125, 0.4921875, 0.60546875, 0.9921875, 0.44140625, 0.9921875, 0.5703125, 0.890625, 0.5703125, 0.68359375, 0.9921875, 0.05859375, 0.875, 0.63671875, 0.546875, 0.0546875, 0.59765625, 0.66796875, 0.6015625, 0.72265625, 0.875, 0.70703125, 0.0, 0.0, 0.45703125, 0.69140625, 0.875, 0.12109375, 0.68359375, 0.87109375, 0.578125, 0.9921875, 0.109375, 0.80859375, 0.0703125, 0.16796875, 0.9921875, 0.0703125, 0.52734375, 0.6875, 0.34375, 0.01953125, 0.0390625, 0.03125, 0.6171875, 0.5078125, 0.49609375, 0.66796875, 0.6640625, 0.59375, 0.07421875, 0.08984375, 0.9921875, 0.65234375, 0.671875, 0.5546875, 0.67578125, 0.0859375, 0.9921875, 0.56640625, 0.046875, 0.6171875, 0.875, 0.9921875, 0.5234375, 0.66015625, 0.57421875, 0.78125, 0.4921875, 0.2890625, 0.7734375, 0.02734375, 0.078125, 0.9921875, 0.71875, 0.87109375, 0.0, 0.9921875, 0.04296875, 0.5, 0.359375, 0.546875, 0.5234375, 0.03515625, 0.73046875, 0.81640625, 0.9921875, 0.58203125, 0.8125, 0.0, 0.0546875, 0.875, 0.9921875, 0.078125, 0.06640625, 0.859375, 0.62109375, 0.63671875, 0.9921875, 0.0, 0.828125, 0.83203125, 0.546875, 0.875, 0.0546875, 0.73046875, 0.57421875, 0.05859375, 0.875, 0.5625, 0.4765625, 0.875, 0.0078125, 0.6015625, 0.03125, 0.703125, 0.5390625, 0.1484375, 0.51953125, 0.08984375, 0.9921875, 0.99609375, 0.6328125, 0.0, 0.6484375, 0.4765625, 0.875, 0.75, 0.46484375, 0.80078125, 0.9921875, 0.04296875, 0.0, 0.03125, 0.11328125, 0.609375, 0.04296875, 0.57421875, 0.01953125, 0.9921875, 0.046875, 0.87109375, 0.9921875, 0.5546875, 0.18359375, 0.48828125, 0.6171875, 0.51953125, 0.0, 0.9921875, 0.43359375, 0.875, 0.1640625, 0.06640625, 0.82421875, 0.01171875, 0.9921875, 0.51171875, 0.67578125, 0.69140625, 0.9921875, 0.02734375, 0.46484375, 0.70703125, 0.09375, 0.87109375]

 sparsity of   [0.0390625, 0.0537109375, 0.10546875, 0.810546875, 0.1767578125, 0.9970703125, 0.05859375, 0.1103515625, 0.1171875, 0.0341796875, 0.998046875, 0.025390625, 0.908203125, 0.0732421875, 0.064453125, 0.9560546875, 0.0673828125, 0.998046875, 0.0087890625, 0.8525390625, 0.998046875, 0.5908203125, 0.8427734375, 0.7939453125, 0.9306640625, 0.08203125, 0.1455078125, 0.9423828125, 0.11328125, 0.9970703125, 0.0068359375, 0.0263671875, 0.998046875, 0.998046875, 0.9990234375, 0.0986328125, 0.998046875, 0.0615234375, 0.0859375, 0.0146484375, 0.0390625, 0.0107421875, 0.91015625, 0.064453125, 0.9013671875, 0.044921875, 0.9306640625, 0.0546875, 0.0166015625, 0.998046875, 0.8935546875, 0.083984375, 0.8984375, 0.8896484375, 0.03125, 0.2958984375, 0.009765625, 0.0703125, 0.021484375, 0.095703125, 0.1142578125, 0.9970703125, 0.0185546875, 0.123046875, 0.9970703125, 0.1240234375, 0.083984375, 0.0341796875, 0.03515625, 0.998046875, 0.912109375, 0.8134765625, 0.015625, 0.7763671875, 0.8994140625, 0.0537109375, 0.033203125, 0.0419921875, 0.365234375, 0.005859375, 0.4560546875, 0.0224609375, 0.04296875, 0.0224609375, 0.90234375, 0.931640625, 0.1552734375, 0.9970703125, 0.9970703125, 0.9970703125, 0.1201171875, 0.1103515625, 0.9033203125, 0.0791015625, 0.017578125, 0.1455078125, 0.9970703125, 0.998046875, 0.0302734375, 0.1533203125, 0.048828125, 0.021484375, 0.8486328125, 0.0234375, 0.998046875, 0.998046875, 0.998046875, 0.0625, 0.013671875, 0.015625, 0.02734375, 0.759765625, 0.01953125, 0.0859375, 0.01953125, 0.8193359375, 0.8623046875, 0.9326171875, 0.892578125, 0.0849609375, 0.998046875, 0.068359375, 0.806640625, 0.0263671875, 0.9970703125, 0.06640625, 0.09765625, 0.0439453125, 0.0205078125, 0.998046875, 0.923828125, 0.15625, 0.9970703125, 0.109375, 0.9169921875, 0.044921875, 0.0185546875, 0.0224609375, 0.095703125, 0.998046875, 0.7724609375, 0.0322265625, 0.8046875, 0.0390625, 0.025390625, 0.0341796875, 0.1220703125, 0.1669921875, 0.1259765625, 0.0322265625, 0.1015625, 0.9453125, 0.7880859375, 0.0771484375, 0.0234375, 0.9970703125, 0.927734375, 0.0380859375, 0.998046875, 0.02734375, 0.0478515625, 0.9970703125, 0.998046875, 0.03515625, 0.1708984375, 0.1064453125, 0.060546875, 0.0927734375, 0.998046875, 0.0390625, 0.5556640625, 0.02734375, 0.0693359375, 0.1904296875, 0.11328125, 0.0263671875, 0.08984375, 0.0166015625, 0.8388671875, 0.998046875, 0.0791015625, 0.0703125, 0.0498046875, 0.875, 0.9970703125, 0.171875, 0.0537109375, 0.921875, 0.998046875, 0.0830078125, 0.0947265625, 0.046875, 0.107421875, 0.669921875, 0.177734375, 0.029296875, 0.0263671875, 0.818359375, 0.8271484375, 0.9970703125, 0.1357421875, 0.041015625, 0.162109375, 0.05078125, 0.029296875, 0.0263671875, 0.0244140625, 0.025390625, 0.1865234375, 0.7421875, 0.01953125, 0.6474609375, 0.021484375, 0.0341796875, 0.998046875, 0.447265625, 0.025390625, 0.044921875, 0.6474609375, 0.029296875, 0.0849609375, 0.998046875, 0.07421875, 0.998046875, 0.029296875, 0.07421875, 0.06640625, 0.0263671875, 0.0283203125, 0.0146484375, 0.1552734375, 0.9970703125, 0.912109375, 0.017578125, 0.1748046875, 0.9326171875, 0.01953125, 0.0537109375, 0.0703125, 0.0283203125, 0.7744140625, 0.15625, 0.09765625, 0.0341796875, 0.5029296875, 0.14453125, 0.0556640625, 0.0146484375, 0.0810546875, 0.0576171875, 0.0400390625, 0.0107421875, 0.0146484375, 0.9990234375, 0.001953125, 0.9970703125]

 sparsity of   [0.0360243059694767, 0.9995659589767456, 0.0164930559694767, 0.0607638880610466, 0.05078125, 0.00824652798473835, 0.02560763992369175, 0.02300347201526165, 0.013454861007630825, 0.1623263955116272, 0.0798611119389534, 0.9986979365348816, 0.0473090298473835, 0.02994791604578495, 0.0125868059694767, 0.01909722201526165, 0.010416666977107525, 0.913194477558136, 0.0490451380610466, 0.082899309694767, 0.14453125, 0.010416666977107525, 0.0486111119389534, 0.0824652761220932, 0.010416666977107525, 0.0186631940305233, 0.0473090298473835, 0.02300347201526165, 0.0390625, 0.0355902798473835, 0.02387152798473835, 0.05078125, 0.0264756940305233, 0.0651041641831398, 0.0416666679084301, 0.1098090261220932, 0.0234375, 0.0425347238779068, 0.0438368059694767, 0.1276041716337204, 0.999131977558136, 0.013020833022892475, 0.0998263880610466, 0.0303819440305233, 0.0303819440305233, 0.9995659589767456, 0.0338541679084301, 0.0243055559694767, 0.0625, 0.1263020783662796, 0.00824652798473835, 0.9995659589767456, 0.0203993059694767, 0.0620659738779068, 0.0290798619389534, 0.02560763992369175, 0.0282118059694767, 0.02387152798473835, 0.0651041641831398, 0.0681423619389534, 0.010850694961845875, 0.03125, 0.118055559694767, 0.0052083334885537624, 0.0251736119389534, 0.0325520820915699, 0.0264756940305233, 0.03125, 0.01822916604578495, 0.68359375, 0.009114583022892475, 0.0503472238779068, 0.0416666679084301, 0.6323784589767456, 0.02690972201526165, 0.21875, 0.0086805559694767, 0.1089409738779068, 0.0303819440305233, 0.01909722201526165, 0.0125868059694767, 0.8667534589767456, 0.02560763992369175, 0.87890625, 0.0629340261220932, 0.6067708134651184, 0.02994791604578495, 0.2018229216337204, 0.0533854179084301, 0.0525173619389534, 0.0173611119389534, 0.999131977558136, 0.0069444444961845875, 0.01822916604578495, 0.7973090410232544, 0.02213541604578495, 0.0685763880610466, 0.03515625, 0.015625, 0.1527777761220932, 0.0251736119389534, 0.0377604179084301, 0.05859375, 0.3910590410232544, 0.12890625, 0.02777777798473835, 0.6015625, 0.819444477558136, 0.9986979365348816, 0.06640625, 0.9995659589767456, 0.999131977558136, 0.02387152798473835, 0.0364583320915699, 0.0455729179084301, 0.0334201380610466, 0.999131977558136, 0.0026041667442768812, 0.0394965298473835, 0.234375, 0.086805559694767, 0.999131977558136, 0.01822916604578495, 0.3151041567325592, 0.0885416641831398, 0.9344618320465088, 0.0564236119389534, 0.7161458134651184, 0.1206597238779068, 0.515625, 0.014322916977107525, 0.0225694440305233, 0.0881076380610466, 0.815538227558136, 0.878038227558136, 0.2109375, 0.0373263880610466, 0.013454861007630825, 0.0282118059694767, 0.0381944440305233, 0.078993059694767, 0.8407118320465088, 0.6362847089767456, 0.8832465410232544, 0.9995659589767456, 0.01996527798473835, 0.0360243059694767, 0.0321180559694767, 0.01605902798473835, 0.0290798619389534, 0.999131977558136, 0.8980034589767456, 0.0442708320915699, 0.0078125, 0.0186631940305233, 0.0412326380610466, 0.02387152798473835, 0.010850694961845875, 0.0030381944961845875, 0.0013020833721384406, 0.999131977558136, 0.1753472238779068, 0.01822916604578495, 0.02473958395421505, 0.0846354141831398, 0.014322916977107525, 0.121961809694767, 0.0325520820915699, 0.8055555820465088, 0.9986979365348816, 0.1427951455116272, 0.02083333395421505, 0.00390625, 0.009982638992369175, 0.7326388955116272, 0.0972222238779068, 0.0164930559694767, 0.1566840261220932, 0.02690972201526165, 0.1488715261220932, 0.134548619389534, 0.999131977558136, 0.0355902798473835, 0.01953125, 0.999131977558136, 0.0282118059694767, 0.01822916604578495, 0.02213541604578495, 0.01779513992369175, 0.7191840410232544, 0.878038227558136, 0.01953125, 0.999131977558136, 0.0368923619389534, 0.02300347201526165, 0.0811631977558136, 0.1189236119389534, 0.0203993059694767, 0.02083333395421505, 0.0434027798473835, 0.013888888992369175, 0.02300347201526165, 0.0243055559694767, 0.1128472238779068, 0.1059027761220932, 0.8138020634651184, 0.0364583320915699, 0.0542534738779068, 0.0342881940305233, 0.999131977558136, 0.00434027798473835, 0.8862847089767456, 0.3220486044883728, 0.015625, 0.0859375, 0.0355902798473835, 0.999131977558136, 0.01779513992369175, 0.1171875, 0.9986979365348816, 0.1263020783662796, 0.0464409738779068, 0.0203993059694767, 0.01171875, 0.5269097089767456, 0.0698784738779068, 0.0329861119389534, 0.1627604216337204, 0.745225727558136, 0.01171875, 0.0438368059694767, 0.0451388880610466, 0.0086805559694767, 0.01909722201526165, 0.0798611119389534, 0.02213541604578495, 0.0416666679084301, 0.140625, 0.02604166604578495, 0.1636284738779068, 0.0381944440305233, 0.0, 0.4930555522441864, 0.5638020634651184, 0.0559895820915699, 0.5868055820465088, 0.9986979365348816, 0.0203993059694767, 0.0772569477558136, 0.0373263880610466, 0.02864583395421505, 0.02864583395421505, 0.6067708134651184, 0.9995659589767456, 0.013020833022892475, 0.0316840298473835]

 sparsity of   [0.9921875, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.1953125, 0.05078125, 0.99609375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.984375, 0.984375, 0.421875, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.90234375, 0.984375, 0.984375, 0.015625, 0.99609375, 0.984375, 0.984375, 0.98828125, 0.9921875, 0.02734375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.99609375, 0.98046875, 0.984375, 0.98828125, 0.984375, 0.99609375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.984375, 0.98828125, 0.8203125, 0.984375, 0.41015625, 0.9921875, 0.984375, 0.984375, 0.1484375, 0.9921875, 0.9765625, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.859375, 0.99609375, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.984375, 0.05859375, 0.9921875, 0.9921875, 0.98046875, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.984375, 0.9765625, 0.15625, 0.3203125, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.07421875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.23046875, 0.98828125, 0.984375, 0.99609375, 0.984375, 0.98046875, 0.984375, 0.38671875, 0.984375, 0.98046875, 0.98046875, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.984375, 0.9921875, 0.99609375, 0.98046875, 0.09765625, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.3515625, 0.984375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98046875, 0.9765625, 0.9921875, 0.984375, 0.984375, 0.1640625, 0.9921875, 0.98046875, 0.1328125, 0.984375, 0.984375, 0.09375, 0.984375, 0.98828125, 0.13671875, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.0703125, 0.99609375, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.02734375, 0.984375, 0.984375, 0.984375, 0.99609375, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.0703125, 0.984375, 0.984375, 0.078125, 0.99609375, 0.984375, 0.03125, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.2578125, 0.984375, 0.9921875, 0.984375, 0.29296875, 0.9921875, 0.984375, 0.98046875, 0.984375, 0.984375, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.98046875, 0.9921875, 0.98828125, 0.89453125, 0.984375, 0.16796875, 0.98828125, 0.2890625, 0.984375, 0.09765625, 0.9921875, 0.99609375, 0.40625, 0.984375, 0.05078125, 0.98828125, 0.0546875, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.796875, 0.984375, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.9765625, 0.98828125, 0.0234375, 0.09375, 0.0546875, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.984375, 0.99609375, 0.984375, 0.984375, 0.984375, 0.984375, 0.01171875, 0.984375, 0.0625, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.98046875, 0.984375, 0.04296875, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.984375, 0.984375, 0.9921875, 0.01953125, 0.07421875, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.984375, 0.984375, 0.98828125, 0.109375, 0.98828125, 0.44921875, 0.984375, 0.984375, 0.03515625, 0.984375, 0.19921875, 0.984375, 0.0625, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.2578125, 0.98046875, 0.9921875, 0.984375, 0.99609375, 0.9921875, 0.98828125, 0.21484375, 0.09375, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.38671875, 0.98046875, 0.08984375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.984375, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.984375, 0.984375, 0.98046875, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.99609375, 0.046875, 0.984375, 0.7890625, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.02734375, 0.9921875, 0.0546875, 0.98828125, 0.98828125, 0.984375, 0.0859375, 0.99609375, 0.9921875, 0.984375, 0.99609375, 0.984375, 0.9921875, 0.1015625, 0.984375, 0.984375, 0.078125, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.984375, 0.9921875, 0.1015625, 0.98828125, 0.984375, 0.4375, 0.98828125, 0.9921875, 0.98828125, 0.4140625, 0.984375, 0.984375, 0.984375, 0.984375, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.98046875, 0.0234375, 0.34375, 0.08984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.0625, 0.53125, 0.08984375, 0.98046875, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.984375, 0.98046875, 0.99609375, 0.984375, 0.12890625, 0.984375, 0.984375, 0.1171875, 0.984375, 0.98828125, 0.05859375, 0.984375, 0.9921875, 0.984375, 0.08984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.10546875, 0.9921875, 0.99609375, 0.98828125, 0.984375, 0.984375, 0.0859375, 0.984375, 0.984375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.99609375, 0.984375, 0.984375, 0.984375, 0.07421875, 0.98828125, 0.984375, 0.984375, 0.984375, 0.19140625, 0.44140625, 0.984375, 0.98828125, 0.9921875, 0.08984375, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.984375, 0.984375, 0.98828125, 0.0703125, 0.98046875, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.984375, 0.234375, 0.984375, 0.984375, 0.09375, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.54296875, 0.9921875, 0.984375, 0.984375, 0.09375, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.1015625, 0.98828125, 0.984375, 0.99609375, 0.984375, 0.02734375, 0.0234375, 0.98046875, 0.984375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.33203125, 0.984375, 0.984375, 0.984375, 0.98828125, 0.14453125, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.9765625, 0.98828125, 0.99609375, 0.984375, 0.98828125, 0.984375, 0.07421875, 0.48046875, 0.984375, 0.984375, 0.98828125, 0.9921875, 0.07421875, 0.09375, 0.13671875, 0.984375, 0.99609375, 0.03515625, 0.06640625, 0.98828125, 0.359375, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.21484375, 0.984375, 0.87890625, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.01953125, 0.984375, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.1328125, 0.9921875, 0.984375, 0.515625, 0.99609375, 0.4609375, 0.9921875, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.0078125, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.81640625, 0.98828125, 0.10546875, 0.984375, 0.984375, 0.40625, 0.984375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.0859375, 0.1328125, 0.98828125, 0.05859375, 0.1171875, 0.98046875, 0.984375, 0.99609375, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.98046875, 0.98828125, 0.984375, 0.25, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.08203125, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.984375, 0.98046875, 0.14453125, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.984375, 0.984375, 0.9921875, 0.08984375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.984375, 0.078125, 0.99609375, 0.9921875, 0.98046875, 0.984375, 0.984375, 0.99609375, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.984375, 0.98828125, 0.98828125, 0.421875, 0.984375, 0.12890625, 0.99609375, 0.98046875, 0.1484375, 0.984375, 0.984375, 0.98046875, 0.9921875, 0.984375, 0.984375, 0.0859375, 0.984375, 0.30078125, 0.98828125, 0.10546875, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.59765625, 0.98828125, 0.984375, 0.1484375, 0.984375, 0.98828125, 0.984375, 0.3515625, 0.9921875, 0.984375, 0.98828125, 0.98046875, 0.984375, 0.4609375, 0.984375, 0.26953125, 0.11328125, 0.984375, 0.125, 0.98046875, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.07421875, 0.99609375, 0.984375, 0.984375, 0.984375, 0.09375, 0.98828125, 0.99609375, 0.984375, 0.31640625, 0.08203125, 0.50390625, 0.9921875, 0.984375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.14453125, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.140625, 0.80859375, 0.98046875, 0.984375, 0.15234375, 0.984375, 0.98828125, 0.984375, 0.984375, 0.98046875, 0.9921875, 0.984375, 0.984375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.99609375, 0.984375, 0.984375, 0.984375, 0.984375, 0.9921875, 0.0625, 0.98828125, 0.984375, 0.984375, 0.08203125, 0.984375, 0.984375, 0.98828125, 0.27734375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.02734375, 0.98828125, 0.99609375, 0.9921875, 0.98046875, 0.984375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.44140625, 0.984375, 0.984375, 0.8203125, 0.98828125, 0.9921875, 0.98828125, 0.9765625, 0.43359375, 0.984375, 0.9921875, 0.984375, 0.98828125, 0.16015625, 0.98828125, 0.9921875, 0.98046875, 0.98828125, 0.99609375, 0.98046875, 0.9921875, 0.984375, 0.98828125, 0.1015625, 0.984375, 0.35546875, 0.984375, 0.98828125, 0.08984375, 0.98828125, 0.078125, 0.99609375, 0.9921875, 0.98828125, 0.984375, 0.01953125, 0.9921875, 0.40625, 0.984375, 0.0703125, 0.0546875, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.46875, 0.0078125, 0.98828125, 0.98046875, 0.984375, 0.984375, 0.015625, 0.9921875, 0.984375, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.98046875, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.984375, 0.0859375, 0.984375, 0.984375, 0.98828125, 0.984375, 0.984375, 0.98046875, 0.9921875, 0.9921875, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.99609375, 0.98828125, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.99609375, 0.98828125, 0.98828125, 0.98046875, 0.46484375, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.984375, 0.99609375, 0.05859375, 0.984375, 0.9921875, 0.984375, 0.984375, 0.171875, 0.984375, 0.98828125, 0.984375, 0.99609375, 0.11328125, 0.98828125, 0.9921875, 0.984375, 0.171875, 0.984375, 0.99609375, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.98046875, 0.99609375, 0.984375, 0.14453125, 0.98828125, 0.984375, 0.40234375, 0.984375, 0.98828125, 0.984375, 0.03125, 0.984375, 0.9921875, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.26953125, 0.984375, 0.12890625, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.9921875, 0.984375, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.984375, 0.984375, 0.1015625, 0.984375, 0.21875, 0.984375, 0.9921875, 0.984375, 0.984375, 0.03125, 0.98828125, 0.9921875, 0.984375, 0.984375, 0.19140625, 0.98828125, 0.984375, 0.07421875, 0.984375, 0.984375, 0.9921875, 0.984375, 0.98046875, 0.98828125, 0.07421875, 0.984375, 0.984375, 0.984375, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.984375, 0.1015625, 0.984375, 0.984375, 0.99609375, 0.984375, 0.9921875, 0.98828125, 0.17578125, 0.984375, 0.98828125, 0.984375]

 sparsity of   [0.03515625, 0.052734375, 0.998046875, 0.998046875, 0.0283203125, 0.1328125, 0.998046875, 0.05859375, 0.0458984375, 0.9970703125, 0.0849609375, 0.1171875, 0.0263671875, 0.998046875, 0.0634765625, 0.09765625, 0.7900390625, 0.9970703125, 0.107421875, 0.01953125, 0.998046875, 0.0234375, 0.0810546875, 0.048828125, 0.9970703125, 0.0166015625, 0.998046875, 0.162109375, 0.998046875, 0.1005859375, 0.998046875, 0.87890625, 0.044921875, 0.9072265625, 0.1796875, 0.998046875, 0.9970703125, 0.998046875, 0.0263671875, 0.998046875, 0.998046875, 0.94140625, 0.0361328125, 0.0791015625, 0.998046875, 0.9970703125, 0.0380859375, 0.998046875, 0.0244140625, 0.998046875, 0.1279296875, 0.91796875, 0.908203125, 0.1318359375, 0.99609375, 0.998046875, 0.1611328125, 0.2646484375, 0.0439453125, 0.998046875, 0.1630859375, 0.998046875, 0.8310546875, 0.0859375, 0.998046875, 0.7197265625, 0.7490234375, 0.0556640625, 0.1318359375, 0.998046875, 0.998046875, 0.880859375, 0.0234375, 0.1318359375, 0.8505859375, 0.998046875, 0.0234375, 0.041015625, 0.044921875, 0.021484375, 0.021484375, 0.7919921875, 0.9267578125, 0.0322265625, 0.0888671875, 0.0234375, 0.0263671875, 0.892578125, 0.998046875, 0.998046875, 0.0166015625, 0.998046875, 0.998046875, 0.998046875, 0.935546875, 0.021484375, 0.9970703125, 0.033203125, 0.064453125, 0.818359375, 0.9052734375, 0.92578125, 0.8154296875, 0.0791015625, 0.0126953125, 0.78515625, 0.03515625, 0.998046875, 0.0859375, 0.7529296875, 0.99609375, 0.998046875, 0.064453125, 0.0263671875, 0.0009765625, 0.81640625, 0.013671875, 0.998046875, 0.998046875, 0.056640625, 0.095703125, 0.9970703125, 0.0595703125, 0.05859375, 0.255859375, 0.9931640625, 0.0322265625, 0.908203125, 0.998046875, 0.998046875, 0.4130859375, 0.048828125, 0.0400390625, 0.998046875, 0.134765625, 0.103515625, 0.998046875, 0.998046875, 0.0361328125, 0.1123046875, 0.03515625, 0.8173828125, 0.041015625, 0.072265625, 0.2392578125, 0.998046875, 0.998046875, 0.998046875, 0.9091796875, 0.998046875, 0.08203125, 0.02734375, 0.037109375, 0.0625, 0.998046875, 0.8740234375, 0.1025390625, 0.7958984375, 0.0283203125, 0.9052734375, 0.0673828125, 0.7236328125, 0.201171875, 0.90625, 0.998046875, 0.9970703125, 0.0859375, 0.013671875, 0.0302734375, 0.998046875, 0.1845703125, 0.1416015625, 0.11328125, 0.998046875, 0.189453125, 0.01953125, 0.998046875, 0.05859375, 0.998046875, 0.998046875, 0.0537109375, 0.998046875, 0.12109375, 0.8984375, 0.0263671875, 0.0087890625, 0.998046875, 0.998046875, 0.9296875, 0.16015625, 0.1025390625, 0.0244140625, 0.0537109375, 0.091796875, 0.060546875, 0.064453125, 0.998046875, 0.998046875, 0.02734375, 0.0556640625, 0.083984375, 0.998046875, 0.9013671875, 0.7451171875, 0.998046875, 0.0517578125, 0.998046875, 0.8876953125, 0.9970703125, 0.0625, 0.796875, 0.1962890625, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9580078125, 0.0859375, 0.998046875, 0.9990234375, 0.021484375, 0.958984375, 0.998046875, 0.0361328125, 0.998046875, 0.0361328125, 0.0, 0.9248046875, 0.998046875, 0.7890625, 0.9990234375, 0.0283203125, 0.869140625, 0.7587890625, 0.998046875, 0.078125, 0.017578125, 0.998046875, 0.8251953125, 0.9482421875, 0.9970703125, 0.99609375, 0.998046875, 0.998046875, 0.0546875, 0.076171875, 0.9267578125, 0.998046875, 0.8359375, 0.029296875, 0.86328125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.8427734375]

 sparsity of   [0.1584201455116272, 0.0251736119389534, 0.0794270858168602, 0.0533854179084301, 0.189236119389534, 0.02300347201526165, 0.9986979365348816, 0.014322916977107525, 0.456597238779068, 0.7769097089767456, 0.1783854216337204, 0.0442708320915699, 0.0668402761220932, 0.9986979365348816, 0.0733506977558136, 0.02690972201526165, 0.01519097201526165, 0.0364583320915699, 0.0360243059694767, 0.0733506977558136, 0.014756944961845875, 0.02083333395421505, 0.4262152910232544, 0.010850694961845875, 0.0164930559694767, 0.3684895932674408, 0.2274305522441864, 0.1475694477558136, 0.01909722201526165, 0.9986979365348816, 0.01692708395421505, 0.009548611007630825, 0.0607638880610466, 0.1011284738779068, 0.0030381944961845875, 0.2408854216337204, 0.0251736119389534, 0.9986979365348816, 0.0889756977558136, 0.02604166604578495, 0.013454861007630825, 0.26953125, 0.9986979365348816, 0.02604166604578495, 0.0186631940305233, 0.0186631940305233, 0.0677083358168602, 0.02083333395421505, 0.0303819440305233, 0.1041666641831398, 0.0321180559694767, 0.0347222238779068, 0.0338541679084301, 0.0620659738779068, 0.0677083358168602, 0.6297743320465088, 0.03081597201526165, 0.1332465261220932, 0.788194477558136, 0.0807291641831398, 0.0490451380610466, 0.0802951380610466, 0.9986979365348816, 0.9995659589767456, 0.0434027798473835, 0.7751736044883728, 0.0616319440305233, 0.002170138992369175, 0.0329861119389534, 0.071180559694767, 0.1940104216337204, 0.03081597201526165, 0.0920138880610466, 0.0390625, 0.1627604216337204, 0.2330729216337204, 0.1080729141831398, 0.02473958395421505, 0.1753472238779068, 0.0802951380610466, 0.01692708395421505, 0.04296875, 0.0768229141831398, 0.0512152798473835, 0.0720486119389534, 0.1375868022441864, 0.009114583022892475, 0.0, 0.0377604179084301, 0.0928819477558136, 0.0347222238779068, 0.0629340261220932, 0.0642361119389534, 0.3224826455116272, 0.0416666679084301, 0.02213541604578495, 0.0390625, 0.0768229141831398, 0.0447048619389534, 0.999131977558136, 0.8628472089767456, 0.106336809694767, 0.0568576380610466, 0.0477430559694767, 0.9986979365348816, 0.1558159738779068, 0.0551215298473835, 0.02300347201526165, 0.0564236119389534, 0.0251736119389534, 0.03515625, 0.02951388992369175, 0.0963541641831398, 0.0164930559694767, 0.0516493059694767, 0.1080729141831398, 0.02213541604578495, 0.0186631940305233, 0.1762152761220932, 0.0407986119389534, 0.01519097201526165, 0.269097238779068, 0.8385416865348816, 0.0377604179084301, 0.0390625, 0.0525173619389534, 0.0416666679084301, 0.1150173619389534, 0.0203993059694767, 0.01822916604578495, 0.6223958134651184, 0.01692708395421505, 0.2413194477558136, 0.0763888880610466, 0.02604166604578495, 0.09765625, 0.6953125, 0.6414930820465088, 0.0933159738779068, 0.0668402761220932, 0.4513888955116272, 0.0164930559694767, 0.7934027910232544, 0.02170138992369175, 0.02777777798473835, 0.7209201455116272, 0.013888888992369175, 0.02994791604578495, 0.02560763992369175, 0.9986979365348816, 0.1028645858168602, 0.0047743055038154125, 0.0034722222480922937, 0.0416666679084301, 0.1610243022441864, 0.0243055559694767, 0.1193576380610466, 0.1150173619389534, 0.2022569477558136, 0.0264756940305233, 0.0837673619389534, 0.02864583395421505, 0.0416666679084301, 0.0915798619389534, 0.0416666679084301, 0.94921875, 0.9986979365348816, 0.0646701380610466, 0.02777777798473835, 0.240017369389534, 0.0013020833721384406, 0.0603298619389534, 0.3263888955116272, 0.009114583022892475, 0.014322916977107525, 0.114149309694767, 0.0316840298473835, 0.02690972201526165, 0.01605902798473835, 0.0290798619389534, 0.1072048619389534, 0.2873263955116272, 0.0321180559694767, 0.0086805559694767, 0.0052083334885537624, 0.5876736044883728, 0.0173611119389534, 0.0568576380610466, 0.1809895783662796, 0.9986979365348816, 0.2078993022441864, 0.1119791641831398, 0.1488715261220932, 0.0290798619389534, 0.0425347238779068, 0.0572916679084301, 0.0529513880610466, 0.2261284738779068, 0.0173611119389534, 0.0342881940305233, 0.01822916604578495, 0.2452256977558136, 0.02300347201526165, 0.013454861007630825, 0.3459201455116272, 0.0251736119389534, 0.0950520858168602, 0.0677083358168602, 0.0251736119389534, 0.0069444444961845875, 0.2122395783662796, 0.1011284738779068, 0.014756944961845875, 0.0486111119389534, 0.0481770820915699, 0.013020833022892475, 0.02690972201526165, 0.0125868059694767, 0.01215277798473835, 0.0737847238779068, 0.999131977558136, 0.0464409738779068, 0.071180559694767, 0.0321180559694767, 0.0464409738779068, 0.0980902761220932, 0.08984375, 0.00824652798473835, 0.0746527761220932, 0.0008680555620230734, 0.0303819440305233, 0.193142369389534, 0.0720486119389534, 0.0243055559694767, 0.0759548619389534, 0.0542534738779068, 0.02560763992369175, 0.0512152798473835, 0.0551215298473835, 0.0460069440305233, 0.0377604179084301, 0.0013020833721384406, 0.0078125, 0.0577256940305233, 0.01909722201526165, 0.0434027798473835, 0.0551215298473835, 0.0499131940305233, 0.6597222089767456, 0.0902777761220932, 0.0598958320915699, 0.0941840261220932, 0.9986979365348816, 0.0529513880610466, 0.02170138992369175, 0.9995659589767456]

 sparsity of   [0.9921875, 0.83984375, 0.99609375, 0.9921875, 0.99609375, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.00390625, 0.9921875, 0.375, 0.9921875, 0.0625, 0.03515625, 0.99609375, 0.00390625, 0.99609375, 0.046875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.01953125, 0.62890625, 0.9921875, 0.99609375, 0.99609375, 0.0390625, 0.98828125, 0.09765625, 0.9921875, 0.24609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0234375, 0.99609375, 0.9921875, 0.7890625, 0.9921875, 0.99609375, 0.9921875, 0.12890625, 0.99609375, 0.99609375, 0.99609375, 0.98828125, 0.02734375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.84765625, 0.9921875, 0.9921875, 0.9921875, 0.0234375, 0.9921875, 0.9921875, 0.109375, 0.0625, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.12109375, 0.9921875, 0.9921875, 0.015625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.2421875, 0.484375, 0.9921875, 0.9921875, 0.98828125, 0.91796875, 0.93359375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.1015625, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.19140625, 0.98828125, 0.9921875, 0.1015625, 0.01953125, 0.9921875, 0.11328125, 0.9921875, 0.8671875, 0.1171875, 0.99609375, 0.30078125, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.078125, 0.9921875, 0.06640625, 0.9921875, 0.9296875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.17578125, 0.99609375, 0.98828125, 0.99609375, 0.375, 0.078125, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.984375, 0.99609375, 0.9921875, 0.04296875, 0.07421875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.98828125, 0.99609375, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.98828125, 0.02734375, 0.1171875, 0.9921875, 0.9921875, 0.0625, 0.9921875, 0.015625, 0.05078125, 0.05078125, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0390625, 0.9921875, 0.99609375, 0.99609375, 0.078125, 0.03125, 0.9921875, 0.9921875, 0.03515625, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9140625, 0.05859375, 0.4453125, 0.03125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.05078125, 0.98828125, 0.99609375, 0.09375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.08984375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.5, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0234375, 0.7421875, 0.9921875, 0.50390625, 0.99609375, 0.98828125, 0.98828125, 0.08203125, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.32421875, 0.99609375, 0.9921875, 0.98828125, 0.78515625, 0.9921875, 0.9921875, 0.87890625, 0.99609375, 0.9921875, 0.98828125, 0.08203125, 0.91015625, 0.99609375, 0.9921875, 0.04296875, 0.07421875, 0.9921875, 0.02734375, 0.9921875, 0.99609375, 0.9921875, 0.08984375, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.046875, 0.0390625, 0.81640625, 0.82421875, 0.99609375, 0.11328125, 0.9921875, 0.125, 0.9921875, 0.9921875, 0.37890625, 0.9921875, 0.9921875, 0.046875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.04296875, 0.04296875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.16015625, 0.07421875, 0.9921875, 0.9921875, 0.99609375, 0.13671875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.06640625, 0.9921875, 0.9921875, 0.0546875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.03515625, 0.99609375, 0.9921875, 0.99609375, 0.03125, 0.3203125, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.03125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.17578125, 0.46484375, 0.99609375, 0.11328125, 0.98828125, 0.99609375, 0.9921875, 0.0, 0.03125, 0.9921875, 0.98828125, 0.9140625, 0.078125, 0.9921875, 0.99609375, 0.26953125, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.04296875, 0.34765625, 0.9921875, 0.9921875, 0.4375, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.05859375, 0.9921875, 0.2578125, 0.9921875, 0.51953125, 0.19921875, 0.99609375, 0.9921875, 0.3046875, 0.015625, 0.84765625, 0.99609375, 0.41015625, 0.99609375, 0.9921875, 0.9921875, 0.24609375, 0.9921875, 0.98828125, 0.99609375, 0.0, 0.9921875, 0.22265625, 0.99609375, 0.05859375, 0.08984375, 0.046875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.12109375, 0.99609375, 0.9921875, 0.9921875, 0.1953125, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0703125, 0.98828125, 0.0390625, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.07421875, 0.99609375, 0.046875, 0.02734375, 0.22265625, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.3046875, 0.9921875, 0.9921875, 0.1015625, 0.9921875, 0.12890625, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.08984375, 0.01953125, 0.9921875, 0.9921875, 0.9921875, 0.46484375, 0.1171875, 0.1171875, 0.05859375, 0.98828125, 0.07421875, 0.99609375, 0.9921875, 0.25, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.0859375, 0.9921875, 0.9921875, 0.98828125, 0.1015625, 0.9921875, 0.9921875, 0.0546875, 0.9921875, 0.0546875, 0.87109375, 0.26171875, 0.13671875, 0.13671875, 0.9921875, 0.9921875, 0.98828125, 0.16796875, 0.76953125, 0.20703125, 0.9921875, 0.015625, 0.9921875, 0.9921875, 0.1171875, 0.1796875, 0.9921875, 0.0703125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.078125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.109375, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.20703125, 0.9921875, 0.2265625, 0.03515625, 0.125, 0.05078125, 0.9921875, 0.9921875, 0.9921875, 0.03125, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.15625, 0.9921875, 0.9921875, 0.9921875, 0.23046875, 0.02734375, 0.0, 0.05078125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.08984375, 0.9921875, 0.98828125, 0.05078125, 0.99609375, 0.99609375, 0.9921875, 0.98828125, 0.984375, 0.12890625, 0.984375, 0.9921875, 0.99609375, 0.99609375, 0.22265625, 0.9921875, 0.15234375, 0.9921875, 0.99609375, 0.99609375, 0.4921875, 0.1171875, 0.15234375, 0.9921875, 0.99609375, 0.1015625, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.07421875, 0.9921875, 0.9921875, 0.12109375, 0.0625, 0.09375, 0.1015625, 0.99609375, 0.9921875, 0.18359375, 0.921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.015625, 0.20703125, 0.99609375, 0.13671875, 0.9921875, 0.98828125, 0.0703125, 0.0390625, 0.99609375, 0.234375, 0.9921875, 0.06640625, 0.9921875, 0.21875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.06640625, 0.15234375, 0.98828125, 0.99609375, 0.1484375, 0.9921875, 0.9921875, 0.9921875, 0.22265625, 0.9921875, 0.0625, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.27734375, 0.29296875, 0.9921875, 0.9921875, 0.15234375, 0.0625, 0.09765625, 0.43359375, 0.17578125, 0.75, 0.08984375, 0.0546875, 0.99609375, 0.9921875, 0.20703125, 0.9921875, 0.9921875, 0.9921875, 0.01171875, 0.98828125, 0.04296875, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.0390625, 0.0, 0.0703125, 0.9921875, 0.98828125, 0.56640625, 0.9921875, 0.08984375, 0.9921875, 0.0859375, 0.1484375, 0.015625, 0.05078125, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.14453125, 0.171875, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.15234375, 0.85546875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.171875, 0.9921875, 0.0, 0.078125, 0.13671875, 0.2421875, 0.98828125, 0.98828125, 0.9921875, 0.0390625, 0.125, 0.11328125, 0.9921875, 0.1328125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.1484375, 0.29296875, 0.08984375, 0.99609375, 0.9921875, 0.11328125, 0.0, 0.0625, 0.25, 0.98828125, 0.078125, 0.9921875, 0.22265625, 0.2109375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0546875, 0.0625, 0.0, 0.98828125, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.5390625, 0.9921875, 0.9921875, 0.20703125, 0.9921875, 0.9921875, 0.9921875, 0.09375, 0.03515625, 0.07421875, 0.98828125, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.19921875, 0.13671875, 0.0234375, 0.9921875, 0.33203125, 0.0625, 0.98828125, 0.06640625, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.046875, 0.07421875, 0.9921875, 0.99609375, 0.1015625, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.0546875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.05859375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0078125, 0.9921875, 0.9921875, 0.04296875, 0.9921875, 0.9921875, 0.26953125, 0.109375, 0.9921875, 0.98828125, 0.9921875, 0.140625, 0.1953125, 0.046875, 0.01953125, 0.16015625, 0.0, 0.98828125, 0.3671875, 0.98828125, 0.9921875, 0.9921875, 0.06640625, 0.9921875, 0.0390625, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.12890625, 0.18359375, 0.9921875, 0.99609375, 0.05859375, 0.9921875, 0.9921875, 0.04296875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0234375, 0.984375, 0.99609375, 0.0546875, 0.9921875, 0.18359375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.09375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.15234375, 0.13671875, 0.16796875, 0.03125, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.109375, 0.0, 0.9921875, 0.0625, 0.9921875, 0.98828125, 0.0625, 0.0, 0.9921875, 0.453125, 0.9921875, 0.9921875, 0.9921875, 0.0546875, 0.3046875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.046875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.17578125, 0.17578125, 0.98828125, 0.9921875, 0.0, 0.23046875, 0.078125, 0.9921875, 0.09765625, 0.9921875, 0.9921875, 0.8359375, 0.98828125, 0.99609375, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.21875, 0.98828125, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.93359375, 0.99609375, 0.98828125, 0.9921875, 0.99609375, 0.99609375, 0.98828125, 0.9921875, 0.84375, 0.9921875, 0.9921875, 0.9921875, 0.2734375, 0.0703125, 0.1171875, 0.98828125, 0.99609375, 0.16015625, 0.0625, 0.0625, 0.05078125, 0.109375, 0.99609375, 0.28125, 0.9921875, 0.99609375, 0.109375, 0.9921875, 0.00390625, 0.00390625, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.04296875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.1640625, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.984375, 0.98828125, 0.99609375, 0.0703125, 0.0859375, 0.0, 0.99609375, 0.203125, 0.04296875, 0.99609375, 0.03125, 0.12109375, 0.9375, 0.9921875, 0.9921875, 0.98828125, 0.390625, 0.9921875, 0.34375, 0.9921875, 0.078125, 0.99609375, 0.9921875, 0.12890625, 0.9921875, 0.99609375, 0.9921875, 0.31640625, 0.1015625, 0.03125, 0.99609375, 0.9921875, 0.00390625, 0.9921875, 0.1171875, 0.984375, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.10546875, 0.0625, 0.1796875, 0.12890625, 0.98828125, 0.0, 0.0703125, 0.9921875, 0.0546875, 0.9921875]

 sparsity of   [0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9970703125, 0.998046875, 0.1435546875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.1298828125, 0.998046875, 0.0810546875, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.2548828125, 0.126953125, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.0849609375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.119140625, 0.9990234375, 0.9990234375, 0.998046875, 0.123046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1357421875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1083984375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.138671875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.12890625, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.0888671875, 0.998046875, 0.998046875, 0.9990234375, 0.2587890625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.4892578125, 0.998046875, 0.0966796875, 0.998046875, 0.998046875, 0.2353515625, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1591796875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.1025390625, 0.2724609375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.0888671875, 0.1181640625, 0.998046875, 0.087890625, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.13671875, 0.2666015625, 0.9990234375, 0.1259765625, 0.9990234375, 0.111328125, 0.9990234375, 0.998046875, 0.998046875, 0.1259765625, 0.9990234375, 0.998046875, 0.998046875, 0.3896484375, 0.998046875, 0.998046875, 0.998046875, 0.2724609375, 0.9970703125, 0.998046875, 0.998046875, 0.4970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.119140625, 0.0322265625, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.1220703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.2666015625, 0.9990234375, 0.998046875, 0.998046875, 0.14453125, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.1435546875, 0.9990234375, 0.9970703125, 0.9990234375, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.0947265625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.10546875, 0.9990234375, 0.12890625, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.1005859375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.09765625, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.267578125, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.2734375, 0.11328125, 0.9990234375, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1337890625, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.078125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.2939453125, 0.998046875, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.2666015625, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.1220703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.1240234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.130859375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.2900390625, 0.1533203125, 0.998046875, 0.998046875, 0.998046875, 0.08984375, 0.9990234375, 0.998046875, 0.0859375, 0.998046875, 0.2509765625, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.466796875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.056640625, 0.998046875, 0.435546875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.1181640625, 0.998046875, 0.9990234375, 0.998046875, 0.1474609375, 0.998046875, 0.998046875, 0.998046875, 0.0908203125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.279296875, 0.2568359375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.080078125, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.0751953125, 0.1943359375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.45703125]

 sparsity of   [0.9995659589767456, 0.0438368059694767, 0.9995659589767456, 0.1590711772441864, 0.9995659589767456, 0.265190988779068, 0.9995659589767456, 0.924913227558136, 0.9995659589767456, 0.0464409738779068, 0.03125, 0.9576823115348816, 0.0394965298473835, 0.0336371548473835, 0.9995659589767456, 0.0954861119389534, 0.9995659589767456, 0.9997829794883728, 0.9993489384651184, 0.9995659589767456, 0.0490451380610466, 0.0418836809694767, 0.02799479104578495, 0.9995659589767456, 0.06640625, 0.9993489384651184, 0.9997829794883728, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.0709635391831398, 0.9997829794883728, 0.0755208358168602, 0.0425347238779068, 0.0264756940305233, 0.0329861119389534, 0.0625, 0.0883246511220932, 0.0460069440305233, 0.9995659589767456, 0.8884548544883728, 0.0481770820915699, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.02365451492369175, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.216579869389534, 0.0368923619389534, 0.1030815988779068, 0.0555555559694767, 0.9993489384651184, 0.0562065988779068, 0.0301649309694767, 0.9995659589767456, 0.0323350690305233, 0.0967881977558136, 0.9995659589767456, 0.1128472238779068, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.856553852558136, 0.0948350727558136, 0.9995659589767456, 0.0440538190305233, 0.9993489384651184, 0.9995659589767456, 0.0935329869389534, 0.052734375, 0.01692708395421505, 0.9995659589767456, 0.9997829794883728, 0.2719184160232544, 0.0423177070915699, 0.9997829794883728, 0.931640625, 0.0412326380610466, 0.111328125, 0.0865885391831398, 0.0366753488779068, 0.1078559011220932, 0.033203125, 0.0536024309694767, 0.0423177070915699, 0.0980902761220932, 0.935546875, 0.9995659589767456, 0.0173611119389534, 0.12109375, 0.046875, 0.9995659589767456, 0.119140625, 0.1278211772441864, 0.0581597238779068, 0.4835069477558136, 0.115234375, 0.9995659589767456, 0.06640625, 0.86328125, 0.171875, 0.9995659589767456, 0.0729166641831398, 0.1807725727558136, 0.9995659589767456, 0.9995659589767456, 0.0555555559694767, 0.0319010429084301, 0.9995659589767456, 0.9993489384651184, 0.0614149309694767, 0.0989583358168602, 0.0516493059694767, 0.1812065988779068, 0.9997829794883728, 0.0677083358168602, 0.0451388880610466, 0.9997829794883728, 0.046875, 0.9997829794883728, 0.1208767369389534, 0.9995659589767456, 0.9995659589767456, 0.0499131940305233, 0.9995659589767456, 0.1078559011220932, 0.9997829794883728, 0.9993489384651184, 0.0431857630610466, 0.0377604179084301, 0.1163194477558136, 0.9416232705116272, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.0555555559694767, 0.0414496548473835, 0.9448784589767456, 0.9995659589767456, 0.8578559160232544, 0.9997829794883728, 0.9993489384651184, 0.1128472238779068, 0.1165364608168602, 0.8988715410232544, 0.1195746511220932, 0.1174045130610466, 0.0590277798473835, 0.1067708358168602, 0.9997829794883728, 0.1148003488779068, 0.0416666679084301, 0.838975727558136, 0.100477434694767, 0.9995659589767456, 0.0349392369389534, 0.0421006940305233, 0.5017361044883728, 0.872178852558136, 0.9778645634651184, 0.1710069477558136, 0.9995659589767456, 0.0436197929084301, 0.9995659589767456, 0.0323350690305233, 0.0631510391831398, 0.9995659589767456, 0.9997829794883728, 0.1334635466337204, 0.1048177108168602, 0.1187065988779068, 0.0559895820915699, 0.9995659589767456, 0.9995659589767456, 0.0685763880610466, 0.1922743022441864, 0.9995659589767456, 0.0423177070915699, 0.9995659589767456, 0.9995659589767456, 0.0685763880610466, 0.9995659589767456, 0.9507378339767456, 0.2738715410232544, 0.0874565988779068, 0.9997829794883728, 0.8708767294883728, 0.0290798619389534, 0.9995659589767456, 0.0733506977558136, 0.0262586809694767, 0.9997829794883728, 0.02951388992369175, 0.8615451455116272, 0.9995659589767456, 0.208767369389534, 0.1056857630610466, 0.0418836809694767, 0.0553385429084301, 0.0282118059694767, 0.0514322929084301, 0.04296875, 0.0577256940305233, 0.9995659589767456, 0.9299045205116272, 0.0520833320915699, 0.9997829794883728, 0.9993489384651184, 0.1000434011220932, 0.9995659589767456, 0.0523003488779068, 0.9993489384651184, 0.9995659589767456, 0.0345052070915699, 0.0310329869389534, 0.0323350690305233, 0.1243489608168602, 0.9995659589767456, 0.1174045130610466, 0.094618059694767, 0.1215277761220932, 0.9997829794883728, 0.0603298619389534, 0.0496961809694767, 0.0334201380610466, 0.8146701455116272, 0.9995659589767456, 0.084852434694767, 0.056640625, 0.9995659589767456, 0.1831597238779068, 0.8580729365348816, 0.0562065988779068, 0.9826388955116272, 0.0481770820915699, 0.1156684011220932, 0.9995659589767456, 0.9995659589767456, 0.052734375, 0.244140625, 0.069227434694767, 0.0963541641831398, 0.8493923544883728, 0.1827256977558136, 0.0716145858168602, 0.1245659738779068, 0.9832899570465088, 0.8815104365348816, 0.9995659589767456, 0.03059895895421505, 0.9995659589767456, 0.1052517369389534, 0.9995659589767456, 0.9997829794883728, 0.0375434048473835, 0.0590277798473835, 0.0464409738779068, 0.1072048619389534, 0.1013454869389534, 0.9995659589767456, 0.1566840261220932, 0.0314670130610466, 0.8865017294883728, 0.166015625, 0.9995659589767456, 0.0310329869389534, 0.0314670130610466, 0.0483940988779068, 0.0668402761220932, 0.0546875, 0.9153645634651184, 0.9342448115348816, 0.107421875, 0.02994791604578495, 0.8166232705116272, 0.0700954869389534, 0.9995659589767456, 0.1319444477558136, 0.1039496511220932, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.1564670205116272, 0.0740017369389534, 0.9194878339767456, 0.8815104365348816, 0.0685763880610466, 0.0334201380610466, 0.8669704794883728, 0.120008684694767, 0.9995659589767456, 0.8947482705116272, 0.0668402761220932, 0.1169704869389534, 0.0642361119389534, 0.0735677108168602, 0.9997829794883728, 0.9813368320465088, 0.8537326455116272, 0.9615885615348816, 0.0379774309694767, 0.922960102558136, 0.9995659589767456, 0.9175347089767456, 0.9995659589767456, 0.0998263880610466, 0.9995659589767456, 0.9995659589767456, 0.012803819961845875, 0.8650173544883728, 0.041015625, 0.9993489384651184, 0.9995659589767456, 0.9557291865348816, 0.0679253488779068, 0.0551215298473835, 0.0483940988779068, 0.0203993059694767, 0.11328125, 0.9153645634651184, 0.1226128488779068, 0.130642369389534, 0.0648871511220932, 0.0579427070915699, 0.1134982630610466, 0.0358072929084301, 0.9995659589767456, 0.044921875, 0.0427517369389534, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0970052108168602, 0.1022135391831398, 0.0475260429084301, 0.9168837070465088, 0.9995659589767456, 0.0377604179084301, 0.0549045130610466, 0.9993489384651184, 0.1119791641831398, 0.8600260615348816, 0.9995659589767456, 0.9995659589767456, 0.102430559694767, 0.9993489384651184, 0.0234375, 0.099609375, 0.060546875, 0.1091579869389534, 0.0824652761220932, 0.0353732630610466, 0.0763888880610466, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.1195746511220932, 0.9995659589767456, 0.9997829794883728, 0.1786024272441864, 0.0837673619389534, 0.2575954794883728, 0.0473090298473835, 0.9995659589767456, 0.02799479104578495, 0.0844184011220932, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0418836809694767, 0.0831163227558136, 0.02690972201526165, 0.0629340261220932, 0.4880642294883728, 0.119140625, 0.041015625, 0.2194010466337204, 0.0206163190305233, 0.03515625, 0.1013454869389534, 0.0466579869389534, 0.9353298544883728, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.0614149309694767, 0.132595494389534, 0.076171875, 0.9995659589767456, 0.0564236119389534, 0.107421875, 0.9997829794883728, 0.0785590261220932, 0.0434027798473835, 0.0392795130610466, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.1137152761220932, 0.1575520783662796, 0.8430989384651184, 0.0579427070915699, 0.9995659589767456, 0.0609809048473835, 0.0549045130610466, 0.05078125, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.1030815988779068, 0.4769965410232544, 0.0457899309694767, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.0381944440305233, 0.064453125, 0.9496527910232544, 0.1182725727558136, 0.0755208358168602, 0.1154513880610466, 0.947265625, 0.1195746511220932, 0.9342448115348816, 0.02756076492369175, 0.9995659589767456, 0.0583767369389534, 0.9997829794883728, 0.0579427070915699, 0.1223958358168602, 0.0362413190305233, 0.1666666716337204, 0.9270833134651184, 0.1395399272441864, 0.9995659589767456, 0.0891927108168602, 0.9995659589767456, 0.0342881940305233, 0.0642361119389534, 0.071180559694767, 0.0518663190305233, 0.9995659589767456, 0.0531684048473835, 0.9995659589767456, 0.8747829794883728, 0.9995659589767456, 0.0501302070915699, 0.9995659589767456, 0.1265190988779068, 0.9995659589767456, 0.9995659589767456, 0.02734375, 0.846788227558136, 0.9995659589767456, 0.8478732705116272, 0.0870225727558136, 0.0394965298473835, 0.0622829869389534, 0.0349392369389534, 0.9995659589767456, 0.9995659589767456, 0.110243059694767, 0.9995659589767456, 0.0625, 0.9995659589767456, 0.8912760615348816, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.0648871511220932, 0.9997829794883728, 0.0983072891831398, 0.1017795130610466, 0.0375434048473835, 0.0444878488779068, 0.0347222238779068, 0.9995659589767456, 0.9995659589767456, 0.922960102558136, 0.1549479216337204, 0.0405815988779068, 0.0494791679084301, 0.8901909589767456, 0.9993489384651184, 0.8752170205116272, 0.9995659589767456, 0.9995659589767456, 0.0414496548473835, 0.0726996511220932, 0.9997829794883728, 0.948350727558136, 0.9995659589767456, 0.0186631940305233, 0.9470486044883728, 0.9995659589767456, 0.148220494389534, 0.9995659589767456, 0.0592447929084301, 0.0438368059694767, 0.0, 0.9490017294883728, 0.1187065988779068, 0.154079869389534, 0.0529513880610466, 0.9850260615348816, 0.0405815988779068, 0.9995659589767456, 0.8910590410232544]

 sparsity of   [0.1015625, 0.892578125, 0.021484375, 0.40625, 0.013671875, 0.021484375, 0.01953125, 0.03515625, 0.015625, 0.07421875, 0.029296875, 0.89453125, 0.00390625, 0.0390625, 0.99609375, 0.0546875, 0.0234375, 0.009765625, 0.0390625, 0.0390625, 0.029296875, 0.0234375, 0.01953125, 0.013671875, 0.951171875, 0.005859375, 0.09765625, 0.142578125, 0.130859375, 0.033203125, 0.99609375, 0.029296875, 0.03515625, 0.0, 0.033203125, 0.025390625, 0.013671875, 0.00390625, 0.052734375, 0.029296875, 0.048828125, 0.03125, 0.083984375, 0.99609375, 0.017578125, 0.03125, 0.060546875, 0.99609375, 0.076171875, 0.22265625, 0.05078125, 0.068359375, 0.00390625, 0.927734375, 0.03515625, 0.08984375, 0.87109375, 0.04296875, 0.99609375, 0.046875, 0.025390625, 0.005859375, 0.798828125, 0.0078125, 0.046875, 0.017578125, 0.099609375, 0.01171875, 0.029296875, 0.044921875, 0.029296875, 0.021484375, 0.0234375, 0.021484375, 0.892578125, 0.01953125, 0.029296875, 0.953125, 0.02734375, 0.017578125, 0.03125, 0.931640625, 0.044921875, 0.93359375, 0.955078125, 0.068359375, 0.02734375, 0.02734375, 0.033203125, 0.7421875, 0.01953125, 0.119140625, 0.138671875, 0.02734375, 0.15625, 0.029296875, 0.076171875, 0.994140625, 0.26171875, 0.828125, 0.05859375, 0.18359375, 0.232421875, 0.005859375, 0.78515625, 0.04296875, 0.029296875, 0.087890625, 0.23828125, 0.0390625, 0.013671875, 0.013671875, 0.076171875, 0.900390625, 0.017578125, 0.017578125, 0.029296875, 0.080078125, 0.02734375, 0.2421875, 0.009765625, 0.01171875, 0.92578125, 0.0, 0.044921875, 0.0390625, 0.029296875, 0.076171875, 0.068359375, 0.013671875, 0.96875, 0.591796875, 0.095703125, 0.9921875, 0.828125, 0.99609375, 0.015625, 0.021484375, 0.796875, 0.017578125, 0.05859375, 0.90625, 0.015625, 0.935546875, 0.009765625, 0.81640625, 0.013671875, 0.720703125, 0.01953125, 0.07421875, 0.017578125, 0.0859375, 0.810546875, 0.9296875, 0.041015625, 0.021484375, 0.03515625, 0.080078125, 0.0234375, 0.859375, 0.994140625, 0.0, 0.017578125, 0.021484375, 0.15625, 0.99609375, 0.8046875, 0.015625, 0.029296875, 0.994140625, 0.123046875, 0.021484375, 0.087890625, 0.919921875, 0.171875, 0.064453125, 0.029296875, 0.037109375, 0.021484375, 0.998046875, 0.99609375, 0.7109375, 0.083984375, 0.02734375, 0.01953125, 0.068359375, 0.0859375, 0.0234375, 0.025390625, 0.0390625, 0.994140625, 0.0078125, 0.115234375, 0.083984375, 0.025390625, 0.056640625, 0.021484375, 0.712890625, 0.12890625, 0.75, 0.02734375, 0.013671875, 0.029296875, 0.015625, 0.91015625, 0.017578125, 0.025390625, 0.015625, 0.724609375, 0.595703125, 0.830078125, 0.9140625, 0.05859375, 0.03515625, 0.0390625, 0.072265625, 0.048828125, 0.0390625, 0.01953125, 0.03515625, 0.01953125, 0.015625, 0.037109375, 0.017578125, 0.8984375, 0.05859375, 0.048828125, 0.03125, 0.02734375, 0.044921875, 0.009765625, 0.08984375, 0.025390625, 0.044921875, 0.072265625, 0.03125, 0.095703125, 0.0, 0.806640625, 0.046875, 0.041015625, 0.041015625, 0.078125, 0.052734375, 0.171875, 0.044921875, 0.013671875, 0.111328125, 0.03515625, 0.00390625, 0.033203125, 0.171875, 0.125, 0.02734375, 0.021484375, 0.998046875, 0.544921875, 0.048828125, 0.033203125, 0.94921875, 0.033203125, 0.0234375, 0.00390625, 0.921875, 0.95703125, 0.0078125, 0.0390625, 0.017578125, 0.041015625, 0.01171875, 0.00390625, 0.0390625, 0.03515625, 0.04296875, 0.046875, 0.994140625, 0.234375, 0.921875, 0.0234375, 0.009765625, 0.91796875, 0.0, 0.076171875, 0.04296875, 0.01953125, 0.037109375, 0.033203125, 0.0234375, 0.005859375, 0.072265625, 0.03125, 0.111328125, 0.8671875, 0.03515625, 0.01953125, 0.03125, 0.048828125, 0.03125, 0.056640625, 0.015625, 0.03515625, 0.029296875, 0.048828125, 0.998046875, 0.048828125, 0.033203125, 0.03125, 0.02734375, 0.072265625, 0.001953125, 0.015625, 0.09375, 0.0078125, 0.125, 0.005859375, 0.01171875, 0.005859375, 0.017578125, 0.02734375, 0.015625, 0.8828125, 0.01953125, 0.005859375, 0.017578125, 0.013671875, 0.029296875, 0.017578125, 0.02734375, 0.0390625, 0.021484375, 0.0390625, 0.056640625, 0.0859375, 0.015625, 0.037109375, 0.013671875, 0.01953125, 0.060546875, 0.056640625, 0.0234375, 0.03125, 0.025390625, 0.921875, 0.900390625, 0.01171875, 0.0703125, 0.029296875, 0.015625, 0.03125, 0.998046875, 0.037109375, 0.193359375, 0.03515625, 0.03515625, 0.09765625, 0.015625, 0.03515625, 0.029296875, 0.767578125, 0.23828125, 0.0234375, 0.0234375, 0.07421875, 0.169921875, 0.033203125, 0.015625, 0.39453125, 0.02734375, 0.0390625, 0.005859375, 0.025390625, 0.267578125, 0.02734375, 0.005859375, 0.08203125, 0.99609375, 0.01953125, 0.037109375, 0.0234375, 0.09765625, 0.9375, 0.03125, 0.0703125, 0.044921875, 0.0078125, 0.8125, 0.017578125, 0.08203125, 0.94140625, 0.99609375, 0.046875, 0.9140625, 0.044921875, 0.025390625, 0.056640625, 0.93359375, 0.015625, 0.818359375, 0.05859375, 0.025390625, 0.0703125, 0.76953125, 0.021484375, 0.16796875, 0.02734375, 0.15234375, 0.126953125, 0.017578125, 0.93359375, 0.068359375, 0.99609375, 0.267578125, 0.0078125, 0.025390625, 0.09375, 0.126953125, 0.99609375, 0.01171875, 0.166015625, 0.041015625, 0.890625, 0.015625, 0.025390625, 0.99609375, 0.046875, 0.947265625, 0.021484375, 0.029296875, 0.939453125, 0.02734375, 0.03515625, 0.12890625, 0.03515625, 0.8984375, 0.01953125, 0.76171875, 0.04296875, 0.1484375, 0.02734375, 0.04296875, 0.998046875, 0.064453125, 0.095703125, 0.013671875, 0.005859375, 0.052734375, 0.111328125, 0.06640625, 0.017578125, 0.0625, 0.02734375, 0.0390625, 0.072265625, 0.017578125, 0.490234375, 0.0078125, 0.021484375, 0.998046875, 0.099609375, 0.021484375, 0.779296875, 0.01953125, 0.0, 0.912109375, 0.0390625, 0.001953125, 0.041015625, 0.041015625, 0.765625, 0.0234375, 0.029296875, 0.03125, 0.03515625, 0.037109375, 0.013671875, 0.0546875, 0.021484375, 0.666015625, 0.017578125, 0.88671875, 0.029296875, 0.916015625, 0.99609375, 0.02734375, 0.0390625, 0.017578125, 0.01171875, 0.044921875, 0.06640625, 0.037109375, 0.021484375, 0.134765625, 0.037109375, 0.994140625, 0.013671875, 0.8828125, 0.03125, 0.04296875, 0.025390625, 0.896484375, 0.0625, 0.013671875, 0.802734375, 0.015625, 0.0703125, 0.01171875, 0.013671875, 0.017578125, 0.994140625, 0.068359375, 0.06640625, 0.015625, 0.787109375, 0.015625, 0.013671875, 0.015625, 0.876953125, 0.072265625, 0.0625, 0.064453125, 0.017578125, 0.0234375, 0.021484375, 0.861328125, 0.1171875, 0.03125, 0.029296875, 0.015625, 0.884765625, 0.021484375, 0.01171875, 0.181640625, 0.8125, 0.072265625, 0.01171875, 0.025390625, 0.912109375, 0.064453125, 0.013671875, 0.01953125, 0.01171875, 0.021484375, 0.0, 0.080078125, 0.0390625, 0.021484375, 0.0703125, 0.013671875, 0.099609375, 0.197265625, 0.013671875, 0.04296875, 0.037109375, 0.89453125, 0.017578125, 0.162109375, 0.9453125, 0.8984375, 0.001953125, 0.943359375, 0.0390625, 0.1953125, 0.09765625, 0.970703125, 0.033203125, 0.998046875, 0.025390625, 0.01171875, 0.029296875, 0.025390625, 0.0234375, 0.013671875, 0.01171875, 0.072265625, 0.060546875, 0.060546875, 0.90234375, 0.021484375, 0.02734375, 0.0859375, 0.046875, 0.017578125, 0.052734375, 0.015625, 0.017578125, 0.013671875, 0.013671875, 0.033203125, 0.130859375, 0.01953125, 0.095703125, 0.79296875, 0.060546875, 0.5234375, 0.001953125, 0.205078125, 0.994140625, 0.99609375, 0.9921875, 0.033203125, 0.666015625, 0.115234375, 0.009765625, 0.029296875, 0.091796875, 0.095703125, 0.365234375, 0.0390625, 0.041015625, 0.998046875, 0.259765625, 0.046875, 0.67578125, 0.017578125, 0.05859375, 0.00390625, 0.048828125, 0.01953125, 0.01171875, 0.046875, 0.05078125, 0.02734375, 0.015625, 0.021484375, 0.025390625, 0.0078125, 0.064453125, 0.01953125, 0.005859375, 0.03125, 0.044921875, 0.017578125, 0.646484375, 0.041015625, 0.041015625, 0.00390625, 0.033203125, 0.02734375, 0.1953125, 0.015625, 0.001953125, 0.03515625, 0.01171875, 0.009765625, 0.08203125, 0.046875, 0.0078125, 0.0, 0.05859375, 0.951171875, 0.826171875, 0.025390625, 0.931640625, 0.08984375, 0.99609375, 0.021484375, 0.97265625, 0.849609375, 0.03515625, 0.09375, 0.03515625, 0.068359375, 0.94921875, 0.84375, 0.01953125, 0.0, 0.9296875, 0.005859375, 0.83203125, 0.091796875, 0.03125, 0.99609375, 0.033203125, 0.017578125, 0.8671875, 0.88671875, 0.0234375, 0.0, 0.91796875, 0.6875, 0.013671875, 0.943359375, 0.037109375, 0.841796875, 0.04296875, 0.01171875, 0.83984375, 0.95703125, 0.982421875, 0.01171875, 0.99609375, 0.19140625, 0.0078125, 0.05859375, 0.052734375, 0.380859375, 0.2734375, 0.18359375, 0.1171875, 0.056640625, 0.802734375, 0.01953125, 0.0234375, 0.0859375, 0.955078125, 0.025390625, 0.01953125, 0.07421875, 0.0546875, 0.99609375, 0.076171875, 0.07421875, 0.99609375, 0.021484375, 0.017578125, 0.021484375, 0.078125, 0.8671875, 0.138671875, 0.126953125, 0.99609375, 0.064453125, 0.8203125, 0.017578125, 0.013671875, 0.0, 0.099609375, 0.15625, 0.009765625, 0.017578125, 0.09765625, 0.228515625, 0.09375, 0.02734375, 0.87109375, 0.0234375, 0.060546875, 0.935546875, 0.037109375, 0.923828125, 0.619140625, 0.869140625, 0.896484375, 0.0234375, 0.029296875, 0.015625, 0.005859375, 0.064453125, 0.017578125, 0.205078125, 0.0, 0.025390625, 0.08984375, 0.017578125, 0.017578125, 0.998046875, 0.025390625, 0.017578125, 0.99609375, 0.09765625, 0.0234375, 0.4453125, 0.96875, 0.083984375, 0.19921875, 0.201171875, 0.04296875, 0.013671875, 0.046875, 0.041015625, 0.99609375, 0.03515625, 0.0, 0.833984375, 0.7734375, 0.01953125, 0.01171875, 0.029296875, 0.0234375, 0.041015625, 0.03125, 0.03125, 0.021484375, 0.12890625, 0.02734375, 0.044921875, 0.015625, 0.005859375, 0.73046875, 0.0390625, 0.06640625, 0.037109375, 0.9609375, 0.029296875, 0.869140625, 0.017578125, 0.068359375, 0.861328125, 0.03515625, 0.1484375, 0.068359375, 0.052734375, 0.01953125, 0.0390625, 0.03125, 0.134765625, 0.900390625, 0.015625, 0.001953125, 0.02734375, 0.919921875, 0.025390625, 0.06640625, 0.06640625, 0.017578125, 0.037109375, 0.041015625, 0.01953125, 0.013671875, 0.03125, 0.0234375, 0.087890625, 0.0, 0.078125, 0.99609375, 0.943359375, 0.033203125, 0.01953125, 0.001953125, 0.02734375, 0.0234375, 0.033203125, 0.88671875, 0.013671875, 0.869140625, 0.99609375, 0.013671875, 0.080078125, 0.904296875, 0.15625, 0.064453125, 0.05859375, 0.0625, 0.01953125, 0.140625, 0.0390625, 0.05078125, 0.02734375, 0.015625, 0.123046875, 0.046875, 0.0234375, 0.0, 0.103515625, 0.068359375, 0.060546875, 0.296875, 0.0, 0.017578125, 0.04296875, 0.01171875, 0.017578125, 0.83984375, 0.041015625, 0.041015625, 0.01171875, 0.203125, 0.0, 0.57421875, 0.99609375, 0.994140625, 0.142578125, 0.630859375, 0.015625, 0.017578125, 0.005859375, 0.021484375, 0.01171875, 0.03125, 0.748046875, 0.099609375, 0.025390625, 0.029296875, 0.0546875, 0.8671875, 0.01171875, 0.099609375, 0.072265625, 0.556640625, 0.080078125, 0.828125, 0.009765625, 0.015625, 0.12890625, 0.017578125, 0.0546875, 0.99609375, 0.015625, 0.017578125, 0.890625, 0.033203125, 0.025390625, 0.01171875, 0.05078125, 0.02734375, 0.912109375, 0.029296875, 0.017578125, 0.94921875, 0.123046875, 0.99609375, 0.03515625, 0.11328125, 0.00390625, 0.029296875, 0.501953125, 0.044921875, 0.029296875, 0.029296875, 0.83984375, 0.919921875, 0.0625, 0.02734375, 0.998046875, 0.00390625, 0.927734375, 0.166015625, 0.0234375, 0.236328125, 0.044921875, 0.013671875, 0.0390625, 0.126953125, 0.029296875, 0.0078125, 0.623046875, 0.01953125, 0.05078125, 0.91796875, 0.927734375, 0.033203125, 0.01953125, 0.017578125, 0.072265625, 0.03515625, 0.10546875, 0.0, 0.87109375, 0.0, 0.017578125, 0.85546875, 0.013671875, 0.029296875, 0.814453125, 0.021484375, 0.033203125, 0.953125, 0.9140625, 0.015625, 0.017578125, 0.029296875, 0.65625, 0.021484375, 0.021484375, 0.068359375, 0.021484375, 0.05078125, 0.0546875, 0.126953125, 0.900390625, 0.005859375, 0.04296875, 0.037109375, 0.109375, 0.111328125, 0.08984375, 0.578125, 0.88671875, 0.029296875, 0.85546875, 0.107421875, 0.03125, 0.029296875, 0.001953125, 0.03125, 0.0078125, 0.01953125, 0.96484375, 0.884765625, 0.072265625, 0.0625, 0.408203125, 0.037109375, 0.05078125, 0.025390625, 0.0, 0.046875, 0.01953125, 0.041015625, 0.064453125, 0.056640625, 0.734375, 0.99609375, 0.025390625, 0.02734375, 0.951171875, 0.0078125, 0.052734375, 0.00390625, 0.779296875, 0.02734375, 0.02734375, 0.015625, 0.0234375, 0.685546875, 0.20703125, 0.029296875, 0.02734375, 0.998046875, 0.423828125, 0.0, 0.138671875, 0.958984375, 0.02734375, 0.025390625, 0.994140625, 0.03125, 0.00390625, 0.125, 0.037109375, 0.017578125, 0.033203125, 0.71484375, 0.017578125, 0.03125, 0.05859375, 0.99609375, 0.068359375, 0.943359375, 0.060546875, 0.994140625, 0.015625, 0.017578125, 0.080078125, 0.7890625, 0.0234375, 0.02734375, 0.013671875, 0.025390625, 0.005859375, 0.044921875, 0.009765625, 0.994140625, 0.83984375, 0.0546875, 0.041015625, 0.904296875, 0.115234375, 0.013671875, 0.095703125, 0.794921875, 0.046875, 0.04296875, 0.109375, 0.0234375, 0.025390625, 0.03515625, 0.03125, 0.07421875, 0.05078125, 0.03515625, 0.046875, 0.08984375, 0.07421875, 0.013671875, 0.01953125, 0.095703125, 0.73046875, 0.1640625, 0.00390625, 0.01953125, 0.890625, 0.16796875, 0.041015625, 0.025390625, 0.0078125, 0.037109375, 0.033203125, 0.048828125, 0.15625, 0.876953125, 0.0234375, 0.072265625, 0.021484375, 0.72265625, 0.029296875, 0.04296875, 0.017578125, 0.99609375, 0.03125, 0.1171875, 0.99609375, 0.0234375, 0.05078125, 0.998046875, 0.00390625, 0.017578125, 0.02734375, 0.921875, 0.052734375, 0.015625, 0.052734375, 0.025390625, 0.05078125, 0.10546875, 0.896484375, 0.015625, 0.15625, 0.0859375, 0.841796875, 0.0390625, 0.033203125, 0.662109375, 0.931640625, 0.990234375, 0.017578125, 0.0078125, 0.0234375, 0.03515625, 0.109375, 0.01953125, 0.01953125, 0.009765625, 0.021484375, 0.01953125, 0.64453125, 0.99609375, 0.025390625, 0.78515625, 0.91015625, 0.0390625, 0.07421875, 0.109375, 0.693359375, 0.20703125, 0.904296875, 0.029296875, 0.044921875, 0.1484375, 0.0234375, 0.259765625, 0.994140625, 0.041015625, 0.064453125, 0.046875, 0.029296875, 0.99609375, 0.0390625, 0.10546875, 0.0234375, 0.0234375, 0.03125, 0.91015625, 0.025390625, 0.201171875, 0.90625, 0.013671875, 0.017578125, 0.037109375, 0.4375, 0.109375, 0.916015625, 0.01953125, 0.021484375, 0.244140625, 0.013671875, 0.013671875, 0.142578125, 0.095703125, 0.013671875, 0.947265625, 0.021484375, 0.029296875, 0.060546875, 0.0078125, 0.025390625, 0.021484375, 0.025390625, 0.99609375, 0.03515625, 0.048828125, 0.0703125, 0.861328125, 0.01171875, 0.009765625, 0.04296875, 0.044921875, 0.99609375, 0.04296875, 0.10546875, 0.06640625, 0.03125, 0.041015625, 0.033203125, 0.9921875, 0.03515625, 0.01171875, 0.58984375, 0.001953125, 0.900390625, 0.048828125, 0.96484375, 0.0703125, 0.041015625, 0.98828125, 0.099609375, 0.013671875, 0.02734375, 0.7421875, 0.07421875, 0.9296875, 0.0078125, 0.044921875, 0.08203125, 0.052734375, 0.044921875, 0.08203125, 0.08203125, 0.052734375, 0.048828125, 0.10546875, 0.021484375, 0.01171875, 0.025390625, 0.94140625, 0.83984375, 0.998046875, 0.041015625, 0.83203125, 0.025390625, 0.01953125, 0.19140625, 0.0625, 0.05859375, 0.0546875, 0.009765625, 0.037109375, 0.01171875, 0.017578125, 0.052734375, 0.62890625, 0.99609375, 0.033203125, 0.072265625, 0.01953125, 0.0390625, 0.091796875, 0.017578125, 0.158203125, 0.939453125, 0.005859375, 0.068359375, 0.830078125, 0.892578125, 0.9765625, 0.044921875, 0.02734375, 0.0859375, 0.064453125, 0.859375, 0.037109375, 0.029296875, 0.04296875, 0.021484375, 0.001953125, 0.021484375, 0.208984375, 0.02734375, 0.228515625, 0.005859375, 0.01953125, 0.078125, 0.044921875, 0.041015625, 0.884765625, 0.013671875, 0.0390625, 0.009765625, 0.005859375, 0.015625, 0.03515625, 0.0078125, 0.044921875, 0.046875, 0.017578125, 0.0, 0.015625, 0.041015625, 0.015625, 0.0234375, 0.021484375, 0.02734375, 0.04296875, 0.0234375, 0.99609375, 0.806640625, 0.8671875, 0.03125, 0.01171875, 0.0, 0.015625, 0.044921875, 0.0234375, 0.01953125, 0.083984375, 0.072265625, 0.021484375, 0.044921875, 0.693359375, 0.02734375, 0.03125, 0.060546875, 0.04296875, 0.044921875, 0.064453125, 0.021484375, 0.091796875, 0.01953125, 0.01953125, 0.021484375, 0.0234375, 0.04296875, 0.021484375, 0.01171875, 0.06640625, 0.05078125, 0.99609375, 0.0703125, 0.044921875, 0.02734375, 0.8203125, 0.83203125, 0.03125, 0.888671875, 0.037109375, 0.01171875, 0.12890625, 0.125, 0.03515625, 0.134765625, 0.18359375, 0.017578125, 0.091796875, 0.0, 0.0234375, 0.919921875, 0.0703125, 0.017578125, 0.216796875, 0.005859375, 0.0, 0.025390625, 0.109375, 0.109375, 0.8203125, 0.04296875, 0.033203125, 0.087890625, 0.03515625, 0.03125, 0.056640625, 0.068359375, 0.037109375, 0.01953125, 0.923828125, 0.025390625, 0.953125, 0.021484375, 0.01171875, 0.755859375, 0.0078125, 0.076171875, 0.080078125, 0.015625, 0.869140625, 0.033203125, 0.01171875, 0.01953125, 0.203125, 0.072265625, 0.076171875, 0.033203125, 0.01171875, 0.015625, 0.0390625, 0.998046875, 0.01953125, 0.99609375, 0.994140625, 0.009765625, 0.19140625, 0.009765625, 0.99609375, 0.123046875, 0.01953125, 0.53515625, 0.13671875, 0.02734375, 0.880859375, 0.01171875, 0.060546875, 0.021484375, 0.89453125, 0.8515625, 0.0390625, 0.080078125, 0.19921875, 0.005859375, 0.037109375, 0.99609375, 0.08203125, 0.037109375, 0.033203125, 0.021484375, 0.072265625, 0.01953125, 0.982421875, 0.009765625, 0.99609375, 0.18359375, 0.052734375, 0.3125, 0.02734375, 0.052734375, 0.029296875, 0.021484375, 0.021484375, 0.021484375, 0.7265625, 0.884765625, 0.033203125, 0.0078125, 0.0625, 0.021484375, 0.041015625, 0.99609375, 0.021484375, 0.025390625, 0.0234375, 0.90625, 0.021484375, 0.056640625, 0.0, 0.927734375, 0.009765625, 0.99609375, 0.01953125, 0.01953125, 0.033203125, 0.052734375, 0.048828125, 0.994140625, 0.078125, 0.048828125, 0.021484375, 0.046875, 0.01171875, 0.99609375, 0.056640625, 0.041015625, 0.05859375, 0.041015625, 0.994140625, 0.0546875, 0.078125, 0.041015625, 0.01953125, 0.017578125, 0.951171875, 0.01953125, 0.03515625, 0.14453125, 0.02734375, 0.037109375, 0.232421875, 0.99609375, 0.89453125, 0.0390625, 0.013671875, 0.033203125, 0.953125, 0.04296875, 0.0390625, 0.03515625, 0.115234375, 0.01953125, 0.037109375, 0.001953125, 0.015625, 0.02734375, 0.017578125, 0.025390625, 0.048828125, 0.037109375, 0.037109375, 0.26171875, 0.7734375, 0.24609375, 0.01171875, 0.029296875, 0.38671875, 0.896484375, 0.021484375, 0.2265625, 0.03125, 0.048828125, 0.466796875, 0.072265625, 0.01953125, 0.015625, 0.751953125, 0.009765625, 0.828125, 0.900390625, 0.115234375, 0.015625, 0.923828125, 0.111328125, 0.009765625, 0.03515625, 0.0234375, 0.0390625, 0.00390625, 0.0234375, 0.013671875, 0.8828125, 0.0546875, 0.0078125, 0.013671875, 0.890625, 0.86328125, 0.0390625, 0.734375, 0.0, 0.8203125, 0.015625, 0.02734375, 0.080078125, 0.087890625, 0.458984375, 0.00390625, 0.029296875, 0.91015625, 0.953125, 0.02734375, 0.99609375, 0.05078125, 0.017578125, 0.041015625, 0.03125, 0.01171875, 0.02734375, 0.3828125, 0.0234375, 0.017578125, 0.751953125, 0.037109375, 0.91796875, 0.80859375, 0.90234375, 0.046875, 0.037109375, 0.767578125, 0.009765625, 0.15234375, 0.994140625, 0.154296875, 0.62109375, 0.904296875, 0.046875, 0.013671875, 0.009765625, 0.033203125, 0.00390625, 0.859375, 0.0234375, 0.02734375, 0.15625, 0.12890625, 0.044921875, 0.79296875, 0.0234375, 0.0703125, 0.857421875, 0.02734375, 0.93359375, 0.31640625, 0.884765625, 0.0625, 0.1328125, 0.025390625, 0.015625, 0.609375, 0.86328125, 0.01953125, 0.0703125, 0.06640625, 0.015625, 0.05078125, 0.0, 0.013671875, 0.025390625, 0.009765625, 0.0625, 0.005859375, 0.099609375, 0.02734375, 0.892578125, 0.03515625, 0.001953125, 0.99609375, 0.021484375, 0.009765625, 0.029296875, 0.767578125, 0.99609375, 0.140625, 0.201171875, 0.02734375, 0.044921875, 0.0, 0.025390625, 0.0625, 0.765625, 0.037109375, 0.033203125, 0.01953125, 0.01953125, 0.01953125, 0.09375, 0.091796875, 0.12890625, 0.06640625, 0.998046875, 0.0625, 0.046875, 0.873046875, 0.3125, 0.896484375, 0.0546875, 0.009765625, 0.3359375, 0.01953125, 0.0390625, 0.16015625, 0.048828125, 0.970703125, 0.99609375, 0.771484375, 0.814453125, 0.8984375, 0.015625, 0.015625, 0.052734375, 0.994140625, 0.142578125, 0.13671875, 0.017578125, 0.0, 0.02734375, 0.279296875, 0.92578125, 0.912109375, 0.931640625, 0.87890625, 0.06640625, 0.927734375, 0.03125, 0.03125, 0.005859375, 0.029296875, 0.017578125, 0.013671875, 0.041015625, 0.021484375, 0.90625, 0.03125, 0.09375, 0.259765625, 0.015625, 0.96875, 0.142578125, 0.52734375, 0.111328125, 0.01953125, 0.046875, 0.03125, 0.517578125, 0.009765625, 0.998046875, 0.994140625, 0.798828125, 0.076171875, 0.998046875, 0.0234375, 0.0390625, 0.880859375, 0.013671875, 0.041015625, 0.009765625, 0.0, 0.904296875, 0.005859375, 0.072265625, 0.03125, 0.04296875, 0.876953125, 0.009765625, 0.107421875, 0.017578125, 0.109375, 0.017578125, 0.0, 0.033203125, 0.01171875, 0.81640625, 0.015625, 0.109375, 0.880859375, 0.142578125, 0.09375, 0.0390625, 0.94921875, 0.09375, 0.0234375, 0.017578125, 0.0, 0.01953125, 0.11328125, 0.021484375, 0.033203125, 0.87890625, 0.412109375, 0.04296875, 0.005859375, 0.015625, 0.095703125, 0.99609375, 0.115234375, 0.029296875, 0.0625, 0.04296875, 0.021484375, 0.0078125, 0.04296875, 0.060546875, 0.0078125, 0.0, 0.025390625, 0.126953125, 0.01953125, 0.927734375, 0.19140625, 0.396484375, 0.01171875, 0.0546875, 0.015625, 0.99609375, 0.078125, 0.0, 0.021484375, 0.033203125, 0.10546875, 0.08984375, 0.033203125, 0.263671875, 0.013671875, 0.056640625, 0.01953125, 0.052734375, 0.03125, 0.064453125, 0.08203125, 0.810546875, 0.021484375, 0.025390625, 0.080078125, 0.810546875, 0.03515625, 0.052734375, 0.01171875, 0.015625, 0.033203125, 0.03515625, 0.0703125, 0.009765625, 0.94140625, 0.046875, 0.01953125, 0.005859375, 0.044921875, 0.11328125, 0.00390625, 0.720703125, 0.072265625, 0.044921875, 0.0234375, 0.015625, 0.017578125, 0.0390625, 0.052734375, 0.037109375, 0.810546875, 0.935546875, 0.169921875, 0.12109375, 0.009765625, 0.775390625, 0.224609375, 0.0, 0.02734375, 0.029296875, 0.01171875, 0.029296875, 0.150390625, 0.12890625, 0.01171875, 0.916015625, 0.064453125, 0.044921875, 0.994140625, 0.021484375, 0.935546875, 0.08203125, 0.064453125, 0.173828125, 0.048828125, 0.03515625, 0.0390625, 0.005859375, 0.037109375, 0.001953125, 0.322265625, 0.7734375, 0.99609375, 0.013671875, 0.0, 0.0390625, 0.095703125, 0.00390625, 0.001953125, 0.02734375, 0.9453125, 0.048828125, 0.078125, 0.0234375, 0.0390625, 0.013671875, 0.03125, 0.03125, 0.025390625, 0.017578125, 0.029296875, 0.087890625, 0.00390625, 0.048828125, 0.0859375, 0.998046875, 0.021484375, 0.068359375, 0.91796875, 0.95703125, 0.0234375, 0.044921875, 0.802734375, 0.041015625, 0.03125, 0.0234375, 0.484375, 0.99609375, 0.78515625, 0.107421875, 0.009765625, 0.03125, 0.994140625, 0.857421875, 0.04296875, 0.046875, 0.044921875, 0.017578125, 0.24609375, 0.150390625, 0.044921875, 0.0, 0.0859375, 0.037109375, 0.673828125, 0.01171875, 0.01953125, 0.06640625, 0.05859375, 0.271484375, 0.056640625, 0.013671875, 0.04296875, 0.041015625, 0.1484375, 0.0234375, 0.017578125, 0.0, 0.078125, 0.0234375, 0.0859375, 0.013671875, 0.998046875, 0.90234375, 0.009765625, 0.0546875, 0.34765625, 0.0, 0.025390625, 0.04296875, 0.060546875, 0.81640625, 0.01953125, 0.072265625, 0.01171875, 0.03125, 0.052734375, 0.00390625, 0.009765625, 0.998046875, 0.064453125, 0.0390625, 0.017578125, 0.041015625, 0.04296875, 0.0859375, 0.935546875, 0.2109375, 0.365234375, 0.046875, 0.10546875, 0.87890625, 0.609375, 0.9921875, 0.021484375, 0.025390625, 0.033203125, 0.8125, 0.095703125, 0.025390625, 0.798828125, 0.9609375, 0.650390625, 0.01953125, 0.060546875, 0.919921875, 0.052734375, 0.0234375, 0.04296875, 0.853515625, 0.00390625, 0.033203125, 0.02734375, 0.0859375, 0.7734375, 0.994140625, 0.90625, 0.99609375, 0.001953125, 0.17578125, 0.064453125, 0.03125, 0.814453125, 0.0390625, 0.9296875, 0.0546875, 0.017578125, 0.1171875, 0.013671875, 0.123046875, 0.10546875, 0.095703125, 0.033203125, 0.7890625, 0.99609375, 0.0234375, 0.140625, 0.01953125, 0.01953125, 0.033203125, 0.009765625, 0.0234375, 0.076171875, 0.07421875, 0.029296875, 0.8828125, 0.01953125, 0.048828125, 0.068359375, 0.02734375, 0.99609375, 0.0078125, 0.03125, 0.05078125, 0.015625, 0.025390625, 0.052734375, 0.03125, 0.109375, 0.91796875, 0.849609375, 0.005859375, 0.029296875, 0.03125, 0.10546875, 0.013671875, 0.99609375, 0.908203125, 0.044921875, 0.71875, 0.90625, 0.15625, 0.998046875, 0.029296875, 0.013671875, 0.87109375, 0.892578125, 0.0390625, 0.021484375, 0.18359375, 0.044921875, 0.01171875, 0.998046875, 0.052734375, 0.013671875, 0.119140625, 0.0546875, 0.724609375, 0.009765625, 0.044921875, 0.99609375, 0.16796875, 0.025390625, 0.78125, 0.015625, 0.02734375]

 sparsity of   [0.0517578125, 0.0595703125, 0.0537109375, 0.0283203125, 0.8857421875, 0.1103515625, 0.0498046875, 0.033203125, 0.8076171875, 0.2109375, 0.0048828125, 0.01171875, 0.0361328125, 0.1875, 0.005859375, 0.0419921875, 0.74609375, 0.125, 0.263671875, 0.0146484375, 0.9970703125, 0.8525390625, 0.0068359375, 0.0380859375, 0.001953125, 0.1884765625, 0.02734375, 0.0, 0.0068359375, 0.013671875, 0.8134765625, 0.9990234375, 0.845703125, 0.01171875, 0.5927734375, 0.1123046875, 0.0966796875, 0.0869140625, 0.056640625, 0.017578125, 0.0625, 0.65234375, 0.1171875, 0.2724609375, 0.126953125, 0.0849609375, 0.005859375, 0.1162109375, 0.1005859375, 0.001953125, 0.005859375, 0.0693359375, 0.169921875, 0.00390625, 0.125, 0.1142578125, 0.013671875, 0.0, 0.037109375, 0.138671875, 0.005859375, 0.06640625, 0.06640625, 0.0498046875, 0.0068359375, 0.0234375, 0.3095703125, 0.009765625, 0.009765625, 0.05078125, 0.0634765625, 0.599609375, 0.0126953125, 0.9970703125, 0.841796875, 0.1298828125, 0.044921875, 0.0615234375, 0.998046875, 0.150390625, 0.0947265625, 0.021484375, 0.013671875, 0.021484375, 0.751953125, 0.1357421875, 0.017578125, 0.0, 0.23046875, 0.2275390625, 0.2197265625, 0.1171875, 0.2265625, 0.095703125, 0.9990234375, 0.005859375, 0.1083984375, 0.9990234375, 0.9560546875, 0.0068359375, 0.08984375, 0.0244140625, 0.0673828125, 0.0087890625, 0.009765625, 0.078125, 0.009765625, 0.9453125, 0.052734375, 0.1044921875, 0.0732421875, 0.08203125, 0.013671875, 0.13671875, 0.005859375, 0.021484375, 0.162109375, 0.0048828125, 0.072265625, 0.013671875, 0.0869140625, 0.103515625, 0.0048828125, 0.07421875, 0.1640625, 0.0673828125, 0.0068359375, 0.0068359375, 0.0703125, 0.9384765625, 0.0224609375, 0.8779296875, 0.734375, 0.1171875, 0.01171875, 0.068359375, 0.017578125, 0.0771484375, 0.0947265625, 0.1728515625, 0.0048828125, 0.158203125, 0.2197265625, 0.044921875, 0.0, 0.001953125, 0.0107421875, 0.01171875, 0.0546875, 0.0244140625, 0.1357421875, 0.01953125, 0.0087890625, 0.0146484375, 0.2080078125, 0.0107421875, 0.0126953125, 0.642578125, 0.0048828125, 0.212890625, 0.23046875, 0.01171875, 0.015625, 0.103515625, 0.0869140625, 0.947265625, 0.009765625, 0.1279296875, 0.3154296875, 0.138671875, 0.0029296875, 0.998046875, 0.2177734375, 0.0791015625, 0.0, 0.03515625, 0.779296875, 0.0419921875, 0.9970703125, 0.5732421875, 0.013671875, 0.0048828125, 0.0810546875, 0.0048828125, 0.0478515625, 0.0556640625, 0.0146484375, 0.0458984375, 0.0615234375, 0.9482421875, 0.013671875, 0.009765625, 0.0087890625, 0.130859375, 0.0859375, 0.0068359375, 0.091796875, 0.0791015625, 0.0146484375, 0.0908203125, 0.00390625, 0.99609375, 0.0078125, 0.048828125, 0.998046875, 0.7685546875, 0.111328125, 0.0361328125, 0.0693359375, 0.095703125, 0.0322265625, 0.9970703125, 0.203125, 0.0048828125, 0.017578125, 0.0693359375, 0.0791015625, 0.193359375, 0.01171875, 0.1181640625, 0.0107421875, 0.0263671875, 0.0888671875, 0.0126953125, 0.5849609375, 0.0576171875, 0.017578125, 0.0361328125, 0.7373046875, 0.1064453125, 0.498046875, 0.005859375, 0.0283203125, 0.0595703125, 0.0400390625, 0.08984375, 0.0576171875, 0.005859375, 0.0009765625, 0.0498046875, 0.06640625, 0.0478515625, 0.3046875, 0.091796875, 0.029296875, 0.2470703125, 0.0263671875, 0.08984375, 0.0078125, 0.060546875, 0.0458984375, 0.0, 0.025390625, 0.939453125, 0.1328125, 0.068359375, 0.13671875, 0.095703125, 0.90234375, 0.01171875, 0.013671875, 0.9970703125, 0.0263671875, 0.0, 0.0107421875, 0.0, 0.0087890625, 0.0107421875, 0.0400390625, 0.18359375, 0.0478515625, 0.029296875, 0.005859375, 0.11328125, 0.12890625, 0.998046875, 0.0146484375, 0.8115234375, 0.109375, 0.0078125, 0.029296875, 0.25, 0.005859375, 0.1025390625, 0.0087890625, 0.9970703125, 0.8544921875, 0.1982421875, 0.09375, 0.9970703125, 0.025390625, 0.0771484375, 0.1015625, 0.85546875, 0.0576171875, 0.23046875, 0.083984375, 0.0771484375, 0.0126953125, 0.05078125, 0.0302734375, 0.103515625, 0.08984375, 0.0615234375, 0.33984375, 0.0205078125, 0.037109375, 0.9990234375, 0.0546875, 0.0302734375, 0.0048828125, 0.08203125, 0.0068359375, 0.2080078125, 0.728515625, 0.048828125, 0.0107421875, 0.9345703125, 0.0263671875, 0.05859375, 0.09765625, 0.109375, 0.0009765625, 0.0068359375, 0.052734375, 0.9970703125, 0.0283203125, 0.021484375, 0.7470703125, 0.0322265625, 0.2265625, 0.0107421875, 0.896484375, 0.0, 0.0810546875, 0.0654296875, 0.072265625, 0.0458984375, 0.0126953125, 0.01953125, 0.005859375, 0.01953125, 0.9970703125, 0.015625, 0.91015625, 0.07421875, 0.0, 0.013671875, 0.01171875, 0.005859375, 0.005859375, 0.9150390625, 0.0400390625, 0.076171875, 0.6083984375, 0.1826171875, 0.0, 0.05078125, 0.833984375, 0.0224609375, 0.26953125, 0.0244140625, 0.41796875, 0.9970703125, 0.8291015625, 0.107421875, 0.0146484375, 0.0224609375, 0.08984375, 0.0341796875, 0.091796875, 0.03515625, 0.005859375, 0.0263671875, 0.10546875, 0.841796875, 0.0048828125, 0.9990234375, 0.94140625, 0.0517578125, 0.01171875, 0.03125, 0.2919921875, 0.083984375, 0.0283203125, 0.9970703125, 0.0869140625, 0.1220703125, 0.267578125, 0.015625, 0.0087890625, 0.02734375, 0.943359375, 0.0498046875, 0.0009765625, 0.0830078125, 0.041015625, 0.0537109375, 0.1494140625, 0.1005859375, 0.0107421875, 0.005859375, 0.11328125, 0.7109375, 0.0791015625, 0.0146484375, 0.0185546875, 0.2314453125, 0.001953125, 0.1708984375, 0.9970703125, 0.06640625, 0.0498046875, 0.03515625, 0.03515625, 0.0126953125, 0.333984375, 0.01171875, 0.0126953125, 0.0546875, 0.0107421875, 0.021484375, 0.19140625, 0.23046875, 0.1728515625, 0.1416015625, 0.154296875, 0.0126953125, 0.0908203125, 0.01171875, 0.0341796875, 0.998046875, 0.0732421875, 0.9306640625, 0.1484375, 0.2197265625, 0.1181640625, 0.2021484375, 0.0263671875, 0.029296875, 0.9970703125, 0.0166015625, 0.9677734375, 0.8310546875, 0.04296875, 0.041015625, 0.0693359375, 0.0654296875, 0.2275390625, 0.0576171875, 0.9990234375, 0.03515625, 0.140625, 0.107421875, 0.0146484375, 0.00390625, 0.14453125, 0.111328125, 0.013671875, 0.30859375, 0.865234375, 0.1767578125, 0.0126953125, 0.056640625, 0.021484375, 0.90234375, 0.009765625, 0.029296875, 0.0146484375, 0.7666015625, 0.1015625, 0.0224609375, 0.0107421875, 0.0078125, 0.0224609375, 0.208984375, 0.0126953125, 0.046875, 0.162109375, 0.0849609375, 0.9990234375, 0.8779296875, 0.123046875, 0.9970703125, 0.076171875, 0.0048828125, 0.0537109375, 0.025390625, 0.013671875, 0.048828125, 0.07421875, 0.0888671875, 0.0087890625, 0.0576171875, 0.05859375, 0.0068359375, 0.099609375, 0.0908203125, 0.0361328125, 0.064453125, 0.0390625, 0.017578125, 0.1005859375, 0.1181640625, 0.095703125, 0.9609375, 0.01171875, 0.228515625, 0.0234375, 0.005859375, 0.1376953125, 0.005859375, 0.0, 0.9970703125, 0.0087890625, 0.0, 0.0029296875, 0.0107421875, 0.068359375, 0.0439453125, 0.8701171875, 0.0244140625, 0.0, 0.4951171875, 0.16796875, 0.0419921875, 0.5537109375, 0.1533203125, 0.0087890625, 0.0185546875, 0.0068359375, 0.8349609375, 0.0078125, 0.1201171875, 0.0078125, 0.0234375, 0.0908203125, 0.0048828125, 0.7734375, 0.16015625, 0.0634765625, 0.0078125, 0.0478515625, 0.013671875, 0.0166015625, 0.1435546875, 0.0, 0.0126953125, 0.9970703125, 0.0, 0.0, 0.0126953125, 0.23828125, 0.2138671875, 0.0625, 0.0126953125, 0.0380859375, 0.0830078125, 0.1337890625, 0.04296875, 0.0400390625, 0.2109375, 0.0751953125, 0.1240234375, 0.0068359375, 0.0068359375, 0.0029296875, 0.0263671875, 0.0, 0.01171875, 0.0048828125, 0.0908203125, 0.0478515625, 0.0498046875, 0.033203125, 0.169921875, 0.9501953125, 0.169921875, 0.046875, 0.0654296875, 0.00390625, 0.068359375, 0.0078125, 0.0849609375, 0.0810546875, 0.0419921875, 0.0087890625, 0.0234375, 0.0400390625, 0.0576171875, 0.380859375, 0.9970703125, 0.044921875, 0.0234375, 0.3359375, 0.0205078125, 0.0224609375, 0.173828125, 0.076171875, 0.8583984375, 0.1884765625, 0.1689453125, 0.0634765625, 0.212890625, 0.95703125, 0.0078125, 0.703125, 0.0126953125, 0.078125, 0.0, 0.0615234375, 0.1162109375, 0.005859375, 0.0009765625, 0.111328125, 0.2197265625, 0.1015625, 0.0068359375, 0.005859375, 0.681640625, 0.0322265625, 0.3349609375, 0.0537109375, 0.0126953125, 0.0146484375, 0.015625, 0.0029296875, 0.0615234375, 0.0283203125, 0.943359375, 0.1064453125, 0.0009765625, 0.01953125, 0.0068359375, 0.2099609375, 0.0146484375, 0.03125, 0.0, 0.048828125, 0.0205078125, 0.24609375, 0.0126953125, 0.0908203125, 0.3017578125, 0.18359375, 0.0078125, 0.0283203125, 0.0302734375, 0.0859375, 0.2236328125, 0.0126953125, 0.9169921875, 0.0478515625, 0.0068359375, 0.27734375, 0.0185546875, 0.80859375, 0.0078125, 0.0126953125, 0.0498046875, 0.126953125, 0.0322265625, 0.9638671875, 0.0244140625, 0.912109375, 0.87890625, 0.837890625, 0.08984375, 0.0751953125, 0.1201171875, 0.10546875, 0.7783203125, 0.3388671875, 0.0234375, 0.107421875, 0.998046875, 0.03125, 0.0068359375, 0.0078125, 0.119140625, 0.130859375, 0.998046875, 0.2646484375, 0.91796875, 0.072265625, 0.0, 0.0078125, 0.001953125, 0.0341796875, 0.01171875, 0.189453125, 0.9990234375, 0.017578125, 0.041015625, 0.048828125, 0.021484375, 0.0986328125, 0.1298828125, 0.943359375, 0.0078125, 0.0732421875, 0.05078125, 0.04296875, 0.0283203125, 0.2451171875, 0.123046875, 0.126953125, 0.005859375, 0.0361328125, 0.0078125, 0.900390625, 0.005859375, 0.0283203125, 0.029296875, 0.1689453125, 0.109375, 0.0146484375, 0.0029296875, 0.0751953125, 0.072265625, 0.0087890625, 0.0380859375, 0.5419921875, 0.23046875, 0.0341796875, 0.2099609375, 0.123046875, 0.02734375, 0.1591796875, 0.212890625, 0.072265625, 0.01953125, 0.9970703125, 0.8134765625, 0.0078125, 0.0068359375, 0.013671875, 0.607421875, 0.033203125, 0.0576171875, 0.0087890625, 0.09375, 0.134765625, 0.0, 0.1474609375, 0.10546875, 0.0595703125, 0.0068359375, 0.18359375, 0.0107421875, 0.1044921875, 0.10546875, 0.029296875, 0.23046875, 0.0244140625, 0.0390625, 0.2294921875, 0.1748046875, 0.0224609375, 0.0458984375, 0.2294921875, 0.0, 0.4111328125, 0.3154296875, 0.6318359375, 0.7021484375, 0.060546875, 0.005859375, 0.0185546875, 0.853515625, 0.0078125, 0.134765625, 0.02734375, 0.9482421875, 0.0556640625, 0.05859375, 0.0400390625, 0.01953125, 0.0009765625, 0.00390625, 0.9970703125, 0.0048828125, 0.0107421875, 0.158203125, 0.0087890625, 0.171875, 0.90234375, 0.251953125, 0.259765625, 0.064453125, 0.111328125, 0.083984375, 0.005859375, 0.0126953125, 0.0126953125, 0.0146484375, 0.033203125, 0.103515625, 0.9990234375, 0.2294921875, 0.298828125, 0.1083984375, 0.015625, 0.1220703125, 0.078125, 0.021484375, 0.9970703125, 0.0068359375, 0.021484375, 0.1494140625, 0.1142578125, 0.85546875, 0.1064453125, 0.1328125, 0.0107421875, 0.07421875, 0.4951171875, 0.1357421875, 0.001953125, 0.009765625, 0.0869140625, 0.853515625, 0.03125, 0.19140625, 0.7685546875, 0.0146484375, 0.998046875, 0.0458984375, 0.00390625, 0.6455078125, 0.078125, 0.0791015625, 0.1640625, 0.1826171875, 0.19921875, 0.1220703125, 0.015625, 0.0390625, 0.0126953125, 0.0419921875, 0.056640625, 0.0244140625, 0.0400390625, 0.0078125, 0.009765625, 0.2275390625, 0.0390625, 0.025390625, 0.0302734375, 0.1552734375, 0.0986328125, 0.162109375, 0.134765625, 0.0068359375, 0.025390625, 0.7021484375, 0.998046875, 0.017578125, 0.0146484375, 0.0390625, 0.3388671875, 0.0419921875, 0.03515625, 0.0693359375, 0.0263671875, 0.0634765625, 0.12109375, 0.0048828125, 0.0361328125, 0.041015625, 0.091796875, 0.076171875, 0.9970703125, 0.1572265625, 0.1494140625, 0.119140625, 0.0, 0.203125, 0.146484375, 0.0068359375, 0.2001953125, 0.7744140625, 0.2265625, 0.0732421875, 0.142578125, 0.0380859375, 0.8359375, 0.0, 0.587890625, 0.04296875, 0.0, 0.12109375, 0.1103515625, 0.013671875, 0.0419921875, 0.01171875, 0.0322265625, 0.1220703125, 0.919921875, 0.0810546875, 0.00390625, 0.0830078125, 0.125, 0.0888671875, 0.2099609375, 0.0693359375, 0.03125, 0.3544921875, 0.0107421875, 0.0205078125, 0.0185546875, 0.017578125, 0.0947265625, 0.0107421875, 0.0078125, 0.3310546875, 0.0908203125, 0.849609375, 0.0390625, 0.005859375, 0.9111328125, 0.0859375, 0.228515625, 0.806640625, 0.009765625, 0.228515625, 0.0146484375, 0.4990234375, 0.0400390625, 0.017578125, 0.0146484375, 0.1533203125, 0.02734375, 0.0078125, 0.9990234375, 0.8603515625, 0.1064453125, 0.14453125, 0.01953125, 0.0205078125, 0.005859375, 0.0029296875, 0.0234375, 0.0517578125, 0.001953125, 0.0126953125, 0.0625, 0.0859375, 0.013671875, 0.0263671875, 0.060546875, 0.2216796875, 0.115234375, 0.18359375, 0.0126953125, 0.19140625, 0.0439453125, 0.1455078125, 0.0302734375, 0.92578125, 0.119140625, 0.080078125, 0.998046875, 0.576171875, 0.166015625, 0.0, 0.6142578125, 0.88671875, 0.0546875, 0.0087890625, 0.193359375, 0.0419921875, 0.1591796875, 0.021484375, 0.1650390625, 0.060546875, 0.201171875, 0.10546875, 0.0224609375, 0.02734375, 0.9970703125, 0.921875, 0.0146484375, 0.015625, 0.083984375, 0.9990234375, 0.009765625, 0.033203125, 0.0361328125, 0.19921875, 0.0888671875, 0.0234375, 0.009765625, 0.1591796875, 0.0107421875, 0.908203125, 0.0615234375, 0.7490234375, 0.0263671875, 0.06640625, 0.05078125, 0.7236328125, 0.158203125, 0.935546875, 0.0810546875, 0.1533203125, 0.0087890625, 0.9970703125, 0.0615234375, 0.1708984375, 0.001953125, 0.1044921875, 0.0400390625, 0.0078125, 0.0810546875, 0.025390625, 0.0205078125, 0.01171875, 0.076171875, 0.1142578125, 0.0205078125, 0.001953125, 0.126953125, 0.03125, 0.0498046875, 0.1669921875, 0.154296875, 0.02734375, 0.1044921875, 0.166015625, 0.0390625, 0.140625, 0.01171875, 0.078125, 0.06640625, 0.00390625, 0.13671875, 0.1298828125, 0.1494140625, 0.099609375, 0.9970703125, 0.0751953125, 0.1640625, 0.0068359375, 0.4853515625, 0.0869140625, 0.0419921875, 0.037109375, 0.0068359375, 0.140625, 0.1748046875, 0.09765625, 0.09375, 0.0, 0.1015625, 0.1923828125, 0.0263671875, 0.134765625, 0.181640625, 0.091796875, 0.41796875, 0.830078125, 0.9970703125, 0.044921875, 0.0244140625, 0.15625, 0.23046875, 0.0771484375, 0.0439453125, 0.9970703125, 0.029296875, 0.0126953125, 0.01171875, 0.060546875, 0.8251953125, 0.0341796875, 0.1044921875, 0.015625, 0.029296875, 0.0107421875, 0.0927734375, 0.01953125, 0.125, 0.0107421875, 0.0234375, 0.392578125, 0.2890625, 0.9990234375, 0.0107421875, 0.0185546875, 0.017578125, 0.01953125, 0.0205078125, 0.0078125, 0.8544921875, 0.0810546875, 0.00390625, 0.072265625, 0.0029296875, 0.0185546875, 0.064453125, 0.0166015625, 0.8193359375, 0.134765625, 0.0146484375, 0.021484375, 0.021484375, 0.2138671875, 0.0439453125, 0.0234375, 0.0029296875, 0.12109375, 0.126953125, 0.0107421875, 0.123046875, 0.052734375, 0.1083984375, 0.2470703125, 0.142578125, 0.0078125, 0.015625, 0.01953125, 0.0810546875, 0.0048828125, 0.03125, 0.1435546875, 0.0087890625, 0.005859375, 0.1767578125, 0.013671875, 0.01953125, 0.0751953125, 0.12109375, 0.08984375, 0.2021484375, 0.0322265625, 0.005859375, 0.072265625, 0.0048828125, 0.1044921875, 0.1435546875, 0.12109375, 0.0029296875, 0.11328125, 0.0078125, 0.119140625, 0.0380859375, 0.009765625, 0.0048828125, 0.009765625, 0.9990234375, 0.04296875, 0.046875, 0.2294921875, 0.2275390625, 0.0185546875, 0.1494140625, 0.0, 0.087890625, 0.1123046875, 0.0947265625, 0.09375, 0.01171875, 0.00390625, 0.0908203125, 0.9970703125, 0.0068359375, 0.0107421875, 0.7275390625, 0.0625, 0.1552734375, 0.9970703125, 0.111328125, 0.0859375, 0.0107421875, 0.1142578125, 0.212890625, 0.005859375, 0.0166015625, 0.09375, 0.08203125, 0.1767578125, 0.0576171875, 0.1123046875, 0.1103515625, 0.8251953125, 0.951171875, 0.1630859375, 0.77734375, 0.087890625, 0.0419921875, 0.0107421875, 0.001953125, 0.009765625, 0.109375, 0.0234375, 0.0029296875, 0.005859375, 0.0771484375, 0.1015625, 0.0009765625, 0.591796875, 0.20703125, 0.01171875, 0.0146484375, 0.0498046875, 0.0498046875, 0.1396484375, 0.7509765625, 0.0029296875, 0.0078125, 0.015625, 0.03125, 0.9970703125, 0.1845703125, 0.0283203125, 0.96875, 0.0087890625, 0.0078125, 0.0703125, 0.12109375, 0.998046875, 0.0205078125, 0.9970703125, 0.0458984375, 0.1015625, 0.013671875, 0.01171875, 0.1953125, 0.1435546875, 0.0107421875, 0.0185546875, 0.154296875, 0.212890625, 0.005859375, 0.0361328125, 0.0546875, 0.9990234375, 0.9970703125, 0.0078125, 0.9189453125, 0.9970703125, 0.208984375, 0.10546875, 0.060546875, 0.150390625, 0.09765625, 0.021484375, 0.076171875, 0.02734375, 0.0078125, 0.02734375, 0.111328125, 0.0087890625, 0.06640625, 0.041015625, 0.091796875, 0.013671875, 0.0830078125, 0.0126953125, 0.17578125, 0.064453125, 0.869140625, 0.03125, 0.0009765625, 0.0126953125, 0.00390625, 0.0302734375, 0.9990234375, 0.0224609375, 0.2265625, 0.1044921875, 0.083984375, 0.0244140625, 0.068359375, 0.0732421875, 0.048828125, 0.9970703125, 0.0205078125, 0.025390625, 0.623046875, 0.140625, 0.1015625, 0.1396484375, 0.0126953125, 0.0068359375, 0.9970703125, 0.0068359375, 0.0322265625, 0.6787109375, 0.01953125, 0.0029296875, 0.0068359375, 0.287109375, 0.01953125, 0.123046875, 0.0439453125, 0.138671875, 0.8828125, 0.0126953125, 0.03125, 0.033203125, 0.0009765625, 0.0126953125, 0.8603515625, 0.0361328125, 0.0400390625, 0.0283203125, 0.0, 0.01171875, 0.052734375, 0.005859375, 0.0810546875, 0.025390625, 0.1328125, 0.03515625, 0.0576171875, 0.1884765625, 0.0078125, 0.23046875, 0.00390625, 0.1904296875, 0.125, 0.001953125, 0.009765625, 0.2705078125, 0.0009765625, 0.0302734375, 0.0751953125, 0.0224609375, 0.0126953125, 0.0078125, 0.0654296875, 0.0419921875, 0.0341796875, 0.0234375, 0.0361328125, 0.0263671875, 0.9970703125, 0.0341796875, 0.9970703125, 0.8154296875, 0.015625, 0.0341796875, 0.095703125, 0.302734375, 0.0, 0.0478515625, 0.0234375, 0.853515625, 0.0146484375, 0.2294921875, 0.8115234375, 0.91015625, 0.140625, 0.03125, 0.9970703125, 0.02734375, 0.017578125, 0.0205078125, 0.1064453125, 0.7509765625, 0.9970703125, 0.0126953125, 0.158203125, 0.005859375, 0.0, 0.86328125, 0.0009765625, 0.9453125, 0.2138671875, 0.0859375, 0.8447265625, 0.6240234375, 0.0322265625, 0.2353515625, 0.091796875, 0.0283203125, 0.23046875, 0.3291015625, 0.0859375, 0.00390625, 0.0126953125, 0.919921875, 0.21875, 0.013671875, 0.08984375, 0.0458984375, 0.0087890625, 0.150390625, 0.1533203125, 0.9375, 0.9990234375, 0.140625, 0.00390625, 0.013671875, 0.0498046875, 0.00390625, 0.9970703125, 0.0595703125, 0.236328125, 0.04296875, 0.869140625, 0.0, 0.0546875, 0.0068359375, 0.1064453125, 0.04296875, 0.0048828125, 0.037109375, 0.8828125, 0.0283203125, 0.9970703125, 0.822265625, 0.9970703125, 0.1357421875, 0.0009765625, 0.6943359375, 0.744140625, 0.0556640625, 0.2763671875, 0.2265625, 0.0087890625, 0.01953125, 0.072265625, 0.052734375, 0.12890625, 0.1416015625, 0.013671875, 0.0263671875, 0.0078125, 0.33203125, 0.01953125, 0.1376953125, 0.955078125, 0.1015625, 0.0458984375, 0.9990234375, 0.9970703125, 0.0673828125, 0.005859375, 0.126953125, 0.0361328125, 0.0146484375, 0.0419921875, 0.8330078125, 0.009765625, 0.02734375, 0.2099609375, 0.8603515625, 0.005859375, 0.7041015625, 0.0830078125, 0.0009765625, 0.013671875, 0.083984375, 0.1123046875, 0.0087890625, 0.220703125, 0.8642578125, 0.1083984375, 0.880859375, 0.0283203125, 0.1201171875, 0.0751953125, 0.0146484375, 0.0068359375, 0.0419921875, 0.005859375, 0.0078125, 0.0869140625, 0.080078125, 0.6435546875, 0.1337890625, 0.029296875, 0.037109375, 0.787109375, 0.072265625, 0.02734375, 0.0693359375, 0.0146484375, 0.12109375, 0.037109375, 0.0400390625, 0.5224609375, 0.0048828125, 0.9970703125, 0.1328125, 0.9033203125, 0.1025390625, 0.0146484375, 0.0224609375, 0.0087890625, 0.1318359375, 0.0, 0.8115234375, 0.009765625, 0.0107421875, 0.044921875, 0.1201171875, 0.0048828125, 0.1025390625, 0.02734375, 0.109375, 0.8916015625, 0.041015625, 0.0048828125, 0.109375, 0.13671875, 0.5771484375, 0.0859375, 0.2294921875, 0.0546875, 0.0390625, 0.09765625, 0.0166015625, 0.0322265625, 0.0087890625, 0.962890625, 0.0966796875, 0.0126953125, 0.064453125, 0.0068359375, 0.0078125, 0.1318359375, 0.9208984375, 0.0869140625, 0.005859375, 0.005859375, 0.00390625, 0.197265625, 0.005859375, 0.0107421875, 0.037109375, 0.12109375, 0.669921875, 0.033203125, 0.015625, 0.0068359375, 0.0986328125, 0.017578125, 0.0146484375, 0.041015625, 0.095703125, 0.9365234375, 0.19140625, 0.322265625, 0.029296875, 0.0087890625, 0.046875, 0.0, 0.0869140625, 0.033203125, 0.1884765625, 0.7451171875, 0.0, 0.0908203125, 0.0390625, 0.0341796875, 0.1513671875, 0.0546875, 0.1357421875, 0.13671875, 0.9970703125, 0.0205078125, 0.03515625, 0.0, 0.0126953125, 0.23046875, 0.03515625, 0.783203125, 0.09765625, 0.0048828125, 0.0029296875, 0.0166015625, 0.0087890625, 0.109375, 0.283203125, 0.01171875, 0.0166015625, 0.044921875, 0.0, 0.1044921875, 0.0009765625, 0.08984375, 0.041015625, 0.013671875, 0.013671875, 0.0009765625, 0.0205078125, 0.1689453125, 0.150390625, 0.0126953125, 0.5390625, 0.1328125, 0.0712890625, 0.1044921875, 0.119140625, 0.0185546875, 0.9970703125, 0.037109375, 0.115234375, 0.0693359375, 0.009765625, 0.205078125, 0.0546875, 0.013671875, 0.0927734375, 0.001953125, 0.0146484375, 0.998046875, 0.796875, 0.0205078125, 0.0830078125, 0.810546875, 0.01171875, 0.998046875, 0.0263671875, 0.0478515625, 0.0380859375, 0.033203125, 0.0087890625, 0.0107421875, 0.041015625, 0.1025390625, 0.0302734375, 0.9970703125, 0.021484375, 0.021484375, 0.021484375, 0.6669921875, 0.111328125, 0.0, 0.056640625, 0.0234375, 0.0205078125, 0.0, 0.0068359375, 0.0283203125, 0.0546875, 0.0283203125, 0.0771484375, 0.9970703125, 0.013671875, 0.009765625, 0.9970703125, 0.0068359375, 0.0458984375, 0.0068359375, 0.013671875, 0.0966796875, 0.123046875, 0.2685546875, 0.11328125, 0.0, 0.0234375, 0.0283203125, 0.0068359375, 0.0185546875, 0.0673828125, 0.1357421875, 0.23046875, 0.10546875, 0.8369140625, 0.09375, 0.193359375, 0.0029296875, 0.0107421875, 0.1396484375, 0.0234375, 0.0302734375, 0.013671875, 0.103515625, 0.8056640625, 0.9990234375, 0.822265625, 0.017578125, 0.037109375, 0.951171875, 0.0126953125, 0.0078125, 0.14453125, 0.0751953125, 0.0498046875, 0.84765625, 0.0009765625, 0.94140625, 0.0068359375, 0.037109375, 0.1025390625, 0.0244140625, 0.126953125, 0.337890625, 0.0322265625, 0.029296875, 0.1201171875, 0.005859375, 0.0029296875, 0.564453125, 0.0224609375, 0.0390625, 0.169921875, 0.1064453125, 0.0859375, 0.212890625, 0.0087890625, 0.0517578125, 0.1005859375, 0.091796875, 0.0244140625, 0.87890625, 0.017578125, 0.0078125, 0.6572265625, 0.0146484375, 0.0, 0.0703125, 0.1806640625, 0.0048828125, 0.087890625, 0.0185546875, 0.9970703125, 0.0361328125, 0.0986328125, 0.0048828125, 0.1923828125, 0.0048828125, 0.01171875, 0.01953125, 0.0107421875, 0.0205078125, 0.17578125, 0.1025390625, 0.0166015625, 0.0146484375, 0.0234375, 0.01171875, 0.1279296875, 0.9443359375, 0.046875, 0.767578125, 0.9169921875, 0.8125, 0.0595703125, 0.0556640625, 0.9404296875, 0.0400390625, 0.009765625, 0.0927734375, 0.03515625, 0.9970703125, 0.7568359375, 0.0009765625, 0.0263671875, 0.01171875, 0.0224609375, 0.0244140625, 0.03515625, 0.8056640625, 0.0146484375, 0.015625, 0.041015625, 0.06640625, 0.06640625, 0.0087890625, 0.0166015625, 0.07421875, 0.0126953125, 0.0732421875, 0.0869140625, 0.6337890625, 0.0615234375, 0.1611328125, 0.0029296875, 0.0068359375, 0.8642578125, 0.203125, 0.0771484375, 0.134765625, 0.7900390625, 0.1328125, 0.107421875, 0.0224609375, 0.0458984375, 0.619140625, 0.2265625, 0.51953125, 0.087890625, 0.0107421875, 0.0078125, 0.203125, 0.0068359375, 0.0810546875, 0.998046875, 0.0380859375, 0.0244140625, 0.0244140625, 0.2275390625, 0.1962890625, 0.033203125, 0.0380859375, 0.03515625, 0.052734375, 0.998046875, 0.1064453125, 0.9970703125, 0.0126953125, 0.08984375, 0.00390625, 0.0107421875, 0.015625, 0.0166015625, 0.0107421875, 0.2529296875, 0.048828125, 0.9970703125, 0.763671875, 0.0341796875, 0.0146484375, 0.078125, 0.099609375, 0.078125, 0.0283203125, 0.2294921875, 0.0146484375, 0.01953125, 0.0673828125, 0.00390625, 0.1982421875, 0.1298828125, 0.01171875, 0.025390625, 0.009765625, 0.4482421875, 0.0849609375, 0.7607421875, 0.4326171875, 0.0107421875, 0.1044921875, 0.3037109375, 0.998046875, 0.044921875, 0.00390625, 0.009765625, 0.1259765625, 0.1005859375, 0.1474609375, 0.1044921875, 0.0205078125, 0.0, 0.0185546875, 0.2412109375, 0.01953125, 0.9306640625, 0.021484375, 0.0166015625, 0.03515625, 0.0, 0.302734375, 0.0146484375, 0.1181640625, 0.9970703125, 0.0205078125, 0.142578125, 0.5712890625, 0.1630859375, 0.068359375, 0.076171875, 0.0400390625, 0.0, 0.1005859375, 0.009765625, 0.015625, 0.0615234375, 0.009765625, 0.017578125, 0.0107421875, 0.0087890625, 0.0068359375, 0.7001953125, 0.87109375, 0.0341796875, 0.0, 0.998046875, 0.0146484375, 0.0068359375, 0.009765625, 0.0341796875, 0.0185546875, 0.015625, 0.0126953125, 0.30859375, 0.08203125, 0.8505859375, 0.0263671875, 0.0078125, 0.0068359375, 0.0546875, 0.0947265625, 0.0107421875, 0.177734375, 0.08984375, 0.14453125, 0.1181640625, 0.0517578125, 0.013671875, 0.0537109375, 0.0419921875, 0.03515625, 0.904296875, 0.0, 0.9326171875, 0.2275390625, 0.0263671875, 0.1484375, 0.9970703125, 0.025390625, 0.0546875, 0.0849609375, 0.0, 0.1318359375, 0.0625, 0.2119140625, 0.21484375, 0.03125, 0.1279296875, 0.0068359375, 0.07421875, 0.0244140625, 0.009765625, 0.0068359375, 0.005859375, 0.0234375, 0.0146484375, 0.1875, 0.1298828125, 0.029296875, 0.1064453125, 0.998046875, 0.341796875, 0.0693359375, 0.9970703125, 0.8857421875, 0.0517578125, 0.060546875, 0.0166015625, 0.310546875, 0.044921875, 0.1103515625, 0.01953125, 0.0283203125, 0.015625, 0.0234375, 0.00390625, 0.169921875, 0.3046875, 0.01953125, 0.0537109375, 0.8681640625, 0.2294921875, 0.19140625, 0.0087890625, 0.896484375, 0.001953125, 0.09375, 0.0029296875, 0.2294921875, 0.0478515625, 0.1201171875, 0.06640625, 0.1162109375, 0.021484375, 0.1318359375, 0.177734375, 0.1923828125, 0.01171875, 0.068359375, 0.1044921875, 0.744140625, 0.00390625, 0.9375, 0.033203125, 0.9033203125, 0.0419921875, 0.1123046875, 0.072265625, 0.123046875, 0.5888671875, 0.1337890625, 0.015625, 0.0947265625, 0.0234375, 0.037109375, 0.21875, 0.998046875, 0.001953125, 0.0361328125, 0.0234375, 0.111328125, 0.041015625, 0.73046875, 0.0703125, 0.0107421875, 0.0068359375, 0.048828125, 0.7177734375, 0.02734375, 0.037109375, 0.1337890625, 0.26953125, 0.1328125, 0.015625, 0.107421875, 0.1044921875, 0.083984375, 0.064453125, 0.0537109375, 0.0029296875, 0.0869140625, 0.0751953125, 0.0, 0.01953125, 0.09375, 0.0517578125, 0.173828125, 0.056640625, 0.0595703125, 0.0390625, 0.0927734375, 0.115234375, 0.0537109375, 0.03515625, 0.77734375, 0.0, 0.0185546875, 0.1943359375, 0.0712890625, 0.9130859375, 0.8955078125, 0.0673828125, 0.01953125, 0.08203125, 0.9970703125, 0.1015625, 0.7216796875, 0.013671875, 0.220703125, 0.7001953125]

 sparsity of   [0.50634765625, 0.083984375, 0.0390625, 0.02490234375, 0.1494140625, 0.53369140625, 0.43408203125, 0.02099609375, 0.015625, 0.2451171875, 0.341796875, 0.59619140625, 0.5087890625, 0.03125, 0.0419921875, 0.0283203125, 0.3271484375, 0.111328125, 0.04150390625, 0.0146484375, 0.021484375, 0.8193359375, 0.017578125, 0.064453125, 0.0380859375, 0.81884765625, 0.056640625, 0.0439453125, 0.99951171875, 0.01806640625, 0.34423828125, 0.486328125, 0.0791015625, 0.99560546875, 0.76171875, 0.04150390625, 0.0986328125, 0.35888671875, 0.1884765625, 0.03857421875, 0.0283203125, 0.0830078125, 0.0986328125, 0.04296875, 0.08447265625, 0.61083984375, 0.5390625, 0.06982421875, 0.3505859375, 0.90087890625, 0.3505859375, 0.1826171875, 0.99853515625, 0.4609375, 0.33447265625, 0.9033203125, 0.2490234375, 0.03125, 0.15380859375, 0.99951171875, 0.0869140625, 0.35400390625, 0.07861328125, 0.1435546875, 0.20556640625, 0.99365234375, 0.0625, 0.99853515625, 0.01171875, 0.0380859375, 0.0322265625, 0.33251953125, 0.0517578125, 0.048828125, 0.05517578125, 0.044921875, 0.3232421875, 0.02197265625, 0.04150390625, 0.3603515625, 0.11328125, 0.013671875, 0.08740234375, 0.53759765625, 0.03759765625, 0.03955078125, 0.06689453125, 0.03857421875, 0.798828125, 0.5185546875, 0.0166015625, 0.2978515625, 0.15576171875, 0.99951171875, 0.0322265625, 0.021484375, 0.34375, 0.15576171875, 0.037109375, 0.64208984375, 0.04296875, 0.0185546875, 0.80224609375, 0.05029296875, 0.04736328125, 0.17431640625, 0.9990234375, 0.2021484375, 0.02880859375, 0.01611328125, 0.10498046875, 0.30908203125, 0.08154296875, 0.33203125, 0.20458984375, 0.0439453125, 0.37890625, 0.01953125, 0.99951171875, 0.41552734375, 0.06494140625, 0.37158203125, 0.037109375, 0.01171875, 0.07275390625, 0.4580078125, 0.05712890625, 0.35888671875, 0.05078125, 0.1923828125, 0.015625, 0.0859375, 0.03076171875, 0.8525390625, 0.24365234375, 0.50341796875, 0.71826171875, 0.1318359375, 0.34228515625, 0.9072265625, 0.80615234375, 0.04150390625, 0.3583984375, 0.0263671875, 0.1181640625, 0.0439453125, 0.03076171875, 0.82568359375, 0.05029296875, 0.01953125, 0.3759765625, 0.01708984375, 0.04052734375, 0.0146484375, 0.0400390625, 0.015625, 0.0380859375, 0.17431640625, 0.8271484375, 0.99951171875, 0.0185546875, 0.0849609375, 0.0205078125, 0.33544921875, 0.0224609375, 0.88427734375, 0.2685546875, 0.99951171875, 0.0361328125, 0.06005859375, 0.38037109375, 0.0126953125, 0.7744140625, 0.296875, 0.99853515625, 0.052734375, 0.0322265625, 0.02685546875, 0.078125, 0.03466796875, 0.92626953125, 0.32666015625, 0.0185546875, 0.80615234375, 0.90771484375, 0.36279296875, 0.3837890625, 0.8515625, 0.05078125, 0.27294921875, 0.44384765625, 0.12890625, 0.341796875, 0.99951171875, 0.06640625, 0.22216796875, 0.8916015625, 0.27685546875, 0.1416015625, 0.99951171875, 0.076171875, 0.12939453125, 0.99951171875, 0.248046875, 0.7998046875, 0.04833984375, 0.02099609375, 0.033203125, 0.0888671875, 0.03759765625, 0.0146484375, 0.36083984375, 0.365234375, 0.01220703125, 0.05126953125, 0.35546875, 0.0732421875, 0.37890625, 0.0400390625, 0.04638671875, 0.021484375, 0.6982421875, 0.03759765625, 0.18359375, 0.34521484375, 0.5234375, 0.12841796875, 0.03955078125, 0.0341796875, 0.36767578125, 0.74609375, 0.21337890625, 0.818359375, 0.26416015625, 0.6162109375, 0.01904296875, 0.0986328125, 0.02734375, 0.02490234375, 0.36767578125, 0.064453125, 0.02734375, 0.21337890625, 0.94677734375, 0.748046875, 0.025390625, 0.01513671875, 0.02587890625, 0.9990234375, 0.0634765625, 0.125, 0.06640625, 0.3671875, 0.24267578125, 0.8544921875, 0.01953125, 0.35888671875, 0.34033203125, 0.04150390625, 0.94970703125, 0.07666015625, 0.0478515625, 0.0126953125, 0.837890625, 0.8935546875, 0.99951171875, 0.9990234375, 0.056640625, 0.0224609375, 0.771484375, 0.43994140625, 0.31982421875, 0.1494140625, 0.7216796875, 0.3642578125, 0.03857421875, 0.85546875, 0.0732421875, 0.05859375, 0.05078125, 0.37744140625, 0.146484375, 0.02001953125, 0.0849609375, 0.5888671875, 0.7490234375, 0.791015625, 0.99853515625, 0.3544921875, 0.02685546875, 0.06591796875, 0.33935546875, 0.02880859375, 0.03759765625, 0.25537109375, 0.5947265625, 0.3544921875, 0.90185546875, 0.9990234375, 0.33837890625, 0.5205078125, 0.99951171875, 0.515625, 0.83056640625, 0.33935546875, 0.4541015625, 0.04443359375, 0.4833984375, 0.33349609375, 0.02001953125, 0.38427734375, 0.0234375, 0.39404296875, 0.05126953125, 0.0419921875, 0.90576171875, 0.0615234375, 0.568359375, 0.47216796875, 0.02783203125, 0.04345703125, 0.041015625, 0.810546875, 0.08203125, 0.03564453125, 0.03173828125, 0.36865234375, 0.0361328125, 0.033203125, 0.0302734375, 0.048828125, 0.11083984375, 0.0224609375, 0.9990234375, 0.03173828125, 0.04638671875, 0.0341796875, 0.03857421875, 0.92041015625, 0.1474609375, 0.0263671875, 0.03662109375, 0.0419921875, 0.921875, 0.01904296875, 0.12890625, 0.04296875, 0.08935546875, 0.02490234375, 0.0615234375, 0.9990234375, 0.0419921875, 0.1875, 0.1904296875, 0.37890625, 0.03173828125, 0.20166015625, 0.046875, 0.0986328125, 0.07763671875, 0.06689453125, 0.0478515625, 0.041015625, 0.013671875, 0.33984375, 0.99951171875, 0.6640625, 0.30615234375, 0.77197265625, 0.3505859375, 0.04150390625, 0.05126953125, 0.03662109375, 0.57177734375, 0.05810546875, 0.19140625, 0.4296875, 0.26806640625, 0.044921875, 0.1416015625, 0.0537109375, 0.06787109375, 0.0166015625, 0.02587890625, 0.9990234375, 0.046875, 0.02197265625, 0.26611328125, 0.4482421875, 0.060546875, 0.03125, 0.4013671875, 0.35302734375, 0.03564453125, 0.8486328125, 0.3408203125, 0.3603515625, 0.02587890625, 0.0908203125, 0.0361328125, 0.01708984375, 0.626953125, 0.16259765625, 0.03369140625, 0.23095703125, 0.9990234375, 0.02880859375, 0.1318359375, 0.70654296875, 0.04443359375, 0.72021484375, 0.376953125, 0.08056640625, 0.06298828125, 0.9990234375, 0.5380859375, 0.0693359375, 0.10205078125, 0.19189453125, 0.015625, 0.35400390625, 0.0615234375, 0.0380859375, 0.91064453125, 0.0556640625, 0.025390625, 0.03857421875, 0.04638671875, 0.2861328125, 0.4296875, 0.296875, 0.02783203125, 0.10546875, 0.33544921875, 0.03515625, 0.0419921875, 0.05517578125, 0.0556640625, 0.02294921875, 0.0302734375, 0.01318359375, 0.0107421875, 0.15380859375, 0.99951171875, 0.48779296875, 0.0380859375, 0.75439453125, 0.4306640625, 0.5537109375, 0.0234375, 0.02490234375, 0.41552734375, 0.947265625, 0.0400390625, 0.111328125, 0.0458984375, 0.0478515625, 0.37890625, 0.1259765625, 0.0341796875, 0.033203125, 0.2099609375, 0.7763671875, 0.037109375, 0.03857421875, 0.064453125, 0.03466796875, 0.19677734375, 0.99951171875, 0.0322265625, 0.35009765625, 0.0302734375, 0.0654296875, 0.0458984375, 0.3642578125, 0.029296875, 0.33935546875, 0.04052734375, 0.03515625, 0.0771484375, 0.78271484375, 0.0439453125, 0.796875, 0.35791015625, 0.0419921875, 0.10498046875, 0.8310546875, 0.560546875, 0.9990234375, 0.99853515625, 0.42724609375, 0.99853515625, 0.431640625, 0.04248046875, 0.021484375, 0.05615234375, 0.71826171875, 0.45703125, 0.04541015625, 0.3115234375, 0.4609375, 0.02587890625, 0.30615234375, 0.03955078125, 0.87939453125, 0.0625, 0.02734375, 0.02685546875, 0.8310546875, 0.11328125, 0.0693359375, 0.04248046875]

 sparsity of   [0.0006510416860692203, 0.02083333395421505, 0.0017361111240461469, 0.0, 0.0, 0.0579427070915699, 0.9993489384651184, 0.015625, 0.0581597238779068, 0.02473958395421505, 0.0614149309694767, 0.03125, 0.0444878488779068, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0186631940305233, 0.02582465298473835, 0.0, 0.0533854179084301, 0.0262586809694767, 0.0405815988779068, 0.0, 0.00021701389050576836, 0.0, 0.010416666977107525, 0.0, 0.0, 0.0015190972480922937, 0.0, 0.0698784738779068, 0.02560763992369175, 0.01323784701526165, 0.0145399309694767, 0.02213541604578495, 0.0, 0.0, 0.0442708320915699, 0.0013020833721384406, 0.0223524309694767, 0.071180559694767, 0.0017361111240461469, 0.0, 0.0954861119389534, 0.01822916604578495, 0.999131977558136, 0.01171875, 0.009548611007630825, 0.0327690988779068, 0.007595486007630825, 0.0436197929084301, 0.0, 0.00021701389050576836, 0.0, 0.0047743055038154125, 0.011501736007630825, 0.02994791604578495, 0.0442708320915699, 0.9993489384651184, 0.0310329869389534, 0.0, 0.0, 0.0, 0.008463541977107525, 0.0028211805038154125, 0.03125, 0.0, 0.0, 0.01323784701526165, 0.001953125, 0.03515625, 0.012369791977107525, 0.0232204869389534, 0.0212673619389534, 0.005859375, 0.9995659589767456, 0.0, 0.0, 0.0622829869389534, 0.0, 0.1206597238779068, 0.0125868059694767, 0.0, 0.0694444477558136, 0.0, 0.0, 0.0, 0.068359375, 0.014322916977107525, 0.4123263955116272, 0.2758246660232544, 0.0321180559694767, 0.0451388880610466, 0.00629340298473835, 0.02387152798473835, 0.0386284738779068, 0.00933159701526165, 0.0271267369389534, 0.0, 0.3832465410232544, 0.0, 0.0360243059694767, 0.0049913194961845875, 0.0, 0.017578125, 0.0310329869389534, 0.0618489570915699, 0.0, 0.02582465298473835, 0.0, 0.0145399309694767, 0.0, 0.00021701389050576836, 0.0, 0.0004340277810115367, 0.0405815988779068, 0.02365451492369175, 0.0023871527519077063, 0.046875, 0.0206163190305233, 0.01584201492369175, 0.00021701389050576836, 0.02365451492369175, 0.0, 0.008463541977107525, 0.0394965298473835, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0251736119389534, 0.01714409701526165, 0.0, 0.0086805559694767, 0.0902777761220932, 0.029296875, 0.0, 0.0008680555620230734, 0.0, 0.0553385429084301, 0.0, 0.0, 0.0, 0.0271267369389534, 0.0, 0.0334201380610466, 0.088758684694767, 0.02213541604578495, 0.0069444444961845875, 0.0006510416860692203, 0.02278645895421505, 0.515625, 0.0321180559694767, 0.0271267369389534, 0.005425347480922937, 0.01171875, 0.0327690988779068, 0.0008680555620230734, 0.0431857630610466, 0.0, 0.01605902798473835, 0.0, 0.0, 0.0, 0.01019965298473835, 0.01888020895421505, 0.0047743055038154125, 0.0, 0.0164930559694767, 0.02734375, 0.0364583320915699, 0.0015190972480922937, 0.046875, 0.0310329869389534, 0.010416666977107525, 0.0006510416860692203, 0.0, 0.0440538190305233, 0.0, 0.0, 0.01519097201526165, 0.02951388992369175, 0.0583767369389534, 0.0, 0.02560763992369175, 0.005859375, 0.0, 0.0349392369389534, 0.011067708022892475, 0.0, 0.0340711809694767, 0.02864583395421505, 0.011935763992369175, 0.0, 0.0015190972480922937, 0.0687934011220932, 0.00021701389050576836, 0.009114583022892475, 0.0316840298473835, 0.006076388992369175, 0.0086805559694767, 0.0030381944961845875, 0.0538194440305233, 0.00390625, 0.0, 0.0, 0.0, 0.0032552082557231188, 0.0431857630610466, 0.01779513992369175, 0.00021701389050576836, 0.00021701389050576836, 0.0358072929084301, 0.0, 0.02734375, 0.2350260466337204, 0.009114583022892475, 0.0, 0.01996527798473835, 0.017578125, 0.0598958320915699, 0.00021701389050576836, 0.007595486007630825, 0.0, 0.0, 0.0, 0.015407986007630825, 0.02495659701526165, 0.0329861119389534, 0.02973090298473835, 0.0145399309694767, 0.0, 0.0, 0.01128472201526165, 0.0, 0.01019965298473835, 0.0418836809694767, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.02690972201526165, 0.0, 0.008029513992369175, 0.0045572915114462376, 0.02387152798473835, 0.0049913194961845875, 0.0245225690305233, 0.04296875, 0.00021701389050576836, 0.0, 0.02278645895421505, 0.0010850694961845875, 0.013020833022892475, 0.01584201492369175, 0.01692708395421505, 0.02669270895421505, 0.0375434048473835, 0.0, 0.0327690988779068, 0.0069444444961845875, 0.0, 0.0425347238779068, 0.009765625, 0.0262586809694767, 0.0047743055038154125, 0.0379774309694767, 0.0, 0.007595486007630825, 0.007378472480922937, 0.0455729179084301, 0.0, 0.0193142369389534, 0.01128472201526165, 0.0047743055038154125, 0.0, 0.00021701389050576836, 0.02105034701526165, 0.0232204869389534, 0.041015625, 0.00021701389050576836, 0.0008680555620230734, 0.0, 0.0, 0.0067274305038154125, 0.00021701389050576836, 0.0475260429084301, 0.02387152798473835, 0.002170138992369175, 0.0475260429084301, 0.0, 0.0, 0.01692708395421505, 0.01215277798473835, 0.02473958395421505, 0.011501736007630825, 0.0405815988779068, 0.0, 0.0, 0.02408854104578495, 0.0034722222480922937, 0.00021701389050576836, 0.0334201380610466, 0.4142795205116272, 0.9963107705116272, 0.0, 0.0, 0.0, 0.0006510416860692203, 0.0232204869389534, 0.0106336809694767, 0.0, 0.014322916977107525, 0.0030381944961845875, 0.01801215298473835, 0.0032552082557231188, 0.069227434694767, 0.0499131940305233, 0.0642361119389534, 0.0, 0.007378472480922937, 0.017578125, 0.0579427070915699, 0.011935763992369175, 0.015625, 0.8943142294883728, 0.0036892362404614687, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0290798619389534, 0.0065104165114462376, 0.0282118059694767, 0.118055559694767, 0.0, 0.0479600690305233, 0.0, 0.00021701389050576836, 0.02278645895421505, 0.012369791977107525, 0.0017361111240461469, 0.0, 0.0319010429084301, 0.0282118059694767, 0.0, 0.0, 0.014756944961845875, 0.009982638992369175, 0.00021701389050576836, 0.0, 0.0173611119389534, 0.0, 0.0421006940305233, 0.0323350690305233, 0.0347222238779068, 0.0067274305038154125, 0.0, 0.03059895895421505, 0.0477430559694767, 0.0355902798473835, 0.0264756940305233, 0.0, 0.0, 0.0271267369389534, 0.0, 0.01215277798473835, 0.0, 0.0, 0.0206163190305233, 0.01974826492369175, 0.0, 0.0368923619389534, 0.0, 0.0145399309694767, 0.0, 0.0759548619389534, 0.0282118059694767, 0.015625, 0.0314670130610466, 0.9995659589767456, 0.0264756940305233, 0.02734375, 0.0, 0.0, 0.0, 0.6048176884651184, 0.0, 0.0, 0.0008680555620230734, 0.0, 0.0520833320915699, 0.02864583395421505, 0.0, 0.0, 0.0588107630610466, 0.02777777798473835, 0.0, 0.00021701389050576836, 0.0381944440305233, 0.0015190972480922937, 0.02105034701526165, 0.0034722222480922937, 0.008029513992369175, 0.0, 0.076171875, 0.0203993059694767, 0.014322916977107525, 0.02734375, 0.0483940988779068, 0.015625, 0.0, 0.0, 0.0004340277810115367, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9125434160232544, 0.0, 0.0470920130610466, 0.0106336809694767, 0.0, 0.007595486007630825, 0.011067708022892475, 0.1169704869389534, 0.010416666977107525, 0.0, 0.02191840298473835, 0.01714409701526165, 0.0069444444961845875, 0.0, 0.0271267369389534, 0.0310329869389534, 0.03081597201526165, 0.0412326380610466, 0.009548611007630825, 0.0, 0.0, 0.0167100690305233, 0.0, 0.0625, 0.0, 0.0, 0.00021701389050576836, 0.0193142369389534, 0.033203125, 0.073133684694767, 0.029296875, 0.0813802108168602, 0.0186631940305233, 0.0, 0.0013020833721384406, 0.01605902798473835, 0.01909722201526165, 0.00021701389050576836, 0.00021701389050576836, 0.00021701389050576836, 0.0206163190305233, 0.0, 0.0540364570915699, 0.0616319440305233, 0.02387152798473835, 0.0, 0.0203993059694767, 0.010850694961845875, 0.009982638992369175, 0.00021701389050576836, 0.0340711809694767, 0.011067708022892475, 0.0, 0.0006510416860692203, 0.0, 0.0282118059694767, 0.0342881940305233, 0.0047743055038154125, 0.008029513992369175, 0.00933159701526165, 0.0, 0.041015625, 0.0, 0.0262586809694767, 0.0, 0.01128472201526165, 0.0, 0.02777777798473835, 0.048828125, 0.0473090298473835, 0.8435329794883728, 0.0, 0.0, 0.0164930559694767, 0.04296875, 0.0, 0.0, 0.02994791604578495, 0.002170138992369175, 0.012369791977107525, 0.02278645895421505, 0.0, 0.004123263992369175, 0.02408854104578495, 0.0, 0.00933159701526165]

 sparsity of   [0.0, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.015625, 0.01953125, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.021484375, 0.00390625, 0.005859375, 0.390625, 0.001953125, 0.00390625, 0.0, 0.021484375, 0.001953125, 0.044921875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.037109375, 0.001953125, 0.0, 0.033203125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.00390625, 0.005859375, 0.005859375, 0.001953125, 0.0, 0.03125, 0.005859375, 0.013671875, 0.0, 0.01171875, 0.029296875, 0.0078125, 0.046875, 0.033203125, 0.0, 0.029296875, 0.009765625, 0.013671875, 0.0, 0.00390625, 0.009765625, 0.001953125, 0.046875, 0.0, 0.01171875, 0.068359375, 0.0, 0.0, 0.0234375, 0.0, 0.041015625, 0.0, 0.0, 0.015625, 0.0, 0.013671875, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.033203125, 0.009765625, 0.0, 0.095703125, 0.12890625, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.017578125, 0.021484375, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.015625, 0.009765625, 0.0, 0.037109375, 0.0, 0.0, 0.0, 0.021484375, 0.04296875, 0.017578125, 0.0078125, 0.0, 0.0, 0.00390625, 0.0078125, 0.00390625, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.080078125, 0.015625, 0.0, 0.021484375, 0.01171875, 0.0, 0.0, 0.0, 0.009765625, 0.03515625, 0.0, 0.033203125, 0.0, 0.0, 0.017578125, 0.021484375, 0.00390625, 0.982421875, 0.0, 0.0, 0.00390625, 0.0, 0.009765625, 0.00390625, 0.0, 0.0078125, 0.0, 0.009765625, 0.0234375, 0.0078125, 0.0, 0.0, 0.0, 0.93359375, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.9296875, 0.041015625, 0.0, 0.005859375, 0.009765625, 0.0, 0.017578125, 0.0, 0.005859375, 0.001953125, 0.0, 0.841796875, 0.013671875, 0.005859375, 0.013671875, 0.0, 0.01171875, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.017578125, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.041015625, 0.025390625, 0.0, 0.025390625, 0.00390625, 0.001953125, 0.0078125, 0.021484375, 0.0546875, 0.0, 0.0, 0.001953125, 0.05859375, 0.005859375, 0.00390625, 0.0, 0.017578125, 0.0, 0.001953125, 0.0, 0.00390625, 0.0, 0.0078125, 0.0, 0.01953125, 0.0, 0.001953125, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.046875, 0.0, 0.001953125, 0.294921875, 0.072265625, 0.927734375, 0.01171875, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.93359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.994140625, 0.0078125, 0.001953125, 0.0, 0.0390625, 0.0, 0.001953125, 0.0, 0.01171875, 0.01171875, 0.0, 0.0, 0.005859375, 0.01953125, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.02734375, 0.0, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.080078125, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.029296875, 0.0078125, 0.009765625, 0.005859375, 0.0, 0.005859375, 0.0, 0.0390625, 0.03515625, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.07421875, 0.029296875, 0.015625, 0.0, 0.0, 0.02734375, 0.93359375, 0.001953125, 0.0, 0.01171875, 0.017578125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.01953125, 0.033203125, 0.029296875, 0.0, 0.0, 0.0, 0.00390625, 0.021484375, 0.931640625, 0.0, 0.001953125, 0.0390625, 0.00390625, 0.0, 0.05078125, 0.9453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0390625, 0.93359375, 0.0, 0.0, 0.0, 0.03515625, 0.001953125, 0.05078125, 0.0, 0.0, 0.05859375, 0.0, 0.02734375, 0.0, 0.416015625, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.0, 0.11328125, 0.001953125, 0.00390625, 0.0078125, 0.0, 0.0390625, 0.0, 0.029296875, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.005859375, 0.01953125, 0.015625, 0.0078125, 0.001953125, 0.0078125, 0.0, 0.0, 0.021484375, 0.00390625, 0.03515625, 0.0, 0.0078125, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.01953125, 0.0, 0.001953125, 0.00390625, 0.0, 0.0078125, 0.025390625, 0.0234375, 0.001953125, 0.015625, 0.0, 0.005859375, 0.0234375, 0.017578125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.888671875, 0.556640625, 0.0703125, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.01171875, 0.0, 0.0, 0.015625, 0.02734375, 0.0, 0.001953125, 0.900390625, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.001953125, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93359375, 0.041015625, 0.005859375, 0.01171875, 0.05078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.025390625, 0.0, 0.00390625, 0.0, 0.015625, 0.017578125, 0.0078125, 0.001953125, 0.78515625, 0.021484375, 0.015625, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.994140625, 0.0, 0.0078125, 0.0, 0.009765625, 0.001953125, 0.0, 0.0, 0.05859375, 0.0, 0.00390625, 0.009765625, 0.0, 0.146484375, 0.021484375, 0.001953125, 0.0, 0.04296875, 0.0078125, 0.0, 0.056640625, 0.009765625, 0.001953125, 0.0078125, 0.001953125, 0.015625, 0.0, 0.03515625, 0.00390625, 0.0, 0.93359375, 0.001953125, 0.0, 0.041015625, 0.044921875, 0.0, 0.0078125, 0.001953125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.93359375, 0.0078125, 0.0, 0.017578125, 0.0, 0.025390625, 0.0, 0.0, 0.087890625, 0.189453125, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.00390625, 0.0859375, 0.001953125, 0.005859375, 0.01171875, 0.0, 0.04296875, 0.001953125, 0.001953125, 0.0, 0.0, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025390625, 0.037109375, 0.0, 0.0, 0.0390625, 0.00390625, 0.001953125, 0.001953125, 0.0, 0.02734375, 0.0, 0.0, 0.01171875, 0.017578125, 0.0, 0.001953125, 0.0078125, 0.001953125, 0.0, 0.0, 0.041015625, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.009765625, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.994140625, 0.005859375, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0703125, 0.01953125, 0.009765625, 0.072265625, 0.01171875, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.01171875, 0.03125, 0.0, 0.0, 0.076171875, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.017578125, 0.056640625, 0.0, 0.001953125, 0.021484375, 0.0, 0.001953125, 0.369140625, 0.005859375, 0.0, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0546875, 0.01953125, 0.0, 0.01171875, 0.02734375, 0.0, 0.01171875, 0.0, 0.0, 0.001953125, 0.001953125, 0.0, 0.0, 0.001953125, 0.005859375, 0.001953125, 0.01171875, 0.0, 0.00390625, 0.0625, 0.0, 0.015625, 0.001953125, 0.017578125, 0.0, 0.0, 0.015625, 0.01171875, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.01171875, 0.0, 0.07421875, 0.14453125, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.009765625, 0.0, 0.0546875, 0.0078125, 0.0, 0.013671875, 0.0, 0.013671875, 0.1015625, 0.02734375, 0.0, 0.044921875, 0.02734375, 0.986328125, 0.0, 0.0, 0.015625, 0.0, 0.001953125, 0.0078125, 0.0, 0.0, 0.0234375, 0.005859375, 0.0, 0.0, 0.26953125, 0.0, 0.025390625, 0.044921875, 0.009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0234375, 0.0, 0.0, 0.021484375, 0.013671875, 0.998046875, 0.0, 0.126953125, 0.01171875, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.001953125, 0.0, 0.0078125, 0.0, 0.0, 0.001953125, 0.015625, 0.013671875, 0.0, 0.03515625, 0.01953125, 0.00390625, 0.0, 0.0, 0.0, 0.171875, 0.01953125, 0.001953125, 0.0, 0.0, 0.025390625, 0.046875, 0.005859375, 0.998046875, 0.017578125, 0.0, 0.0, 0.041015625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.001953125, 0.001953125, 0.0, 0.01953125, 0.0, 0.0078125, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.009765625, 0.005859375, 0.00390625, 0.015625, 0.025390625, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.03125, 0.0, 0.056640625, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.009765625, 0.0, 0.095703125, 0.0078125, 0.0, 0.08203125, 0.0, 0.0390625, 0.931640625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.044921875, 0.04296875, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.001953125, 0.0, 0.00390625, 0.009765625, 0.0, 0.0, 0.0546875, 0.021484375, 0.029296875, 0.0, 0.0, 0.0, 0.005859375, 0.509765625, 0.00390625, 0.0, 0.0, 0.0, 0.00390625, 0.00390625, 0.001953125, 0.0390625, 0.001953125, 0.005859375, 0.0, 0.00390625, 0.00390625, 0.041015625, 0.001953125, 0.03515625, 0.01953125, 0.01171875, 0.001953125, 0.0, 0.005859375, 0.0, 0.015625, 0.0, 0.001953125, 0.005859375, 0.025390625, 0.0, 0.0, 0.001953125, 0.0, 0.01953125, 0.0, 0.0, 0.03515625, 0.015625, 0.05859375, 0.001953125, 0.0, 0.0, 0.0, 0.283203125, 0.0, 0.005859375, 0.0, 0.005859375, 0.00390625, 0.0, 0.0, 0.03125, 0.033203125, 0.015625, 0.0, 0.005859375, 0.0, 0.03515625, 0.142578125, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0078125, 0.013671875, 0.0, 0.0, 0.0, 0.005859375, 0.00390625, 0.009765625, 0.0234375, 0.052734375, 0.0, 0.0, 0.0, 0.001953125, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.013671875, 0.0, 0.0, 0.052734375, 0.009765625, 0.0078125, 0.09375, 0.0, 0.01171875, 0.00390625, 0.0, 0.9296875, 0.0, 0.0, 0.0, 0.00390625, 0.0078125, 0.001953125, 0.0, 0.029296875, 0.0, 0.01953125, 0.0, 0.0, 0.021484375, 0.0, 0.03125, 0.14453125, 0.0, 0.0546875, 0.005859375, 0.0, 0.03125, 0.017578125, 0.0, 0.001953125, 0.01953125, 0.00390625, 0.025390625, 0.00390625, 0.01171875, 0.0, 0.009765625, 0.037109375, 0.0, 0.0, 0.00390625, 0.017578125, 0.0, 0.015625, 0.025390625, 0.70703125, 0.013671875, 0.0, 0.0, 0.025390625, 0.056640625, 0.0, 0.0, 0.0078125, 0.0, 0.025390625, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.001953125, 0.033203125, 0.021484375, 0.001953125, 0.056640625, 0.00390625, 0.0078125, 0.0, 0.0, 0.0078125, 0.009765625, 0.078125, 0.00390625, 0.0, 0.013671875, 0.021484375, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.025390625, 0.0, 0.0, 0.017578125, 0.0, 0.080078125, 0.025390625, 0.0078125, 0.01953125, 0.0, 0.013671875, 0.0, 0.015625, 0.005859375, 0.001953125, 0.0, 0.0234375, 0.0, 0.0, 0.033203125, 0.0, 0.0, 0.001953125, 0.001953125, 0.005859375, 0.015625, 0.158203125, 0.0, 0.046875, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.015625, 0.005859375, 0.0, 0.0078125, 0.033203125, 0.017578125, 0.0, 0.021484375, 0.017578125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.01953125, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.01171875, 0.017578125, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.015625, 0.001953125, 0.0, 0.0, 0.017578125, 0.00390625, 0.998046875, 0.0, 0.005859375, 0.0, 0.0, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.001953125, 0.7421875, 0.00390625, 0.994140625, 0.015625, 0.0, 0.005859375, 0.0, 0.0, 0.00390625, 0.009765625, 0.0, 0.00390625, 0.0, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.005859375, 0.046875, 0.013671875, 0.0078125, 0.048828125, 0.0, 0.001953125, 0.00390625, 0.046875, 0.0, 0.001953125, 0.251953125, 0.0, 0.005859375, 0.025390625, 0.001953125, 0.0, 0.0546875, 0.05859375, 0.037109375, 0.0, 0.0, 0.017578125, 0.029296875, 0.0, 0.015625, 0.015625, 0.013671875, 0.001953125, 0.03515625, 0.1484375, 0.0, 0.0078125, 0.001953125, 0.0, 0.0, 0.001953125, 0.0078125, 0.0078125, 0.0, 0.0, 0.796875, 0.83984375, 0.02734375, 0.0, 0.0, 0.060546875, 0.01171875, 0.0, 0.0, 0.021484375, 0.0078125, 0.0, 0.0, 0.015625, 0.0, 0.025390625, 0.0, 0.0, 0.767578125, 0.001953125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.01953125, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.025390625, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.01171875, 0.0, 0.0, 0.0, 0.931640625, 0.0, 0.0078125, 0.0078125, 0.009765625, 0.0, 0.9296875, 0.00390625, 0.0, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.029296875, 0.0, 0.01171875, 0.0, 0.015625, 0.005859375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.001953125, 0.033203125, 0.03515625, 0.01953125, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.05078125, 0.01953125, 0.0703125, 0.001953125, 0.005859375, 0.0078125, 0.03125, 0.15234375, 0.05078125, 0.0, 0.021484375, 0.005859375, 0.0, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.01953125, 0.931640625, 0.0, 0.009765625, 0.00390625, 0.0, 0.04296875, 0.005859375, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.93359375, 0.0, 0.009765625, 0.017578125, 0.072265625, 0.0, 0.095703125, 0.1171875, 0.0, 0.001953125, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.021484375, 0.037109375, 0.048828125, 0.013671875, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0078125, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.021484375, 0.0, 0.013671875, 0.0, 0.013671875, 0.0, 0.00390625, 0.009765625, 0.0, 0.0546875, 0.916015625, 0.00390625, 0.005859375, 0.0, 0.0, 0.00390625, 0.052734375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.017578125, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.017578125, 0.001953125, 0.994140625, 0.04296875, 0.0, 0.001953125, 0.083984375, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.005859375, 0.0, 0.021484375, 0.041015625, 0.017578125, 0.0, 0.013671875, 0.0, 0.001953125, 0.00390625, 0.0, 0.03125, 0.025390625, 0.0, 0.931640625, 0.0, 0.021484375, 0.001953125, 0.001953125, 0.013671875, 0.0, 0.89453125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.02734375, 0.998046875, 0.013671875, 0.0, 0.93359375, 0.025390625, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.017578125, 0.0, 0.005859375, 0.0, 0.0, 0.01953125, 0.025390625, 0.06640625, 0.0, 0.00390625, 0.01953125, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.001953125, 0.0, 0.009765625, 0.03515625, 0.005859375, 0.1328125, 0.021484375, 0.0, 0.01953125, 0.0, 0.015625, 0.025390625, 0.017578125, 0.015625, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.01171875, 0.025390625, 0.001953125, 0.98828125, 0.1015625, 0.0, 0.0, 0.99609375, 0.0625, 0.0, 0.3984375, 0.927734375, 0.0390625, 0.0, 0.00390625, 0.03515625, 0.7734375, 0.0, 0.037109375, 0.00390625, 0.001953125, 0.0, 0.0078125, 0.00390625, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.013671875, 0.0078125, 0.0, 0.0, 0.04296875, 0.001953125, 0.0, 0.001953125, 0.041015625, 0.0, 0.93359375, 0.017578125, 0.0, 0.01171875, 0.009765625, 0.5625, 0.005859375, 0.001953125, 0.015625, 0.0, 0.0, 0.09765625, 0.03515625, 0.0, 0.0, 0.02734375, 0.0, 0.93359375, 0.0234375, 0.03515625, 0.0, 0.0, 0.01171875, 0.00390625, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.005859375, 0.00390625, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.001953125, 0.01953125, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.69140625, 0.048828125, 0.0, 0.0, 0.0, 0.93359375, 0.0, 0.0, 0.0234375, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.005859375, 0.0, 0.021484375, 0.0, 0.0, 0.052734375, 0.09375, 0.0, 0.00390625, 0.005859375, 0.0, 0.826171875, 0.001953125, 0.0, 0.033203125, 0.0, 0.0, 0.00390625, 0.0, 0.021484375, 0.033203125, 0.0078125, 0.005859375, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.001953125, 0.01171875, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.931640625, 0.0, 0.0078125, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.93359375, 0.080078125, 0.033203125, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.01171875, 0.84765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.0078125, 0.0, 0.0, 0.03125, 0.0, 0.009765625, 0.0, 0.9296875, 0.017578125, 0.435546875, 0.0, 0.001953125, 0.017578125, 0.01953125, 0.0, 0.00390625, 0.01171875, 0.0, 0.0, 0.0, 0.009765625, 0.0078125, 0.015625, 0.0, 0.025390625, 0.005859375, 0.009765625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.001953125, 0.0, 0.001953125, 0.0, 0.013671875, 0.00390625, 0.99609375, 0.0, 0.0, 0.001953125, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.12109375, 0.048828125, 0.0, 0.0, 0.046875, 0.029296875, 0.013671875, 0.0, 0.0, 0.01953125, 0.931640625, 0.0, 0.015625, 0.017578125, 0.0, 0.0, 0.01953125, 0.001953125, 0.0, 0.0, 0.0, 0.015625, 0.009765625, 0.005859375, 0.0, 0.0, 0.009765625, 0.0390625, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.005859375, 0.017578125, 0.0234375, 0.015625, 0.03125, 0.0, 0.119140625, 0.06640625, 0.0078125, 0.041015625, 0.0, 0.00390625, 0.001953125, 0.01953125, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.001953125, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.123046875, 0.021484375, 0.013671875, 0.005859375, 0.005859375, 0.98828125, 0.0, 0.005859375, 0.0, 0.037109375, 0.0, 0.02734375, 0.03515625, 0.998046875, 0.0, 0.01171875, 0.0, 0.0, 0.021484375, 0.0, 0.05078125, 0.0, 0.0, 0.07421875, 0.0234375, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.005859375, 0.001953125, 0.0, 0.017578125, 0.00390625, 0.0, 0.015625, 0.0078125, 0.00390625, 0.0, 0.01171875, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.064453125, 0.001953125, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.998046875, 0.0, 0.0, 0.0078125, 0.216796875, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.931640625, 0.0, 0.0078125, 0.0, 0.0, 0.0390625, 0.029296875, 0.0, 0.01171875, 0.0, 0.013671875, 0.0078125, 0.02734375, 0.0, 0.01171875, 0.01171875, 0.0, 0.009765625, 0.00390625, 0.001953125, 0.015625, 0.0, 0.005859375, 0.021484375, 0.93359375, 0.076171875, 0.03515625, 0.00390625, 0.12890625, 0.0, 0.392578125, 0.0, 0.001953125, 0.03125, 0.986328125, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.916015625, 0.0, 0.197265625, 0.029296875, 0.92578125, 0.0, 0.001953125, 0.0, 0.0, 0.001953125, 0.033203125, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.03515625, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.001953125, 0.01171875, 0.001953125, 0.0078125, 0.0, 0.00390625, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.033203125, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.0078125, 0.115234375, 0.00390625, 0.0, 0.0234375, 0.931640625, 0.001953125, 0.025390625, 0.0, 0.0, 0.021484375, 0.0, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.015625, 0.93359375, 0.0]

 sparsity of   [0.03271484375, 0.025390625, 0.0224609375, 0.0263671875, 0.0029296875, 0.0439453125, 0.0361328125, 0.0244140625, 0.021484375, 0.0625, 0.021484375, 0.01220703125, 0.05224609375, 0.0302734375, 0.02685546875, 0.02978515625, 0.01953125, 0.0439453125, 0.02880859375, 0.021484375, 0.0302734375, 0.99853515625, 0.00341796875, 0.03125, 0.04638671875, 0.021484375, 0.02490234375, 0.0283203125, 0.00830078125, 0.08935546875, 0.025390625, 0.99560546875, 0.015625, 0.0390625, 0.03955078125, 0.95751953125, 0.0419921875, 0.048828125, 0.0224609375, 0.01953125, 0.0, 0.99853515625, 0.02294921875, 0.06005859375, 0.12890625, 0.0, 0.04931640625, 0.01123046875, 0.0263671875, 0.0576171875, 0.02001953125, 0.0, 0.91015625, 0.00634765625, 0.00341796875, 0.0263671875, 0.02783203125, 0.0244140625, 0.0380859375, 0.02587890625, 0.03173828125, 0.02490234375, 0.02587890625, 0.08984375, 0.02783203125, 0.02734375, 0.03173828125, 0.0234375, 0.02685546875, 0.0224609375, 0.029296875, 0.0263671875, 0.02978515625, 0.0263671875, 0.0478515625, 0.0244140625, 0.01708984375, 0.03857421875, 0.04248046875, 0.03369140625, 0.02099609375, 0.01953125, 0.2060546875, 0.0751953125, 0.025390625, 0.02783203125, 0.029296875, 0.0771484375, 0.1123046875, 0.0185546875, 0.0498046875, 0.033203125, 0.033203125, 0.09765625, 0.0380859375, 0.00390625, 0.0, 0.03125, 0.5947265625, 0.0, 0.02587890625, 0.02734375, 0.013671875, 0.03564453125, 0.03271484375, 0.0986328125, 0.0361328125, 0.05517578125, 0.03955078125, 0.0009765625, 0.0537109375, 0.03125, 0.02880859375, 0.02392578125, 0.09619140625, 0.02294921875, 0.05126953125, 0.0576171875, 0.02001953125, 0.01416015625, 0.0185546875, 0.4423828125, 0.02294921875, 0.00439453125, 0.01806640625, 0.02001953125, 0.0068359375, 0.00048828125, 0.85205078125, 0.0224609375, 0.0068359375, 0.03515625, 0.0185546875, 0.03662109375, 0.0439453125, 0.0458984375, 0.0146484375, 0.0, 0.0439453125, 0.02978515625, 0.02880859375, 0.05615234375, 0.76611328125, 0.02294921875, 0.037109375, 0.00048828125, 0.02978515625, 0.02490234375, 0.021484375, 0.029296875, 0.03125, 0.04931640625, 0.05615234375, 0.021484375, 0.0859375, 0.0107421875, 0.0263671875, 0.02978515625, 0.05078125, 0.025390625, 0.0107421875, 0.02880859375, 0.013671875, 0.03125, 0.01171875, 0.31640625, 0.6845703125, 0.001953125, 0.017578125, 0.16748046875, 0.0146484375, 0.0673828125, 0.0810546875, 0.08935546875, 0.03173828125, 0.0361328125, 0.02685546875, 0.0234375, 0.00830078125, 0.1328125, 0.66943359375, 0.05419921875, 0.0537109375, 0.0107421875, 0.0302734375, 0.06103515625, 0.54638671875, 0.009765625, 0.0234375, 0.044921875, 0.033203125, 0.13720703125, 0.05322265625, 0.04541015625, 0.00927734375, 0.02685546875, 0.00341796875, 0.0380859375, 0.015625, 0.0263671875, 0.00341796875, 0.09912109375, 0.01318359375, 0.00732421875, 0.0244140625, 0.2763671875, 0.01806640625, 0.56005859375, 0.005859375, 0.033203125, 0.0, 0.0791015625, 0.01220703125, 0.947265625, 0.91650390625, 0.03173828125, 0.01806640625, 0.03271484375, 0.02099609375, 0.4892578125, 0.029296875, 0.00537109375, 0.02099609375, 0.67236328125, 0.1123046875, 0.03564453125, 0.02685546875, 0.005859375, 0.5390625, 0.07373046875, 0.02490234375, 0.03369140625, 0.0380859375, 0.03076171875, 0.06640625, 0.01171875, 0.0, 0.0830078125, 0.00048828125, 0.0087890625, 0.07861328125, 0.033203125, 0.02490234375, 0.00927734375, 0.0, 0.0, 0.015625, 0.0068359375, 0.01708984375, 0.0126953125, 0.982421875, 0.0341796875, 0.0322265625, 0.00537109375, 0.09716796875, 0.02197265625, 0.041015625, 0.0029296875, 0.48193359375, 0.03564453125, 0.02099609375, 0.0009765625, 0.01171875, 0.01953125, 0.04296875, 0.02587890625, 0.00390625, 0.00048828125, 0.00537109375, 0.08544921875, 0.00048828125, 0.0185546875, 0.47802734375, 0.07080078125, 0.044921875, 0.0234375, 0.04931640625, 0.01171875, 0.02880859375, 0.04638671875, 0.017578125, 0.02734375, 0.02294921875, 0.09521484375, 0.01611328125, 0.0, 0.04345703125, 0.01708984375, 0.015625, 0.01318359375, 0.1171875, 0.0048828125, 0.10009765625, 0.02294921875, 0.0224609375, 0.0546875, 0.04541015625, 0.0302734375, 0.03955078125, 0.013671875, 0.04443359375, 0.013671875, 0.044921875, 0.0126953125, 0.22509765625, 0.02294921875, 0.025390625, 0.015625, 0.01123046875, 0.0986328125, 0.06982421875, 0.0146484375, 0.01806640625, 0.02880859375, 0.0634765625, 0.03369140625, 0.0400390625, 0.01220703125, 0.09619140625, 0.166015625, 0.0087890625, 0.04736328125, 0.00927734375, 0.0244140625, 0.021484375, 0.00634765625, 0.02978515625, 0.021484375, 0.091796875, 0.0048828125, 0.03125, 0.01220703125, 0.03564453125, 0.021484375, 0.03173828125, 0.0166015625, 0.0, 0.0, 0.02587890625, 0.02001953125, 0.01171875, 0.033203125, 0.02783203125, 0.0, 0.08544921875, 0.01611328125, 0.0205078125, 0.017578125, 0.01611328125, 0.17626953125, 0.0234375, 0.02587890625, 0.0244140625, 0.03857421875, 0.0263671875, 0.00830078125, 0.0, 0.00830078125, 0.02783203125, 0.02294921875, 0.00390625, 0.001953125, 0.03466796875, 0.015625, 0.06494140625, 0.02783203125, 0.021484375, 0.00634765625, 0.201171875, 0.02392578125, 0.0, 0.044921875, 0.0166015625, 0.03857421875, 0.0205078125, 0.041015625, 0.02783203125, 0.0361328125, 0.0126953125, 0.02294921875, 0.0244140625, 0.03173828125, 0.0, 0.0341796875, 0.01025390625, 0.03125, 0.037109375, 0.0302734375, 0.01806640625, 0.03759765625, 0.0126953125, 0.0234375, 0.015625, 0.45068359375, 0.0126953125, 0.04736328125, 0.0478515625, 0.25048828125, 0.02001953125, 0.0048828125, 0.00634765625, 0.02880859375, 0.0458984375, 0.99853515625, 0.03125, 0.900390625, 0.04833984375, 0.02734375, 0.75341796875, 0.04931640625, 0.02001953125, 0.0205078125, 0.0234375, 0.02978515625, 0.0, 0.45166015625, 0.072265625, 0.0, 0.05712890625, 0.02587890625, 0.0302734375, 0.02294921875, 0.0146484375, 0.03125, 0.00927734375, 0.0205078125, 0.021484375, 0.017578125, 0.013671875, 0.04248046875, 0.01220703125, 0.060546875, 0.03173828125, 0.06787109375, 0.0390625, 0.015625, 0.02197265625, 0.005859375, 0.095703125, 0.1171875, 0.04638671875, 0.90966796875, 0.009765625, 0.03759765625, 0.01220703125, 0.0361328125, 0.05126953125, 0.04443359375, 0.0712890625, 0.02734375, 0.96142578125, 0.56982421875, 0.5693359375, 0.5810546875, 0.111328125, 0.01318359375, 0.05322265625, 0.0185546875, 0.015625, 0.10498046875, 0.03759765625, 0.025390625, 0.00341796875, 0.03271484375, 0.05322265625, 0.0068359375, 0.01611328125, 0.12890625, 0.02001953125, 0.00048828125, 0.021484375, 0.0146484375, 0.0625, 0.421875, 0.02294921875, 0.00244140625, 0.0693359375, 0.01171875, 0.017578125, 0.0, 0.0400390625, 0.0341796875, 0.0234375, 0.0029296875, 0.2041015625, 0.05859375, 0.05419921875, 0.03369140625, 0.99853515625, 0.0302734375, 0.02294921875, 0.013671875, 0.00634765625, 0.02880859375, 0.58447265625, 0.11572265625, 0.0341796875, 0.04541015625, 0.00927734375, 0.01318359375, 0.0, 0.07080078125, 0.03271484375, 0.38232421875, 0.033203125, 0.037109375, 0.02490234375, 0.10205078125, 0.91943359375, 0.0625, 0.03515625, 0.060546875]

 sparsity of   [0.0, 0.00824652798473835, 0.0004340277810115367, 0.0067274305038154125, 0.0, 0.0394965298473835, 0.0026041667442768812, 0.013888888992369175, 0.00021701389050576836, 0.014973958022892475, 0.02604166604578495, 0.0164930559694767, 0.0049913194961845875, 0.0492621548473835, 0.0421006940305233, 0.0, 0.0568576380610466, 0.5926649570465088, 0.0086805559694767, 0.0412326380610466, 0.013454861007630825, 0.009114583022892475, 0.01605902798473835, 0.0, 0.0388454869389534, 0.004123263992369175, 0.0164930559694767, 0.0, 0.001953125, 0.767578125, 0.0783420130610466, 0.0440538190305233, 0.0850694477558136, 0.02973090298473835, 0.0531684048473835, 0.6647135615348816, 0.0, 0.0045572915114462376, 0.002170138992369175, 0.01410590298473835, 0.0030381944961845875, 0.0323350690305233, 0.0, 0.0486111119389534, 0.0520833320915699, 0.0004340277810115367, 0.7037760615348816, 0.01909722201526165, 0.0, 0.01215277798473835, 0.013020833022892475, 0.0401475690305233, 0.0, 0.0, 0.0340711809694767, 0.012369791977107525, 0.1137152761220932, 0.01909722201526165, 0.0, 0.0635850727558136, 0.0818142369389534, 0.02387152798473835, 0.011067708022892475, 0.0492621548473835, 0.1221788227558136, 0.0, 0.0271267369389534, 0.02669270895421505, 0.0, 0.005642361007630825, 0.0, 0.9995659589767456, 0.0, 0.0004340277810115367, 0.0540364570915699, 0.0036892362404614687, 0.02604166604578495, 0.0282118059694767, 0.02408854104578495, 0.005859375, 0.0455729179084301, 0.009982638992369175, 0.0, 0.0, 0.0034722222480922937, 0.02777777798473835, 0.002170138992369175, 0.01996527798473835, 0.0, 0.02408854104578495, 0.008897569961845875, 0.0885416641831398, 0.00021701389050576836, 0.3736979067325592, 0.0388454869389534, 0.0032552082557231188, 0.0004340277810115367, 0.0, 0.007595486007630825, 0.005859375, 0.0, 0.0894097238779068, 0.0496961809694767, 0.0390625, 0.4166666567325592, 0.4513888955116272, 0.02951388992369175, 0.02690972201526165, 0.0, 0.01779513992369175, 0.0004340277810115367, 0.0004340277810115367, 0.03059895895421505, 0.0, 0.005859375, 0.0885416641831398, 0.0438368059694767, 0.02191840298473835, 0.01128472201526165, 0.03515625, 0.0674913227558136, 0.0212673619389534, 0.0034722222480922937, 0.01888020895421505, 0.005642361007630825, 0.0705295130610466, 0.01584201492369175, 0.0225694440305233, 0.01519097201526165, 0.0212673619389534, 0.0329861119389534, 0.0776909738779068, 0.0, 0.00021701389050576836, 0.0013020833721384406, 0.002170138992369175, 0.0, 0.017578125, 0.0349392369389534, 0.0355902798473835, 0.0015190972480922937, 0.0301649309694767, 0.00021701389050576836, 0.02495659701526165, 0.0, 0.0342881940305233, 0.0529513880610466, 0.0379774309694767, 0.01019965298473835, 0.0499131940305233, 0.02777777798473835, 0.0, 0.9993489384651184, 0.0, 0.0327690988779068, 0.0390625, 0.0, 0.009982638992369175, 0.0, 0.02365451492369175, 0.2723524272441864, 0.0, 0.0028211805038154125, 0.009548611007630825, 0.0234375, 0.1432291716337204, 0.0164930559694767, 0.5093315839767456, 0.4509548544883728, 0.02777777798473835, 0.01714409701526165, 0.02582465298473835, 0.0, 0.0896267369389534, 0.1295572966337204, 0.0831163227558136, 0.0, 0.0303819440305233, 0.00021701389050576836, 0.0431857630610466, 0.0, 0.03515625, 0.005642361007630825, 0.029296875, 0.01519097201526165, 0.0006510416860692203, 0.0, 0.314453125, 0.1163194477558136, 0.0, 0.0032552082557231188, 0.0, 0.0, 0.0614149309694767, 0.01714409701526165, 0.0203993059694767, 0.7799479365348816, 0.0, 0.0, 0.0390625, 0.014322916977107525, 0.0026041667442768812, 0.010416666977107525, 0.0407986119389534, 0.00933159701526165, 0.0425347238779068, 0.0006510416860692203, 0.0909288227558136, 0.0514322929084301, 0.0052083334885537624, 0.95703125, 0.0, 0.0783420130610466, 0.0015190972480922937, 0.014756944961845875, 0.0635850727558136, 0.013020833022892475, 0.0, 0.02994791604578495, 0.0049913194961845875, 0.0575086809694767, 0.015407986007630825, 0.0416666679084301, 0.0345052070915699, 0.1608072966337204, 0.004123263992369175, 0.01323784701526165, 0.014756944961845875, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.02278645895421505, 0.00021701389050576836, 0.01779513992369175, 0.00021701389050576836, 0.0798611119389534, 0.0377604179084301, 0.01584201492369175, 0.0655381977558136, 0.0707465261220932, 0.0, 0.0438368059694767, 0.0453559048473835, 0.0, 0.0, 0.069227434694767, 0.0, 0.0366753488779068, 0.0, 0.0479600690305233, 0.0572916679084301, 0.03081597201526165, 0.0, 0.0681423619389534, 0.0, 0.0013020833721384406, 0.0, 0.0327690988779068, 0.002170138992369175, 0.142361119389534, 0.0303819440305233, 0.0004340277810115367, 0.3572048544883728, 0.0023871527519077063, 0.009548611007630825, 0.013020833022892475, 0.0030381944961845875, 0.0301649309694767, 0.00021701389050576836, 0.0010850694961845875, 0.01323784701526165, 0.0373263880610466, 0.00021701389050576836, 0.0017361111240461469, 0.001953125, 0.0, 0.0, 0.0384114570915699, 0.0314670130610466, 0.801866352558136, 0.0212673619389534, 0.0, 0.0026041667442768812, 0.0, 0.060546875, 0.00390625, 0.0069444444961845875, 0.0301649309694767, 0.01584201492369175, 0.0049913194961845875, 0.02734375, 0.00021701389050576836, 0.0473090298473835, 0.01953125, 0.067274309694767, 0.02170138992369175, 0.0015190972480922937, 0.0596788190305233, 0.0540364570915699, 0.01909722201526165, 0.029296875, 0.0, 0.08203125, 0.01801215298473835, 0.0, 0.0, 0.0774739608168602, 0.02734375, 0.0212673619389534, 0.0902777761220932, 0.9995659589767456, 0.0086805559694767, 0.2601996660232544, 0.025390625, 0.044921875, 0.00021701389050576836, 0.0407986119389534, 0.0, 0.0, 0.0, 0.01019965298473835, 0.0004340277810115367, 0.0492621548473835, 0.0, 0.0, 0.0, 0.0496961809694767, 0.0575086809694767, 0.4231770932674408, 0.01410590298473835, 0.005425347480922937, 0.00021701389050576836, 0.0620659738779068, 0.01519097201526165, 0.1260850727558136, 0.0052083334885537624, 0.0, 0.0004340277810115367, 0.0340711809694767, 0.00021701389050576836, 0.0, 0.0390625, 0.0, 0.00021701389050576836, 0.02278645895421505, 0.04296875, 0.0, 0.999131977558136, 0.0, 0.01909722201526165, 0.021484375, 0.004123263992369175, 0.00434027798473835, 0.01692708395421505, 0.025390625, 0.0, 0.0375434048473835, 0.00390625, 0.0225694440305233, 0.004123263992369175, 0.02473958395421505, 0.0, 0.0282118059694767, 0.0601128488779068, 0.1078559011220932, 0.0, 0.9986979365348816, 0.0885416641831398, 0.0, 0.008897569961845875, 0.0381944440305233, 0.004123263992369175, 0.0, 0.02408854104578495, 0.02365451492369175, 0.0885416641831398, 0.0065104165114462376, 0.00021701389050576836, 0.0403645820915699, 0.005642361007630825, 0.08984375, 0.0, 0.014322916977107525, 0.02473958395421505, 0.0, 0.0627170130610466, 0.0, 0.02864583395421505, 0.01888020895421505, 0.462456613779068, 0.00021701389050576836, 0.0212673619389534, 0.0, 0.02300347201526165, 0.009114583022892475, 0.0, 0.114149309694767, 0.0, 0.0023871527519077063, 0.009765625, 0.0579427070915699, 0.0355902798473835, 0.0, 0.0, 0.8606770634651184, 0.00824652798473835, 0.9995659589767456, 0.1545138955116272, 0.119140625, 0.3009982705116272, 0.00021701389050576836, 0.0607638880610466, 0.0, 0.014756944961845875, 0.0, 0.0071614584885537624, 0.0358072929084301, 0.03125, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.0390625, 0.0347222238779068, 0.9986979365348816, 0.00021701389050576836, 0.4806857705116272, 0.0933159738779068, 0.0145399309694767, 0.0, 0.01714409701526165, 0.01909722201526165, 0.0, 0.0006510416860692203, 0.06640625, 0.0, 0.1202256977558136, 0.0453559048473835, 0.013020833022892475, 0.02278645895421505, 0.0438368059694767, 0.025390625, 0.0392795130610466, 0.0885416641831398, 0.0536024309694767, 0.5631510615348816, 0.0, 0.6041666865348816, 0.0, 0.00021701389050576836, 0.00021701389050576836, 0.02886284701526165, 0.0052083334885537624, 0.0193142369389534, 0.0282118059694767, 0.005642361007630825, 0.0, 0.0347222238779068, 0.009114583022892475, 0.0453559048473835, 0.0779079869389534, 0.015407986007630825, 0.12109375, 0.0006510416860692203, 0.01019965298473835, 0.1143663227558136, 0.0, 0.01605902798473835, 0.0993923619389534, 0.014756944961845875, 0.0, 0.0861545130610466, 0.00021701389050576836, 0.0418836809694767, 0.0026041667442768812, 0.013454861007630825, 0.005642361007630825, 0.0329861119389534, 0.0690104141831398, 0.03125, 0.03059895895421505, 0.0388454869389534, 0.0013020833721384406, 0.0772569477558136, 0.0525173619389534, 0.00021701389050576836, 0.02083333395421505, 0.0, 0.008463541977107525, 0.0, 0.0032552082557231188, 0.033203125, 0.0173611119389534, 0.00434027798473835, 0.00021701389050576836, 0.008029513992369175, 0.0538194440305233, 0.0, 0.0008680555620230734, 0.0, 0.02018229104578495, 0.0316840298473835, 0.1703559011220932, 0.0004340277810115367, 0.01953125, 0.0, 0.0451388880610466, 0.0577256940305233, 0.01909722201526165, 0.0321180559694767, 0.02278645895421505, 0.02582465298473835, 0.082899309694767]

 sparsity of   [0.0234375, 0.1328125, 0.783203125, 0.99609375, 0.033203125, 0.994140625, 0.123046875, 0.0078125, 0.0546875, 0.001953125, 0.115234375, 0.275390625, 0.064453125, 0.046875, 0.9609375, 0.8125, 0.05078125, 0.033203125, 0.044921875, 0.662109375, 0.029296875, 0.125, 0.876953125, 0.017578125, 0.171875, 0.095703125, 0.05859375, 0.0234375, 0.04296875, 0.19140625, 0.046875, 0.01171875, 0.04296875, 0.056640625, 0.01171875, 0.11328125, 0.04296875, 0.00390625, 0.35546875, 0.060546875, 0.048828125, 0.0859375, 0.001953125, 0.09375, 0.052734375, 0.583984375, 0.01953125, 0.791015625, 0.005859375, 0.0234375, 0.03125, 0.572265625, 0.091796875, 0.083984375, 0.017578125, 0.0234375, 0.20703125, 0.01171875, 0.0546875, 0.0546875, 0.05078125, 0.01171875, 0.03125, 0.025390625, 0.6875, 0.0234375, 0.447265625, 0.03125, 0.04296875, 0.060546875, 0.033203125, 0.09375, 0.046875, 0.017578125, 0.951171875, 0.03515625, 0.0703125, 0.015625, 0.009765625, 0.029296875, 0.033203125, 0.03125, 0.009765625, 0.0078125, 0.265625, 0.998046875, 0.29296875, 0.0390625, 0.05078125, 0.037109375, 0.685546875, 0.021484375, 0.01953125, 0.904296875, 0.998046875, 0.0546875, 0.46875, 0.005859375, 0.009765625, 0.208984375, 0.755859375, 0.00390625, 0.61328125, 0.01171875, 0.994140625, 0.064453125, 0.87890625, 0.046875, 0.095703125, 0.11328125, 0.1171875, 0.126953125, 0.0625, 0.0, 0.013671875, 0.04296875, 0.076171875, 0.12109375, 0.07421875, 0.1484375, 0.82421875, 0.044921875, 0.052734375, 0.994140625, 0.927734375, 0.8203125, 0.220703125, 0.970703125, 0.015625, 0.052734375, 0.048828125, 0.0234375, 0.015625, 0.021484375, 0.994140625, 0.34765625, 0.009765625, 0.076171875, 0.017578125, 0.021484375, 0.08203125, 0.671875, 0.03515625, 0.015625, 0.1015625, 0.876953125, 0.01171875, 0.033203125, 0.095703125, 0.0703125, 0.01953125, 0.001953125, 0.005859375, 0.244140625, 0.052734375, 0.83984375, 0.091796875, 0.1484375, 0.015625, 0.037109375, 0.259765625, 0.0859375, 0.02734375, 0.087890625, 0.025390625, 0.556640625, 0.060546875, 0.12109375, 0.49609375, 0.125, 0.017578125, 0.201171875, 0.14453125, 0.009765625, 0.048828125, 0.05078125, 0.015625, 0.013671875, 0.126953125, 0.046875, 0.060546875, 0.025390625, 0.048828125, 0.046875, 0.0703125, 0.0625, 0.025390625, 0.13671875, 0.03515625, 0.0078125, 0.787109375, 0.01171875, 0.03125, 0.05859375, 0.048828125, 0.05859375, 0.2265625, 0.0390625, 0.0234375, 0.04296875, 0.041015625, 0.03125, 0.01953125, 0.330078125, 0.0234375, 0.009765625, 0.576171875, 0.005859375, 0.083984375, 0.81640625, 0.05078125, 0.06640625, 0.033203125, 0.09765625, 0.013671875, 0.015625, 0.0234375, 0.599609375, 0.0546875, 0.1015625, 0.080078125, 0.015625, 0.07421875, 0.083984375, 0.009765625, 0.03125, 0.08984375, 0.109375, 0.048828125, 0.048828125, 0.13671875, 0.046875, 0.029296875, 0.01171875, 0.01171875, 0.1640625, 0.99609375, 0.044921875, 0.0390625, 0.03125, 0.212890625, 0.03125, 0.048828125, 0.056640625, 0.03125, 0.017578125, 0.060546875, 0.529296875, 0.271484375, 0.08984375, 0.001953125, 0.029296875, 0.037109375, 0.181640625, 0.025390625, 0.017578125, 0.00390625, 0.1015625, 0.580078125, 0.021484375, 0.0390625, 0.8515625, 0.02734375, 0.02734375, 0.08203125, 0.103515625, 0.015625, 0.060546875, 0.0078125, 0.083984375, 0.30859375, 0.10546875, 0.1015625, 0.06640625, 0.009765625, 0.0078125, 0.99609375, 0.05078125, 0.091796875, 0.7578125, 0.0546875, 0.048828125, 0.04296875, 0.001953125, 0.04296875, 0.31640625, 0.146484375, 0.001953125, 0.0703125, 0.3984375, 0.005859375, 0.041015625, 0.029296875, 0.01953125, 0.0546875, 0.015625, 0.00390625, 0.06640625, 0.03515625, 0.00390625, 0.015625, 0.1953125, 0.84375, 0.99609375, 0.03125, 0.025390625, 0.015625, 0.994140625, 0.998046875, 0.171875, 0.845703125, 0.55859375, 0.076171875, 0.0625, 0.078125, 0.0625, 0.0703125, 0.033203125, 0.052734375, 0.01171875, 0.1015625, 0.048828125, 0.029296875, 0.10546875, 0.0546875, 0.025390625, 0.017578125, 0.2578125, 0.87109375, 0.2265625, 0.001953125, 0.001953125, 0.966796875, 0.005859375, 0.083984375, 0.046875, 0.0546875, 0.017578125, 0.146484375, 0.068359375, 0.017578125, 0.04296875, 0.8828125, 0.072265625, 0.025390625, 0.021484375, 0.04296875, 0.845703125, 0.03515625, 0.0703125, 0.021484375, 0.05078125, 0.041015625, 0.00390625, 0.16796875, 0.048828125, 0.1015625, 0.22265625, 0.009765625, 0.0859375, 0.015625, 0.3125, 0.130859375, 0.947265625, 0.041015625, 0.99609375, 0.2265625, 0.341796875, 0.0234375, 0.0390625, 0.453125, 0.1171875, 0.01953125, 0.060546875, 0.99609375, 0.052734375, 0.056640625, 0.03515625, 0.234375, 0.021484375, 0.576171875, 0.427734375, 0.052734375, 0.068359375, 0.0234375, 0.44140625, 0.1640625, 0.1875, 0.005859375, 0.001953125, 0.029296875, 0.001953125, 0.771484375, 0.044921875, 0.6796875, 0.052734375, 0.0703125, 0.111328125, 0.064453125, 0.072265625, 0.044921875, 0.171875, 0.068359375, 0.017578125, 0.0078125, 0.06640625, 0.017578125, 0.03125, 0.08984375, 0.037109375, 0.001953125, 0.017578125, 0.193359375, 0.02734375, 0.14453125, 0.0390625, 0.6640625, 0.072265625, 0.03125, 0.017578125, 0.021484375, 0.033203125, 0.994140625, 0.82421875, 0.0859375, 0.193359375, 0.056640625, 0.083984375, 0.001953125, 0.009765625, 0.03515625, 0.021484375, 0.01171875, 0.236328125, 0.609375, 0.560546875, 0.08203125, 0.029296875, 0.015625, 0.02734375, 0.134765625, 0.044921875, 0.021484375, 0.029296875, 0.611328125, 0.037109375, 0.025390625, 0.140625, 0.154296875, 0.00390625, 0.947265625, 0.603515625, 0.18359375, 0.17578125, 0.033203125, 0.01953125, 0.025390625, 0.013671875, 0.01953125, 0.0234375, 0.0546875, 0.0625, 0.14453125, 0.05859375, 0.07421875, 0.10546875, 0.025390625, 0.994140625, 0.009765625, 0.013671875, 0.044921875, 0.0078125, 0.056640625, 0.0234375, 0.0390625, 0.8203125, 0.04296875, 0.05078125, 0.005859375, 0.107421875, 0.03515625, 0.13671875, 0.021484375, 0.021484375, 0.060546875, 0.017578125, 0.140625, 0.072265625, 0.099609375, 0.015625, 0.076171875, 0.99609375, 0.0703125, 0.189453125, 0.04296875, 0.02734375, 0.99609375, 0.099609375, 0.0859375, 0.19921875, 0.013671875, 0.044921875, 0.03125, 0.064453125, 0.0078125, 0.001953125, 0.0234375, 0.005859375, 0.017578125, 0.033203125, 0.02734375, 0.111328125, 0.03515625, 0.033203125, 0.814453125, 0.04296875, 0.12890625, 0.00390625, 0.013671875, 0.005859375, 0.009765625, 0.0234375, 0.34765625, 0.009765625, 0.123046875, 0.080078125, 0.12109375, 0.005859375, 0.12109375, 0.595703125, 0.041015625, 0.013671875, 0.005859375, 0.060546875, 0.005859375, 0.021484375, 0.1328125, 0.01171875, 0.19140625, 0.044921875, 0.349609375, 0.091796875, 0.91015625, 0.013671875, 0.05078125, 0.994140625, 0.001953125, 0.02734375, 0.05078125, 0.009765625, 0.130859375, 0.15625, 0.015625, 0.12890625, 0.0390625, 0.05078125, 0.1953125, 0.0703125, 0.009765625, 0.025390625, 0.26171875, 0.048828125, 0.046875, 0.013671875, 0.04296875, 0.029296875, 0.0078125, 0.103515625, 0.052734375, 0.013671875, 0.064453125, 0.021484375, 0.236328125, 0.005859375, 0.03515625, 0.0078125, 0.076171875, 0.009765625, 0.05859375, 0.02734375, 0.01171875, 0.017578125, 0.208984375, 0.11328125, 0.130859375, 0.01953125, 0.103515625, 0.259765625, 0.119140625, 0.046875, 0.013671875, 0.0078125, 0.0625, 0.044921875, 0.67578125, 0.2890625, 0.1484375, 0.009765625, 0.0234375, 0.0703125, 0.998046875, 0.0625, 0.005859375, 0.0546875, 0.017578125, 0.03125, 0.078125, 0.0234375, 0.994140625, 0.005859375, 0.029296875, 0.1640625, 0.041015625, 0.068359375, 0.697265625, 0.03125, 0.732421875, 0.015625, 0.068359375, 0.923828125, 0.029296875, 0.15234375, 0.146484375, 0.037109375, 0.052734375, 0.99609375, 0.095703125, 0.703125, 0.015625, 0.00390625, 0.166015625, 0.037109375, 0.99609375, 0.173828125, 0.06640625, 0.041015625, 0.119140625, 0.05859375, 0.107421875, 0.0625, 0.697265625, 0.15625, 0.796875, 0.3984375, 0.05078125, 0.009765625, 0.0078125, 0.056640625, 0.0390625, 0.017578125, 0.052734375, 0.904296875, 0.03125, 0.052734375, 0.029296875, 0.564453125, 0.091796875, 0.0390625, 0.994140625, 0.025390625, 0.048828125, 0.01953125, 0.265625, 0.55078125, 0.091796875, 0.021484375, 0.990234375, 0.9375, 0.005859375, 0.375, 0.107421875, 0.0625, 0.119140625, 0.0546875, 0.02734375, 0.908203125, 0.216796875, 0.048828125, 0.041015625, 0.0390625, 0.10546875, 0.017578125, 0.056640625, 0.32421875, 0.013671875, 0.73046875, 0.111328125, 0.994140625, 0.041015625, 0.005859375, 0.515625, 0.044921875, 0.0390625, 0.037109375, 0.0, 0.173828125, 0.6171875, 0.7890625, 0.193359375, 0.8125, 0.083984375, 0.05078125, 0.099609375, 0.994140625, 0.001953125, 0.087890625, 0.1015625, 0.79296875, 0.03515625, 0.01953125, 0.31640625, 0.0234375, 0.91796875, 0.060546875, 0.126953125, 0.013671875, 0.01953125, 0.525390625, 0.037109375, 0.626953125, 0.041015625, 0.029296875, 0.009765625, 0.025390625, 0.0078125, 0.732421875, 0.029296875, 0.919921875, 0.171875, 0.12890625, 0.005859375, 0.033203125, 0.10546875, 0.00390625, 0.03515625, 0.74609375, 0.01171875, 0.015625, 0.0859375, 0.087890625, 0.04296875, 0.224609375, 0.962890625, 0.072265625, 0.01171875, 0.6171875, 0.03515625, 0.025390625, 0.00390625, 0.150390625, 0.05078125, 0.021484375, 0.609375, 0.11328125, 0.029296875, 0.0546875, 0.025390625, 0.67578125, 0.01171875, 0.0546875, 0.0390625, 0.0546875, 0.69921875, 0.052734375, 0.537109375, 0.033203125, 0.072265625, 0.041015625, 0.994140625, 0.072265625, 0.064453125, 0.05859375, 0.025390625, 0.013671875, 0.05859375, 0.046875, 0.017578125, 0.025390625, 0.03125, 0.142578125, 0.224609375, 0.01953125, 0.240234375, 0.103515625, 0.041015625, 0.05078125, 0.080078125, 0.064453125, 0.013671875, 0.99609375, 0.025390625, 0.0390625, 0.013671875, 0.044921875, 0.041015625, 0.041015625, 0.044921875, 0.017578125, 0.88671875, 0.037109375, 0.07421875, 0.03125, 0.080078125, 0.0703125, 0.146484375, 0.01953125, 0.0078125, 0.162109375, 0.837890625, 0.03515625, 0.216796875, 0.013671875, 0.99609375, 0.015625, 0.03125, 0.01953125, 0.087890625, 0.037109375, 0.03125, 0.11328125, 0.01171875, 0.001953125, 0.0078125, 0.048828125, 0.80859375, 0.044921875, 0.125, 0.052734375, 0.943359375, 0.02734375, 0.095703125, 0.537109375, 0.220703125, 0.01171875, 0.791015625, 0.05078125, 0.0390625, 0.99609375, 0.05078125, 0.0, 0.171875, 0.05078125, 0.0078125, 0.8671875, 0.02734375, 0.03515625, 0.00390625, 0.0078125, 0.482421875, 0.0546875, 0.28125, 0.802734375, 0.828125, 0.060546875, 0.02734375, 0.994140625, 0.013671875, 0.068359375, 0.00390625, 0.109375, 0.140625, 0.0625, 0.072265625, 0.001953125, 0.048828125, 0.908203125, 0.037109375, 0.056640625, 0.0078125, 0.005859375, 0.0078125, 0.0078125, 0.6796875, 0.0390625, 0.15625, 0.99609375, 0.11328125, 0.08203125, 0.27734375, 0.064453125, 0.05078125, 0.033203125, 0.052734375, 0.06640625, 0.78515625, 0.041015625, 0.06640625, 0.013671875, 0.044921875, 0.041015625, 0.021484375, 0.109375, 0.01953125, 0.00390625, 0.1171875, 0.037109375, 0.015625, 0.0390625, 0.154296875, 0.0546875, 0.0078125, 0.01171875, 0.052734375, 0.033203125, 0.0234375, 0.7421875, 0.009765625, 0.029296875, 0.041015625, 0.9453125, 0.060546875, 0.021484375, 0.08203125, 0.068359375, 0.009765625, 0.052734375, 0.548828125, 0.8515625, 0.5625, 0.029296875, 0.009765625, 0.076171875, 0.046875, 0.00390625, 0.01171875, 0.0234375, 0.998046875, 0.93359375, 0.029296875, 0.01171875, 0.044921875, 0.064453125, 0.09375, 0.11328125, 0.00390625, 0.705078125, 0.0078125, 0.029296875, 0.076171875, 0.041015625, 0.087890625, 0.017578125, 0.0078125, 0.01171875, 0.154296875, 0.861328125, 0.09375, 0.01953125, 0.029296875, 0.01171875, 0.30078125, 0.03125, 0.07421875, 0.03515625, 0.017578125, 0.056640625, 0.00390625, 0.02734375, 0.994140625, 0.0078125, 0.052734375, 0.0234375, 0.005859375, 0.025390625, 0.03515625, 0.029296875, 0.03515625, 0.994140625, 0.015625, 0.052734375, 0.025390625, 0.025390625, 0.025390625, 0.04296875, 0.025390625, 0.62890625, 0.021484375, 0.068359375, 0.0234375, 0.03515625, 0.046875, 0.05859375, 0.021484375, 0.02734375, 0.01171875, 0.04296875, 0.017578125, 0.439453125, 0.0234375, 0.078125, 0.01171875, 0.06640625, 0.072265625, 0.0234375, 0.77734375, 0.078125, 0.041015625, 0.0, 0.52734375, 0.03125, 0.076171875, 0.01953125, 0.005859375, 0.001953125, 0.12109375, 0.08984375, 0.009765625, 0.119140625, 0.068359375, 0.080078125, 0.994140625, 0.857421875, 0.736328125, 0.037109375, 0.025390625, 0.044921875, 0.908203125, 0.068359375, 0.029296875, 0.091796875, 0.021484375, 0.021484375, 0.99609375, 0.037109375, 0.0703125, 0.09375, 0.0234375, 0.076171875, 0.0234375, 0.015625, 0.205078125, 0.07421875, 0.06640625, 0.015625, 0.037109375, 0.05078125, 0.060546875, 0.025390625, 0.046875, 0.037109375, 0.04296875, 0.052734375, 0.0546875, 0.7109375, 0.021484375, 0.06640625, 0.857421875, 0.169921875, 0.111328125, 0.09765625, 0.017578125, 0.62109375, 0.05078125, 0.994140625, 0.0546875, 0.052734375, 0.1328125, 0.060546875, 0.001953125, 0.22265625, 0.30859375, 0.99609375, 0.11328125, 0.876953125, 0.029296875, 0.009765625, 0.056640625, 0.001953125, 0.041015625, 0.146484375, 0.072265625, 0.36328125, 0.0234375, 0.068359375, 0.5078125, 0.037109375, 0.625, 0.1640625, 0.03515625, 0.025390625, 0.994140625, 0.015625, 0.0546875, 0.033203125, 0.06640625, 0.04296875, 0.046875, 0.03515625, 0.669921875, 0.1171875, 0.05078125, 0.015625, 0.017578125, 0.041015625, 0.1640625, 0.046875, 0.1015625, 0.013671875, 0.05859375, 0.0390625, 0.029296875, 0.015625, 0.8046875, 0.060546875, 0.046875, 0.068359375, 0.59765625, 0.04296875, 0.013671875, 0.03125, 0.0234375, 0.005859375, 0.04296875, 0.02734375, 0.033203125, 0.044921875, 0.021484375, 0.998046875, 0.068359375, 0.7109375, 0.0703125, 0.140625, 0.158203125, 0.048828125, 0.060546875, 0.24609375, 0.025390625, 0.05859375, 0.044921875, 0.73046875, 0.06640625, 0.919921875, 0.048828125, 0.158203125, 0.998046875, 0.041015625, 0.998046875, 0.015625, 0.943359375, 0.99609375, 0.046875, 0.041015625, 0.171875, 0.08203125, 0.005859375, 0.0234375, 0.015625, 0.12109375, 0.044921875, 0.064453125, 0.091796875, 0.033203125, 0.029296875, 0.015625, 0.060546875, 0.076171875, 0.08203125, 0.0390625, 0.021484375, 0.85546875, 0.390625, 0.033203125, 0.029296875, 0.03515625, 0.048828125, 0.708984375, 0.15234375, 0.8984375, 0.00390625, 0.005859375, 0.544921875, 0.01953125, 0.23046875, 0.99609375, 0.037109375, 0.044921875, 0.037109375, 0.013671875, 0.8203125, 0.06640625, 0.0234375, 0.181640625, 0.01171875, 0.044921875, 0.076171875, 0.71875, 0.02734375, 0.125, 0.46484375, 0.017578125, 0.015625, 0.001953125, 0.10546875, 0.029296875, 0.01171875, 0.19921875, 0.01953125, 0.03125, 0.046875, 0.0390625, 0.068359375, 0.0234375, 0.197265625, 0.009765625, 0.080078125, 0.041015625, 0.029296875, 0.060546875, 0.63671875, 0.033203125, 0.111328125, 0.14453125, 0.85546875, 0.150390625, 0.001953125, 0.091796875, 0.07421875, 0.185546875, 0.017578125, 0.998046875, 0.037109375, 0.001953125, 0.01953125, 0.994140625, 0.720703125, 0.01171875, 0.994140625, 0.029296875, 0.0, 0.017578125, 0.072265625, 0.01171875, 0.025390625, 0.04296875, 0.919921875, 0.568359375, 0.994140625, 0.013671875, 0.00390625, 0.017578125, 0.681640625, 0.173828125, 0.056640625, 0.10546875, 0.0234375, 0.0390625, 0.146484375, 0.013671875, 0.689453125, 0.029296875, 0.0078125, 0.05078125, 0.03125, 0.03125, 0.05078125, 0.041015625, 0.208984375, 0.03125, 0.0546875, 0.951171875, 0.005859375, 0.0625, 0.0078125, 0.23046875, 0.154296875, 0.048828125, 0.03515625, 0.046875, 0.08203125, 0.296875, 0.103515625, 0.21484375, 0.17578125, 0.00390625, 0.8828125, 0.73828125, 0.0703125, 0.033203125, 0.021484375, 0.068359375, 0.029296875, 0.1875, 0.068359375, 0.037109375, 0.71484375, 0.33984375, 0.0234375, 0.453125, 0.01171875, 0.994140625, 0.060546875, 0.021484375, 0.078125, 0.12890625, 0.01953125, 0.802734375, 0.0390625, 0.822265625, 0.064453125, 0.11328125, 0.044921875, 0.001953125, 0.1875, 0.068359375, 0.0546875, 0.587890625, 0.041015625, 0.1171875, 0.228515625, 0.0625, 0.876953125, 0.072265625, 0.029296875, 0.01953125, 0.0546875, 0.080078125, 0.072265625, 0.03125, 0.322265625, 0.013671875, 0.076171875, 0.171875, 0.048828125, 0.3203125, 0.318359375, 0.048828125, 0.015625, 0.056640625, 0.015625, 0.041015625, 0.994140625, 0.048828125, 0.017578125, 0.03125, 0.08203125, 0.009765625, 0.091796875, 0.046875, 0.58984375, 0.0546875, 0.423828125, 0.958984375, 0.173828125, 0.138671875, 0.998046875, 0.021484375, 0.013671875, 0.7421875, 0.908203125, 0.015625, 0.033203125, 0.998046875, 0.0234375, 0.00390625, 0.025390625, 0.005859375, 0.025390625, 0.013671875, 0.0234375, 0.025390625, 0.044921875, 0.16796875, 0.2734375, 0.095703125, 0.6484375, 0.99609375, 0.927734375, 0.025390625, 0.013671875, 0.041015625, 0.046875, 0.0859375, 0.0078125, 0.076171875, 0.083984375, 0.01953125, 0.05859375, 0.107421875, 0.01171875, 0.015625, 0.603515625, 0.123046875, 0.837890625, 0.00390625, 0.08203125, 0.021484375, 0.05859375, 0.0234375, 0.0, 0.041015625, 0.42578125, 0.810546875, 0.00390625, 0.013671875, 0.068359375, 0.056640625, 0.0859375, 0.041015625, 0.015625, 0.0078125, 0.5, 0.044921875, 0.98046875, 0.056640625, 0.037109375, 0.072265625, 0.078125, 0.00390625, 0.0390625, 0.056640625, 0.091796875, 0.775390625, 0.794921875, 0.236328125, 0.017578125, 0.802734375, 0.0859375, 0.025390625, 0.0390625, 0.0234375, 0.046875, 0.05078125, 0.017578125, 0.0390625, 0.015625, 0.05859375, 0.044921875, 0.025390625, 0.041015625, 0.359375, 0.794921875, 0.0546875, 0.04296875, 0.037109375, 0.90234375, 0.029296875, 0.04296875, 0.009765625, 0.37890625, 0.0546875, 0.01171875, 0.01171875, 0.013671875, 0.025390625, 0.037109375, 0.078125, 0.017578125, 0.09765625, 0.01953125, 0.025390625, 0.072265625, 0.013671875, 0.001953125, 0.03125, 0.072265625, 0.873046875, 0.533203125, 0.029296875, 0.0625, 0.1640625, 0.056640625, 0.0, 0.080078125, 0.19921875, 0.0625, 0.173828125, 0.998046875, 0.013671875, 0.0390625, 0.263671875, 0.05859375, 0.052734375, 0.20703125, 0.830078125, 0.591796875, 0.384765625, 0.923828125, 0.015625, 0.0234375, 0.26953125, 0.09765625, 0.27734375, 0.66015625, 0.03125, 0.013671875, 0.029296875, 0.015625, 0.07421875, 0.0078125, 0.61328125, 0.03125, 0.267578125, 0.0859375, 0.8046875, 0.0078125, 0.017578125, 0.048828125, 0.0078125, 0.025390625, 0.052734375, 0.298828125, 0.068359375, 0.060546875, 0.1171875, 0.09765625, 0.0546875, 0.162109375, 0.037109375, 0.07421875, 0.017578125, 0.041015625, 0.015625, 0.107421875, 0.03125, 0.123046875, 0.0078125, 0.046875, 0.16796875, 0.0390625, 0.015625, 0.025390625, 0.982421875, 0.044921875, 0.01171875, 0.0078125, 0.02734375, 0.11328125, 0.04296875, 0.08984375, 0.140625, 0.015625, 0.03515625, 0.994140625, 0.904296875, 0.0625, 0.068359375, 0.099609375, 0.009765625, 0.017578125, 0.177734375, 0.5625, 0.017578125, 0.009765625, 0.01171875, 0.009765625, 0.072265625, 0.009765625, 0.06640625, 0.056640625, 0.080078125, 0.650390625, 0.041015625, 0.001953125, 0.490234375, 0.021484375, 0.041015625, 0.017578125, 0.009765625, 0.09375, 0.94921875, 0.044921875, 0.013671875, 0.03125, 0.021484375, 0.005859375, 0.189453125, 0.15625, 0.048828125, 0.05859375, 0.8984375, 0.005859375, 0.197265625, 0.041015625, 0.01953125, 0.40234375, 0.07421875, 0.23046875, 0.03515625, 0.6484375, 0.01953125, 0.994140625, 0.029296875, 0.607421875, 0.068359375, 0.037109375, 0.076171875, 0.03125, 0.923828125, 0.19921875, 0.109375, 0.087890625, 0.060546875, 0.021484375, 0.841796875, 0.087890625, 0.021484375, 0.064453125, 0.017578125, 0.103515625, 0.564453125, 0.998046875, 0.04296875, 0.064453125, 0.091796875, 0.103515625, 0.884765625, 0.998046875, 0.013671875, 0.064453125, 0.04296875, 0.01171875, 0.01171875, 0.05859375, 0.025390625, 0.994140625, 0.125, 0.263671875, 0.021484375, 0.017578125, 0.005859375, 0.015625, 0.033203125, 0.064453125, 0.994140625, 0.056640625, 0.0390625, 0.00390625, 0.048828125, 0.033203125, 0.001953125, 0.505859375, 0.04296875, 0.01953125, 0.021484375, 0.15234375, 0.314453125, 0.068359375, 0.587890625, 0.04296875, 0.02734375, 0.095703125, 0.05078125, 0.013671875, 0.173828125, 0.046875, 0.021484375, 0.00390625, 0.0, 0.0625, 0.005859375, 0.009765625, 0.03515625, 0.453125, 0.77734375, 0.802734375, 0.53125, 0.205078125, 0.015625, 0.55078125, 0.033203125, 0.0390625, 0.037109375, 0.095703125, 0.2421875, 0.1171875, 0.05078125, 0.021484375, 0.013671875, 0.056640625, 0.115234375, 0.220703125, 0.09765625, 0.341796875, 0.041015625, 0.99609375, 0.033203125, 0.998046875, 0.892578125, 0.068359375, 0.001953125, 0.998046875, 0.14453125, 0.060546875, 0.990234375, 0.306640625, 0.056640625, 0.0390625, 0.3671875, 0.666015625, 0.052734375, 0.03515625, 0.021484375, 0.0390625, 0.18359375, 0.015625, 0.595703125, 0.013671875, 0.896484375, 0.681640625, 0.07421875, 0.0390625, 0.802734375, 0.0078125, 0.005859375, 0.07421875, 0.015625, 0.916015625, 0.03125, 0.029296875, 0.17578125, 0.01171875, 0.013671875, 0.041015625, 0.14453125, 0.02734375, 0.009765625, 0.037109375, 0.20703125, 0.083984375, 0.87890625, 0.02734375, 0.099609375, 0.029296875, 0.84375, 0.021484375, 0.955078125, 0.431640625, 0.041015625, 0.173828125, 0.0546875, 0.248046875, 0.005859375, 0.046875, 0.35546875, 0.037109375, 0.017578125, 0.04296875, 0.99609375, 0.701171875, 0.009765625, 0.560546875, 0.220703125, 0.193359375, 0.05078125, 0.16015625, 0.095703125, 0.71484375, 0.21484375, 0.935546875, 0.02734375, 0.041015625, 0.05078125, 0.1328125, 0.0390625, 0.0390625, 0.1171875, 0.0390625, 0.45703125, 0.01953125, 0.99609375, 0.021484375, 0.0, 0.994140625, 0.009765625, 0.046875, 0.99609375, 0.0, 0.435546875, 0.08203125, 0.0390625, 0.1015625, 0.185546875, 0.244140625, 0.09765625, 0.013671875, 0.03125, 0.005859375, 0.15234375, 0.041015625, 0.068359375, 0.041015625, 0.28125, 0.048828125, 0.03515625, 0.009765625, 0.005859375, 0.099609375, 0.01953125, 0.79296875, 0.83984375, 0.046875, 0.634765625, 0.189453125, 0.716796875, 0.5390625, 0.015625, 0.08984375, 0.107421875, 0.994140625, 0.013671875, 0.0234375, 0.681640625, 0.037109375, 0.513671875, 0.1015625, 0.791015625, 0.025390625, 0.056640625, 0.021484375, 0.142578125, 0.064453125, 0.029296875, 0.96484375, 0.08203125, 0.11328125, 0.0078125, 0.09765625, 0.64453125, 0.03515625, 0.025390625, 0.751953125, 0.048828125, 0.05078125, 0.0234375, 0.078125, 0.04296875, 0.00390625, 0.103515625, 0.111328125, 0.01953125, 0.025390625, 0.0078125, 0.01953125, 0.5703125, 0.037109375, 0.125, 0.04296875, 0.0546875, 0.0234375, 0.994140625, 0.021484375, 0.134765625, 0.03125, 0.837890625, 0.10546875, 0.017578125, 0.03515625, 0.05078125, 0.03125, 0.05078125, 0.021484375, 0.013671875, 0.7734375, 0.53125, 0.080078125, 0.03515625, 0.0234375, 0.181640625, 0.0078125, 0.99609375, 0.01953125, 0.673828125, 0.041015625, 0.076171875, 0.087890625, 0.017578125, 0.005859375, 0.0625, 0.5390625, 0.2109375, 0.044921875, 0.076171875, 0.623046875, 0.04296875, 0.009765625, 0.02734375, 0.048828125, 0.060546875, 0.998046875, 0.8984375, 0.05859375, 0.0078125, 0.08984375, 0.021484375, 0.037109375, 0.046875, 0.00390625, 0.005859375, 0.044921875, 0.013671875, 0.974609375, 0.052734375, 0.01171875, 0.052734375, 0.00390625, 0.060546875, 0.041015625, 0.05859375, 0.103515625, 0.0390625, 0.826171875, 0.037109375, 0.056640625, 0.0546875, 0.029296875, 0.560546875, 0.009765625, 0.017578125, 0.056640625, 0.005859375, 0.11328125, 0.998046875, 0.11328125, 0.15234375, 0.0390625, 0.16796875, 0.046875, 0.099609375, 0.09375, 0.0390625, 0.09765625, 0.017578125, 0.048828125, 0.068359375, 0.009765625, 0.01171875, 0.04296875, 0.13671875, 0.13671875, 0.013671875, 0.021484375, 0.09375, 0.015625, 0.083984375, 0.013671875, 0.142578125, 0.021484375, 0.056640625, 0.09765625, 0.103515625, 0.029296875, 0.03515625, 0.125, 0.01171875, 0.064453125, 0.013671875, 0.017578125, 0.046875, 0.033203125, 0.173828125, 0.033203125, 0.041015625, 0.001953125, 0.025390625, 0.86328125, 0.0078125, 0.07421875, 0.994140625, 0.083984375, 0.05859375, 0.01953125, 0.04296875, 0.0234375, 0.037109375, 0.998046875, 0.03125, 0.99609375, 0.005859375, 0.005859375, 0.880859375, 0.2265625, 0.794921875, 0.22265625, 0.08984375, 0.041015625, 0.07421875, 0.07421875, 0.07421875, 0.056640625, 0.0703125, 0.162109375, 0.0546875, 0.244140625, 0.107421875, 0.05078125, 0.005859375, 0.669921875, 0.005859375, 0.908203125, 0.01953125, 0.427734375, 0.0, 0.060546875, 0.1171875, 0.0, 0.490234375, 0.029296875, 0.130859375, 0.052734375, 0.658203125, 0.482421875, 0.984375, 0.01953125, 0.994140625, 0.0625, 0.0078125, 0.15625, 0.041015625, 0.033203125, 0.03125, 0.01953125, 0.044921875, 0.759765625, 0.521484375, 0.0390625, 0.02734375, 0.072265625, 0.00390625, 0.52734375, 0.19921875, 0.0390625, 0.01953125, 0.09765625, 0.37890625, 0.009765625, 0.021484375, 0.068359375, 0.103515625, 0.029296875, 0.994140625, 0.048828125, 0.01171875, 0.02734375, 0.07421875, 0.005859375, 0.01953125, 0.677734375, 0.07421875, 0.994140625, 0.00390625, 0.943359375, 0.71484375, 0.060546875, 0.84765625, 0.033203125, 0.041015625, 0.025390625, 0.1796875]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 6837253.022988811 (unstructured) 0 (structured)

max weight is  tensor([3.7483e-01, 6.5851e-03, 2.1914e-08, 3.3822e-01, 1.1773e-01, 6.7025e-02,
        3.7664e-09, 2.9026e-08, 5.7424e-09, 7.0710e-02, 4.2548e-01, 4.3086e-10,
        3.8372e-01, 4.1261e-01, 1.0723e-08, 2.9026e-08, 3.9303e-09, 3.0137e-01,
        4.0901e-09, 5.0309e-09, 8.8361e-01, 6.7240e-02, 2.4352e-01, 3.6833e-09,
        3.0257e-01, 1.0435e-02, 7.7109e-09, 2.2384e-08, 1.3681e-08, 2.9026e-08,
        4.6691e-09, 2.2771e-08, 5.6400e-01, 1.8722e-01, 3.2539e-01, 4.3564e-09,
        2.9026e-08, 1.0723e-08, 3.2653e-01, 5.0332e-02, 1.0723e-08, 7.2503e-09,
        1.4518e-01, 1.8466e-02, 7.6551e-02, 2.2771e-08, 1.0723e-08, 6.2351e-10,
        1.0723e-08, 3.1634e-01, 2.4986e-01, 2.1597e-01, 4.8924e-01, 3.6553e-09,
        5.2030e-02, 2.2771e-08, 9.8664e-09, 8.4469e-02, 9.1034e-01, 2.1425e-08,
        7.2503e-09, 2.3468e-02, 2.3440e-01, 5.3127e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.1147e-01, 2.1638e-08, 9.3824e-02, 8.9641e-02, 3.6702e-08, 5.3937e-08,
        2.2128e-08, 2.1637e-08, 7.7228e-02, 2.1637e-08, 1.9744e-08, 5.3937e-08,
        4.3288e-02, 7.8086e-08, 4.6151e-08, 4.6151e-08, 5.3937e-08, 1.0785e-07,
        2.7740e-08, 1.9744e-08, 3.7776e-02, 4.4968e-01, 2.7741e-08, 6.3763e-02,
        5.9820e-08, 3.6702e-08, 8.5339e-02, 2.9556e-01, 5.3937e-08, 4.6151e-08,
        1.9744e-08, 2.3073e-02, 1.0063e-01, 1.7523e-08, 4.6151e-08, 1.8926e-08,
        4.6151e-08, 9.2746e-08, 2.7645e-08, 4.6151e-08, 1.2751e-02, 6.2108e-08,
        5.9820e-08, 3.6702e-08, 3.6702e-08, 2.2128e-08, 1.3462e-01, 3.6702e-08,
        8.2705e-02, 2.1637e-08, 3.6702e-08, 4.4913e-08, 4.8135e-08, 3.0925e-08,
        2.1638e-08, 2.8112e-01, 7.1747e-02, 4.4913e-08, 2.1589e-08, 6.7352e-02,
        3.6702e-08, 4.4607e-08, 2.1637e-08, 1.3920e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3052e-06, 2.4320e-01, 6.3897e-07, 5.6583e-07, 2.3450e-07, 1.2677e-06,
        1.5883e-07, 4.3487e-02, 1.2828e-06, 2.3450e-07, 2.3450e-07, 4.7504e-08,
        3.1614e-07, 2.3450e-07, 3.5483e-01, 8.6451e-07, 8.5515e-03, 1.6793e-06,
        3.1402e-07, 2.1989e-01, 2.6277e-07, 3.8955e-07, 1.6793e-06, 2.3162e-07,
        2.3450e-07, 8.1377e-02, 1.8584e-07, 1.6492e-06, 3.1402e-07, 1.5883e-07,
        1.1132e-06, 4.1848e-02, 1.1690e-07, 5.9816e-07, 2.7943e-01, 5.6583e-07,
        5.9343e-07, 2.6277e-07, 3.1402e-07, 4.6937e-07, 3.1614e-07, 2.2762e-02,
        2.3450e-07, 2.4009e-02, 2.3450e-07, 2.3450e-07, 5.6583e-07, 2.3450e-07,
        3.3327e-01, 3.1402e-07, 6.7666e-07, 1.0739e-07, 8.5669e-07, 4.6834e-07,
        1.8078e-02, 1.3052e-06, 3.6585e-07, 4.3973e-07, 2.6277e-07, 4.6937e-07,
        6.5268e-07, 3.1827e-07, 2.3450e-07, 2.5269e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0253e-07, 7.3670e-08, 1.1433e-02, 5.6243e-02, 2.0416e-02, 6.5692e-02,
        5.2288e-02, 2.4576e-08, 5.9938e-08, 7.8620e-08, 6.6932e-02, 5.3196e-08,
        1.5696e-01, 7.0364e-03, 8.5085e-08, 5.9937e-08, 2.3929e-08, 3.7731e-08,
        3.6879e-08, 5.8358e-08, 5.4735e-08, 1.1399e-07, 2.3092e-08, 5.4735e-08,
        3.7731e-08, 5.1098e-02, 3.6623e-08, 3.6276e-02, 2.6356e-02, 7.6377e-08,
        9.1331e-08, 2.5857e-08, 1.8638e-07, 8.5540e-03, 7.3901e-08, 8.0031e-08,
        7.7546e-08, 4.2553e-02, 4.8273e-08, 1.2230e-01, 2.7733e-02, 9.8939e-08,
        5.1442e-02, 9.1396e-08, 9.0589e-08, 1.4335e-02, 1.7649e-02, 5.6864e-08,
        2.6319e-02, 2.1536e-08, 1.1399e-07, 1.1622e-02, 1.0582e-01, 8.2940e-08,
        5.4766e-08, 8.2599e-08, 9.0589e-08, 1.4046e-02, 1.7316e-02, 1.0126e-07,
        1.1230e-07, 1.8259e-01, 4.5291e-08, 3.0217e-08, 7.1292e-08, 1.7798e-01,
        9.9236e-08, 3.6026e-02, 5.6179e-08, 3.9822e-08, 8.2940e-08, 1.0152e-02,
        2.5857e-08, 4.4598e-08, 4.4525e-02, 1.5911e-01, 3.5653e-08, 3.9627e-08,
        1.4910e-02, 5.4044e-08, 7.8921e-06, 4.7034e-08, 7.8416e-08, 1.2871e-07,
        1.1356e-01, 8.7942e-08, 1.0695e-07, 9.1251e-03, 4.9559e-08, 1.9167e-02,
        2.2629e-08, 6.2952e-08, 6.0521e-08, 1.1931e-07, 2.3993e-02, 1.2101e-02,
        7.5070e-08, 1.9558e-02, 1.6813e-02, 8.2293e-02, 3.0885e-08, 8.2343e-08,
        6.3274e-08, 8.2940e-08, 1.8513e-02, 8.0031e-08, 2.1361e-02, 8.0031e-08,
        1.4968e-01, 1.9357e-01, 3.0663e-08, 5.3195e-08, 2.1352e-02, 1.0262e-07,
        7.9085e-03, 9.5461e-02, 1.8425e-01, 1.7741e-02, 3.5511e-02, 5.9938e-08,
        1.0253e-07, 3.5653e-08, 1.2602e-07, 1.9343e-02, 7.6377e-08, 6.2315e-03,
        1.7184e-02, 9.1927e-02, 5.2674e-08, 9.3470e-08, 9.6498e-02, 1.2729e-02,
        1.2806e-07, 9.6007e-02, 2.3929e-08, 6.7410e-08, 1.9810e-01, 9.2858e-08,
        5.7634e-08, 2.3929e-08, 8.1539e-03, 1.9854e-08, 8.7405e-08, 1.6424e-07,
        1.1145e-07, 2.0730e-02, 7.7220e-08, 1.6833e-02, 5.1686e-08, 6.4735e-08,
        4.7034e-08, 5.9938e-08, 8.0031e-08, 1.1578e-01, 6.1378e-08, 5.1001e-02,
        5.9954e-08, 7.6088e-08, 3.9153e-02, 1.6175e-08, 5.1699e-06, 8.2940e-08,
        1.2192e-07, 7.9436e-02, 1.4053e-07, 4.8378e-03, 9.3131e-08, 1.8576e-02,
        3.3678e-08, 1.5956e-02, 1.5065e-01, 1.2668e-02, 8.4657e-08, 6.3528e-02,
        1.8053e-02, 6.4147e-08, 9.9347e-08, 5.1686e-08, 1.9527e-02, 9.1263e-03,
        6.4811e-08, 8.7942e-08, 1.7620e-02, 1.3531e-02, 6.5878e-02, 3.0885e-08,
        1.0461e-02, 1.8543e-02, 7.2919e-03, 7.9421e-08, 1.1972e-02, 1.0996e-01,
        1.0145e-07, 1.0362e-07, 3.0204e-02, 5.1845e-08, 7.7716e-08, 8.8143e-08,
        1.1333e-07, 1.2742e-07, 1.0703e-07, 1.8678e-02, 1.1083e-07, 7.5890e-08,
        8.5226e-08, 9.3533e-03, 8.6725e-08, 7.3503e-03, 3.6294e-08, 1.3651e-07,
        8.4657e-08, 9.4665e-02, 5.3147e-08, 3.5280e-02, 3.8270e-08, 3.7731e-08,
        3.6294e-08, 6.0597e-02, 4.2264e-02, 3.6260e-08, 8.7942e-08, 8.0031e-08,
        3.5653e-08, 1.0742e-07, 4.1690e-02, 1.4800e-01, 1.3996e-02, 5.8673e-03,
        5.9938e-08, 5.1133e-08, 6.0056e-08, 1.0778e-02, 3.6137e-08, 1.6690e-07,
        2.9838e-02, 1.0600e-07, 8.2940e-08, 2.3929e-08, 2.0327e-01, 1.1426e-02,
        7.6780e-08, 5.0534e-02, 9.6055e-08, 7.3764e-03, 9.4461e-08, 1.1399e-07,
        3.3229e-08, 1.9978e-01, 2.2479e-02, 6.8465e-02, 5.2099e-08, 5.3147e-08,
        1.0276e-07, 1.1143e-07, 7.3432e-03, 1.1890e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.2949e-08, 4.6837e-02, 1.6170e-01, 1.3411e-01, 7.4163e-02, 1.0893e-01,
        1.1716e-01, 3.3274e-08, 3.3274e-08, 3.0521e-02, 8.0711e-02, 1.1627e-01,
        4.0847e-02, 1.6281e-01, 1.1533e-01, 1.9927e-08, 3.3274e-08, 2.3815e-08,
        2.3815e-08, 4.0230e-02, 1.9927e-08, 1.8971e-08, 3.3492e-08, 2.0792e-08,
        3.1234e-08, 2.5512e-01, 3.7093e-08, 5.6003e-02, 8.1949e-02, 2.3815e-08,
        3.3274e-08, 3.1234e-08, 1.3209e-02, 7.4723e-02, 1.1501e-01, 4.5987e-08,
        4.4037e-02, 6.3054e-02, 3.3274e-08, 3.3374e-02, 3.0711e-02, 7.6642e-02,
        5.2744e-02, 8.8214e-09, 1.9927e-08, 1.6468e-01, 1.4798e-01, 6.8962e-03,
        9.9916e-02, 2.0159e-08, 3.3274e-08, 3.6214e-02, 3.2556e-02, 2.0384e-08,
        1.2949e-08, 8.8014e-02, 3.7093e-08, 1.1059e-01, 2.0301e-01, 1.9642e-08,
        1.0152e-01, 1.2897e-01, 4.5258e-02, 3.7204e-08, 7.0959e-02, 3.2516e-02,
        2.0792e-08, 1.2773e-01, 8.9315e-02, 4.3609e-02, 2.2667e-08, 9.9767e-02,
        1.9927e-08, 3.4827e-02, 9.6395e-02, 3.9400e-02, 2.0384e-08, 7.2642e-02,
        1.3530e-01, 6.9205e-02, 8.6506e-02, 1.9927e-08, 4.1396e-02, 7.3382e-02,
        1.0202e-01, 2.0384e-08, 1.9642e-08, 1.5758e-01, 1.3981e-08, 2.0147e-01,
        1.2949e-08, 3.3274e-08, 6.4661e-02, 7.1472e-02, 1.1343e-01, 1.0865e-01,
        7.5158e-02, 1.7547e-01, 1.1130e-01, 1.4202e-01, 1.2949e-08, 1.3656e-01,
        3.7093e-08, 3.3274e-08, 1.4910e-01, 1.2021e-08, 2.8458e-02, 1.1387e-08,
        9.9799e-02, 2.4604e-02, 8.7861e-02, 6.1666e-02, 2.2820e-01, 7.3945e-02,
        1.2642e-01, 1.2724e-01, 3.8581e-02, 1.1178e-01, 3.8730e-02, 3.3274e-08,
        1.9642e-08, 3.3274e-08, 2.8583e-02, 1.6085e-01, 1.3143e-08, 1.1292e-01,
        1.5656e-01, 9.4711e-02, 7.9302e-02, 8.7724e-02, 6.3478e-02, 3.8014e-02,
        3.4784e-02, 1.1589e-01, 2.0034e-08, 2.0923e-02, 3.5024e-02, 8.1736e-02,
        3.1200e-02, 3.3274e-08, 1.1410e-01, 1.9642e-08, 6.6837e-02, 3.3274e-08,
        8.3597e-02, 1.9880e-01, 1.2065e-01, 2.6130e-02, 1.9927e-08, 4.3312e-02,
        3.7093e-08, 1.2949e-08, 5.0360e-08, 2.0074e-02, 4.5495e-02, 8.2966e-02,
        2.3815e-08, 2.3815e-08, 7.7456e-02, 2.4761e-02, 1.0667e-02, 1.2949e-08,
        1.1233e-01, 8.1367e-02, 8.7207e-02, 9.2587e-02, 4.4149e-03, 1.9844e-02,
        5.4156e-02, 1.9953e-01, 2.9041e-02, 2.4979e-01, 4.9366e-09, 9.7070e-02,
        1.6938e-01, 6.9830e-02, 1.4154e-01, 3.3274e-08, 2.3682e-01, 9.4204e-02,
        7.6780e-02, 1.2949e-08, 6.4259e-02, 1.6421e-01, 3.6541e-02, 3.3274e-08,
        3.2225e-02, 1.9806e-02, 1.5667e-01, 4.8443e-02, 2.0834e-01, 2.5672e-02,
        5.8112e-02, 4.6364e-02, 1.4124e-01, 4.3177e-02, 7.1219e-08, 4.8471e-02,
        1.1910e-01, 7.0240e-02, 9.5406e-02, 1.3720e-01, 1.5988e-01, 9.3426e-03,
        5.9353e-02, 1.2072e-02, 9.8072e-02, 1.3800e-01, 3.0254e-10, 3.2346e-02,
        1.9927e-08, 1.2232e-01, 1.9642e-08, 1.3600e-01, 3.8334e-02, 2.3815e-08,
        2.7464e-08, 1.2659e-01, 1.1113e-01, 1.2949e-08, 3.3274e-08, 1.9927e-08,
        1.9642e-08, 3.3970e-02, 2.8620e-02, 1.1660e-01, 1.4145e-01, 1.1007e-01,
        3.3274e-08, 5.4443e-02, 9.1772e-02, 1.6964e-01, 1.5452e-08, 2.3815e-08,
        5.0341e-02, 1.9642e-08, 1.0360e-08, 3.1804e-08, 3.3860e-02, 1.2790e-01,
        2.9569e-02, 7.5611e-02, 6.3951e-02, 1.5126e-01, 9.4403e-02, 3.3274e-08,
        1.0519e-01, 1.0490e-01, 2.6323e-02, 1.2635e-01, 6.3770e-02, 3.7363e-08,
        6.8243e-02, 2.3815e-08, 8.2455e-02, 2.3815e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0907e-01, 3.4122e-07, 1.1430e-07, 4.2463e-02, 1.8094e-07, 5.1427e-02,
        5.2687e-02, 6.5965e-02, 9.4515e-02, 6.7455e-08, 5.8243e-07, 1.9793e-07,
        3.4122e-07, 5.5404e-02, 5.0169e-07, 4.7323e-02, 2.8298e-02, 7.7736e-02,
        1.0629e-07, 1.5227e-07, 3.9953e-02, 1.0629e-07, 3.2359e-07, 1.3174e-07,
        1.2757e-07, 1.1430e-07, 3.4122e-07, 6.8719e-08, 3.2359e-07, 2.6329e-07,
        2.3366e-07, 4.0438e-08, 1.3174e-07, 3.4122e-07, 1.3174e-07, 1.5920e-07,
        1.9793e-07, 1.2757e-07, 5.0169e-07, 1.4188e-07, 6.8106e-07, 3.5804e-07,
        9.4516e-08, 9.4516e-08, 1.3174e-07, 3.1332e-07, 3.6609e-08, 4.8665e-02,
        9.4516e-08, 8.2952e-02, 3.3024e-07, 2.3366e-07, 2.9256e-07, 3.2359e-07,
        1.1430e-07, 1.0671e-07, 1.3174e-07, 4.4456e-07, 1.4775e-07, 3.4122e-07,
        1.1430e-07, 1.0671e-07, 2.3366e-07, 2.3366e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.1333e-07, 2.2963e-07, 5.9232e-07, 7.6758e-02, 8.8241e-02, 2.4697e-07,
        4.6683e-07, 4.6683e-07, 2.2964e-07, 5.9232e-07, 7.6037e-07, 5.4000e-07,
        4.7259e-07, 5.8918e-07, 7.0766e-07, 1.7457e-07, 7.6037e-07, 1.8115e-07,
        3.1237e-07, 1.7457e-07, 1.2481e-07, 1.7457e-07, 1.7457e-07, 3.2659e-07,
        6.1815e-02, 5.9232e-07, 1.8852e-07, 2.8559e-07, 2.6444e-01, 3.0153e-01,
        4.7689e-07, 1.0385e-01, 4.1895e-02, 5.5039e-07, 1.2278e-06, 4.0993e-07,
        4.5155e-07, 7.6037e-07, 1.3868e-07, 2.1266e-02, 5.9232e-07, 5.2824e-02,
        4.6683e-07, 8.8771e-02, 5.3721e-02, 3.1995e-02, 5.2801e-02, 9.5472e-07,
        3.1237e-07, 4.7348e-07, 5.4000e-07, 1.7457e-07, 2.8559e-07, 3.6943e-02,
        3.9102e-02, 1.7457e-07, 5.5193e-02, 5.4000e-07, 2.2964e-07, 1.0406e-06,
        1.0573e-01, 5.8918e-07, 2.1333e-07, 3.8211e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([9.5580e-08, 5.5020e-08, 5.4956e-02, 1.7895e-02, 9.6188e-08, 2.5204e-02,
        6.8594e-03, 1.1485e-01, 2.7406e-08, 2.0104e-02, 2.9127e-03, 7.7445e-02,
        1.9952e-02, 1.2526e-07, 3.2241e-02, 8.3353e-02, 4.1199e-08, 2.9512e-02,
        3.0081e-08, 2.6487e-08, 1.6942e-01, 2.7405e-08, 1.0492e-07, 4.9197e-08,
        3.0081e-08, 2.6344e-01, 6.9322e-08, 2.2792e-02, 9.1002e-02, 3.9710e-02,
        2.6528e-02, 6.9322e-08, 7.2272e-08, 8.9304e-08, 3.3793e-08, 8.0701e-08,
        3.1591e-08, 1.6429e-02, 4.4186e-02, 3.0991e-02, 5.1956e-08, 5.3498e-08,
        9.2943e-08, 8.3185e-02, 5.6595e-02, 7.1598e-02, 1.0570e-01, 8.1376e-08,
        1.5449e-02, 5.3177e-02, 2.0957e-02, 9.2015e-03, 1.1372e-07, 2.7444e-08,
        4.0451e-02, 6.2094e-08, 2.1712e-08, 9.6749e-03, 1.6591e-02, 3.6581e-08,
        1.8925e-08, 5.6050e-08, 5.6914e-08, 4.5916e-08, 3.0513e-08, 9.7714e-08,
        6.7646e-02, 1.3283e-07, 1.4193e-08, 9.4093e-08, 7.6120e-02, 1.6566e-01,
        4.1623e-08, 3.6793e-08, 1.5481e-02, 1.6016e-02, 6.9867e-08, 1.4288e-07,
        1.1391e-02, 7.2608e-08, 1.5265e-02, 2.2842e-08, 7.9299e-08, 6.3621e-02,
        4.7082e-02, 8.0701e-08, 8.2281e-02, 3.1659e-02, 8.4151e-02, 1.9302e-02,
        3.3643e-08, 4.2425e-02, 4.9967e-08, 8.4678e-08, 4.0897e-02, 9.1339e-03,
        1.4081e-08, 6.1733e-02, 9.6371e-08, 5.7554e-03, 2.0112e-02, 5.0581e-08,
        3.6791e-02, 2.0727e-02, 6.9639e-02, 5.9713e-08, 7.0666e-08, 4.6426e-08,
        1.6604e-02, 2.3868e-02, 6.4071e-08, 9.4546e-08, 2.1952e-02, 5.9128e-02,
        7.7474e-02, 1.1593e-02, 1.6498e-02, 2.4886e-02, 5.8038e-08, 8.3265e-02,
        3.8738e-08, 2.7405e-08, 1.1119e-07, 6.4219e-02, 2.1287e-02, 2.2389e-02,
        6.2368e-02, 6.7991e-03, 8.1596e-08, 7.1731e-08, 1.4773e-02, 1.2077e-07,
        6.6684e-08, 9.7426e-08, 9.6160e-08, 6.8796e-08, 1.2650e-02, 1.5955e-02,
        8.8419e-08, 4.4257e-08, 8.9610e-02, 7.0275e-02, 2.9398e-02, 1.6893e-07,
        2.9207e-08, 9.9627e-02, 2.1504e-02, 2.0726e-02, 8.2160e-02, 2.7416e-08,
        1.0143e-01, 2.5194e-02, 6.9322e-08, 1.1982e-02, 1.8455e-08, 7.5781e-08,
        7.2059e-08, 7.2284e-08, 1.3643e-02, 1.9158e-02, 6.9546e-08, 1.5113e-07,
        1.1858e-07, 4.6891e-03, 5.2837e-02, 6.5748e-02, 8.4723e-08, 3.2944e-08,
        4.9391e-02, 1.8418e-01, 1.3147e-02, 2.1487e-02, 7.5113e-08, 7.8092e-08,
        6.8927e-02, 4.4917e-03, 1.1606e-07, 6.3453e-02, 3.1109e-02, 6.9134e-02,
        3.5531e-08, 2.1712e-08, 1.1613e-02, 1.1015e-02, 4.0418e-08, 3.9934e-02,
        4.3900e-02, 5.4159e-08, 4.5211e-08, 3.8296e-08, 7.0934e-02, 1.1896e-02,
        6.6780e-08, 2.6805e-08, 4.7123e-02, 2.9962e-08, 1.9284e-08, 2.6497e-02,
        8.0685e-08, 3.6632e-08, 8.3544e-02, 1.4361e-01, 6.6960e-08, 5.5299e-08,
        6.9103e-02, 3.0605e-06, 5.2554e-08, 4.3455e-08, 1.6893e-07, 3.9560e-08,
        1.2760e-01, 1.3328e-02, 6.9322e-08, 1.2241e-02, 9.6565e-08, 6.6834e-08,
        8.0701e-08, 3.3022e-02, 3.4422e-08, 1.0424e-07, 1.6893e-07, 1.0857e-01,
        9.1837e-08, 3.5039e-08, 3.6563e-08, 3.0689e-03, 4.9409e-02, 1.2816e-07,
        4.3793e-02, 6.9388e-08, 1.3003e-07, 7.2453e-02, 8.2252e-08, 2.1712e-08,
        1.4246e-07, 3.2989e-08, 8.0701e-08, 1.0890e-02, 1.6274e-02, 4.6357e-02,
        3.3817e-08, 8.6572e-08, 1.4161e-07, 1.3013e-02, 5.1422e-08, 7.6246e-02,
        9.1122e-03, 1.4700e-02, 3.0286e-08, 4.3073e-03, 3.0792e-08, 9.1837e-08,
        2.7294e-08, 6.9867e-08, 6.3411e-02, 7.2687e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3895e-02, 5.1741e-07, 1.5171e-07, 1.1452e-01, 2.2215e-01, 7.3739e-07,
        1.3269e-01, 4.6110e-07, 1.5155e-01, 2.9082e-07, 1.9718e-07, 4.8226e-02,
        5.0140e-02, 6.8690e-07, 3.1239e-07, 7.7574e-03, 1.5055e-01, 4.9072e-02,
        7.7220e-08, 6.6757e-03, 7.2877e-07, 5.3133e-08, 5.3133e-08, 1.2471e-02,
        1.2697e-07, 1.3579e-01, 5.8931e-02, 6.4865e-03, 1.9335e-01, 2.1575e-07,
        1.5586e-07, 3.5917e-07, 2.5306e-03, 4.0505e-07, 5.3133e-08, 5.2768e-07,
        8.8348e-03, 6.8763e-03, 5.4276e-02, 4.8061e-02, 1.1304e-01, 1.9415e-01,
        1.1142e-01, 2.8159e-02, 4.0157e-02, 6.4589e-07, 2.8421e-07, 5.3133e-08,
        3.3616e-07, 1.0585e-01, 2.5222e-02, 1.8012e-02, 4.3693e-07, 2.2103e-07,
        4.2052e-07, 2.4790e-07, 4.3693e-07, 5.5744e-07, 1.1383e-02, 2.8421e-07,
        1.5631e-01, 3.5917e-07, 1.3219e-02, 6.7829e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.3245e-07, 1.1867e-01, 5.1362e-07, 2.1330e-02, 8.9046e-07, 1.0872e-02,
        1.8024e-01, 1.3159e-06, 1.4114e-06, 1.0956e-01, 2.0274e-06, 8.7593e-07,
        9.4383e-03, 5.4847e-07, 1.3099e-01, 7.7504e-07, 3.8146e-07, 1.7425e-06,
        1.0590e-06, 1.8177e-06, 6.4072e-07, 1.2407e-01, 1.5487e-01, 5.7516e-07,
        4.7234e-07, 1.2007e-02, 3.6353e-02, 7.1990e-07, 5.1041e-07, 9.4020e-07,
        9.4100e-02, 1.3855e-01, 1.5286e-07, 5.7377e-07, 9.6891e-02, 8.0439e-07,
        9.8660e-02, 1.4114e-06, 1.1297e-01, 7.4834e-07, 8.5505e-07, 8.5505e-07,
        3.2574e-02, 5.1041e-07, 1.3020e-01, 1.1405e-01, 6.3960e-07, 4.0970e-02,
        1.1273e-02, 1.4161e-02, 3.3998e-02, 9.1369e-02, 2.0274e-06, 6.6265e-07,
        1.3108e-06, 1.2362e-01, 9.9892e-02, 1.6169e-06, 9.4911e-02, 5.0099e-02,
        1.2238e-01, 8.0439e-07, 8.3687e-07, 4.7009e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.0693e-08, 2.6626e-02, 8.9283e-03, 1.1470e-02, 5.4283e-08, 2.7841e-02,
        1.1185e-02, 1.5468e-07, 3.9376e-08, 1.2389e-07, 2.1919e-02, 5.9271e-03,
        1.5588e-02, 1.1996e-02, 3.2710e-02, 5.0748e-02, 2.3085e-07, 5.3090e-02,
        5.2958e-08, 4.7950e-08, 7.2539e-03, 7.1647e-03, 1.0066e-02, 6.4625e-02,
        3.3433e-02, 6.7711e-02, 2.8069e-02, 1.3294e-02, 2.5445e-02, 2.8926e-01,
        5.0942e-02, 2.0363e-02, 1.7568e-08, 6.3316e-08, 4.4697e-03, 6.8980e-02,
        3.7623e-08, 7.0562e-02, 6.9513e-02, 3.7131e-02, 1.9969e-02, 4.8371e-02,
        6.3560e-08, 6.8889e-03, 1.2942e-01, 7.4435e-02, 7.8816e-02, 1.2640e-07,
        4.2542e-08, 1.3730e-01, 2.3775e-02, 9.5827e-08, 4.2125e-02, 4.1290e-08,
        7.7214e-02, 2.3458e-01, 1.0536e-01, 1.1120e-02, 1.3666e-02, 2.5491e-02,
        4.6934e-03, 1.0691e-02, 2.0048e-07, 6.9771e-02, 5.3542e-08, 1.6242e-03,
        5.2066e-02, 6.5797e-03, 9.0795e-03, 5.2361e-08, 1.2900e-01, 3.2459e-02,
        4.8565e-02, 1.4244e-02, 6.6225e-03, 1.2506e-01, 2.6702e-02, 6.0191e-08,
        9.7032e-03, 7.3500e-08, 1.2543e-07, 1.2276e-01, 3.4008e-02, 6.1102e-08,
        8.8958e-03, 4.6528e-02, 9.7615e-03, 4.1900e-02, 1.1198e-01, 1.3600e-02,
        3.5070e-02, 4.6893e-02, 2.3135e-07, 4.0917e-02, 8.4900e-02, 1.2244e-02,
        6.0644e-08, 1.2589e-02, 2.6070e-02, 6.8495e-03, 2.3981e-02, 8.2519e-03,
        9.1895e-08, 6.1699e-02, 1.1526e-02, 2.0316e-02, 4.2754e-03, 5.5576e-02,
        9.7489e-03, 2.6068e-02, 7.1048e-03, 4.1030e-02, 2.1407e-02, 5.3670e-02,
        4.8612e-02, 8.6609e-02, 1.3393e-02, 1.3764e-07, 1.2190e-07, 6.6978e-03,
        4.9877e-08, 1.7855e-02, 9.7944e-08, 2.6557e-02, 7.1970e-02, 2.4075e-03,
        3.7034e-02, 1.0631e-02, 8.2440e-08, 2.5022e-03, 6.6945e-02, 5.4798e-08,
        4.0528e-08, 9.7703e-03, 1.5807e-01, 1.0678e-07, 6.8414e-03, 7.4797e-08,
        2.8719e-08, 1.2418e-02, 3.6123e-03, 1.2914e-01, 6.8949e-08, 7.5113e-02,
        2.2858e-08, 7.5341e-02, 1.1053e-07, 2.4487e-02, 7.3211e-03, 9.7046e-08,
        6.0744e-03, 3.7568e-02, 1.1668e-07, 8.6341e-03, 8.1338e-08, 4.9722e-03,
        5.4606e-02, 2.2483e-02, 3.7978e-08, 8.9357e-08, 6.4514e-08, 1.0112e-07,
        9.5466e-08, 7.8288e-03, 2.3159e-02, 7.4593e-02, 3.2715e-02, 2.2373e-03,
        1.3391e-02, 1.1737e-01, 1.3819e-02, 1.2736e-01, 5.4570e-02, 7.8890e-03,
        1.3782e-02, 9.7016e-08, 4.3677e-02, 4.2322e-08, 1.5385e-02, 1.1948e-01,
        1.5303e-07, 7.1955e-02, 3.8780e-08, 1.1334e-02, 8.7629e-08, 4.7319e-08,
        1.4224e-02, 4.8451e-03, 5.9729e-03, 1.2674e-07, 1.2830e-01, 9.7767e-03,
        2.2041e-02, 7.3393e-08, 4.2940e-02, 1.9898e-07, 6.3097e-02, 8.9139e-08,
        5.0886e-02, 6.0959e-08, 1.0094e-07, 8.2471e-02, 1.2861e-02, 1.3364e-07,
        1.1141e-07, 8.6775e-08, 3.3802e-08, 1.0884e-02, 3.7967e-08, 4.0933e-08,
        5.1993e-02, 9.7235e-03, 1.6382e-01, 1.0228e-02, 1.1601e-07, 8.6659e-03,
        7.8340e-03, 9.8275e-03, 8.1111e-03, 5.5438e-02, 4.9718e-08, 5.0612e-02,
        6.4192e-02, 4.0738e-08, 9.7499e-08, 7.4165e-03, 1.3520e-01, 6.9488e-03,
        1.2127e-01, 1.4382e-07, 1.2579e-07, 5.3125e-02, 3.6223e-08, 1.5328e-01,
        2.1380e-02, 1.4592e-01, 3.6390e-03, 4.2429e-08, 8.2191e-03, 1.0428e-02,
        6.1610e-08, 5.9490e-03, 2.2355e-03, 1.1723e-02, 3.4246e-02, 5.5392e-03,
        9.5990e-03, 1.4991e-02, 5.8636e-08, 6.1836e-03, 2.1601e-08, 7.2813e-02,
        8.8021e-02, 8.0678e-02, 3.3182e-03, 7.2733e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.2531e-02, 3.3774e-02, 1.2528e-02, 2.0643e-07, 5.9037e-07, 5.3470e-07,
        1.5518e-02, 2.2986e-02, 5.5320e-02, 2.6407e-07, 4.9703e-02, 1.6499e-07,
        2.7244e-07, 5.7689e-02, 2.6334e-02, 4.7647e-07, 1.6122e-07, 4.4835e-07,
        3.6404e-07, 2.7244e-07, 4.0346e-02, 6.2940e-08, 1.5863e-07, 9.8867e-08,
        7.5705e-07, 1.5163e-01, 8.1704e-08, 9.9270e-08, 2.0643e-07, 2.1790e-02,
        4.8100e-02, 2.6105e-07, 1.6122e-07, 6.4279e-07, 3.5945e-02, 2.4958e-07,
        6.5778e-02, 5.9037e-07, 3.8211e-07, 3.0327e-02, 5.7133e-02, 1.3767e-07,
        3.1222e-07, 5.2610e-02, 3.3517e-07, 3.2555e-02, 2.9427e-07, 5.3864e-02,
        2.4506e-07, 7.5537e-07, 1.1097e-07, 2.9326e-07, 7.2086e-02, 5.1989e-07,
        6.4279e-07, 7.3458e-02, 4.7647e-07, 4.3101e-07, 8.3196e-08, 7.5537e-07,
        3.5636e-02, 5.6137e-02, 6.5483e-02, 2.4168e-07, 6.2940e-08, 1.5948e-01,
        3.7839e-07, 5.1989e-07, 3.6226e-07, 7.5278e-07, 3.5754e-02, 1.1956e-01,
        3.6475e-02, 3.1222e-07, 9.8867e-08, 1.2725e-01, 2.8394e-02, 5.3766e-02,
        1.6322e-02, 5.1989e-07, 3.1222e-07, 8.4022e-08, 7.5914e-07, 6.4279e-07,
        4.7647e-07, 6.4279e-07, 1.5863e-07, 1.6122e-07, 9.1645e-06, 4.2382e-02,
        6.4279e-07, 5.7564e-02, 7.0797e-02, 6.4279e-07, 5.3614e-02, 1.6122e-07,
        7.5315e-07, 2.3859e-07, 5.7195e-07, 3.0360e-07, 1.5254e-07, 2.0661e-05,
        9.8867e-08, 1.5863e-07, 5.0059e-02, 4.7019e-02, 3.4074e-06, 4.2340e-07,
        1.8040e-07, 4.0953e-02, 2.4677e-02, 1.6181e-07, 2.6983e-07, 1.2933e-01,
        8.3196e-08, 9.8759e-02, 6.5646e-02, 5.6935e-02, 5.7195e-07, 1.5254e-07,
        5.6875e-02, 2.9523e-02, 6.4279e-07, 9.8220e-03, 2.0643e-07, 2.3189e-07,
        1.6497e-07, 2.4185e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5176e-06, 1.7765e-06, 9.7293e-07, 1.0772e-07, 4.2459e-02, 3.2293e-06,
        1.1286e-06, 1.6314e-06, 1.5323e-06, 1.2949e-06, 3.2660e-02, 1.2816e-06,
        2.3873e-06, 3.1052e-02, 3.9451e-02, 9.9194e-02, 7.2299e-07, 6.1348e-07,
        4.1144e-02, 1.2036e-06, 8.0361e-02, 2.1958e-06, 1.9301e-06, 3.3888e-02,
        3.9612e-02, 3.3022e-06, 1.4604e-06, 3.9533e-02, 4.0720e-02, 8.7297e-02,
        1.1574e-06, 1.1249e-01, 3.2600e-02, 1.6974e-06, 3.4740e-02, 9.3725e-07,
        3.0205e-02, 2.6215e-06, 1.1258e-06, 1.6887e-06, 6.1348e-07, 8.2071e-02,
        9.0572e-07, 1.6529e-06, 6.1348e-07, 4.2833e-02, 3.1937e-06, 9.2641e-02,
        4.3755e-06, 3.3022e-06, 1.5424e-06, 1.8498e-06, 2.3873e-06, 8.4549e-02,
        2.9266e-02, 6.1348e-07, 1.6314e-06, 1.5176e-06, 1.4860e-06, 2.3873e-06,
        3.5496e-02, 2.6171e-06, 4.3755e-06, 5.2525e-07, 8.6319e-07, 1.8707e-06,
        8.6319e-07, 3.5235e-02, 3.0225e-02, 3.0614e-06, 9.1853e-02, 3.8657e-06,
        8.8552e-02, 3.3106e-02, 3.6250e-02, 8.5684e-07, 1.5179e-06, 1.7765e-06,
        7.7518e-07, 3.0030e-06, 4.2557e-06, 1.0036e-01, 1.5323e-06, 1.5176e-06,
        1.2543e-01, 1.0931e-01, 1.8270e-02, 1.5517e-06, 4.4407e-02, 3.1725e-06,
        3.9753e-02, 6.8553e-07, 3.9229e-02, 2.9811e-06, 2.2377e-06, 8.5992e-02,
        2.4258e-02, 3.4645e-02, 1.4389e-06, 4.3755e-06, 1.5176e-06, 4.0109e-02,
        1.7472e-06, 3.6137e-02, 3.3022e-06, 3.5012e-02, 2.1958e-06, 1.2746e-06,
        2.4228e-06, 3.8658e-02, 9.7293e-07, 4.7681e-06, 9.9014e-02, 3.8075e-02,
        2.5117e-06, 3.3522e-06, 3.0614e-06, 2.9074e-06, 4.0143e-02, 8.8598e-07,
        3.5323e-06, 9.1539e-02, 3.3388e-06, 1.6935e-06, 2.1787e-06, 6.1348e-07,
        1.1605e-06, 1.4604e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5997e-02, 2.1712e-07, 1.2043e-02, 2.1712e-07, 2.1712e-07, 9.2177e-03,
        6.7390e-08, 7.7308e-08, 7.7308e-08, 2.0238e-07, 2.0591e-02, 2.1272e-02,
        1.9728e-07, 9.0918e-03, 1.9029e-02, 1.6428e-01, 1.3789e-07, 6.8083e-03,
        7.5069e-03, 7.6054e-02, 5.2798e-08, 1.3810e-02, 4.0066e-02, 7.1407e-08,
        8.2377e-02, 2.0882e-02, 1.2837e-01, 3.9448e-08, 6.3988e-08, 1.3789e-07,
        1.4620e-02, 3.5393e-07, 1.4703e-02, 6.3774e-08, 1.2851e-02, 1.3951e-02,
        4.8402e-02, 2.1712e-07, 3.9327e-02, 1.0404e-07, 1.2202e-02, 4.8173e-08,
        9.7697e-03, 1.0404e-07, 3.5393e-07, 5.7414e-08, 6.3988e-08, 5.1890e-02,
        9.9331e-02, 7.4255e-02, 1.2388e-02, 9.4619e-08, 9.4619e-08, 7.2171e-02,
        1.0404e-07, 1.0518e-02, 1.2901e-02, 1.5748e-07, 5.2798e-08, 5.6222e-08,
        1.3241e-02, 4.3477e-08, 1.9316e-07, 7.5657e-02, 2.7035e-02, 4.5427e-02,
        2.0015e-07, 1.2965e-02, 3.8641e-02, 7.1407e-08, 1.2343e-02, 6.3988e-08,
        7.7308e-08, 8.9385e-03, 2.1427e-02, 4.7292e-08, 9.1229e-03, 1.4243e-02,
        7.1637e-02, 1.6354e-01, 1.2767e-02, 1.3789e-07, 1.2786e-01, 1.0799e-02,
        1.8342e-02, 2.4860e-02, 1.5111e-07, 1.0450e-07, 1.3412e-02, 5.1459e-02,
        4.3819e-02, 9.6101e-02, 8.3245e-08, 9.9059e-02, 6.6213e-08, 1.6845e-07,
        3.5884e-02, 7.7308e-08, 1.5217e-07, 2.8025e-02, 1.3789e-07, 2.3358e-02,
        3.1282e-07, 1.0175e-02, 9.1659e-03, 1.0404e-07, 8.2022e-02, 2.7024e-02,
        4.2031e-02, 1.8260e-02, 1.6558e-07, 1.7742e-07, 4.3477e-08, 7.5408e-02,
        1.5141e-07, 1.8182e-02, 4.1396e-02, 2.3730e-02, 3.5666e-02, 7.5536e-02,
        5.9666e-02, 1.1356e-02, 9.5108e-08, 1.0828e-07, 8.3246e-08, 5.2555e-02,
        7.6439e-08, 3.0969e-07, 1.9374e-07, 3.0055e-02, 2.2512e-02, 3.6417e-02,
        1.0450e-07, 6.7390e-08, 6.9035e-08, 3.2224e-03, 1.5111e-07, 6.3430e-02,
        9.6043e-03, 4.3477e-08, 1.2977e-02, 6.3749e-02, 2.2070e-02, 1.3634e-02,
        4.0929e-08, 1.3855e-02, 7.1407e-08, 4.5476e-08, 1.8557e-02, 5.2798e-08,
        1.5111e-07, 7.6627e-08, 2.0507e-02, 8.3252e-08, 9.7984e-08, 1.0781e-07,
        4.8018e-02, 5.3777e-02, 9.0261e-03, 7.4721e-02, 1.1384e-07, 8.6258e-08,
        1.8244e-07, 2.1712e-07, 8.3251e-08, 9.5107e-08, 1.0855e-01, 1.3211e-02,
        7.7308e-08, 5.6526e-02, 1.0099e-02, 7.6439e-08, 1.7742e-07, 1.4828e-02,
        3.0954e-07, 2.5250e-07, 3.4918e-02, 1.9728e-07, 1.7683e-02, 1.4715e-02,
        3.5393e-07, 3.5995e-07, 5.2173e-02, 3.7424e-02, 9.1463e-02, 1.0781e-07,
        1.4458e-02, 1.8313e-02, 6.4512e-03, 2.0238e-07, 1.9728e-07, 1.3977e-01,
        2.8285e-02, 1.9239e-07, 6.9133e-03, 1.5748e-07, 5.7414e-08, 1.5451e-02,
        4.9998e-02, 4.8401e-02, 4.0929e-08, 6.2063e-02, 8.9006e-08, 9.5108e-08,
        5.2798e-08, 1.4880e-07, 2.3645e-02, 1.0696e-07, 6.5933e-08, 9.1456e-08,
        9.4954e-08, 2.3653e-03, 2.4630e-02, 1.3897e-07, 5.5733e-02, 1.6909e-07,
        3.1282e-07, 1.5748e-07, 9.4619e-08, 3.5393e-07, 6.8062e-02, 7.6439e-08,
        7.0883e-02, 1.4804e-07, 1.0404e-07, 6.5933e-08, 1.5386e-02, 6.3774e-08,
        6.4603e-02, 6.3988e-08, 2.0862e-02, 1.3789e-07, 1.2022e-07, 1.4517e-07,
        1.5217e-07, 2.5037e-02, 2.5250e-07, 4.4269e-02, 1.5111e-07, 8.9006e-08,
        4.3390e-02, 5.7414e-08, 7.6018e-02, 5.1586e-03, 3.0679e-02, 9.1603e-03,
        1.7453e-07, 1.9728e-07, 1.0751e-01, 1.3824e-02, 6.2195e-02, 3.8307e-02,
        1.4786e-02, 1.5672e-02, 6.7746e-03, 4.8138e-02, 5.7414e-08, 2.0399e-02,
        1.4421e-02, 1.0450e-07, 1.7613e-01, 1.5131e-02, 3.2305e-02, 1.5121e-07,
        6.0117e-02, 4.4146e-02, 4.3041e-02, 6.4353e-02, 7.4771e-02, 1.0404e-07,
        1.0450e-07, 2.1712e-07, 1.1119e-01, 8.5329e-03, 1.4517e-07, 6.3988e-08,
        3.5393e-07, 1.9728e-07, 9.5108e-08, 1.2909e-02, 3.4274e-02, 4.3265e-02,
        1.4726e-02, 1.7008e-02, 4.3477e-08, 2.5250e-07, 1.7115e-02, 1.8769e-07,
        6.8589e-02, 2.7084e-07, 1.0285e-07, 1.2022e-07, 6.3988e-08, 1.1964e-01,
        8.0636e-03, 8.6486e-03, 2.0238e-07, 3.1893e-02, 7.6439e-08, 9.4977e-03,
        2.0386e-07, 1.2441e-02, 6.1821e-03, 3.5393e-07, 4.5476e-08, 2.0841e-07,
        6.7390e-08, 1.0898e-02, 1.2871e-02, 1.6909e-07, 2.6630e-02, 1.3845e-07,
        4.4560e-02, 9.7099e-03, 1.1824e-02, 1.9239e-07, 3.6107e-07, 5.7957e-02,
        4.3477e-08, 3.1282e-07, 1.9728e-07, 7.7308e-08, 5.5290e-02, 1.7265e-02,
        5.7414e-08, 1.0404e-07, 1.2857e-02, 8.2777e-08, 6.3988e-08, 2.5246e-02,
        1.0781e-07, 1.4517e-07, 8.2372e-02, 3.5393e-07, 2.7408e-02, 1.4517e-07,
        2.8640e-02, 4.5059e-08, 3.6460e-02, 6.3224e-03, 8.6050e-08, 3.1886e-02,
        5.7414e-08, 4.7293e-08, 1.0940e-02, 8.7390e-02, 9.5859e-02, 9.8006e-02,
        2.4456e-02, 1.3674e-02, 8.2777e-08, 1.3372e-07, 1.2809e-02, 2.7326e-02,
        3.5188e-02, 1.0736e-02, 6.5542e-03, 2.0841e-07, 7.7614e-03, 8.7967e-02,
        5.0634e-02, 2.0238e-07, 2.4554e-07, 9.7984e-08, 7.8449e-02, 2.5693e-02,
        1.8211e-01, 5.2418e-02, 9.5108e-08, 5.6639e-02, 9.7984e-08, 7.7308e-08,
        7.1862e-02, 2.1712e-07, 3.6699e-02, 6.1985e-08, 4.8625e-02, 3.0920e-07,
        1.2289e-02, 1.5141e-07, 2.0696e-02, 7.6626e-08, 1.3372e-07, 9.0841e-02,
        4.5476e-08, 2.1712e-07, 2.0238e-07, 1.5111e-07, 1.3897e-07, 2.5195e-08,
        1.0404e-07, 8.5725e-02, 3.5393e-07, 2.1574e-02, 1.1934e-02, 1.4880e-07,
        1.5949e-02, 1.0781e-07, 9.7984e-08, 7.5452e-03, 1.6909e-07, 3.7546e-02,
        6.2475e-08, 8.3252e-08, 2.5276e-03, 1.6509e-02, 2.6768e-02, 2.9730e-02,
        1.9728e-07, 6.3583e-02, 1.3901e-07, 2.5250e-07, 1.1054e-02, 7.7308e-08,
        4.8664e-03, 9.4619e-08, 6.6213e-08, 1.0063e-02, 4.3474e-02, 9.7984e-08,
        1.0404e-07, 1.3789e-07, 9.8848e-02, 1.6720e-01, 4.3565e-02, 2.5250e-07,
        6.3774e-08, 4.5476e-08, 1.2037e-02, 1.1598e-01, 8.3375e-03, 4.5476e-08,
        2.0238e-07, 1.3789e-07, 3.3628e-02, 2.2500e-02, 1.9416e-02, 1.9466e-02,
        7.7308e-08, 5.6222e-08, 9.8483e-03, 7.7308e-08, 3.5393e-07, 7.6439e-08,
        5.1002e-02, 5.9689e-08, 4.0146e-03, 1.0128e-01, 1.0404e-07, 4.2005e-02,
        1.3789e-07, 5.3911e-02, 3.5393e-07, 7.6439e-08, 1.3408e-01, 1.1952e-07,
        3.4609e-02, 2.8948e-02, 1.2069e-02, 5.7460e-02, 5.4776e-02, 6.8521e-03,
        2.4969e-02, 1.4579e-02, 3.6493e-02, 4.3478e-08, 2.0699e-02, 3.7009e-02,
        5.4846e-02, 1.0615e-01, 3.5592e-02, 2.0238e-07, 4.3477e-08, 1.4517e-07,
        7.7308e-08, 2.3005e-02, 2.3159e-02, 2.2835e-02, 1.7742e-07, 2.0025e-01,
        8.2625e-02, 1.3789e-07, 1.6716e-07, 8.6554e-08, 5.8933e-02, 8.4296e-02,
        6.3774e-08, 5.7414e-08, 9.4619e-08, 1.7570e-02, 1.2656e-02, 1.6368e-02,
        1.5217e-07, 2.2796e-01, 1.6716e-07, 2.3505e-01, 6.9034e-08, 4.2915e-03,
        1.1785e-02, 9.5108e-08, 1.8508e-02, 1.8026e-07, 5.7477e-02, 5.6950e-02,
        2.1712e-07, 2.1712e-07, 1.1952e-07, 7.8737e-02, 9.6819e-08, 1.0404e-07,
        1.4999e-02, 1.5141e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.1202e-02, 3.4000e-07, 8.2710e-02, 2.2944e-07, 1.0247e-07, 7.8847e-02,
        1.1229e-07, 9.6414e-08, 3.3444e-07, 4.3114e-07, 7.6611e-02, 2.3241e-07,
        1.3903e-07, 7.0014e-02, 9.1157e-02, 8.1195e-03, 6.1843e-08, 8.3697e-02,
        6.0644e-02, 6.6878e-03, 5.9542e-07, 7.5792e-02, 6.3726e-02, 6.6902e-08,
        8.7716e-03, 1.6197e-03, 4.2742e-02, 3.6715e-07, 8.4580e-08, 7.6682e-08,
        3.9543e-02, 9.6414e-08, 3.2774e-02, 1.7974e-07, 9.2519e-02, 7.0970e-02,
        3.4156e-03, 1.3903e-07, 4.3104e-02, 3.6715e-07, 4.9156e-02, 9.6414e-08,
        5.7480e-02, 8.4288e-08, 7.6682e-08, 1.0029e-07, 3.6715e-07, 4.1136e-03,
        5.9130e-03, 5.0565e-03, 5.2634e-02, 9.6414e-08, 1.5750e-07, 3.3450e-03,
        9.1287e-08, 1.1364e-01, 6.8515e-02, 6.0212e-08, 8.4288e-08, 8.4288e-08,
        1.0293e-01, 1.6613e-07, 3.1314e-07, 6.3018e-02, 6.1318e-08, 3.0190e-02,
        2.4197e-07, 7.5687e-02, 2.5738e-07, 9.1288e-08, 5.7871e-02, 4.5076e-07,
        1.5750e-07, 3.6131e-02, 1.2841e-01, 3.6715e-07, 5.4781e-02, 5.2366e-02,
        5.4987e-03, 4.1352e-03, 1.0244e-01, 1.7652e-07, 5.1839e-03, 1.9638e-02,
        1.0839e-07, 6.4402e-02, 1.9746e-07, 2.2944e-07, 7.2490e-02, 8.0630e-02,
        6.3352e-02, 4.0918e-03, 7.6681e-08, 7.5481e-03, 3.3751e-07, 3.4000e-07,
        1.0932e-07, 1.9746e-07, 3.6715e-07, 4.1022e-02, 3.6715e-07, 8.2269e-02,
        3.6715e-07, 3.2901e-02, 7.2180e-02, 1.9730e-07, 1.1679e-02, 7.1062e-03,
        6.4568e-02, 9.1761e-02, 2.5322e-07, 8.4288e-08, 4.5144e-08, 2.9266e-02,
        6.6902e-08, 5.8106e-02, 1.2750e-03, 9.0837e-02, 3.8955e-02, 6.7845e-03,
        6.2663e-02, 5.9293e-02, 7.6681e-08, 9.6414e-08, 8.4288e-08, 2.9695e-03,
        2.6569e-07, 5.0217e-07, 2.2999e-07, 4.1433e-02, 6.2195e-02, 9.7559e-02,
        8.4288e-08, 1.4994e-07, 6.6902e-08, 1.8602e-02, 4.5912e-07, 1.0315e-01,
        7.3999e-02, 5.0217e-07, 9.5934e-02, 6.9065e-02, 5.7904e-02, 5.2335e-02,
        1.1229e-07, 7.3111e-02, 5.9544e-07, 8.4288e-08, 7.2740e-02, 9.6414e-08,
        3.3444e-07, 1.0247e-07, 2.1281e-07, 2.2944e-07, 1.9746e-07, 4.5076e-07,
        7.7082e-02, 1.3343e-02, 6.7463e-02, 4.6311e-03, 7.6681e-08, 3.6715e-07,
        1.1229e-07, 9.6414e-08, 1.0247e-07, 1.9746e-07, 5.9297e-03, 7.9313e-02,
        3.4000e-07, 4.5664e-03, 3.2116e-02, 8.4288e-08, 3.6715e-07, 4.1900e-02,
        7.6681e-08, 1.1229e-07, 4.1028e-02, 9.6414e-08, 8.6769e-02, 8.1396e-02,
        6.1005e-07, 1.5772e-07, 4.1553e-02, 3.2229e-02, 1.3708e-02, 7.6682e-08,
        4.1808e-02, 6.1202e-02, 5.4940e-02, 8.4288e-08, 3.3444e-07, 7.5817e-03,
        1.8669e-07, 6.1843e-08, 6.5736e-02, 3.4000e-07, 6.1005e-07, 1.0560e-01,
        6.0559e-02, 5.7437e-03, 2.7493e-07, 1.0890e-01, 1.9730e-07, 3.6715e-07,
        2.2944e-07, 1.9746e-07, 8.9323e-02, 9.6414e-08, 6.1843e-08, 6.6902e-08,
        4.7111e-07, 8.8074e-08, 1.1261e-02, 2.6098e-07, 3.6863e-03, 1.1229e-07,
        1.0247e-07, 8.4288e-08, 6.6902e-08, 2.8128e-07, 2.8904e-03, 3.3444e-07,
        1.6168e-03, 1.5750e-07, 8.4288e-08, 1.5750e-07, 7.3471e-02, 3.3444e-07,
        4.6621e-03, 8.4288e-08, 5.5138e-02, 6.6902e-08, 6.6902e-08, 3.6715e-07,
        3.0091e-08, 8.2551e-02, 6.1005e-07, 5.5084e-02, 3.3444e-07, 2.6098e-07,
        5.9621e-02, 4.5076e-07, 5.0172e-03, 1.9703e-02, 5.6009e-02, 5.9140e-02,
        1.7115e-02, 4.7111e-07, 4.7714e-03, 7.7233e-02, 4.0175e-03, 7.9755e-02,
        6.1808e-02, 8.8051e-02, 7.1223e-02, 6.3339e-02, 3.4000e-07, 2.8052e-02,
        7.2908e-02, 4.7111e-07, 6.8841e-03, 5.9163e-02, 5.5585e-02, 8.0830e-08,
        6.1430e-03, 2.7195e-07, 7.7157e-02, 6.4846e-02, 4.6346e-03, 3.4000e-07,
        8.4288e-08, 2.7319e-07, 5.3408e-03, 5.5094e-02, 8.4288e-08, 6.1843e-08,
        8.4288e-08, 8.4288e-08, 3.6715e-07, 7.3771e-02, 4.1799e-02, 6.0042e-02,
        7.8212e-02, 9.4761e-02, 1.0247e-07, 3.6715e-07, 7.7868e-02, 3.3444e-07,
        1.6879e-03, 6.1005e-07, 9.6414e-08, 3.4000e-07, 8.4288e-08, 1.0628e-02,
        3.6470e-02, 9.3563e-02, 8.4288e-08, 4.2863e-02, 6.1005e-07, 4.4214e-02,
        3.4000e-07, 5.5770e-02, 1.2806e-03, 1.5750e-07, 3.3444e-07, 3.6715e-07,
        9.1288e-08, 6.6635e-02, 3.1021e-02, 2.1661e-07, 1.5147e-03, 3.4000e-07,
        2.7445e-03, 8.6716e-02, 5.7663e-02, 4.5076e-07, 2.9390e-02, 6.5045e-03,
        1.5750e-07, 4.5770e-07, 8.4288e-08, 6.6902e-08, 1.2269e-02, 1.0011e-01,
        9.6414e-08, 6.6902e-08, 3.7727e-02, 3.4000e-07, 6.6902e-08, 4.7991e-02,
        4.3114e-07, 1.1229e-07, 5.6568e-03, 3.4000e-07, 6.0907e-02, 1.5190e-07,
        6.2230e-08, 4.6724e-07, 6.9988e-02, 9.8986e-02, 2.2999e-07, 3.4694e-02,
        4.5076e-07, 3.1798e-07, 2.6819e-02, 2.3390e-03, 9.3922e-03, 4.3657e-03,
        6.6507e-02, 6.0524e-02, 8.4288e-08, 4.5076e-07, 6.1205e-02, 8.0723e-02,
        5.5078e-02, 7.7996e-02, 6.5400e-02, 2.3562e-07, 3.6852e-02, 4.2207e-03,
        1.5356e-07, 1.9730e-07, 1.0247e-07, 9.1288e-08, 2.3456e-02, 4.9912e-02,
        7.5979e-03, 5.3436e-02, 1.5750e-07, 1.0444e-03, 3.6715e-07, 1.9314e-07,
        3.9290e-03, 8.4288e-08, 4.5427e-02, 8.4288e-08, 1.7747e-07, 3.4000e-07,
        1.7896e-06, 3.4000e-07, 7.1526e-02, 8.4288e-08, 7.6681e-08, 4.3266e-02,
        4.6180e-07, 4.5076e-07, 1.9314e-07, 2.1385e-07, 1.9746e-07, 3.4000e-07,
        6.6902e-08, 1.8886e-03, 2.5322e-07, 6.8988e-02, 6.1752e-02, 4.5912e-07,
        9.0434e-08, 2.4197e-07, 9.1287e-08, 2.0621e-02, 4.5906e-07, 1.0708e-01,
        8.4288e-08, 9.6414e-08, 3.7369e-07, 4.4232e-02, 1.4176e-07, 2.0722e-01,
        8.4288e-08, 2.8179e-02, 1.1557e-02, 2.7493e-07, 5.1923e-02, 3.3444e-07,
        3.4347e-02, 3.6715e-07, 2.6098e-07, 9.5626e-02, 1.2890e-03, 7.6682e-08,
        4.6181e-07, 4.5076e-07, 6.9122e-03, 2.5696e-03, 1.5553e-03, 3.6715e-07,
        6.6902e-08, 9.6414e-08, 8.0139e-02, 7.9899e-03, 4.8148e-02, 2.1385e-07,
        4.7111e-07, 6.6902e-08, 3.9427e-02, 8.2280e-02, 8.2857e-02, 7.7402e-02,
        9.6414e-08, 2.5322e-07, 1.1471e-01, 1.9730e-07, 1.0247e-07, 1.3902e-07,
        7.3443e-03, 5.4890e-02, 3.7159e-02, 4.2149e-03, 9.6414e-08, 6.8065e-02,
        8.4288e-08, 8.7342e-02, 3.1798e-07, 4.5906e-07, 5.8087e-03, 3.4000e-07,
        3.3208e-02, 2.8827e-02, 7.3441e-02, 6.1486e-02, 6.4838e-02, 2.8468e-02,
        5.2085e-02, 5.6572e-02, 6.7079e-02, 6.1843e-08, 9.6140e-02, 8.2555e-02,
        6.8479e-02, 3.9321e-03, 9.1068e-08, 1.1526e-07, 8.4288e-08, 7.6681e-08,
        8.4288e-08, 3.4388e-02, 1.3026e-07, 8.2000e-02, 9.6414e-08, 9.3146e-03,
        3.1355e-03, 2.6098e-07, 3.6715e-07, 2.7319e-07, 3.3581e-03, 3.8092e-03,
        4.5076e-07, 6.9117e-08, 6.1843e-08, 9.0348e-02, 7.9545e-02, 6.1467e-02,
        4.7111e-07, 2.8284e-03, 1.1526e-07, 4.0732e-03, 4.6182e-07, 3.5428e-02,
        4.9065e-02, 3.6715e-07, 6.5540e-02, 3.8480e-02, 8.5646e-02, 7.9600e-02,
        6.6902e-08, 8.4288e-08, 6.1843e-08, 3.5675e-03, 6.6902e-08, 8.4288e-08,
        4.9512e-02, 6.6902e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.8238e-07, 1.3272e-02, 5.4664e-02, 8.6431e-02, 1.0545e-06, 6.1097e-07,
        4.9995e-07, 7.1722e-07, 3.4836e-02, 6.8947e-07, 1.4088e-07, 2.3841e-07,
        7.1641e-02, 4.7492e-07, 6.8947e-07, 2.3841e-07, 1.8151e-01, 2.9732e-07,
        4.9995e-07, 1.4088e-07, 4.9995e-07, 1.4088e-07, 2.7528e-02, 9.7687e-08,
        4.2507e-07, 4.7084e-02, 7.2694e-02, 6.8238e-07, 4.7492e-07, 3.8877e-07,
        5.6687e-02, 7.1722e-07, 6.1613e-02, 5.2242e-02, 6.3199e-02, 1.9745e-07,
        8.1615e-07, 4.0192e-07, 1.4088e-07, 1.4032e-07, 4.7492e-07, 1.1028e-07,
        3.6064e-02, 4.1347e-07, 2.7473e-02, 6.1097e-07, 2.6548e-07, 2.3783e-01,
        5.7949e-02, 4.7416e-02, 1.4032e-07, 2.3841e-07, 8.1615e-07, 1.5653e-07,
        1.4088e-07, 3.1246e-02, 6.0911e-08, 1.4088e-07, 4.9995e-07, 3.8877e-07,
        4.5197e-02, 2.6548e-07, 7.1722e-07, 6.8238e-07, 1.1073e-07, 2.5065e-07,
        4.6999e-02, 6.8947e-07, 9.1224e-02, 5.3605e-07, 1.0940e-06, 1.0554e-06,
        3.8254e-02, 1.4088e-07, 1.5653e-07, 1.1028e-07, 3.0653e-07, 6.1889e-02,
        5.3124e-02, 5.7039e-07, 6.8417e-07, 6.0201e-02, 7.8497e-07, 7.1722e-07,
        2.6547e-07, 4.8895e-02, 8.5114e-02, 7.7175e-02, 7.1722e-07, 1.9750e-07,
        4.8799e-02, 9.9637e-02, 5.8037e-02, 1.0333e-01, 1.1028e-07, 6.1892e-02,
        3.0653e-07, 6.2079e-07, 1.4531e-02, 5.0778e-02, 8.4740e-02, 1.4032e-07,
        1.9745e-07, 1.4032e-07, 4.2007e-07, 2.9627e-07, 1.1028e-07, 7.1722e-07,
        3.8877e-07, 2.5065e-07, 4.9396e-02, 6.7502e-02, 5.8915e-02, 7.7942e-02,
        4.9995e-07, 2.9732e-07, 4.2768e-07, 6.8238e-07, 5.8394e-02, 2.3841e-07,
        4.9995e-07, 1.9745e-07, 6.6775e-02, 5.0866e-02, 4.6717e-02, 1.0840e-06,
        7.7166e-02, 3.0023e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.4339e-07, 8.3752e-07, 8.9824e-02, 7.1803e-02, 5.4339e-07, 3.3228e-07,
        7.3443e-07, 6.2021e-07, 6.7751e-02, 1.2386e-01, 1.2851e-01, 5.4339e-07,
        9.3817e-02, 7.7255e-02, 8.6555e-07, 1.2776e-01, 8.5531e-02, 1.4490e-06,
        1.3935e-06, 3.3228e-07, 1.6814e-01, 7.3054e-02, 1.2625e-01, 1.0781e-01,
        3.3228e-07, 4.4032e-02, 2.5861e-07, 5.4339e-07, 6.8477e-02, 3.3228e-07,
        4.0699e-02, 7.8154e-07, 1.1082e-01, 8.3650e-02, 1.3440e-06, 1.1847e-01,
        8.3752e-07, 9.5539e-02, 1.2244e-01, 1.3291e-01, 3.3228e-07, 1.1676e-01,
        1.1234e-06, 6.9636e-02, 5.3859e-07, 1.8496e-06, 8.9889e-07, 7.8291e-07,
        7.7707e-02, 1.8496e-06, 8.4722e-02, 1.8496e-06, 1.3440e-06, 7.3443e-07,
        8.4546e-07, 2.3528e-06, 1.1596e-01, 8.2744e-07, 4.6269e-02, 1.1438e-01,
        1.0303e-01, 5.3859e-07, 3.3228e-07, 8.6848e-02, 7.3443e-07, 1.0184e-01,
        4.9747e-07, 6.2021e-07, 1.4269e-06, 1.2717e-01, 6.2212e-07, 1.1417e-01,
        4.1371e-07, 1.1044e-01, 2.3534e-06, 1.2234e-01, 1.2398e-06, 1.0337e-01,
        9.9967e-02, 7.1653e-07, 3.8865e-07, 1.2262e-06, 1.3848e-01, 1.0434e-01,
        5.0586e-07, 3.3228e-07, 6.6505e-02, 7.3443e-07, 1.1234e-06, 8.3425e-02,
        8.3440e-02, 1.3272e-01, 1.8496e-06, 7.3443e-07, 3.3228e-07, 3.8865e-07,
        8.4546e-07, 8.4871e-02, 7.1812e-02, 6.9876e-02, 9.3589e-02, 7.3443e-07,
        6.9864e-02, 1.1068e-01, 8.7863e-02, 7.8154e-07, 1.0408e-07, 1.2627e-01,
        3.3228e-07, 9.3503e-02, 1.4490e-06, 8.6555e-07, 5.4339e-07, 7.8291e-07,
        8.7750e-02, 8.9889e-07, 1.3294e-01, 3.5886e-07, 5.4338e-07, 3.3228e-07,
        1.0246e-01, 1.3990e-01, 1.0499e-01, 1.5813e-06, 7.8291e-07, 9.6731e-02,
        1.2792e-01, 1.8350e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.3255e-02, 1.7475e-07, 3.4984e-02, 6.2762e-02, 1.7766e-07, 1.3098e-02,
        2.3179e-02, 5.4527e-03, 7.9261e-02, 2.7793e-07, 5.6026e-02, 3.2939e-08,
        2.1823e-03, 5.5367e-02, 5.0876e-02, 4.0362e-02, 5.1542e-02, 3.7488e-02,
        1.1063e-02, 1.2791e-02, 7.0569e-08, 4.6979e-02, 8.2823e-02, 8.7566e-02,
        4.5137e-02, 1.3024e-07, 5.3173e-02, 5.6060e-02, 7.9110e-02, 4.5605e-07,
        6.1357e-02, 2.8542e-02, 7.1285e-02, 1.8505e-07, 4.6784e-02, 1.6980e-01,
        5.5213e-02, 4.5338e-08, 6.6412e-02, 9.1883e-02, 5.3501e-02, 4.5306e-02,
        5.5217e-02, 2.2213e-07, 1.5359e-01, 6.8959e-02, 3.7735e-02, 1.3036e-01,
        1.4060e-01, 7.7066e-03, 4.4449e-03, 5.2618e-02, 1.3479e-01, 3.9523e-02,
        9.6499e-03, 3.7861e-02, 5.6537e-02, 1.4356e-01, 7.0569e-08, 3.4161e-07,
        3.7447e-02, 2.2592e-07, 1.6911e-01, 2.7275e-02, 4.7906e-02, 5.2191e-02,
        6.3376e-02, 4.7420e-02, 3.2368e-07, 2.5496e-02, 5.0493e-02, 1.1669e-01,
        2.2862e-07, 1.3129e-01, 6.3210e-02, 3.4217e-07, 6.0910e-02, 5.8194e-02,
        7.6397e-02, 3.6959e-02, 3.3044e-02, 2.2593e-07, 1.1470e-02, 1.6634e-01,
        2.5430e-02, 6.3653e-02, 8.7684e-08, 5.2228e-08, 2.7140e-02, 2.4185e-02,
        5.2815e-02, 3.0115e-02, 8.0726e-02, 6.8057e-02, 6.9806e-02, 9.0264e-08,
        7.8676e-02, 5.9033e-02, 4.5086e-02, 6.0919e-02, 8.2582e-02, 1.5325e-02,
        4.3412e-02, 4.6039e-02, 3.3171e-02, 2.9303e-02, 6.1906e-02, 1.2102e-02,
        6.2642e-02, 1.4825e-02, 5.7452e-08, 1.6157e-02, 1.3548e-07, 4.7362e-02,
        5.7406e-02, 5.8812e-02, 4.9070e-02, 5.6544e-02, 2.1322e-02, 5.3295e-02,
        3.5862e-02, 2.5646e-02, 4.3263e-02, 9.6228e-03, 1.4801e-07, 6.0043e-03,
        9.6545e-02, 2.2593e-07, 7.0569e-08, 8.5986e-02, 6.4373e-02, 3.5891e-02,
        5.4852e-02, 6.1980e-02, 1.5013e-01, 3.9566e-02, 3.4161e-07, 5.8195e-02,
        2.9497e-02, 7.3309e-02, 4.6894e-02, 1.7269e-02, 5.4646e-02, 4.6744e-02,
        8.7348e-08, 9.4485e-02, 2.6990e-02, 6.4308e-02, 6.6685e-02, 7.0611e-08,
        3.6698e-02, 5.9062e-08, 2.1300e-07, 1.3887e-03, 1.0342e-01, 6.6875e-02,
        4.1808e-02, 6.8137e-03, 6.2253e-03, 4.8708e-02, 1.1938e-07, 3.5494e-02,
        2.0195e-07, 1.7373e-07, 6.6022e-02, 6.4724e-02, 9.9498e-03, 4.2404e-02,
        7.5688e-02, 6.3167e-02, 6.0733e-02, 2.2213e-07, 2.1686e-07, 6.6808e-02,
        3.5945e-07, 9.0817e-02, 1.5481e-01, 3.4011e-07, 4.4014e-02, 2.1457e-02,
        9.5481e-08, 8.9745e-02, 4.1990e-02, 4.3831e-02, 6.1495e-02, 1.3336e-07,
        3.0899e-02, 3.4631e-02, 2.0200e-02, 8.7684e-08, 1.3319e-07, 5.2248e-02,
        1.4787e-02, 1.0905e-07, 1.8624e-01, 1.2882e-01, 1.7306e-02, 3.0200e-02,
        5.4166e-02, 9.7987e-02, 1.9233e-02, 1.1955e-02, 2.2048e-02, 2.3100e-01,
        7.0569e-08, 8.7684e-08, 5.5004e-02, 4.6541e-02, 3.6514e-02, 7.7775e-02,
        9.0416e-02, 1.6405e-07, 1.9869e-07, 2.4302e-01, 5.6671e-02, 7.0611e-08,
        5.8939e-02, 5.7452e-08, 1.3265e-07, 9.5481e-08, 4.7651e-02, 2.3996e-07,
        1.6219e-01, 1.2688e-01, 7.3023e-02, 1.6841e-07, 2.5223e-02, 7.1473e-08,
        1.4326e-02, 6.2302e-02, 1.7055e-02, 4.2859e-08, 4.1579e-02, 1.4009e-01,
        2.5401e-07, 1.7451e-02, 3.4980e-02, 4.8122e-02, 5.3522e-02, 9.5481e-08,
        8.1368e-02, 8.3607e-02, 5.6736e-02, 9.7277e-02, 7.0452e-02, 6.3068e-02,
        1.2527e-02, 1.3323e-07, 6.0439e-02, 8.7675e-03, 5.8498e-02, 5.5862e-02,
        3.1869e-02, 5.4303e-02, 4.4845e-02, 4.4995e-02, 6.9432e-03, 1.0506e-01,
        8.1576e-03, 5.5448e-02, 1.5675e-02, 8.7076e-02, 6.0770e-02, 2.3996e-07,
        8.8162e-03, 1.7752e-07, 7.8083e-02, 2.3686e-02, 5.7239e-03, 1.1872e-07,
        1.2498e-04, 1.3911e-07, 1.4828e-02, 6.1616e-02, 4.8838e-02, 7.4098e-02,
        6.0503e-02, 4.2732e-02, 9.5480e-08, 2.7174e-02, 5.8052e-02, 5.3312e-02,
        6.6388e-02, 6.4883e-02, 6.1715e-02, 2.6368e-02, 3.5976e-02, 1.5968e-01,
        2.3488e-02, 4.7739e-02, 3.2621e-02, 6.0689e-02, 1.3323e-07, 3.5044e-02,
        4.2200e-02, 3.6936e-02, 6.8422e-02, 3.0176e-02, 7.6350e-08, 5.6492e-02,
        3.4474e-02, 6.1214e-02, 6.7151e-02, 1.3063e-07, 1.9441e-02, 2.0195e-07,
        4.8544e-02, 5.9566e-02, 2.3097e-01, 2.2766e-07, 5.4149e-02, 4.6061e-08,
        4.6965e-02, 4.0866e-02, 5.1954e-02, 1.1938e-07, 3.0523e-07, 1.8172e-02,
        5.9062e-08, 1.4498e-01, 3.9131e-02, 5.0983e-02, 1.3501e-01, 7.4954e-02,
        4.3637e-02, 7.0569e-08, 1.7526e-02, 7.0611e-08, 1.9726e-02, 6.6209e-02,
        3.9356e-08, 9.2767e-02, 4.0359e-02, 1.4322e-01, 4.7304e-02, 1.3323e-07,
        1.8169e-07, 2.0195e-07, 3.9179e-02, 3.4227e-02, 9.5481e-08, 3.1608e-02,
        1.5088e-01, 1.2173e-02, 6.2781e-02, 8.1542e-03, 1.9220e-02, 5.5664e-02,
        3.1460e-02, 5.9138e-03, 2.4731e-02, 5.3283e-08, 1.3817e-02, 5.5968e-02,
        3.0975e-02, 4.4952e-02, 5.0147e-02, 7.6570e-02, 5.0165e-02, 1.9154e-02,
        1.2796e-07, 5.2272e-02, 3.6898e-02, 6.4026e-02, 3.5029e-02, 2.8303e-02,
        3.4341e-02, 1.3131e-01, 3.5928e-08, 1.0899e-07, 6.0151e-02, 6.3667e-02,
        4.9632e-02, 4.6061e-08, 7.1284e-03, 1.1487e-07, 6.5399e-02, 1.8882e-01,
        1.8347e-01, 8.6867e-08, 2.7897e-02, 7.0569e-08, 1.7766e-07, 3.2681e-02,
        8.0255e-02, 1.7766e-07, 1.3398e-01, 1.3323e-07, 6.2265e-02, 6.3952e-08,
        1.8505e-07, 2.9566e-02, 1.8732e-07, 5.3235e-02, 6.9281e-02, 3.4045e-07,
        2.5485e-07, 5.9707e-02, 1.3960e-01, 6.5363e-02, 5.5654e-02, 2.1825e-02,
        8.6112e-02, 1.0828e-03, 4.5819e-02, 2.4666e-02, 2.7330e-02, 5.3549e-02,
        7.9685e-02, 5.4586e-02, 4.0178e-02, 7.2528e-02, 5.4905e-02, 2.2213e-07,
        2.7766e-07, 1.4340e-01, 9.5300e-08, 5.5407e-02, 7.7338e-03, 5.7452e-08,
        1.6988e-08, 3.4011e-07, 1.1443e-02, 2.0742e-02, 3.3601e-03, 2.2213e-07,
        1.3232e-07, 5.8756e-02, 4.8769e-02, 1.9645e-01, 6.5420e-02, 3.4045e-07,
        2.4865e-08, 1.3078e-07, 8.2575e-02, 7.2699e-02, 6.1473e-02, 2.7827e-02,
        1.4902e-07, 2.4303e-02, 2.7609e-02, 2.2213e-07, 2.2213e-07, 4.9916e-02,
        1.0043e-02, 6.9212e-03, 4.0699e-02, 1.0773e-02, 7.4285e-02, 6.1995e-02,
        9.9184e-08, 5.1875e-02, 1.2775e-02, 6.4489e-02, 1.5129e-02, 2.2593e-07,
        1.1422e-02, 1.8412e-07, 3.0469e-02, 9.3671e-03, 6.3253e-02, 3.7288e-02,
        6.3855e-03, 1.7101e-01, 7.4997e-02, 6.3259e-02, 3.9827e-02, 2.8620e-02,
        2.0620e-02, 3.7143e-02, 1.0855e-07, 1.0428e-01, 9.2359e-02, 7.4571e-02,
        8.1091e-02, 9.8627e-03, 1.3049e-07, 6.9792e-02, 1.5945e-01, 2.4792e-02,
        1.6860e-01, 3.4576e-02, 3.4161e-07, 6.8367e-02, 7.1925e-02, 7.3391e-03,
        6.3628e-08, 9.5481e-08, 5.4200e-02, 1.5670e-02, 5.0215e-02, 8.7878e-02,
        6.3628e-08, 2.7089e-02, 1.4754e-01, 2.0314e-02, 2.2310e-01, 3.5207e-02,
        7.0378e-02, 3.4161e-07, 5.0280e-02, 5.6037e-02, 3.8121e-02, 1.4568e-02,
        2.4812e-08, 7.8299e-02, 1.3232e-07, 3.7807e-02, 1.5956e-01, 7.0569e-08,
        5.9088e-02, 8.8941e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.4882e-02, 1.1000e-01, 4.0391e-02, 7.0605e-07, 1.7002e-01, 5.1175e-02,
        4.1348e-02, 9.4457e-07, 3.1778e-02, 3.6702e-02, 5.2253e-02, 5.6826e-02,
        1.2659e-06, 5.0569e-07, 3.9354e-02, 1.2417e-01, 4.4286e-07, 1.2229e-01,
        9.7702e-07, 3.0096e-02, 3.9276e-02, 3.8852e-02, 3.6909e-02, 4.8966e-07,
        4.6114e-02, 3.5258e-02, 2.7891e-07, 1.6956e-06, 8.9660e-07, 4.2755e-02,
        4.1949e-02, 4.6628e-02, 3.9273e-02, 2.7368e-02, 1.3334e-01, 2.7569e-02,
        4.6777e-07, 1.7005e-06, 3.7481e-02, 1.0206e-06, 4.0298e-02, 3.9755e-02,
        3.4120e-02, 1.2555e-01, 1.4333e-01, 5.2789e-02, 9.7702e-07, 1.6696e-01,
        3.6367e-07, 3.2804e-07, 4.7127e-02, 5.1239e-02, 5.2289e-02, 1.2754e-01,
        2.7414e-03, 5.5832e-02, 1.0206e-06, 4.2864e-02, 4.7514e-02, 1.0985e-01,
        4.4286e-07, 4.2885e-02, 5.6144e-02, 3.2010e-02, 1.2287e-06, 1.6011e-06,
        1.0828e-06, 9.2700e-07, 4.9027e-02, 4.8689e-02, 6.4304e-07, 1.1307e-01,
        3.9693e-02, 6.3379e-07, 3.4009e-07, 3.5741e-07, 4.4620e-02, 4.3161e-02,
        2.3903e-07, 4.4977e-02, 4.4468e-07, 3.6691e-02, 4.1727e-02, 4.5560e-02,
        4.0509e-02, 5.7640e-02, 4.8714e-02, 3.6944e-08, 4.3883e-02, 3.3688e-07,
        4.7639e-02, 5.0607e-02, 4.8297e-02, 6.2594e-07, 4.3588e-02, 6.3379e-07,
        5.8920e-02, 6.5891e-07, 1.3885e-01, 4.5940e-02, 4.2353e-02, 4.4348e-02,
        5.6968e-07, 4.7225e-02, 7.4747e-07, 4.9347e-02, 5.1056e-02, 4.9508e-02,
        4.6061e-02, 8.5812e-07, 4.6406e-02, 5.0021e-02, 4.2599e-07, 1.5707e-06,
        4.5499e-07, 4.7807e-07, 4.7890e-02, 7.0536e-07, 4.2048e-02, 1.4304e-06,
        4.2225e-02, 1.7406e-01, 1.1235e-01, 4.8986e-02, 5.1269e-02, 9.4479e-07,
        4.4341e-02, 5.2550e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1074e-06, 1.0657e-01, 5.0324e-06, 2.5088e-06, 8.6756e-02, 2.7006e-06,
        5.0324e-06, 9.6249e-02, 7.9261e-02, 1.0593e-06, 3.0964e-06, 2.2519e-06,
        6.4679e-06, 1.0998e-01, 3.8412e-06, 5.7332e-06, 1.3753e-01, 8.3825e-02,
        7.9348e-02, 7.4584e-07, 7.7164e-02, 9.9346e-02, 3.9373e-06, 1.5703e-06,
        3.2039e-06, 3.3762e-06, 8.8013e-02, 4.4445e-02, 3.8306e-02, 3.0940e-06,
        4.5328e-02, 2.0296e-06, 3.3437e-02, 9.0660e-02, 3.5235e-06, 1.6461e-06,
        1.0530e-06, 1.0156e-01, 1.9053e-06, 3.0706e-06, 6.6962e-06, 1.5146e-06,
        2.6386e-06, 1.0953e-06, 2.9225e-06, 2.7083e-06, 2.2087e-07, 2.1890e-06,
        8.1025e-02, 9.9907e-02, 2.6348e-06, 1.4643e-06, 8.3855e-02, 4.8152e-06,
        1.1728e-06, 3.6195e-02, 3.6010e-06, 9.7008e-02, 7.8288e-02, 8.1028e-02,
        3.4326e-06, 8.3252e-02, 1.1629e-01, 4.2557e-07, 1.8191e-06, 9.3196e-02,
        1.4035e-06, 2.6389e-06, 8.7262e-02, 1.8370e-06, 1.2524e-01, 2.0287e-06,
        1.8370e-06, 1.2100e-01, 1.0163e-01, 9.7558e-02, 3.5620e-02, 1.2482e-01,
        1.6557e-06, 8.6405e-02, 2.4219e-06, 7.2025e-02, 1.9540e-06, 3.8858e-02,
        1.0120e-01, 4.0136e-06, 2.4259e-06, 2.6629e-06, 2.6277e-06, 2.6369e-06,
        8.9841e-02, 1.4643e-06, 8.7028e-02, 3.7597e-06, 1.0396e-01, 2.4093e-06,
        9.1232e-02, 3.0940e-06, 1.1378e-01, 1.1508e-01, 2.1676e-06, 8.8144e-07,
        7.9292e-02, 1.1598e-07, 4.0136e-06, 3.8054e-06, 3.6010e-06, 2.2086e-07,
        1.2705e-06, 8.0119e-02, 9.6083e-02, 4.0136e-06, 4.5916e-06, 3.2586e-06,
        4.2596e-06, 1.3984e-06, 8.9594e-02, 2.5271e-06, 1.6195e-06, 8.8533e-02,
        1.2798e-06, 1.0226e-01, 4.8418e-06, 1.5703e-06, 2.7630e-07, 9.5156e-02,
        7.8830e-02, 1.0664e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.8707e-02, 1.3878e-07, 4.8368e-02, 7.7008e-02, 3.6210e-07, 1.8279e-02,
        4.8645e-02, 4.8387e-03, 7.1956e-02, 1.7714e-07, 7.4657e-02, 1.4930e-07,
        1.1393e-07, 3.4301e-02, 2.4994e-02, 3.8908e-02, 6.9502e-02, 2.3396e-02,
        1.4992e-01, 2.1552e-02, 3.6700e-08, 5.4992e-02, 5.3413e-02, 3.3261e-02,
        1.9410e-01, 4.3178e-02, 8.4818e-02, 6.6042e-02, 1.1708e-02, 1.1641e-07,
        2.8851e-02, 1.1087e-07, 7.8734e-02, 3.6700e-08, 2.7361e-02, 3.4342e-02,
        6.9528e-08, 7.2202e-08, 2.3011e-02, 6.2583e-02, 2.3201e-02, 1.2789e-02,
        4.7468e-02, 6.4753e-08, 5.3419e-02, 1.2491e-02, 9.7264e-08, 3.3071e-02,
        1.2244e-02, 9.0286e-03, 1.7291e-07, 1.4574e-01, 5.2862e-02, 1.7062e-02,
        1.3972e-07, 5.7635e-02, 6.2325e-02, 4.1195e-02, 6.6280e-08, 3.6037e-02,
        7.6890e-02, 1.2574e-01, 1.7307e-02, 6.8930e-02, 2.3646e-07, 1.3272e-01,
        6.7942e-02, 7.5956e-02, 2.3305e-07, 8.6876e-08, 6.7482e-02, 6.2887e-02,
        2.3586e-07, 4.0620e-02, 7.3182e-02, 3.4871e-07, 5.5040e-02, 6.4381e-02,
        1.3774e-02, 3.0865e-02, 6.0204e-02, 5.3048e-02, 1.8008e-02, 1.7720e-02,
        7.4908e-08, 5.6662e-02, 3.3025e-02, 1.8629e-01, 7.9573e-02, 2.0642e-02,
        4.2141e-02, 6.6128e-02, 2.3239e-02, 1.3113e-02, 5.0923e-02, 3.6407e-07,
        1.8799e-07, 3.2955e-02, 1.3688e-01, 4.5342e-02, 1.7306e-01, 4.2804e-02,
        1.2630e-01, 1.8224e-01, 4.6884e-02, 3.1461e-02, 1.2955e-01, 6.7820e-02,
        3.0156e-02, 4.7379e-02, 8.7696e-08, 8.1999e-03, 1.9925e-07, 7.6024e-02,
        2.5809e-07, 6.3440e-02, 2.4837e-02, 5.1805e-02, 1.1412e-02, 2.6727e-02,
        1.4874e-01, 3.6443e-02, 1.1588e-07, 1.7361e-07, 1.1578e-07, 4.1894e-02,
        3.1462e-02, 1.1641e-07, 2.6567e-07, 5.3054e-02, 1.3805e-02, 3.2972e-02,
        5.2449e-02, 6.3330e-02, 1.6884e-02, 1.0456e-07, 1.2588e-07, 8.2589e-02,
        5.4492e-02, 1.3141e-01, 4.5806e-02, 2.7309e-02, 7.2090e-02, 2.8437e-02,
        1.7093e-07, 3.1204e-02, 1.5591e-01, 7.9364e-02, 4.0739e-02, 3.6208e-07,
        2.1496e-01, 1.9876e-07, 1.0952e-02, 7.2597e-02, 7.9524e-02, 1.7386e-02,
        1.9408e-02, 3.3561e-02, 2.5336e-02, 2.3666e-02, 3.6700e-08, 3.5516e-07,
        3.6301e-07, 5.8700e-08, 3.3326e-02, 2.5449e-02, 3.8155e-02, 5.2261e-02,
        4.4310e-02, 3.8360e-02, 4.7952e-02, 1.5411e-07, 6.9210e-02, 6.9584e-02,
        1.3878e-07, 1.7221e-02, 4.7785e-02, 1.9023e-07, 5.1388e-02, 5.1918e-02,
        2.5010e-07, 2.7006e-02, 2.3032e-02, 2.8508e-02, 4.7652e-02, 6.6280e-08,
        5.0604e-02, 1.9706e-01, 2.0864e-02, 7.2388e-08, 4.8363e-02, 3.4964e-02,
        3.1346e-02, 8.4420e-02, 5.4586e-02, 5.5948e-02, 3.1292e-07, 2.5266e-02,
        3.8734e-02, 5.4406e-02, 4.4161e-02, 2.8909e-02, 1.6983e-07, 2.7969e-02,
        7.3476e-02, 1.9925e-07, 1.3511e-02, 8.0577e-02, 4.2544e-08, 8.7296e-02,
        4.1154e-02, 1.6955e-07, 2.8908e-02, 1.8813e-01, 1.4746e-01, 8.2210e-08,
        3.9599e-02, 8.2251e-08, 6.1927e-02, 1.1387e-07, 3.8192e-02, 1.9925e-07,
        3.2218e-02, 3.6274e-02, 7.0644e-02, 1.0375e-07, 5.5487e-02, 1.7093e-07,
        4.9092e-02, 5.0164e-02, 2.6322e-02, 5.6143e-02, 1.4321e-01, 2.6464e-02,
        1.7550e-07, 8.7538e-02, 2.5009e-07, 1.8301e-01, 3.3685e-02, 7.2475e-02,
        4.0047e-02, 1.7765e-02, 5.2646e-02, 4.1898e-02, 5.1934e-02, 4.8643e-02,
        3.7432e-08, 3.1009e-07, 2.5644e-02, 2.1486e-02, 5.3245e-02, 4.7962e-02,
        4.5541e-02, 7.0815e-02, 6.9051e-08, 3.7437e-02, 2.3677e-07, 7.1271e-02,
        4.9172e-02, 7.3139e-02, 3.4538e-02, 6.0421e-02, 6.9782e-02, 1.5411e-07,
        2.8296e-02, 1.0173e-07, 2.4229e-02, 5.6789e-02, 1.0704e-02, 8.6949e-08,
        8.3037e-02, 7.4108e-08, 3.2847e-02, 2.1768e-07, 1.3950e-01, 1.7052e-01,
        1.9198e-01, 1.5383e-01, 1.2569e-07, 3.5132e-02, 2.0969e-02, 4.0568e-02,
        2.9646e-02, 4.2078e-02, 7.9236e-02, 9.8207e-08, 4.1622e-02, 6.5794e-02,
        2.5168e-02, 5.0166e-02, 1.2355e-01, 7.7448e-02, 6.0196e-08, 8.1464e-02,
        5.8965e-02, 4.8349e-02, 3.5096e-02, 3.8653e-02, 1.3878e-07, 8.8483e-02,
        7.1141e-08, 5.8403e-02, 2.6320e-02, 2.2213e-07, 1.2060e-07, 3.6700e-08,
        5.9499e-02, 1.6491e-01, 1.7575e-02, 1.3227e-07, 3.4370e-02, 2.8932e-07,
        3.3631e-02, 2.4684e-02, 1.8870e-01, 1.1326e-07, 3.5949e-02, 1.5545e-02,
        2.3566e-07, 4.9389e-02, 8.4913e-02, 6.8669e-02, 2.5181e-02, 6.9728e-02,
        4.4327e-02, 7.2388e-08, 2.0020e-01, 1.5411e-07, 5.4785e-02, 1.3569e-02,
        2.2518e-07, 2.6651e-02, 6.6091e-02, 3.4909e-02, 3.4742e-02, 7.9447e-08,
        7.8768e-08, 1.9876e-07, 1.8169e-02, 4.9502e-02, 3.1009e-07, 1.0485e-07,
        6.9142e-02, 1.6433e-02, 2.0090e-01, 3.0539e-02, 1.9631e-02, 3.7093e-02,
        4.5113e-02, 3.5713e-02, 4.6427e-02, 1.0305e-07, 3.5825e-02, 4.6103e-02,
        1.5019e-02, 5.9441e-02, 7.9739e-02, 2.1364e-01, 8.7065e-03, 3.5802e-02,
        1.6918e-07, 4.3801e-02, 1.6736e-07, 6.4513e-02, 3.6611e-07, 4.0308e-02,
        3.7995e-02, 1.3560e-01, 1.7093e-07, 1.6386e-07, 1.8017e-02, 6.4854e-02,
        2.9452e-02, 4.4207e-02, 6.6338e-02, 9.1169e-08, 4.2541e-02, 1.5350e-02,
        6.1680e-02, 9.9421e-08, 3.2392e-02, 2.6114e-07, 7.2429e-02, 9.9829e-03,
        5.7630e-02, 6.0583e-02, 5.1863e-02, 7.9448e-08, 3.3486e-02, 3.6700e-08,
        1.5411e-07, 4.7703e-02, 7.6544e-02, 7.9326e-02, 2.0149e-02, 8.5075e-08,
        1.7219e-07, 3.9168e-02, 5.2743e-02, 5.5010e-02, 1.3629e-02, 6.9706e-02,
        2.0072e-01, 8.2243e-08, 2.8040e-02, 2.0175e-01, 3.8448e-02, 6.4774e-02,
        1.7278e-01, 5.8218e-02, 1.4871e-02, 7.3758e-08, 3.5256e-02, 3.6210e-07,
        8.9955e-03, 3.0541e-02, 1.7550e-07, 3.1838e-02, 3.6713e-02, 1.7946e-01,
        1.3878e-07, 7.2974e-02, 1.2353e-02, 1.6759e-02, 6.0715e-02, 1.2510e-07,
        1.9925e-07, 3.4393e-02, 3.1789e-02, 3.9089e-02, 3.9947e-02, 1.1641e-07,
        3.5389e-07, 3.1009e-07, 7.1812e-02, 5.8201e-02, 5.1038e-02, 2.7514e-02,
        1.1443e-07, 3.2378e-08, 5.0206e-02, 4.3824e-02, 3.1009e-07, 9.1765e-03,
        6.6420e-02, 3.0945e-02, 1.0967e-02, 3.8628e-02, 5.6281e-02, 6.4547e-02,
        3.7445e-08, 5.2164e-02, 9.9884e-08, 7.9457e-02, 4.6551e-02, 9.6620e-02,
        2.2059e-02, 2.6461e-07, 5.7654e-02, 6.0518e-02, 5.5356e-02, 2.4169e-07,
        4.9269e-02, 6.5407e-02, 3.0541e-02, 7.7930e-03, 5.9891e-02, 4.7120e-02,
        7.9208e-02, 3.1423e-02, 6.9495e-08, 8.9679e-02, 1.6967e-01, 7.9118e-02,
        2.2541e-02, 4.2568e-07, 8.1383e-08, 5.5037e-02, 4.4661e-02, 2.5622e-02,
        5.4297e-02, 6.5377e-02, 3.6700e-08, 3.3084e-02, 3.8786e-02, 1.1960e-02,
        6.0511e-02, 2.2917e-07, 2.7274e-02, 3.6886e-02, 4.8953e-02, 5.2148e-02,
        7.4445e-08, 4.4411e-02, 1.1675e-02, 5.0348e-02, 1.9021e-01, 9.1812e-08,
        3.1123e-02, 1.7550e-07, 2.9479e-02, 8.7618e-08, 1.9328e-02, 5.5680e-02,
        6.6280e-08, 6.6529e-02, 7.7111e-02, 5.1342e-02, 7.0111e-02, 2.2917e-07,
        5.6254e-02, 5.7378e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.8702e-01, 4.4570e-02, 1.4754e-01, 4.2458e-02, 1.0482e-06, 1.1264e-06,
        7.3671e-07, 5.5199e-02, 5.1731e-07, 8.1902e-07, 1.5149e-01, 2.2142e-06,
        1.1073e-01, 1.6943e-06, 7.9479e-07, 2.7361e-07, 3.9485e-07, 8.5786e-07,
        6.6720e-07, 1.2372e-01, 2.3546e-07, 4.3314e-07, 5.5362e-02, 8.1129e-07,
        1.5751e-06, 1.0645e-01, 1.1124e-06, 1.3393e-06, 9.2649e-02, 1.4611e-01,
        8.1488e-07, 8.2496e-07, 6.5134e-07, 6.8842e-07, 7.5700e-07, 8.5059e-02,
        2.4135e-06, 8.2496e-07, 1.9514e-06, 5.1731e-07, 2.0963e-06, 7.0098e-07,
        4.1408e-02, 1.6315e-06, 3.5375e-07, 9.8616e-02, 5.7911e-07, 1.0162e-06,
        9.1143e-07, 1.0497e-01, 5.0322e-07, 7.3671e-07, 3.9909e-07, 5.2884e-02,
        1.6761e-01, 7.9226e-07, 1.0647e-06, 7.7916e-07, 1.6179e-01, 2.0943e-07,
        4.2211e-02, 8.7039e-02, 8.9072e-07, 1.0647e-06, 1.1538e-01, 1.3127e-01,
        4.6328e-02, 1.4404e-06, 1.3295e-06, 1.3268e-06, 8.9326e-07, 1.1264e-06,
        8.3686e-02, 4.0389e-07, 4.7710e-02, 9.0718e-02, 1.4020e-06, 8.2496e-07,
        8.9651e-07, 5.4185e-02, 9.1117e-07, 1.5641e-01, 1.3652e-06, 4.5152e-02,
        1.1641e-01, 1.7938e-06, 1.9625e-06, 9.7501e-07, 8.0490e-07, 5.3650e-07,
        9.9354e-02, 1.3297e-06, 1.0476e-01, 6.8842e-07, 7.7348e-07, 7.7348e-07,
        1.9833e-01, 9.7501e-07, 1.1626e-01, 6.8842e-07, 1.6717e-01, 5.1989e-02,
        7.6782e-07, 1.1016e-01, 6.3138e-07, 1.1910e-06, 2.2938e-07, 3.9649e-02,
        5.7843e-07, 1.4727e-06, 8.7140e-02, 1.7211e-06, 6.5133e-07, 3.0028e-02,
        7.5700e-07, 1.1904e-06, 1.4727e-06, 1.0647e-06, 4.5140e-02, 1.4148e-06,
        4.2198e-02, 8.3501e-07, 6.6940e-07, 7.2756e-07, 1.0025e-01, 1.4233e-06,
        1.4404e-06, 9.1862e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.1979e-06, 2.7788e-06, 1.5972e-06, 1.6763e-01, 2.7490e-06, 1.0848e-06,
        1.1005e-01, 4.8411e-06, 1.0497e-01, 7.7552e-07, 1.2416e-01, 9.0655e-02,
        3.2118e-06, 4.8411e-06, 2.1159e-06, 1.5302e-06, 3.4090e-06, 1.1175e-01,
        1.0165e-01, 1.5194e-06, 8.5523e-07, 6.5890e-07, 1.7805e-06, 1.1216e-06,
        2.8094e-06, 9.1254e-07, 2.9071e-06, 1.0870e-01, 1.7027e-06, 3.1986e-06,
        1.3957e-01, 2.7237e-06, 1.1645e-01, 2.7237e-06, 6.1701e-07, 1.5972e-06,
        1.5972e-06, 2.0107e-06, 1.8096e-06, 1.6235e-06, 4.1591e-02, 3.4594e-06,
        7.9685e-07, 1.4537e-06, 1.5389e-06, 1.3466e-01, 1.5389e-06, 2.1135e-06,
        1.5157e-01, 1.1034e-01, 8.8768e-07, 1.0666e-01, 1.1960e-06, 7.1455e-07,
        1.1960e-06, 2.2158e-07, 1.0449e-01, 1.5931e-06, 1.2850e-06, 6.6794e-07,
        1.8267e-06, 2.9366e-06, 1.0848e-06, 8.1622e-07, 3.2931e-06, 1.1697e-01,
        3.1328e-02, 1.7027e-06, 2.6434e-06, 9.8509e-07, 4.6369e-06, 6.0156e-07,
        1.7177e-06, 2.1159e-06, 2.7570e-06, 1.5972e-06, 8.3757e-07, 1.5127e-06,
        1.1960e-06, 1.7610e-06, 9.3577e-02, 1.0089e-06, 1.1003e-01, 9.4735e-02,
        3.3118e-06, 1.1303e-06, 1.0841e-01, 1.4276e-01, 1.5976e-06, 1.7177e-06,
        7.7552e-07, 4.6369e-06, 8.8768e-07, 7.7552e-07, 1.2805e-06, 4.8183e-06,
        4.9309e-06, 8.3757e-07, 2.3635e-06, 4.4015e-06, 8.1954e-02, 1.1368e-01,
        6.6794e-07, 9.3521e-07, 1.2850e-06, 2.9494e-06, 1.9998e-02, 4.4596e-06,
        3.4630e-07, 5.9324e-07, 2.7788e-06, 1.5972e-06, 7.0895e-07, 1.7197e-06,
        2.9071e-06, 2.2799e-02, 1.9362e-06, 9.0835e-02, 1.1951e-01, 1.7177e-06,
        1.4647e-06, 1.0052e-01, 1.0410e-01, 3.1986e-06, 1.0848e-06, 1.1303e-06,
        3.2822e-06, 3.4594e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2042e-07, 7.0985e-09, 1.0759e-02, 5.6013e-02, 8.4176e-08, 4.1015e-02,
        1.7441e-02, 1.2902e-07, 6.4132e-03, 1.6555e-07, 2.8243e-02, 3.1958e-07,
        1.6906e-07, 5.2994e-02, 2.9192e-02, 3.5711e-02, 8.0002e-03, 1.5629e-02,
        4.0245e-02, 8.0759e-02, 4.5988e-07, 2.3726e-02, 9.9289e-03, 6.0590e-02,
        6.7863e-03, 2.1277e-07, 4.0894e-02, 3.2577e-07, 9.5604e-03, 1.3924e-07,
        1.3010e-02, 2.1518e-02, 2.6655e-07, 8.4176e-08, 9.4106e-03, 1.1774e-02,
        2.7364e-02, 2.1347e-07, 1.9660e-07, 3.3128e-02, 6.2300e-02, 6.4728e-02,
        2.4937e-02, 1.2041e-07, 6.1455e-03, 9.2264e-02, 9.7169e-08, 9.3272e-03,
        4.8266e-03, 1.7231e-02, 3.7461e-07, 5.0980e-02, 2.0069e-02, 6.6019e-02,
        1.0953e-07, 2.6840e-02, 1.3901e-01, 1.4522e-01, 3.5278e-07, 8.1068e-08,
        2.7240e-02, 1.4472e-01, 3.2298e-02, 1.7910e-02, 1.2601e-07, 4.7337e-02,
        1.4585e-02, 4.0841e-03, 1.6212e-07, 2.2834e-07, 3.0913e-02, 3.5524e-02,
        7.6909e-08, 3.2469e-02, 3.3906e-02, 2.3019e-07, 2.8293e-02, 3.6256e-02,
        1.8841e-02, 2.4400e-02, 1.3812e-02, 8.2140e-02, 4.1067e-03, 5.0291e-02,
        3.3404e-08, 1.8580e-02, 1.4345e-07, 2.7675e-07, 2.8255e-02, 3.1741e-02,
        2.3819e-02, 2.1557e-02, 1.0957e-03, 9.4398e-03, 1.3278e-02, 6.8458e-08,
        1.2237e-07, 5.9667e-02, 7.4629e-02, 2.7796e-02, 3.9857e-02, 2.1399e-02,
        4.8801e-03, 3.4343e-02, 2.8250e-02, 7.7046e-02, 1.6196e-02, 2.0834e-01,
        1.3046e-02, 1.3486e-02, 3.6131e-07, 7.0921e-08, 1.4910e-07, 2.0784e-02,
        2.2775e-07, 2.7307e-02, 6.6379e-08, 3.5714e-02, 6.1860e-02, 4.2641e-03,
        8.0138e-02, 1.1098e-07, 2.8026e-02, 9.0927e-08, 4.9261e-08, 1.4463e-07,
        1.7608e-07, 4.5141e-07, 1.2820e-07, 5.4652e-02, 1.4366e-02, 3.0678e-02,
        5.5917e-02, 5.8633e-02, 1.6551e-02, 2.1325e-07, 3.6887e-07, 3.2986e-02,
        2.4282e-02, 5.0609e-03, 1.2836e-01, 5.8985e-02, 1.5570e-01, 4.0852e-02,
        6.4102e-08, 2.3225e-02, 3.7054e-02, 4.4826e-02, 2.4732e-02, 3.4025e-07,
        3.0227e-02, 9.6185e-08, 3.5346e-07, 1.8185e-02, 2.8631e-02, 5.1578e-03,
        7.1426e-03, 3.7313e-02, 2.5808e-02, 2.3138e-02, 3.0273e-07, 4.1729e-08,
        5.4893e-02, 3.5278e-07, 3.9746e-07, 6.3748e-02, 1.7270e-02, 3.4482e-02,
        5.8683e-02, 6.5774e-08, 3.5589e-02, 8.4622e-02, 9.9988e-08, 2.9063e-02,
        7.1119e-08, 6.2771e-02, 3.0502e-02, 1.3886e-07, 2.5324e-02, 8.5725e-08,
        4.9536e-08, 8.7317e-08, 6.9473e-02, 1.0174e-02, 1.1785e-07, 2.1555e-07,
        4.2231e-02, 8.1048e-02, 5.4293e-02, 1.2046e-07, 7.9040e-08, 5.6652e-02,
        3.8062e-08, 4.2628e-02, 6.0238e-03, 1.4016e-01, 1.8759e-07, 3.9530e-02,
        1.5269e-07, 6.7796e-02, 5.8367e-02, 5.0681e-02, 1.0917e-07, 1.2754e-02,
        7.9793e-02, 1.3924e-07, 3.4866e-02, 7.1182e-08, 7.8316e-08, 1.6681e-02,
        2.5710e-02, 2.2799e-07, 3.4794e-02, 1.7649e-07, 4.0320e-02, 9.2184e-03,
        2.0447e-01, 8.5249e-08, 1.2671e-07, 7.1119e-08, 4.5143e-02, 4.9536e-08,
        2.2758e-02, 1.3967e-01, 3.0425e-02, 7.6909e-08, 1.0880e-02, 2.7995e-07,
        4.1149e-02, 6.4187e-08, 1.5626e-07, 6.4358e-02, 3.9027e-02, 3.4879e-02,
        1.2041e-07, 7.2738e-02, 1.5372e-02, 3.7657e-02, 8.3386e-03, 1.3276e-07,
        4.7833e-02, 1.8298e-02, 1.9170e-02, 2.5997e-07, 5.0719e-08, 3.7820e-02,
        1.1199e-01, 4.9316e-08, 7.9721e-03, 1.5479e-02, 5.9703e-03, 3.3913e-02,
        5.2502e-02, 2.5019e-02, 7.0055e-08, 9.5831e-03, 1.6259e-07, 4.7095e-02,
        2.1199e-02, 2.2201e-02, 2.3642e-02, 1.8471e-02, 6.5311e-02, 4.4409e-02,
        1.6579e-07, 7.3152e-03, 1.2741e-02, 3.0529e-02, 3.0784e-02, 1.7851e-01,
        5.1379e-02, 9.3528e-08, 2.2175e-02, 6.0052e-03, 6.9376e-02, 1.2977e-01,
        4.2410e-02, 7.8482e-02, 1.0001e-07, 1.5731e-02, 1.6380e-01, 3.4955e-02,
        2.9973e-02, 3.5496e-02, 1.5851e-01, 2.7859e-02, 2.8356e-02, 1.4647e-02,
        6.6817e-08, 1.5646e-07, 9.0503e-02, 4.5317e-02, 1.2396e-07, 1.1158e-02,
        6.3119e-02, 1.3345e-02, 8.3155e-02, 4.2487e-02, 1.1264e-07, 3.3143e-02,
        2.1126e-07, 7.4586e-02, 1.2194e-01, 8.2535e-08, 6.2169e-02, 7.6491e-02,
        6.0436e-08, 1.9161e-02, 9.5962e-03, 1.5608e-01, 7.1853e-02, 1.6964e-07,
        2.4049e-02, 2.2939e-02, 1.8037e-02, 1.3924e-07, 1.4857e-07, 4.7594e-03,
        2.4176e-07, 3.3940e-02, 2.7850e-02, 8.7738e-08, 1.4774e-01, 3.9766e-02,
        5.4955e-02, 1.6666e-07, 3.5235e-02, 1.7298e-07, 1.2551e-07, 1.8163e-07,
        3.5044e-07, 5.3457e-02, 4.2281e-02, 2.4835e-07, 1.6471e-07, 9.6185e-08,
        8.5281e-08, 9.8355e-08, 2.6166e-02, 6.0703e-02, 1.6115e-07, 5.2017e-08,
        4.1293e-02, 5.8049e-08, 3.9088e-02, 1.2589e-02, 1.4172e-02, 5.7827e-05,
        1.8288e-02, 4.2363e-02, 1.4747e-07, 1.6329e-07, 9.0542e-02, 2.2607e-02,
        3.8511e-02, 2.9592e-02, 6.1574e-02, 1.6808e-02, 6.2253e-08, 1.3700e-03,
        1.0857e-07, 1.7592e-01, 6.2999e-08, 3.8807e-02, 1.3335e-07, 3.3134e-02,
        1.3291e-07, 2.6116e-02, 3.5278e-07, 1.6952e-07, 1.6383e-07, 3.2389e-02,
        5.9899e-03, 1.4110e-02, 3.9545e-02, 4.9240e-02, 1.8269e-07, 1.3967e-01,
        1.9474e-02, 9.1819e-02, 1.2017e-07, 1.0989e-07, 1.4402e-07, 1.1325e-02,
        3.3116e-02, 7.0568e-02, 5.3562e-02, 1.7962e-07, 5.3884e-02, 1.2820e-07,
        3.4416e-07, 2.2458e-07, 7.8494e-02, 5.0147e-02, 3.1686e-02, 1.1568e-07,
        1.6136e-07, 1.6417e-07, 2.0592e-02, 5.2704e-02, 1.3019e-02, 1.7594e-02,
        1.2203e-02, 1.3922e-07, 6.2335e-08, 4.1366e-02, 7.1362e-08, 1.0130e-02,
        5.0552e-02, 2.2897e-02, 6.7907e-08, 3.1092e-02, 4.4251e-02, 1.3924e-07,
        3.1697e-02, 6.8749e-03, 9.9701e-08, 3.4608e-02, 5.0528e-08, 6.4515e-02,
        1.2045e-07, 1.1962e-02, 1.7415e-02, 1.3664e-07, 3.2793e-07, 2.2771e-07,
        2.2771e-07, 8.0791e-08, 1.7314e-02, 1.6692e-07, 2.7389e-02, 8.4920e-08,
        1.2045e-07, 7.1129e-09, 3.2415e-02, 3.7604e-02, 4.4932e-08, 2.3828e-07,
        2.7995e-07, 3.2202e-07, 1.1240e-02, 7.3350e-02, 2.7754e-02, 3.1330e-02,
        3.9329e-02, 2.1285e-07, 2.0283e-07, 3.0854e-02, 1.3746e-02, 4.7693e-02,
        1.2820e-07, 5.5862e-02, 2.8448e-02, 5.4066e-02, 1.4902e-02, 2.4139e-02,
        1.2669e-02, 6.0221e-08, 6.2415e-02, 4.1979e-02, 6.7988e-02, 1.1780e-07,
        8.7206e-08, 2.5031e-02, 5.6297e-02, 7.1203e-02, 2.9482e-02, 1.4884e-02,
        6.1785e-02, 3.8684e-08, 1.5989e-07, 7.0949e-02, 4.3847e-02, 6.8680e-02,
        7.7150e-08, 5.6688e-02, 2.5374e-07, 1.6531e-02, 5.0767e-02, 1.3338e-02,
        2.9032e-02, 2.4008e-02, 2.1347e-07, 2.3662e-02, 2.0753e-07, 1.8295e-02,
        3.7355e-07, 2.1347e-07, 2.7614e-02, 2.7585e-02, 3.0220e-02, 2.8204e-02,
        1.0989e-07, 2.7327e-03, 4.7506e-02, 2.9177e-02, 1.2546e-01, 8.4229e-08,
        1.8427e-02, 2.3019e-07, 2.7201e-02, 2.1877e-02, 1.2389e-02, 1.8673e-02,
        8.8364e-08, 1.4442e-02, 8.6186e-03, 1.4718e-02, 3.3602e-08, 2.5598e-07,
        2.5856e-07, 1.1480e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.5318e-02, 3.4121e-07, 1.0831e-01, 9.1529e-02, 1.9287e-07, 9.8370e-02,
        1.6862e-07, 2.2466e-07, 7.4528e-07, 9.0568e-02, 1.0979e-01, 2.3290e-07,
        1.0299e-01, 1.0226e-01, 9.5956e-02, 9.0368e-07, 6.8016e-07, 6.8016e-07,
        6.2228e-07, 1.0356e-01, 2.2466e-07, 7.4528e-07, 9.2028e-02, 1.0532e-01,
        7.4528e-07, 1.7077e-07, 2.2297e-07, 9.8225e-02, 1.7903e-07, 1.0873e-01,
        9.7214e-02, 2.8912e-07, 1.2354e-01, 4.4024e-07, 9.2776e-07, 1.9287e-07,
        9.6977e-02, 1.6862e-07, 1.1121e-01, 9.8706e-02, 1.6862e-07, 1.6862e-07,
        1.0948e-01, 1.3384e-07, 1.2584e-01, 9.5005e-02, 2.2466e-07, 1.3384e-07,
        1.1231e-01, 9.4116e-07, 9.0005e-02, 9.5704e-02, 1.0046e-06, 9.7503e-02,
        2.6885e-07, 3.1509e-07, 1.0141e-01, 5.2210e-07, 1.3384e-07, 1.0224e-01,
        1.1062e-01, 9.2032e-02, 6.6222e-07, 4.5901e-07, 1.0478e-01, 6.8016e-07,
        5.5010e-07, 1.0520e-01, 9.9127e-02, 1.2011e-01, 1.0232e-01, 1.9287e-07,
        1.2204e-06, 1.9287e-07, 2.0500e-07, 1.0399e-01, 9.2797e-02, 9.3393e-02,
        1.5340e-07, 1.6862e-07, 9.2455e-02, 3.5958e-07, 6.8016e-07, 9.4180e-07,
        9.4116e-07, 6.3276e-07, 1.2784e-01, 1.1451e-01, 6.8016e-07, 1.7077e-07,
        6.3276e-07, 9.7031e-02, 3.5196e-07, 6.2228e-07, 8.4069e-02, 7.9477e-02,
        1.6862e-07, 2.6885e-07, 6.6903e-07, 1.1228e-01, 2.3058e-07, 1.2046e-01,
        2.7874e-07, 1.0337e-01, 1.0349e-01, 1.0815e-01, 9.1609e-07, 2.2466e-07,
        1.1967e-06, 1.0474e-01, 2.0500e-07, 1.1060e-01, 9.8922e-02, 2.7911e-07,
        5.3138e-07, 1.0123e-01, 1.0779e-01, 1.9287e-07, 8.7960e-02, 2.2466e-07,
        9.4798e-02, 9.7031e-02, 8.9169e-08, 1.6862e-07, 7.4763e-07, 9.7859e-02,
        1.9287e-07, 1.0430e-01, 8.9169e-08, 3.9503e-07, 7.9062e-08, 9.5100e-02,
        1.2672e-01, 2.7080e-07, 1.9287e-07, 1.3023e-01, 1.1958e-01, 1.6862e-07,
        8.9935e-02, 8.9169e-08, 9.9154e-02, 8.7086e-02, 1.1131e-01, 1.0046e-06,
        6.6903e-07, 3.9503e-07, 9.2722e-02, 6.6903e-07, 8.8837e-02, 9.2894e-02,
        6.6903e-07, 8.9169e-08, 1.0911e-01, 1.0106e-01, 1.1101e-01, 1.1224e-01,
        7.7351e-02, 9.0351e-02, 3.5314e-07, 1.3384e-07, 1.7077e-07, 9.2153e-07,
        6.8016e-07, 9.8018e-02, 1.0112e-01, 6.8016e-07, 2.5339e-07, 9.3994e-02,
        2.2466e-07, 7.4528e-07, 2.2466e-07, 1.0397e-01, 1.1620e-01, 6.3276e-07,
        7.4528e-07, 9.8515e-02, 1.0983e-01, 6.8016e-07, 8.9169e-08, 1.2204e-06,
        3.9503e-07, 1.0345e-01, 9.5633e-02, 1.6862e-07, 9.4476e-02, 1.2204e-06,
        6.8016e-07, 2.2466e-07, 1.6862e-07, 1.1565e-01, 6.8016e-07, 8.9636e-02,
        1.0896e-01, 1.0432e-01, 1.1511e-01, 7.2591e-02, 1.1253e-01, 9.0369e-07,
        1.2138e-01, 6.6903e-07, 3.1509e-07, 1.5340e-07, 2.4954e-07, 8.9169e-08,
        1.0117e-01, 1.0663e-01, 1.3384e-07, 1.2039e-01, 1.7672e-07, 6.8016e-07,
        1.9287e-07, 6.6903e-07, 1.1214e-01, 1.0877e-01, 1.2476e-01, 1.9287e-07,
        1.0226e-01, 3.9503e-07, 1.3384e-07, 7.3159e-07, 1.1796e-01, 4.8553e-07,
        6.8016e-07, 1.6862e-07, 1.1070e-01, 1.9287e-07, 3.1509e-07, 9.3249e-02,
        1.4668e-07, 1.9287e-07, 9.6633e-02, 9.2703e-07, 2.0500e-07, 1.1002e-01,
        8.9267e-02, 1.0427e-01, 1.1008e-01, 5.5010e-07, 1.9287e-07, 1.0388e-06,
        1.6862e-07, 9.7158e-07, 8.6403e-02, 3.1509e-07, 1.2601e-01, 4.2768e-07,
        1.6862e-07, 1.0497e-01, 9.0369e-07, 1.9287e-07, 3.2897e-07, 3.9503e-07,
        1.1925e-01, 2.3058e-07, 6.8016e-07, 2.8019e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.1373e-02, 1.4946e-06, 9.5840e-07, 7.8436e-02, 9.5840e-07, 7.5983e-02,
        7.6471e-02, 1.2601e-06, 5.2535e-06, 4.2458e-06, 9.5840e-07, 1.2816e-06,
        5.2540e-06, 9.5840e-07, 1.2816e-06, 1.2816e-06, 9.1606e-02, 2.8064e-06,
        2.5432e-06, 6.0460e-02, 1.2816e-06, 3.6335e-06, 5.9073e-06, 2.7652e-06,
        6.0542e-02, 7.5942e-07, 5.9073e-06, 4.1936e-06, 2.4249e-06, 2.4249e-06,
        7.7784e-02, 1.0347e-06, 5.3931e-06, 7.9098e-02, 7.3948e-02, 9.5840e-07,
        4.5536e-06, 2.5432e-06, 2.4249e-06, 7.5773e-02, 5.2340e-06, 8.4559e-02,
        2.7652e-06, 7.4645e-02, 5.2318e-06, 4.1936e-06, 3.5186e-06, 3.6335e-06,
        2.4249e-06, 6.4358e-02, 1.9133e-06, 2.0897e-06, 4.2825e-06, 2.8216e-06,
        2.4249e-06, 2.8064e-06, 1.3088e-06, 7.5314e-02, 8.3807e-02, 2.4249e-06,
        6.5325e-02, 5.2293e-06, 3.5186e-06, 9.2240e-02, 6.8616e-02, 2.2784e-06,
        8.3315e-07, 5.9073e-06, 2.5432e-06, 7.8874e-02, 7.1065e-02, 9.5840e-07,
        8.1425e-02, 1.0899e-06, 9.5840e-07, 9.5840e-07, 2.4249e-06, 3.5186e-06,
        1.0347e-06, 3.3513e-06, 7.1134e-02, 4.5538e-06, 3.2118e-06, 2.8064e-06,
        1.2601e-06, 2.4249e-06, 7.8754e-02, 1.5982e-06, 6.6223e-02, 8.6951e-02,
        2.3150e-06, 1.4239e-06, 7.1280e-02, 7.0692e-02, 5.9073e-06, 6.6478e-02,
        1.6290e-06, 9.5840e-07, 6.6967e-02, 9.5840e-07, 9.5840e-07, 2.8216e-06,
        5.9073e-06, 2.4249e-06, 6.6622e-02, 2.8064e-06, 1.9922e-06, 2.8064e-06,
        5.9073e-06, 2.3150e-06, 7.4543e-02, 1.6900e-06, 6.6298e-02, 6.8049e-06,
        8.1370e-02, 2.8216e-06, 1.0725e-06, 2.8216e-06, 6.8348e-07, 8.3315e-07,
        7.7236e-02, 7.5942e-07, 1.7985e-06, 8.3315e-07, 1.0899e-06, 2.4249e-06,
        2.4249e-06, 2.4249e-06, 2.4249e-06, 4.1936e-06, 4.5533e-06, 3.5186e-06,
        1.7530e-06, 9.5840e-07, 1.6290e-06, 9.5840e-07, 5.9073e-06, 1.0899e-06,
        7.6227e-02, 5.9073e-06, 8.3315e-07, 7.4709e-02, 8.0268e-02, 7.4956e-02,
        3.5186e-06, 6.6992e-02, 3.3513e-06, 6.8957e-02, 2.4440e-06, 2.0897e-06,
        2.8064e-06, 6.8049e-06, 7.1239e-02, 5.1933e-06, 2.8216e-06, 8.3315e-07,
        2.0897e-06, 4.1936e-06, 8.3521e-02, 3.3882e-06, 5.9073e-06, 7.3599e-02,
        3.3513e-06, 6.7450e-02, 4.5541e-06, 1.5982e-06, 1.7565e-06, 7.0476e-02,
        2.8064e-06, 6.8049e-06, 1.4946e-06, 1.0725e-06, 2.4249e-06, 1.7866e-06,
        7.6372e-02, 1.7866e-06, 9.5840e-07, 5.1933e-06, 1.4946e-06, 3.3513e-06,
        3.3882e-06, 6.8049e-06, 9.5840e-07, 2.4249e-06, 5.3794e-06, 7.3087e-02,
        2.8216e-06, 2.8216e-06, 5.9073e-06, 1.7564e-06, 7.3330e-02, 1.2616e-06,
        9.5840e-07, 2.3150e-06, 2.4249e-06, 1.6290e-06, 6.2813e-02, 9.5840e-07,
        5.2319e-06, 5.3113e-06, 3.3513e-06, 5.9073e-06, 1.2601e-06, 7.8488e-02,
        7.5411e-02, 5.9073e-06, 5.9073e-06, 4.1936e-06, 2.4249e-06, 7.2639e-02,
        5.9073e-06, 6.9032e-02, 5.9073e-06, 2.8064e-06, 2.8216e-06, 6.8049e-06,
        1.0725e-06, 8.3315e-07, 2.8216e-06, 5.2293e-06, 1.2966e-06, 2.0390e-06,
        9.6894e-07, 2.2784e-06, 1.0725e-06, 5.9073e-06, 3.3513e-06, 6.3702e-02,
        9.5840e-07, 1.2601e-06, 1.3381e-06, 8.9482e-07, 3.5186e-06, 2.5432e-06,
        1.2601e-06, 8.1493e-02, 5.2319e-06, 8.2743e-02, 2.0897e-06, 1.2601e-06,
        1.4946e-06, 1.6290e-06, 2.8064e-06, 6.4201e-02, 3.5186e-06, 5.1931e-06,
        2.4249e-06, 3.5186e-06, 2.5432e-06, 2.8216e-06, 5.9073e-06, 2.0897e-06,
        2.4249e-06, 1.2816e-06, 3.3882e-06, 7.5942e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.5569e-07, 2.3927e-07, 3.5674e-07,  ..., 1.3749e-07, 1.4173e-01,
        2.3927e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0894e-06, 1.1533e-06, 1.1533e-06,  ..., 6.4915e-07, 3.3520e-02,
        1.1533e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.8818e-07, 2.0504e-06, 4.0344e-07, 2.7137e-02, 1.6795e-06, 2.1881e-06,
        7.5181e-07, 3.8100e-07, 1.6126e-06, 1.3448e-06, 1.2891e-06, 1.4839e-06,
        2.3701e-06, 8.1956e-07, 3.4217e-06, 6.6726e-07, 3.1815e-07, 4.3554e-07,
        3.4217e-06, 4.2350e-02, 3.1815e-07, 1.4602e-06, 3.6159e-07, 1.9107e-06,
        1.2042e-06, 1.3429e-06, 9.1635e-07, 3.4507e-06, 6.7696e-07, 3.0863e-06,
        1.5846e-06, 8.4909e-07, 1.2818e-06, 2.5191e-06, 2.1357e-07, 1.0956e-06,
        1.3165e-06, 1.4602e-06, 2.2195e-06, 5.0855e-07, 2.0449e-06, 2.3958e-06,
        1.4177e-06, 4.2868e-02, 9.7084e-07, 1.6824e-06, 1.2891e-06, 3.5187e-06,
        1.2671e-06, 3.8112e-07, 2.0504e-06, 9.2894e-07, 9.1522e-07, 7.3201e-07,
        1.0956e-06, 8.8897e-07, 9.1635e-07, 9.1521e-07, 2.1881e-06, 1.4143e-06,
        1.4541e-06, 1.7752e-06, 1.1439e-06, 2.0499e-06, 2.0504e-06, 2.1881e-06,
        1.1568e-06, 1.0956e-06, 2.3165e-06, 9.1635e-07, 1.6824e-06, 1.5607e-06,
        2.5191e-06, 2.4106e-06, 1.8288e-06, 4.2346e-02, 2.3958e-06, 7.8818e-07,
        1.0956e-06, 1.7983e-06, 5.4882e-07, 1.0561e-06, 9.1635e-07, 5.2989e-07,
        7.8818e-07, 6.8152e-07, 6.0547e-07, 7.9840e-07, 1.1439e-06, 3.8930e-07,
        1.4602e-06, 2.2024e-06, 2.3165e-06, 1.6126e-06, 3.6159e-07, 2.6582e-07,
        1.2794e-06, 9.1635e-07, 2.3371e-02, 2.1881e-06, 1.6514e-06, 4.7398e-02,
        6.4502e-07, 6.8581e-07, 5.6496e-07, 1.4177e-06, 1.3672e-06, 2.2677e-06,
        8.4094e-07, 3.7407e-06, 3.6376e-02, 2.3958e-06, 8.0555e-07, 1.4711e-07,
        4.0344e-07, 2.1881e-06, 1.4602e-06, 1.6824e-06, 3.6159e-07, 1.7752e-06,
        1.1313e-01, 1.2794e-06, 3.6159e-07, 1.3448e-06, 1.2369e-06, 1.7752e-06,
        2.9530e-06, 1.2671e-06, 4.0213e-02, 2.2677e-06, 6.8581e-07, 1.7219e-06,
        3.1815e-07, 8.5806e-07, 6.7696e-07, 1.8857e-06, 9.1635e-07, 9.0365e-07,
        1.6824e-06, 1.2199e-06, 7.8818e-07, 1.8857e-06, 2.2359e-06, 1.1323e-06,
        4.3554e-07, 3.6159e-07, 1.2887e-06, 2.2024e-06, 4.3604e-02, 1.4839e-06,
        1.6257e-06, 1.8288e-06, 2.2024e-06, 1.3079e-06, 1.6824e-06, 1.2369e-06,
        1.3448e-06, 2.3165e-06, 3.5124e-02, 2.1439e-06, 1.3672e-06, 2.0504e-06,
        1.1950e-06, 4.7117e-02, 3.1096e-02, 7.2022e-07, 2.1881e-06, 3.6159e-07,
        1.2383e-06, 4.8115e-02, 2.1881e-06, 8.2013e-07, 1.0725e-06, 9.1521e-07,
        2.3958e-06, 1.0380e-06, 2.3701e-06, 3.1815e-07, 3.6159e-07, 1.0725e-06,
        2.1881e-06, 3.3922e-06, 4.9050e-02, 2.0504e-06, 1.4108e-06, 1.2794e-06,
        1.1889e-06, 2.4106e-06, 1.1657e-06, 2.2677e-06, 1.3970e-06, 3.9640e-02,
        1.1502e-06, 3.4232e-06, 1.0406e-06, 3.0764e-07, 2.1881e-06, 3.3504e-06,
        3.1815e-07, 4.3271e-02, 1.1502e-06, 4.3554e-07, 3.3504e-06, 8.4094e-07,
        2.1000e-06, 9.1635e-07, 1.1003e-06, 7.8818e-07, 1.5831e-06, 2.3165e-06,
        3.4217e-06, 1.1889e-06, 1.1950e-06, 1.3672e-06, 2.3701e-06, 4.9798e-02,
        1.0561e-06, 1.0535e-06, 4.3947e-02, 4.0033e-02, 9.7084e-07, 3.8112e-07,
        1.3672e-06, 7.5181e-07, 1.4177e-06, 2.1881e-06, 3.1815e-07, 1.6824e-06,
        2.2677e-06, 4.4961e-02, 6.7696e-07, 2.1746e-06, 2.1881e-06, 2.5191e-06,
        3.9692e-02, 3.1815e-07, 7.5181e-07, 2.2677e-06, 1.3164e-06, 4.9656e-07,
        7.3201e-07, 2.4106e-06, 3.0863e-06, 9.1635e-07, 2.1057e-06, 6.5308e-07,
        1.5363e-02, 3.0863e-06, 4.9563e-07, 7.2022e-07, 3.1815e-07, 2.1881e-06,
        2.3165e-06, 1.7752e-06, 1.1054e-06, 2.3165e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.3812e-07, 1.3601e-06, 4.1089e-06, 4.1089e-06, 1.9157e-06, 5.9743e-06,
        4.0168e-06, 1.6954e-06, 1.8632e-06, 3.8060e-06, 4.1089e-06, 5.8724e-06,
        1.9157e-06, 2.8848e-06, 4.8557e-02, 1.3082e-06, 2.8848e-06, 3.8555e-06,
        1.2853e-06, 2.1519e-06, 3.4070e-06, 1.2853e-06, 1.0892e-06, 1.2853e-06,
        2.8848e-06, 2.7182e-06, 9.3302e-07, 2.2692e-06, 1.3792e-06, 1.4979e-06,
        4.1089e-06, 5.0076e-02, 2.9897e-06, 1.3601e-06, 3.0428e-06, 2.8704e-06,
        7.4611e-08, 3.9311e-06, 1.9741e-06, 1.8632e-06, 6.3812e-07, 9.4495e-07,
        2.8108e-06, 2.8848e-06, 4.6057e-07, 2.8848e-06, 1.8632e-06, 9.3302e-07,
        2.8848e-06, 3.2915e-06, 1.8632e-06, 9.4496e-07, 2.7984e-06, 9.4494e-07,
        1.3203e-06, 1.8632e-06, 2.1519e-06, 3.2915e-06, 2.6803e-02, 9.4494e-07,
        2.7531e-06, 5.7557e-06, 3.6964e-06, 2.1612e-06, 1.3601e-06, 4.5157e-06,
        3.4274e-06, 3.6964e-06, 5.8724e-06, 4.5157e-06, 2.9546e-06, 1.4478e-06,
        1.6954e-06, 3.6964e-06, 1.9157e-06, 2.2692e-06, 1.2853e-06, 2.7984e-06,
        6.3813e-07, 1.3792e-06, 2.1612e-06, 3.8223e-06, 3.6964e-06, 3.3846e-02,
        5.9729e-06, 1.3792e-06, 9.3302e-07, 1.3792e-06, 3.8555e-06, 1.7610e-06,
        1.3031e-06, 1.2853e-06, 2.9897e-06, 1.6879e-06, 4.1500e-02, 1.0892e-06,
        1.8632e-06, 1.6733e-06, 1.3792e-06, 5.9778e-06, 4.6053e-02, 1.3082e-06,
        3.8941e-02, 1.3792e-06, 1.3792e-06, 2.9897e-06, 1.4979e-06, 3.1797e-02,
        3.8555e-06, 8.6255e-07, 2.5495e-06, 2.7531e-06, 1.2853e-06, 3.1700e-06,
        4.1089e-06, 8.6255e-07, 1.3601e-06, 5.6209e-06, 2.9897e-06, 1.3082e-06,
        1.4465e-06, 6.3733e-07, 2.0460e-06, 5.7848e-06, 1.0281e-06, 4.1401e-07,
        1.6954e-06, 1.2853e-06, 1.5350e-06, 1.8481e-06, 3.8555e-06, 1.3601e-06,
        3.2915e-06, 2.8013e-06, 3.6754e-06, 1.3031e-06, 2.5476e-06, 2.8848e-06,
        5.7755e-02, 2.2692e-06, 9.3302e-07, 1.3601e-06, 9.3302e-07, 2.5476e-06,
        2.8848e-06, 1.9741e-06, 2.0286e-06, 2.2692e-06, 3.1801e-06, 2.9897e-06,
        9.3302e-07, 1.4979e-06, 5.1777e-06, 1.2853e-06, 2.7341e-06, 1.3792e-06,
        5.1650e-02, 2.8768e-06, 3.8555e-06, 3.8555e-06, 4.4172e-02, 9.0202e-07,
        4.1089e-06, 1.3203e-06, 2.9897e-06, 3.9480e-08, 5.2385e-02, 9.0069e-07,
        3.1801e-06, 1.8632e-06, 9.4496e-07, 1.3601e-06, 1.3792e-06, 1.3165e-06,
        1.9741e-06, 2.8013e-06, 2.1519e-06, 3.4274e-06, 3.6964e-06, 1.3166e-06,
        4.6057e-07, 2.5495e-06, 3.1152e-06, 2.1612e-06, 1.7453e-06, 2.8848e-06,
        2.1519e-06, 2.0460e-06, 1.3601e-06, 1.2853e-06, 4.1089e-06, 2.7984e-06,
        9.4496e-07, 1.3166e-06, 1.1717e-06, 2.1519e-06, 2.1704e-06, 1.8632e-06,
        2.9897e-06, 3.7656e-06, 1.2559e-06, 3.6964e-06, 1.3069e-06, 1.3069e-06,
        2.1612e-06, 1.4979e-06, 1.6879e-06, 2.8013e-06, 4.1089e-06, 3.8555e-06,
        2.8013e-06, 3.6500e-06, 1.4979e-06, 6.5598e-07, 4.1089e-06, 2.5476e-06,
        5.4340e-02, 5.9771e-06, 5.1524e-02, 3.8555e-06, 1.9741e-06, 3.8555e-06,
        1.6954e-06, 1.9741e-06, 1.2853e-06, 1.3069e-06, 4.1089e-06, 4.0168e-06,
        1.6354e-06, 2.8162e-02, 1.3082e-06, 1.8632e-06, 4.2311e-02, 3.1390e-06,
        1.3082e-06, 2.9897e-06, 2.5476e-06, 5.0275e-02, 2.1519e-06, 3.5898e-02,
        2.3465e-06, 1.8632e-06, 2.1519e-06, 1.2853e-06, 1.3031e-06, 1.1777e-06,
        2.1519e-06, 3.1700e-06, 1.3082e-06, 2.8013e-06, 1.4543e-01, 3.6964e-06,
        1.3792e-06, 4.1920e-02, 1.3792e-06, 2.4051e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.8165e-07, 1.4460e-07, 2.6218e-07,  ..., 3.0869e-02, 3.3357e-02,
        2.0000e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.1621e-06, 1.0037e-06, 1.0698e-06, 6.4460e-07, 6.3246e-07, 9.8399e-07,
        2.0080e-06, 2.0511e-06, 7.5491e-07, 9.0109e-07, 3.9771e-07, 8.3704e-07,
        1.0037e-06, 5.5019e-07, 6.8512e-07, 9.6121e-07, 7.4880e-07, 6.5502e-07,
        1.0450e-06, 5.2429e-07, 2.8795e-06, 1.1166e-06, 1.8163e-06, 1.5667e-06,
        8.5924e-07, 2.6660e-06, 2.6660e-06, 4.3696e-02, 4.7383e-02, 2.0419e-06,
        8.9827e-07, 1.0037e-06, 4.3328e-02, 5.7197e-07, 1.5182e-06, 6.5056e-07,
        5.7196e-07, 4.0078e-07, 1.0840e-06, 8.5738e-07, 2.6660e-06, 1.2224e-06,
        7.9123e-07, 3.0414e-06, 5.2495e-02, 3.7371e-07, 6.3246e-07, 8.0678e-03,
        1.0891e-06, 5.1035e-02, 2.0502e-06, 4.0079e-07, 1.2833e-06, 4.0078e-07,
        3.0607e-06, 9.0109e-07, 2.6660e-06, 3.0897e-02, 6.9726e-07, 3.5371e-07,
        7.4880e-07, 1.5667e-06, 7.4880e-07, 3.1590e-06, 8.5738e-07, 2.7618e-06,
        2.9894e-06, 5.9870e-07, 5.4887e-02, 6.3785e-07, 4.2422e-02, 2.1622e-06,
        7.5491e-07, 1.3127e-06, 2.1622e-06, 1.0891e-06, 4.8843e-02, 6.0406e-02,
        4.6918e-02, 1.0370e-06, 9.0109e-07, 5.5383e-07, 1.4769e-06, 5.5383e-07,
        2.7624e-07, 4.0079e-07, 9.0109e-07, 9.8367e-07, 1.4568e-06, 9.9260e-03,
        4.8503e-02, 3.1610e-06, 8.5738e-07, 6.3246e-07, 1.0847e-06, 7.9123e-07,
        7.9123e-07, 9.0109e-07, 1.0891e-06, 5.3935e-02, 7.9123e-07, 2.9040e-02,
        2.0502e-06, 6.1746e-07, 6.9726e-07, 1.8334e-07, 9.7791e-07, 4.4733e-02,
        4.4400e-07, 2.0502e-06, 2.1012e-06, 1.4769e-06, 5.0196e-02, 3.7203e-02,
        3.0751e-06, 1.5726e-06, 3.9972e-02, 1.2807e-06, 6.4878e-07, 1.1166e-06,
        6.5502e-07, 3.6439e-02, 3.3132e-06, 7.9374e-07, 7.6781e-07, 1.6357e-06,
        5.6415e-02, 2.3679e-06, 9.4777e-07, 4.1423e-02, 4.6570e-02, 5.3726e-02,
        2.0419e-06, 5.2429e-07, 1.5726e-06, 6.5056e-07, 6.3246e-07, 9.8388e-07,
        1.5974e-01, 9.0109e-07, 5.7196e-07, 1.1149e-06, 1.1976e-06, 3.1618e-06,
        2.0419e-06, 2.3665e-06, 6.5056e-07, 2.3679e-06, 1.1297e-06, 6.8512e-07,
        4.6666e-02, 6.3246e-07, 6.8512e-07, 1.0472e-06, 8.6371e-07, 7.0955e-07,
        1.4790e-06, 7.6781e-07, 5.1448e-02, 1.0101e-06, 2.6660e-06, 8.5738e-07,
        8.5738e-07, 6.8512e-07, 3.3112e-02, 5.7196e-07, 2.3766e-06, 3.5317e-02,
        1.2403e-06, 7.0591e-02, 4.0078e-07, 9.1597e-07, 2.9894e-06, 1.0037e-06,
        4.5592e-02, 3.1631e-06, 1.0037e-06, 1.1551e-01, 9.8367e-07, 6.5502e-07,
        7.0955e-07, 4.4400e-07, 4.2203e-02, 6.5502e-07, 4.9059e-02, 8.5738e-07,
        4.3019e-02, 2.6660e-06, 8.5738e-07, 2.0419e-06, 2.7618e-06, 4.0078e-07,
        2.8795e-06, 2.0502e-06, 7.5633e-02, 6.6390e-02, 2.0399e-06, 1.0037e-06,
        3.7061e-02, 1.0959e-06, 6.5502e-07, 2.0399e-06, 7.4880e-07, 5.9339e-02,
        5.7196e-07, 9.1597e-07, 3.9026e-02, 3.7413e-07, 2.0399e-06, 1.4769e-06,
        3.1590e-06, 2.1622e-06, 7.5491e-07, 4.0078e-07, 3.4363e-07, 9.0109e-07,
        3.0751e-06, 2.4288e-06, 1.0698e-06, 1.4790e-06, 6.5502e-07, 6.5056e-07,
        8.8212e-07, 6.5056e-07, 2.0730e-06, 7.0955e-07, 4.0078e-07, 2.0419e-06,
        4.0078e-07, 1.2828e-06, 1.0546e-06, 2.9623e-02, 6.3349e-07, 1.1297e-06,
        3.4363e-07, 1.0101e-06, 1.4383e-01, 6.5056e-07, 6.5502e-07, 1.5044e-06,
        3.8685e-02, 5.7196e-07, 2.6660e-06, 2.0502e-06, 5.1053e-02, 4.8154e-07,
        9.1597e-07, 4.7236e-02, 4.1240e-02, 4.3800e-02, 2.0399e-06, 3.2556e-02,
        1.3117e-06, 9.8367e-07, 6.8512e-07, 2.1622e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.3499e-06, 9.0071e-06, 4.4792e-06, 3.2319e-06, 8.8354e-02, 8.5292e-02,
        2.3566e-06, 6.7502e-06, 3.6902e-06, 2.2244e-06, 3.8932e-06, 1.2002e-06,
        5.1854e-06, 5.5057e-06, 4.6450e-07, 4.6416e-06, 7.1415e-02, 2.6171e-06,
        5.7824e-06, 3.4957e-06, 7.8557e-06, 3.0319e-06, 1.0216e-06, 8.2314e-02,
        6.7502e-06, 3.1010e-06, 5.3703e-06, 7.8557e-06, 2.8373e-06, 3.9208e-06,
        8.6976e-02, 7.0019e-02, 6.4773e-06, 5.3197e-06, 5.5325e-06, 8.1635e-07,
        9.0749e-02, 6.6620e-06, 1.0621e-05, 5.5552e-06, 6.7502e-06, 1.3185e-05,
        4.4473e-06, 5.1057e-06, 7.3310e-02, 4.5322e-06, 5.1801e-06, 1.0046e-01,
        7.1655e-06, 7.9732e-06, 5.1042e-06, 5.5108e-06, 3.8371e-06, 1.0453e-01,
        5.5610e-06, 4.3538e-06, 7.2683e-06, 4.6416e-06, 1.7058e-06, 1.2150e-05,
        1.9094e-06, 3.1269e-06, 2.7157e-06, 7.9732e-06, 1.4234e-06, 5.5931e-06,
        3.3978e-06, 2.3566e-06, 7.7549e-06, 7.7402e-06, 5.8621e-06, 5.8428e-06,
        5.3703e-06, 3.0602e-06, 5.3703e-06, 9.6410e-06, 8.1020e-02, 5.7162e-06,
        4.6879e-06, 7.1165e-06, 8.3350e-02, 2.6956e-07, 7.4748e-06, 2.3067e-06,
        8.4510e-06, 4.5813e-06, 3.2787e-06, 8.8920e-06, 1.6139e-06, 1.1684e-05,
        4.5813e-06, 5.1552e-06, 2.1605e-06, 6.7397e-06, 4.1309e-06, 7.7195e-02,
        3.6611e-06, 2.7156e-06, 2.2853e-06, 7.8152e-02, 2.8696e-06, 8.2233e-06,
        2.1746e-06, 4.4473e-06, 5.3774e-06, 2.6527e-06, 3.8894e-06, 5.6842e-06,
        9.6354e-02, 3.5302e-06, 1.9655e-06, 7.9732e-06, 3.0601e-06, 2.8696e-06,
        3.0362e-06, 8.1322e-02, 2.0222e-06, 1.1177e-05, 1.4623e-05, 9.5561e-06,
        3.2801e-06, 5.3703e-06, 2.5406e-02, 8.3514e-06, 7.3246e-02, 4.6450e-07,
        5.3106e-06, 3.5737e-06, 2.1939e-06, 2.9313e-06, 7.6080e-06, 3.8371e-06,
        9.7542e-02, 3.0248e-06, 7.8557e-06, 3.2825e-06, 1.7114e-06, 8.8920e-06,
        4.6450e-07, 5.3958e-06, 3.1175e-06, 5.0512e-06, 3.5555e-06, 5.3106e-06,
        2.7835e-07, 6.0465e-06, 8.5562e-02, 2.0688e-06, 3.2225e-06, 6.2256e-06,
        4.9220e-06, 5.1854e-06, 6.3680e-06, 3.8843e-06, 5.3703e-06, 5.5326e-06,
        5.0512e-06, 1.4335e-05, 1.8610e-06, 7.2859e-06, 8.7138e-02, 1.5798e-06,
        3.2801e-06, 8.6525e-06, 6.5388e-06, 3.9790e-06, 4.4792e-06, 5.5078e-06,
        2.8373e-06, 3.7691e-06, 2.1633e-02, 6.8738e-06, 7.3613e-06, 1.6729e-06,
        3.2825e-06, 1.7300e-06, 5.7000e-06, 6.8747e-06, 8.0217e-02, 3.5050e-06,
        8.2909e-06, 4.8745e-06, 3.8244e-06, 4.6403e-06, 2.7825e-06, 3.2786e-06,
        4.6449e-07, 1.8610e-06, 5.7063e-06, 5.7824e-06, 6.8400e-02, 4.3264e-06,
        3.6630e-06, 8.1722e-06, 3.8905e-06, 3.0248e-06, 2.3722e-06, 5.5552e-06,
        4.0761e-06, 4.1112e-06, 4.5010e-06, 8.2086e-02, 3.5302e-06, 3.2801e-06,
        3.0248e-06, 7.6546e-06, 4.5150e-06, 4.5935e-06, 3.2786e-06, 2.3507e-06,
        3.3828e-06, 9.4237e-02, 1.5139e-06, 7.1165e-06, 5.1161e-06, 3.2169e-06,
        1.5235e-06, 7.9081e-06, 3.1175e-06, 4.5935e-06, 1.5798e-06, 8.0238e-06,
        2.0222e-06, 8.2233e-06, 1.9571e-06, 2.8696e-06, 7.0516e-06, 1.9499e-06,
        3.0885e-06, 2.2074e-06, 3.4717e-06, 1.7058e-06, 3.8371e-06, 3.5975e-06,
        5.5627e-06, 4.0818e-06, 5.2107e-06, 1.1836e-05, 2.0222e-06, 2.7161e-06,
        5.3958e-06, 9.1887e-06, 3.0510e-06, 2.8696e-06, 1.3020e-01, 8.2641e-02,
        8.0600e-02, 1.1177e-05, 2.6956e-07, 2.3667e-06, 2.1995e-06, 2.0794e-06,
        2.8696e-06, 8.2550e-06, 5.7005e-06, 5.1552e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([9.9599e-08, 6.8370e-08, 4.4033e-07,  ..., 1.3287e-07, 4.8705e-02,
        3.9195e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.7591e-06, 3.3303e-06, 1.9325e-06, 2.8855e-06, 2.4401e-06, 1.2249e-01,
        1.8960e-06, 1.6700e-06, 6.0898e-07, 1.1252e-06, 2.1024e-06, 1.4997e-06,
        9.9197e-02, 3.0412e-06, 1.4530e-06, 1.2951e-06, 1.0212e-06, 6.1406e-07,
        1.3360e-06, 4.7189e-06, 1.2294e-06, 1.7524e-06, 2.2346e-06, 4.0629e-06,
        1.6582e-06, 2.6139e-06, 1.0896e-06, 9.1857e-02, 1.2951e-06, 1.0444e-01,
        1.1673e-01, 1.2377e-06, 1.3060e-06, 2.8751e-06, 2.1745e-06, 1.6201e-06,
        2.8855e-06, 9.3549e-02, 6.1406e-07, 1.8176e-06, 4.0494e-06, 1.0905e-01,
        1.1559e-01, 2.9266e-06, 1.0203e-06, 2.8339e-06, 2.3421e-06, 2.4026e-06,
        3.2868e-06, 1.7504e-06, 1.3318e-06, 1.0212e-06, 1.3690e-06, 4.1986e-06,
        1.0339e-01, 1.0365e-06, 2.2576e-06, 1.4030e-01, 1.3090e-06, 8.0267e-07,
        3.2350e-06, 1.2688e-01, 1.0310e-01, 1.4585e-06, 2.0432e-06, 2.4971e-06,
        2.1923e-06, 8.0059e-07, 1.2234e-01, 3.4972e-06, 1.4328e-01, 2.1024e-06,
        1.1435e-06, 1.9191e-06, 3.8258e-07, 1.4585e-06, 1.3060e-06, 2.4256e-06,
        1.0212e-06, 2.6212e-06, 1.3084e-06, 3.1374e-06, 1.0043e-06, 9.9050e-07,
        9.5877e-02, 1.2405e-01, 1.0212e-06, 9.5454e-07, 1.2377e-06, 9.4952e-02,
        1.7371e-06, 9.3610e-07, 9.9888e-02, 1.2737e-06, 2.4775e-06, 2.5641e-06,
        2.3348e-06, 9.4602e-07, 3.9279e-06, 3.5973e-06, 2.0564e-06, 1.4681e-06,
        9.3928e-02, 1.3223e-06, 1.7310e-06, 9.4826e-02, 1.9397e-06, 2.8855e-06,
        1.1652e-01, 3.1741e-06, 1.2315e-01, 5.6172e-06, 4.3591e-07, 2.3154e-06,
        1.3459e-01, 1.9191e-06, 2.7442e-07, 1.3153e-01, 1.1549e-01, 9.7749e-02,
        1.0047e-06, 1.5510e-01, 2.2738e-06, 1.2377e-06, 3.0704e-07, 3.0762e-06,
        2.3998e-06, 1.4145e-06, 1.2377e-06, 2.8855e-06, 3.2871e-06, 4.3904e-06,
        1.7276e-06, 1.3212e-01, 1.3476e-06, 3.0082e-06, 6.4987e-06, 1.5377e-06,
        2.6071e-06, 1.9397e-06, 2.1917e-06, 2.9629e-06, 5.4078e-06, 9.4602e-07,
        1.6582e-06, 2.3154e-06, 1.2023e-01, 1.8176e-06, 1.5990e-06, 3.0139e-06,
        2.9412e-06, 1.4653e-06, 4.7899e-06, 1.1287e-06, 9.0681e-02, 1.7371e-06,
        1.0786e-01, 1.4339e-06, 1.2377e-06, 1.4894e-06, 2.0203e-06, 9.4602e-07,
        1.3309e-01, 1.4894e-06, 1.6806e-06, 1.3060e-06, 9.5826e-02, 2.0203e-06,
        2.1024e-06, 1.2377e-06, 3.0696e-06, 4.7754e-06, 2.8855e-06, 8.0231e-07,
        3.2010e-06, 1.0614e-01, 2.8855e-06, 4.7350e-06, 1.9807e-06, 2.3154e-06,
        1.0318e-01, 2.1917e-06, 2.6071e-06, 1.2212e-01, 3.8858e-06, 1.6547e-06,
        2.9155e-06, 1.2592e-06, 1.0220e-06, 9.8116e-07, 1.1791e-01, 4.4032e-07,
        1.0823e-01, 9.1799e-07, 1.8741e-06, 2.3985e-06, 1.0155e-06, 2.1024e-06,
        8.0766e-07, 1.4681e-06, 1.9807e-06, 5.4438e-06, 1.9807e-06, 1.9397e-06,
        1.9807e-06, 1.2592e-06, 1.4833e-06, 2.2576e-06, 1.5925e-01, 4.7754e-06,
        1.4894e-06, 2.0564e-06, 3.2809e-06, 1.1280e-06, 6.1406e-07, 2.7661e-06,
        2.0720e-06, 2.4395e-06, 3.2350e-06, 2.5641e-06, 3.4376e-06, 3.0696e-06,
        6.1406e-07, 1.9856e-06, 4.2306e-07, 9.5454e-07, 2.8632e-06, 1.1189e-01,
        2.1024e-06, 1.6396e-06, 2.6343e-06, 1.0164e-06, 2.7312e-06, 1.9237e-06,
        3.2871e-06, 3.2871e-06, 1.0104e-01, 3.1539e-06, 1.1252e-06, 3.2871e-06,
        3.8670e-06, 1.6639e-06, 7.7122e-07, 1.6547e-06, 6.4987e-06, 1.0203e-06,
        1.1287e-06, 2.4523e-06, 9.4894e-07, 1.3188e-06, 1.1783e-01, 1.1170e-01,
        2.7873e-06, 6.5203e-07, 6.1406e-07, 2.2287e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0546e-05, 1.2765e-05, 1.4891e-06, 6.6566e-06, 1.1433e-05, 4.2064e-06,
        4.3660e-06, 7.2206e-02, 6.5908e-06, 5.4439e-06, 3.7429e-06, 3.5190e-06,
        6.1212e-06, 5.6655e-06, 6.7010e-02, 7.4191e-06, 4.8915e-06, 6.1601e-06,
        6.6816e-06, 2.1909e-06, 1.1538e-06, 4.2064e-06, 1.3667e-06, 5.5965e-06,
        1.2519e-06, 7.9665e-06, 5.6897e-06, 8.5757e-02, 9.5524e-02, 8.0880e-07,
        1.6487e-06, 1.1246e-05, 7.7598e-06, 4.5464e-06, 8.6314e-02, 8.1131e-06,
        8.7281e-06, 6.1800e-02, 3.3396e-06, 3.6787e-06, 8.0229e-02, 7.3392e-06,
        5.3420e-06, 4.5683e-06, 6.5568e-02, 1.5273e-06, 5.8069e-06, 2.4288e-06,
        8.8737e-06, 8.4325e-06, 1.1662e-05, 7.6728e-06, 3.4567e-06, 1.1206e-05,
        2.6740e-06, 1.9291e-05, 4.0244e-06, 6.6069e-06, 4.9460e-06, 6.2795e-06,
        3.0499e-06, 4.8593e-06, 4.8593e-06, 5.3847e-06, 1.0129e-05, 6.4135e-06,
        7.7597e-06, 1.8462e-06, 5.6898e-06, 1.0065e-05, 7.9657e-02, 6.8550e-02,
        4.7138e-06, 4.1136e-06, 7.4191e-06, 9.0391e-06, 6.1454e-06, 1.9793e-05,
        6.9986e-06, 6.6567e-06, 9.8948e-06, 3.4079e-06, 6.4873e-02, 1.0129e-05,
        1.4423e-06, 4.7502e-06, 5.5255e-06, 2.1597e-06, 9.2187e-06, 1.2480e-06,
        8.0861e-06, 4.8038e-06, 5.4557e-06, 3.1741e-06, 7.7588e-06, 4.4722e-06,
        7.5471e-02, 3.1001e-06, 9.4076e-06, 8.1657e-06, 6.1454e-06, 9.5708e-06,
        4.0842e-06, 7.4925e-06, 1.0386e-05, 7.4925e-06, 3.1431e-06, 3.7349e-06,
        2.7403e-06, 6.4839e-06, 2.2568e-06, 1.9793e-05, 3.4567e-06, 8.5380e-06,
        6.3242e-06, 1.3667e-06, 6.0311e-06, 6.7030e-02, 6.6809e-06, 5.2091e-06,
        5.1748e-06, 1.5015e-05, 2.4468e-06, 5.1708e-06, 3.4871e-06, 7.6777e-02,
        8.2531e-06, 5.0524e-06, 5.2695e-06, 5.3420e-06, 6.2939e-06, 1.2969e-05,
        4.4805e-06, 6.3414e-06, 2.8388e-06, 9.1726e-06, 1.3667e-06, 9.1772e-06,
        7.1395e-06, 5.3847e-06, 7.3463e-06, 1.3667e-06, 4.4805e-06, 3.1087e-06,
        6.0769e-06, 5.8022e-06, 6.0127e-06, 4.9778e-06, 8.9001e-02, 8.2032e-06,
        5.2456e-02, 8.6219e-02, 6.6457e-06, 6.0303e-06, 8.4917e-06, 5.7245e-02,
        4.7502e-06, 3.8832e-06, 1.9793e-05, 8.2336e-06, 4.7710e-06, 1.9294e-05,
        6.2224e-06, 8.2108e-06, 2.3414e-06, 6.2939e-06, 7.8604e-06, 9.4596e-06,
        7.4625e-02, 1.0621e-05, 9.9527e-06, 2.5257e-06, 1.0440e-05, 4.8392e-06,
        7.3962e-06, 9.1047e-06, 6.0807e-02, 8.3422e-06, 9.9347e-06, 1.1433e-05,
        1.9614e-05, 2.5561e-06, 5.0034e-06, 4.9778e-06, 7.5789e-06, 7.1255e-06,
        1.6107e-05, 7.3463e-06, 5.1058e-06, 1.3778e-05, 8.1637e-02, 6.7932e-06,
        1.3790e-05, 9.3525e-06, 4.4933e-06, 3.5491e-06, 9.7816e-02, 5.1183e-06,
        7.1524e-02, 6.1524e-06, 1.1740e-05, 6.4135e-06, 4.4997e-06, 9.2577e-06,
        7.9641e-06, 7.5698e-06, 8.3490e-06, 1.4423e-06, 1.0030e-05, 7.4714e-06,
        2.6386e-06, 6.8411e-02, 9.1726e-06, 6.1489e-06, 9.4912e-06, 4.2995e-06,
        1.3872e-05, 7.8604e-06, 1.4893e-05, 1.3667e-06, 6.8487e-06, 1.4423e-06,
        7.9521e-06, 1.3094e-05, 2.7346e-06, 8.8057e-02, 4.4135e-06, 8.4917e-06,
        5.2927e-06, 5.9052e-06, 2.5360e-06, 5.3847e-06, 9.9338e-06, 5.1183e-06,
        8.2983e-07, 8.7633e-02, 8.8640e-06, 7.6170e-02, 5.0941e-06, 5.8791e-06,
        8.8640e-06, 4.5079e-06, 7.0038e-02, 9.4931e-06, 1.0112e-05, 3.9424e-06,
        4.8127e-06, 6.6457e-06, 9.9047e-06, 3.9114e-06, 9.3525e-06, 5.3779e-06,
        7.4366e-06, 6.2109e-06, 4.8127e-06, 7.8604e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.4073e-07, 1.0262e-01, 3.6183e-07,  ..., 3.3438e-07, 1.1519e-06,
        3.0561e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.5325e-06, 3.2960e-06, 1.9073e-06, 7.4341e-07, 3.2960e-06, 2.6177e-06,
        2.5942e-06, 1.3660e-06, 1.9073e-06, 1.1666e-06, 1.4170e-06, 8.9723e-07,
        3.1201e-06, 2.9985e-06, 1.6960e-06, 3.0146e-06, 2.9771e-06, 1.6862e-06,
        2.1613e-06, 2.2313e-06, 1.5810e-06, 4.1158e-06, 1.9524e-06, 2.5283e-06,
        5.9288e-07, 3.9821e-06, 2.6067e-06, 8.7261e-07, 2.3851e-06, 3.9821e-06,
        6.8840e-07, 2.6067e-06, 2.4491e-06, 1.6862e-06, 2.3452e-06, 2.1941e-06,
        6.2180e-07, 5.0820e-07, 2.0428e-06, 4.1158e-06, 2.0428e-06, 1.0709e-06,
        2.0637e-06, 3.5298e-06, 7.2452e-07, 4.1158e-06, 2.2313e-06, 6.3429e-07,
        2.5942e-06, 3.3745e-07, 3.8304e-06, 1.5603e-06, 3.8304e-06, 1.4981e-06,
        1.5247e-06, 4.4303e-06, 1.0033e-06, 8.3391e-07, 1.5114e-06, 2.8580e-06,
        1.8074e-06, 3.8807e-06, 1.9962e-07, 1.3660e-06, 1.1806e-06, 1.2118e-06,
        2.5805e-06, 2.0428e-06, 9.7474e-07, 1.6862e-06, 1.4177e-06, 1.9524e-06,
        4.3764e-06, 1.5546e-06, 2.3464e-06, 2.0637e-06, 3.0870e-06, 1.0836e-06,
        5.7698e-06, 1.8074e-06, 1.4414e-06, 4.9845e-06, 1.9598e-06, 2.6658e-06,
        8.7261e-07, 3.8304e-06, 1.3660e-06, 8.4941e-07, 2.0222e-06, 1.5247e-06,
        1.6316e-06, 1.9073e-06, 3.8304e-06, 1.2349e-06, 7.2452e-07, 3.3712e-06,
        2.4284e-06, 1.6648e-06, 1.9820e-06, 3.3712e-06, 1.5114e-06, 8.4614e-07,
        1.7193e-06, 6.3429e-07, 5.7697e-06, 2.6780e-06, 6.0725e-07, 9.6289e-07,
        1.0033e-06, 1.8781e-06, 1.3990e-06, 1.9524e-06, 4.0923e-06, 1.4177e-06,
        2.1249e-06, 1.7193e-06, 2.5805e-06, 7.2452e-07, 1.4414e-06, 2.0428e-06,
        1.6862e-06, 8.6413e-07, 1.5546e-06, 4.1158e-06, 1.7154e-06, 9.6289e-07,
        2.0979e-06, 2.9396e-06, 5.7389e-07, 6.3429e-07, 4.4890e-07, 3.3712e-06,
        1.4170e-06, 1.5603e-06, 1.7083e-06, 6.8840e-07, 4.1158e-06, 1.6943e-06,
        1.5247e-06, 6.2929e-07, 5.9288e-07, 1.6006e-06, 5.9288e-07, 1.8027e-06,
        2.0428e-06, 2.0241e-06, 3.1688e-06, 1.8817e-06, 2.0428e-06, 1.9073e-06,
        9.6289e-07, 8.9735e-07, 1.8027e-06, 2.9134e-06, 1.5247e-06, 1.0640e-06,
        1.3660e-06, 1.6083e-06, 3.0887e-06, 5.9288e-07, 1.7291e-07, 4.2500e-06,
        1.2171e-06, 1.9073e-06, 2.9134e-06, 2.0979e-06, 2.1087e-06, 2.0979e-06,
        5.7697e-06, 2.8391e-06, 3.0146e-06, 4.4280e-06, 4.8176e-07, 3.3712e-06,
        1.6960e-06, 2.0570e-06, 5.6250e-07, 8.3804e-07, 6.7047e-07, 7.5207e-07,
        2.5382e-06, 8.0832e-07, 9.6289e-07, 7.2452e-07, 3.9821e-06, 3.3712e-06,
        1.5063e-06, 2.3464e-06, 1.6862e-06, 2.5382e-06, 2.0979e-06, 1.0616e-06,
        2.2313e-06, 1.8755e-06, 3.9821e-06, 2.6658e-06, 8.9735e-07, 5.9288e-07,
        7.4341e-07, 1.9073e-06, 1.6941e-06, 2.2313e-06, 9.6289e-07, 4.4563e-06,
        1.1854e-06, 1.6316e-06, 1.2499e-06, 1.4413e-06, 9.0766e-07, 5.9288e-07,
        6.3429e-07, 1.2782e-06, 1.8636e-06, 1.5546e-06, 5.7697e-06, 1.2161e-06,
        5.9288e-07, 2.4177e-06, 5.3541e-07, 4.1158e-06, 6.5023e-07, 1.6862e-06,
        2.4500e-06, 6.0725e-07, 1.8027e-06, 2.7655e-06, 2.2313e-06, 1.5114e-06,
        1.2118e-06, 3.7767e-06, 3.3712e-06, 3.8807e-06, 8.7261e-07, 2.2313e-06,
        2.6417e-06, 2.9134e-06, 2.2341e-06, 2.4177e-06, 4.0923e-06, 5.9288e-07,
        5.9288e-07, 2.4088e-06, 1.6960e-06, 1.4688e-06, 8.1001e-07, 2.3178e-06,
        9.6289e-07, 1.0033e-06, 1.1278e-06, 1.4001e-06, 2.4177e-06, 4.0923e-06,
        2.7171e-06, 4.6062e-06, 1.3990e-06, 2.6177e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.8407e-06, 2.7050e-07, 9.7127e-06, 9.7127e-06, 2.6622e-06, 2.7660e-06,
        6.9528e-06, 8.2425e-06, 7.2225e-06, 2.7536e-06, 8.7895e-07, 7.9004e-06,
        6.1313e-06, 7.9958e-06, 2.1810e-06, 2.0102e-06, 4.1470e-06, 6.7189e-06,
        3.5352e-06, 2.1892e-06, 1.3246e-06, 4.6146e-06, 1.4085e-06, 5.5287e-06,
        4.3300e-06, 3.2610e-06, 4.4209e-06, 3.4350e-06, 1.0815e-05, 4.9887e-06,
        4.9273e-06, 1.1111e-05, 2.8529e-06, 3.9559e-06, 3.0421e-06, 5.3121e-06,
        3.6420e-06, 3.8147e-06, 3.8147e-06, 4.6452e-06, 3.9550e-06, 1.0155e-06,
        2.5193e-06, 7.2423e-06, 1.4652e-06, 2.1540e-06, 5.2034e-06, 3.0421e-06,
        1.8501e-06, 4.8899e-06, 5.4757e-06, 1.8499e-07, 5.6506e-06, 1.8193e-06,
        1.1111e-05, 3.4524e-06, 2.7499e-06, 4.1130e-06, 3.9559e-06, 1.9929e-06,
        5.1525e-06, 2.9142e-06, 2.3259e-06, 3.0421e-06, 3.1956e-06, 2.7872e-06,
        2.8529e-06, 9.2220e-06, 7.1332e-06, 6.7108e-06, 3.7791e-06, 5.6233e-06,
        4.4534e-06, 7.8096e-06, 4.4209e-06, 4.4888e-06, 5.4646e-06, 1.8455e-06,
        1.1111e-05, 7.4430e-06, 3.0582e-06, 4.1538e-06, 4.5666e-06, 5.5640e-06,
        6.2993e-06, 2.1892e-06, 4.6096e-06, 4.3720e-06, 2.7221e-06, 5.0539e-06,
        3.7069e-06, 1.0155e-06, 4.6146e-06, 3.2223e-06, 2.2650e-06, 4.5666e-06,
        3.8044e-06, 4.8804e-06, 2.1892e-06, 4.5022e-06, 1.5814e-05, 9.2220e-06,
        4.1525e-06, 6.0691e-06, 8.1746e-06, 5.5577e-06, 7.8096e-06, 1.8193e-06,
        9.9247e-06, 4.3686e-06, 5.2059e-06, 6.0691e-06, 5.1811e-06, 7.5837e-06,
        8.8407e-06, 2.3579e-06, 2.2204e-06, 4.1393e-06, 4.3720e-06, 1.9929e-06,
        4.5032e-06, 1.2451e-05, 2.4024e-06, 8.6018e-06, 3.8707e-06, 4.9518e-06,
        8.7109e-06, 4.6561e-07, 3.1997e-06, 4.1621e-06, 8.2425e-06, 3.0705e-06,
        4.6080e-06, 2.9918e-06, 2.3460e-06, 3.0378e-06, 8.3399e-06, 2.8529e-06,
        8.4515e-06, 2.1880e-06, 6.5732e-06, 5.5287e-06, 5.3367e-06, 4.0523e-06,
        2.1395e-07, 4.2137e-06, 3.8749e-06, 6.0110e-06, 7.4895e-06, 4.0316e-06,
        3.2476e-06, 3.0031e-06, 1.4154e-06, 7.8616e-06, 4.6452e-06, 3.8147e-06,
        6.5668e-06, 5.7106e-06, 2.1892e-06, 2.8214e-06, 3.2610e-06, 6.0234e-06,
        4.3256e-06, 4.5666e-06, 1.8501e-06, 7.2225e-06, 2.2822e-06, 3.0421e-06,
        1.4154e-06, 4.8627e-06, 3.6216e-06, 4.6581e-06, 7.5837e-06, 2.7000e-06,
        6.9601e-06, 1.3451e-05, 4.6146e-06, 4.8380e-06, 1.2463e-05, 2.3527e-06,
        4.6561e-07, 1.0919e-05, 4.2422e-06, 3.4650e-06, 1.2451e-05, 4.4122e-06,
        7.8616e-06, 3.7734e-06, 6.9528e-06, 2.8529e-06, 2.8529e-06, 1.0155e-06,
        8.1778e-06, 1.5763e-05, 4.5666e-06, 5.3387e-06, 3.2871e-06, 4.3285e-06,
        3.1984e-06, 4.0684e-06, 1.5763e-05, 2.3579e-06, 4.8252e-06, 2.5387e-06,
        3.8924e-06, 4.1525e-06, 2.8214e-06, 3.4524e-06, 5.8275e-06, 4.6561e-07,
        7.5837e-06, 2.3460e-06, 1.6081e-06, 2.2020e-06, 4.5666e-06, 2.6021e-06,
        3.2610e-06, 4.9470e-06, 8.7895e-07, 1.4533e-05, 2.6370e-06, 7.5837e-06,
        1.9155e-06, 4.0397e-06, 2.2020e-06, 1.1355e-06, 2.6567e-06, 1.3068e-06,
        1.1282e-06, 5.4757e-06, 5.0860e-06, 3.0705e-06, 4.2332e-06, 4.5666e-06,
        1.4154e-06, 4.8676e-06, 1.3427e-05, 3.7129e-06, 8.8407e-06, 4.8899e-06,
        6.0110e-06, 7.5837e-06, 2.9454e-06, 4.4090e-06, 4.2864e-06, 5.3367e-06,
        1.9551e-06, 5.0211e-06, 4.1180e-06, 8.8407e-06, 4.3256e-06, 2.2936e-06,
        4.4090e-06, 2.7231e-06, 2.7693e-06, 4.5849e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.0054e-07, 4.7599e-07, 1.7685e-07,  ..., 6.2523e-07, 7.0918e-07,
        1.7685e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.0475e-06, 3.4461e-06, 3.3396e-06, 1.4738e-06, 1.7931e-06, 1.3773e-06,
        3.5662e-06, 4.9785e-07, 1.6867e-06, 9.2456e-07, 1.4663e-06, 3.7719e-06,
        1.3192e-06, 1.2764e-06, 1.4283e-06, 3.4461e-06, 1.4211e-06, 1.9379e-06,
        8.5841e-07, 1.6867e-06, 1.2764e-06, 7.9282e-07, 4.9785e-07, 3.0240e-06,
        2.7168e-06, 1.5286e-06, 1.6890e-07, 2.2379e-06, 1.3993e-06, 1.9379e-06,
        2.0721e-06, 6.9781e-07, 7.5711e-07, 2.8778e-06, 1.6299e-06, 1.1571e-06,
        1.1650e-06, 1.1217e-06, 3.1345e-06, 6.3699e-07, 6.9678e-07, 1.8201e-06,
        1.3245e-06, 9.7813e-07, 1.4169e-06, 2.1294e-06, 1.3613e-06, 2.9436e-07,
        7.4748e-07, 2.7168e-06, 2.9077e-06, 2.8778e-06, 2.8778e-06, 1.9379e-06,
        5.3037e-06, 1.4169e-06, 1.3209e-06, 7.5711e-07, 1.5015e-06, 4.5848e-07,
        1.2330e-06, 5.9930e-07, 2.8778e-06, 3.5465e-06, 2.8497e-07, 1.4211e-06,
        4.9785e-07, 1.7645e-06, 1.9379e-06, 1.4169e-06, 2.9436e-07, 1.0529e-06,
        7.9282e-07, 1.6299e-06, 1.5695e-06, 1.4483e-06, 1.4738e-06, 1.1217e-06,
        1.3192e-06, 2.0408e-06, 2.3222e-06, 1.1217e-06, 6.9781e-07, 9.2456e-07,
        4.5918e-07, 1.9379e-06, 9.6825e-07, 1.4494e-06, 6.9678e-07, 6.9678e-07,
        1.6405e-06, 9.9123e-07, 1.0528e-06, 1.4169e-06, 8.4134e-07, 2.6761e-06,
        1.4209e-06, 1.0577e-06, 3.4461e-06, 1.0194e-06, 4.9869e-07, 1.6771e-06,
        1.1217e-06, 1.4952e-06, 2.6761e-06, 1.4211e-06, 1.3245e-06, 1.6890e-07,
        9.7813e-07, 1.2155e-06, 2.7150e-07, 9.3064e-07, 1.8357e-06, 1.6867e-06,
        1.3773e-06, 1.5695e-06, 4.5918e-07, 7.7503e-07, 1.7931e-06, 7.9282e-07,
        9.7813e-07, 1.8047e-06, 2.0408e-06, 1.7195e-06, 1.6299e-06, 4.2752e-06,
        9.0511e-07, 1.4494e-06, 8.5841e-07, 1.3245e-06, 1.2330e-06, 1.3245e-06,
        1.3097e-06, 1.4169e-06, 1.0987e-06, 7.2576e-07, 1.5421e-06, 1.9090e-06,
        2.4339e-06, 8.5841e-07, 8.8558e-07, 2.8778e-06, 1.0528e-06, 1.5015e-06,
        2.4912e-06, 4.9785e-07, 1.4169e-06, 2.4339e-06, 7.5711e-07, 6.3699e-07,
        1.1015e-06, 3.5679e-06, 1.2155e-06, 3.5465e-06, 1.1835e-06, 1.7622e-06,
        9.7813e-07, 1.1217e-06, 8.7237e-07, 2.8778e-06, 3.5465e-06, 1.0528e-06,
        5.1490e-06, 1.0537e-06, 1.3014e-06, 1.0577e-06, 2.1408e-06, 1.4738e-06,
        9.6581e-07, 1.7931e-06, 1.1650e-06, 2.0159e-06, 7.2576e-07, 2.3425e-06,
        1.6299e-06, 1.9379e-06, 8.7237e-07, 7.9282e-07, 1.4169e-06, 5.1189e-07,
        6.2924e-07, 2.6761e-06, 1.9379e-06, 6.8515e-07, 8.5841e-07, 7.9016e-07,
        1.0066e-06, 6.4091e-07, 4.9869e-07, 1.9379e-06, 1.0537e-06, 1.0537e-06,
        3.1789e-06, 1.5834e-06, 1.0577e-06, 9.0326e-07, 1.3097e-06, 2.9077e-06,
        1.9847e-06, 1.2763e-06, 7.0389e-07, 1.4169e-06, 7.3731e-07, 1.5695e-06,
        6.2924e-07, 4.0475e-06, 1.4738e-06, 2.8778e-06, 1.0066e-06, 5.9930e-07,
        1.4211e-06, 1.6299e-06, 2.9436e-07, 5.6671e-07, 2.3222e-06, 1.3422e-06,
        8.4134e-07, 7.2576e-07, 1.4169e-06, 1.8823e-06, 2.4912e-06, 8.4134e-07,
        1.5015e-06, 2.5265e-06, 1.8232e-06, 1.2835e-06, 1.4069e-06, 2.8778e-06,
        1.2764e-06, 3.5465e-06, 5.9652e-07, 3.5662e-06, 2.2674e-06, 1.4211e-06,
        2.2379e-06, 1.5834e-06, 1.7645e-06, 5.9808e-07, 1.0194e-06, 8.4134e-07,
        4.3407e-06, 2.7150e-07, 1.6890e-07, 1.4169e-06, 2.5500e-06, 7.2576e-07,
        1.3787e-06, 1.4209e-06, 1.6317e-06, 9.2456e-07, 1.0529e-06, 9.7813e-07,
        8.7237e-07, 9.4273e-07, 1.0537e-06, 1.4740e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.6588e-06, 1.1297e-05, 4.0920e-06, 9.4384e-06, 8.3163e-06, 9.4384e-06,
        3.5429e-06, 7.4324e-06, 3.4188e-06, 8.9766e-06, 2.2898e-06, 2.2149e-06,
        7.9814e-06, 2.4570e-06, 4.3082e-06, 5.3045e-06, 4.0009e-06, 7.1265e-06,
        8.5236e-06, 1.6129e-06, 2.9037e-06, 3.2203e-06, 6.9213e-06, 7.4695e-06,
        2.9037e-06, 1.1593e-05, 6.6210e-06, 4.8182e-06, 3.4203e-06, 1.3620e-06,
        3.1524e-06, 2.5092e-06, 1.0752e-05, 2.2841e-06, 2.6121e-06, 2.9701e-06,
        3.5989e-06, 7.3509e-06, 9.1626e-07, 1.5145e-06, 2.9037e-06, 2.9701e-06,
        1.0114e-05, 3.9393e-06, 2.9915e-06, 2.9037e-06, 1.0400e-05, 3.8701e-06,
        4.5226e-06, 2.0624e-05, 7.6134e-06, 3.4203e-06, 2.9037e-06, 5.7771e-06,
        7.2150e-06, 6.4177e-06, 1.2290e-05, 5.7422e-07, 3.1923e-06, 2.9701e-06,
        1.0128e-05, 3.2517e-06, 4.2857e-06, 6.4642e-06, 5.7210e-06, 2.4397e-06,
        7.9544e-06, 2.6121e-06, 2.9358e-06, 4.0713e-06, 8.7534e-06, 2.9542e-06,
        2.9915e-06, 5.6746e-06, 2.9701e-06, 4.8084e-06, 3.4502e-06, 9.4706e-06,
        4.4822e-06, 9.8109e-06, 6.3711e-06, 6.3806e-06, 4.8753e-06, 1.1297e-05,
        8.5068e-06, 3.8189e-06, 2.9037e-06, 7.9998e-06, 5.0964e-06, 3.1923e-06,
        3.8701e-06, 3.9005e-06, 6.8645e-06, 3.3113e-06, 4.8870e-06, 1.7372e-06,
        3.2526e-06, 2.8034e-06, 8.6466e-06, 4.2829e-06, 2.4566e-06, 2.4397e-06,
        2.2992e-06, 5.7429e-06, 7.0439e-06, 2.4397e-06, 7.3070e-06, 5.5659e-06,
        4.8753e-06, 1.0489e-05, 6.3873e-06, 6.1147e-06, 4.7903e-06, 1.4399e-06,
        1.0290e-05, 1.0652e-05, 6.1117e-06, 3.9656e-06, 4.9732e-06, 1.7532e-06,
        8.8283e-06, 4.1485e-06, 1.6767e-06, 7.5084e-06, 9.9100e-06, 5.7429e-06,
        1.0058e-05, 7.1381e-06, 7.5084e-06, 1.1028e-05, 3.7045e-06, 1.0972e-05,
        5.6401e-06, 7.0286e-06, 8.7499e-06, 8.8866e-06, 2.9915e-06, 2.9915e-06,
        3.1883e-06, 4.0009e-06, 3.1536e-06, 3.1884e-06, 3.1828e-06, 3.9005e-06,
        7.5084e-06, 3.1559e-06, 5.7974e-06, 6.1015e-06, 3.8701e-06, 6.3061e-06,
        1.1028e-05, 4.2504e-06, 1.0965e-05, 2.7291e-06, 8.7534e-06, 1.0972e-05,
        2.0624e-05, 1.3761e-06, 4.6677e-06, 5.7428e-06, 5.9326e-06, 2.3953e-06,
        2.4686e-06, 2.1549e-06, 9.9100e-06, 1.1591e-05, 6.3061e-06, 1.1593e-05,
        2.4031e-06, 2.6542e-06, 1.0965e-05, 2.7675e-06, 7.3953e-06, 1.9399e-06,
        8.2721e-06, 5.1979e-06, 6.7132e-06, 4.1440e-06, 5.9617e-06, 9.8109e-06,
        3.4203e-06, 7.3430e-06, 1.6779e-06, 2.1775e-06, 2.9358e-06, 7.0922e-06,
        1.4500e-06, 9.6679e-06, 5.5179e-06, 7.2756e-06, 6.7132e-06, 5.0614e-06,
        1.6881e-06, 1.1028e-05, 7.9519e-06, 5.5491e-06, 9.8686e-06, 7.7904e-06,
        8.6452e-06, 3.2452e-06, 8.7383e-06, 2.9701e-06, 3.1923e-06, 1.9399e-06,
        3.3113e-06, 5.8302e-06, 4.4822e-06, 4.5751e-06, 6.2779e-06, 4.5106e-06,
        1.8836e-06, 6.6210e-06, 2.5818e-06, 4.0920e-06, 2.5137e-06, 2.9037e-06,
        8.0882e-06, 4.3082e-06, 2.4031e-06, 6.6151e-06, 9.7113e-06, 6.3656e-06,
        2.7427e-06, 6.3711e-06, 4.6677e-06, 4.6916e-06, 7.3070e-06, 2.1775e-06,
        6.9822e-06, 1.0965e-05, 3.5473e-06, 5.9809e-06, 7.2752e-06, 7.0386e-06,
        1.0290e-05, 4.4163e-06, 5.5378e-06, 4.8241e-06, 5.9617e-06, 2.7308e-06,
        2.3122e-06, 4.0581e-06, 2.9358e-06, 7.1497e-06, 4.4436e-06, 5.9617e-06,
        6.3619e-06, 2.1176e-06, 3.1828e-06, 7.9877e-06, 7.9998e-06, 3.1883e-06,
        1.0093e-05, 2.2992e-06, 5.7429e-06, 1.7512e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.5631e-07, 2.1897e-07, 8.0216e-07,  ..., 5.2195e-07, 2.5924e-07,
        3.5607e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.9545e-07, 5.1759e-07, 2.0677e-06, 2.6989e-06, 7.1373e-07, 2.8667e-06,
        2.5331e-07, 9.0939e-07, 2.2601e-06, 2.5331e-07, 2.0391e-06, 9.3354e-07,
        6.1721e-07, 1.3025e-06, 1.8337e-06, 5.1759e-07, 2.8482e-06, 6.5419e-07,
        9.7264e-07, 2.4441e-06, 6.2090e-07, 2.8667e-06, 1.3185e-06, 2.5330e-07,
        6.7948e-07, 5.4289e-07, 5.8613e-07, 2.8667e-06, 8.7737e-07, 1.5574e-06,
        1.3454e-06, 2.0363e-06, 2.3564e-07, 1.9523e-06, 1.8396e-06, 1.9523e-06,
        1.0258e-06, 7.7657e-07, 5.9860e-07, 8.4765e-07, 2.1658e-07, 1.1241e-06,
        1.5128e-06, 1.8574e-06, 5.9544e-07, 6.2090e-07, 2.4441e-06, 6.1721e-07,
        6.2206e-07, 1.9523e-06, 9.3353e-07, 1.1241e-06, 5.7393e-07, 5.9545e-07,
        1.6124e-06, 1.3077e-06, 2.5176e-07, 2.5330e-07, 6.7948e-07, 1.7662e-06,
        1.3595e-06, 3.6878e-07, 4.4012e-07, 2.0677e-06, 5.3878e-07, 1.3185e-06,
        6.3022e-07, 1.3246e-06, 5.9545e-07, 1.4276e-06, 7.6320e-07, 1.3025e-06,
        7.1373e-07, 7.6320e-07, 7.1373e-07, 1.8396e-06, 2.5331e-07, 2.1658e-07,
        8.7737e-07, 8.0825e-07, 1.9523e-06, 9.3825e-07, 1.7662e-06, 6.7948e-07,
        1.2038e-06, 2.0677e-06, 1.1308e-06, 3.2532e-07, 1.2038e-06, 2.8482e-06,
        1.8396e-06, 2.6989e-06, 1.8396e-06, 6.1721e-07, 1.9523e-06, 1.4218e-06,
        6.2090e-07, 2.7692e-07, 8.7737e-07, 2.8667e-06, 1.4218e-06, 2.8482e-06,
        7.7657e-07, 6.2090e-07, 4.6850e-07, 2.7692e-07, 2.2601e-06, 5.7393e-07,
        2.8422e-06, 7.1373e-07, 1.5155e-06, 1.3025e-06, 2.0677e-06, 2.8482e-06,
        1.9523e-06, 2.2601e-06, 2.4441e-06, 5.7393e-07, 1.0777e-06, 2.5331e-07,
        5.8882e-07, 6.2085e-07, 6.2090e-07, 7.1373e-07, 9.3353e-07, 6.9575e-07,
        2.7692e-07, 6.1721e-07, 1.9523e-06, 8.8383e-07, 2.7692e-07, 1.1241e-06,
        9.7264e-07, 4.6549e-07, 6.2090e-07, 6.2090e-07, 8.1015e-07, 6.2090e-07,
        4.6850e-07, 6.7948e-07, 6.2090e-07, 2.7790e-06, 2.0677e-06, 1.4218e-06,
        3.6878e-07, 1.8396e-06, 2.2601e-06, 1.0777e-06, 1.4218e-06, 2.8408e-06,
        2.8482e-06, 2.5331e-07, 1.8396e-06, 9.3353e-07, 1.3246e-06, 1.5574e-06,
        9.3353e-07, 2.6989e-06, 8.7702e-07, 1.0838e-06, 1.2128e-06, 1.3702e-06,
        6.7948e-07, 1.7662e-06, 5.8882e-07, 3.6878e-07, 1.3185e-06, 6.7948e-07,
        1.8337e-06, 4.6850e-07, 1.7662e-06, 3.9798e-07, 6.9575e-07, 9.3353e-07,
        4.6549e-07, 2.0677e-06, 3.8234e-07, 7.7657e-07, 9.3353e-07, 2.5331e-07,
        1.3772e-06, 2.7790e-06, 8.3911e-07, 1.8574e-06, 5.8431e-08, 1.8396e-06,
        1.1364e-06, 6.2090e-07, 1.4218e-06, 1.7662e-06, 1.5093e-06, 9.3353e-07,
        1.8574e-06, 8.1015e-07, 7.1373e-07, 1.1241e-06, 1.1241e-06, 1.0258e-06,
        6.4561e-07, 8.7737e-07, 3.9798e-07, 8.1015e-07, 2.8667e-06, 7.1373e-07,
        6.7948e-07, 2.2601e-06, 1.5574e-06, 1.2038e-06, 1.1241e-06, 2.0391e-06,
        2.4441e-06, 5.9545e-07, 1.1241e-06, 2.7692e-07, 2.6989e-06, 5.9545e-07,
        2.8482e-06, 2.0677e-06, 2.8482e-06, 5.7393e-07, 5.8772e-07, 1.8396e-06,
        7.7657e-07, 1.8396e-06, 1.3772e-06, 7.1373e-07, 1.1241e-06, 1.8574e-06,
        2.8482e-06, 2.7790e-06, 2.5331e-07, 5.1759e-07, 6.7948e-07, 8.0825e-07,
        5.4289e-07, 1.8574e-06, 2.8667e-06, 5.8613e-07, 1.2106e-06, 2.5331e-07,
        2.0677e-06, 5.9545e-07, 6.2090e-07, 7.6163e-07, 1.5162e-06, 1.3701e-06,
        6.2291e-07, 1.5574e-06, 9.0939e-07, 8.8383e-07, 9.3825e-07, 2.7692e-07,
        1.0838e-06, 1.1241e-06, 9.7264e-07, 7.6426e-07, 6.7948e-07, 1.5134e-06,
        6.3711e-07, 1.0837e-06, 1.0838e-06, 2.5331e-07, 1.1241e-06, 7.1373e-07,
        1.4218e-06, 2.8482e-06, 9.3825e-07, 1.0838e-06, 1.7662e-06, 9.8539e-07,
        1.3077e-06, 1.9294e-06, 1.8396e-06, 1.8396e-06, 1.1241e-06, 9.3353e-07,
        7.1373e-07, 1.9523e-06, 1.8574e-06, 2.7692e-07, 2.2601e-06, 2.8667e-06,
        1.4449e-06, 2.8667e-06, 7.4148e-07, 2.8401e-06, 1.5621e-06, 5.9545e-07,
        2.8482e-06, 1.3077e-06, 4.6549e-07, 6.1721e-07, 2.8415e-06, 2.5331e-07,
        1.1670e-06, 7.1373e-07, 6.9575e-07, 1.7699e-06, 2.4441e-06, 1.8574e-06,
        2.8482e-06, 2.8667e-06, 6.7948e-07, 8.8756e-07, 9.7121e-07, 2.8667e-06,
        5.9545e-07, 5.8613e-07, 2.0677e-06, 4.6549e-07, 5.8613e-07, 2.2601e-06,
        4.6549e-07, 2.0363e-06, 5.9545e-07, 6.7948e-07, 1.5216e-06, 2.8482e-06,
        1.8396e-06, 1.8396e-06, 8.9306e-07, 7.6166e-07, 2.7692e-07, 2.5331e-07,
        1.8396e-06, 1.3595e-06, 9.0939e-07, 2.8482e-06, 5.8882e-07, 9.3353e-07,
        1.3702e-06, 3.6782e-06, 2.5331e-07, 5.9545e-07, 1.0258e-06, 7.7657e-07,
        5.9545e-07, 2.8482e-06, 1.3077e-06, 1.8396e-06, 1.0777e-06, 9.3353e-07,
        1.3702e-06, 1.8396e-06, 6.3022e-07, 8.3836e-07, 1.8337e-06, 2.5176e-07,
        1.2038e-06, 6.1839e-07, 1.8508e-06, 9.7264e-07, 2.5331e-07, 1.8396e-06,
        8.7737e-07, 2.8413e-06, 2.7692e-07, 2.0677e-06, 2.7790e-06, 1.3275e-06,
        1.0838e-06, 8.0495e-07, 1.1241e-06, 5.8882e-07, 2.2601e-06, 1.4449e-06,
        2.0677e-06, 1.4218e-06, 5.9545e-07, 2.6989e-06, 5.5046e-07, 9.5357e-07,
        4.6549e-07, 5.1759e-07, 6.2090e-07, 2.5331e-07, 3.7357e-07, 8.8756e-07,
        1.0838e-06, 2.2601e-06, 7.1373e-07, 2.8482e-06, 2.7790e-06, 1.0838e-06,
        2.8482e-06, 1.8574e-06, 5.7393e-07, 6.2090e-07, 1.5099e-06, 2.7790e-06,
        3.6878e-07, 1.8396e-06, 5.8613e-07, 1.0838e-06, 6.3022e-07, 2.8354e-06,
        2.6989e-06, 1.0838e-06, 2.5331e-07, 2.8427e-06, 5.8613e-07, 1.2038e-06,
        5.9545e-07, 6.7948e-07, 1.7662e-06, 1.9523e-06, 1.6989e-06, 1.8574e-06,
        6.7948e-07, 1.4218e-06, 4.6549e-07, 1.8396e-06, 2.0677e-06, 4.6548e-07,
        1.9523e-06, 7.4148e-07, 8.7737e-07, 5.8882e-07, 7.7657e-07, 5.0591e-07,
        1.0838e-06, 1.9523e-06, 1.7699e-06, 2.4596e-06, 6.7948e-07, 2.4441e-06,
        1.4218e-06, 2.7790e-06, 9.7264e-07, 2.4595e-06, 1.7346e-06, 3.6878e-07,
        2.2601e-06, 5.8613e-07, 2.8311e-06, 8.9306e-07, 1.8396e-06, 1.2038e-06,
        6.1721e-07, 1.5621e-06, 1.8442e-06, 7.7657e-07, 1.5621e-06, 9.4978e-07,
        1.3185e-06, 6.7948e-07, 4.6549e-07, 2.5331e-07, 8.1015e-07, 6.7948e-07,
        1.9523e-06, 7.1373e-07, 8.8756e-07, 1.2106e-06, 3.1713e-06, 1.2038e-06,
        7.7657e-07, 8.7737e-07, 5.9545e-07, 1.3702e-06, 3.9798e-07, 8.7737e-07,
        1.1241e-06, 1.9523e-06, 2.2601e-06, 1.7662e-06, 1.8396e-06, 8.7702e-07,
        3.6878e-07, 7.7657e-07, 1.2038e-06, 8.9306e-07, 1.9523e-06, 1.7699e-06,
        6.2225e-07, 1.0838e-06, 7.1373e-07, 1.1241e-06, 2.8482e-06, 9.7264e-07,
        2.5331e-07, 7.1373e-07, 2.0677e-06, 1.0258e-06, 1.0258e-06, 9.7359e-07,
        4.9227e-07, 9.8181e-07, 6.9575e-07, 5.9545e-07, 1.0838e-06, 1.1241e-06,
        1.5621e-06, 5.8772e-07, 3.6878e-07, 8.0825e-07, 2.8482e-06, 9.0939e-07,
        2.8482e-06, 2.8482e-06, 1.3702e-06, 1.4218e-06, 5.8882e-07, 2.5331e-07,
        1.1241e-06, 5.7393e-07, 1.0838e-06, 2.6989e-06, 6.7948e-07, 6.1721e-07,
        1.0838e-06, 7.7657e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.8585e-06, 5.3311e-06, 1.3444e-06, 2.8780e-06, 5.1299e-06, 6.1380e-06,
        9.4745e-06, 1.2244e-05, 5.1396e-06, 5.9646e-06, 4.1191e-06, 6.0577e-06,
        1.0371e-05, 8.2977e-06, 8.0609e-06, 6.2659e-06, 1.6941e-05, 8.2996e-06,
        8.2977e-06, 5.1299e-06, 1.2821e-06, 7.0054e-06, 1.4377e-05, 5.3311e-06,
        1.8635e-06, 5.9776e-06, 9.5110e-07, 1.2421e-05, 3.1679e-06, 9.4745e-06,
        5.0226e-06, 1.1016e-05, 3.0531e-06, 5.0226e-06, 5.9929e-06, 9.7319e-06,
        3.9895e-06, 7.6080e-06, 1.6226e-05, 6.6249e-06, 8.0609e-06, 4.4747e-06,
        6.0577e-06, 8.2995e-06, 1.1558e-05, 6.4965e-06, 6.7405e-06, 3.4634e-06,
        8.3394e-06, 3.6275e-06, 5.4150e-06, 2.3093e-06, 3.9494e-06, 1.3153e-05,
        7.0054e-06, 6.7405e-06, 6.6249e-06, 1.2244e-05, 5.6385e-06, 7.6080e-06,
        1.3153e-05, 2.3093e-06, 1.7539e-05, 6.4836e-06, 6.4965e-06, 9.5261e-06,
        4.4747e-06, 1.3153e-05, 3.6275e-06, 6.0325e-06, 9.7367e-06, 8.8635e-06,
        4.8178e-06, 6.0577e-06, 6.1718e-06, 8.0609e-06, 8.2996e-06, 1.3153e-05,
        1.0796e-05, 2.2729e-05, 1.2244e-05, 1.2244e-05, 8.0609e-06, 1.8635e-06,
        5.9646e-06, 1.1659e-05, 2.8780e-06, 3.9494e-06, 1.2244e-05, 4.3109e-06,
        1.2244e-05, 1.3444e-06, 8.0218e-06, 1.3924e-06, 7.0054e-06, 9.4745e-06,
        8.0609e-06, 1.1389e-05, 6.7189e-06, 1.1659e-05, 1.1389e-05, 3.6275e-06,
        1.6226e-05, 3.8474e-06, 9.3325e-06, 5.3311e-06, 1.8635e-06, 2.8780e-06,
        5.1299e-06, 6.4836e-06, 2.3093e-06, 1.4844e-05, 8.8635e-06, 5.0226e-06,
        2.8134e-06, 3.4094e-06, 7.0054e-06, 1.8635e-06, 9.5111e-07, 4.4747e-06,
        1.2244e-05, 9.7760e-06, 5.1299e-06, 2.2729e-05, 9.9167e-06, 1.3444e-06,
        5.1299e-06, 1.3719e-05, 1.1558e-05, 9.7367e-06, 8.2996e-06, 4.6674e-06,
        2.5911e-06, 6.0496e-06, 8.2977e-06, 1.3153e-05, 4.6335e-06, 7.1111e-07,
        3.8241e-06, 1.1558e-05, 6.0496e-06, 6.0496e-06, 8.5781e-06, 4.4747e-06,
        8.2995e-06, 5.6385e-06, 4.1405e-06, 1.3312e-05, 1.2399e-05, 1.3924e-06,
        1.3924e-06, 2.3093e-06, 2.2724e-06, 1.3249e-06, 8.0609e-06, 6.1718e-06,
        2.3546e-05, 1.6226e-05, 1.3312e-05, 8.8635e-06, 6.7405e-06, 6.0496e-06,
        9.3397e-06, 5.6077e-06, 2.8780e-06, 6.4836e-06, 1.2244e-05, 9.9167e-06,
        1.3669e-05, 4.2273e-06, 3.6275e-06, 8.2995e-06, 6.7189e-06, 8.0609e-06,
        1.1659e-05, 3.9895e-06, 3.6275e-06, 3.6275e-06, 1.8635e-06, 8.5781e-06,
        9.4745e-06, 3.9494e-06, 1.1937e-05, 1.3153e-05, 1.8635e-06, 1.5215e-06,
        1.2244e-05, 9.9167e-06, 4.8178e-06, 8.2995e-06, 1.0362e-05, 6.0577e-06,
        5.1299e-06, 3.9494e-06, 1.2244e-05, 8.2995e-06, 1.0371e-05, 4.4747e-06,
        6.8678e-06, 4.6024e-06, 1.6226e-05, 7.6080e-06, 7.6080e-06, 9.3267e-06,
        1.7573e-05, 1.3669e-05, 4.2273e-06, 5.9316e-06, 1.2244e-05, 3.8474e-06,
        2.2729e-05, 4.2273e-06, 6.5963e-06, 6.7405e-06, 8.1145e-06, 3.4634e-06,
        8.3306e-06, 6.0577e-06, 8.2007e-06, 7.7069e-06, 9.9167e-06, 9.4745e-06,
        8.2977e-06, 9.7367e-06, 9.9167e-06, 8.8458e-06, 3.8474e-06, 1.3719e-05,
        4.8188e-06, 4.4747e-06, 9.7319e-06, 9.5261e-06, 7.0054e-06, 4.1966e-06,
        1.8503e-05, 4.5994e-06, 8.4606e-06, 5.6077e-06, 9.5261e-06, 1.1659e-05,
        1.6941e-05, 6.4965e-06, 5.1299e-06, 6.1380e-06, 4.4747e-06, 1.3924e-06,
        4.4747e-06, 2.8780e-06, 1.8635e-06, 1.3924e-06, 5.6077e-06, 4.4747e-06,
        9.4745e-06, 3.9494e-06, 6.0577e-06, 8.2977e-06, 6.6099e-06, 9.5110e-07,
        1.4377e-05, 8.8635e-06, 2.8780e-06, 1.5273e-05, 1.6226e-05, 5.1299e-06,
        2.8780e-06, 5.0226e-06, 2.3093e-06, 1.6226e-05, 2.3093e-06, 1.6317e-05,
        4.3109e-06, 3.9494e-06, 2.8134e-06, 6.0496e-06, 4.8146e-06, 1.3153e-05,
        1.1389e-05, 4.8178e-06, 4.4747e-06, 4.8188e-06, 5.6385e-06, 3.8746e-06,
        9.7367e-06, 3.1331e-06, 1.5644e-05, 6.4965e-06, 8.8635e-06, 1.1786e-05,
        6.2712e-06, 1.0362e-05, 5.2954e-06, 1.6317e-05, 4.4747e-06, 1.3924e-06,
        1.7548e-06, 3.9494e-06, 4.4747e-06, 1.5273e-05, 8.8635e-06, 4.4747e-06,
        8.2995e-06, 5.6077e-06, 4.4747e-06, 1.3153e-05, 3.9895e-06, 4.8146e-06,
        1.6317e-05, 4.0334e-06, 1.6941e-05, 9.7367e-06, 6.4965e-06, 6.4965e-06,
        1.1558e-05, 3.8474e-06, 7.0054e-06, 6.4965e-06, 6.7405e-06, 4.4754e-06,
        2.0963e-06, 7.0054e-06, 2.0107e-05, 1.3153e-05, 4.2273e-06, 1.3153e-05,
        8.0609e-06, 3.2193e-06, 8.9702e-06, 3.0096e-06, 4.6024e-06, 7.7069e-06,
        5.9646e-06, 7.6080e-06, 7.1762e-06, 6.3001e-06, 6.4965e-06, 3.6275e-06,
        9.7367e-06, 9.1333e-06, 6.7405e-06, 4.8146e-06, 1.6226e-05, 7.1762e-06,
        4.8178e-06, 5.3311e-06, 9.7367e-06, 4.4747e-06, 6.4965e-06, 6.1380e-06,
        1.3924e-06, 1.5769e-05, 1.3153e-05, 1.3153e-05, 7.1762e-06, 1.6226e-05,
        7.4533e-06, 1.1659e-05, 6.5963e-06, 1.3444e-06, 5.0966e-06, 6.4965e-06,
        9.7367e-06, 6.3987e-06, 8.2794e-06, 8.8635e-06, 7.0054e-06, 6.3001e-06,
        4.6024e-06, 8.3394e-06, 1.3153e-05, 9.7367e-06, 2.7403e-06, 1.6317e-05,
        7.1762e-06, 7.1762e-06, 5.6385e-06, 7.5180e-06, 1.6226e-05, 1.1659e-05,
        4.9711e-06, 6.0577e-06, 2.8780e-06, 6.0577e-06, 3.9895e-06, 4.4747e-06,
        4.8254e-06, 4.0334e-06, 1.5769e-05, 9.4745e-06, 6.4836e-06, 1.2821e-06,
        8.0609e-06, 3.9494e-06, 3.6275e-06, 1.3669e-05, 3.2193e-06, 1.7193e-05,
        4.2273e-06, 8.4606e-06, 7.7069e-06, 1.8562e-06, 1.3444e-06, 3.6275e-06,
        6.3987e-06, 6.3389e-06, 4.4747e-06, 1.1389e-05, 5.1299e-06, 1.0633e-05,
        6.4915e-06, 1.1456e-05, 1.8562e-06, 5.1299e-06, 2.3546e-05, 3.8111e-06,
        1.1659e-05, 7.6080e-06, 8.2725e-06, 3.9494e-06, 8.3394e-06, 6.0577e-06,
        2.8780e-06, 1.3153e-05, 1.3924e-06, 4.2534e-06, 8.2977e-06, 1.3153e-05,
        1.3153e-05, 4.2273e-06, 3.1464e-06, 5.9646e-06, 9.3325e-06, 8.2996e-06,
        2.5911e-06, 1.3924e-06, 1.2244e-05, 5.1299e-06, 4.8146e-06, 4.6024e-06,
        1.7814e-06, 1.3153e-05, 6.4836e-06, 6.7405e-06, 3.0096e-06, 3.9494e-06,
        6.0496e-06, 1.6226e-05, 7.0054e-06, 5.6385e-06, 1.3153e-05, 8.3394e-06,
        2.3546e-05, 1.2421e-05, 9.9167e-06, 7.0523e-06, 1.3444e-06, 8.0218e-06,
        4.4747e-06, 6.4965e-06, 4.4747e-06, 3.8474e-06, 6.0577e-06, 2.2789e-06,
        5.0901e-06, 3.1331e-06, 6.1380e-06, 4.2273e-06, 3.1331e-06, 7.6080e-06,
        2.1904e-06, 9.3397e-06, 5.9776e-06, 5.6385e-06, 1.1558e-05, 2.0963e-06,
        7.6155e-06, 1.1659e-05, 1.3924e-06, 7.5768e-06, 1.1937e-05, 6.7405e-06,
        5.3311e-06, 1.1670e-05, 1.3153e-05, 8.8635e-06, 6.0577e-06, 5.0226e-06,
        9.3397e-06, 3.4634e-06, 1.2399e-05, 2.7403e-06, 4.3109e-06, 7.2438e-06,
        5.1299e-06, 1.0519e-05, 1.2244e-05, 6.4836e-06, 4.3109e-06, 4.4747e-06,
        8.3306e-06, 8.8635e-06, 3.5437e-06, 2.6755e-06, 3.9895e-06, 9.7197e-06,
        1.2244e-05, 1.5273e-05, 3.4355e-06, 7.0054e-06, 5.6077e-06, 1.0623e-05,
        3.9895e-06, 9.3397e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.0704e-07, 1.1985e-06, 1.0298e-06,  ..., 7.6112e-07, 1.5650e-06,
        1.8155e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.0128e-06, 1.2799e-06, 8.6891e-07,  ..., 2.7605e-06, 1.2469e-06,
        4.6464e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.6982e-06, 6.2110e-06, 2.5888e-06, 1.5597e-06, 2.2915e-06, 2.0017e-06,
        2.8038e-06, 3.4047e-06, 2.8038e-06, 7.1667e-06, 7.3949e-06, 5.3287e-06,
        2.0941e-06, 7.3949e-06, 8.9994e-06, 8.2369e-06, 2.1758e-06, 4.3379e-06,
        6.5141e-06, 5.2181e-06, 2.6146e-06, 8.7263e-07, 9.0401e-06, 2.5888e-06,
        4.0604e-06, 7.5934e-07, 7.6049e-06, 9.4954e-06, 2.4432e-07, 6.7575e-06,
        5.6179e-06, 4.7850e-06, 1.0575e-05, 2.9797e-06, 9.9949e-06, 2.4535e-06,
        1.8426e-06, 2.6087e-06, 4.6982e-06, 2.5983e-06, 2.2106e-06, 4.4177e-06,
        3.8310e-06, 9.4954e-06, 6.8131e-06, 4.9644e-06, 4.7544e-06, 1.4502e-06,
        6.6787e-06, 8.7263e-07, 4.1614e-06, 6.5333e-06, 6.2912e-06, 3.4915e-06,
        4.2535e-06, 2.9830e-06, 3.7578e-06, 1.9737e-06, 1.5911e-06, 1.3225e-05,
        2.5888e-06, 3.8310e-06, 9.8771e-06, 7.9331e-06, 7.1667e-06, 1.2485e-06,
        7.5484e-06, 7.9331e-06, 5.1673e-06, 6.0551e-06, 1.9863e-06, 2.5611e-06,
        4.5290e-06, 5.7792e-06, 1.9748e-06, 2.1152e-06, 4.6330e-06, 4.6220e-06,
        7.5934e-07, 2.5888e-06, 1.9748e-06, 6.2110e-06, 2.5888e-06, 2.0017e-06,
        5.2181e-06, 4.8727e-06, 3.9086e-06, 2.2663e-06, 2.0241e-06, 3.5534e-06,
        1.8804e-06, 1.1548e-05, 3.5534e-06, 1.1825e-05, 2.6146e-06, 4.2535e-06,
        2.3766e-06, 3.9259e-06, 2.4535e-06, 8.7263e-07, 7.5934e-07, 2.5611e-06,
        6.5514e-06, 1.5084e-06, 4.7081e-06, 2.4535e-06, 4.4975e-06, 4.1673e-06,
        3.4839e-06, 4.7081e-06, 2.3766e-06, 1.2172e-06, 7.6049e-06, 1.2172e-06,
        1.2172e-06, 1.1387e-06, 2.8038e-06, 6.8584e-06, 1.7461e-06, 4.3379e-06,
        3.4839e-06, 4.1620e-06, 4.6330e-06, 2.6146e-06, 2.5059e-06, 4.7850e-06,
        2.7681e-06, 2.5888e-06, 2.6146e-06, 2.6146e-06, 7.3949e-06, 6.7537e-06,
        8.3735e-06, 2.0241e-06, 1.5911e-06, 4.7850e-06, 4.2550e-06, 4.1673e-06,
        5.7797e-06, 7.6049e-06, 4.7544e-06, 1.8916e-06, 1.1490e-05, 2.6694e-06,
        4.7850e-06, 6.5136e-06, 4.7081e-06, 3.9259e-06, 2.9044e-06, 2.9394e-06,
        1.0575e-05, 4.3379e-06, 3.1689e-06, 2.2373e-06, 3.6094e-06, 6.2206e-06,
        4.8121e-06, 4.7850e-06, 3.1316e-06, 4.3919e-07, 4.3379e-06, 3.2799e-06,
        1.9737e-06, 5.6179e-06, 8.9858e-06, 3.0986e-06, 7.1667e-06, 4.6885e-07,
        2.8440e-06, 3.8310e-06, 7.2705e-06, 4.3379e-06, 4.8604e-06, 5.4997e-06,
        5.4359e-06, 4.1464e-06, 3.1689e-06, 2.8038e-06, 7.6049e-06, 2.9044e-06,
        4.3379e-06, 4.2535e-06, 6.1888e-06, 3.1316e-06, 2.6398e-06, 5.6748e-06,
        3.8720e-06, 1.2172e-06, 2.2663e-06, 3.6481e-06, 2.0241e-06, 5.2181e-06,
        3.5325e-06, 1.7461e-06, 4.5239e-06, 7.4367e-06, 2.2373e-06, 3.4227e-06,
        1.1490e-05, 4.8370e-06, 2.2301e-06, 1.2460e-06, 1.0152e-06, 4.7850e-06,
        4.8604e-06, 1.1139e-05, 4.3844e-06, 5.9362e-06, 2.2301e-06, 4.1614e-06,
        6.8584e-06, 4.6330e-06, 4.8604e-06, 2.1152e-06, 1.5911e-06, 4.7544e-06,
        4.7287e-06, 2.5888e-06, 3.4473e-06, 7.4367e-06, 4.7081e-06, 2.9394e-06,
        4.5841e-06, 2.8231e-06, 4.7081e-06, 2.4535e-06, 2.8231e-06, 4.1956e-06,
        5.8835e-06, 3.9549e-06, 2.0133e-06, 5.2650e-06, 2.4535e-06, 5.7797e-06,
        6.5514e-06, 3.8720e-06, 3.6609e-06, 6.3255e-06, 8.5974e-06, 6.8131e-06,
        2.5059e-06, 2.8038e-06, 5.7471e-06, 4.3379e-06, 2.4535e-06, 2.5888e-06,
        6.7537e-06, 1.2066e-06, 3.8445e-06, 3.2419e-06, 1.8254e-06, 6.3454e-06,
        4.4364e-06, 3.2888e-06, 2.5888e-06, 6.8584e-06, 1.2460e-06, 1.9748e-06,
        3.8720e-06, 2.5844e-06, 6.8131e-06, 4.7081e-06, 2.3227e-06, 2.4535e-06,
        7.7866e-06, 6.9024e-06, 2.4535e-06, 3.1316e-06, 3.8310e-06, 2.0241e-06,
        7.9331e-06, 8.1990e-06, 9.0902e-06, 7.9600e-06, 2.5844e-06, 4.4092e-06,
        1.2172e-06, 2.5888e-06, 6.4299e-06, 1.5339e-06, 6.7537e-06, 4.1673e-06,
        5.7201e-06, 3.9259e-06, 5.7201e-06, 1.3736e-06, 1.4784e-06, 5.4359e-06,
        4.8604e-06, 4.6330e-06, 2.6398e-06, 4.6330e-06, 1.6919e-06, 6.7575e-06,
        4.7571e-06, 9.4954e-06, 4.3844e-06, 8.7262e-07, 1.6650e-06, 4.3379e-06,
        8.6381e-06, 4.8210e-06, 2.0241e-06, 4.1464e-06, 4.0367e-06, 5.7699e-06,
        2.8811e-06, 4.9104e-06, 3.0366e-06, 5.6508e-06, 3.2799e-06, 2.3033e-06,
        4.7850e-06, 2.5888e-06, 5.9623e-06, 3.6882e-06, 1.1548e-05, 4.1464e-06,
        4.0367e-06, 5.5238e-06, 1.8804e-06, 7.9600e-06, 4.1464e-06, 4.0042e-06,
        4.0042e-06, 1.9737e-06, 2.6713e-06, 4.8032e-06, 2.3664e-06, 1.6919e-06,
        7.9336e-06, 4.7081e-06, 2.0757e-06, 5.9362e-06, 2.3213e-06, 4.6330e-06,
        3.6609e-06, 2.8038e-06, 1.6779e-06, 1.5873e-06, 4.1956e-06, 9.0401e-06,
        1.6004e-06, 5.0408e-06, 2.5611e-06, 4.7316e-06, 3.8486e-06, 3.1316e-06,
        2.8038e-06, 2.0577e-06, 7.5934e-07, 1.6919e-06, 1.2460e-06, 2.9566e-06,
        1.3017e-06, 1.2245e-05, 1.5911e-06, 6.6446e-07, 2.0017e-06, 5.6748e-06,
        3.8603e-06, 2.8038e-06, 8.5974e-06, 3.2481e-06, 4.3379e-06, 1.7475e-06,
        2.4954e-06, 1.8804e-06, 6.9527e-06, 3.1689e-06, 1.8804e-06, 1.1356e-05,
        2.2663e-06, 6.5514e-06, 7.6049e-06, 4.1974e-06, 3.2481e-06, 7.9331e-06,
        6.2912e-06, 4.7850e-06, 3.7116e-06, 9.5483e-06, 3.4642e-06, 5.5862e-06,
        1.2460e-06, 2.4535e-06, 5.7602e-06, 9.9551e-07, 4.6982e-06, 4.0042e-06,
        2.5888e-06, 1.2172e-06, 2.8038e-06, 1.3015e-06, 2.5888e-06, 2.3766e-06,
        4.1620e-06, 6.4861e-06, 1.5084e-06, 7.4367e-06, 1.2896e-05, 4.7544e-06,
        5.8272e-06, 2.2106e-06, 8.9932e-06, 1.6650e-06, 1.8916e-06, 7.1425e-06,
        2.4535e-06, 2.5807e-06, 3.1316e-06, 2.5888e-06, 2.8038e-06, 2.2687e-06,
        7.3949e-06, 3.1316e-06, 2.2870e-06, 3.9086e-06, 4.7850e-06, 7.2705e-06,
        3.1689e-06, 5.7602e-06, 1.0575e-05, 9.4875e-08, 1.0575e-05, 4.5239e-06,
        4.8665e-06, 2.2373e-06, 1.1490e-05, 4.1833e-06, 4.7081e-06, 5.7797e-06,
        5.2098e-06, 5.4279e-06, 4.0976e-06, 8.5974e-06, 8.9858e-06, 2.5887e-06,
        4.7081e-06, 4.1673e-06, 9.4929e-06, 3.0324e-06, 4.6982e-06, 1.3560e-06,
        1.5597e-06, 6.4863e-06, 2.5888e-06, 1.2460e-06, 1.5597e-06, 2.2373e-06,
        2.6705e-06, 6.0029e-06, 2.7783e-06, 2.6146e-06, 7.8448e-06, 3.6188e-06,
        1.2172e-06, 2.1875e-06, 4.8955e-06, 2.5887e-06, 4.4739e-06, 7.9336e-06,
        1.3541e-06, 3.6481e-06, 2.6146e-06, 4.3454e-06, 2.3766e-06, 1.9344e-06,
        2.4432e-07, 2.3664e-06, 4.3379e-06, 5.7201e-06, 4.3379e-06, 2.0223e-06,
        3.8603e-06, 1.9241e-06, 1.8916e-06, 5.6179e-06, 4.7778e-06, 4.3318e-06,
        1.4784e-06, 2.4954e-06, 4.7544e-06, 3.0324e-06, 3.7813e-06, 2.1042e-06,
        2.5888e-06, 5.7797e-06, 2.4535e-06, 5.5041e-06, 9.5483e-06, 2.9103e-06,
        2.7452e-06, 4.7081e-06, 6.2206e-06, 4.3454e-06, 9.8133e-06, 1.2460e-06,
        3.9259e-06, 3.4227e-06, 2.5888e-06, 2.8038e-06, 6.5136e-06, 5.6179e-06,
        2.2668e-06, 1.0701e-06, 8.6381e-06, 4.2535e-06, 2.4535e-06, 4.8955e-06,
        4.7544e-06, 7.3949e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.7403e-05, 3.5219e-06, 1.9890e-05, 1.0622e-05, 1.6333e-05, 2.4696e-05,
        1.4139e-05, 2.0447e-05, 1.9108e-05, 2.1472e-05, 1.2775e-05, 5.1233e-06,
        7.6332e-06, 3.0234e-05, 1.9406e-05, 1.5779e-05, 3.5582e-05, 1.2267e-05,
        1.4811e-05, 6.6135e-06, 1.8609e-05, 1.7423e-05, 1.1785e-05, 9.0742e-06,
        1.7574e-05, 9.8676e-06, 1.0619e-05, 2.4629e-05, 2.0391e-05, 2.1846e-05,
        5.9543e-06, 8.7839e-06, 2.1335e-05, 9.0177e-06, 9.0854e-06, 1.3980e-05,
        3.8139e-07, 2.2331e-05, 1.3960e-05, 2.7538e-05, 6.1235e-06, 1.1798e-05,
        1.3200e-05, 3.3851e-05, 1.5195e-05, 1.4479e-05, 1.4906e-05, 1.8654e-05,
        1.8687e-05, 1.2137e-05, 2.3641e-05, 2.1371e-05, 2.4583e-05, 9.9381e-06,
        4.6102e-06, 1.0998e-05, 1.1798e-05, 1.3628e-05, 1.1862e-05, 6.2720e-06,
        5.1299e-06, 1.3188e-05, 2.3364e-05, 1.1870e-05, 1.4075e-05, 2.4084e-05,
        1.9650e-05, 1.4883e-05, 9.0854e-06, 1.4033e-05, 2.0852e-05, 7.1665e-06,
        1.6522e-05, 1.3462e-05, 2.5855e-05, 1.9625e-05, 2.9500e-05, 3.5245e-05,
        1.5119e-05, 8.7907e-06, 3.0047e-05, 3.1960e-05, 1.5719e-05, 1.2775e-05,
        1.3634e-05, 2.3318e-05, 7.4634e-06, 1.6457e-05, 1.3222e-05, 5.8615e-06,
        1.7698e-05, 9.1445e-06, 1.5622e-05, 1.3015e-05, 1.3546e-05, 1.9892e-05,
        1.2581e-05, 7.3684e-06, 1.0204e-05, 1.7421e-05, 8.4697e-06, 2.0491e-05,
        8.6733e-06, 9.4815e-06, 1.3185e-05, 6.2102e-06, 3.0948e-05, 1.1727e-05,
        1.5822e-05, 3.1735e-05, 8.4300e-06, 1.0619e-05, 2.7399e-05, 2.5978e-05,
        6.4729e-06, 4.8349e-06, 2.5941e-05, 8.4492e-06, 1.6536e-05, 1.6898e-05,
        7.7917e-07, 3.0000e-05, 3.0289e-05, 2.7538e-05, 2.3929e-05, 2.6112e-05,
        1.1967e-05, 2.6844e-05, 7.7621e-06, 8.0089e-06, 2.7140e-05, 1.0607e-05,
        1.8119e-05, 1.1798e-05, 1.7775e-05, 2.2175e-05, 1.6457e-05, 7.2421e-06,
        2.3305e-05, 2.5107e-05, 1.5827e-05, 2.3861e-05, 1.7024e-05, 7.1156e-06,
        1.2470e-05, 2.0564e-05, 3.0549e-05, 2.0341e-05, 1.7419e-05, 2.4419e-05,
        1.2420e-05, 6.4313e-06, 1.4772e-05, 1.5202e-05, 1.1557e-05, 1.0578e-05,
        1.5755e-05, 1.2474e-05, 2.1290e-05, 1.4483e-05, 2.1623e-05, 1.5379e-05,
        2.4605e-05, 5.7954e-06, 1.0998e-05, 6.4313e-06, 6.4729e-06, 6.0119e-06,
        3.1254e-05, 4.6205e-06, 2.3061e-05, 1.4090e-05, 7.7621e-06, 9.8530e-06,
        1.1869e-05, 1.5920e-05, 1.8609e-05, 1.3377e-05, 9.3204e-06, 1.4483e-05,
        1.1154e-05, 2.4824e-05, 8.7839e-06, 1.7974e-05, 1.2058e-05, 5.4363e-06,
        8.4697e-06, 1.5270e-05, 3.0549e-05, 1.8845e-05, 1.3031e-05, 3.1821e-05,
        5.9543e-06, 1.8948e-05, 1.3588e-05, 8.7907e-06, 1.5515e-05, 1.3200e-05,
        3.8367e-05, 1.9750e-05, 1.7509e-05, 1.3863e-05, 1.3569e-05, 5.6148e-06,
        2.5935e-05, 9.1211e-06, 3.4171e-05, 1.7060e-05, 1.5722e-05, 2.0442e-05,
        6.7251e-06, 1.4790e-05, 2.0871e-05, 1.7699e-05, 2.6967e-05, 7.1917e-06,
        1.4975e-05, 6.7301e-06, 9.0177e-06, 2.5137e-05, 3.7036e-05, 2.9213e-05,
        1.2420e-05, 1.4434e-05, 1.4693e-05, 1.3222e-05, 2.5829e-05, 1.5845e-05,
        1.7269e-05, 5.6600e-06, 8.0313e-06, 1.3930e-05, 1.6457e-05, 1.8046e-05,
        8.1603e-06, 1.9604e-05, 1.3755e-05, 6.0614e-06, 1.4219e-05, 1.4645e-05,
        2.5855e-05, 1.6852e-05, 7.1916e-06, 2.4696e-05, 2.0442e-05, 3.3245e-06,
        6.1192e-06, 3.2383e-05, 6.9703e-06, 9.2729e-06, 1.6414e-05, 2.7952e-05,
        2.7924e-05, 1.8352e-05, 1.4883e-05, 1.3569e-05, 1.2554e-05, 2.0564e-05,
        1.2723e-05, 2.6812e-05, 2.0564e-05, 1.2474e-05, 2.1982e-05, 1.2420e-05,
        7.3790e-06, 7.7621e-06, 6.3779e-06, 1.1786e-05, 1.1114e-05, 7.4243e-06,
        1.2420e-05, 1.7893e-05, 1.5072e-05, 2.4420e-05, 6.6526e-06, 1.0766e-05,
        2.8041e-05, 3.3713e-05, 2.1429e-05, 9.1257e-06, 2.6844e-05, 1.2420e-05,
        1.4276e-05, 9.4531e-06, 6.1064e-06, 1.5121e-05, 1.9625e-05, 1.7543e-05,
        3.4238e-05, 9.9381e-06, 2.0605e-05, 6.6135e-06, 2.4614e-05, 1.2942e-05,
        2.3086e-05, 2.0841e-05, 9.0013e-06, 7.1652e-06, 1.7269e-05, 1.3755e-05,
        8.4697e-06, 1.3200e-05, 2.6575e-05, 8.2872e-06, 2.3167e-05, 1.9153e-05,
        7.4911e-06, 1.9345e-05, 9.8944e-06, 1.5650e-05, 1.6578e-05, 8.4697e-06,
        2.0841e-05, 8.6736e-06, 1.3755e-05, 1.5894e-05, 1.8679e-05, 6.8123e-06,
        1.2420e-05, 2.3153e-05, 3.8515e-05, 2.2582e-05, 1.0622e-05, 1.8247e-05,
        3.9553e-06, 7.3790e-06, 1.7574e-05, 1.7509e-05, 2.3061e-05, 9.0177e-06,
        1.3569e-05, 1.6799e-05, 1.3614e-05, 1.4417e-05, 3.6122e-05, 1.3726e-05,
        9.7186e-06, 2.3086e-05, 2.5855e-05, 2.3368e-05, 1.7698e-05, 1.4956e-05,
        3.0707e-05, 9.6422e-06, 1.0617e-05, 1.2942e-05, 2.4599e-05, 1.8845e-05,
        1.3334e-05, 1.0617e-05, 9.0888e-06, 1.2751e-05, 2.1335e-05, 3.0183e-05,
        6.4729e-06, 7.5339e-06, 1.7947e-05, 9.7676e-06, 3.1245e-05, 1.1274e-05,
        3.0183e-05, 7.3702e-06, 6.6135e-06, 4.6005e-06, 3.4936e-05, 1.4476e-05,
        9.9708e-06, 3.1327e-05, 1.6522e-05, 2.4696e-05, 1.2569e-05, 1.4479e-05,
        1.4479e-05, 1.2405e-05, 1.0619e-05, 1.4819e-05, 1.5822e-05, 2.4771e-05,
        1.1781e-05, 6.8028e-06, 3.9726e-05, 1.0619e-05, 4.9712e-06, 1.2420e-05,
        1.9930e-05, 3.7036e-05, 3.5932e-05, 8.4096e-06, 1.0622e-05, 9.9381e-06,
        8.4492e-06, 1.7664e-05, 1.4410e-05, 1.6457e-05, 1.0056e-05, 5.0536e-06,
        1.9604e-05, 2.9667e-05, 2.2438e-05, 2.8044e-05, 2.3727e-05, 9.1351e-06,
        9.7936e-06, 2.1312e-05, 3.4238e-05, 7.2243e-06, 2.9500e-05, 1.3755e-05,
        1.2058e-05, 1.2259e-05, 3.9892e-06, 2.1040e-05, 1.0278e-05, 7.9115e-06,
        1.5283e-05, 3.0549e-05, 1.7421e-05, 2.5437e-05, 9.7758e-06, 2.1846e-05,
        3.7535e-05, 3.1217e-05, 1.2512e-05, 1.6457e-05, 2.0463e-05, 2.2668e-05,
        1.3462e-05, 2.7317e-05, 1.3382e-05, 7.7621e-06, 1.1765e-05, 5.0785e-06,
        6.7677e-06, 2.6844e-05, 9.3474e-06, 2.4420e-05, 1.2420e-05, 1.3868e-05,
        1.2058e-05, 2.2175e-05, 1.6774e-05, 2.4179e-05, 1.7824e-05, 1.0607e-05,
        1.4382e-05, 1.7664e-05, 2.1852e-05, 9.2729e-06, 9.6703e-06, 3.0707e-05,
        2.6304e-05, 2.0871e-05, 3.0549e-05, 1.6260e-05, 2.4420e-05, 8.7366e-06,
        1.7974e-05, 9.2729e-06, 1.6876e-05, 1.6352e-05, 2.1600e-05, 1.0619e-05,
        1.8609e-05, 9.5237e-06, 2.3641e-05, 1.0619e-05, 1.0050e-05, 1.7910e-05,
        1.4883e-05, 1.9434e-05, 1.7159e-05, 1.1141e-05, 1.3793e-05, 1.0756e-05,
        9.7676e-06, 1.2569e-05, 2.1082e-05, 9.9717e-06, 9.7263e-06, 1.6457e-05,
        9.0615e-06, 1.5779e-05, 1.3755e-05, 9.0013e-06, 3.0234e-05, 7.0354e-06,
        6.4313e-06, 1.0484e-05, 2.0341e-05, 1.6105e-05, 1.7638e-05, 1.3138e-05,
        1.1798e-05, 1.3569e-05, 1.3200e-05, 1.8609e-05, 1.0799e-06, 1.2420e-05,
        1.0316e-05, 3.0077e-05, 1.7433e-05, 7.3213e-06, 2.4268e-05, 9.4531e-06,
        1.5977e-05, 1.8222e-05, 2.4267e-05, 5.1478e-06, 2.3727e-05, 1.7635e-05,
        1.8638e-05, 2.3270e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.4748e-06, 1.4685e-06, 2.2735e-06,  ..., 1.5901e-06, 3.1247e-06,
        1.2284e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.7973e-06, 4.4240e-06, 2.7196e-06, 7.0605e-06, 8.1982e-06, 6.8531e-06,
        5.5420e-06, 8.7734e-06, 2.3569e-06, 9.4781e-06, 2.8389e-06, 6.5536e-06,
        8.5503e-06, 1.1851e-05, 1.1851e-05, 4.9228e-06, 4.7529e-06, 5.9174e-06,
        9.2533e-06, 6.3286e-06, 5.7791e-06, 5.7134e-06, 6.1157e-06, 4.4742e-06,
        5.0490e-06, 1.1683e-05, 5.7791e-06, 2.2393e-06, 1.6935e-05, 3.8954e-06,
        5.8125e-06, 7.7626e-06, 5.3872e-06, 1.6750e-06, 5.3123e-06, 2.9358e-06,
        2.8483e-06, 2.8462e-06, 3.4282e-06, 3.7768e-06, 3.6197e-06, 9.2524e-06,
        4.4293e-06, 9.2016e-06, 4.1157e-06, 5.1518e-06, 5.5086e-06, 6.6635e-06,
        2.1975e-06, 3.8984e-06, 9.4950e-06, 9.9006e-06, 1.4179e-05, 1.6935e-05,
        4.5268e-06, 4.0596e-06, 1.1457e-05, 2.1736e-06, 1.4981e-05, 5.1227e-06,
        1.0157e-05, 7.7720e-06, 1.0307e-05, 5.6210e-06, 1.9387e-06, 5.8125e-06,
        1.2141e-06, 7.5894e-06, 5.8994e-06, 4.6060e-06, 4.5059e-06, 1.1452e-05,
        5.7069e-06, 3.7320e-06, 7.3396e-06, 6.5956e-06, 8.0230e-06, 2.3049e-06,
        9.7018e-06, 5.0822e-06, 9.1284e-06, 9.4539e-06, 7.1058e-06, 6.2607e-06,
        5.8230e-06, 7.2916e-06, 1.4076e-05, 7.2780e-06, 5.3525e-06, 4.4293e-06,
        5.1226e-06, 1.3751e-05, 9.0538e-06, 1.4438e-05, 1.1851e-05, 5.9552e-06,
        7.0524e-06, 1.3248e-05, 5.0298e-06, 6.0960e-06, 5.9805e-06, 4.4240e-06,
        4.4293e-06, 4.5333e-06, 8.9525e-07, 3.4017e-06, 4.0626e-06, 7.5894e-06,
        2.2217e-06, 3.3920e-06, 6.2210e-06, 1.1033e-05, 1.0728e-05, 3.6703e-06,
        9.9158e-06, 6.8531e-06, 6.1903e-06, 2.4299e-06, 5.1227e-06, 7.5629e-06,
        6.6154e-06, 7.3396e-06, 7.0010e-06, 3.4897e-06, 1.4182e-05, 7.7708e-06,
        8.1982e-06, 2.7196e-06, 9.8298e-06, 4.9776e-06, 4.2087e-06, 4.4748e-06,
        5.8436e-06, 6.6928e-06, 1.1748e-05, 1.1300e-05, 3.4632e-06, 5.0045e-06,
        5.2680e-06, 6.9318e-06, 1.1260e-05, 3.6150e-06, 1.0447e-05, 9.1966e-06,
        2.9782e-06, 1.8804e-06, 5.7791e-06, 8.5052e-06, 1.0305e-05, 6.3527e-06,
        1.6981e-06, 7.4971e-06, 1.8108e-05, 5.7078e-06, 8.2124e-06, 4.2087e-06,
        5.7791e-06, 1.2619e-05, 7.8644e-06, 4.4293e-06, 9.6497e-06, 5.7791e-06,
        6.0035e-06, 5.7791e-06, 6.9318e-06, 7.3798e-06, 1.4179e-05, 5.8125e-06,
        9.8153e-06, 5.3525e-06, 5.8095e-06, 3.3639e-06, 2.6523e-06, 1.6380e-05,
        7.2101e-06, 2.7603e-06, 8.4824e-06, 1.7614e-06, 3.5565e-06, 5.9805e-06,
        9.8298e-06, 3.7320e-06, 1.4179e-05, 4.5748e-06, 7.1058e-06, 5.1494e-06,
        3.0646e-06, 5.2292e-06, 5.0298e-06, 1.2936e-05, 5.7804e-06, 3.4017e-06,
        1.3248e-05, 1.0981e-05, 3.0911e-06, 4.0596e-06, 1.0517e-05, 2.8312e-06,
        5.1150e-06, 8.2139e-06, 1.1300e-05, 3.7320e-06, 2.8998e-06, 4.7477e-06,
        3.6703e-06, 6.5798e-06, 6.7972e-06, 5.8050e-06, 9.3840e-06, 9.2626e-06,
        5.4786e-06, 9.0204e-06, 4.7529e-06, 5.6664e-06, 1.0190e-05, 3.8005e-06,
        1.6954e-05, 5.1304e-06, 1.6935e-05, 4.7618e-06, 5.1124e-06, 7.1098e-06,
        4.5455e-06, 3.4060e-06, 3.4017e-06, 3.4361e-06, 4.2666e-06, 3.4898e-06,
        7.3396e-06, 3.8712e-06, 4.4293e-06, 2.3409e-06, 5.0490e-06, 7.7847e-06,
        3.0805e-06, 6.9759e-06, 4.1083e-06, 2.0640e-06, 8.8559e-06, 4.6277e-06,
        9.4029e-06, 1.4434e-05, 5.8230e-06, 2.7909e-06, 3.4712e-06, 6.2975e-06,
        3.4060e-06, 6.7191e-06, 4.4293e-06, 3.1430e-06, 6.3726e-06, 9.7018e-06,
        5.5596e-06, 8.2310e-06, 3.3474e-06, 4.4293e-06, 7.9262e-06, 6.4132e-06,
        8.5052e-06, 5.3657e-06, 5.2815e-06, 3.5725e-06, 7.3993e-06, 5.7791e-06,
        5.3577e-06, 6.0814e-06, 6.2752e-06, 5.8953e-06, 6.0545e-06, 8.7986e-06,
        4.4240e-06, 2.2526e-06, 9.2161e-06, 5.7791e-06, 8.7065e-06, 4.4293e-06,
        3.2364e-06, 3.5497e-06, 5.7791e-06, 4.5238e-06, 4.1489e-06, 9.2626e-06,
        6.1719e-06, 9.8240e-06, 2.6523e-06, 4.4293e-06, 3.3342e-06, 2.9782e-06,
        8.0230e-06, 7.2916e-06, 1.0181e-05, 4.3133e-06, 8.2283e-06, 6.6635e-06,
        1.3811e-05, 1.6665e-05, 4.5471e-06, 7.4316e-06, 3.4282e-06, 9.0204e-06,
        8.7398e-06, 9.0762e-06, 3.0513e-06, 7.1119e-06, 3.2364e-06, 4.0625e-06,
        3.1672e-06, 7.6758e-06, 4.0635e-06, 3.0512e-06, 4.0625e-06, 8.1035e-06,
        7.5521e-06, 6.3065e-06, 5.7791e-06, 9.8913e-06, 5.1164e-06, 7.5769e-06,
        1.2626e-05, 1.1978e-05, 4.2087e-06, 2.3409e-06, 1.0443e-05, 4.2234e-06,
        2.1736e-06, 3.5963e-06, 8.8910e-07, 9.4539e-06, 5.2971e-06, 4.2087e-06,
        1.9581e-06, 6.2519e-06, 3.6289e-06, 1.0278e-05, 1.1158e-05, 4.4293e-06,
        3.6981e-06, 1.6954e-05, 9.2001e-06, 7.3396e-06, 3.0911e-06, 5.3551e-06,
        4.9419e-06, 5.8157e-06, 4.8262e-06, 6.8531e-06, 5.4940e-06, 4.4293e-06,
        4.3062e-06, 7.1058e-06, 4.2572e-06, 9.8945e-06, 2.4190e-06, 3.0010e-06,
        6.2214e-06, 9.0539e-06, 5.8996e-06, 6.9994e-06, 3.2414e-06, 8.2619e-07,
        1.1528e-05, 1.3368e-05, 5.7069e-06, 9.6309e-06, 2.9358e-06, 9.3153e-06,
        5.7791e-06, 9.8913e-06, 1.0131e-05, 9.8913e-06, 1.2135e-05, 1.6380e-05,
        8.1982e-06, 4.1505e-06, 4.4293e-06, 6.2974e-06, 6.1915e-06, 7.5629e-06,
        5.8994e-06, 2.5332e-06, 4.3062e-06, 4.0596e-06, 5.1054e-06, 3.8091e-06,
        3.4060e-06, 6.9885e-06, 2.8991e-06, 5.7791e-06, 8.9007e-06, 9.7018e-06,
        1.0476e-05, 5.1156e-06, 1.3646e-05, 3.2414e-06, 7.2546e-06, 3.8237e-06,
        6.4122e-06, 8.6748e-06, 5.2238e-06, 7.8644e-06, 5.3657e-06, 5.8994e-06,
        3.4632e-06, 3.8954e-06, 1.1898e-05, 1.0357e-07, 5.7078e-06, 1.1851e-05,
        9.1996e-07, 1.2862e-05, 3.4363e-06, 7.7476e-06, 5.8230e-06, 1.0933e-05,
        1.2909e-05, 1.6526e-06, 6.0041e-06, 5.2548e-06, 4.5333e-06, 6.1268e-06,
        6.2593e-06, 7.4971e-06, 1.0640e-05, 1.6935e-05, 7.4930e-06, 1.4784e-06,
        4.7746e-06, 5.1701e-06, 6.9277e-06, 9.0455e-06, 5.6350e-06, 1.0289e-05,
        1.1851e-05, 7.9280e-06, 1.3350e-05, 2.1496e-06, 3.0274e-06, 2.2716e-06,
        5.0988e-06, 7.2780e-06, 1.1898e-05, 2.3017e-06, 5.1164e-06, 9.8153e-06,
        9.2161e-06, 5.9805e-06, 3.2364e-06, 7.1079e-06, 9.2161e-06, 7.9280e-06,
        1.4179e-05, 6.2210e-06, 2.4843e-06, 7.5629e-06, 7.0606e-06, 1.1552e-06,
        6.5035e-06, 3.1673e-06, 9.5459e-06, 6.3279e-06, 1.8804e-06, 7.7847e-06,
        4.0635e-06, 1.2477e-06, 9.8240e-06, 9.9006e-06, 6.0708e-06, 6.2593e-06,
        5.7791e-06, 2.1868e-06, 5.1494e-06, 5.4569e-06, 3.0237e-06, 5.9536e-06,
        1.3098e-06, 2.3241e-06, 1.0090e-05, 1.1460e-05, 2.3049e-06, 1.1026e-05,
        6.8606e-06, 6.9088e-06, 4.6060e-06, 2.7909e-06, 3.4017e-06, 3.4017e-06,
        6.3279e-06, 8.0229e-06, 3.8984e-06, 2.8378e-06, 4.4293e-06, 7.7720e-06,
        4.9498e-06, 2.1736e-06, 1.6526e-06, 9.4605e-06, 7.2050e-06, 8.7580e-06,
        7.4553e-06, 4.5455e-06, 9.1996e-07, 8.3818e-06, 3.9629e-06, 7.0585e-06,
        1.4784e-06, 6.9250e-06, 2.6523e-06, 6.9318e-06, 6.8334e-06, 6.2974e-06,
        6.6075e-06, 3.4859e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.4447e-06, 1.9412e-05, 1.4588e-05, 4.7532e-06, 1.6477e-05, 8.4181e-06,
        1.4817e-05, 1.1059e-05, 1.5749e-05, 2.0832e-05, 8.9013e-06, 1.8282e-05,
        8.2837e-06, 1.3484e-05, 1.9853e-05, 1.8595e-05, 1.4975e-05, 1.7274e-05,
        1.4392e-05, 1.1649e-05, 4.7532e-06, 1.5994e-05, 2.0733e-05, 2.3506e-05,
        9.2046e-06, 5.1303e-06, 2.5230e-05, 5.4660e-06, 2.2839e-05, 1.3715e-05,
        2.9755e-06, 7.8510e-06, 1.3146e-05, 1.1690e-05, 4.4117e-06, 9.8146e-06,
        6.9289e-07, 1.2771e-05, 1.6120e-05, 1.7910e-05, 1.1985e-05, 8.4528e-06,
        1.5433e-05, 1.1280e-05, 1.7903e-05, 6.4511e-06, 1.6704e-05, 7.7420e-06,
        2.4838e-05, 1.3388e-05, 2.0832e-05, 9.3232e-06, 1.1019e-05, 3.6010e-05,
        1.3484e-05, 1.9853e-05, 7.1426e-06, 2.4424e-05, 6.2718e-06, 1.9797e-05,
        1.2092e-05, 7.9201e-06, 1.6792e-05, 2.4490e-05, 1.8951e-05, 3.1428e-05,
        1.3484e-05, 4.7532e-06, 8.1395e-06, 2.0867e-05, 9.9067e-06, 7.7421e-06,
        2.3200e-05, 1.1985e-05, 1.8499e-05, 2.3637e-05, 1.1788e-05, 1.1963e-05,
        9.0972e-06, 2.7941e-05, 1.1649e-05, 6.3900e-06, 1.8983e-05, 2.6759e-05,
        5.1303e-06, 8.0135e-06, 1.3889e-05, 1.6338e-05, 1.3763e-05, 1.9853e-05,
        8.1395e-06, 1.6986e-05, 2.3206e-05, 1.2879e-05, 1.8282e-05, 1.6822e-05,
        7.6117e-06, 1.7731e-05, 8.7227e-06, 5.4746e-06, 8.1395e-06, 1.0727e-05,
        1.4009e-05, 3.2240e-05, 1.7274e-05, 1.2188e-05, 2.4369e-05, 8.9423e-06,
        1.3763e-05, 3.1199e-05, 1.5994e-05, 7.6117e-06, 8.1395e-06, 1.2356e-05,
        8.9886e-06, 1.6986e-05, 2.7941e-05, 1.1956e-05, 1.2090e-05, 1.3484e-05,
        1.9170e-05, 1.3484e-05, 1.2090e-05, 1.0323e-05, 1.2248e-05, 7.8510e-06,
        2.2115e-05, 2.6893e-05, 8.9438e-06, 1.1649e-05, 1.9853e-05, 2.2409e-05,
        1.5566e-05, 1.0515e-05, 4.7532e-06, 2.0733e-05, 8.7227e-06, 1.0437e-05,
        1.3484e-05, 1.3484e-05, 1.6415e-05, 1.6822e-05, 1.1092e-05, 4.6755e-06,
        1.5319e-05, 9.6513e-06, 2.7262e-05, 2.0832e-05, 1.5831e-05, 2.6720e-05,
        1.2444e-05, 1.4096e-05, 3.0501e-05, 1.5870e-05, 1.9002e-05, 1.6572e-05,
        2.5688e-05, 1.6120e-05, 1.0026e-05, 8.0135e-06, 1.6610e-05, 1.6524e-05,
        5.1303e-06, 1.5443e-05, 2.0832e-05, 1.7123e-05, 1.8282e-05, 1.7274e-05,
        7.7933e-06, 9.4115e-06, 7.5648e-06, 1.0727e-05, 9.5986e-06, 3.3001e-05,
        1.0095e-05, 7.8510e-06, 5.4660e-06, 8.2837e-06, 1.6776e-05, 1.0014e-05,
        8.7227e-06, 8.7340e-06, 1.6572e-05, 1.5831e-05, 2.2296e-05, 1.4698e-05,
        1.0585e-05, 1.6610e-05, 1.1782e-05, 1.4821e-05, 4.7532e-06, 3.1896e-05,
        2.4369e-05, 2.2187e-05, 2.0200e-05, 2.5623e-05, 7.5648e-06, 5.0672e-06,
        2.1010e-05, 3.0578e-05, 2.0832e-05, 2.2746e-05, 1.5831e-05, 1.4783e-05,
        3.1118e-05, 1.2820e-05, 1.6524e-05, 7.1426e-06, 1.4090e-05, 9.2182e-06,
        1.6335e-05, 2.0617e-05, 2.4625e-05, 1.1737e-05, 5.1303e-06, 1.3758e-05,
        6.3474e-06, 8.1949e-06, 1.3567e-05, 1.9170e-05, 2.0028e-05, 8.2837e-06,
        1.4697e-05, 1.3318e-05, 1.8627e-05, 6.4144e-06, 2.4974e-05, 1.7903e-05,
        1.1962e-05, 9.4836e-06, 1.6610e-05, 3.3061e-06, 1.5327e-05, 1.2011e-05,
        2.6673e-05, 1.2733e-05, 2.4464e-05, 9.4115e-06, 1.8499e-05, 1.0059e-05,
        1.1250e-05, 5.2720e-06, 2.6720e-05, 5.4313e-06, 4.0530e-06, 7.3346e-06,
        1.8499e-05, 6.7835e-06, 6.7473e-06, 1.0066e-05, 8.4558e-06, 2.5623e-05,
        1.1904e-05, 2.5623e-05, 2.6676e-05, 3.4756e-06, 6.3467e-06, 8.2982e-06,
        1.0026e-05, 2.7262e-05, 1.7292e-05, 1.7046e-05, 8.2984e-06, 1.7527e-05,
        9.4115e-06, 1.4300e-05, 3.9568e-05, 1.2678e-05, 6.2060e-06, 1.2248e-05,
        1.0625e-05, 1.5120e-05, 1.3341e-05, 1.6524e-05, 1.5278e-05, 1.0488e-05,
        1.3611e-05, 2.5623e-05, 2.6425e-05, 1.1634e-05, 1.6986e-05, 4.9403e-06,
        8.7227e-06, 5.4660e-06, 4.1889e-06, 1.9296e-05, 1.9711e-05, 1.0059e-05,
        1.4783e-05, 1.6572e-05, 2.0133e-05, 1.2292e-05, 2.0206e-05, 8.3005e-06,
        4.1889e-06, 1.0066e-05, 1.4081e-05, 1.4009e-05, 1.5994e-05, 2.0832e-05,
        1.1649e-05, 6.4511e-06, 7.1426e-06, 1.7903e-05, 9.6594e-06, 1.2543e-05,
        1.6480e-05, 1.3715e-05, 2.7245e-05, 7.1426e-06, 6.8574e-06, 1.0059e-05,
        1.6081e-05, 7.8510e-06, 1.5223e-05, 2.6685e-05, 1.0625e-05, 8.8467e-06,
        7.8510e-06, 3.4368e-05, 1.3889e-05, 1.6964e-05, 1.1956e-05, 2.1112e-05,
        4.8460e-06, 1.2283e-05, 1.3484e-05, 2.8761e-05, 1.4814e-05, 1.8282e-05,
        1.0754e-05, 8.2940e-06, 9.5155e-06, 2.2771e-05, 1.8627e-05, 3.1152e-05,
        1.2356e-05, 1.7500e-05, 1.5831e-05, 1.3642e-05, 2.3562e-05, 1.4009e-05,
        9.6594e-06, 9.4753e-06, 1.8727e-05, 8.4181e-06, 1.1248e-05, 9.4555e-06,
        1.3758e-05, 1.6814e-05, 1.6610e-05, 1.5838e-05, 2.7034e-05, 9.7876e-06,
        1.7903e-05, 1.6415e-05, 8.8714e-06, 1.2560e-05, 8.0002e-06, 1.1019e-05,
        9.6513e-06, 9.2046e-06, 1.4783e-05, 1.2099e-05, 2.7245e-05, 1.4400e-05,
        1.4764e-05, 7.8510e-06, 1.1650e-05, 1.3323e-05, 1.5385e-05, 9.4836e-06,
        1.7224e-05, 1.6986e-05, 1.3290e-05, 2.5623e-05, 8.0135e-06, 1.1092e-05,
        1.9853e-05, 2.3637e-05, 1.1956e-05, 8.7340e-06, 2.7941e-05, 1.6814e-05,
        1.0672e-05, 1.1280e-05, 8.0076e-06, 1.9853e-05, 3.2198e-05, 1.7897e-05,
        1.6282e-05, 1.2292e-05, 2.5360e-05, 2.1573e-05, 1.9853e-05, 3.7016e-05,
        1.4595e-05, 3.6010e-05, 1.2968e-05, 1.3889e-05, 1.6338e-05, 1.1366e-05,
        9.3836e-06, 4.7212e-06, 1.4689e-05, 1.3509e-05, 1.3974e-05, 2.4419e-05,
        7.1426e-06, 1.7274e-05, 1.4081e-05, 1.9296e-05, 1.7440e-05, 1.3290e-05,
        1.3426e-05, 8.7227e-06, 9.8146e-06, 5.2229e-06, 2.2390e-05, 9.4836e-06,
        1.3341e-05, 2.2296e-05, 1.3244e-05, 8.8340e-06, 1.9542e-05, 1.2483e-05,
        8.2984e-06, 3.9568e-05, 8.7227e-06, 1.9516e-05, 1.4783e-05, 1.7731e-05,
        1.6822e-05, 1.6610e-05, 1.5119e-05, 5.0208e-06, 2.4429e-05, 2.3665e-05,
        1.2675e-05, 2.0867e-05, 1.6964e-05, 6.7835e-06, 1.6986e-05, 7.8510e-06,
        1.2188e-05, 1.0488e-05, 1.8377e-05, 9.1249e-06, 1.4091e-05, 9.4836e-06,
        1.1737e-05, 1.4634e-05, 1.1092e-05, 1.9853e-05, 1.1985e-05, 1.8367e-05,
        1.6610e-05, 6.6373e-06, 1.7046e-05, 1.0845e-05, 1.2495e-05, 1.9135e-05,
        1.3290e-05, 3.1806e-05, 1.6179e-05, 2.7067e-05, 1.2341e-05, 1.4081e-05,
        1.4118e-05, 1.4052e-05, 2.8157e-06, 2.0153e-05, 1.3567e-05, 2.2066e-05,
        5.1303e-06, 1.6822e-05, 1.3484e-05, 1.0103e-05, 1.1082e-05, 2.0302e-05,
        1.4090e-05, 1.8440e-05, 2.6928e-05, 1.2463e-05, 2.8833e-05, 6.2916e-06,
        1.1904e-05, 1.9516e-05, 8.7177e-06, 5.2582e-06, 1.7628e-05, 4.1007e-06,
        1.6822e-05, 4.9269e-06, 1.6056e-05, 1.2463e-05, 1.8515e-05, 4.7532e-06,
        1.1196e-05, 2.0948e-05, 1.2924e-05, 1.9135e-05, 1.1985e-05, 2.0832e-05,
        1.5595e-05, 1.1510e-05, 4.6567e-06, 7.5512e-06, 2.2746e-05, 1.6822e-05,
        1.7903e-05, 3.5906e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7065e-06, 1.8150e-06, 2.5717e-07,  ..., 7.7653e-07, 5.2363e-06,
        1.3523e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2140, 1.3909, 1.2476, 1.2996, 1.3584, 1.5411, 1.5785, 1.4580, 1.5694,
        1.9561, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.5, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375]

 sparsity of   [1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.694444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 0.7152777910232544, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6805555820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6979166865348816, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.8125, 0.796875, 0.8125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.78125, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 0.796875, 1.0, 0.796875, 1.0, 1.0, 0.796875, 0.796875, 1.0, 0.78125, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.796875, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.796875, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.8125, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.796875, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.796875, 0.78125, 0.78125, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.8125, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 0.828125, 1.0, 0.78125, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.78125, 1.0, 0.796875, 1.0, 0.8125, 0.796875, 0.8125, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 0.78125, 0.796875, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 0.78125, 0.796875, 0.8125, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.8125, 1.0, 0.78125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0]

 sparsity of   [1.0, 0.484375, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 1.0, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.515625, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 0.5, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.5, 0.484375, 1.0, 0.5, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.5, 1.0, 0.484375, 0.484375, 0.484375, 0.5, 0.484375, 0.484375, 0.5, 0.484375, 0.5, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.515625, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.53125, 0.484375, 0.53125, 1.0, 0.5, 1.0, 0.484375, 1.0, 0.484375, 0.515625, 0.5, 0.484375, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 1.0, 1.0, 0.5, 0.5, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 0.484375, 0.5, 0.546875, 0.484375, 0.484375, 0.515625, 0.5, 1.0, 0.5, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.5, 1.0, 0.5, 0.5, 0.484375, 0.484375, 0.484375, 0.5625, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.515625, 0.5, 0.484375, 0.5, 0.515625, 1.0, 0.484375, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.484375, 0.5, 0.5, 0.484375, 1.0, 0.484375, 0.5, 0.484375, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 0.5, 0.484375, 1.0, 0.484375, 1.0, 0.484375, 1.0]

 sparsity of   [0.3046875, 1.0, 1.0, 0.3125, 1.0, 0.31640625, 0.3125, 0.30078125, 0.30859375, 1.0, 1.0, 1.0, 1.0, 0.3046875, 1.0, 0.3125, 0.30859375, 0.28515625, 1.0, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 1.0, 0.29296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.7951388955116272, 0.796875, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 0.7986111044883728, 1.0, 0.796875, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 0.7986111044883728, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.703125, 0.71875, 1.0, 0.71875, 0.71875, 0.703125, 1.0, 0.703125, 0.734375, 0.703125, 0.71875, 1.0, 0.71875, 0.703125, 1.0, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.71875, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.734375, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.703125, 1.0, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.734375, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.71875, 0.75, 1.0, 0.71875, 0.71875, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.671875, 0.734375, 0.734375, 0.703125, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.703125, 0.71875, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.71875, 1.0, 0.703125, 0.703125, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.6875, 0.734375, 0.703125, 1.0, 1.0, 0.6875, 0.734375, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.71875, 1.0, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.6875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.71875, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.75, 0.703125, 1.0, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.6875, 0.703125, 0.71875, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0]

 sparsity of   [0.203125, 1.0, 1.0, 0.171875, 0.171875, 1.0, 0.17578125, 1.0, 0.1796875, 1.0, 1.0, 0.1796875, 0.16796875, 1.0, 1.0, 0.19921875, 0.171875, 0.1796875, 1.0, 0.21484375, 1.0, 1.0, 1.0, 0.19921875, 1.0, 0.17578125, 0.17578125, 0.2109375, 0.16796875, 1.0, 1.0, 1.0, 0.21875, 1.0, 1.0, 1.0, 0.21484375, 0.20703125, 0.18359375, 0.171875, 0.171875, 0.16796875, 0.1796875, 0.171875, 0.17578125, 1.0, 1.0, 1.0, 1.0, 0.16796875, 0.1796875, 0.19921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.203125, 1.0, 0.171875, 1.0, 0.20703125, 1.0]

 sparsity of   [1.0, 0.4930555522441864, 1.0, 0.5052083134651184, 1.0, 0.5086805820465088, 0.4739583432674408, 1.0, 1.0, 0.484375, 1.0, 1.0, 0.5086805820465088, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 0.4982638955116272, 1.0, 1.0, 0.506944477558136, 0.5017361044883728, 1.0, 1.0, 1.0, 0.506944477558136, 0.4982638955116272, 1.0, 1.0, 0.5, 1.0, 0.4965277910232544, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4930555522441864, 0.4930555522441864, 1.0, 0.5017361044883728, 0.5104166865348816, 0.506944477558136, 0.4930555522441864, 0.4982638955116272, 1.0, 1.0, 1.0, 0.4965277910232544, 0.5034722089767456, 1.0, 0.4965277910232544, 0.4930555522441864, 0.4930555522441864, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.546875, 0.546875, 0.5625, 1.0, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.5625, 0.546875, 0.53125, 0.5625, 0.546875, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.5625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.546875, 0.546875, 1.0, 1.0, 0.546875, 0.53125, 1.0, 0.53125, 0.546875, 0.53125, 0.53125, 0.53125, 1.0, 0.5625, 0.546875, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 0.546875, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.546875, 0.546875, 0.546875, 0.546875, 0.53125, 1.0, 0.53125, 1.0, 0.578125, 0.546875, 0.546875, 0.546875, 1.0, 0.53125, 0.546875, 0.53125, 0.53125, 0.546875, 0.53125, 0.546875, 1.0, 0.5625, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.5625, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 0.53125, 0.546875, 1.0, 0.5625, 0.546875, 0.578125, 0.53125, 0.5625, 1.0, 0.53125, 0.546875, 0.546875, 0.546875, 0.546875, 0.53125, 0.53125, 0.5625, 0.53125, 0.546875, 0.53125, 0.515625, 0.515625, 0.5625, 1.0, 1.0, 0.546875, 1.0, 0.53125, 1.0, 0.546875, 0.53125, 0.5625, 0.53125, 0.546875, 1.0, 0.578125, 0.53125, 1.0, 1.0, 0.5625, 0.53125, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.546875, 1.0, 0.546875, 0.546875, 1.0, 0.5625, 1.0, 0.5625, 0.546875, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 0.546875, 0.53125, 0.546875, 0.5625, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.5625, 0.5625, 1.0, 0.546875, 1.0, 0.546875, 0.53125, 1.0, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 1.0, 1.0, 0.515625, 0.5625, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.53125, 0.546875, 1.0, 0.546875, 0.5625, 0.546875, 0.546875, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 1.0, 0.546875, 0.53125, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.5625, 1.0, 0.53125, 0.546875, 1.0, 0.546875, 0.5625, 0.546875, 0.5625, 0.546875, 0.546875, 0.546875, 1.0, 0.546875, 1.0, 0.53125, 0.53125, 0.546875, 0.546875, 0.53125]

 sparsity of   [0.046875, 0.0546875, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.05859375, 0.04296875, 1.0, 0.04296875, 1.0, 1.0, 0.04296875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 0.04296875, 0.05078125, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.046875, 1.0, 1.0, 0.05859375, 0.04296875, 1.0, 1.0, 0.03515625, 1.0, 0.046875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.0546875, 0.03515625, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 0.04296875, 1.0, 1.0, 0.0390625, 0.05859375, 0.0390625, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 0.04296875, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.03125, 1.0, 1.0, 1.0, 0.0390625, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 0.03515625, 0.03125, 0.04296875, 1.0, 1.0, 0.03515625, 0.0390625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.6015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6032986044883728, 1.0, 1.0, 0.6032986044883728, 0.5980902910232544, 0.6032986044883728, 1.0, 1.0, 0.6015625, 1.0, 0.5998263955116272, 1.0, 1.0, 0.5980902910232544, 0.5989583134651184, 1.0, 1.0, 0.5980902910232544, 0.6015625, 0.5954861044883728, 1.0, 0.600694477558136, 0.6015625, 1.0, 0.6032986044883728, 1.0, 0.6015625, 1.0, 1.0, 1.0, 1.0, 0.5972222089767456, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 0.600694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5989583134651184, 0.600694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5998263955116272, 0.5980902910232544, 1.0, 0.5989583134651184, 1.0, 0.5998263955116272, 0.5963541865348816, 0.5989583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59375, 1.0, 1.0, 0.59375, 0.5980902910232544, 0.6059027910232544, 1.0, 0.5963541865348816, 1.0, 0.5972222089767456, 1.0, 0.5963541865348816, 1.0, 1.0, 0.5980902910232544, 0.6015625, 0.6059027910232544, 1.0, 1.0, 1.0, 0.5989583134651184, 1.0, 0.6015625, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 0.6032986044883728, 1.0, 1.0, 0.5980902910232544, 0.5989583134651184, 1.0, 1.0, 1.0, 1.0, 0.5998263955116272, 1.0, 1.0, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 1.0, 0.65625, 0.640625, 0.640625, 1.0, 0.6484375, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.65625, 0.640625, 0.640625, 0.65625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 0.6484375, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.65625, 1.0, 0.640625, 0.65625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 0.65625, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 0.640625, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 0.640625, 0.6484375, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.6640625, 0.640625, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 0.640625, 1.0, 0.640625, 0.6484375, 0.640625, 0.6484375, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.6484375, 1.0, 0.65625, 1.0, 0.640625, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 1.0]

 sparsity of   [0.046875, 1.0, 0.05078125, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 0.05078125, 0.0546875, 0.0859375, 1.0, 0.0546875, 0.04296875, 0.109375, 1.0, 0.046875, 0.0390625, 1.0, 0.109375, 0.234375, 0.046875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.05078125, 1.0, 0.05078125, 0.05078125, 0.1484375, 1.0, 0.05078125, 1.0, 0.04296875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.1328125, 0.08984375, 0.11328125, 0.0625, 1.0, 1.0, 0.171875, 1.0, 0.046875, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.05078125, 1.0, 0.0625, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.046875, 0.046875, 1.0, 0.046875, 0.046875, 0.11328125, 0.125, 0.05078125, 1.0, 0.11328125, 0.07421875, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.04296875, 0.04296875, 0.140625, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.04296875, 1.0, 0.05859375, 0.05078125, 1.0, 0.046875, 0.07421875, 0.0390625, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.04296875, 0.27734375, 0.05078125, 0.0625, 0.0859375, 0.046875, 0.05078125, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 0.04296875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.04296875, 0.046875, 1.0, 0.0546875, 0.04296875, 0.046875, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 0.0703125, 0.0625, 0.08984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.04296875, 1.0, 0.10546875, 0.05078125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 0.0390625, 1.0, 1.0, 0.046875, 0.046875, 0.07421875, 1.0, 0.046875, 0.0546875, 0.05078125, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.04296875, 0.09765625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.2265625, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.1328125, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 0.125, 0.0546875, 0.046875, 0.05078125, 0.078125, 1.0, 0.09765625, 0.04296875, 0.12890625, 0.04296875, 0.04296875, 0.046875, 0.05859375, 0.0546875, 1.0, 0.05078125, 0.046875, 1.0, 0.109375, 0.05078125, 0.046875, 1.0, 0.12890625, 1.0, 0.05078125, 0.04296875, 0.15234375, 1.0, 1.0, 1.0, 0.12890625, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.04296875, 0.0390625, 0.04296875, 0.04296875, 1.0, 1.0, 0.04296875, 1.0, 0.22265625, 1.0, 1.0, 1.0, 1.0, 0.06640625, 0.046875, 0.0390625, 1.0, 0.05078125, 1.0, 0.0546875, 1.0, 0.04296875, 0.26953125, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.05859375, 1.0, 0.28125, 1.0, 0.1796875, 0.04296875, 0.04296875, 1.0, 0.06640625, 0.08984375, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.04296875, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.1015625, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.046875, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.0546875, 0.19921875, 0.078125, 0.12109375, 0.05078125, 0.046875, 1.0, 1.0, 0.046875, 0.04296875, 0.046875, 0.046875, 0.046875, 1.0, 0.05859375, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.0390625, 0.1015625, 0.05859375, 1.0, 0.2890625, 1.0, 1.0, 0.15234375, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.04296875, 0.0390625, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.046875, 1.0, 0.05859375, 0.0859375, 1.0, 0.04296875, 1.0, 0.07421875, 1.0, 1.0, 0.046875, 0.26953125, 1.0, 1.0, 1.0, 0.09765625, 0.16796875, 0.2578125, 1.0, 1.0, 1.0, 0.0625, 0.08984375, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 0.0390625, 0.0390625, 0.05078125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.09765625, 0.05078125, 0.05859375, 0.1484375, 1.0, 0.04296875, 1.0, 0.0390625, 1.0, 1.0, 0.11328125, 1.0, 0.0546875, 0.05078125, 0.05078125, 0.05078125, 0.0546875, 0.0625, 0.046875, 0.046875, 0.05078125, 1.0, 0.046875, 0.046875, 0.04296875, 0.1328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.04296875, 1.0, 0.08203125, 0.16796875, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 1.0, 1.0, 1.0, 0.0390625, 0.0390625, 0.05078125, 1.0, 0.21484375, 1.0, 0.08984375, 1.0, 0.046875, 0.046875, 1.0, 0.046875, 0.05859375, 0.0390625, 0.05859375, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.0625, 1.0]

 sparsity of   [1.0, 0.4765625, 0.462890625, 0.458984375, 1.0, 1.0, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 0.458984375, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.470703125, 1.0, 1.0, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 0.470703125, 1.0, 0.462890625, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.478515625, 1.0, 1.0, 0.4609375, 0.462890625, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.466796875, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4609375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 0.45703125, 0.46484375, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 0.4609375, 0.462890625, 0.458984375, 1.0, 1.0, 0.466796875, 0.453125, 0.45703125, 0.4609375, 1.0, 0.4609375, 1.0, 1.0, 0.482421875, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45703125, 0.458984375, 0.4609375, 0.458984375, 1.0, 1.0, 1.0, 1.0, 0.46484375, 1.0, 1.0, 1.0, 0.458984375, 0.462890625, 0.46484375, 1.0, 0.458984375, 0.462890625]

 sparsity of   [1.0, 1.0, 0.6336805820465088, 0.6328125, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 0.6145833134651184, 0.6302083134651184, 1.0, 0.6302083134651184, 0.6310763955116272, 1.0, 0.6293402910232544, 0.6345486044883728, 1.0, 1.0, 1.0, 0.6102430820465088, 0.6293402910232544, 0.6215277910232544, 0.6276041865348816, 1.0, 0.6362847089767456, 1.0, 1.0, 0.6258680820465088, 1.0, 0.6354166865348816, 1.0, 0.6232638955116272, 0.6310763955116272, 1.0, 0.6137152910232544, 1.0, 0.6223958134651184, 0.6302083134651184, 0.6102430820465088, 1.0, 0.6232638955116272, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 0.6215277910232544, 1.0, 0.6302083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6206597089767456, 1.0, 0.6354166865348816, 0.6215277910232544, 0.6232638955116272, 1.0, 1.0, 0.6102430820465088, 1.0, 0.6206597089767456, 1.0, 1.0, 1.0, 0.6145833134651184, 1.0, 0.6232638955116272, 1.0, 0.6215277910232544, 1.0, 0.6059027910232544, 1.0, 0.6206597089767456, 0.608506977558136, 1.0, 1.0, 1.0, 0.6145833134651184, 0.6310763955116272, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 0.625, 0.631944477558136, 0.6206597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.6354166865348816, 0.6302083134651184, 0.6293402910232544, 1.0, 0.6284722089767456, 0.6302083134651184, 0.624131977558136, 1.0, 1.0, 0.6267361044883728, 1.0, 0.6293402910232544, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.625, 0.6276041865348816, 0.6284722089767456, 1.0, 1.0, 0.624131977558136, 0.6171875, 1.0]

 sparsity of   [0.515625, 1.0, 0.5234375, 0.5078125, 1.0, 0.5390625, 0.53125, 0.5390625, 0.515625, 1.0, 0.515625, 1.0, 0.546875, 0.53125, 0.5234375, 0.515625, 0.5078125, 0.515625, 0.53125, 0.5390625, 1.0, 0.515625, 0.5234375, 0.5234375, 0.515625, 1.0, 0.515625, 0.5234375, 0.5078125, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.5234375, 0.5078125, 0.515625, 1.0, 0.5078125, 0.5078125, 0.515625, 0.5234375, 0.5234375, 1.0, 0.515625, 0.515625, 0.53125, 0.5078125, 0.5078125, 0.53125, 0.546875, 0.515625, 0.515625, 0.5234375, 0.53125, 0.53125, 0.515625, 0.515625, 1.0, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.5234375, 0.5234375, 0.5234375, 0.5078125, 1.0, 0.5390625, 0.5234375, 0.515625, 1.0, 0.515625, 0.5234375, 1.0, 0.5234375, 0.5078125, 0.5, 0.515625, 0.53125, 1.0, 0.53125, 0.5234375, 0.5390625, 0.515625, 1.0, 1.0, 0.5390625, 0.53125, 0.515625, 0.5234375, 0.515625, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.515625, 0.5234375, 0.5, 0.53125, 0.53125, 0.515625, 0.53125, 0.515625, 0.5, 0.5234375, 0.5234375, 0.5234375, 1.0, 0.5390625, 1.0, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.53125, 1.0, 0.5390625, 0.5078125, 1.0, 1.0, 0.5234375, 0.5, 0.515625, 0.515625, 0.5234375, 0.5078125, 0.5234375, 1.0, 0.5078125, 0.515625, 0.5078125, 0.515625, 0.53125, 0.515625, 0.5234375, 1.0, 0.5078125, 0.5234375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.5546875, 0.5234375, 0.5078125, 0.5234375, 0.53125, 0.5546875, 0.5234375, 1.0, 0.5234375, 1.0, 1.0, 0.5078125, 0.515625, 0.5390625, 0.53125, 0.515625, 0.515625, 0.4921875, 1.0, 1.0, 0.515625, 1.0, 0.5234375, 0.515625, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.515625, 0.5234375, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 0.5234375, 0.5390625, 1.0, 0.515625, 0.515625, 0.5390625, 0.5234375, 0.5078125, 0.5234375, 0.53125, 0.53125, 0.5390625, 0.5234375, 1.0, 1.0, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.515625, 1.0, 1.0, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 1.0, 0.5234375, 1.0, 0.5, 0.5234375, 0.515625, 1.0, 0.515625, 1.0, 0.53125, 0.5234375, 0.53125, 1.0, 0.515625, 0.515625, 1.0, 0.53125, 0.515625, 0.515625, 0.515625, 1.0, 0.515625, 0.5078125, 0.515625, 0.515625, 0.515625, 0.5078125, 0.5390625, 1.0, 0.5078125, 0.53125, 0.515625, 0.515625, 0.515625, 0.5078125, 0.515625, 0.5234375, 0.546875, 0.515625, 0.53125, 0.515625, 0.53125, 0.5, 0.515625, 1.0, 0.5390625, 1.0, 0.5078125, 0.53125, 0.5390625, 1.0, 0.640625, 1.0, 0.53125, 0.5234375, 0.5234375, 0.515625, 0.5234375, 0.5234375, 1.0, 0.5234375, 0.515625, 0.515625, 0.515625, 0.5078125, 0.515625, 0.53125, 0.5390625, 0.5, 0.5234375, 0.5234375, 0.515625, 0.515625, 1.0, 0.5234375, 0.53125, 0.515625, 0.515625, 0.515625, 1.0, 0.515625, 0.5234375, 0.5234375, 0.5, 1.0, 0.5234375, 1.0, 0.5234375, 0.5, 0.5, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.5078125, 0.515625, 0.5234375, 0.515625, 0.515625, 0.5234375, 1.0, 0.53125, 1.0, 0.53125, 0.515625, 1.0, 0.5234375, 0.5234375, 0.515625, 0.515625, 1.0, 1.0, 1.0, 0.515625, 0.5234375, 1.0, 0.5390625, 0.515625, 0.53125, 0.5078125, 0.53125, 0.53125, 0.515625, 0.5390625, 0.5703125, 0.5234375, 1.0, 0.53125, 0.515625, 0.5234375, 0.5234375, 0.5234375, 0.5078125, 0.515625, 0.5234375, 1.0, 0.515625, 0.546875, 0.515625, 0.53125, 0.53125, 0.5234375, 0.5078125, 1.0, 1.0, 0.5078125, 0.53125, 0.515625, 1.0, 0.5390625, 1.0, 0.5234375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.5390625, 0.5078125, 1.0, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.515625, 1.0, 0.5234375, 0.5234375, 1.0, 1.0, 0.53125, 0.515625, 0.515625, 0.53125, 0.53125, 0.5078125, 0.53125, 0.53125, 0.5234375, 0.515625, 0.53125, 0.515625, 0.5234375, 0.5234375, 0.515625, 0.5234375, 1.0, 1.0, 0.515625, 1.0, 0.5078125, 0.53125, 1.0, 1.0, 1.0, 0.546875, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 0.5078125, 0.5078125, 0.5234375, 1.0, 1.0, 1.0, 0.5078125, 0.515625, 0.5234375, 0.515625, 1.0, 0.53125, 0.53125, 1.0, 1.0, 0.5234375, 0.5390625, 0.5546875, 0.5234375, 0.53125, 0.515625, 0.515625, 1.0, 0.515625, 0.5390625, 0.515625, 0.5234375, 1.0, 0.5390625, 1.0, 0.5234375, 0.53125, 0.5234375, 0.5234375, 0.546875, 0.515625, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.515625, 0.53125, 1.0, 0.5078125, 0.515625, 0.5078125, 0.515625, 0.53125, 1.0, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.515625, 1.0, 0.515625, 0.5078125, 0.5390625, 1.0, 1.0, 0.5078125, 0.5390625, 0.53125, 0.5078125, 1.0, 0.53125, 0.5234375, 0.5234375, 0.5234375, 0.5234375, 0.5234375, 1.0, 0.515625, 0.515625, 0.5390625, 0.53125, 1.0, 0.5, 1.0, 0.53125, 0.5078125, 1.0, 0.5234375, 0.515625]

 sparsity of   [0.20703125, 0.197265625, 0.2109375, 1.0, 0.19921875, 0.19921875, 0.201171875, 1.0, 0.20703125, 0.20703125, 0.201171875, 0.201171875, 1.0, 1.0, 0.208984375, 0.203125, 1.0, 0.201171875, 1.0, 0.20703125, 0.19921875, 0.19921875, 0.205078125, 1.0, 0.201171875, 0.203125, 1.0, 1.0, 1.0, 0.203125, 0.2109375, 0.19921875, 0.201171875, 0.208984375, 0.197265625, 0.20703125, 1.0, 1.0, 0.208984375, 1.0, 0.201171875, 0.203125, 0.203125, 0.203125, 0.19921875, 0.1953125, 1.0, 0.20703125, 1.0, 1.0, 0.205078125, 0.197265625, 0.197265625, 0.197265625, 0.28515625, 0.201171875, 1.0, 0.201171875, 0.19921875, 0.193359375, 1.0, 0.205078125, 0.201171875, 0.212890625, 1.0, 1.0, 1.0, 1.0, 0.201171875, 0.205078125, 1.0, 0.197265625, 0.205078125, 1.0, 1.0, 1.0, 0.19921875, 0.205078125, 1.0, 0.19921875, 1.0, 0.203125, 0.208984375, 0.201171875, 0.203125, 0.201171875, 0.19921875, 1.0, 0.197265625, 1.0, 0.197265625, 0.197265625, 0.203125, 1.0, 0.19921875, 1.0, 0.203125, 1.0, 0.197265625, 0.201171875, 0.205078125, 0.201171875, 1.0, 0.197265625, 1.0, 0.201171875, 0.197265625, 0.19921875, 0.205078125, 1.0, 0.19921875, 0.193359375, 1.0, 1.0, 1.0, 1.0, 0.203125, 1.0, 0.203125, 1.0, 0.197265625, 0.1953125, 0.1953125, 0.205078125, 0.197265625, 1.0, 0.19921875, 0.197265625]

 sparsity of   [1.0, 0.3263888955116272, 1.0, 1.0, 0.328125, 1.0, 1.0, 0.3307291567325592, 0.323784738779068, 1.0, 1.0, 1.0, 1.0, 0.3307291567325592, 1.0, 1.0, 0.3263888955116272, 0.3255208432674408, 0.3263888955116272, 1.0, 0.3324652910232544, 0.3307291567325592, 1.0, 1.0, 1.0, 1.0, 0.3289930522441864, 0.3255208432674408, 0.323784738779068, 1.0, 0.3255208432674408, 1.0, 0.3289930522441864, 0.328125, 1.0, 1.0, 1.0, 0.3263888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3298611044883728, 0.3298611044883728, 1.0, 1.0, 0.3289930522441864, 1.0, 1.0, 0.3263888955116272, 1.0, 0.323784738779068, 0.3272569477558136, 0.3255208432674408, 1.0, 0.3246527910232544, 0.3272569477558136, 1.0, 1.0, 0.3263888955116272, 1.0, 1.0, 0.3289930522441864, 1.0, 0.3289930522441864, 1.0, 1.0, 0.3255208432674408, 0.323784738779068, 0.3272569477558136, 0.3289930522441864, 0.328125, 1.0, 0.3298611044883728, 1.0, 0.3246527910232544, 1.0, 0.3289930522441864, 0.3263888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3246527910232544, 1.0, 0.3263888955116272, 1.0, 0.3298611044883728, 1.0, 0.3289930522441864, 1.0, 0.3298611044883728, 0.3272569477558136, 1.0, 1.0, 0.3255208432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3298611044883728, 0.3255208432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3246527910232544, 1.0, 1.0, 0.3263888955116272, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.3246527910232544, 0.323784738779068, 1.0]

 sparsity of   [0.6015625, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 1.0, 1.0, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.6015625, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 1.0, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.609375, 1.0, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.6015625, 0.6015625, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 1.0, 0.6015625, 0.6015625, 1.0, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.609375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 1.0, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.6015625, 0.609375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 0.6015625, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.6015625, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375]

 sparsity of   [0.16015625, 0.158203125, 0.158203125, 0.15625, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.15625, 1.0, 0.158203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.16015625, 1.0, 1.0, 0.154296875, 1.0, 1.0, 0.154296875, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 1.0, 0.158203125, 0.1640625, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.16015625, 0.158203125, 1.0, 1.0, 0.158203125, 0.15234375, 0.1640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.162109375, 0.154296875, 1.0, 1.0, 1.0, 0.1640625, 1.0, 0.154296875, 1.0, 0.1640625, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16015625, 1.0, 0.158203125, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.154296875, 1.0, 0.15234375, 0.1640625, 1.0, 0.162109375, 1.0, 1.0, 1.0, 0.166015625, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.166015625, 1.0, 1.0, 1.0, 1.0, 0.162109375, 1.0, 0.162109375, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 0.16015625]

 sparsity of   [1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 0.6380208134651184, 1.0, 0.6371527910232544, 1.0, 0.6267361044883728, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631944477558136, 1.0, 1.0, 0.639756977558136, 1.0, 0.631944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6432291865348816, 1.0, 1.0, 1.0, 1.0, 0.6336805820465088, 1.0, 1.0, 0.6328125, 0.639756977558136, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 0.6371527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 0.6432291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.639756977558136, 1.0, 0.6267361044883728, 0.6432291865348816, 1.0, 1.0, 0.631944477558136, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.639756977558136, 0.6284722089767456, 1.0, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6432291865348816, 1.0, 0.6302083134651184, 0.6371527910232544, 1.0, 1.0, 0.6423611044883728, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.78125, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.765625, 0.765625, 0.765625, 0.7578125, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.765625, 0.765625, 0.7578125, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7734375, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.765625, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7734375, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.765625, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125]

 sparsity of   [0.150390625, 1.0, 0.1484375, 0.14453125, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.14453125, 0.1328125, 1.0, 0.146484375, 0.140625, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.146484375, 0.1328125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.142578125, 0.140625, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.140625, 0.140625, 1.0, 1.0, 0.146484375, 1.0, 0.140625, 0.142578125, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 0.142578125, 1.0, 0.146484375, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.140625, 0.142578125, 0.140625, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.140625, 0.146484375, 0.146484375, 0.146484375, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.142578125, 0.146484375, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.1484375, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.1484375, 0.142578125, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 0.140625, 0.14453125, 0.142578125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.134765625, 0.140625, 1.0, 1.0, 0.146484375, 0.14453125, 1.0, 0.150390625, 1.0, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.138671875, 0.140625, 1.0, 1.0, 0.13671875, 0.142578125, 1.0, 0.14453125, 1.0, 0.140625, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.14453125, 0.140625, 1.0, 1.0, 0.138671875, 0.142578125, 0.14453125, 0.142578125, 0.140625, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.140625, 0.14453125, 1.0, 1.0, 0.14453125, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.140625, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.13671875, 1.0, 0.15234375, 0.140625, 0.138671875, 0.13671875, 0.14453125, 0.142578125, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.14453125, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.14453125, 0.140625, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.13671875, 1.0, 1.0, 0.13671875, 0.14453125, 0.142578125, 0.134765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.146484375, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0]

 sparsity of   [0.5590277910232544, 1.0, 1.0, 0.5590277910232544, 1.0, 0.5590277910232544, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5607638955116272, 1.0, 1.0, 0.5581597089767456, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 0.5603298544883728, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 0.5603298544883728, 1.0, 0.5625, 1.0, 1.0, 0.5598958134651184, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 0.5603298544883728, 1.0, 0.5633680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 0.5620659589767456, 0.5581597089767456, 1.0, 1.0, 0.5607638955116272, 0.5625, 1.0, 0.5594618320465088, 1.0, 1.0, 0.557725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 0.5581597089767456, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5607638955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 1.0, 0.55859375, 0.5594618320465088, 0.5603298544883728, 1.0, 0.561631977558136, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 0.5603298544883728, 1.0, 0.5620659589767456, 1.0, 1.0, 1.0, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.561631977558136, 1.0, 1.0, 1.0, 1.0, 0.5620659589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5603298544883728, 0.5625, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.561631977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.77734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.79296875, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.77734375, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.77734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.77734375, 0.7734375, 1.0, 1.0, 0.77734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 0.78125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.15234375, 0.14453125, 1.0, 0.154296875, 0.142578125, 1.0, 0.1484375, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 1.0, 0.14453125, 0.14453125, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.138671875, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 0.333984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 1.0, 1.0, 0.15234375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.15234375, 0.154296875, 1.0, 0.150390625, 1.0, 0.150390625, 0.150390625, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.150390625, 0.15234375, 1.0, 1.0, 1.0, 0.154296875, 1.0, 0.1484375, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.15234375, 0.146484375, 1.0, 0.1484375, 0.15234375, 0.1484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.158203125, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.162109375, 1.0, 0.15234375, 0.15234375, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.146484375, 0.154296875, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.146484375, 0.146484375, 0.150390625, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.236328125, 1.0, 1.0, 0.189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.150390625, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.138671875, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.15234375, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.150390625, 0.1484375, 0.150390625, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.146484375, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.150390625, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.1640625, 1.0, 1.0, 1.0, 0.1953125, 0.140625, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.146484375, 1.0, 0.142578125, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 0.15625, 1.0, 1.0, 1.0, 0.142578125, 0.1484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.142578125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 0.1484375, 0.15234375, 1.0, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 0.146484375, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.169921875, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.150390625, 1.0, 1.0, 0.14453125, 1.0, 0.1484375, 1.0, 0.150390625, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.14453125, 0.15234375, 0.150390625, 1.0, 1.0, 0.1484375, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.1484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 0.1484375, 0.15625, 1.0, 1.0, 0.15625, 0.140625, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.1484375, 0.142578125, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.1484375, 0.154296875, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 0.1640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.291015625, 0.1484375, 0.15625, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 0.146484375, 0.169921875, 1.0, 0.154296875, 0.15625, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.150390625, 0.154296875, 0.1484375, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.14453125, 0.154296875, 1.0, 1.0, 1.0, 0.1484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 0.1484375, 1.0, 1.0, 0.146484375, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.158203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.716796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7236328125, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.71484375, 0.720703125, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 0.7197265625, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7216796875, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.909288227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.909288227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.905381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 0.9105902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9066840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.905381977558136, 1.0, 1.0, 1.0, 0.9066840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9071180820465088, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9032118320465088, 1.0, 1.0, 0.9071180820465088, 1.0, 0.9066840410232544]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 0.91796875, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 0.9140625, 0.921875, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 0.91796875, 0.921875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.921875, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 0.921875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 0.921875, 0.921875, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 0.91796875, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.9140625, 0.91796875, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.705078125, 0.7041015625, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 0.7158203125, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 0.69921875, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71484375, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 0.7041015625, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 0.705078125, 0.7021484375, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7080078125, 1.0, 1.0, 0.7041015625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 0.701171875, 1.0, 0.7060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 0.7001953125, 1.0, 1.0, 0.7099609375, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.7021484375, 0.7041015625, 0.703125, 1.0, 0.7041015625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8103298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.88671875, 0.8828125, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.8828125, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.8828125, 0.8828125, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.890625, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 0.6962890625, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 0.6962890625, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 0.697265625, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 0.6962890625, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.6962890625, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 0.693359375, 1.0, 1.0, 0.697265625, 0.697265625, 0.6962890625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 0.6904296875, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 0.6982421875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831163227558136, 1.0, 1.0, 0.8333333134651184, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 0.831163227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8337673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8315972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8342013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 0.8328993320465088, 0.83203125, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831163227558136, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 0.88671875, 0.890625, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.890625, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6923828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 0.6982421875, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6923828125, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.603515625, 0.60205078125, 0.60205078125, 0.6025390625, 0.6005859375, 0.603515625, 0.60205078125, 0.60205078125, 0.60107421875, 0.60205078125, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375]

Total parameter pruned: 22962742.0038929 (unstructured) 21964741 (structured)

Test: [0/79]	Time 0.221 (0.221)	Loss 0.3883 (0.3883) ([0.249]+[0.139])	Prec@1 96.094 (96.094)
 * Prec@1 93.170

 Total elapsed time  3:50:43.932996 
 FINETUNING


 sparsity of   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.5, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375]

 sparsity of   [1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.694444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6753472089767456, 1.0, 0.7152777910232544, 1.0, 1.0, 0.6736111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6805555820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6979166865348816, 1.0, 1.0, 0.6753472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.8125, 0.796875, 0.8125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.78125, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 0.796875, 1.0, 0.796875, 1.0, 1.0, 0.796875, 0.796875, 1.0, 0.78125, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.796875, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.796875, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.8125, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.796875, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.796875, 0.78125, 0.78125, 0.78125, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.8125, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 0.828125, 1.0, 0.78125, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.78125, 1.0, 0.796875, 1.0, 0.8125, 0.796875, 0.8125, 1.0, 0.78125, 0.78125, 1.0, 1.0, 1.0, 0.78125, 0.796875, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 0.78125, 0.796875, 0.8125, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.8125, 1.0, 0.78125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0]

 sparsity of   [1.0, 0.484375, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 1.0, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.515625, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 0.5, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.5, 0.484375, 1.0, 0.5, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.484375, 1.0, 1.0, 0.484375, 1.0, 0.5, 1.0, 0.484375, 0.484375, 0.484375, 0.5, 0.484375, 0.484375, 0.5, 0.484375, 0.5, 0.484375, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.515625, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.53125, 0.484375, 0.53125, 1.0, 0.5, 1.0, 0.484375, 1.0, 0.484375, 0.515625, 0.5, 0.484375, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 1.0, 1.0, 0.5, 0.5, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 0.484375, 0.5, 0.546875, 0.484375, 0.484375, 0.515625, 0.5, 1.0, 0.5, 0.5, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.5, 1.0, 0.5, 0.5, 0.484375, 0.484375, 0.484375, 0.5625, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.5, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.515625, 0.5, 0.484375, 0.5, 0.515625, 1.0, 0.484375, 1.0, 0.484375, 1.0, 0.484375, 0.484375, 1.0, 1.0, 0.484375, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.484375, 0.5, 0.5, 0.484375, 1.0, 0.484375, 0.5, 0.484375, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 0.484375, 1.0, 0.484375, 0.484375, 0.5, 0.5, 0.484375, 1.0, 0.484375, 1.0, 0.484375, 1.0]

 sparsity of   [0.3046875, 1.0, 1.0, 0.3125, 1.0, 0.31640625, 0.3125, 0.30078125, 0.30859375, 1.0, 1.0, 1.0, 1.0, 0.3046875, 1.0, 0.3125, 0.30859375, 0.28515625, 1.0, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 1.0, 0.29296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.7951388955116272, 0.796875, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 0.7986111044883728, 1.0, 0.796875, 0.796875, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 0.7986111044883728, 1.0, 0.796875, 1.0, 1.0, 1.0, 0.796875, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.703125, 0.71875, 1.0, 0.71875, 0.71875, 0.703125, 1.0, 0.703125, 0.734375, 0.703125, 0.71875, 1.0, 0.71875, 0.703125, 1.0, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.71875, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.734375, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.703125, 1.0, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 0.703125, 0.703125, 0.734375, 1.0, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 1.0, 0.703125, 1.0, 0.71875, 0.75, 1.0, 0.71875, 0.71875, 0.703125, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.671875, 0.734375, 0.734375, 0.703125, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.703125, 0.71875, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 1.0, 1.0, 0.703125, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.703125, 0.703125, 0.71875, 1.0, 0.703125, 0.703125, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 0.6875, 0.734375, 0.703125, 1.0, 1.0, 0.6875, 0.734375, 1.0, 0.703125, 0.703125, 0.703125, 1.0, 1.0, 0.703125, 0.71875, 1.0, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.703125, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.6875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 0.71875, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.75, 0.703125, 1.0, 0.71875, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.703125, 0.703125, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.6875, 0.703125, 0.71875, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0]

 sparsity of   [0.203125, 1.0, 1.0, 0.171875, 0.171875, 1.0, 0.17578125, 1.0, 0.1796875, 1.0, 1.0, 0.1796875, 0.16796875, 1.0, 1.0, 0.19921875, 0.171875, 0.1796875, 1.0, 0.21484375, 1.0, 1.0, 1.0, 0.19921875, 1.0, 0.17578125, 0.17578125, 0.2109375, 0.16796875, 1.0, 1.0, 1.0, 0.21875, 1.0, 1.0, 1.0, 0.21484375, 0.20703125, 0.18359375, 0.171875, 0.171875, 0.16796875, 0.1796875, 0.171875, 0.17578125, 1.0, 1.0, 1.0, 1.0, 0.16796875, 0.1796875, 0.19921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.203125, 1.0, 0.171875, 1.0, 0.20703125, 1.0]

 sparsity of   [1.0, 0.4930555522441864, 1.0, 0.5052083134651184, 1.0, 0.5086805820465088, 0.4739583432674408, 1.0, 1.0, 0.484375, 1.0, 1.0, 0.5086805820465088, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 0.4982638955116272, 1.0, 1.0, 0.506944477558136, 0.5017361044883728, 1.0, 1.0, 1.0, 0.506944477558136, 0.4982638955116272, 1.0, 1.0, 0.5, 1.0, 0.4965277910232544, 1.0, 0.4965277910232544, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4930555522441864, 0.4930555522441864, 1.0, 0.5017361044883728, 0.5104166865348816, 0.506944477558136, 0.4930555522441864, 0.4982638955116272, 1.0, 1.0, 1.0, 0.4965277910232544, 0.5034722089767456, 1.0, 0.4965277910232544, 0.4930555522441864, 0.4930555522441864, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.546875, 0.546875, 0.5625, 1.0, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.5625, 0.546875, 0.53125, 0.5625, 0.546875, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.5625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.546875, 0.546875, 1.0, 1.0, 0.546875, 0.53125, 1.0, 0.53125, 0.546875, 0.53125, 0.53125, 0.53125, 1.0, 0.5625, 0.546875, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 0.546875, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.546875, 0.546875, 0.546875, 0.546875, 0.53125, 1.0, 0.53125, 1.0, 0.578125, 0.546875, 0.546875, 0.546875, 1.0, 0.53125, 0.546875, 0.53125, 0.53125, 0.546875, 0.53125, 0.546875, 1.0, 0.5625, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.5625, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 0.53125, 0.546875, 1.0, 0.5625, 0.546875, 0.578125, 0.53125, 0.5625, 1.0, 0.53125, 0.546875, 0.546875, 0.546875, 0.546875, 0.53125, 0.53125, 0.5625, 0.53125, 0.546875, 0.53125, 0.515625, 0.515625, 0.5625, 1.0, 1.0, 0.546875, 1.0, 0.53125, 1.0, 0.546875, 0.53125, 0.5625, 0.53125, 0.546875, 1.0, 0.578125, 0.53125, 1.0, 1.0, 0.5625, 0.53125, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.546875, 1.0, 0.546875, 0.546875, 1.0, 0.5625, 1.0, 0.5625, 0.546875, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 0.546875, 0.53125, 0.546875, 0.5625, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.5625, 0.5625, 1.0, 0.546875, 1.0, 0.546875, 0.53125, 1.0, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 1.0, 1.0, 0.515625, 0.5625, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 0.53125, 0.546875, 0.53125, 0.546875, 1.0, 0.546875, 0.5625, 0.546875, 0.546875, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 1.0, 0.546875, 0.53125, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.5625, 1.0, 0.53125, 0.546875, 1.0, 0.546875, 0.5625, 0.546875, 0.5625, 0.546875, 0.546875, 0.546875, 1.0, 0.546875, 1.0, 0.53125, 0.53125, 0.546875, 0.546875, 0.53125]

 sparsity of   [0.046875, 0.0546875, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.05859375, 0.04296875, 1.0, 0.04296875, 1.0, 1.0, 0.04296875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 0.04296875, 0.05078125, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.046875, 1.0, 1.0, 0.05859375, 0.04296875, 1.0, 1.0, 0.03515625, 1.0, 0.046875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.0546875, 0.03515625, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 0.04296875, 1.0, 1.0, 0.0390625, 0.05859375, 0.0390625, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 0.04296875, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.03125, 1.0, 1.0, 1.0, 0.0390625, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 0.03515625, 0.03125, 0.04296875, 1.0, 1.0, 0.03515625, 0.0390625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.6015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6032986044883728, 1.0, 1.0, 0.6032986044883728, 0.5980902910232544, 0.6032986044883728, 1.0, 1.0, 0.6015625, 1.0, 0.5998263955116272, 1.0, 1.0, 0.5980902910232544, 0.5989583134651184, 1.0, 1.0, 0.5980902910232544, 0.6015625, 0.5954861044883728, 1.0, 0.600694477558136, 0.6015625, 1.0, 0.6032986044883728, 1.0, 0.6015625, 1.0, 1.0, 1.0, 1.0, 0.5972222089767456, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 0.600694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5989583134651184, 0.600694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5998263955116272, 0.5980902910232544, 1.0, 0.5989583134651184, 1.0, 0.5998263955116272, 0.5963541865348816, 0.5989583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59375, 1.0, 1.0, 0.59375, 0.5980902910232544, 0.6059027910232544, 1.0, 0.5963541865348816, 1.0, 0.5972222089767456, 1.0, 0.5963541865348816, 1.0, 1.0, 0.5980902910232544, 0.6015625, 0.6059027910232544, 1.0, 1.0, 1.0, 0.5989583134651184, 1.0, 0.6015625, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 0.6032986044883728, 1.0, 1.0, 0.5980902910232544, 0.5989583134651184, 1.0, 1.0, 1.0, 1.0, 0.5998263955116272, 1.0, 1.0, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 1.0, 0.65625, 0.640625, 0.640625, 1.0, 0.6484375, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.65625, 0.640625, 0.640625, 0.65625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 0.6484375, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 1.0, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.65625, 1.0, 0.640625, 0.65625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 0.65625, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 0.640625, 1.0, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 0.640625, 0.6484375, 0.640625, 1.0, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 1.0, 0.6640625, 0.640625, 1.0, 0.6484375, 1.0, 0.640625, 1.0, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 0.6484375, 0.640625, 0.640625, 0.640625, 0.640625, 0.6484375, 0.640625, 1.0, 0.640625, 0.6484375, 0.640625, 0.6484375, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.640625, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 0.640625, 0.6484375, 1.0, 0.65625, 1.0, 0.640625, 1.0, 0.65625, 0.6484375, 1.0, 0.640625, 1.0, 0.640625, 0.640625, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 1.0]

 sparsity of   [0.046875, 1.0, 0.05078125, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 0.05078125, 0.0546875, 0.0859375, 1.0, 0.0546875, 0.04296875, 0.109375, 1.0, 0.046875, 0.0390625, 1.0, 0.109375, 0.234375, 0.046875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.05078125, 1.0, 0.05078125, 0.05078125, 0.1484375, 1.0, 0.05078125, 1.0, 0.04296875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.1328125, 0.08984375, 0.11328125, 0.0625, 1.0, 1.0, 0.171875, 1.0, 0.046875, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.05078125, 1.0, 0.0625, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.046875, 0.046875, 1.0, 0.046875, 0.046875, 0.11328125, 0.125, 0.05078125, 1.0, 0.11328125, 0.07421875, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.04296875, 0.04296875, 0.140625, 1.0, 0.08203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.04296875, 1.0, 0.05859375, 0.05078125, 1.0, 0.046875, 0.07421875, 0.0390625, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.04296875, 0.27734375, 0.05078125, 0.0625, 0.0859375, 0.046875, 0.05078125, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 0.04296875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.04296875, 0.046875, 1.0, 0.0546875, 0.04296875, 0.046875, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 0.0703125, 0.0625, 0.08984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.04296875, 1.0, 0.10546875, 0.05078125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 0.0390625, 1.0, 1.0, 0.046875, 0.046875, 0.07421875, 1.0, 0.046875, 0.0546875, 0.05078125, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.04296875, 0.09765625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.2265625, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.1328125, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 0.125, 0.0546875, 0.046875, 0.05078125, 0.078125, 1.0, 0.09765625, 0.04296875, 0.12890625, 0.04296875, 0.04296875, 0.046875, 0.05859375, 0.0546875, 1.0, 0.05078125, 0.046875, 1.0, 0.109375, 0.05078125, 0.046875, 1.0, 0.12890625, 1.0, 0.05078125, 0.04296875, 0.15234375, 1.0, 1.0, 1.0, 0.12890625, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.04296875, 0.0390625, 0.04296875, 0.04296875, 1.0, 1.0, 0.04296875, 1.0, 0.22265625, 1.0, 1.0, 1.0, 1.0, 0.06640625, 0.046875, 0.0390625, 1.0, 0.05078125, 1.0, 0.0546875, 1.0, 0.04296875, 0.26953125, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.05859375, 1.0, 0.28125, 1.0, 0.1796875, 0.04296875, 0.04296875, 1.0, 0.06640625, 0.08984375, 1.0, 1.0, 1.0, 1.0, 0.0859375, 0.04296875, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.1015625, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.046875, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.0546875, 0.19921875, 0.078125, 0.12109375, 0.05078125, 0.046875, 1.0, 1.0, 0.046875, 0.04296875, 0.046875, 0.046875, 0.046875, 1.0, 0.05859375, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.0390625, 0.1015625, 0.05859375, 1.0, 0.2890625, 1.0, 1.0, 0.15234375, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.04296875, 0.0390625, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.046875, 1.0, 0.05859375, 0.0859375, 1.0, 0.04296875, 1.0, 0.07421875, 1.0, 1.0, 0.046875, 0.26953125, 1.0, 1.0, 1.0, 0.09765625, 0.16796875, 0.2578125, 1.0, 1.0, 1.0, 0.0625, 0.08984375, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 0.0390625, 0.0390625, 0.05078125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.09765625, 0.05078125, 0.05859375, 0.1484375, 1.0, 0.04296875, 1.0, 0.0390625, 1.0, 1.0, 0.11328125, 1.0, 0.0546875, 0.05078125, 0.05078125, 0.05078125, 0.0546875, 0.0625, 0.046875, 0.046875, 0.05078125, 1.0, 0.046875, 0.046875, 0.04296875, 0.1328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.04296875, 1.0, 0.08203125, 0.16796875, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 1.0, 1.0, 1.0, 0.0390625, 0.0390625, 0.05078125, 1.0, 0.21484375, 1.0, 0.08984375, 1.0, 0.046875, 0.046875, 1.0, 0.046875, 0.05859375, 0.0390625, 0.05859375, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.0625, 1.0]

 sparsity of   [1.0, 0.4765625, 0.462890625, 0.458984375, 1.0, 1.0, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 0.458984375, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.470703125, 1.0, 1.0, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 0.470703125, 1.0, 0.462890625, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.478515625, 1.0, 1.0, 0.4609375, 0.462890625, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.466796875, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4609375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 0.45703125, 0.46484375, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 0.4609375, 0.462890625, 0.458984375, 1.0, 1.0, 0.466796875, 0.453125, 0.45703125, 0.4609375, 1.0, 0.4609375, 1.0, 1.0, 0.482421875, 0.46484375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45703125, 0.458984375, 0.4609375, 0.458984375, 1.0, 1.0, 1.0, 1.0, 0.46484375, 1.0, 1.0, 1.0, 0.458984375, 0.462890625, 0.46484375, 1.0, 0.458984375, 0.462890625]

 sparsity of   [1.0, 1.0, 0.6336805820465088, 0.6328125, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 0.6145833134651184, 0.6302083134651184, 1.0, 0.6302083134651184, 0.6310763955116272, 1.0, 0.6293402910232544, 0.6345486044883728, 1.0, 1.0, 1.0, 0.6102430820465088, 0.6293402910232544, 0.6215277910232544, 0.6276041865348816, 1.0, 0.6362847089767456, 1.0, 1.0, 0.6258680820465088, 1.0, 0.6354166865348816, 1.0, 0.6232638955116272, 0.6310763955116272, 1.0, 0.6137152910232544, 1.0, 0.6223958134651184, 0.6302083134651184, 0.6102430820465088, 1.0, 0.6232638955116272, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 0.6215277910232544, 1.0, 0.6302083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6206597089767456, 1.0, 0.6354166865348816, 0.6215277910232544, 0.6232638955116272, 1.0, 1.0, 0.6102430820465088, 1.0, 0.6206597089767456, 1.0, 1.0, 1.0, 0.6145833134651184, 1.0, 0.6232638955116272, 1.0, 0.6215277910232544, 1.0, 0.6059027910232544, 1.0, 0.6206597089767456, 0.608506977558136, 1.0, 1.0, 1.0, 0.6145833134651184, 0.6310763955116272, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 0.625, 0.631944477558136, 0.6206597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.6354166865348816, 0.6302083134651184, 0.6293402910232544, 1.0, 0.6284722089767456, 0.6302083134651184, 0.624131977558136, 1.0, 1.0, 0.6267361044883728, 1.0, 0.6293402910232544, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.625, 0.6276041865348816, 0.6284722089767456, 1.0, 1.0, 0.624131977558136, 0.6171875, 1.0]

 sparsity of   [0.515625, 1.0, 0.5234375, 0.5078125, 1.0, 0.5390625, 0.53125, 0.5390625, 0.515625, 1.0, 0.515625, 1.0, 0.546875, 0.53125, 0.5234375, 0.515625, 0.5078125, 0.515625, 0.53125, 0.5390625, 1.0, 0.515625, 0.5234375, 0.5234375, 0.515625, 1.0, 0.515625, 0.5234375, 0.5078125, 1.0, 0.515625, 0.53125, 0.515625, 1.0, 0.5234375, 0.5078125, 0.515625, 1.0, 0.5078125, 0.5078125, 0.515625, 0.5234375, 0.5234375, 1.0, 0.515625, 0.515625, 0.53125, 0.5078125, 0.5078125, 0.53125, 0.546875, 0.515625, 0.515625, 0.5234375, 0.53125, 0.53125, 0.515625, 0.515625, 1.0, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.5234375, 0.5234375, 0.5234375, 0.5078125, 1.0, 0.5390625, 0.5234375, 0.515625, 1.0, 0.515625, 0.5234375, 1.0, 0.5234375, 0.5078125, 0.5, 0.515625, 0.53125, 1.0, 0.53125, 0.5234375, 0.5390625, 0.515625, 1.0, 1.0, 0.5390625, 0.53125, 0.515625, 0.5234375, 0.515625, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.515625, 0.5234375, 0.5, 0.53125, 0.53125, 0.515625, 0.53125, 0.515625, 0.5, 0.5234375, 0.5234375, 0.5234375, 1.0, 0.5390625, 1.0, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.53125, 1.0, 0.5390625, 0.5078125, 1.0, 1.0, 0.5234375, 0.5, 0.515625, 0.515625, 0.5234375, 0.5078125, 0.5234375, 1.0, 0.5078125, 0.515625, 0.5078125, 0.515625, 0.53125, 0.515625, 0.5234375, 1.0, 0.5078125, 0.5234375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.5546875, 0.5234375, 0.5078125, 0.5234375, 0.53125, 0.5546875, 0.5234375, 1.0, 0.5234375, 1.0, 1.0, 0.5078125, 0.515625, 0.5390625, 0.53125, 0.515625, 0.515625, 0.4921875, 1.0, 1.0, 0.515625, 1.0, 0.5234375, 0.515625, 1.0, 0.515625, 0.53125, 1.0, 0.515625, 0.515625, 0.515625, 0.5234375, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 0.5234375, 0.5390625, 1.0, 0.515625, 0.515625, 0.5390625, 0.5234375, 0.5078125, 0.5234375, 0.53125, 0.53125, 0.5390625, 0.5234375, 1.0, 1.0, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.515625, 1.0, 1.0, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 1.0, 0.5234375, 1.0, 0.5, 0.5234375, 0.515625, 1.0, 0.515625, 1.0, 0.53125, 0.5234375, 0.53125, 1.0, 0.515625, 0.515625, 1.0, 0.53125, 0.515625, 0.515625, 0.515625, 1.0, 0.515625, 0.5078125, 0.515625, 0.515625, 0.515625, 0.5078125, 0.5390625, 1.0, 0.5078125, 0.53125, 0.515625, 0.515625, 0.515625, 0.5078125, 0.515625, 0.5234375, 0.546875, 0.515625, 0.53125, 0.515625, 0.53125, 0.5, 0.515625, 1.0, 0.5390625, 1.0, 0.5078125, 0.53125, 0.5390625, 1.0, 0.640625, 1.0, 0.53125, 0.5234375, 0.5234375, 0.515625, 0.5234375, 0.5234375, 1.0, 0.5234375, 0.515625, 0.515625, 0.515625, 0.5078125, 0.515625, 0.53125, 0.5390625, 0.5, 0.5234375, 0.5234375, 0.515625, 0.515625, 1.0, 0.5234375, 0.53125, 0.515625, 0.515625, 0.515625, 1.0, 0.515625, 0.5234375, 0.5234375, 0.5, 1.0, 0.5234375, 1.0, 0.5234375, 0.5, 0.5, 1.0, 0.53125, 1.0, 0.515625, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.5078125, 0.515625, 0.5234375, 0.515625, 0.515625, 0.5234375, 1.0, 0.53125, 1.0, 0.53125, 0.515625, 1.0, 0.5234375, 0.5234375, 0.515625, 0.515625, 1.0, 1.0, 1.0, 0.515625, 0.5234375, 1.0, 0.5390625, 0.515625, 0.53125, 0.5078125, 0.53125, 0.53125, 0.515625, 0.5390625, 0.5703125, 0.5234375, 1.0, 0.53125, 0.515625, 0.5234375, 0.5234375, 0.5234375, 0.5078125, 0.515625, 0.5234375, 1.0, 0.515625, 0.546875, 0.515625, 0.53125, 0.53125, 0.5234375, 0.5078125, 1.0, 1.0, 0.5078125, 0.53125, 0.515625, 1.0, 0.5390625, 1.0, 0.5234375, 0.515625, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.5390625, 0.5078125, 1.0, 0.515625, 1.0, 0.515625, 1.0, 1.0, 0.515625, 1.0, 0.5234375, 0.5234375, 1.0, 1.0, 0.53125, 0.515625, 0.515625, 0.53125, 0.53125, 0.5078125, 0.53125, 0.53125, 0.5234375, 0.515625, 0.53125, 0.515625, 0.5234375, 0.5234375, 0.515625, 0.5234375, 1.0, 1.0, 0.515625, 1.0, 0.5078125, 0.53125, 1.0, 1.0, 1.0, 0.546875, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 0.5078125, 0.5078125, 0.5234375, 1.0, 1.0, 1.0, 0.5078125, 0.515625, 0.5234375, 0.515625, 1.0, 0.53125, 0.53125, 1.0, 1.0, 0.5234375, 0.5390625, 0.5546875, 0.5234375, 0.53125, 0.515625, 0.515625, 1.0, 0.515625, 0.5390625, 0.515625, 0.5234375, 1.0, 0.5390625, 1.0, 0.5234375, 0.53125, 0.5234375, 0.5234375, 0.546875, 0.515625, 0.515625, 0.5234375, 0.515625, 0.5234375, 0.515625, 0.53125, 1.0, 0.5078125, 0.515625, 0.5078125, 0.515625, 0.53125, 1.0, 0.5234375, 0.515625, 0.5234375, 0.5234375, 0.515625, 1.0, 0.515625, 0.5078125, 0.5390625, 1.0, 1.0, 0.5078125, 0.5390625, 0.53125, 0.5078125, 1.0, 0.53125, 0.5234375, 0.5234375, 0.5234375, 0.5234375, 0.5234375, 1.0, 0.515625, 0.515625, 0.5390625, 0.53125, 1.0, 0.5, 1.0, 0.53125, 0.5078125, 1.0, 0.5234375, 0.515625]

 sparsity of   [0.20703125, 0.197265625, 0.2109375, 1.0, 0.19921875, 0.19921875, 0.201171875, 1.0, 0.20703125, 0.20703125, 0.201171875, 0.201171875, 1.0, 1.0, 0.208984375, 0.203125, 1.0, 0.201171875, 1.0, 0.20703125, 0.19921875, 0.19921875, 0.205078125, 1.0, 0.201171875, 0.203125, 1.0, 1.0, 1.0, 0.203125, 0.2109375, 0.19921875, 0.201171875, 0.208984375, 0.197265625, 0.20703125, 1.0, 1.0, 0.208984375, 1.0, 0.201171875, 0.203125, 0.203125, 0.203125, 0.19921875, 0.1953125, 1.0, 0.20703125, 1.0, 1.0, 0.205078125, 0.197265625, 0.197265625, 0.197265625, 0.28515625, 0.201171875, 1.0, 0.201171875, 0.19921875, 0.193359375, 1.0, 0.205078125, 0.201171875, 0.212890625, 1.0, 1.0, 1.0, 1.0, 0.201171875, 0.205078125, 1.0, 0.197265625, 0.205078125, 1.0, 1.0, 1.0, 0.19921875, 0.205078125, 1.0, 0.19921875, 1.0, 0.203125, 0.208984375, 0.201171875, 0.203125, 0.201171875, 0.19921875, 1.0, 0.197265625, 1.0, 0.197265625, 0.197265625, 0.203125, 1.0, 0.19921875, 1.0, 0.203125, 1.0, 0.197265625, 0.201171875, 0.205078125, 0.201171875, 1.0, 0.197265625, 1.0, 0.201171875, 0.197265625, 0.19921875, 0.205078125, 1.0, 0.19921875, 0.193359375, 1.0, 1.0, 1.0, 1.0, 0.203125, 1.0, 0.203125, 1.0, 0.197265625, 0.1953125, 0.1953125, 0.205078125, 0.197265625, 1.0, 0.19921875, 0.197265625]

 sparsity of   [1.0, 0.3263888955116272, 1.0, 1.0, 0.328125, 1.0, 1.0, 0.3307291567325592, 0.323784738779068, 1.0, 1.0, 1.0, 1.0, 0.3307291567325592, 1.0, 1.0, 0.3263888955116272, 0.3255208432674408, 0.3263888955116272, 1.0, 0.3324652910232544, 0.3307291567325592, 1.0, 1.0, 1.0, 1.0, 0.3289930522441864, 0.3255208432674408, 0.323784738779068, 1.0, 0.3255208432674408, 1.0, 0.3289930522441864, 0.328125, 1.0, 1.0, 1.0, 0.3263888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3298611044883728, 0.3298611044883728, 1.0, 1.0, 0.3289930522441864, 1.0, 1.0, 0.3263888955116272, 1.0, 0.323784738779068, 0.3272569477558136, 0.3255208432674408, 1.0, 0.3246527910232544, 0.3272569477558136, 1.0, 1.0, 0.3263888955116272, 1.0, 1.0, 0.3289930522441864, 1.0, 0.3289930522441864, 1.0, 1.0, 0.3255208432674408, 0.323784738779068, 0.3272569477558136, 0.3289930522441864, 0.328125, 1.0, 0.3298611044883728, 1.0, 0.3246527910232544, 1.0, 0.3289930522441864, 0.3263888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3246527910232544, 1.0, 0.3263888955116272, 1.0, 0.3298611044883728, 1.0, 0.3289930522441864, 1.0, 0.3298611044883728, 0.3272569477558136, 1.0, 1.0, 0.3255208432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3298611044883728, 0.3255208432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3246527910232544, 1.0, 1.0, 0.3263888955116272, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.3246527910232544, 0.323784738779068, 1.0]

 sparsity of   [0.6015625, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 1.0, 1.0, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.6015625, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 1.0, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.609375, 1.0, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.6015625, 0.6015625, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 1.0, 0.6015625, 0.6015625, 1.0, 0.59375, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.609375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.6015625, 1.0, 0.6015625, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 1.0, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.6015625, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.6015625, 0.609375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 0.6015625, 0.59375, 1.0, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 1.0, 0.59375, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 0.6015625, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.6015625, 1.0, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.6015625, 0.6015625, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 1.0, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375]

 sparsity of   [0.16015625, 0.158203125, 0.158203125, 0.15625, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.15625, 1.0, 0.158203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.16015625, 1.0, 1.0, 0.154296875, 1.0, 1.0, 0.154296875, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 1.0, 0.158203125, 0.1640625, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.16015625, 0.158203125, 1.0, 1.0, 0.158203125, 0.15234375, 0.1640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.162109375, 0.154296875, 1.0, 1.0, 1.0, 0.1640625, 1.0, 0.154296875, 1.0, 0.1640625, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16015625, 1.0, 0.158203125, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.154296875, 1.0, 0.15234375, 0.1640625, 1.0, 0.162109375, 1.0, 1.0, 1.0, 0.166015625, 1.0, 1.0, 0.158203125, 1.0, 1.0, 0.166015625, 1.0, 1.0, 1.0, 1.0, 0.162109375, 1.0, 0.162109375, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 0.16015625]

 sparsity of   [1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 0.6380208134651184, 1.0, 0.6371527910232544, 1.0, 0.6267361044883728, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631944477558136, 1.0, 1.0, 0.639756977558136, 1.0, 0.631944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6432291865348816, 1.0, 1.0, 1.0, 1.0, 0.6336805820465088, 1.0, 1.0, 0.6328125, 0.639756977558136, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 0.6371527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 0.6432291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.639756977558136, 1.0, 0.6267361044883728, 0.6432291865348816, 1.0, 1.0, 0.631944477558136, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.639756977558136, 0.6284722089767456, 1.0, 1.0, 1.0, 1.0, 0.6484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6432291865348816, 1.0, 0.6302083134651184, 0.6371527910232544, 1.0, 1.0, 0.6423611044883728, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.78125, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.765625, 0.765625, 0.765625, 0.7578125, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.765625, 0.765625, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.765625, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.765625, 0.765625, 0.7578125, 0.765625, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7734375, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.765625, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7734375, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.765625, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.765625, 1.0, 1.0, 0.7578125, 0.7578125, 0.765625, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 0.7578125, 1.0, 0.7578125, 0.7578125, 0.7578125, 1.0, 1.0, 1.0, 0.7578125]

 sparsity of   [0.150390625, 1.0, 0.1484375, 0.14453125, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.14453125, 0.1328125, 1.0, 0.146484375, 0.140625, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.146484375, 0.1328125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.142578125, 0.140625, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.140625, 0.140625, 1.0, 1.0, 0.146484375, 1.0, 0.140625, 0.142578125, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 0.142578125, 1.0, 0.146484375, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.140625, 0.142578125, 0.140625, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.140625, 0.146484375, 0.146484375, 0.146484375, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.142578125, 0.146484375, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.1484375, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.1484375, 0.142578125, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 0.140625, 0.14453125, 0.142578125, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.134765625, 0.140625, 1.0, 1.0, 0.146484375, 0.14453125, 1.0, 0.150390625, 1.0, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.138671875, 0.140625, 1.0, 1.0, 0.13671875, 0.142578125, 1.0, 0.14453125, 1.0, 0.140625, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.14453125, 0.140625, 1.0, 1.0, 0.138671875, 0.142578125, 0.14453125, 0.142578125, 0.140625, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.140625, 0.14453125, 1.0, 1.0, 0.14453125, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.140625, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.13671875, 1.0, 0.15234375, 0.140625, 0.138671875, 0.13671875, 0.14453125, 0.142578125, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.14453125, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.14453125, 0.140625, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.13671875, 1.0, 1.0, 0.13671875, 0.14453125, 0.142578125, 0.134765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.146484375, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0]

 sparsity of   [0.5590277910232544, 1.0, 1.0, 0.5590277910232544, 1.0, 0.5590277910232544, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5607638955116272, 1.0, 1.0, 0.5581597089767456, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 0.5603298544883728, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 0.5603298544883728, 1.0, 0.5625, 1.0, 1.0, 0.5598958134651184, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 0.5603298544883728, 1.0, 0.5633680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 0.5620659589767456, 0.5581597089767456, 1.0, 1.0, 0.5607638955116272, 0.5625, 1.0, 0.5594618320465088, 1.0, 1.0, 0.557725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 0.5581597089767456, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5607638955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 1.0, 0.55859375, 0.5594618320465088, 0.5603298544883728, 1.0, 0.561631977558136, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 0.5603298544883728, 1.0, 0.5620659589767456, 1.0, 1.0, 1.0, 0.5603298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.561631977558136, 1.0, 1.0, 1.0, 1.0, 0.5620659589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5603298544883728, 0.5625, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 0.5590277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5594618320465088, 1.0, 0.5611979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.561631977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.77734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.79296875, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.77734375, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.77734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.77734375, 0.7734375, 1.0, 1.0, 0.77734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7890625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.7734375, 0.7734375, 0.7734375, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78515625, 1.0, 0.78125, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 0.7734375, 1.0, 0.7734375, 0.78125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.15234375, 0.14453125, 1.0, 0.154296875, 0.142578125, 1.0, 0.1484375, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 1.0, 0.14453125, 0.14453125, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.138671875, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 0.333984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 1.0, 1.0, 0.15234375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.15234375, 0.154296875, 1.0, 0.150390625, 1.0, 0.150390625, 0.150390625, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 0.150390625, 0.15234375, 1.0, 1.0, 1.0, 0.154296875, 1.0, 0.1484375, 0.146484375, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.15234375, 0.146484375, 1.0, 0.1484375, 0.15234375, 0.1484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.158203125, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.162109375, 1.0, 0.15234375, 0.15234375, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.146484375, 0.154296875, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.146484375, 0.146484375, 0.150390625, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.236328125, 1.0, 1.0, 0.189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.150390625, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 0.138671875, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.15234375, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.150390625, 0.1484375, 0.150390625, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.15234375, 1.0, 1.0, 0.146484375, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.150390625, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.1640625, 1.0, 1.0, 1.0, 0.1953125, 0.140625, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 0.146484375, 1.0, 0.142578125, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 0.15625, 1.0, 1.0, 1.0, 0.142578125, 0.1484375, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.142578125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 0.1484375, 0.15234375, 1.0, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 0.146484375, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.169921875, 1.0, 1.0, 1.0, 1.0, 0.15625, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.150390625, 1.0, 1.0, 0.14453125, 1.0, 0.1484375, 1.0, 0.150390625, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.14453125, 0.15234375, 0.150390625, 1.0, 1.0, 0.1484375, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.1484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 0.1484375, 0.15625, 1.0, 1.0, 0.15625, 0.140625, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.1484375, 0.142578125, 1.0, 1.0, 0.15625, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.1484375, 0.154296875, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 0.1484375, 1.0, 0.1640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.291015625, 0.1484375, 0.15625, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 0.146484375, 0.169921875, 1.0, 0.154296875, 0.15625, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.146484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.150390625, 0.154296875, 0.1484375, 0.146484375, 1.0, 1.0, 0.146484375, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0, 0.14453125, 0.154296875, 1.0, 1.0, 1.0, 0.1484375, 0.15234375, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 0.1484375, 1.0, 1.0, 0.146484375, 0.14453125, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.158203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15234375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.716796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7236328125, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.720703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 0.71484375, 0.720703125, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7177734375, 1.0, 1.0, 0.7197265625, 0.7177734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7216796875, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.909288227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.909288227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.905381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 0.9105902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9066840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.905381977558136, 1.0, 1.0, 1.0, 0.9066840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9071180820465088, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9088541865348816, 1.0, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 0.9071180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9032118320465088, 1.0, 1.0, 0.9071180820465088, 1.0, 0.9066840410232544]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 0.91796875, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 0.9140625, 0.921875, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 0.91796875, 0.921875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.921875, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 0.921875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 0.921875, 0.921875, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 0.91796875, 0.91796875, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.9140625, 0.91796875, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 0.9140625, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 0.9140625, 0.91796875, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 0.9140625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.705078125, 0.7041015625, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 0.7158203125, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 0.69921875, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71484375, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 0.7041015625, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 0.705078125, 0.7021484375, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7080078125, 1.0, 1.0, 0.7041015625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 0.7041015625, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 0.701171875, 1.0, 0.7060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 0.7001953125, 1.0, 1.0, 0.7099609375, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 0.705078125, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 0.7021484375, 0.7041015625, 0.703125, 1.0, 0.7041015625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8103298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8090277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8098958134651184, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.88671875, 0.8828125, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.8828125, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.8828125, 0.8828125, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.890625, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8828125, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 0.6962890625, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 0.6962890625, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 0.697265625, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 0.6962890625, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.69921875, 1.0, 1.0, 0.6962890625, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 0.693359375, 1.0, 1.0, 0.697265625, 0.697265625, 0.6962890625, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 0.6904296875, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 0.6982421875, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831163227558136, 1.0, 1.0, 0.8333333134651184, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 0.831163227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8337673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8315972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8342013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 0.8328993320465088, 0.83203125, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8328993320465088, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.831163227558136, 1.0, 0.8324652910232544, 1.0, 1.0, 1.0, 1.0, 0.8307291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 0.88671875, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 0.88671875, 0.890625, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.890625, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6923828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 0.6982421875, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6923828125, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7021484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.603515625, 0.60205078125, 0.60205078125, 0.6025390625, 0.6005859375, 0.603515625, 0.60205078125, 0.60205078125, 0.60107421875, 0.60205078125, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375]

Total parameter pruned: 22962742.0038929 (unstructured) 21964741 (structured)

Test: [0/79]	Time 0.219 (0.219)	Loss 0.3884 (0.3884) ([0.249]+[0.139])	Prec@1 96.094 (96.094)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(11.1427, device='cuda:0')
Epoch: [300][0/391]	Time 0.253 (0.253)	Data 0.182 (0.182)	Loss 0.0226 (0.0226) ([0.023]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [300][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0078 (0.0111) ([0.008]+[0.000])	Prec@1 100.000 (99.807)
Epoch: [300][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0257 (0.0102) ([0.026]+[0.000])	Prec@1 98.438 (99.841)
Epoch: [300][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0098) ([0.007]+[0.000])	Prec@1 100.000 (99.852)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2365 (0.2365) ([0.237]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.1840, device='cuda:0')
Epoch: [301][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0156 (0.0105) ([0.016]+[0.000])	Prec@1 99.219 (99.814)
Epoch: [301][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0365 (0.0103) ([0.037]+[0.000])	Prec@1 98.438 (99.825)
Epoch: [301][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0083 (0.0102) ([0.008]+[0.000])	Prec@1 100.000 (99.831)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2294 (0.2294) ([0.229]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.300
current lr 1.00000e-03
Grad=  tensor(2.6961, device='cuda:0')
Epoch: [302][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0166 (0.0166) ([0.017]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0079 (0.0091) ([0.008]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [302][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0093) ([0.008]+[0.000])	Prec@1 100.000 (99.856)
Epoch: [302][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0131 (0.0095) ([0.013]+[0.000])	Prec@1 100.000 (99.849)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2466 (0.2466) ([0.247]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.4869, device='cuda:0')
Epoch: [303][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0052 (0.0102) ([0.005]+[0.000])	Prec@1 100.000 (99.861)
Epoch: [303][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0097) ([0.005]+[0.000])	Prec@1 100.000 (99.868)
Epoch: [303][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0185 (0.0098) ([0.018]+[0.000])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2506 (0.2506) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(11.1043, device='cuda:0')
Epoch: [304][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0254 (0.0254) ([0.025]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [304][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0047 (0.0098) ([0.005]+[0.000])	Prec@1 100.000 (99.853)
Epoch: [304][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0104 (0.0093) ([0.010]+[0.000])	Prec@1 100.000 (99.872)
Epoch: [304][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0094) ([0.006]+[0.000])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2316 (0.2316) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(0.0416, device='cuda:0')
Epoch: [305][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0103) ([0.002]+[0.000])	Prec@1 100.000 (99.783)
Epoch: [305][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0106) ([0.008]+[0.000])	Prec@1 100.000 (99.778)
Epoch: [305][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0165 (0.0103) ([0.017]+[0.000])	Prec@1 99.219 (99.811)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2082 (0.2082) ([0.208]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(0.0464, device='cuda:0')
Epoch: [306][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0102 (0.0086) ([0.010]+[0.000])	Prec@1 100.000 (99.892)
Epoch: [306][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0099 (0.0090) ([0.010]+[0.000])	Prec@1 100.000 (99.887)
Epoch: [306][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0094) ([0.004]+[0.000])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2357 (0.2357) ([0.236]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(1.5251, device='cuda:0')
Epoch: [307][0/391]	Time 0.255 (0.255)	Data 0.186 (0.186)	Loss 0.0089 (0.0089) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0215 (0.0105) ([0.021]+[0.000])	Prec@1 99.219 (99.783)
Epoch: [307][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0100) ([0.006]+[0.000])	Prec@1 100.000 (99.825)
Epoch: [307][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0098) ([0.006]+[0.000])	Prec@1 100.000 (99.842)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2031 (0.2031) ([0.203]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(5.5116, device='cuda:0')
Epoch: [308][0/391]	Time 0.260 (0.260)	Data 0.191 (0.191)	Loss 0.0175 (0.0175) ([0.017]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [308][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0041 (0.0091) ([0.004]+[0.000])	Prec@1 100.000 (99.853)
Epoch: [308][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0083 (0.0095) ([0.008]+[0.000])	Prec@1 100.000 (99.841)
Epoch: [308][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0096) ([0.010]+[0.000])	Prec@1 100.000 (99.842)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2071 (0.2071) ([0.207]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.370
current lr 1.00000e-03
Grad=  tensor(0.0932, device='cuda:0')
Epoch: [309][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0039 (0.0093) ([0.004]+[0.000])	Prec@1 100.000 (99.830)
Epoch: [309][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0087) ([0.005]+[0.000])	Prec@1 100.000 (99.868)
Epoch: [309][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0304 (0.0087) ([0.030]+[0.000])	Prec@1 98.438 (99.865)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2321 (0.2321) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.190
current lr 1.00000e-03
Grad=  tensor(1.1483, device='cuda:0')
Epoch: [310][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0078 (0.0078) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0080 (0.0098) ([0.008]+[0.000])	Prec@1 100.000 (99.814)
Epoch: [310][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0155 (0.0095) ([0.016]+[0.000])	Prec@1 99.219 (99.837)
Epoch: [310][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0097) ([0.006]+[0.000])	Prec@1 100.000 (99.834)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2344 (0.2344) ([0.234]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(0.1577, device='cuda:0')
Epoch: [311][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0047 (0.0078) ([0.005]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [311][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0308 (0.0094) ([0.031]+[0.000])	Prec@1 99.219 (99.845)
Epoch: [311][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0107 (0.0095) ([0.011]+[0.000])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2135 (0.2135) ([0.214]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.350
current lr 1.00000e-03
Grad=  tensor(0.3356, device='cuda:0')
Epoch: [312][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0060 (0.0060) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0041 (0.0096) ([0.004]+[0.000])	Prec@1 100.000 (99.869)
Epoch: [312][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0093) ([0.006]+[0.000])	Prec@1 100.000 (99.848)
Epoch: [312][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0088) ([0.009]+[0.000])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2128 (0.2128) ([0.213]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(0.3023, device='cuda:0')
Epoch: [313][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0058 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0075 (0.0093) ([0.007]+[0.000])	Prec@1 100.000 (99.845)
Epoch: [313][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0091) ([0.008]+[0.000])	Prec@1 100.000 (99.852)
Epoch: [313][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0122 (0.0086) ([0.012]+[0.000])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2297 (0.2297) ([0.230]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(0.5633, device='cuda:0')
Epoch: [314][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0053 (0.0084) ([0.005]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [314][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0085) ([0.004]+[0.000])	Prec@1 100.000 (99.883)
Epoch: [314][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0086) ([0.007]+[0.000])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2541 (0.2541) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.340
current lr 1.00000e-03
Grad=  tensor(0.0159, device='cuda:0')
Epoch: [315][0/391]	Time 0.269 (0.269)	Data 0.199 (0.199)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0093) ([0.002]+[0.000])	Prec@1 100.000 (99.830)
Epoch: [315][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0090) ([0.005]+[0.000])	Prec@1 100.000 (99.837)
Epoch: [315][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0088) ([0.008]+[0.000])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2627 (0.2627) ([0.263]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(6.1076, device='cuda:0')
Epoch: [316][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0203 (0.0203) ([0.020]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [316][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0085 (0.0100) ([0.008]+[0.000])	Prec@1 100.000 (99.861)
Epoch: [316][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0098) ([0.003]+[0.000])	Prec@1 100.000 (99.845)
Epoch: [316][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0113 (0.0096) ([0.011]+[0.000])	Prec@1 99.219 (99.836)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2434 (0.2434) ([0.243]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.0668, device='cuda:0')
Epoch: [317][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0125 (0.0089) ([0.012]+[0.000])	Prec@1 100.000 (99.869)
Epoch: [317][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0089) ([0.006]+[0.000])	Prec@1 100.000 (99.852)
Epoch: [317][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0085) ([0.006]+[0.000])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2432 (0.2432) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.2734, device='cuda:0')
Epoch: [318][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0078) ([0.001]+[0.000])	Prec@1 100.000 (99.869)
Epoch: [318][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0082) ([0.003]+[0.000])	Prec@1 100.000 (99.868)
Epoch: [318][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0084) ([0.004]+[0.000])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2186 (0.2186) ([0.219]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.380
current lr 1.00000e-03
Grad=  tensor(3.9767, device='cuda:0')
Epoch: [319][0/391]	Time 0.261 (0.261)	Data 0.190 (0.190)	Loss 0.0087 (0.0087) ([0.009]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [319][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0141 (0.0086) ([0.014]+[0.000])	Prec@1 99.219 (99.876)
Epoch: [319][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0087) ([0.005]+[0.000])	Prec@1 100.000 (99.872)
Epoch: [319][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0064 (0.0087) ([0.006]+[0.000])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2672 (0.2672) ([0.267]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(0.8807, device='cuda:0')
Epoch: [320][0/391]	Time 0.260 (0.260)	Data 0.189 (0.189)	Loss 0.0068 (0.0068) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0063 (0.0085) ([0.006]+[0.000])	Prec@1 100.000 (99.830)
Epoch: [320][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0070 (0.0085) ([0.007]+[0.000])	Prec@1 100.000 (99.841)
Epoch: [320][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0082) ([0.006]+[0.000])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2511 (0.2511) ([0.251]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(1.0636, device='cuda:0')
Epoch: [321][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0052 (0.0076) ([0.005]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [321][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0078) ([0.006]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [321][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0097 (0.0079) ([0.010]+[0.000])	Prec@1 100.000 (99.886)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2701 (0.2701) ([0.270]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.0874, device='cuda:0')
Epoch: [322][0/391]	Time 0.253 (0.253)	Data 0.183 (0.183)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0169 (0.0089) ([0.017]+[0.000])	Prec@1 99.219 (99.869)
Epoch: [322][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0182 (0.0087) ([0.018]+[0.000])	Prec@1 99.219 (99.876)
Epoch: [322][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0159 (0.0084) ([0.016]+[0.000])	Prec@1 99.219 (99.873)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2497 (0.2497) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(3.3677, device='cuda:0')
Epoch: [323][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0120 (0.0120) ([0.012]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [323][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0081) ([0.003]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [323][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0081) ([0.005]+[0.000])	Prec@1 100.000 (99.895)
Epoch: [323][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0079) ([0.006]+[0.000])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2266 (0.2266) ([0.227]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(7.2263, device='cuda:0')
Epoch: [324][0/391]	Time 0.271 (0.271)	Data 0.201 (0.201)	Loss 0.0110 (0.0110) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [324][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0063 (0.0080) ([0.006]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [324][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0084) ([0.003]+[0.000])	Prec@1 100.000 (99.856)
Epoch: [324][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0285 (0.0085) ([0.029]+[0.000])	Prec@1 98.438 (99.855)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2241 (0.2241) ([0.224]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(0.2440, device='cuda:0')
Epoch: [325][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0086 (0.0070) ([0.009]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [325][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0078) ([0.005]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [325][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0077 (0.0079) ([0.008]+[0.000])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2227 (0.2227) ([0.223]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.410
current lr 1.00000e-03
Grad=  tensor(1.1204, device='cuda:0')
Epoch: [326][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0108 (0.0108) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0119 (0.0085) ([0.012]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [326][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0083) ([0.004]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [326][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0084) ([0.006]+[0.000])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2169 (0.2169) ([0.217]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(4.2284, device='cuda:0')
Epoch: [327][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0213 (0.0213) ([0.021]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [327][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0070) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [327][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0074 (0.0078) ([0.007]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [327][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0078) ([0.007]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2270 (0.2270) ([0.227]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.120
current lr 1.00000e-03
Grad=  tensor(0.4595, device='cuda:0')
Epoch: [328][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0076 (0.0076) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0128 (0.0072) ([0.013]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [328][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0077 (0.0071) ([0.008]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [328][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0070 (0.0074) ([0.007]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2268 (0.2268) ([0.227]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.550
current lr 1.00000e-03
Grad=  tensor(0.4022, device='cuda:0')
Epoch: [329][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0137 (0.0078) ([0.014]+[0.000])	Prec@1 99.219 (99.884)
Epoch: [329][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0083 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [329][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0078) ([0.005]+[0.000])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2437 (0.2437) ([0.244]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(0.6738, device='cuda:0')
Epoch: [330][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0071 (0.0071) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0111 (0.0079) ([0.011]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [330][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0154 (0.0083) ([0.015]+[0.000])	Prec@1 99.219 (99.864)
Epoch: [330][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0161 (0.0084) ([0.016]+[0.000])	Prec@1 99.219 (99.852)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2392 (0.2392) ([0.239]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.0847, device='cuda:0')
Epoch: [331][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0053 (0.0074) ([0.005]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [331][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0111 (0.0075) ([0.011]+[0.000])	Prec@1 99.219 (99.911)
Epoch: [331][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0110 (0.0080) ([0.011]+[0.000])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2509 (0.2509) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(0.2710, device='cuda:0')
Epoch: [332][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0068 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [332][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0073) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [332][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0076) ([0.004]+[0.000])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2570 (0.2570) ([0.257]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(2.2749, device='cuda:0')
Epoch: [333][0/391]	Time 0.264 (0.264)	Data 0.195 (0.195)	Loss 0.0103 (0.0103) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0295 (0.0076) ([0.029]+[0.000])	Prec@1 99.219 (99.892)
Epoch: [333][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0061 (0.0081) ([0.006]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [333][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0061 (0.0078) ([0.006]+[0.000])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2482 (0.2482) ([0.248]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(1.3196, device='cuda:0')
Epoch: [334][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0075 (0.0075) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0086 (0.0073) ([0.009]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [334][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0071) ([0.003]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [334][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0073) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2550 (0.2550) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(0.2112, device='cuda:0')
Epoch: [335][0/391]	Time 0.263 (0.263)	Data 0.194 (0.194)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [335][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0041 (0.0078) ([0.004]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [335][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0113 (0.0076) ([0.011]+[0.000])	Prec@1 100.000 (99.883)
Epoch: [335][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0073) ([0.008]+[0.000])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2378 (0.2378) ([0.238]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.440
current lr 1.00000e-03
Grad=  tensor(7.2368, device='cuda:0')
Epoch: [336][0/391]	Time 0.255 (0.255)	Data 0.186 (0.186)	Loss 0.0278 (0.0278) ([0.028]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [336][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0035 (0.0071) ([0.004]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [336][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0073) ([0.006]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [336][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0071) ([0.003]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2540 (0.2540) ([0.254]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.2985, device='cuda:0')
Epoch: [337][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0066) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [337][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0069) ([0.004]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [337][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0070) ([0.005]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2164 (0.2164) ([0.216]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(1.1366, device='cuda:0')
Epoch: [338][0/391]	Time 0.255 (0.255)	Data 0.186 (0.186)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0131 (0.0077) ([0.013]+[0.000])	Prec@1 99.219 (99.899)
Epoch: [338][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0086 (0.0077) ([0.009]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [338][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0075) ([0.006]+[0.000])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2672 (0.2672) ([0.267]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(2.0442, device='cuda:0')
Epoch: [339][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0116 (0.0116) ([0.012]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0079) ([0.003]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [339][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0075) ([0.006]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [339][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0076) ([0.005]+[0.000])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2452 (0.2452) ([0.245]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(0.0113, device='cuda:0')
Epoch: [340][0/391]	Time 0.254 (0.254)	Data 0.186 (0.186)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0078) ([0.003]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [340][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0075) ([0.005]+[0.000])	Prec@1 100.000 (99.895)
Epoch: [340][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0073) ([0.005]+[0.000])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2443 (0.2443) ([0.244]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [341][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0072 (0.0075) ([0.007]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [341][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0137 (0.0070) ([0.014]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [341][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0069) ([0.002]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2837 (0.2837) ([0.284]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.510
current lr 1.00000e-03
Grad=  tensor(0.0464, device='cuda:0')
Epoch: [342][0/391]	Time 0.267 (0.267)	Data 0.198 (0.198)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0068) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [342][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0068) ([0.003]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [342][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0068) ([0.004]+[0.000])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2655 (0.2655) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.100
current lr 1.00000e-03
Grad=  tensor(0.4752, device='cuda:0')
Epoch: [343][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0091 (0.0091) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0084 (0.0070) ([0.008]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [343][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0066) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [343][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0070) ([0.002]+[0.000])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2752 (0.2752) ([0.275]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.580
current lr 1.00000e-03
Grad=  tensor(1.3693, device='cuda:0')
Epoch: [344][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0078 (0.0078) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0086 (0.0069) ([0.009]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [344][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0068) ([0.003]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [344][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0125 (0.0074) ([0.012]+[0.000])	Prec@1 99.219 (99.891)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2642 (0.2642) ([0.264]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.540
current lr 1.00000e-03
Grad=  tensor(0.1090, device='cuda:0')
Epoch: [345][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0070) ([0.001]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [345][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0126 (0.0071) ([0.013]+[0.000])	Prec@1 100.000 (99.903)
Epoch: [345][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0071) ([0.004]+[0.000])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2899 (0.2899) ([0.290]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(6.2666, device='cuda:0')
Epoch: [346][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0172 (0.0172) ([0.017]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [346][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0052 (0.0063) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [346][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0074 (0.0064) ([0.007]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [346][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0065) ([0.005]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2788 (0.2788) ([0.279]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.540
current lr 1.00000e-03
Grad=  tensor(0.1913, device='cuda:0')
Epoch: [347][0/391]	Time 0.255 (0.255)	Data 0.186 (0.186)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0072 (0.0071) ([0.007]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [347][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0132 (0.0074) ([0.013]+[0.000])	Prec@1 100.000 (99.860)
Epoch: [347][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0075) ([0.007]+[0.000])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2676 (0.2676) ([0.268]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.5241, device='cuda:0')
Epoch: [348][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0368 (0.0073) ([0.037]+[0.000])	Prec@1 99.219 (99.892)
Epoch: [348][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0074) ([0.004]+[0.000])	Prec@1 100.000 (99.891)
Epoch: [348][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0077) ([0.007]+[0.000])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2658 (0.2658) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(0.0401, device='cuda:0')
Epoch: [349][0/391]	Time 0.268 (0.268)	Data 0.198 (0.198)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0108 (0.0079) ([0.011]+[0.000])	Prec@1 100.000 (99.838)
Epoch: [349][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0077) ([0.003]+[0.000])	Prec@1 100.000 (99.845)
Epoch: [349][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0143 (0.0072) ([0.014]+[0.000])	Prec@1 99.219 (99.883)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.3125 (0.3125) ([0.312]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.550
current lr 1.00000e-04
Grad=  tensor(0.9158, device='cuda:0')
Epoch: [350][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0077 (0.0077) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0027 (0.0067) ([0.003]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [350][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0064) ([0.005]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [350][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0064) ([0.002]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3008 (0.3008) ([0.301]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-04
Grad=  tensor(1.0283, device='cuda:0')
Epoch: [351][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0047 (0.0061) ([0.005]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [351][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0078 (0.0061) ([0.008]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [351][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0111 (0.0062) ([0.011]+[0.000])	Prec@1 99.219 (99.920)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.3032 (0.3032) ([0.303]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(0.9732, device='cuda:0')
Epoch: [352][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [352][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0061) ([0.007]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [352][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0060) ([0.003]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2877 (0.2877) ([0.288]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-04
Grad=  tensor(0.8342, device='cuda:0')
Epoch: [353][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0036 (0.0059) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [353][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0059) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [353][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0120 (0.0061) ([0.012]+[0.000])	Prec@1 99.219 (99.925)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2764 (0.2764) ([0.276]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(4.9524, device='cuda:0')
Epoch: [354][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [354][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0057) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [354][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2814 (0.2814) ([0.281]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-04
Grad=  tensor(0.1081, device='cuda:0')
Epoch: [355][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0061) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [355][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0145 (0.0060) ([0.015]+[0.000])	Prec@1 99.219 (99.922)
Epoch: [355][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2796 (0.2796) ([0.280]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-04
Grad=  tensor(0.2016, device='cuda:0')
Epoch: [356][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [356][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [356][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0057) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2734 (0.2734) ([0.273]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.550
current lr 1.00000e-04
Grad=  tensor(0.5310, device='cuda:0')
Epoch: [357][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0071 (0.0071) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0061 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [357][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0058) ([0.005]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [357][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0057) ([0.003]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2795 (0.2795) ([0.279]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-04
Grad=  tensor(0.1403, device='cuda:0')
Epoch: [358][0/391]	Time 0.271 (0.271)	Data 0.202 (0.202)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0032 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [358][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [358][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0094 (0.0052) ([0.009]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2914 (0.2914) ([0.291]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.710
current lr 1.00000e-04
Grad=  tensor(2.8126, device='cuda:0')
Epoch: [359][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0154 (0.0154) ([0.015]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0044 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [359][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0058) ([0.005]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [359][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0059) ([0.007]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2723 (0.2723) ([0.272]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-04
Grad=  tensor(0.1189, device='cuda:0')
Epoch: [360][0/391]	Time 0.263 (0.263)	Data 0.194 (0.194)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0040 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [360][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0057) ([0.008]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [360][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0061 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2707 (0.2707) ([0.271]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.610
current lr 1.00000e-04
Grad=  tensor(0.2124, device='cuda:0')
Epoch: [361][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [361][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [361][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0131 (0.0055) ([0.013]+[0.000])	Prec@1 99.219 (99.948)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2854 (0.2854) ([0.285]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-04
Grad=  tensor(0.5333, device='cuda:0')
Epoch: [362][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0058 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0049 (0.0059) ([0.005]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [362][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0058) ([0.003]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [362][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0055) ([0.007]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2697 (0.2697) ([0.270]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.590
current lr 1.00000e-04
Grad=  tensor(0.1160, device='cuda:0')
Epoch: [363][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0131 (0.0047) ([0.013]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [363][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0110 (0.0052) ([0.011]+[0.000])	Prec@1 99.219 (99.953)
Epoch: [363][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2579 (0.2579) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-04
Grad=  tensor(1.8963, device='cuda:0')
Epoch: [364][0/391]	Time 0.259 (0.259)	Data 0.190 (0.190)	Loss 0.0082 (0.0082) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0081 (0.0051) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [364][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [364][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2669 (0.2669) ([0.267]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.670
current lr 1.00000e-04
Grad=  tensor(0.0324, device='cuda:0')
Epoch: [365][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [365][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0159 (0.0056) ([0.016]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [365][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0056) ([0.007]+[0.000])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2561 (0.2561) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-04
Grad=  tensor(0.0782, device='cuda:0')
Epoch: [366][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0060) ([0.002]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [366][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0060) ([0.001]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [366][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2581 (0.2581) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-04
Grad=  tensor(0.1681, device='cuda:0')
Epoch: [367][0/391]	Time 0.269 (0.269)	Data 0.199 (0.199)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [367][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [367][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0053) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2575 (0.2575) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-04
Grad=  tensor(0.1043, device='cuda:0')
Epoch: [368][0/391]	Time 0.259 (0.259)	Data 0.190 (0.190)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0010 (0.0052) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [368][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [368][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0086 (0.0056) ([0.009]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2521 (0.2521) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.560
current lr 1.00000e-04
Grad=  tensor(0.1508, device='cuda:0')
Epoch: [369][0/391]	Time 0.270 (0.270)	Data 0.200 (0.200)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0018 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [369][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0053) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [369][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2671 (0.2671) ([0.267]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.700
current lr 1.00000e-04
Grad=  tensor(0.0154, device='cuda:0')
Epoch: [370][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0041 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [370][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [370][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2594 (0.2594) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.710
current lr 1.00000e-04
Grad=  tensor(5.0219, device='cuda:0')
Epoch: [371][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0095 (0.0095) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0054 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [371][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0050) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [371][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2582 (0.2582) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-04
Grad=  tensor(6.5600, device='cuda:0')
Epoch: [372][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0137 (0.0137) ([0.014]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [372][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0200 (0.0055) ([0.020]+[0.000])	Prec@1 99.219 (99.907)
Epoch: [372][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0057) ([0.002]+[0.000])	Prec@1 100.000 (99.903)
Epoch: [372][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2668 (0.2668) ([0.267]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.700
current lr 1.00000e-04
Grad=  tensor(0.3469, device='cuda:0')
Epoch: [373][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0071 (0.0050) ([0.007]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [373][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [373][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0010 (0.0052) ([0.001]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2850 (0.2850) ([0.285]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-04
Grad=  tensor(0.0278, device='cuda:0')
Epoch: [374][0/391]	Time 0.261 (0.261)	Data 0.190 (0.190)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0060 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [374][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [374][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2861 (0.2861) ([0.286]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.590
current lr 1.00000e-04
Grad=  tensor(0.3991, device='cuda:0')
Epoch: [375][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0114 (0.0061) ([0.011]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [375][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0060) ([0.008]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [375][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2621 (0.2621) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-04
Grad=  tensor(0.3965, device='cuda:0')
Epoch: [376][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0065 (0.0052) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [376][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [376][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2617 (0.2617) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-04
Grad=  tensor(0.0957, device='cuda:0')
Epoch: [377][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [377][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [377][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2522 (0.2522) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.610
current lr 1.00000e-04
Grad=  tensor(0.5739, device='cuda:0')
Epoch: [378][0/391]	Time 0.260 (0.260)	Data 0.191 (0.191)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [378][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [378][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0108 (0.0053) ([0.011]+[0.000])	Prec@1 99.219 (99.943)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2764 (0.2764) ([0.276]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.480
current lr 1.00000e-04
Grad=  tensor(0.1537, device='cuda:0')
Epoch: [379][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0044 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [379][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0227 (0.0051) ([0.023]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [379][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2655 (0.2655) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-04
Grad=  tensor(0.3287, device='cuda:0')
Epoch: [380][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0080 (0.0048) ([0.008]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [380][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [380][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2536 (0.2536) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-04
Grad=  tensor(0.2528, device='cuda:0')
Epoch: [381][0/391]	Time 0.259 (0.259)	Data 0.190 (0.190)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0014 (0.0055) ([0.001]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [381][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0055) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [381][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2594 (0.2594) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-04
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [382][0/391]	Time 0.254 (0.254)	Data 0.185 (0.185)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0046 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [382][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [382][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2657 (0.2657) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-04
Grad=  tensor(0.1896, device='cuda:0')
Epoch: [383][0/391]	Time 0.255 (0.255)	Data 0.185 (0.185)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0040 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [383][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0186 (0.0051) ([0.019]+[0.000])	Prec@1 99.219 (99.953)
Epoch: [383][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2616 (0.2616) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.690
current lr 1.00000e-04
Grad=  tensor(0.0578, device='cuda:0')
Epoch: [384][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [384][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0053) ([0.001]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [384][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2678 (0.2678) ([0.268]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(0.0055, device='cuda:0')
Epoch: [385][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0044 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [385][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [385][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0066 (0.0048) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2684 (0.2684) ([0.268]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-04
Grad=  tensor(0.0491, device='cuda:0')
Epoch: [386][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0049) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [386][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0050) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [386][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2567 (0.2567) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.670
current lr 1.00000e-04
Grad=  tensor(8.7134, device='cuda:0')
Epoch: [387][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0209 (0.0209) ([0.021]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [387][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0093 (0.0058) ([0.009]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [387][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0054) ([0.007]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [387][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2687 (0.2687) ([0.269]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-04
Grad=  tensor(4.0908, device='cuda:0')
Epoch: [388][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0105 (0.0105) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0026 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [388][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [388][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2606 (0.2606) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-04
Grad=  tensor(0.0209, device='cuda:0')
Epoch: [389][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0066 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [389][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [389][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2589 (0.2589) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-04
Grad=  tensor(1.0516, device='cuda:0')
Epoch: [390][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0088 (0.0088) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0051) ([0.001]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [390][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0052) ([0.006]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [390][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2771 (0.2771) ([0.277]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(0.3188, device='cuda:0')
Epoch: [391][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0032 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [391][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [391][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2557 (0.2557) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.590
current lr 1.00000e-04
Grad=  tensor(0.2983, device='cuda:0')
Epoch: [392][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0069 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [392][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [392][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2545 (0.2545) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.500
current lr 1.00000e-04
Grad=  tensor(1.5591, device='cuda:0')
Epoch: [393][0/391]	Time 0.255 (0.255)	Data 0.185 (0.185)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0140 (0.0052) ([0.014]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [393][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [393][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2372 (0.2372) ([0.237]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-04
Grad=  tensor(4.8219, device='cuda:0')
Epoch: [394][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0220 (0.0220) ([0.022]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [394][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0039 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [394][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [394][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2400 (0.2400) ([0.240]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(0.1847, device='cuda:0')
Epoch: [395][0/391]	Time 0.255 (0.255)	Data 0.185 (0.185)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0036 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [395][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [395][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2580 (0.2580) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-04
Grad=  tensor(10.3333, device='cuda:0')
Epoch: [396][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0142 (0.0142) ([0.014]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [396][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0052 (0.0056) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [396][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0052) ([0.008]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [396][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0133 (0.0054) ([0.013]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2621 (0.2621) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-04
Grad=  tensor(0.1651, device='cuda:0')
Epoch: [397][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [397][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0051) ([0.012]+[0.000])	Prec@1 99.219 (99.953)
Epoch: [397][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2522 (0.2522) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-04
Grad=  tensor(0.1999, device='cuda:0')
Epoch: [398][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0070 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [398][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [398][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0074 (0.0052) ([0.007]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2681 (0.2681) ([0.268]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-04
Grad=  tensor(0.2897, device='cuda:0')
Epoch: [399][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0124 (0.0050) ([0.012]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [399][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [399][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2736 (0.2736) ([0.274]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-05
Grad=  tensor(0.4985, device='cuda:0')
Epoch: [400][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0064 (0.0049) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [400][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0141 (0.0052) ([0.014]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [400][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0071 (0.0053) ([0.007]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2601 (0.2601) ([0.260]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-05
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [401][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [401][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0044) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [401][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0078 (0.0046) ([0.008]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2703 (0.2703) ([0.270]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.0125, device='cuda:0')
Epoch: [402][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0021 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [402][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [402][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2460 (0.2460) ([0.246]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-05
Grad=  tensor(0.1345, device='cuda:0')
Epoch: [403][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [403][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [403][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2555 (0.2555) ([0.256]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.590
current lr 1.00000e-05
Grad=  tensor(0.4608, device='cuda:0')
Epoch: [404][0/391]	Time 0.256 (0.256)	Data 0.187 (0.187)	Loss 0.0078 (0.0078) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0032 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [404][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0048) ([0.001]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [404][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0047) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2504 (0.2504) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-05
Grad=  tensor(0.0238, device='cuda:0')
Epoch: [405][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0049 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [405][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [405][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2539 (0.2539) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.690
current lr 1.00000e-05
Grad=  tensor(0.2361, device='cuda:0')
Epoch: [406][0/391]	Time 0.260 (0.260)	Data 0.189 (0.189)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0105 (0.0054) ([0.010]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [406][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [406][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0009 (0.0049) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2516 (0.2516) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-05
Grad=  tensor(0.2934, device='cuda:0')
Epoch: [407][0/391]	Time 0.311 (0.311)	Data 0.241 (0.241)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.062 (0.065)	Data 0.000 (0.002)	Loss 0.0047 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [407][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [407][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2618 (0.2618) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(0.2103, device='cuda:0')
Epoch: [408][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [408][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [408][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2618 (0.2618) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.2328, device='cuda:0')
Epoch: [409][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [409][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [409][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2626 (0.2626) ([0.263]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.460
current lr 1.00000e-05
Grad=  tensor(0.0514, device='cuda:0')
Epoch: [410][0/391]	Time 0.255 (0.255)	Data 0.186 (0.186)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0045 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [410][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0127 (0.0054) ([0.013]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [410][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0053) ([0.008]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2495 (0.2495) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-05
Grad=  tensor(0.5225, device='cuda:0')
Epoch: [411][0/391]	Time 0.261 (0.261)	Data 0.190 (0.190)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0035 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [411][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [411][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2518 (0.2518) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.590
current lr 1.00000e-05
Grad=  tensor(0.0118, device='cuda:0')
Epoch: [412][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0050 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [412][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0121 (0.0049) ([0.012]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [412][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0154 (0.0050) ([0.015]+[0.000])	Prec@1 99.219 (99.956)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2564 (0.2564) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-05
Grad=  tensor(2.9091, device='cuda:0')
Epoch: [413][0/391]	Time 0.264 (0.264)	Data 0.195 (0.195)	Loss 0.0068 (0.0068) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0056 (0.0049) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [413][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [413][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2578 (0.2578) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(0.6093, device='cuda:0')
Epoch: [414][0/391]	Time 0.260 (0.260)	Data 0.191 (0.191)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0055 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [414][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [414][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2607 (0.2607) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-05
Grad=  tensor(0.1408, device='cuda:0')
Epoch: [415][0/391]	Time 0.259 (0.259)	Data 0.190 (0.190)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0032 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [415][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0144 (0.0048) ([0.014]+[0.000])	Prec@1 99.219 (99.957)
Epoch: [415][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0049) ([0.010]+[0.000])	Prec@1 99.219 (99.958)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2513 (0.2513) ([0.251]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(2.7465, device='cuda:0')
Epoch: [416][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0126 (0.0126) ([0.013]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0039 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [416][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0053) ([0.002]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [416][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2617 (0.2617) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-05
Grad=  tensor(0.1645, device='cuda:0')
Epoch: [417][0/391]	Time 0.262 (0.262)	Data 0.193 (0.193)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [417][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [417][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0083 (0.0052) ([0.008]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2558 (0.2558) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(0.2308, device='cuda:0')
Epoch: [418][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0016 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [418][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [418][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2560 (0.2560) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-05
Grad=  tensor(8.4020, device='cuda:0')
Epoch: [419][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0164 (0.0164) ([0.016]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [419][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [419][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0051) ([0.006]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [419][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2535 (0.2535) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-05
Grad=  tensor(0.0219, device='cuda:0')
Epoch: [420][0/391]	Time 0.257 (0.257)	Data 0.186 (0.186)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [420][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0093 (0.0051) ([0.009]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [420][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0053) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2475 (0.2475) ([0.247]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-05
Grad=  tensor(1.4852, device='cuda:0')
Epoch: [421][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0081 (0.0081) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0018 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [421][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [421][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2525 (0.2525) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-05
Grad=  tensor(0.7965, device='cuda:0')
Epoch: [422][0/391]	Time 0.259 (0.259)	Data 0.190 (0.190)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0013 (0.0053) ([0.001]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [422][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0158 (0.0049) ([0.016]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [422][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0070 (0.0050) ([0.007]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2449 (0.2449) ([0.245]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.530
current lr 1.00000e-05
Grad=  tensor(0.1110, device='cuda:0')
Epoch: [423][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [423][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [423][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0256 (0.0048) ([0.026]+[0.000])	Prec@1 99.219 (99.958)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2613 (0.2613) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(0.6602, device='cuda:0')
Epoch: [424][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0070 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [424][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0124 (0.0050) ([0.012]+[0.000])	Prec@1 99.219 (99.957)
Epoch: [424][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2547 (0.2547) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-05
Grad=  tensor(0.0364, device='cuda:0')
Epoch: [425][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0051 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [425][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [425][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2553 (0.2553) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-05
Grad=  tensor(0.4900, device='cuda:0')
Epoch: [426][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0071 (0.0047) ([0.007]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [426][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0049) ([0.006]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [426][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2576 (0.2576) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-05
Grad=  tensor(0.9518, device='cuda:0')
Epoch: [427][0/391]	Time 0.261 (0.261)	Data 0.190 (0.190)	Loss 0.0074 (0.0074) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0039 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [427][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [427][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2473 (0.2473) ([0.247]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-05
Grad=  tensor(0.3219, device='cuda:0')
Epoch: [428][0/391]	Time 0.302 (0.302)	Data 0.231 (0.231)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.062 (0.065)	Data 0.000 (0.002)	Loss 0.0037 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [428][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [428][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0084 (0.0049) ([0.008]+[0.000])	Prec@1 99.219 (99.964)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2569 (0.2569) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.400
current lr 1.00000e-05
Grad=  tensor(0.0327, device='cuda:0')
Epoch: [429][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0035 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [429][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [429][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2493 (0.2493) ([0.249]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-05
Grad=  tensor(0.2367, device='cuda:0')
Epoch: [430][0/391]	Time 0.258 (0.258)	Data 0.187 (0.187)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0027 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [430][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [430][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2555 (0.2555) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-05
Grad=  tensor(2.4195, device='cuda:0')
Epoch: [431][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0047 (0.0044) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [431][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [431][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2501 (0.2501) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(0.0174, device='cuda:0')
Epoch: [432][0/391]	Time 0.257 (0.257)	Data 0.188 (0.188)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [432][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0057) ([0.005]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [432][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2586 (0.2586) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.0235, device='cuda:0')
Epoch: [433][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [433][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [433][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2564 (0.2564) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.670
current lr 1.00000e-05
Grad=  tensor(0.2540, device='cuda:0')
Epoch: [434][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [434][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [434][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2519 (0.2519) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.510
current lr 1.00000e-05
Grad=  tensor(0.1865, device='cuda:0')
Epoch: [435][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.062 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [435][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [435][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2492 (0.2492) ([0.249]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(0.4821, device='cuda:0')
Epoch: [436][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0059) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [436][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [436][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0049) ([0.009]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2545 (0.2545) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.3006, device='cuda:0')
Epoch: [437][0/391]	Time 0.267 (0.267)	Data 0.197 (0.197)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [437][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0115 (0.0047) ([0.012]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [437][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0045) ([0.006]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2469 (0.2469) ([0.247]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-05
Grad=  tensor(0.3109, device='cuda:0')
Epoch: [438][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0031 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [438][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [438][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2537 (0.2537) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-05
Grad=  tensor(0.4946, device='cuda:0')
Epoch: [439][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [439][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [439][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0050) ([0.008]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2571 (0.2571) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(0.1660, device='cuda:0')
Epoch: [440][0/391]	Time 0.254 (0.254)	Data 0.185 (0.185)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0016 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [440][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0050) ([0.010]+[0.000])	Prec@1 99.219 (99.938)
Epoch: [440][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0049) ([0.009]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2552 (0.2552) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-05
Grad=  tensor(0.0583, device='cuda:0')
Epoch: [441][0/391]	Time 0.265 (0.265)	Data 0.196 (0.196)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0057 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [441][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [441][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0048) ([0.006]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2565 (0.2565) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(0.0442, device='cuda:0')
Epoch: [442][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [442][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0047) ([0.008]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [442][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2477 (0.2477) ([0.248]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.0435, device='cuda:0')
Epoch: [443][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0036 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [443][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0044) ([0.008]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [443][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2650 (0.2650) ([0.265]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.550
current lr 1.00000e-05
Grad=  tensor(2.5130, device='cuda:0')
Epoch: [444][0/391]	Time 0.267 (0.267)	Data 0.198 (0.198)	Loss 0.0104 (0.0104) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0031 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [444][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [444][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0106 (0.0052) ([0.011]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2592 (0.2592) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-05
Grad=  tensor(0.2479, device='cuda:0')
Epoch: [445][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0052 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [445][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0118 (0.0050) ([0.012]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [445][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0049) ([0.001]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2479 (0.2479) ([0.248]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-05
Grad=  tensor(1.6194, device='cuda:0')
Epoch: [446][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [446][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0050) ([0.007]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [446][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0048) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2537 (0.2537) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.720
current lr 1.00000e-05
Grad=  tensor(1.6957, device='cuda:0')
Epoch: [447][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0068 (0.0068) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0069 (0.0055) ([0.007]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [447][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [447][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0116 (0.0052) ([0.012]+[0.000])	Prec@1 99.219 (99.945)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2527 (0.2527) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-05
Grad=  tensor(0.2560, device='cuda:0')
Epoch: [448][0/391]	Time 0.260 (0.260)	Data 0.191 (0.191)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0043 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [448][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0045) ([0.008]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [448][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2596 (0.2596) ([0.260]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-05
Grad=  tensor(0.1597, device='cuda:0')
Epoch: [449][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0074 (0.0046) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [449][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [449][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2604 (0.2604) ([0.260]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-06
Grad=  tensor(0.4231, device='cuda:0')
Epoch: [450][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0083 (0.0083) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0083 (0.0053) ([0.008]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [450][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [450][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2499 (0.2499) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-06
Grad=  tensor(0.3370, device='cuda:0')
Epoch: [451][0/391]	Time 0.267 (0.267)	Data 0.197 (0.197)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0026 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [451][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0051) ([0.008]+[0.000])	Prec@1 99.219 (99.938)
Epoch: [451][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2587 (0.2587) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-06
Grad=  tensor(0.1182, device='cuda:0')
Epoch: [452][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [452][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0050) ([0.007]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [452][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2553 (0.2553) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.700
current lr 1.00000e-06
Grad=  tensor(0.2898, device='cuda:0')
Epoch: [453][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [453][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0102 (0.0048) ([0.010]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [453][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0049) ([0.007]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2608 (0.2608) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-06
Grad=  tensor(0.0527, device='cuda:0')
Epoch: [454][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0039 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [454][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0110 (0.0050) ([0.011]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [454][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2671 (0.2671) ([0.267]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-06
Grad=  tensor(0.2693, device='cuda:0')
Epoch: [455][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0031 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [455][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0076 (0.0047) ([0.008]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [455][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2511 (0.2511) ([0.251]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.610
current lr 1.00000e-06
Grad=  tensor(0.0084, device='cuda:0')
Epoch: [456][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [456][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [456][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2536 (0.2536) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-06
Grad=  tensor(0.1183, device='cuda:0')
Epoch: [457][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0037 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [457][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0048) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [457][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2551 (0.2551) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-06
Grad=  tensor(0.2593, device='cuda:0')
Epoch: [458][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0028 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [458][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [458][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2606 (0.2606) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.670
current lr 1.00000e-06
Grad=  tensor(0.0054, device='cuda:0')
Epoch: [459][0/391]	Time 0.266 (0.266)	Data 0.196 (0.196)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0021 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [459][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [459][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2540 (0.2540) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-06
Grad=  tensor(0.0420, device='cuda:0')
Epoch: [460][0/391]	Time 0.259 (0.259)	Data 0.191 (0.191)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0057 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.892)
Epoch: [460][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [460][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2528 (0.2528) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-06
Grad=  tensor(2.1114, device='cuda:0')
Epoch: [461][0/391]	Time 0.266 (0.266)	Data 0.197 (0.197)	Loss 0.0107 (0.0107) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0035 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [461][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [461][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2548 (0.2548) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-06
Grad=  tensor(2.8394, device='cuda:0')
Epoch: [462][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0098 (0.0098) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0063 (0.0053) ([0.006]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [462][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0143 (0.0053) ([0.014]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [462][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0051) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2519 (0.2519) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-06
Grad=  tensor(0.1175, device='cuda:0')
Epoch: [463][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0021 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [463][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [463][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0048) ([0.006]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2561 (0.2561) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-06
Grad=  tensor(0.9320, device='cuda:0')
Epoch: [464][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [464][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0051) ([0.012]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [464][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2653 (0.2653) ([0.265]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-06
Grad=  tensor(2.9995, device='cuda:0')
Epoch: [465][0/391]	Time 0.267 (0.267)	Data 0.197 (0.197)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [465][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0072 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [465][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2582 (0.2582) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-06
Grad=  tensor(4.5382, device='cuda:0')
Epoch: [466][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0096 (0.0096) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0028 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [466][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [466][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0049) ([0.009]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2623 (0.2623) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.700
current lr 1.00000e-06
Grad=  tensor(0.1682, device='cuda:0')
Epoch: [467][0/391]	Time 0.267 (0.267)	Data 0.197 (0.197)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0013 (0.0048) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [467][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [467][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2639 (0.2639) ([0.264]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.470
current lr 1.00000e-06
Grad=  tensor(0.4100, device='cuda:0')
Epoch: [468][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [468][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0113 (0.0047) ([0.011]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [468][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2609 (0.2609) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-06
Grad=  tensor(0.0601, device='cuda:0')
Epoch: [469][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [469][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [469][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2589 (0.2589) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-06
Grad=  tensor(0.0314, device='cuda:0')
Epoch: [470][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0028 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [470][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0097 (0.0054) ([0.010]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [470][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0053) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2463 (0.2463) ([0.246]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-06
Grad=  tensor(0.6223, device='cuda:0')
Epoch: [471][0/391]	Time 0.259 (0.259)	Data 0.188 (0.188)	Loss 0.0077 (0.0077) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [471][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [471][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2598 (0.2598) ([0.260]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-06
Grad=  tensor(0.0894, device='cuda:0')
Epoch: [472][0/391]	Time 0.267 (0.267)	Data 0.197 (0.197)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [472][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [472][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2592 (0.2592) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.430
current lr 1.00000e-06
Grad=  tensor(4.4517, device='cuda:0')
Epoch: [473][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0028 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [473][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [473][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0046) ([0.006]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2427 (0.2427) ([0.243]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.550
current lr 1.00000e-06
Grad=  tensor(1.1681, device='cuda:0')
Epoch: [474][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0025 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [474][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [474][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2640 (0.2640) ([0.264]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.520
current lr 1.00000e-06
Grad=  tensor(0.1461, device='cuda:0')
Epoch: [475][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0041 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [475][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [475][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0046) ([0.009]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2592 (0.2592) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-06
Grad=  tensor(0.5310, device='cuda:0')
Epoch: [476][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0019 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [476][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0048) ([0.001]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [476][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2529 (0.2529) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.440
current lr 1.00000e-06
Grad=  tensor(0.0284, device='cuda:0')
Epoch: [477][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [477][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [477][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2719 (0.2719) ([0.272]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-06
Grad=  tensor(0.5124, device='cuda:0')
Epoch: [478][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [478][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [478][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2496 (0.2496) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.480
current lr 1.00000e-06
Grad=  tensor(0.0964, device='cuda:0')
Epoch: [479][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0109 (0.0045) ([0.011]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [479][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [479][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0048) ([0.006]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2633 (0.2633) ([0.263]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-06
Grad=  tensor(0.4117, device='cuda:0')
Epoch: [480][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [480][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0096 (0.0048) ([0.010]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [480][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0096 (0.0050) ([0.010]+[0.000])	Prec@1 99.219 (99.943)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2548 (0.2548) ([0.255]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.710
current lr 1.00000e-06
Grad=  tensor(1.8116, device='cuda:0')
Epoch: [481][0/391]	Time 0.266 (0.266)	Data 0.196 (0.196)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0033 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [481][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0093 (0.0046) ([0.009]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [481][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2614 (0.2614) ([0.261]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-06
Grad=  tensor(2.1763, device='cuda:0')
Epoch: [482][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0101 (0.0101) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0118 (0.0052) ([0.012]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [482][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [482][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0049) ([0.009]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2567 (0.2567) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.560
current lr 1.00000e-06
Grad=  tensor(0.2752, device='cuda:0')
Epoch: [483][0/391]	Time 0.258 (0.258)	Data 0.189 (0.189)	Loss 0.0045 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0135 (0.0044) ([0.013]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [483][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [483][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0114 (0.0045) ([0.011]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2622 (0.2622) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.630
current lr 1.00000e-06
Grad=  tensor(0.1083, device='cuda:0')
Epoch: [484][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0137 (0.0059) ([0.014]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [484][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [484][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0049) ([0.001]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2520 (0.2520) ([0.252]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-06
Grad=  tensor(0.1197, device='cuda:0')
Epoch: [485][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [485][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0044) ([0.005]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [485][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2662 (0.2662) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.670
current lr 1.00000e-06
Grad=  tensor(0.3498, device='cuda:0')
Epoch: [486][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0058 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [486][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0050) ([0.010]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [486][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2541 (0.2541) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-06
Grad=  tensor(0.2456, device='cuda:0')
Epoch: [487][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0068 (0.0045) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [487][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [487][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2577 (0.2577) ([0.258]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.580
current lr 1.00000e-06
Grad=  tensor(0.0078, device='cuda:0')
Epoch: [488][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0091 (0.0045) ([0.009]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [488][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0044) ([0.008]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [488][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0044) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2719 (0.2719) ([0.272]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-06
Grad=  tensor(3.8361, device='cuda:0')
Epoch: [489][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0115 (0.0115) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [489][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0044) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [489][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0046) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2674 (0.2674) ([0.267]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.620
current lr 1.00000e-06
Grad=  tensor(0.0621, device='cuda:0')
Epoch: [490][0/391]	Time 0.264 (0.264)	Data 0.195 (0.195)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [490][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0115 (0.0048) ([0.012]+[0.000])	Prec@1 99.219 (99.957)
Epoch: [490][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2560 (0.2560) ([0.256]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.680
current lr 1.00000e-06
Grad=  tensor(3.0724, device='cuda:0')
Epoch: [491][0/391]	Time 0.261 (0.261)	Data 0.192 (0.192)	Loss 0.0141 (0.0141) ([0.014]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0020 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [491][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [491][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2595 (0.2595) ([0.260]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.530
current lr 1.00000e-06
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [492][0/391]	Time 0.261 (0.261)	Data 0.193 (0.193)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [492][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [492][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2656 (0.2656) ([0.266]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
current lr 1.00000e-06
Grad=  tensor(0.7642, device='cuda:0')
Epoch: [493][0/391]	Time 0.258 (0.258)	Data 0.188 (0.188)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0040 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [493][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0058 (0.0049) ([0.006]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [493][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2537 (0.2537) ([0.254]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.530
current lr 1.00000e-06
Grad=  tensor(0.2628, device='cuda:0')
Epoch: [494][0/391]	Time 0.257 (0.257)	Data 0.187 (0.187)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0036 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [494][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [494][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2618 (0.2618) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.550
current lr 1.00000e-06
Grad=  tensor(0.0643, device='cuda:0')
Epoch: [495][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0040 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [495][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [495][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0052) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2589 (0.2589) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.540
current lr 1.00000e-06
Grad=  tensor(1.8705, device='cuda:0')
Epoch: [496][0/391]	Time 0.261 (0.261)	Data 0.191 (0.191)	Loss 0.0074 (0.0074) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0130 (0.0051) ([0.013]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [496][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [496][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2439 (0.2439) ([0.244]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-06
Grad=  tensor(1.4856, device='cuda:0')
Epoch: [497][0/391]	Time 0.263 (0.263)	Data 0.194 (0.194)	Loss 0.0080 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [497][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [497][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2634 (0.2634) ([0.263]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.550
current lr 1.00000e-06
Grad=  tensor(4.2311, device='cuda:0')
Epoch: [498][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0084 (0.0084) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0081 (0.0042) ([0.008]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [498][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [498][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2571 (0.2571) ([0.257]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.570
current lr 1.00000e-06
Grad=  tensor(0.1657, device='cuda:0')
Epoch: [499][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0086 (0.0054) ([0.009]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [499][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0052) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [499][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.2618 (0.2618) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600

 Elapsed time for training  5:19:33.849366

 sparsity of   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.328125, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.421875, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.69140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.603515625, 0.60205078125, 0.60205078125, 0.6025390625, 0.6005859375, 0.603515625, 0.60205078125, 0.60205078125, 0.60107421875, 0.60205078125, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375, 0.89599609375]
Total parameter pruned: 22143290.0 (unstructured) 21964741 (structured)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2618 (0.2618) ([0.262]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.600
Best accuracy:  93.72
