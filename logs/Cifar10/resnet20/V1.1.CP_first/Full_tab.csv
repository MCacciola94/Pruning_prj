lr	lambda	alpha	epochs	ft_epochs	batch_size	threshold	momentum	weight_decay	M_scale	Sparsity	Final_Accuracy	Tot_time	Pr_time	Accuracy	Train_loss
0.1	0.5	0.001	300	200	128	0.05	0.9	0.0005	1.0	115620	92.23	1:31:56.361835	1:10:14.730570	91.6	0.1228
0.1	0.5	0.01	300	200	128	0.05	0.9	0.0005	1.0	106384	92.26	1:27:03.126834	1:07:12.541364	91.69	0.1364
0.1	0.5	0.1	300	200	128	0.05	0.9	0.0005	1.0	120992	91.86	1:31:28.579110	1:10:21.427418	91.3	0.2005
0.1	0.8	0.0001	300	200	128	0.05	0.9	0.0005	1.0	169958	91.2	1:33:28.063926	1:12:13.136725	90.67	0.1918
0.1	0.8	0.0005	300	200	128	0.05	0.9	0.0005	1.0	163015	91.47	1:27:21.146544	1:07:33.055960	90.62	0.1813
0.1	0.8	0.001	300	200	128	0.05	0.9	0.0005	1.0	149089	91.74	1:33:20.476997	1:11:58.199907	90.99	0.1771
0.1	0.8	0.01	300	200	128	0.05	0.9	0.0005	1.0	155730	91.39	1:31:41.266154	1:10:00.031485	90.86	0.1992
0.1	0.8	0.1	300	200	128	0.05	0.9	0.0005	1.0	172658	91.22	1:33:19.809583	1:11:49.617609	90.57	0.2833
0.1	0.8	0.5	300	200	128	0.05	0.9	0.0005	1.0	195272	90.11	1:31:37.721065	1:11:24.683402	88.76	0.3734
0.1	1.0	0.0001	300	200	128	0.05	0.9	0.0005	1.0	176338	90.91	1:29:12.012969	1:08:39.713835	90.6	0.2148
0.1	1.0	0.0005	300	200	128	0.05	0.9	0.0005	1.0	179266	90.45	1:31:18.720975	1:09:52.500355	89.78	0.2207
0.1	1.0	0.001	300	200	128	0.05	0.9	0.0005	1.0	178425	90.97	1:33:18.314297	1:11:57.936617	90.0	0.2202
0.1	1.0	0.01	300	200	128	0.05	0.9	0.0005	1.0	177414	90.79	1:33:22.563436	1:12:07.244261	90.5	0.2509
0.1	1.0	0.1	300	200	128	0.05	0.9	0.0005	1.0	186474	90.63	1:34:38.415909	1:13:04.252945	88.74	0.3281
0.1	1.0	0.5	300	200	128	0.05	0.9	0.0005	1.0	209435	89.43	1:35:39.086465	1:14:21.315672	88.66	0.4221
0.1	1.3	0.0001	300	200	128	0.05	0.9	0.0005	1.0	203154	90.03	1:29:55.278962	1:09:31.977066	89.42	0.2823
0.1	1.3	0.0005	300	200	128	0.05	0.9	0.0005	1.0	200097	89.74	1:30:06.423744	1:09:38.572463	86.02	0.2861
0.1	1.3	0.001	300	200	128	0.05	0.9	0.0005	1.0	195178	90.25	1:33:07.514792	1:11:53.490324	89.43	0.2685
0.1	1.3	0.01	300	200	128	0.05	0.9	0.0005	1.0	193937	90.16	1:33:00.411970	1:11:36.087090	87.45	0.2958
0.1	1.3	0.1	300	200	128	0.05	0.9	0.0005	1.0	213958	89.46	1:32:09.113994	1:10:16.240850	84.64	0.4028
0.1	1.3	0.5	300	200	128	0.05	0.9	0.0005	1.0	228094	87.99	1:33:54.510318	1:13:13.377630	82.35	0.4742
0.1	1.6	0.0001	300	200	128	0.05	0.9	0.0005	1.0	206401	89.93	1:29:33.616547	1:09:13.516043	88.46	0.3115
0.1	1.6	0.0005	300	200	128	0.05	0.9	0.0005	1.0	212349	89.78	1:32:04.547517	1:10:51.800094	88.75	0.3228
0.1	1.6	0.001	300	200	128	0.05	0.9	0.0005	1.0	203937	89.29	1:33:14.483645	1:12:07.412198	88.59	0.3102
0.1	1.6	0.01	300	200	128	0.05	0.9	0.0005	1.0	205846	89.88	1:33:13.413274	1:11:57.466000	88.85	0.3355
0.1	1.6	0.1	300	200	128	0.05	0.9	0.0005	1.0	214794	88.82	1:31:17.747971	1:09:31.444262	86.69	0.4444
0.1	1.6	0.5	300	200	128	0.05	0.9	0.0005	1.0	232409	86.81	1:34:17.988177	1:13:43.675948	74.24	0.539
0.1	1.9	0.0001	300	200	128	0.05	0.9	0.0005	1.0	213198	89.37	1:28:16.604997	1:08:36.344274	87.94	0.3414
0.1	1.9	0.0005	300	200	128	0.05	0.9	0.0005	1.0	219076	88.31	1:32:55.840593	1:11:55.118458	86.87	0.3603
0.1	1.9	0.001	300	200	128	0.05	0.9	0.0005	1.0	212361	89.45	1:33:28.017238	1:12:31.140317	88.36	0.3428
0.1	1.9	0.01	300	200	128	0.05	0.9	0.0005	1.0	215339	88.85	1:33:19.539633	1:12:09.675027	86.5	0.3726
0.1	1.9	0.1	300	200	128	0.05	0.9	0.0005	1.0	221032	88.58	1:31:42.337652	1:09:59.926940	86.68	0.4824
0.1	1.9	0.5	300	200	128	0.05	0.9	0.0005	1.0	242424	85.63	1:33:19.791725	1:12:50.100884	81.29	0.6004
