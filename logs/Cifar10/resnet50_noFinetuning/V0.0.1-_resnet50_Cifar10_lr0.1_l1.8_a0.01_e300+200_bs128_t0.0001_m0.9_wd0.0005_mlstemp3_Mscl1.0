V0.0.1-_resnet50_Cifar10_lr0.1_l1.8_a0.01_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5254763960838318, Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.24196940660476685, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.15759095549583435, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.13501641154289246, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.3461485505104065, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.17473100125789642, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21617008745670319, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.14946585893630981, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09060623496770859, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08498729020357132, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11497705429792404, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11804789304733276, Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08379501849412918, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1424030363559723, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.19753389060497284, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.16684924066066742, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.22829987108707428, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12801074981689453, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09603530913591385, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06901206821203232, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12272872775793076, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0835055485367775, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.07954221963882446, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12703275680541992, Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.19747452437877655, Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.15407174825668335, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1602816879749298, Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.10645194351673126, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10600411146879196, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.13483507931232452, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.21709460020065308, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11353497207164764, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0660422295331955, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10686782747507095, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06808818876743317, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.05323619768023491, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.08759226649999619, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07965513318777084, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06643471866846085, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12164679169654846, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09185265004634857, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06330689787864685, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1110600158572197, Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12582343816757202, Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.09035182744264603, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07410024106502533, Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.07018566876649857, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0687103122472763, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.04065759852528572, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.03755198046565056, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.04725675657391548, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.045549724251031876, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06192586570978165, Linear(in_features=2048, out_features=100, bias=True): 0.44340774416923523}
current lr 1.00000e-01
Grad=  tensor(5794.3735, device='cuda:0')
Epoch: [0][0/391]	Time 0.369 (0.369)	Data 0.227 (0.227)	Loss 6.8397 (6.8397) ([4.684]+[2.156])	Prec@1 1.562 (1.562)
Epoch: [0][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 7.9665 (9.7583) ([2.551]+[5.415])	Prec@1 10.938 (10.860)
Epoch: [0][200/391]	Time 0.116 (0.112)	Data 0.000 (0.001)	Loss 7.0484 (8.7212) ([2.195]+[4.854])	Prec@1 14.844 (12.912)
Epoch: [0][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 6.4219 (8.0467) ([2.104]+[4.318])	Prec@1 21.094 (15.467)
Test: [0/79]	Time 0.265 (0.265)	Loss 5.9206 (5.9206) ([2.037]+[3.883])	Prec@1 25.781 (25.781)
 * Prec@1 20.410
current lr 1.00000e-01
Grad=  tensor(0.6664, device='cuda:0')
Epoch: [1][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 5.9416 (5.9416) ([2.058]+[3.883])	Prec@1 24.219 (24.219)
Epoch: [1][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 5.3672 (5.6639) ([1.910]+[3.457])	Prec@1 28.906 (24.103)
Epoch: [1][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 5.0219 (5.4413) ([1.942]+[3.080])	Prec@1 27.344 (25.117)
Epoch: [1][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 4.6300 (5.2352) ([1.879]+[2.751])	Prec@1 22.656 (26.280)
Test: [0/79]	Time 0.252 (0.252)	Loss 4.3078 (4.3078) ([1.825]+[2.483])	Prec@1 29.688 (29.688)
 * Prec@1 33.320
current lr 1.00000e-01
Grad=  tensor(0.8891, device='cuda:0')
Epoch: [2][0/391]	Time 0.344 (0.344)	Data 0.222 (0.222)	Loss 4.4457 (4.4457) ([1.963]+[2.483])	Prec@1 28.906 (28.906)
Epoch: [2][100/391]	Time 0.116 (0.117)	Data 0.000 (0.002)	Loss 3.9619 (4.1848) ([1.732]+[2.230])	Prec@1 35.938 (31.412)
Epoch: [2][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 3.9590 (4.0573) ([1.941]+[2.018])	Prec@1 31.250 (32.416)
Epoch: [2][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 3.4450 (3.9304) ([1.631]+[1.814])	Prec@1 39.062 (33.352)
Test: [0/79]	Time 0.257 (0.257)	Loss 3.2394 (3.2394) ([1.595]+[1.644])	Prec@1 42.188 (42.188)
 * Prec@1 38.180
current lr 1.00000e-01
Grad=  tensor(0.7370, device='cuda:0')
Epoch: [3][0/391]	Time 0.354 (0.354)	Data 0.231 (0.231)	Loss 3.3410 (3.3410) ([1.697]+[1.644])	Prec@1 36.719 (36.719)
Epoch: [3][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 3.2681 (3.2718) ([1.783]+[1.485])	Prec@1 32.031 (37.005)
Epoch: [3][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 3.0713 (3.1902) ([1.735]+[1.337])	Prec@1 32.812 (37.263)
Epoch: [3][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 2.8154 (3.0975) ([1.612]+[1.203])	Prec@1 37.500 (37.983)
Test: [0/79]	Time 0.257 (0.257)	Loss 2.6817 (2.6817) ([1.581]+[1.101])	Prec@1 40.625 (40.625)
 * Prec@1 39.630
current lr 1.00000e-01
Grad=  tensor(1.2909, device='cuda:0')
Epoch: [4][0/391]	Time 0.354 (0.354)	Data 0.235 (0.235)	Loss 2.8839 (2.8839) ([1.783]+[1.101])	Prec@1 39.062 (39.062)
Epoch: [4][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 2.4960 (2.6601) ([1.487]+[1.009])	Prec@1 41.406 (40.811)
Epoch: [4][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 2.4588 (2.6005) ([1.538]+[0.921])	Prec@1 41.406 (41.243)
Epoch: [4][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 2.3729 (2.5465) ([1.523]+[0.849])	Prec@1 39.844 (41.829)
Test: [0/79]	Time 0.211 (0.211)	Loss 2.1642 (2.1642) ([1.375]+[0.789])	Prec@1 50.781 (50.781)
 * Prec@1 45.650
current lr 1.00000e-01
Grad=  tensor(1.4172, device='cuda:0')
Epoch: [5][0/391]	Time 0.301 (0.301)	Data 0.182 (0.182)	Loss 2.2548 (2.2548) ([1.466]+[0.789])	Prec@1 46.875 (46.875)
Epoch: [5][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 2.4290 (2.2534) ([1.702]+[0.727])	Prec@1 45.312 (45.421)
Epoch: [5][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 2.0872 (2.2140) ([1.407]+[0.681])	Prec@1 52.344 (46.016)
Epoch: [5][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 2.1391 (2.1777) ([1.505]+[0.634])	Prec@1 42.969 (46.387)
Test: [0/79]	Time 0.261 (0.261)	Loss 2.1766 (2.1766) ([1.576]+[0.601])	Prec@1 46.094 (46.094)
 * Prec@1 42.610
current lr 1.00000e-01
Grad=  tensor(0.8680, device='cuda:0')
Epoch: [6][0/391]	Time 0.358 (0.358)	Data 0.228 (0.228)	Loss 1.7896 (1.7896) ([1.189]+[0.601])	Prec@1 53.125 (53.125)
Epoch: [6][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 2.0979 (1.9693) ([1.529]+[0.568])	Prec@1 44.531 (49.482)
Epoch: [6][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.9064 (1.9356) ([1.372]+[0.534])	Prec@1 48.438 (50.637)
Epoch: [6][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.7771 (1.9006) ([1.260]+[0.517])	Prec@1 56.250 (51.490)
Test: [0/79]	Time 0.253 (0.253)	Loss 2.0719 (2.0719) ([1.577]+[0.495])	Prec@1 42.188 (42.188)
 * Prec@1 38.400
current lr 1.00000e-01
Grad=  tensor(1.4941, device='cuda:0')
Epoch: [7][0/391]	Time 0.342 (0.342)	Data 0.216 (0.216)	Loss 1.8413 (1.8413) ([1.346]+[0.495])	Prec@1 53.906 (53.906)
Epoch: [7][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 1.5598 (1.7519) ([1.087]+[0.473])	Prec@1 62.500 (54.819)
Epoch: [7][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.6483 (1.7122) ([1.190]+[0.458])	Prec@1 53.125 (55.737)
Epoch: [7][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.6005 (1.6887) ([1.155]+[0.445])	Prec@1 60.156 (56.281)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.5808 (1.5808) ([1.145]+[0.435])	Prec@1 53.125 (53.125)
 * Prec@1 56.710
current lr 1.00000e-01
Grad=  tensor(1.3579, device='cuda:0')
Epoch: [8][0/391]	Time 0.348 (0.348)	Data 0.229 (0.229)	Loss 1.4521 (1.4521) ([1.017]+[0.435])	Prec@1 64.844 (64.844)
Epoch: [8][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.6766 (1.5667) ([1.252]+[0.424])	Prec@1 51.562 (59.661)
Epoch: [8][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 1.4648 (1.5408) ([1.051]+[0.413])	Prec@1 55.469 (60.366)
Epoch: [8][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.5620 (1.5150) ([1.156]+[0.406])	Prec@1 57.812 (61.070)
Test: [0/79]	Time 0.214 (0.214)	Loss 1.5074 (1.5074) ([1.110]+[0.398])	Prec@1 61.719 (61.719)
 * Prec@1 59.770
current lr 1.00000e-01
Grad=  tensor(1.2212, device='cuda:0')
Epoch: [9][0/391]	Time 0.306 (0.306)	Data 0.188 (0.188)	Loss 1.4592 (1.4592) ([1.061]+[0.398])	Prec@1 60.938 (60.938)
Epoch: [9][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 1.2907 (1.4545) ([0.896]+[0.394])	Prec@1 68.750 (62.252)
Epoch: [9][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.4662 (1.4208) ([1.083]+[0.383])	Prec@1 60.156 (63.293)
Epoch: [9][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.3794 (1.4144) ([1.002]+[0.377])	Prec@1 58.594 (63.458)
Test: [0/79]	Time 0.202 (0.202)	Loss 1.7236 (1.7236) ([1.355]+[0.369])	Prec@1 54.688 (54.688)
 * Prec@1 53.800
current lr 1.00000e-01
Grad=  tensor(2.1501, device='cuda:0')
Epoch: [10][0/391]	Time 0.293 (0.293)	Data 0.171 (0.171)	Loss 1.4655 (1.4655) ([1.097]+[0.369])	Prec@1 66.406 (66.406)
Epoch: [10][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 1.3464 (1.3291) ([0.981]+[0.366])	Prec@1 64.844 (65.803)
Epoch: [10][200/391]	Time 0.116 (0.112)	Data 0.000 (0.001)	Loss 1.2523 (1.3373) ([0.894]+[0.358])	Prec@1 70.312 (65.337)
Epoch: [10][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 1.2030 (1.3304) ([0.850]+[0.353])	Prec@1 74.219 (65.519)
Test: [0/79]	Time 0.257 (0.257)	Loss 1.4527 (1.4527) ([1.098]+[0.354])	Prec@1 66.406 (66.406)
 * Prec@1 60.860
current lr 1.00000e-01
Grad=  tensor(2.1912, device='cuda:0')
Epoch: [11][0/391]	Time 0.353 (0.353)	Data 0.229 (0.229)	Loss 1.4327 (1.4327) ([1.078]+[0.354])	Prec@1 64.844 (64.844)
Epoch: [11][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 1.2410 (1.2812) ([0.893]+[0.348])	Prec@1 68.750 (67.799)
Epoch: [11][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.1904 (1.2683) ([0.848]+[0.342])	Prec@1 71.875 (67.965)
Epoch: [11][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.2343 (1.2646) ([0.894]+[0.341])	Prec@1 69.531 (67.870)
Test: [0/79]	Time 0.255 (0.255)	Loss 1.3323 (1.3323) ([0.998]+[0.334])	Prec@1 62.500 (62.500)
 * Prec@1 63.180
current lr 1.00000e-01
Grad=  tensor(2.0009, device='cuda:0')
Epoch: [12][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 1.2570 (1.2570) ([0.923]+[0.334])	Prec@1 67.969 (67.969)
Epoch: [12][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 1.0678 (1.1954) ([0.735]+[0.333])	Prec@1 75.000 (69.910)
Epoch: [12][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.1371 (1.1913) ([0.809]+[0.328])	Prec@1 72.656 (69.889)
Epoch: [12][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 1.2067 (1.1916) ([0.882]+[0.325])	Prec@1 73.438 (69.900)
Test: [0/79]	Time 0.255 (0.255)	Loss 1.1213 (1.1213) ([0.801]+[0.321])	Prec@1 69.531 (69.531)
 * Prec@1 67.040
current lr 1.00000e-01
Grad=  tensor(1.7574, device='cuda:0')
Epoch: [13][0/391]	Time 0.339 (0.339)	Data 0.216 (0.216)	Loss 1.1712 (1.1712) ([0.851]+[0.321])	Prec@1 72.656 (72.656)
Epoch: [13][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 1.1589 (1.1416) ([0.842]+[0.317])	Prec@1 72.656 (71.171)
Epoch: [13][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 1.1817 (1.1427) ([0.864]+[0.318])	Prec@1 71.875 (71.222)
Epoch: [13][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 1.1418 (1.1401) ([0.827]+[0.315])	Prec@1 71.875 (71.299)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.7977 (1.7977) ([1.482]+[0.315])	Prec@1 53.125 (53.125)
 * Prec@1 57.740
current lr 1.00000e-01
Grad=  tensor(1.9174, device='cuda:0')
Epoch: [14][0/391]	Time 0.353 (0.353)	Data 0.230 (0.230)	Loss 1.1177 (1.1177) ([0.802]+[0.315])	Prec@1 68.750 (68.750)
Epoch: [14][100/391]	Time 0.116 (0.116)	Data 0.000 (0.002)	Loss 0.8930 (1.0726) ([0.580]+[0.313])	Prec@1 72.656 (73.345)
Epoch: [14][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9828 (1.0774) ([0.673]+[0.310])	Prec@1 78.125 (73.325)
Epoch: [14][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 1.0944 (1.0776) ([0.786]+[0.308])	Prec@1 76.562 (73.487)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.1056 (1.1056) ([0.798]+[0.307])	Prec@1 70.312 (70.312)
 * Prec@1 68.870
current lr 1.00000e-01
Grad=  tensor(2.1689, device='cuda:0')
Epoch: [15][0/391]	Time 0.361 (0.361)	Data 0.236 (0.236)	Loss 1.1495 (1.1495) ([0.842]+[0.307])	Prec@1 72.656 (72.656)
Epoch: [15][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.9792 (1.0378) ([0.675]+[0.304])	Prec@1 78.906 (75.178)
Epoch: [15][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.0002 (1.0502) ([0.697]+[0.304])	Prec@1 73.438 (74.425)
Epoch: [15][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.0370 (1.0481) ([0.736]+[0.301])	Prec@1 73.438 (74.240)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.0030 (1.0030) ([0.704]+[0.299])	Prec@1 75.000 (75.000)
 * Prec@1 71.570
current lr 1.00000e-01
Grad=  tensor(1.8024, device='cuda:0')
Epoch: [16][0/391]	Time 0.350 (0.350)	Data 0.226 (0.226)	Loss 0.9632 (0.9632) ([0.665]+[0.299])	Prec@1 80.469 (80.469)
Epoch: [16][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 1.0294 (0.9947) ([0.732]+[0.297])	Prec@1 71.875 (75.897)
Epoch: [16][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.1027 (1.0008) ([0.806]+[0.297])	Prec@1 75.000 (75.770)
Epoch: [16][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.1030 (1.0017) ([0.809]+[0.294])	Prec@1 68.750 (75.825)
Test: [0/79]	Time 0.262 (0.262)	Loss 1.1423 (1.1423) ([0.850]+[0.293])	Prec@1 70.312 (70.312)
 * Prec@1 66.170
current lr 1.00000e-01
Grad=  tensor(1.8385, device='cuda:0')
Epoch: [17][0/391]	Time 0.350 (0.350)	Data 0.226 (0.226)	Loss 0.9296 (0.9296) ([0.637]+[0.293])	Prec@1 75.781 (75.781)
Epoch: [17][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.9095 (0.9733) ([0.618]+[0.292])	Prec@1 79.688 (76.369)
Epoch: [17][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9535 (0.9832) ([0.663]+[0.291])	Prec@1 78.906 (75.956)
Epoch: [17][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.9354 (0.9795) ([0.644]+[0.291])	Prec@1 79.688 (76.088)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.9926 (0.9926) ([0.703]+[0.289])	Prec@1 72.656 (72.656)
 * Prec@1 72.810
current lr 1.00000e-01
Grad=  tensor(1.9353, device='cuda:0')
Epoch: [18][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.9996 (0.9996) ([0.710]+[0.289])	Prec@1 77.344 (77.344)
Epoch: [18][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.8752 (0.9586) ([0.588]+[0.287])	Prec@1 80.469 (76.887)
Epoch: [18][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.9071 (0.9615) ([0.618]+[0.289])	Prec@1 78.906 (76.753)
Epoch: [18][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8822 (0.9613) ([0.595]+[0.287])	Prec@1 78.906 (76.684)
Test: [0/79]	Time 0.253 (0.253)	Loss 1.0561 (1.0561) ([0.771]+[0.285])	Prec@1 75.781 (75.781)
 * Prec@1 71.490
current lr 1.00000e-01
Grad=  tensor(2.1495, device='cuda:0')
Epoch: [19][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.8725 (0.8725) ([0.587]+[0.285])	Prec@1 80.469 (80.469)
Epoch: [19][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.9392 (0.9317) ([0.654]+[0.285])	Prec@1 74.219 (77.460)
Epoch: [19][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.0212 (0.9329) ([0.737]+[0.284])	Prec@1 75.000 (77.542)
Epoch: [19][300/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 1.1514 (0.9399) ([0.868]+[0.284])	Prec@1 68.750 (77.339)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.9749 (0.9749) ([0.692]+[0.282])	Prec@1 75.000 (75.000)
 * Prec@1 72.600
current lr 1.00000e-01
Grad=  tensor(1.8306, device='cuda:0')
Epoch: [20][0/391]	Time 0.340 (0.340)	Data 0.216 (0.216)	Loss 0.9054 (0.9054) ([0.623]+[0.282])	Prec@1 76.562 (76.562)
Epoch: [20][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 1.0109 (0.9275) ([0.729]+[0.282])	Prec@1 72.656 (77.622)
Epoch: [20][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.7002 (0.9237) ([0.420]+[0.280])	Prec@1 85.938 (77.853)
Epoch: [20][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.9589 (0.9215) ([0.679]+[0.280])	Prec@1 73.438 (77.894)
Test: [0/79]	Time 0.256 (0.256)	Loss 1.1281 (1.1281) ([0.848]+[0.280])	Prec@1 71.875 (71.875)
 * Prec@1 66.160
current lr 1.00000e-01
Grad=  tensor(2.0274, device='cuda:0')
Epoch: [21][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 1.0832 (1.0832) ([0.803]+[0.280])	Prec@1 77.344 (77.344)
Epoch: [21][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.9432 (0.9055) ([0.664]+[0.279])	Prec@1 78.125 (78.543)
Epoch: [21][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9741 (0.9182) ([0.696]+[0.279])	Prec@1 72.656 (77.989)
Epoch: [21][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.1224 (0.9192) ([0.844]+[0.278])	Prec@1 69.531 (77.951)
Test: [0/79]	Time 0.265 (0.265)	Loss 1.0937 (1.0937) ([0.817]+[0.277])	Prec@1 70.312 (70.312)
 * Prec@1 71.390
current lr 1.00000e-01
Grad=  tensor(1.8724, device='cuda:0')
Epoch: [22][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.8912 (0.8912) ([0.614]+[0.277])	Prec@1 77.344 (77.344)
Epoch: [22][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8521 (0.8978) ([0.576]+[0.276])	Prec@1 81.250 (78.403)
Epoch: [22][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7867 (0.9009) ([0.511]+[0.276])	Prec@1 83.594 (78.401)
Epoch: [22][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 1.0097 (0.8982) ([0.736]+[0.274])	Prec@1 73.438 (78.473)
Test: [0/79]	Time 0.261 (0.261)	Loss 1.2473 (1.2473) ([0.973]+[0.274])	Prec@1 67.969 (67.969)
 * Prec@1 67.050
current lr 1.00000e-01
Grad=  tensor(1.6990, device='cuda:0')
Epoch: [23][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.9161 (0.9161) ([0.642]+[0.274])	Prec@1 77.344 (77.344)
Epoch: [23][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 1.0136 (0.8965) ([0.740]+[0.274])	Prec@1 73.438 (78.504)
Epoch: [23][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.8094 (0.9019) ([0.535]+[0.275])	Prec@1 81.250 (78.611)
Epoch: [23][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8878 (0.8939) ([0.614]+[0.273])	Prec@1 79.688 (78.771)
Test: [0/79]	Time 0.257 (0.257)	Loss 1.6032 (1.6032) ([1.330]+[0.274])	Prec@1 60.938 (60.938)
 * Prec@1 62.050
current lr 1.00000e-01
Grad=  tensor(2.0548, device='cuda:0')
Epoch: [24][0/391]	Time 0.339 (0.339)	Data 0.217 (0.217)	Loss 0.9096 (0.9096) ([0.636]+[0.274])	Prec@1 72.656 (72.656)
Epoch: [24][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.8971 (0.8891) ([0.624]+[0.273])	Prec@1 79.688 (78.999)
Epoch: [24][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6846 (0.8756) ([0.413]+[0.272])	Prec@1 87.500 (79.419)
Epoch: [24][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.8035 (0.8814) ([0.531]+[0.272])	Prec@1 82.812 (79.192)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.1026 (1.1026) ([0.831]+[0.272])	Prec@1 71.875 (71.875)
 * Prec@1 69.720
current lr 1.00000e-01
Grad=  tensor(1.6411, device='cuda:0')
Epoch: [25][0/391]	Time 0.349 (0.349)	Data 0.225 (0.225)	Loss 0.8048 (0.8048) ([0.533]+[0.272])	Prec@1 83.594 (83.594)
Epoch: [25][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8537 (0.8561) ([0.583]+[0.271])	Prec@1 80.469 (79.966)
Epoch: [25][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8149 (0.8708) ([0.544]+[0.271])	Prec@1 80.469 (79.575)
Epoch: [25][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8194 (0.8681) ([0.550]+[0.269])	Prec@1 80.469 (79.669)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.2386 (1.2386) ([0.970]+[0.269])	Prec@1 69.531 (69.531)
 * Prec@1 69.560
current lr 1.00000e-01
Grad=  tensor(2.2247, device='cuda:0')
Epoch: [26][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.9379 (0.9379) ([0.669]+[0.269])	Prec@1 75.781 (75.781)
Epoch: [26][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.7785 (0.8596) ([0.510]+[0.268])	Prec@1 82.812 (79.718)
Epoch: [26][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6814 (0.8564) ([0.414]+[0.268])	Prec@1 88.281 (79.777)
Epoch: [26][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.7814 (0.8649) ([0.514]+[0.268])	Prec@1 80.469 (79.402)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.8963 (0.8963) ([0.629]+[0.267])	Prec@1 76.562 (76.562)
 * Prec@1 75.700
current lr 1.00000e-01
Grad=  tensor(2.5449, device='cuda:0')
Epoch: [27][0/391]	Time 0.343 (0.343)	Data 0.221 (0.221)	Loss 0.9299 (0.9299) ([0.663]+[0.267])	Prec@1 80.469 (80.469)
Epoch: [27][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.8210 (0.8556) ([0.553]+[0.268])	Prec@1 84.375 (79.997)
Epoch: [27][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 1.0168 (0.8612) ([0.749]+[0.268])	Prec@1 76.562 (79.738)
Epoch: [27][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.7167 (0.8577) ([0.450]+[0.267])	Prec@1 85.156 (79.874)
Test: [0/79]	Time 0.264 (0.264)	Loss 1.0066 (1.0066) ([0.740]+[0.267])	Prec@1 78.125 (78.125)
 * Prec@1 75.210
current lr 1.00000e-01
Grad=  tensor(1.3492, device='cuda:0')
Epoch: [28][0/391]	Time 0.342 (0.342)	Data 0.220 (0.220)	Loss 0.7735 (0.7735) ([0.507]+[0.267])	Prec@1 85.156 (85.156)
Epoch: [28][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8331 (0.8483) ([0.567]+[0.266])	Prec@1 80.469 (79.695)
Epoch: [28][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7784 (0.8435) ([0.513]+[0.265])	Prec@1 82.812 (79.878)
Epoch: [28][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8923 (0.8396) ([0.628]+[0.264])	Prec@1 77.344 (80.248)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.9950 (0.9950) ([0.730]+[0.265])	Prec@1 73.438 (73.438)
 * Prec@1 75.250
current lr 1.00000e-01
Grad=  tensor(1.9400, device='cuda:0')
Epoch: [29][0/391]	Time 0.336 (0.336)	Data 0.212 (0.212)	Loss 0.8786 (0.8786) ([0.613]+[0.265])	Prec@1 78.125 (78.125)
Epoch: [29][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7834 (0.8476) ([0.518]+[0.266])	Prec@1 81.250 (80.113)
Epoch: [29][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7149 (0.8437) ([0.450]+[0.265])	Prec@1 83.594 (80.115)
Epoch: [29][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7890 (0.8454) ([0.524]+[0.265])	Prec@1 81.250 (80.017)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.0889 (1.0889) ([0.825]+[0.264])	Prec@1 75.000 (75.000)
 * Prec@1 73.210
current lr 1.00000e-01
Grad=  tensor(1.7663, device='cuda:0')
Epoch: [30][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.7692 (0.7692) ([0.505]+[0.264])	Prec@1 82.031 (82.031)
Epoch: [30][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6267 (0.8361) ([0.362]+[0.264])	Prec@1 85.938 (80.183)
Epoch: [30][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7589 (0.8427) ([0.494]+[0.265])	Prec@1 80.469 (79.831)
Epoch: [30][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.9410 (0.8426) ([0.677]+[0.264])	Prec@1 72.656 (79.794)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.9669 (0.9669) ([0.703]+[0.263])	Prec@1 75.781 (75.781)
 * Prec@1 76.520
current lr 1.00000e-01
Grad=  tensor(1.7555, device='cuda:0')
Epoch: [31][0/391]	Time 0.352 (0.352)	Data 0.229 (0.229)	Loss 0.7877 (0.7877) ([0.524]+[0.263])	Prec@1 82.812 (82.812)
Epoch: [31][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7764 (0.8375) ([0.513]+[0.264])	Prec@1 84.375 (80.306)
Epoch: [31][200/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.8909 (0.8364) ([0.627]+[0.264])	Prec@1 75.781 (80.201)
Epoch: [31][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7935 (0.8371) ([0.531]+[0.263])	Prec@1 81.250 (80.207)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.9277 (0.9277) ([0.666]+[0.262])	Prec@1 77.344 (77.344)
 * Prec@1 75.840
current lr 1.00000e-01
Grad=  tensor(2.0730, device='cuda:0')
Epoch: [32][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.8842 (0.8842) ([0.622]+[0.262])	Prec@1 78.125 (78.125)
Epoch: [32][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.9156 (0.8318) ([0.654]+[0.261])	Prec@1 75.781 (80.709)
Epoch: [32][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6989 (0.8240) ([0.438]+[0.261])	Prec@1 82.031 (80.861)
Epoch: [32][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8697 (0.8281) ([0.609]+[0.261])	Prec@1 75.781 (80.495)
Test: [0/79]	Time 0.263 (0.263)	Loss 1.2126 (1.2126) ([0.952]+[0.261])	Prec@1 67.188 (67.188)
 * Prec@1 72.310
current lr 1.00000e-01
Grad=  tensor(2.4259, device='cuda:0')
Epoch: [33][0/391]	Time 0.352 (0.352)	Data 0.221 (0.221)	Loss 0.9575 (0.9575) ([0.696]+[0.261])	Prec@1 78.125 (78.125)
Epoch: [33][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7670 (0.8283) ([0.505]+[0.262])	Prec@1 85.938 (80.546)
Epoch: [33][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.7277 (0.8206) ([0.466]+[0.261])	Prec@1 86.719 (80.986)
Epoch: [33][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8975 (0.8272) ([0.636]+[0.261])	Prec@1 78.906 (80.650)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.0320 (1.0320) ([0.772]+[0.260])	Prec@1 71.875 (71.875)
 * Prec@1 73.230
current lr 1.00000e-01
Grad=  tensor(1.7539, device='cuda:0')
Epoch: [34][0/391]	Time 0.347 (0.347)	Data 0.228 (0.228)	Loss 0.6946 (0.6946) ([0.434]+[0.260])	Prec@1 86.719 (86.719)
Epoch: [34][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.6836 (0.8111) ([0.423]+[0.260])	Prec@1 85.938 (81.281)
Epoch: [34][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.9080 (0.8109) ([0.648]+[0.260])	Prec@1 78.906 (81.347)
Epoch: [34][300/391]	Time 0.115 (0.110)	Data 0.000 (0.001)	Loss 0.7513 (0.8155) ([0.492]+[0.260])	Prec@1 82.812 (81.063)
Test: [0/79]	Time 0.276 (0.276)	Loss 1.0662 (1.0662) ([0.807]+[0.259])	Prec@1 72.656 (72.656)
 * Prec@1 73.970
current lr 1.00000e-01
Grad=  tensor(2.7019, device='cuda:0')
Epoch: [35][0/391]	Time 0.341 (0.341)	Data 0.221 (0.221)	Loss 0.9589 (0.9589) ([0.700]+[0.259])	Prec@1 78.906 (78.906)
Epoch: [35][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.7674 (0.8180) ([0.508]+[0.259])	Prec@1 82.812 (80.894)
Epoch: [35][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.8127 (0.8182) ([0.553]+[0.259])	Prec@1 77.344 (80.826)
Epoch: [35][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.9055 (0.8143) ([0.646]+[0.260])	Prec@1 75.781 (80.881)
Test: [0/79]	Time 0.210 (0.210)	Loss 1.0794 (1.0794) ([0.820]+[0.260])	Prec@1 71.875 (71.875)
 * Prec@1 71.630
current lr 1.00000e-01
Grad=  tensor(2.0840, device='cuda:0')
Epoch: [36][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.8522 (0.8522) ([0.593]+[0.260])	Prec@1 80.469 (80.469)
Epoch: [36][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7946 (0.8092) ([0.535]+[0.260])	Prec@1 79.688 (80.910)
Epoch: [36][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.8496 (0.8032) ([0.591]+[0.259])	Prec@1 78.906 (81.250)
Epoch: [36][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.9100 (0.8011) ([0.652]+[0.258])	Prec@1 81.250 (81.284)
Test: [0/79]	Time 0.250 (0.250)	Loss 1.5782 (1.5782) ([1.320]+[0.258])	Prec@1 60.938 (60.938)
 * Prec@1 66.320
current lr 1.00000e-01
Grad=  tensor(2.1747, device='cuda:0')
Epoch: [37][0/391]	Time 0.343 (0.343)	Data 0.221 (0.221)	Loss 0.9226 (0.9226) ([0.664]+[0.258])	Prec@1 77.344 (77.344)
Epoch: [37][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.8255 (0.8045) ([0.567]+[0.258])	Prec@1 82.031 (81.498)
Epoch: [37][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.7437 (0.8042) ([0.486]+[0.258])	Prec@1 81.250 (81.192)
Epoch: [37][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7975 (0.8053) ([0.540]+[0.257])	Prec@1 77.344 (81.271)
Test: [0/79]	Time 0.208 (0.208)	Loss 1.0435 (1.0435) ([0.786]+[0.257])	Prec@1 73.438 (73.438)
 * Prec@1 73.570
current lr 1.00000e-01
Grad=  tensor(2.3371, device='cuda:0')
Epoch: [38][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.8367 (0.8367) ([0.580]+[0.257])	Prec@1 80.469 (80.469)
Epoch: [38][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7999 (0.8085) ([0.543]+[0.257])	Prec@1 80.469 (81.320)
Epoch: [38][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.8115 (0.8129) ([0.554]+[0.258])	Prec@1 81.250 (81.188)
Epoch: [38][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8658 (0.8143) ([0.609]+[0.257])	Prec@1 81.250 (81.048)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.6206 (1.6206) ([1.364]+[0.257])	Prec@1 62.500 (62.500)
 * Prec@1 60.570
current lr 1.00000e-01
Grad=  tensor(1.8886, device='cuda:0')
Epoch: [39][0/391]	Time 0.348 (0.348)	Data 0.217 (0.217)	Loss 0.7040 (0.7040) ([0.447]+[0.257])	Prec@1 82.031 (82.031)
Epoch: [39][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8735 (0.8016) ([0.616]+[0.257])	Prec@1 76.562 (81.343)
Epoch: [39][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7571 (0.7968) ([0.501]+[0.256])	Prec@1 82.812 (81.448)
Epoch: [39][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8150 (0.8032) ([0.559]+[0.256])	Prec@1 80.469 (81.315)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.9082 (0.9082) ([0.652]+[0.256])	Prec@1 75.781 (75.781)
 * Prec@1 76.390
current lr 1.00000e-01
Grad=  tensor(2.4764, device='cuda:0')
Epoch: [40][0/391]	Time 0.342 (0.342)	Data 0.217 (0.217)	Loss 0.8712 (0.8712) ([0.615]+[0.256])	Prec@1 77.344 (77.344)
Epoch: [40][100/391]	Time 0.116 (0.117)	Data 0.000 (0.002)	Loss 0.9195 (0.7928) ([0.664]+[0.255])	Prec@1 78.906 (81.791)
Epoch: [40][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7721 (0.7923) ([0.517]+[0.255])	Prec@1 81.250 (81.576)
Epoch: [40][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8284 (0.7940) ([0.573]+[0.255])	Prec@1 80.469 (81.530)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.8308 (0.8308) ([0.576]+[0.255])	Prec@1 81.250 (81.250)
 * Prec@1 79.840
current lr 1.00000e-01
Grad=  tensor(1.7905, device='cuda:0')
Epoch: [41][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.6713 (0.6713) ([0.417]+[0.255])	Prec@1 85.938 (85.938)
Epoch: [41][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.8416 (0.7875) ([0.587]+[0.254])	Prec@1 80.469 (81.830)
Epoch: [41][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.8947 (0.7958) ([0.641]+[0.254])	Prec@1 79.688 (81.460)
Epoch: [41][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8239 (0.7966) ([0.570]+[0.254])	Prec@1 82.031 (81.419)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.8855 (0.8855) ([0.633]+[0.252])	Prec@1 82.812 (82.812)
 * Prec@1 78.040
current lr 1.00000e-01
Grad=  tensor(1.4974, device='cuda:0')
Epoch: [42][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.7274 (0.7274) ([0.475]+[0.252])	Prec@1 84.375 (84.375)
Epoch: [42][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.7536 (0.8080) ([0.501]+[0.253])	Prec@1 83.594 (81.211)
Epoch: [42][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6556 (0.8011) ([0.403]+[0.253])	Prec@1 85.938 (81.304)
Epoch: [42][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7335 (0.7983) ([0.481]+[0.253])	Prec@1 79.688 (81.393)
Test: [0/79]	Time 0.255 (0.255)	Loss 1.1886 (1.1886) ([0.936]+[0.252])	Prec@1 73.438 (73.438)
 * Prec@1 68.220
current lr 1.00000e-01
Grad=  tensor(1.3682, device='cuda:0')
Epoch: [43][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.6855 (0.6855) ([0.433]+[0.252])	Prec@1 86.719 (86.719)
Epoch: [43][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.7223 (0.7818) ([0.470]+[0.253])	Prec@1 87.500 (81.884)
Epoch: [43][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8231 (0.7820) ([0.571]+[0.252])	Prec@1 80.469 (81.942)
Epoch: [43][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7407 (0.7871) ([0.489]+[0.252])	Prec@1 82.812 (81.738)
Test: [0/79]	Time 0.261 (0.261)	Loss 1.0564 (1.0564) ([0.804]+[0.252])	Prec@1 71.875 (71.875)
 * Prec@1 70.770
current lr 1.00000e-01
Grad=  tensor(1.7060, device='cuda:0')
Epoch: [44][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.7513 (0.7513) ([0.499]+[0.252])	Prec@1 83.594 (83.594)
Epoch: [44][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.6766 (0.7755) ([0.425]+[0.251])	Prec@1 85.156 (82.101)
Epoch: [44][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8159 (0.7808) ([0.565]+[0.251])	Prec@1 82.031 (82.062)
Epoch: [44][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.8440 (0.7876) ([0.594]+[0.250])	Prec@1 78.125 (81.792)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.9775 (0.9775) ([0.726]+[0.251])	Prec@1 76.562 (76.562)
 * Prec@1 74.730
current lr 1.00000e-01
Grad=  tensor(1.7843, device='cuda:0')
Epoch: [45][0/391]	Time 0.332 (0.332)	Data 0.212 (0.212)	Loss 0.7554 (0.7554) ([0.504]+[0.251])	Prec@1 82.031 (82.031)
Epoch: [45][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.7289 (0.7823) ([0.478]+[0.251])	Prec@1 79.688 (81.536)
Epoch: [45][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8436 (0.7868) ([0.593]+[0.251])	Prec@1 82.812 (81.351)
Epoch: [45][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.7750 (0.7897) ([0.524]+[0.251])	Prec@1 82.031 (81.367)
Test: [0/79]	Time 0.262 (0.262)	Loss 1.1113 (1.1113) ([0.861]+[0.250])	Prec@1 72.656 (72.656)
 * Prec@1 70.250
current lr 1.00000e-01
Grad=  tensor(1.6898, device='cuda:0')
Epoch: [46][0/391]	Time 0.351 (0.351)	Data 0.226 (0.226)	Loss 0.7182 (0.7182) ([0.468]+[0.250])	Prec@1 86.719 (86.719)
Epoch: [46][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6127 (0.7735) ([0.363]+[0.250])	Prec@1 85.938 (82.124)
Epoch: [46][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9230 (0.7762) ([0.674]+[0.249])	Prec@1 75.000 (81.841)
Epoch: [46][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9964 (0.7821) ([0.747]+[0.250])	Prec@1 71.094 (81.676)
Test: [0/79]	Time 0.262 (0.262)	Loss 1.4950 (1.4950) ([1.246]+[0.249])	Prec@1 67.969 (67.969)
 * Prec@1 66.700
current lr 1.00000e-01
Grad=  tensor(2.1706, device='cuda:0')
Epoch: [47][0/391]	Time 0.340 (0.340)	Data 0.221 (0.221)	Loss 0.7922 (0.7922) ([0.544]+[0.249])	Prec@1 82.031 (82.031)
Epoch: [47][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.6159 (0.7823) ([0.367]+[0.248])	Prec@1 89.062 (81.938)
Epoch: [47][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.9074 (0.7814) ([0.659]+[0.248])	Prec@1 76.562 (81.779)
Epoch: [47][300/391]	Time 0.115 (0.110)	Data 0.000 (0.001)	Loss 1.0483 (0.7819) ([0.801]+[0.248])	Prec@1 72.656 (81.824)
Test: [0/79]	Time 0.268 (0.268)	Loss 1.3621 (1.3621) ([1.114]+[0.248])	Prec@1 70.312 (70.312)
 * Prec@1 71.240
current lr 1.00000e-01
Grad=  tensor(2.6729, device='cuda:0')
Epoch: [48][0/391]	Time 0.344 (0.344)	Data 0.224 (0.224)	Loss 0.9002 (0.9002) ([0.652]+[0.248])	Prec@1 78.125 (78.125)
Epoch: [48][100/391]	Time 0.115 (0.112)	Data 0.000 (0.002)	Loss 0.7761 (0.7667) ([0.527]+[0.249])	Prec@1 78.906 (82.031)
Epoch: [48][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.9020 (0.7693) ([0.653]+[0.249])	Prec@1 78.906 (82.257)
Epoch: [48][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.7164 (0.7786) ([0.468]+[0.249])	Prec@1 83.594 (81.800)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.9966 (0.9966) ([0.747]+[0.249])	Prec@1 77.344 (77.344)
 * Prec@1 74.320
current lr 1.00000e-01
Grad=  tensor(2.0415, device='cuda:0')
Epoch: [49][0/391]	Time 0.343 (0.343)	Data 0.220 (0.220)	Loss 0.8689 (0.8689) ([0.620]+[0.249])	Prec@1 78.906 (78.906)
Epoch: [49][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.8681 (0.7869) ([0.618]+[0.250])	Prec@1 82.031 (81.204)
Epoch: [49][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6457 (0.7800) ([0.397]+[0.249])	Prec@1 85.938 (81.681)
Epoch: [49][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6960 (0.7814) ([0.447]+[0.249])	Prec@1 86.719 (81.668)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.9664 (0.9664) ([0.719]+[0.247])	Prec@1 78.125 (78.125)
 * Prec@1 76.840
current lr 1.00000e-01
Grad=  tensor(2.3223, device='cuda:0')
Epoch: [50][0/391]	Time 0.340 (0.340)	Data 0.215 (0.215)	Loss 0.8804 (0.8804) ([0.633]+[0.247])	Prec@1 79.688 (79.688)
Epoch: [50][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 1.0153 (0.7855) ([0.768]+[0.247])	Prec@1 71.875 (81.428)
Epoch: [50][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6696 (0.7796) ([0.423]+[0.247])	Prec@1 85.938 (81.646)
Epoch: [50][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7767 (0.7820) ([0.529]+[0.247])	Prec@1 81.250 (81.616)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.0981 (1.0981) ([0.852]+[0.246])	Prec@1 75.781 (75.781)
 * Prec@1 73.060
current lr 1.00000e-01
Grad=  tensor(2.5472, device='cuda:0')
Epoch: [51][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.8291 (0.8291) ([0.583]+[0.246])	Prec@1 85.156 (85.156)
Epoch: [51][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.7557 (0.7816) ([0.509]+[0.247])	Prec@1 82.812 (81.791)
Epoch: [51][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.5794 (0.7764) ([0.333]+[0.247])	Prec@1 89.062 (81.907)
Epoch: [51][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8990 (0.7786) ([0.652]+[0.247])	Prec@1 78.125 (81.813)
Test: [0/79]	Time 0.260 (0.260)	Loss 1.0875 (1.0875) ([0.840]+[0.247])	Prec@1 71.094 (71.094)
 * Prec@1 68.930
current lr 1.00000e-01
Grad=  tensor(2.5353, device='cuda:0')
Epoch: [52][0/391]	Time 0.337 (0.337)	Data 0.215 (0.215)	Loss 0.9569 (0.9569) ([0.710]+[0.247])	Prec@1 79.688 (79.688)
Epoch: [52][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 1.0718 (0.7803) ([0.825]+[0.247])	Prec@1 72.656 (81.954)
Epoch: [52][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8986 (0.7805) ([0.652]+[0.247])	Prec@1 73.438 (81.903)
Epoch: [52][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7502 (0.7817) ([0.504]+[0.246])	Prec@1 83.594 (81.847)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.8551 (0.8551) ([0.610]+[0.245])	Prec@1 79.688 (79.688)
 * Prec@1 76.180
current lr 1.00000e-01
Grad=  tensor(1.5226, device='cuda:0')
Epoch: [53][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.7599 (0.7599) ([0.515]+[0.245])	Prec@1 85.156 (85.156)
Epoch: [53][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.8897 (0.7649) ([0.645]+[0.244])	Prec@1 78.906 (82.155)
Epoch: [53][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7375 (0.7690) ([0.492]+[0.246])	Prec@1 84.375 (82.082)
Epoch: [53][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9349 (0.7746) ([0.689]+[0.245])	Prec@1 76.562 (81.860)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.8994 (0.8994) ([0.654]+[0.245])	Prec@1 81.250 (81.250)
 * Prec@1 76.560
current lr 1.00000e-01
Grad=  tensor(2.0740, device='cuda:0')
Epoch: [54][0/391]	Time 0.349 (0.349)	Data 0.228 (0.228)	Loss 0.7181 (0.7181) ([0.473]+[0.245])	Prec@1 84.375 (84.375)
Epoch: [54][100/391]	Time 0.116 (0.116)	Data 0.000 (0.002)	Loss 0.7738 (0.7695) ([0.528]+[0.245])	Prec@1 82.031 (81.946)
Epoch: [54][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7805 (0.7705) ([0.535]+[0.246])	Prec@1 79.688 (82.004)
Epoch: [54][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7755 (0.7800) ([0.529]+[0.246])	Prec@1 83.594 (81.696)
Test: [0/79]	Time 0.273 (0.273)	Loss 0.9901 (0.9901) ([0.745]+[0.245])	Prec@1 73.438 (73.438)
 * Prec@1 73.430
current lr 1.00000e-01
Grad=  tensor(1.3558, device='cuda:0')
Epoch: [55][0/391]	Time 0.344 (0.344)	Data 0.221 (0.221)	Loss 0.6232 (0.6232) ([0.378]+[0.245])	Prec@1 85.156 (85.156)
Epoch: [55][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7090 (0.7676) ([0.464]+[0.245])	Prec@1 86.719 (82.016)
Epoch: [55][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.5911 (0.7757) ([0.346]+[0.245])	Prec@1 88.281 (81.779)
Epoch: [55][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.8204 (0.7737) ([0.575]+[0.245])	Prec@1 82.031 (81.800)
Test: [0/79]	Time 0.262 (0.262)	Loss 1.0387 (1.0387) ([0.794]+[0.244])	Prec@1 72.656 (72.656)
 * Prec@1 73.780
current lr 1.00000e-01
Grad=  tensor(2.2148, device='cuda:0')
Epoch: [56][0/391]	Time 0.348 (0.348)	Data 0.225 (0.225)	Loss 0.8502 (0.8502) ([0.606]+[0.244])	Prec@1 79.688 (79.688)
Epoch: [56][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.6586 (0.7563) ([0.415]+[0.244])	Prec@1 82.812 (83.029)
Epoch: [56][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6172 (0.7688) ([0.373]+[0.244])	Prec@1 86.719 (82.132)
Epoch: [56][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6679 (0.7723) ([0.423]+[0.244])	Prec@1 89.844 (82.052)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.9609 (0.9609) ([0.717]+[0.244])	Prec@1 74.219 (74.219)
 * Prec@1 75.620
current lr 1.00000e-01
Grad=  tensor(2.0670, device='cuda:0')
Epoch: [57][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.7588 (0.7588) ([0.515]+[0.244])	Prec@1 78.906 (78.906)
Epoch: [57][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7233 (0.7411) ([0.480]+[0.243])	Prec@1 84.375 (82.681)
Epoch: [57][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.6676 (0.7551) ([0.424]+[0.243])	Prec@1 84.375 (82.140)
Epoch: [57][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7844 (0.7651) ([0.541]+[0.243])	Prec@1 82.812 (81.909)
Test: [0/79]	Time 0.270 (0.270)	Loss 0.9175 (0.9175) ([0.675]+[0.243])	Prec@1 78.125 (78.125)
 * Prec@1 76.580
current lr 1.00000e-01
Grad=  tensor(1.8343, device='cuda:0')
Epoch: [58][0/391]	Time 0.355 (0.355)	Data 0.231 (0.231)	Loss 0.7653 (0.7653) ([0.523]+[0.243])	Prec@1 81.250 (81.250)
Epoch: [58][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.8450 (0.7590) ([0.602]+[0.243])	Prec@1 78.125 (82.666)
Epoch: [58][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7849 (0.7685) ([0.542]+[0.243])	Prec@1 80.469 (82.136)
Epoch: [58][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.7323 (0.7720) ([0.489]+[0.243])	Prec@1 82.031 (82.029)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.0447 (1.0447) ([0.802]+[0.242])	Prec@1 72.656 (72.656)
 * Prec@1 72.300
current lr 1.00000e-01
Grad=  tensor(2.1206, device='cuda:0')
Epoch: [59][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.8133 (0.8133) ([0.571]+[0.242])	Prec@1 78.125 (78.125)
Epoch: [59][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7727 (0.7523) ([0.530]+[0.243])	Prec@1 80.469 (82.178)
Epoch: [59][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.7988 (0.7586) ([0.556]+[0.243])	Prec@1 83.594 (82.008)
Epoch: [59][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7897 (0.7615) ([0.548]+[0.242])	Prec@1 81.250 (82.044)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.0167 (1.0167) ([0.774]+[0.242])	Prec@1 73.438 (73.438)
 * Prec@1 75.770
current lr 1.00000e-01
Grad=  tensor(2.7632, device='cuda:0')
Epoch: [60][0/391]	Time 0.342 (0.342)	Data 0.218 (0.218)	Loss 0.8917 (0.8917) ([0.649]+[0.242])	Prec@1 78.125 (78.125)
Epoch: [60][100/391]	Time 0.116 (0.117)	Data 0.000 (0.002)	Loss 0.8755 (0.7706) ([0.633]+[0.243])	Prec@1 78.906 (82.101)
Epoch: [60][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.9372 (0.7682) ([0.695]+[0.242])	Prec@1 74.219 (81.930)
Epoch: [60][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9390 (0.7702) ([0.697]+[0.242])	Prec@1 78.125 (81.878)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.6988 (0.6988) ([0.457]+[0.242])	Prec@1 80.469 (80.469)
 * Prec@1 79.930
current lr 1.00000e-01
Grad=  tensor(1.9368, device='cuda:0')
Epoch: [61][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.7751 (0.7751) ([0.533]+[0.242])	Prec@1 82.031 (82.031)
Epoch: [61][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.9157 (0.7486) ([0.674]+[0.242])	Prec@1 78.125 (82.843)
Epoch: [61][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.9791 (0.7593) ([0.737]+[0.242])	Prec@1 75.781 (82.346)
Epoch: [61][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7734 (0.7595) ([0.532]+[0.241])	Prec@1 75.781 (82.301)
Test: [0/79]	Time 0.268 (0.268)	Loss 0.9540 (0.9540) ([0.713]+[0.241])	Prec@1 75.000 (75.000)
 * Prec@1 77.620
current lr 1.00000e-01
Grad=  tensor(1.8163, device='cuda:0')
Epoch: [62][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.7115 (0.7115) ([0.471]+[0.241])	Prec@1 82.031 (82.031)
Epoch: [62][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.5677 (0.7459) ([0.327]+[0.241])	Prec@1 89.062 (82.410)
Epoch: [62][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7196 (0.7515) ([0.479]+[0.241])	Prec@1 83.594 (82.327)
Epoch: [62][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7187 (0.7567) ([0.478]+[0.241])	Prec@1 81.250 (82.236)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.8967 (0.8967) ([0.656]+[0.241])	Prec@1 78.906 (78.906)
 * Prec@1 77.410
current lr 1.00000e-01
Grad=  tensor(1.4970, device='cuda:0')
Epoch: [63][0/391]	Time 0.336 (0.336)	Data 0.211 (0.211)	Loss 0.6456 (0.6456) ([0.405]+[0.241])	Prec@1 85.938 (85.938)
Epoch: [63][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.6536 (0.7682) ([0.413]+[0.240])	Prec@1 88.281 (82.124)
Epoch: [63][200/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.7099 (0.7689) ([0.469]+[0.241])	Prec@1 85.938 (81.996)
Epoch: [63][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7915 (0.7655) ([0.551]+[0.240])	Prec@1 82.031 (82.179)
Test: [0/79]	Time 0.262 (0.262)	Loss 1.0878 (1.0878) ([0.848]+[0.240])	Prec@1 75.781 (75.781)
 * Prec@1 73.440
current lr 1.00000e-01
Grad=  tensor(1.8555, device='cuda:0')
Epoch: [64][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.7249 (0.7249) ([0.485]+[0.240])	Prec@1 83.594 (83.594)
Epoch: [64][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.6002 (0.7528) ([0.360]+[0.240])	Prec@1 84.375 (82.348)
Epoch: [64][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8100 (0.7564) ([0.570]+[0.240])	Prec@1 78.906 (82.276)
Epoch: [64][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7655 (0.7599) ([0.525]+[0.240])	Prec@1 80.469 (82.216)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.9699 (0.9699) ([0.730]+[0.240])	Prec@1 75.781 (75.781)
 * Prec@1 74.190
current lr 1.00000e-01
Grad=  tensor(1.5608, device='cuda:0')
Epoch: [65][0/391]	Time 0.347 (0.347)	Data 0.221 (0.221)	Loss 0.7133 (0.7133) ([0.474]+[0.240])	Prec@1 86.719 (86.719)
Epoch: [65][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8107 (0.7519) ([0.571]+[0.240])	Prec@1 80.469 (82.426)
Epoch: [65][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.9061 (0.7430) ([0.667]+[0.239])	Prec@1 78.906 (82.770)
Epoch: [65][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.5821 (0.7496) ([0.343]+[0.239])	Prec@1 85.156 (82.480)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.8971 (0.8971) ([0.658]+[0.239])	Prec@1 77.344 (77.344)
 * Prec@1 78.190
current lr 1.00000e-01
Grad=  tensor(1.6787, device='cuda:0')
Epoch: [66][0/391]	Time 0.348 (0.348)	Data 0.225 (0.225)	Loss 0.7388 (0.7388) ([0.500]+[0.239])	Prec@1 86.719 (86.719)
Epoch: [66][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7230 (0.7497) ([0.484]+[0.239])	Prec@1 84.375 (82.480)
Epoch: [66][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.9611 (0.7550) ([0.723]+[0.238])	Prec@1 76.562 (82.303)
Epoch: [66][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7908 (0.7576) ([0.552]+[0.239])	Prec@1 79.688 (82.226)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.7504 (0.7504) ([0.512]+[0.238])	Prec@1 78.906 (78.906)
 * Prec@1 79.760
current lr 1.00000e-01
Grad=  tensor(2.1759, device='cuda:0')
Epoch: [67][0/391]	Time 0.342 (0.342)	Data 0.218 (0.218)	Loss 0.7312 (0.7312) ([0.493]+[0.238])	Prec@1 82.031 (82.031)
Epoch: [67][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.6745 (0.7326) ([0.437]+[0.237])	Prec@1 83.594 (83.192)
Epoch: [67][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7019 (0.7430) ([0.464]+[0.238])	Prec@1 82.031 (82.781)
Epoch: [67][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7062 (0.7536) ([0.468]+[0.238])	Prec@1 86.719 (82.449)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.8072 (0.8072) ([0.570]+[0.237])	Prec@1 80.469 (80.469)
 * Prec@1 78.760
current lr 1.00000e-01
Grad=  tensor(1.7050, device='cuda:0')
Epoch: [68][0/391]	Time 0.351 (0.351)	Data 0.223 (0.223)	Loss 0.7087 (0.7087) ([0.472]+[0.237])	Prec@1 85.156 (85.156)
Epoch: [68][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.6710 (0.7567) ([0.433]+[0.238])	Prec@1 84.375 (82.372)
Epoch: [68][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7275 (0.7533) ([0.490]+[0.238])	Prec@1 84.375 (82.261)
Epoch: [68][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7647 (0.7559) ([0.527]+[0.237])	Prec@1 81.250 (82.125)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.9198 (0.9198) ([0.682]+[0.238])	Prec@1 72.656 (72.656)
 * Prec@1 77.150
current lr 1.00000e-01
Grad=  tensor(0.9949, device='cuda:0')
Epoch: [69][0/391]	Time 0.339 (0.339)	Data 0.216 (0.216)	Loss 0.6087 (0.6087) ([0.371]+[0.238])	Prec@1 87.500 (87.500)
Epoch: [69][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.9907 (0.7373) ([0.753]+[0.238])	Prec@1 75.000 (82.983)
Epoch: [69][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.7869 (0.7452) ([0.549]+[0.238])	Prec@1 78.906 (82.708)
Epoch: [69][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6292 (0.7501) ([0.392]+[0.237])	Prec@1 85.938 (82.527)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.0166 (1.0166) ([0.779]+[0.237])	Prec@1 76.562 (76.562)
 * Prec@1 76.140
current lr 1.00000e-01
Grad=  tensor(2.0507, device='cuda:0')
Epoch: [70][0/391]	Time 0.351 (0.351)	Data 0.228 (0.228)	Loss 0.7236 (0.7236) ([0.486]+[0.237])	Prec@1 83.594 (83.594)
Epoch: [70][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.7011 (0.7398) ([0.464]+[0.237])	Prec@1 82.812 (82.472)
Epoch: [70][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7133 (0.7437) ([0.476]+[0.237])	Prec@1 84.375 (82.591)
Epoch: [70][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7606 (0.7506) ([0.524]+[0.236])	Prec@1 82.812 (82.441)
Test: [0/79]	Time 0.260 (0.260)	Loss 1.5031 (1.5031) ([1.267]+[0.236])	Prec@1 64.062 (64.062)
 * Prec@1 62.600
current lr 1.00000e-01
Grad=  tensor(1.5095, device='cuda:0')
Epoch: [71][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.6165 (0.6165) ([0.381]+[0.236])	Prec@1 85.156 (85.156)
Epoch: [71][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.6827 (0.7383) ([0.447]+[0.236])	Prec@1 85.938 (82.952)
Epoch: [71][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7304 (0.7449) ([0.495]+[0.236])	Prec@1 82.031 (82.673)
Epoch: [71][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9654 (0.7460) ([0.730]+[0.236])	Prec@1 77.344 (82.628)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.8819 (0.8819) ([0.647]+[0.235])	Prec@1 76.562 (76.562)
 * Prec@1 78.250
current lr 1.00000e-01
Grad=  tensor(1.5465, device='cuda:0')
Epoch: [72][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.5969 (0.5969) ([0.362]+[0.235])	Prec@1 89.062 (89.062)
Epoch: [72][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6926 (0.7590) ([0.457]+[0.236])	Prec@1 82.812 (82.039)
Epoch: [72][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7859 (0.7496) ([0.551]+[0.235])	Prec@1 82.812 (82.494)
Epoch: [72][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.8312 (0.7454) ([0.597]+[0.234])	Prec@1 84.375 (82.602)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.0267 (1.0267) ([0.792]+[0.235])	Prec@1 72.656 (72.656)
 * Prec@1 74.920
current lr 1.00000e-01
Grad=  tensor(2.0410, device='cuda:0')
Epoch: [73][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.7154 (0.7154) ([0.480]+[0.235])	Prec@1 84.375 (84.375)
Epoch: [73][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7295 (0.7431) ([0.494]+[0.235])	Prec@1 82.812 (82.758)
Epoch: [73][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.6912 (0.7432) ([0.456]+[0.235])	Prec@1 87.500 (82.692)
Epoch: [73][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7888 (0.7425) ([0.554]+[0.235])	Prec@1 82.031 (82.742)
Test: [0/79]	Time 0.260 (0.260)	Loss 1.0092 (1.0092) ([0.775]+[0.234])	Prec@1 77.344 (77.344)
 * Prec@1 73.030
current lr 1.00000e-01
Grad=  tensor(2.1077, device='cuda:0')
Epoch: [74][0/391]	Time 0.346 (0.346)	Data 0.218 (0.218)	Loss 0.7176 (0.7176) ([0.483]+[0.234])	Prec@1 82.812 (82.812)
Epoch: [74][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7674 (0.7457) ([0.533]+[0.235])	Prec@1 80.469 (82.101)
Epoch: [74][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7970 (0.7428) ([0.563]+[0.234])	Prec@1 85.156 (82.408)
Epoch: [74][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6713 (0.7456) ([0.437]+[0.235])	Prec@1 85.156 (82.397)
Test: [0/79]	Time 0.297 (0.297)	Loss 1.0658 (1.0658) ([0.831]+[0.235])	Prec@1 73.438 (73.438)
 * Prec@1 74.540
current lr 1.00000e-01
Grad=  tensor(2.1734, device='cuda:0')
Epoch: [75][0/391]	Time 0.530 (0.530)	Data 0.366 (0.366)	Loss 0.7519 (0.7519) ([0.517]+[0.235])	Prec@1 82.812 (82.812)
Epoch: [75][100/391]	Time 0.114 (0.114)	Data 0.000 (0.004)	Loss 0.7405 (0.7189) ([0.506]+[0.235])	Prec@1 83.594 (83.230)
Epoch: [75][200/391]	Time 0.114 (0.114)	Data 0.000 (0.002)	Loss 0.6943 (0.7300) ([0.460]+[0.235])	Prec@1 82.812 (83.081)
Epoch: [75][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.9044 (0.7433) ([0.671]+[0.234])	Prec@1 75.000 (82.574)
Test: [0/79]	Time 0.261 (0.261)	Loss 1.0269 (1.0269) ([0.793]+[0.233])	Prec@1 71.875 (71.875)
 * Prec@1 76.040
current lr 1.00000e-01
Grad=  tensor(1.9121, device='cuda:0')
Epoch: [76][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.7573 (0.7573) ([0.524]+[0.233])	Prec@1 80.469 (80.469)
Epoch: [76][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.7056 (0.7304) ([0.473]+[0.233])	Prec@1 82.031 (82.774)
Epoch: [76][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6694 (0.7319) ([0.436]+[0.233])	Prec@1 86.719 (82.921)
Epoch: [76][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.7491 (0.7444) ([0.515]+[0.234])	Prec@1 79.688 (82.511)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.2019 (1.2019) ([0.969]+[0.233])	Prec@1 66.406 (66.406)
 * Prec@1 69.750
current lr 1.00000e-01
Grad=  tensor(1.3680, device='cuda:0')
Epoch: [77][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.6374 (0.6374) ([0.404]+[0.233])	Prec@1 85.938 (85.938)
Epoch: [77][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6773 (0.7339) ([0.444]+[0.233])	Prec@1 84.375 (83.431)
Epoch: [77][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8424 (0.7559) ([0.608]+[0.234])	Prec@1 78.125 (82.284)
Epoch: [77][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7877 (0.7522) ([0.555]+[0.233])	Prec@1 79.688 (82.358)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.9954 (0.9954) ([0.763]+[0.233])	Prec@1 72.656 (72.656)
 * Prec@1 73.880
current lr 1.00000e-01
Grad=  tensor(1.8535, device='cuda:0')
Epoch: [78][0/391]	Time 0.344 (0.344)	Data 0.224 (0.224)	Loss 0.6748 (0.6748) ([0.442]+[0.233])	Prec@1 85.156 (85.156)
Epoch: [78][100/391]	Time 0.114 (0.113)	Data 0.000 (0.002)	Loss 0.7367 (0.7357) ([0.503]+[0.233])	Prec@1 80.469 (82.774)
Epoch: [78][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.7560 (0.7422) ([0.522]+[0.234])	Prec@1 82.812 (82.560)
Epoch: [78][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.7030 (0.7425) ([0.470]+[0.233])	Prec@1 82.031 (82.519)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.2687 (1.2687) ([1.036]+[0.233])	Prec@1 69.531 (69.531)
 * Prec@1 72.250
current lr 1.00000e-01
Grad=  tensor(2.0983, device='cuda:0')
Epoch: [79][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.7219 (0.7219) ([0.489]+[0.233])	Prec@1 82.812 (82.812)
Epoch: [79][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.7498 (0.7456) ([0.516]+[0.234])	Prec@1 79.688 (82.093)
Epoch: [79][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.6561 (0.7539) ([0.422]+[0.234])	Prec@1 85.938 (82.070)
Epoch: [79][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7897 (0.7523) ([0.556]+[0.234])	Prec@1 82.031 (82.169)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.9200 (0.9200) ([0.686]+[0.234])	Prec@1 76.562 (76.562)
 * Prec@1 74.820
current lr 1.00000e-01
Grad=  tensor(1.6512, device='cuda:0')
Epoch: [80][0/391]	Time 0.349 (0.349)	Data 0.223 (0.223)	Loss 0.6970 (0.6970) ([0.463]+[0.234])	Prec@1 82.812 (82.812)
Epoch: [80][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7994 (0.7394) ([0.566]+[0.234])	Prec@1 79.688 (82.828)
Epoch: [80][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7483 (0.7398) ([0.515]+[0.233])	Prec@1 82.031 (82.793)
Epoch: [80][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.6058 (0.7379) ([0.373]+[0.232])	Prec@1 85.938 (82.812)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.3074 (1.3074) ([1.076]+[0.231])	Prec@1 67.188 (67.188)
 * Prec@1 72.520
current lr 1.00000e-01
Grad=  tensor(1.7398, device='cuda:0')
Epoch: [81][0/391]	Time 0.342 (0.342)	Data 0.219 (0.219)	Loss 0.5941 (0.5941) ([0.363]+[0.231])	Prec@1 88.281 (88.281)
Epoch: [81][100/391]	Time 0.114 (0.114)	Data 0.000 (0.002)	Loss 0.8327 (0.7206) ([0.601]+[0.231])	Prec@1 79.688 (83.253)
Epoch: [81][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6808 (0.7335) ([0.450]+[0.231])	Prec@1 83.594 (82.851)
Epoch: [81][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.6607 (0.7322) ([0.430]+[0.231])	Prec@1 87.500 (82.872)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.8872 (0.8872) ([0.657]+[0.231])	Prec@1 78.125 (78.125)
 * Prec@1 74.970
current lr 1.00000e-01
Grad=  tensor(1.8033, device='cuda:0')
Epoch: [82][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.6987 (0.6987) ([0.468]+[0.231])	Prec@1 85.156 (85.156)
Epoch: [82][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7143 (0.7356) ([0.483]+[0.231])	Prec@1 79.688 (82.812)
Epoch: [82][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7586 (0.7309) ([0.528]+[0.231])	Prec@1 82.812 (82.816)
Epoch: [82][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7636 (0.7352) ([0.533]+[0.231])	Prec@1 83.594 (82.602)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.9682 (0.9682) ([0.737]+[0.231])	Prec@1 77.344 (77.344)
 * Prec@1 73.980
current lr 1.00000e-01
Grad=  tensor(1.5277, device='cuda:0')
Epoch: [83][0/391]	Time 0.342 (0.342)	Data 0.220 (0.220)	Loss 0.7128 (0.7128) ([0.481]+[0.231])	Prec@1 83.594 (83.594)
Epoch: [83][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.6576 (0.7176) ([0.427]+[0.231])	Prec@1 84.375 (83.455)
Epoch: [83][200/391]	Time 0.113 (0.111)	Data 0.000 (0.001)	Loss 0.8220 (0.7313) ([0.591]+[0.231])	Prec@1 78.906 (83.019)
Epoch: [83][300/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.8126 (0.7352) ([0.582]+[0.230])	Prec@1 80.469 (82.825)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.4068 (1.4068) ([1.176]+[0.231])	Prec@1 64.062 (64.062)
 * Prec@1 66.790
current lr 1.00000e-01
Grad=  tensor(2.5071, device='cuda:0')
Epoch: [84][0/391]	Time 0.310 (0.310)	Data 0.186 (0.186)	Loss 0.8123 (0.8123) ([0.582]+[0.231])	Prec@1 80.469 (80.469)
Epoch: [84][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.7285 (0.7497) ([0.498]+[0.231])	Prec@1 81.250 (82.426)
Epoch: [84][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7183 (0.7427) ([0.488]+[0.230])	Prec@1 83.594 (82.451)
Epoch: [84][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.5740 (0.7411) ([0.344]+[0.230])	Prec@1 86.719 (82.467)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.8260 (0.8260) ([0.596]+[0.230])	Prec@1 81.250 (81.250)
 * Prec@1 75.420
current lr 1.00000e-01
Grad=  tensor(1.8482, device='cuda:0')
Epoch: [85][0/391]	Time 0.350 (0.350)	Data 0.226 (0.226)	Loss 0.6724 (0.6724) ([0.443]+[0.230])	Prec@1 85.156 (85.156)
Epoch: [85][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8233 (0.7508) ([0.594]+[0.229])	Prec@1 78.125 (82.178)
Epoch: [85][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7241 (0.7462) ([0.494]+[0.230])	Prec@1 81.250 (82.319)
Epoch: [85][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7893 (0.7421) ([0.560]+[0.229])	Prec@1 83.594 (82.493)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.7899 (0.7899) ([0.561]+[0.229])	Prec@1 81.250 (81.250)
 * Prec@1 79.250
current lr 1.00000e-01
Grad=  tensor(1.6961, device='cuda:0')
Epoch: [86][0/391]	Time 0.346 (0.346)	Data 0.215 (0.215)	Loss 0.7618 (0.7618) ([0.532]+[0.229])	Prec@1 82.812 (82.812)
Epoch: [86][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.8179 (0.7472) ([0.589]+[0.229])	Prec@1 76.562 (82.379)
Epoch: [86][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7229 (0.7416) ([0.494]+[0.229])	Prec@1 82.812 (82.478)
Epoch: [86][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8221 (0.7372) ([0.593]+[0.229])	Prec@1 77.344 (82.654)
Test: [0/79]	Time 0.253 (0.253)	Loss 1.7550 (1.7550) ([1.526]+[0.229])	Prec@1 58.594 (58.594)
 * Prec@1 62.510
current lr 1.00000e-01
Grad=  tensor(1.9364, device='cuda:0')
Epoch: [87][0/391]	Time 0.336 (0.336)	Data 0.215 (0.215)	Loss 0.7137 (0.7137) ([0.485]+[0.229])	Prec@1 85.938 (85.938)
Epoch: [87][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.8220 (0.7128) ([0.594]+[0.229])	Prec@1 76.562 (83.393)
Epoch: [87][200/391]	Time 0.115 (0.110)	Data 0.000 (0.001)	Loss 0.7193 (0.7308) ([0.490]+[0.229])	Prec@1 82.031 (82.735)
Epoch: [87][300/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.8034 (0.7358) ([0.574]+[0.230])	Prec@1 78.906 (82.571)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.7245 (0.7245) ([0.496]+[0.229])	Prec@1 85.156 (85.156)
 * Prec@1 81.320
current lr 1.00000e-01
Grad=  tensor(2.0771, device='cuda:0')
Epoch: [88][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.7860 (0.7860) ([0.557]+[0.229])	Prec@1 77.344 (77.344)
Epoch: [88][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.8039 (0.7176) ([0.575]+[0.229])	Prec@1 80.469 (83.021)
Epoch: [88][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7957 (0.7180) ([0.568]+[0.228])	Prec@1 78.906 (83.116)
Epoch: [88][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8291 (0.7277) ([0.601]+[0.228])	Prec@1 75.781 (82.849)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.2462 (1.2462) ([1.017]+[0.229])	Prec@1 73.438 (73.438)
 * Prec@1 64.880
current lr 1.00000e-01
Grad=  tensor(2.1353, device='cuda:0')
Epoch: [89][0/391]	Time 0.353 (0.353)	Data 0.228 (0.228)	Loss 0.8041 (0.8041) ([0.575]+[0.229])	Prec@1 79.688 (79.688)
Epoch: [89][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.7932 (0.7309) ([0.564]+[0.229])	Prec@1 80.469 (82.642)
Epoch: [89][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.7586 (0.7326) ([0.530]+[0.228])	Prec@1 80.469 (82.673)
Epoch: [89][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8031 (0.7319) ([0.575]+[0.228])	Prec@1 81.250 (82.792)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.9158 (0.9158) ([0.688]+[0.228])	Prec@1 74.219 (74.219)
 * Prec@1 76.890
current lr 1.00000e-01
Grad=  tensor(2.4413, device='cuda:0')
Epoch: [90][0/391]	Time 0.340 (0.340)	Data 0.219 (0.219)	Loss 0.7328 (0.7328) ([0.505]+[0.228])	Prec@1 82.031 (82.031)
Epoch: [90][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.6140 (0.7044) ([0.387]+[0.227])	Prec@1 88.281 (83.400)
Epoch: [90][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7903 (0.7278) ([0.562]+[0.228])	Prec@1 79.688 (82.661)
Epoch: [90][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7163 (0.7304) ([0.488]+[0.228])	Prec@1 86.719 (82.636)
Test: [0/79]	Time 0.213 (0.213)	Loss 1.1076 (1.1076) ([0.880]+[0.228])	Prec@1 71.094 (71.094)
 * Prec@1 71.360
current lr 1.00000e-01
Grad=  tensor(1.7890, device='cuda:0')
Epoch: [91][0/391]	Time 0.354 (0.354)	Data 0.229 (0.229)	Loss 0.7272 (0.7272) ([0.500]+[0.228])	Prec@1 85.156 (85.156)
Epoch: [91][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.7892 (0.7265) ([0.561]+[0.228])	Prec@1 78.906 (82.642)
Epoch: [91][200/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.7101 (0.7334) ([0.483]+[0.228])	Prec@1 86.719 (82.676)
Epoch: [91][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8016 (0.7296) ([0.575]+[0.227])	Prec@1 78.125 (82.776)
Test: [0/79]	Time 0.256 (0.256)	Loss 1.0804 (1.0804) ([0.854]+[0.227])	Prec@1 72.656 (72.656)
 * Prec@1 72.670
current lr 1.00000e-01
Grad=  tensor(2.4458, device='cuda:0')
Epoch: [92][0/391]	Time 0.350 (0.350)	Data 0.222 (0.222)	Loss 0.7782 (0.7782) ([0.552]+[0.227])	Prec@1 82.031 (82.031)
Epoch: [92][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.5796 (0.7394) ([0.352]+[0.228])	Prec@1 85.156 (82.163)
Epoch: [92][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7447 (0.7417) ([0.517]+[0.228])	Prec@1 80.469 (82.237)
Epoch: [92][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6244 (0.7382) ([0.396]+[0.228])	Prec@1 86.719 (82.460)
Test: [0/79]	Time 0.253 (0.253)	Loss 1.1336 (1.1336) ([0.906]+[0.227])	Prec@1 74.219 (74.219)
 * Prec@1 76.520
current lr 1.00000e-01
Grad=  tensor(2.5397, device='cuda:0')
Epoch: [93][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.7449 (0.7449) ([0.517]+[0.227])	Prec@1 82.812 (82.812)
Epoch: [93][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.7558 (0.7390) ([0.528]+[0.227])	Prec@1 82.812 (82.101)
Epoch: [93][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.8217 (0.7369) ([0.594]+[0.228])	Prec@1 82.812 (82.400)
Epoch: [93][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8377 (0.7370) ([0.610]+[0.228])	Prec@1 78.906 (82.472)
Test: [0/79]	Time 0.263 (0.263)	Loss 1.0410 (1.0410) ([0.814]+[0.227])	Prec@1 74.219 (74.219)
 * Prec@1 75.980
current lr 1.00000e-01
Grad=  tensor(3.4519, device='cuda:0')
Epoch: [94][0/391]	Time 0.350 (0.350)	Data 0.226 (0.226)	Loss 0.9044 (0.9044) ([0.678]+[0.227])	Prec@1 75.781 (75.781)
Epoch: [94][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6091 (0.7221) ([0.382]+[0.227])	Prec@1 89.062 (83.168)
Epoch: [94][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.7269 (0.7257) ([0.500]+[0.226])	Prec@1 82.031 (83.069)
Epoch: [94][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7755 (0.7343) ([0.550]+[0.226])	Prec@1 80.469 (82.646)
Test: [0/79]	Time 0.259 (0.259)	Loss 1.0073 (1.0073) ([0.781]+[0.226])	Prec@1 76.562 (76.562)
 * Prec@1 74.500
current lr 1.00000e-01
Grad=  tensor(2.6798, device='cuda:0')
Epoch: [95][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.8462 (0.8462) ([0.620]+[0.226])	Prec@1 78.906 (78.906)
Epoch: [95][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.6626 (0.7286) ([0.436]+[0.227])	Prec@1 85.156 (82.805)
Epoch: [95][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6141 (0.7291) ([0.387]+[0.227])	Prec@1 88.281 (82.676)
Epoch: [95][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8514 (0.7361) ([0.625]+[0.226])	Prec@1 78.125 (82.618)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.8732 (0.8732) ([0.647]+[0.226])	Prec@1 76.562 (76.562)
 * Prec@1 78.570
current lr 1.00000e-01
Grad=  tensor(2.1763, device='cuda:0')
Epoch: [96][0/391]	Time 0.341 (0.341)	Data 0.219 (0.219)	Loss 0.7135 (0.7135) ([0.488]+[0.226])	Prec@1 82.031 (82.031)
Epoch: [96][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.8327 (0.7410) ([0.607]+[0.226])	Prec@1 78.125 (82.480)
Epoch: [96][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6800 (0.7298) ([0.454]+[0.226])	Prec@1 88.281 (82.820)
Epoch: [96][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.8014 (0.7293) ([0.576]+[0.225])	Prec@1 81.250 (82.737)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.7337 (0.7337) ([0.509]+[0.225])	Prec@1 82.812 (82.812)
 * Prec@1 78.240
current lr 1.00000e-01
Grad=  tensor(2.0804, device='cuda:0')
Epoch: [97][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.7579 (0.7579) ([0.533]+[0.225])	Prec@1 81.250 (81.250)
Epoch: [97][100/391]	Time 0.114 (0.118)	Data 0.000 (0.002)	Loss 0.8007 (0.7249) ([0.576]+[0.225])	Prec@1 83.594 (83.099)
Epoch: [97][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.6856 (0.7222) ([0.461]+[0.225])	Prec@1 85.156 (83.135)
Epoch: [97][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8850 (0.7304) ([0.660]+[0.225])	Prec@1 75.000 (82.678)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.9207 (0.9207) ([0.697]+[0.224])	Prec@1 75.781 (75.781)
 * Prec@1 76.070
current lr 1.00000e-01
Grad=  tensor(2.0551, device='cuda:0')
Epoch: [98][0/391]	Time 0.350 (0.350)	Data 0.224 (0.224)	Loss 0.5988 (0.5988) ([0.375]+[0.224])	Prec@1 82.031 (82.031)
Epoch: [98][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.8775 (0.7341) ([0.653]+[0.225])	Prec@1 74.219 (82.604)
Epoch: [98][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.6882 (0.7327) ([0.464]+[0.224])	Prec@1 85.156 (82.606)
Epoch: [98][300/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.8069 (0.7340) ([0.583]+[0.224])	Prec@1 82.812 (82.545)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.9545 (0.9545) ([0.730]+[0.225])	Prec@1 76.562 (76.562)
 * Prec@1 73.310
current lr 1.00000e-01
Grad=  tensor(2.0468, device='cuda:0')
Epoch: [99][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.7094 (0.7094) ([0.485]+[0.225])	Prec@1 82.031 (82.031)
Epoch: [99][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.7843 (0.7222) ([0.560]+[0.225])	Prec@1 79.688 (82.921)
Epoch: [99][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.5876 (0.7345) ([0.363]+[0.225])	Prec@1 89.062 (82.369)
Epoch: [99][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.9019 (0.7327) ([0.678]+[0.224])	Prec@1 79.688 (82.615)
Test: [0/79]	Time 0.266 (0.266)	Loss 0.8390 (0.8390) ([0.615]+[0.224])	Prec@1 77.344 (77.344)
 * Prec@1 74.710
current lr 1.00000e-02
Grad=  tensor(2.2700, device='cuda:0')
Epoch: [100][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.7977 (0.7977) ([0.574]+[0.224])	Prec@1 77.344 (77.344)
Epoch: [100][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.4633 (0.5948) ([0.254]+[0.210])	Prec@1 89.844 (86.966)
Epoch: [100][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.5972 (0.5601) ([0.389]+[0.208])	Prec@1 85.156 (88.091)
Epoch: [100][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4122 (0.5423) ([0.206]+[0.206])	Prec@1 92.969 (88.655)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4084 (0.4084) ([0.203]+[0.205])	Prec@1 92.969 (92.969)
 * Prec@1 89.320
current lr 1.00000e-02
Grad=  tensor(0.9832, device='cuda:0')
Epoch: [101][0/391]	Time 0.339 (0.339)	Data 0.215 (0.215)	Loss 0.4246 (0.4246) ([0.220]+[0.205])	Prec@1 93.750 (93.750)
Epoch: [101][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.4984 (0.4669) ([0.295]+[0.203])	Prec@1 90.625 (91.143)
Epoch: [101][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.5175 (0.4665) ([0.315]+[0.202])	Prec@1 91.406 (91.029)
Epoch: [101][300/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.4359 (0.4610) ([0.235]+[0.201])	Prec@1 92.188 (91.162)
Test: [0/79]	Time 0.270 (0.270)	Loss 0.3985 (0.3985) ([0.199]+[0.199])	Prec@1 92.969 (92.969)
 * Prec@1 89.770
current lr 1.00000e-02
Grad=  tensor(1.7586, device='cuda:0')
Epoch: [102][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.4178 (0.4178) ([0.218]+[0.199])	Prec@1 92.188 (92.188)
Epoch: [102][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3641 (0.4402) ([0.166]+[0.198])	Prec@1 96.094 (91.615)
Epoch: [102][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4611 (0.4396) ([0.264]+[0.197])	Prec@1 93.750 (91.717)
Epoch: [102][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3856 (0.4353) ([0.190]+[0.196])	Prec@1 92.969 (91.868)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4209 (0.4209) ([0.226]+[0.195])	Prec@1 91.406 (91.406)
 * Prec@1 90.290
current lr 1.00000e-02
Grad=  tensor(1.3761, device='cuda:0')
Epoch: [103][0/391]	Time 0.339 (0.339)	Data 0.214 (0.214)	Loss 0.3582 (0.3582) ([0.164]+[0.195])	Prec@1 92.969 (92.969)
Epoch: [103][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.3432 (0.4101) ([0.150]+[0.193])	Prec@1 95.312 (92.628)
Epoch: [103][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.3961 (0.4151) ([0.204]+[0.192])	Prec@1 92.969 (92.448)
Epoch: [103][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3595 (0.4177) ([0.168]+[0.191])	Prec@1 94.531 (92.390)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.3937 (0.3937) ([0.204]+[0.190])	Prec@1 91.406 (91.406)
 * Prec@1 90.550
current lr 1.00000e-02
Grad=  tensor(1.2585, device='cuda:0')
Epoch: [104][0/391]	Time 0.356 (0.356)	Data 0.231 (0.231)	Loss 0.3473 (0.3473) ([0.157]+[0.190])	Prec@1 96.094 (96.094)
Epoch: [104][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.4768 (0.4030) ([0.288]+[0.189])	Prec@1 89.844 (92.675)
Epoch: [104][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3518 (0.4026) ([0.164]+[0.188])	Prec@1 93.750 (92.728)
Epoch: [104][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.4050 (0.4024) ([0.218]+[0.187])	Prec@1 93.750 (92.701)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4351 (0.4351) ([0.249]+[0.186])	Prec@1 90.625 (90.625)
 * Prec@1 90.200
current lr 1.00000e-02
Grad=  tensor(1.4233, device='cuda:0')
Epoch: [105][0/391]	Time 0.353 (0.353)	Data 0.228 (0.228)	Loss 0.3409 (0.3409) ([0.155]+[0.186])	Prec@1 95.312 (95.312)
Epoch: [105][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.4189 (0.3852) ([0.234]+[0.185])	Prec@1 91.406 (93.162)
Epoch: [105][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3363 (0.3857) ([0.152]+[0.184])	Prec@1 94.531 (93.058)
Epoch: [105][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4264 (0.3856) ([0.244]+[0.183])	Prec@1 91.406 (93.021)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4345 (0.4345) ([0.253]+[0.182])	Prec@1 92.188 (92.188)
 * Prec@1 90.910
current lr 1.00000e-02
Grad=  tensor(1.9736, device='cuda:0')
Epoch: [106][0/391]	Time 0.353 (0.353)	Data 0.227 (0.227)	Loss 0.3949 (0.3949) ([0.213]+[0.182])	Prec@1 92.969 (92.969)
Epoch: [106][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3353 (0.3758) ([0.154]+[0.181])	Prec@1 95.312 (93.294)
Epoch: [106][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3530 (0.3738) ([0.173]+[0.180])	Prec@1 94.531 (93.268)
Epoch: [106][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3942 (0.3761) ([0.215]+[0.179])	Prec@1 93.750 (93.148)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4345 (0.4345) ([0.256]+[0.178])	Prec@1 89.844 (89.844)
 * Prec@1 90.140
current lr 1.00000e-02
Grad=  tensor(1.4801, device='cuda:0')
Epoch: [107][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.2790 (0.2790) ([0.101]+[0.178])	Prec@1 95.312 (95.312)
Epoch: [107][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3003 (0.3606) ([0.123]+[0.177])	Prec@1 96.094 (93.765)
Epoch: [107][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3335 (0.3638) ([0.157]+[0.177])	Prec@1 95.312 (93.560)
Epoch: [107][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.4358 (0.3643) ([0.260]+[0.176])	Prec@1 89.062 (93.535)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3881 (0.3881) ([0.213]+[0.175])	Prec@1 91.406 (91.406)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(1.4143, device='cuda:0')
Epoch: [108][0/391]	Time 0.340 (0.340)	Data 0.215 (0.215)	Loss 0.2881 (0.2881) ([0.113]+[0.175])	Prec@1 96.094 (96.094)
Epoch: [108][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2801 (0.3507) ([0.106]+[0.174])	Prec@1 97.656 (93.773)
Epoch: [108][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3515 (0.3518) ([0.178]+[0.173])	Prec@1 93.750 (93.843)
Epoch: [108][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3144 (0.3511) ([0.142]+[0.173])	Prec@1 94.531 (93.924)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3746 (0.3746) ([0.203]+[0.172])	Prec@1 92.969 (92.969)
 * Prec@1 91.050
current lr 1.00000e-02
Grad=  tensor(2.1132, device='cuda:0')
Epoch: [109][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.2905 (0.2905) ([0.119]+[0.172])	Prec@1 96.875 (96.875)
Epoch: [109][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.3784 (0.3298) ([0.207]+[0.171])	Prec@1 93.750 (94.547)
Epoch: [109][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3383 (0.3365) ([0.168]+[0.170])	Prec@1 93.750 (94.279)
Epoch: [109][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3674 (0.3434) ([0.198]+[0.170])	Prec@1 94.531 (94.043)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4218 (0.4218) ([0.253]+[0.169])	Prec@1 89.062 (89.062)
 * Prec@1 90.800
current lr 1.00000e-02
Grad=  tensor(1.8525, device='cuda:0')
Epoch: [110][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.2921 (0.2921) ([0.123]+[0.169])	Prec@1 94.531 (94.531)
Epoch: [110][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2479 (0.3244) ([0.079]+[0.169])	Prec@1 97.656 (94.539)
Epoch: [110][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3978 (0.3318) ([0.230]+[0.168])	Prec@1 91.406 (94.321)
Epoch: [110][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2972 (0.3375) ([0.130]+[0.167])	Prec@1 96.094 (94.087)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3870 (0.3870) ([0.220]+[0.167])	Prec@1 89.844 (89.844)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(1.9117, device='cuda:0')
Epoch: [111][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.2730 (0.2730) ([0.106]+[0.167])	Prec@1 97.656 (97.656)
Epoch: [111][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3070 (0.3261) ([0.141]+[0.166])	Prec@1 93.750 (94.469)
Epoch: [111][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3027 (0.3276) ([0.137]+[0.165])	Prec@1 95.312 (94.395)
Epoch: [111][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2560 (0.3305) ([0.091]+[0.165])	Prec@1 96.875 (94.285)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3579 (0.3579) ([0.194]+[0.164])	Prec@1 92.188 (92.188)
 * Prec@1 89.340
current lr 1.00000e-02
Grad=  tensor(5.9164, device='cuda:0')
Epoch: [112][0/391]	Time 0.340 (0.340)	Data 0.215 (0.215)	Loss 0.3854 (0.3854) ([0.221]+[0.164])	Prec@1 91.406 (91.406)
Epoch: [112][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2727 (0.3214) ([0.109]+[0.164])	Prec@1 96.875 (94.593)
Epoch: [112][200/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.4053 (0.3203) ([0.242]+[0.163])	Prec@1 92.188 (94.675)
Epoch: [112][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.3158 (0.3254) ([0.153]+[0.163])	Prec@1 94.531 (94.409)
Test: [0/79]	Time 0.419 (0.419)	Loss 0.4046 (0.4046) ([0.242]+[0.162])	Prec@1 90.625 (90.625)
 * Prec@1 90.160
current lr 1.00000e-02
Grad=  tensor(3.2284, device='cuda:0')
Epoch: [113][0/391]	Time 0.442 (0.442)	Data 0.272 (0.272)	Loss 0.2894 (0.2894) ([0.127]+[0.162])	Prec@1 95.312 (95.312)
Epoch: [113][100/391]	Time 0.114 (0.118)	Data 0.000 (0.003)	Loss 0.3110 (0.3115) ([0.149]+[0.162])	Prec@1 95.312 (94.926)
Epoch: [113][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3195 (0.3226) ([0.158]+[0.161])	Prec@1 92.969 (94.520)
Epoch: [113][300/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.3920 (0.3268) ([0.231]+[0.161])	Prec@1 92.969 (94.300)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.5175 (0.5175) ([0.357]+[0.160])	Prec@1 86.719 (86.719)
 * Prec@1 88.900
current lr 1.00000e-02
Grad=  tensor(3.6561, device='cuda:0')
Epoch: [114][0/391]	Time 0.348 (0.348)	Data 0.221 (0.221)	Loss 0.2857 (0.2857) ([0.125]+[0.160])	Prec@1 94.531 (94.531)
Epoch: [114][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3544 (0.3134) ([0.195]+[0.160])	Prec@1 93.750 (94.972)
Epoch: [114][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4153 (0.3173) ([0.256]+[0.159])	Prec@1 89.844 (94.683)
Epoch: [114][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3109 (0.3178) ([0.152]+[0.159])	Prec@1 94.531 (94.625)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4247 (0.4247) ([0.266]+[0.158])	Prec@1 91.406 (91.406)
 * Prec@1 89.980
current lr 1.00000e-02
Grad=  tensor(4.4531, device='cuda:0')
Epoch: [115][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.3604 (0.3604) ([0.202]+[0.158])	Prec@1 94.531 (94.531)
Epoch: [115][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2877 (0.3101) ([0.130]+[0.158])	Prec@1 95.312 (94.779)
Epoch: [115][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3263 (0.3091) ([0.169]+[0.157])	Prec@1 94.531 (94.803)
Epoch: [115][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3849 (0.3131) ([0.228]+[0.157])	Prec@1 89.062 (94.588)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4072 (0.4072) ([0.250]+[0.157])	Prec@1 91.406 (91.406)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(5.8616, device='cuda:0')
Epoch: [116][0/391]	Time 0.340 (0.340)	Data 0.219 (0.219)	Loss 0.3340 (0.3340) ([0.177]+[0.157])	Prec@1 93.750 (93.750)
Epoch: [116][100/391]	Time 0.111 (0.111)	Data 0.000 (0.002)	Loss 0.2698 (0.3065) ([0.113]+[0.156])	Prec@1 96.094 (94.841)
Epoch: [116][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.3456 (0.3138) ([0.190]+[0.156])	Prec@1 94.531 (94.477)
Epoch: [116][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.2971 (0.3115) ([0.142]+[0.156])	Prec@1 96.094 (94.560)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.3880 (0.3880) ([0.233]+[0.155])	Prec@1 92.969 (92.969)
 * Prec@1 89.860
current lr 1.00000e-02
Grad=  tensor(7.7283, device='cuda:0')
Epoch: [117][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 0.3285 (0.3285) ([0.173]+[0.155])	Prec@1 93.750 (93.750)
Epoch: [117][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2813 (0.3032) ([0.126]+[0.155])	Prec@1 96.094 (95.011)
Epoch: [117][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2905 (0.3068) ([0.136]+[0.155])	Prec@1 96.094 (94.869)
Epoch: [117][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4090 (0.3123) ([0.255]+[0.154])	Prec@1 92.188 (94.632)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.4320 (0.4320) ([0.278]+[0.154])	Prec@1 92.188 (92.188)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(7.1183, device='cuda:0')
Epoch: [118][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.3752 (0.3752) ([0.221]+[0.154])	Prec@1 93.750 (93.750)
Epoch: [118][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2908 (0.3171) ([0.137]+[0.154])	Prec@1 95.312 (94.346)
Epoch: [118][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3885 (0.3139) ([0.235]+[0.154])	Prec@1 92.969 (94.488)
Epoch: [118][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2630 (0.3153) ([0.110]+[0.153])	Prec@1 96.094 (94.365)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4142 (0.4142) ([0.261]+[0.153])	Prec@1 87.500 (87.500)
 * Prec@1 89.710
current lr 1.00000e-02
Grad=  tensor(4.6160, device='cuda:0')
Epoch: [119][0/391]	Time 0.340 (0.340)	Data 0.215 (0.215)	Loss 0.3182 (0.3182) ([0.165]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [119][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2838 (0.3038) ([0.131]+[0.153])	Prec@1 93.750 (94.655)
Epoch: [119][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2387 (0.3061) ([0.086]+[0.152])	Prec@1 97.656 (94.539)
Epoch: [119][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3095 (0.3117) ([0.157]+[0.152])	Prec@1 95.312 (94.381)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3640 (0.3640) ([0.212]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 89.770
current lr 1.00000e-02
Grad=  tensor(4.2258, device='cuda:0')
Epoch: [120][0/391]	Time 0.335 (0.335)	Data 0.211 (0.211)	Loss 0.2552 (0.2552) ([0.103]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [120][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2944 (0.2960) ([0.143]+[0.152])	Prec@1 95.312 (95.003)
Epoch: [120][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2619 (0.3055) ([0.110]+[0.152])	Prec@1 96.094 (94.597)
Epoch: [120][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3545 (0.3076) ([0.203]+[0.151])	Prec@1 91.406 (94.529)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3536 (0.3536) ([0.202]+[0.151])	Prec@1 92.969 (92.969)
 * Prec@1 89.670
current lr 1.00000e-02
Grad=  tensor(5.7164, device='cuda:0')
Epoch: [121][0/391]	Time 0.342 (0.342)	Data 0.218 (0.218)	Loss 0.2874 (0.2874) ([0.136]+[0.151])	Prec@1 93.750 (93.750)
Epoch: [121][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.4374 (0.2979) ([0.287]+[0.151])	Prec@1 91.406 (94.926)
Epoch: [121][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2561 (0.3035) ([0.105]+[0.151])	Prec@1 96.094 (94.652)
Epoch: [121][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2652 (0.3058) ([0.115]+[0.151])	Prec@1 97.656 (94.596)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3591 (0.3591) ([0.209]+[0.151])	Prec@1 94.531 (94.531)
 * Prec@1 89.500
current lr 1.00000e-02
Grad=  tensor(2.3352, device='cuda:0')
Epoch: [122][0/391]	Time 0.341 (0.341)	Data 0.218 (0.218)	Loss 0.2338 (0.2338) ([0.083]+[0.151])	Prec@1 98.438 (98.438)
Epoch: [122][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.2885 (0.2993) ([0.138]+[0.150])	Prec@1 92.969 (94.802)
Epoch: [122][200/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.3082 (0.3001) ([0.158]+[0.150])	Prec@1 93.750 (94.776)
Epoch: [122][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.3239 (0.3038) ([0.174]+[0.150])	Prec@1 94.531 (94.677)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.3453 (0.3453) ([0.196]+[0.150])	Prec@1 96.094 (96.094)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(10.7845, device='cuda:0')
Epoch: [123][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.3869 (0.3869) ([0.237]+[0.150])	Prec@1 90.625 (90.625)
Epoch: [123][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3024 (0.2919) ([0.153]+[0.150])	Prec@1 94.531 (95.034)
Epoch: [123][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3087 (0.2973) ([0.159]+[0.149])	Prec@1 93.750 (94.908)
Epoch: [123][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3069 (0.3041) ([0.158]+[0.149])	Prec@1 94.531 (94.651)
Test: [0/79]	Time 0.267 (0.267)	Loss 0.4226 (0.4226) ([0.273]+[0.149])	Prec@1 89.844 (89.844)
 * Prec@1 89.110
current lr 1.00000e-02
Grad=  tensor(7.4911, device='cuda:0')
Epoch: [124][0/391]	Time 0.339 (0.339)	Data 0.214 (0.214)	Loss 0.3563 (0.3563) ([0.207]+[0.149])	Prec@1 92.969 (92.969)
Epoch: [124][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3130 (0.3060) ([0.164]+[0.149])	Prec@1 95.312 (94.694)
Epoch: [124][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2783 (0.3013) ([0.129]+[0.149])	Prec@1 96.094 (94.811)
Epoch: [124][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3642 (0.3053) ([0.215]+[0.149])	Prec@1 92.188 (94.736)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4955 (0.4955) ([0.347]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 88.740
current lr 1.00000e-02
Grad=  tensor(5.0229, device='cuda:0')
Epoch: [125][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.2621 (0.2621) ([0.113]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [125][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.3057 (0.2943) ([0.157]+[0.148])	Prec@1 94.531 (94.972)
Epoch: [125][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2498 (0.3030) ([0.101]+[0.148])	Prec@1 96.094 (94.481)
Epoch: [125][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2825 (0.3033) ([0.134]+[0.148])	Prec@1 95.312 (94.560)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4499 (0.4499) ([0.302]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(8.2206, device='cuda:0')
Epoch: [126][0/391]	Time 0.349 (0.349)	Data 0.226 (0.226)	Loss 0.4036 (0.4036) ([0.255]+[0.148])	Prec@1 90.625 (90.625)
Epoch: [126][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3193 (0.2961) ([0.171]+[0.148])	Prec@1 92.188 (94.918)
Epoch: [126][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2514 (0.3001) ([0.103]+[0.148])	Prec@1 98.438 (94.753)
Epoch: [126][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2872 (0.3026) ([0.139]+[0.148])	Prec@1 93.750 (94.578)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4549 (0.4549) ([0.307]+[0.148])	Prec@1 89.062 (89.062)
 * Prec@1 89.580
current lr 1.00000e-02
Grad=  tensor(5.9141, device='cuda:0')
Epoch: [127][0/391]	Time 0.347 (0.347)	Data 0.224 (0.224)	Loss 0.3135 (0.3135) ([0.166]+[0.148])	Prec@1 94.531 (94.531)
Epoch: [127][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2892 (0.2990) ([0.141]+[0.148])	Prec@1 95.312 (94.787)
Epoch: [127][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3402 (0.3034) ([0.192]+[0.148])	Prec@1 93.750 (94.609)
Epoch: [127][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2785 (0.3091) ([0.131]+[0.148])	Prec@1 95.312 (94.409)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.5528 (0.5528) ([0.405]+[0.148])	Prec@1 86.719 (86.719)
 * Prec@1 88.480
current lr 1.00000e-02
Grad=  tensor(5.6746, device='cuda:0')
Epoch: [128][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.2730 (0.2730) ([0.125]+[0.148])	Prec@1 96.094 (96.094)
Epoch: [128][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2462 (0.3026) ([0.099]+[0.148])	Prec@1 96.875 (94.647)
Epoch: [128][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2969 (0.3047) ([0.149]+[0.147])	Prec@1 95.312 (94.543)
Epoch: [128][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3129 (0.3076) ([0.165]+[0.147])	Prec@1 95.312 (94.401)
Test: [0/79]	Time 0.266 (0.266)	Loss 0.3690 (0.3690) ([0.222]+[0.147])	Prec@1 93.750 (93.750)
 * Prec@1 89.450
current lr 1.00000e-02
Grad=  tensor(4.6759, device='cuda:0')
Epoch: [129][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.2811 (0.2811) ([0.134]+[0.147])	Prec@1 96.094 (96.094)
Epoch: [129][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3153 (0.2956) ([0.168]+[0.147])	Prec@1 95.312 (94.825)
Epoch: [129][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2578 (0.3029) ([0.111]+[0.147])	Prec@1 96.875 (94.543)
Epoch: [129][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4008 (0.3059) ([0.254]+[0.147])	Prec@1 89.062 (94.440)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4288 (0.4288) ([0.282]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(5.8620, device='cuda:0')
Epoch: [130][0/391]	Time 0.339 (0.339)	Data 0.217 (0.217)	Loss 0.3042 (0.3042) ([0.157]+[0.147])	Prec@1 93.750 (93.750)
Epoch: [130][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.3309 (0.2873) ([0.184]+[0.147])	Prec@1 92.188 (95.212)
Epoch: [130][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3382 (0.2924) ([0.191]+[0.147])	Prec@1 94.531 (94.986)
Epoch: [130][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3851 (0.3009) ([0.238]+[0.147])	Prec@1 90.625 (94.648)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.3598 (0.3598) ([0.213]+[0.147])	Prec@1 89.844 (89.844)
 * Prec@1 89.560
current lr 1.00000e-02
Grad=  tensor(2.9196, device='cuda:0')
Epoch: [131][0/391]	Time 0.346 (0.346)	Data 0.225 (0.225)	Loss 0.2387 (0.2387) ([0.092]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [131][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2462 (0.2962) ([0.100]+[0.147])	Prec@1 96.875 (94.918)
Epoch: [131][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3848 (0.3043) ([0.238]+[0.147])	Prec@1 90.625 (94.586)
Epoch: [131][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.4268 (0.3069) ([0.280]+[0.147])	Prec@1 90.625 (94.549)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4374 (0.4374) ([0.291]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 90.590
current lr 1.00000e-02
Grad=  tensor(9.5827, device='cuda:0')
Epoch: [132][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.3880 (0.3880) ([0.241]+[0.147])	Prec@1 93.750 (93.750)
Epoch: [132][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2359 (0.2917) ([0.089]+[0.147])	Prec@1 96.094 (94.980)
Epoch: [132][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2838 (0.2978) ([0.137]+[0.147])	Prec@1 95.312 (94.706)
Epoch: [132][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3262 (0.3006) ([0.180]+[0.146])	Prec@1 91.406 (94.588)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3955 (0.3955) ([0.249]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 89.210
current lr 1.00000e-02
Grad=  tensor(7.2997, device='cuda:0')
Epoch: [133][0/391]	Time 0.339 (0.339)	Data 0.214 (0.214)	Loss 0.2615 (0.2615) ([0.115]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [133][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2857 (0.2926) ([0.139]+[0.146])	Prec@1 93.750 (94.864)
Epoch: [133][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3300 (0.2978) ([0.184]+[0.146])	Prec@1 93.750 (94.691)
Epoch: [133][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4136 (0.3055) ([0.267]+[0.147])	Prec@1 90.625 (94.464)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.5858 (0.5858) ([0.439]+[0.147])	Prec@1 86.719 (86.719)
 * Prec@1 88.220
current lr 1.00000e-02
Grad=  tensor(10.0711, device='cuda:0')
Epoch: [134][0/391]	Time 0.344 (0.344)	Data 0.218 (0.218)	Loss 0.3350 (0.3350) ([0.188]+[0.147])	Prec@1 92.969 (92.969)
Epoch: [134][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.3743 (0.2916) ([0.228]+[0.146])	Prec@1 91.406 (94.779)
Epoch: [134][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3691 (0.2985) ([0.223]+[0.146])	Prec@1 92.188 (94.621)
Epoch: [134][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3620 (0.2990) ([0.216]+[0.146])	Prec@1 93.750 (94.638)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4008 (0.4008) ([0.254]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 89.580
current lr 1.00000e-02
Grad=  tensor(8.5668, device='cuda:0')
Epoch: [135][0/391]	Time 0.347 (0.347)	Data 0.214 (0.214)	Loss 0.3733 (0.3733) ([0.227]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [135][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3063 (0.2970) ([0.160]+[0.146])	Prec@1 95.312 (94.903)
Epoch: [135][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3208 (0.3008) ([0.174]+[0.146])	Prec@1 92.969 (94.597)
Epoch: [135][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.4555 (0.3026) ([0.309]+[0.146])	Prec@1 91.406 (94.536)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4358 (0.4358) ([0.289]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 90.820
current lr 1.00000e-02
Grad=  tensor(5.8510, device='cuda:0')
Epoch: [136][0/391]	Time 0.341 (0.341)	Data 0.218 (0.218)	Loss 0.2985 (0.2985) ([0.152]+[0.146])	Prec@1 93.750 (93.750)
Epoch: [136][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2911 (0.2916) ([0.145]+[0.146])	Prec@1 92.969 (94.918)
Epoch: [136][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2597 (0.2973) ([0.113]+[0.146])	Prec@1 94.531 (94.764)
Epoch: [136][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2581 (0.3024) ([0.112]+[0.146])	Prec@1 95.312 (94.575)
Test: [0/79]	Time 0.266 (0.266)	Loss 0.4111 (0.4111) ([0.265]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 89.330
current lr 1.00000e-02
Grad=  tensor(8.4653, device='cuda:0')
Epoch: [137][0/391]	Time 0.356 (0.356)	Data 0.230 (0.230)	Loss 0.3197 (0.3197) ([0.173]+[0.146])	Prec@1 92.188 (92.188)
Epoch: [137][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2931 (0.2863) ([0.147]+[0.146])	Prec@1 92.969 (94.949)
Epoch: [137][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2362 (0.2920) ([0.090]+[0.146])	Prec@1 97.656 (94.850)
Epoch: [137][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4121 (0.3014) ([0.266]+[0.146])	Prec@1 92.188 (94.464)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4475 (0.4475) ([0.301]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(9.0974, device='cuda:0')
Epoch: [138][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.3391 (0.3391) ([0.193]+[0.146])	Prec@1 92.188 (92.188)
Epoch: [138][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2419 (0.2866) ([0.096]+[0.146])	Prec@1 96.875 (95.227)
Epoch: [138][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3265 (0.2967) ([0.180]+[0.146])	Prec@1 93.750 (94.784)
Epoch: [138][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2985 (0.2994) ([0.152]+[0.146])	Prec@1 92.969 (94.695)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4705 (0.4705) ([0.324]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(4.3760, device='cuda:0')
Epoch: [139][0/391]	Time 0.341 (0.341)	Data 0.219 (0.219)	Loss 0.2452 (0.2452) ([0.099]+[0.146])	Prec@1 96.094 (96.094)
Epoch: [139][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.3859 (0.2914) ([0.240]+[0.146])	Prec@1 91.406 (95.227)
Epoch: [139][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2407 (0.2882) ([0.095]+[0.146])	Prec@1 96.094 (95.169)
Epoch: [139][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2990 (0.2935) ([0.153]+[0.146])	Prec@1 95.312 (94.918)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.4884 (0.4884) ([0.342]+[0.146])	Prec@1 87.500 (87.500)
 * Prec@1 88.770
current lr 1.00000e-02
Grad=  tensor(4.2923, device='cuda:0')
Epoch: [140][0/391]	Time 0.354 (0.354)	Data 0.231 (0.231)	Loss 0.2637 (0.2637) ([0.117]+[0.146])	Prec@1 96.094 (96.094)
Epoch: [140][100/391]	Time 0.108 (0.113)	Data 0.000 (0.002)	Loss 0.2306 (0.2883) ([0.084]+[0.146])	Prec@1 96.875 (95.057)
Epoch: [140][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2528 (0.2910) ([0.107]+[0.146])	Prec@1 95.312 (95.056)
Epoch: [140][300/391]	Time 0.114 (0.111)	Data 0.000 (0.001)	Loss 0.2948 (0.3001) ([0.149]+[0.146])	Prec@1 95.312 (94.653)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.5034 (0.5034) ([0.357]+[0.146])	Prec@1 89.844 (89.844)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(6.0637, device='cuda:0')
Epoch: [141][0/391]	Time 0.351 (0.351)	Data 0.225 (0.225)	Loss 0.2886 (0.2886) ([0.142]+[0.146])	Prec@1 95.312 (95.312)
Epoch: [141][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2880 (0.2802) ([0.142]+[0.146])	Prec@1 93.750 (95.568)
Epoch: [141][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2855 (0.2915) ([0.139]+[0.146])	Prec@1 95.312 (95.165)
Epoch: [141][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3176 (0.2957) ([0.171]+[0.146])	Prec@1 95.312 (95.024)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.3599 (0.3599) ([0.214]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(10.8509, device='cuda:0')
Epoch: [142][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.3526 (0.3526) ([0.207]+[0.146])	Prec@1 91.406 (91.406)
Epoch: [142][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.4073 (0.2863) ([0.261]+[0.146])	Prec@1 90.625 (95.026)
Epoch: [142][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3551 (0.2915) ([0.209]+[0.146])	Prec@1 92.188 (94.889)
Epoch: [142][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2377 (0.2957) ([0.091]+[0.146])	Prec@1 97.656 (94.723)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.3614 (0.3614) ([0.215]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 89.030
current lr 1.00000e-02
Grad=  tensor(4.5424, device='cuda:0')
Epoch: [143][0/391]	Time 0.359 (0.359)	Data 0.235 (0.235)	Loss 0.2406 (0.2406) ([0.094]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [143][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3415 (0.2852) ([0.195]+[0.146])	Prec@1 95.312 (95.336)
Epoch: [143][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3197 (0.2903) ([0.174]+[0.146])	Prec@1 93.750 (95.126)
Epoch: [143][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3259 (0.2933) ([0.180]+[0.146])	Prec@1 93.750 (94.897)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4760 (0.4760) ([0.330]+[0.146])	Prec@1 90.625 (90.625)
 * Prec@1 88.460
current lr 1.00000e-02
Grad=  tensor(5.0677, device='cuda:0')
Epoch: [144][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.2408 (0.2408) ([0.094]+[0.146])	Prec@1 96.875 (96.875)
Epoch: [144][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2770 (0.2928) ([0.131]+[0.146])	Prec@1 94.531 (94.957)
Epoch: [144][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3459 (0.2979) ([0.200]+[0.146])	Prec@1 93.750 (94.737)
Epoch: [144][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.3484 (0.2968) ([0.202]+[0.146])	Prec@1 93.750 (94.783)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4292 (0.4292) ([0.283]+[0.146])	Prec@1 91.406 (91.406)
 * Prec@1 89.620
current lr 1.00000e-02
Grad=  tensor(4.5165, device='cuda:0')
Epoch: [145][0/391]	Time 0.341 (0.341)	Data 0.219 (0.219)	Loss 0.2399 (0.2399) ([0.094]+[0.146])	Prec@1 97.656 (97.656)
Epoch: [145][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.3325 (0.2870) ([0.186]+[0.146])	Prec@1 94.531 (95.135)
Epoch: [145][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3624 (0.2915) ([0.216]+[0.146])	Prec@1 91.406 (94.947)
Epoch: [145][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2979 (0.2991) ([0.151]+[0.147])	Prec@1 93.750 (94.705)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.4552 (0.4552) ([0.309]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 88.050
current lr 1.00000e-02
Grad=  tensor(2.9435, device='cuda:0')
Epoch: [146][0/391]	Time 0.348 (0.348)	Data 0.225 (0.225)	Loss 0.2101 (0.2101) ([0.064]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [146][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2975 (0.3008) ([0.151]+[0.147])	Prec@1 96.094 (94.709)
Epoch: [146][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3173 (0.2956) ([0.171]+[0.147])	Prec@1 94.531 (94.877)
Epoch: [146][300/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.2193 (0.2963) ([0.073]+[0.147])	Prec@1 96.875 (94.832)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4454 (0.4454) ([0.299]+[0.147])	Prec@1 90.625 (90.625)
 * Prec@1 88.990
current lr 1.00000e-02
Grad=  tensor(5.2009, device='cuda:0')
Epoch: [147][0/391]	Time 0.345 (0.345)	Data 0.223 (0.223)	Loss 0.2553 (0.2553) ([0.109]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [147][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.3296 (0.2857) ([0.183]+[0.147])	Prec@1 92.188 (95.235)
Epoch: [147][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2426 (0.2940) ([0.096]+[0.147])	Prec@1 96.094 (94.924)
Epoch: [147][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.3311 (0.2966) ([0.184]+[0.147])	Prec@1 96.094 (94.843)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.5091 (0.5091) ([0.362]+[0.147])	Prec@1 87.500 (87.500)
 * Prec@1 87.980
current lr 1.00000e-02
Grad=  tensor(15.2594, device='cuda:0')
Epoch: [148][0/391]	Time 0.354 (0.354)	Data 0.231 (0.231)	Loss 0.2857 (0.2857) ([0.139]+[0.147])	Prec@1 94.531 (94.531)
Epoch: [148][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2238 (0.2947) ([0.077]+[0.147])	Prec@1 98.438 (94.995)
Epoch: [148][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2738 (0.3010) ([0.127]+[0.147])	Prec@1 95.312 (94.726)
Epoch: [148][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3939 (0.3005) ([0.247]+[0.147])	Prec@1 88.281 (94.684)
Test: [0/79]	Time 0.266 (0.266)	Loss 0.5072 (0.5072) ([0.360]+[0.147])	Prec@1 89.062 (89.062)
 * Prec@1 88.900
current lr 1.00000e-02
Grad=  tensor(4.8664, device='cuda:0')
Epoch: [149][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 0.2357 (0.2357) ([0.089]+[0.147])	Prec@1 97.656 (97.656)
Epoch: [149][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.3260 (0.3008) ([0.179]+[0.147])	Prec@1 92.188 (94.740)
Epoch: [149][200/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2645 (0.2969) ([0.117]+[0.147])	Prec@1 96.094 (94.823)
Epoch: [149][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2991 (0.2951) ([0.152]+[0.147])	Prec@1 93.750 (94.889)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4520 (0.4520) ([0.305]+[0.147])	Prec@1 88.281 (88.281)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(5.8908, device='cuda:0')
Epoch: [150][0/391]	Time 0.336 (0.336)	Data 0.211 (0.211)	Loss 0.2909 (0.2909) ([0.144]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [150][100/391]	Time 0.117 (0.116)	Data 0.000 (0.002)	Loss 0.2469 (0.2821) ([0.100]+[0.147])	Prec@1 96.094 (95.282)
Epoch: [150][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3212 (0.2895) ([0.174]+[0.147])	Prec@1 94.531 (95.013)
Epoch: [150][300/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.3552 (0.2911) ([0.208]+[0.147])	Prec@1 92.969 (94.931)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3984 (0.3984) ([0.251]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 88.880
current lr 1.00000e-02
Grad=  tensor(9.2054, device='cuda:0')
Epoch: [151][0/391]	Time 0.352 (0.352)	Data 0.225 (0.225)	Loss 0.3162 (0.3162) ([0.169]+[0.147])	Prec@1 93.750 (93.750)
Epoch: [151][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2621 (0.2903) ([0.115]+[0.147])	Prec@1 98.438 (95.189)
Epoch: [151][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2806 (0.2894) ([0.133]+[0.147])	Prec@1 95.312 (95.087)
Epoch: [151][300/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.2960 (0.2948) ([0.149]+[0.147])	Prec@1 92.188 (94.866)
Test: [0/79]	Time 0.267 (0.267)	Loss 0.3498 (0.3498) ([0.202]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(6.8161, device='cuda:0')
Epoch: [152][0/391]	Time 0.343 (0.343)	Data 0.221 (0.221)	Loss 0.3000 (0.3000) ([0.153]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [152][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.3398 (0.2838) ([0.192]+[0.147])	Prec@1 93.750 (95.367)
Epoch: [152][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3714 (0.2924) ([0.224]+[0.147])	Prec@1 92.188 (94.924)
Epoch: [152][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2830 (0.2970) ([0.136]+[0.147])	Prec@1 95.312 (94.783)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.4623 (0.4623) ([0.315]+[0.148])	Prec@1 89.844 (89.844)
 * Prec@1 88.970
current lr 1.00000e-02
Grad=  tensor(13.2423, device='cuda:0')
Epoch: [153][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.3535 (0.3535) ([0.206]+[0.148])	Prec@1 90.625 (90.625)
Epoch: [153][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2163 (0.2934) ([0.069]+[0.147])	Prec@1 98.438 (95.065)
Epoch: [153][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2487 (0.2932) ([0.101]+[0.148])	Prec@1 96.875 (95.064)
Epoch: [153][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3130 (0.2994) ([0.165]+[0.148])	Prec@1 93.750 (94.817)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.4720 (0.4720) ([0.324]+[0.148])	Prec@1 87.500 (87.500)
 * Prec@1 88.630
current lr 1.00000e-02
Grad=  tensor(5.8386, device='cuda:0')
Epoch: [154][0/391]	Time 0.353 (0.353)	Data 0.228 (0.228)	Loss 0.2613 (0.2613) ([0.113]+[0.148])	Prec@1 96.094 (96.094)
Epoch: [154][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2900 (0.2865) ([0.142]+[0.148])	Prec@1 95.312 (95.359)
Epoch: [154][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2786 (0.2909) ([0.131]+[0.148])	Prec@1 95.312 (95.122)
Epoch: [154][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2917 (0.2936) ([0.144]+[0.148])	Prec@1 95.312 (94.993)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.5236 (0.5236) ([0.376]+[0.148])	Prec@1 89.062 (89.062)
 * Prec@1 88.880
current lr 1.00000e-02
Grad=  tensor(12.0265, device='cuda:0')
Epoch: [155][0/391]	Time 0.350 (0.350)	Data 0.224 (0.224)	Loss 0.4005 (0.4005) ([0.253]+[0.148])	Prec@1 91.406 (91.406)
Epoch: [155][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.3345 (0.2892) ([0.187]+[0.148])	Prec@1 91.406 (95.042)
Epoch: [155][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3408 (0.2900) ([0.193]+[0.148])	Prec@1 91.406 (95.013)
Epoch: [155][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2922 (0.2925) ([0.145]+[0.148])	Prec@1 96.094 (94.936)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4120 (0.4120) ([0.264]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 89.890
current lr 1.00000e-02
Grad=  tensor(4.7950, device='cuda:0')
Epoch: [156][0/391]	Time 0.355 (0.355)	Data 0.225 (0.225)	Loss 0.2617 (0.2617) ([0.114]+[0.148])	Prec@1 96.094 (96.094)
Epoch: [156][100/391]	Time 0.116 (0.117)	Data 0.000 (0.002)	Loss 0.3016 (0.2815) ([0.154]+[0.148])	Prec@1 95.312 (95.367)
Epoch: [156][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2889 (0.2834) ([0.141]+[0.148])	Prec@1 96.875 (95.425)
Epoch: [156][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2554 (0.2880) ([0.108]+[0.148])	Prec@1 96.875 (95.222)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3264 (0.3264) ([0.179]+[0.148])	Prec@1 93.750 (93.750)
 * Prec@1 89.520
current lr 1.00000e-02
Grad=  tensor(12.3174, device='cuda:0')
Epoch: [157][0/391]	Time 0.337 (0.337)	Data 0.216 (0.216)	Loss 0.3333 (0.3333) ([0.185]+[0.148])	Prec@1 93.750 (93.750)
Epoch: [157][100/391]	Time 0.115 (0.113)	Data 0.000 (0.002)	Loss 0.3134 (0.2894) ([0.166]+[0.148])	Prec@1 96.094 (95.080)
Epoch: [157][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.3246 (0.2880) ([0.177]+[0.148])	Prec@1 96.094 (95.184)
Epoch: [157][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2189 (0.2911) ([0.071]+[0.148])	Prec@1 96.875 (95.040)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.5550 (0.5550) ([0.407]+[0.148])	Prec@1 88.281 (88.281)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(9.4983, device='cuda:0')
Epoch: [158][0/391]	Time 0.357 (0.357)	Data 0.226 (0.226)	Loss 0.3238 (0.3238) ([0.176]+[0.148])	Prec@1 93.750 (93.750)
Epoch: [158][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2947 (0.2992) ([0.147]+[0.148])	Prec@1 96.875 (94.841)
Epoch: [158][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2891 (0.2953) ([0.141]+[0.148])	Prec@1 95.312 (94.963)
Epoch: [158][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2725 (0.2968) ([0.124]+[0.148])	Prec@1 96.875 (94.848)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4823 (0.4823) ([0.334]+[0.148])	Prec@1 88.281 (88.281)
 * Prec@1 89.470
current lr 1.00000e-02
Grad=  tensor(11.3115, device='cuda:0')
Epoch: [159][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.3513 (0.3513) ([0.203]+[0.148])	Prec@1 92.188 (92.188)
Epoch: [159][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2837 (0.2889) ([0.136]+[0.148])	Prec@1 96.875 (95.173)
Epoch: [159][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2957 (0.2884) ([0.148]+[0.148])	Prec@1 95.312 (95.235)
Epoch: [159][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2726 (0.2897) ([0.125]+[0.148])	Prec@1 96.094 (95.193)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4475 (0.4475) ([0.299]+[0.148])	Prec@1 91.406 (91.406)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(6.3395, device='cuda:0')
Epoch: [160][0/391]	Time 0.342 (0.342)	Data 0.222 (0.222)	Loss 0.2612 (0.2612) ([0.113]+[0.148])	Prec@1 96.875 (96.875)
Epoch: [160][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.3174 (0.2749) ([0.169]+[0.148])	Prec@1 92.969 (95.599)
Epoch: [160][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2726 (0.2824) ([0.125]+[0.148])	Prec@1 96.094 (95.332)
Epoch: [160][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2209 (0.2883) ([0.073]+[0.148])	Prec@1 99.219 (95.074)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4235 (0.4235) ([0.275]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 89.660
current lr 1.00000e-02
Grad=  tensor(4.8477, device='cuda:0')
Epoch: [161][0/391]	Time 0.347 (0.347)	Data 0.224 (0.224)	Loss 0.2548 (0.2548) ([0.107]+[0.148])	Prec@1 95.312 (95.312)
Epoch: [161][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.3045 (0.2751) ([0.156]+[0.148])	Prec@1 96.094 (95.645)
Epoch: [161][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2484 (0.2817) ([0.100]+[0.148])	Prec@1 97.656 (95.464)
Epoch: [161][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3439 (0.2853) ([0.196]+[0.148])	Prec@1 92.969 (95.297)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.5027 (0.5027) ([0.354]+[0.148])	Prec@1 89.844 (89.844)
 * Prec@1 87.190
current lr 1.00000e-02
Grad=  tensor(12.1324, device='cuda:0')
Epoch: [162][0/391]	Time 0.348 (0.348)	Data 0.225 (0.225)	Loss 0.3700 (0.3700) ([0.222]+[0.148])	Prec@1 93.750 (93.750)
Epoch: [162][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1999 (0.2831) ([0.052]+[0.148])	Prec@1 100.000 (95.359)
Epoch: [162][200/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.4618 (0.2847) ([0.314]+[0.148])	Prec@1 92.188 (95.305)
Epoch: [162][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2498 (0.2840) ([0.102]+[0.148])	Prec@1 96.875 (95.315)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3418 (0.3418) ([0.193]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 89.040
current lr 1.00000e-02
Grad=  tensor(9.9200, device='cuda:0')
Epoch: [163][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.3133 (0.3133) ([0.165]+[0.148])	Prec@1 94.531 (94.531)
Epoch: [163][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2298 (0.2847) ([0.081]+[0.148])	Prec@1 96.875 (95.212)
Epoch: [163][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.3304 (0.2894) ([0.182]+[0.148])	Prec@1 93.750 (95.103)
Epoch: [163][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2993 (0.2906) ([0.151]+[0.148])	Prec@1 93.750 (95.069)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4822 (0.4822) ([0.334]+[0.149])	Prec@1 88.281 (88.281)
 * Prec@1 88.480
current lr 1.00000e-02
Grad=  tensor(3.7256, device='cuda:0')
Epoch: [164][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.2135 (0.2135) ([0.065]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [164][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2962 (0.2823) ([0.148]+[0.149])	Prec@1 95.312 (95.173)
Epoch: [164][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.3026 (0.2921) ([0.154]+[0.149])	Prec@1 94.531 (94.827)
Epoch: [164][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2470 (0.2953) ([0.098]+[0.149])	Prec@1 96.875 (94.747)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4936 (0.4936) ([0.345]+[0.149])	Prec@1 90.625 (90.625)
 * Prec@1 89.200
current lr 1.00000e-02
Grad=  tensor(9.7013, device='cuda:0')
Epoch: [165][0/391]	Time 0.352 (0.352)	Data 0.222 (0.222)	Loss 0.3276 (0.3276) ([0.179]+[0.149])	Prec@1 94.531 (94.531)
Epoch: [165][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2525 (0.2833) ([0.104]+[0.149])	Prec@1 96.875 (95.382)
Epoch: [165][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3317 (0.2856) ([0.183]+[0.149])	Prec@1 92.188 (95.266)
Epoch: [165][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3616 (0.2919) ([0.213]+[0.149])	Prec@1 92.969 (95.019)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4465 (0.4465) ([0.298]+[0.149])	Prec@1 89.844 (89.844)
 * Prec@1 90.660
current lr 1.00000e-02
Grad=  tensor(5.1270, device='cuda:0')
Epoch: [166][0/391]	Time 0.344 (0.344)	Data 0.223 (0.223)	Loss 0.2668 (0.2668) ([0.118]+[0.149])	Prec@1 95.312 (95.312)
Epoch: [166][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2472 (0.2881) ([0.098]+[0.149])	Prec@1 98.438 (95.282)
Epoch: [166][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2924 (0.2861) ([0.144]+[0.149])	Prec@1 94.531 (95.278)
Epoch: [166][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.4112 (0.2919) ([0.262]+[0.149])	Prec@1 92.188 (95.092)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3834 (0.3834) ([0.234]+[0.149])	Prec@1 92.188 (92.188)
 * Prec@1 89.930
current lr 1.00000e-02
Grad=  tensor(7.8626, device='cuda:0')
Epoch: [167][0/391]	Time 0.314 (0.314)	Data 0.190 (0.190)	Loss 0.2929 (0.2929) ([0.144]+[0.149])	Prec@1 94.531 (94.531)
Epoch: [167][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2622 (0.2822) ([0.113]+[0.149])	Prec@1 96.094 (95.266)
Epoch: [167][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2490 (0.2834) ([0.100]+[0.149])	Prec@1 97.656 (95.371)
Epoch: [167][300/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.3430 (0.2882) ([0.194]+[0.149])	Prec@1 93.750 (95.224)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3922 (0.3922) ([0.243]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(6.7171, device='cuda:0')
Epoch: [168][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.2869 (0.2869) ([0.138]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [168][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2531 (0.2729) ([0.104]+[0.149])	Prec@1 96.875 (95.769)
Epoch: [168][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2361 (0.2744) ([0.087]+[0.149])	Prec@1 96.875 (95.752)
Epoch: [168][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3416 (0.2834) ([0.193]+[0.149])	Prec@1 94.531 (95.479)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4532 (0.4532) ([0.304]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 87.730
current lr 1.00000e-02
Grad=  tensor(2.5207, device='cuda:0')
Epoch: [169][0/391]	Time 0.362 (0.362)	Data 0.236 (0.236)	Loss 0.1935 (0.1935) ([0.045]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [169][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2883 (0.2883) ([0.139]+[0.149])	Prec@1 95.312 (95.351)
Epoch: [169][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3380 (0.2856) ([0.189]+[0.149])	Prec@1 92.188 (95.386)
Epoch: [169][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2328 (0.2894) ([0.084]+[0.149])	Prec@1 98.438 (95.211)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4953 (0.4953) ([0.346]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(7.8408, device='cuda:0')
Epoch: [170][0/391]	Time 0.361 (0.361)	Data 0.236 (0.236)	Loss 0.2549 (0.2549) ([0.106]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [170][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2210 (0.2647) ([0.072]+[0.149])	Prec@1 97.656 (96.009)
Epoch: [170][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3615 (0.2790) ([0.212]+[0.149])	Prec@1 94.531 (95.511)
Epoch: [170][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3400 (0.2835) ([0.191]+[0.149])	Prec@1 92.188 (95.344)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4534 (0.4534) ([0.304]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 87.710
current lr 1.00000e-02
Grad=  tensor(9.1255, device='cuda:0')
Epoch: [171][0/391]	Time 0.349 (0.349)	Data 0.219 (0.219)	Loss 0.2865 (0.2865) ([0.137]+[0.149])	Prec@1 93.750 (93.750)
Epoch: [171][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2853 (0.2864) ([0.136]+[0.149])	Prec@1 95.312 (95.328)
Epoch: [171][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3003 (0.2882) ([0.151]+[0.149])	Prec@1 95.312 (95.250)
Epoch: [171][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2603 (0.2873) ([0.111]+[0.149])	Prec@1 96.875 (95.235)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.4535 (0.4535) ([0.304]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 88.320
current lr 1.00000e-02
Grad=  tensor(3.9033, device='cuda:0')
Epoch: [172][0/391]	Time 0.348 (0.348)	Data 0.219 (0.219)	Loss 0.2269 (0.2269) ([0.078]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [172][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2527 (0.2716) ([0.104]+[0.149])	Prec@1 95.312 (95.900)
Epoch: [172][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4661 (0.2806) ([0.317]+[0.149])	Prec@1 89.844 (95.542)
Epoch: [172][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2593 (0.2835) ([0.110]+[0.149])	Prec@1 96.875 (95.383)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4987 (0.4987) ([0.349]+[0.149])	Prec@1 88.281 (88.281)
 * Prec@1 89.180
current lr 1.00000e-02
Grad=  tensor(4.0167, device='cuda:0')
Epoch: [173][0/391]	Time 0.351 (0.351)	Data 0.219 (0.219)	Loss 0.2342 (0.2342) ([0.085]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [173][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2417 (0.2752) ([0.092]+[0.149])	Prec@1 96.094 (95.583)
Epoch: [173][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3249 (0.2864) ([0.175]+[0.149])	Prec@1 93.750 (95.122)
Epoch: [173][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2040 (0.2872) ([0.055]+[0.149])	Prec@1 99.219 (95.159)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4495 (0.4495) ([0.300]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(5.0473, device='cuda:0')
Epoch: [174][0/391]	Time 0.344 (0.344)	Data 0.224 (0.224)	Loss 0.2384 (0.2384) ([0.089]+[0.150])	Prec@1 96.875 (96.875)
Epoch: [174][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2788 (0.2841) ([0.129]+[0.149])	Prec@1 95.312 (95.382)
Epoch: [174][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3214 (0.2797) ([0.172]+[0.149])	Prec@1 94.531 (95.519)
Epoch: [174][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3601 (0.2819) ([0.211]+[0.149])	Prec@1 93.750 (95.333)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4066 (0.4066) ([0.257]+[0.149])	Prec@1 90.625 (90.625)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(9.5065, device='cuda:0')
Epoch: [175][0/391]	Time 0.358 (0.358)	Data 0.234 (0.234)	Loss 0.3445 (0.3445) ([0.195]+[0.149])	Prec@1 94.531 (94.531)
Epoch: [175][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2648 (0.2677) ([0.116]+[0.149])	Prec@1 95.312 (95.808)
Epoch: [175][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2561 (0.2784) ([0.107]+[0.149])	Prec@1 95.312 (95.429)
Epoch: [175][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3642 (0.2834) ([0.215]+[0.149])	Prec@1 94.531 (95.292)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4146 (0.4146) ([0.265]+[0.150])	Prec@1 91.406 (91.406)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(10.8954, device='cuda:0')
Epoch: [176][0/391]	Time 0.344 (0.344)	Data 0.223 (0.223)	Loss 0.2793 (0.2793) ([0.130]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [176][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2984 (0.2885) ([0.149]+[0.150])	Prec@1 94.531 (95.374)
Epoch: [176][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2655 (0.2881) ([0.116]+[0.150])	Prec@1 95.312 (95.250)
Epoch: [176][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2421 (0.2914) ([0.092]+[0.150])	Prec@1 98.438 (95.102)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4535 (0.4535) ([0.304]+[0.150])	Prec@1 89.062 (89.062)
 * Prec@1 89.870
current lr 1.00000e-02
Grad=  tensor(8.4068, device='cuda:0')
Epoch: [177][0/391]	Time 0.292 (0.292)	Data 0.173 (0.173)	Loss 0.3155 (0.3155) ([0.166]+[0.150])	Prec@1 92.969 (92.969)
Epoch: [177][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3620 (0.2846) ([0.212]+[0.150])	Prec@1 93.750 (95.289)
Epoch: [177][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3088 (0.2842) ([0.159]+[0.150])	Prec@1 94.531 (95.312)
Epoch: [177][300/391]	Time 0.114 (0.109)	Data 0.000 (0.001)	Loss 0.2327 (0.2855) ([0.083]+[0.150])	Prec@1 97.656 (95.274)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.3753 (0.3753) ([0.225]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 88.940
current lr 1.00000e-02
Grad=  tensor(6.3327, device='cuda:0')
Epoch: [178][0/391]	Time 0.352 (0.352)	Data 0.233 (0.233)	Loss 0.2539 (0.2539) ([0.104]+[0.150])	Prec@1 96.875 (96.875)
Epoch: [178][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3290 (0.2721) ([0.179]+[0.150])	Prec@1 94.531 (95.761)
Epoch: [178][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2468 (0.2758) ([0.097]+[0.150])	Prec@1 96.875 (95.651)
Epoch: [178][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2939 (0.2826) ([0.144]+[0.150])	Prec@1 96.875 (95.411)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.5391 (0.5391) ([0.389]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 88.840
current lr 1.00000e-02
Grad=  tensor(7.9565, device='cuda:0')
Epoch: [179][0/391]	Time 0.305 (0.305)	Data 0.187 (0.187)	Loss 0.2453 (0.2453) ([0.095]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [179][100/391]	Time 0.109 (0.110)	Data 0.000 (0.002)	Loss 0.3375 (0.2771) ([0.187]+[0.150])	Prec@1 93.750 (95.684)
Epoch: [179][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2376 (0.2841) ([0.087]+[0.150])	Prec@1 96.875 (95.417)
Epoch: [179][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3274 (0.2852) ([0.177]+[0.150])	Prec@1 94.531 (95.357)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.5874 (0.5874) ([0.437]+[0.150])	Prec@1 87.500 (87.500)
 * Prec@1 89.300
current lr 1.00000e-02
Grad=  tensor(8.4347, device='cuda:0')
Epoch: [180][0/391]	Time 0.291 (0.291)	Data 0.172 (0.172)	Loss 0.3122 (0.3122) ([0.162]+[0.150])	Prec@1 95.312 (95.312)
Epoch: [180][100/391]	Time 0.110 (0.110)	Data 0.000 (0.002)	Loss 0.3053 (0.2762) ([0.155]+[0.150])	Prec@1 92.969 (95.560)
Epoch: [180][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2983 (0.2809) ([0.148]+[0.150])	Prec@1 96.094 (95.515)
Epoch: [180][300/391]	Time 0.108 (0.108)	Data 0.000 (0.001)	Loss 0.2333 (0.2819) ([0.083]+[0.150])	Prec@1 97.656 (95.484)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.4290 (0.4290) ([0.279]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 88.000
current lr 1.00000e-02
Grad=  tensor(9.2419, device='cuda:0')
Epoch: [181][0/391]	Time 0.287 (0.287)	Data 0.169 (0.169)	Loss 0.3126 (0.3126) ([0.162]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [181][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2992 (0.2667) ([0.149]+[0.150])	Prec@1 93.750 (96.094)
Epoch: [181][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2821 (0.2695) ([0.132]+[0.150])	Prec@1 95.312 (95.892)
Epoch: [181][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2195 (0.2771) ([0.069]+[0.150])	Prec@1 96.875 (95.580)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.5445 (0.5445) ([0.394]+[0.150])	Prec@1 89.844 (89.844)
 * Prec@1 88.920
current lr 1.00000e-02
Grad=  tensor(9.9078, device='cuda:0')
Epoch: [182][0/391]	Time 0.286 (0.286)	Data 0.167 (0.167)	Loss 0.3137 (0.3137) ([0.163]+[0.150])	Prec@1 93.750 (93.750)
Epoch: [182][100/391]	Time 0.114 (0.114)	Data 0.000 (0.002)	Loss 0.2362 (0.2737) ([0.086]+[0.150])	Prec@1 98.438 (95.877)
Epoch: [182][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2733 (0.2790) ([0.123]+[0.150])	Prec@1 96.875 (95.596)
Epoch: [182][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.4102 (0.2822) ([0.260]+[0.150])	Prec@1 92.188 (95.390)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.3814 (0.3814) ([0.231]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 90.140
current lr 1.00000e-02
Grad=  tensor(10.6161, device='cuda:0')
Epoch: [183][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.3825 (0.3825) ([0.232]+[0.150])	Prec@1 92.969 (92.969)
Epoch: [183][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2917 (0.2654) ([0.142]+[0.150])	Prec@1 93.750 (96.024)
Epoch: [183][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3033 (0.2719) ([0.153]+[0.150])	Prec@1 92.969 (95.728)
Epoch: [183][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2340 (0.2808) ([0.084]+[0.150])	Prec@1 99.219 (95.416)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4201 (0.4201) ([0.270]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 89.220
current lr 1.00000e-02
Grad=  tensor(6.4835, device='cuda:0')
Epoch: [184][0/391]	Time 0.344 (0.344)	Data 0.222 (0.222)	Loss 0.2749 (0.2749) ([0.124]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [184][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2806 (0.2710) ([0.130]+[0.150])	Prec@1 95.312 (96.016)
Epoch: [184][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3091 (0.2761) ([0.159]+[0.150])	Prec@1 95.312 (95.736)
Epoch: [184][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3734 (0.2795) ([0.223]+[0.150])	Prec@1 92.188 (95.658)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.4193 (0.4193) ([0.269]+[0.151])	Prec@1 90.625 (90.625)
 * Prec@1 89.140
current lr 1.00000e-02
Grad=  tensor(7.2716, device='cuda:0')
Epoch: [185][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.2313 (0.2313) ([0.081]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [185][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3028 (0.2755) ([0.152]+[0.150])	Prec@1 93.750 (95.838)
Epoch: [185][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2999 (0.2771) ([0.149]+[0.150])	Prec@1 93.750 (95.728)
Epoch: [185][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2655 (0.2806) ([0.115]+[0.150])	Prec@1 96.094 (95.569)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3762 (0.3762) ([0.226]+[0.150])	Prec@1 91.406 (91.406)
 * Prec@1 89.130
current lr 1.00000e-02
Grad=  tensor(8.6012, device='cuda:0')
Epoch: [186][0/391]	Time 0.342 (0.342)	Data 0.219 (0.219)	Loss 0.3591 (0.3591) ([0.209]+[0.150])	Prec@1 95.312 (95.312)
Epoch: [186][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2484 (0.2696) ([0.098]+[0.150])	Prec@1 96.094 (95.823)
Epoch: [186][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2556 (0.2683) ([0.105]+[0.150])	Prec@1 97.656 (95.888)
Epoch: [186][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2259 (0.2748) ([0.076]+[0.150])	Prec@1 97.656 (95.678)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.3995 (0.3995) ([0.249]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 89.600
current lr 1.00000e-02
Grad=  tensor(9.2606, device='cuda:0')
Epoch: [187][0/391]	Time 0.352 (0.352)	Data 0.229 (0.229)	Loss 0.2717 (0.2717) ([0.121]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [187][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2577 (0.2893) ([0.107]+[0.151])	Prec@1 95.312 (95.104)
Epoch: [187][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3124 (0.2874) ([0.162]+[0.151])	Prec@1 93.750 (95.274)
Epoch: [187][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2511 (0.2892) ([0.100]+[0.151])	Prec@1 96.094 (95.141)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4272 (0.4272) ([0.276]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(7.1696, device='cuda:0')
Epoch: [188][0/391]	Time 0.343 (0.343)	Data 0.217 (0.217)	Loss 0.3052 (0.3052) ([0.155]+[0.151])	Prec@1 95.312 (95.312)
Epoch: [188][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2722 (0.2710) ([0.122]+[0.151])	Prec@1 95.312 (95.722)
Epoch: [188][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3255 (0.2736) ([0.175]+[0.151])	Prec@1 92.969 (95.616)
Epoch: [188][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2652 (0.2766) ([0.115]+[0.151])	Prec@1 96.094 (95.518)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.5348 (0.5348) ([0.384]+[0.151])	Prec@1 88.281 (88.281)
 * Prec@1 89.360
current lr 1.00000e-02
Grad=  tensor(7.5920, device='cuda:0')
Epoch: [189][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.3125 (0.3125) ([0.162]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [189][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2388 (0.2817) ([0.088]+[0.151])	Prec@1 97.656 (95.560)
Epoch: [189][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2380 (0.2810) ([0.087]+[0.151])	Prec@1 96.094 (95.484)
Epoch: [189][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2565 (0.2820) ([0.106]+[0.151])	Prec@1 96.875 (95.492)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.5120 (0.5120) ([0.361]+[0.151])	Prec@1 89.062 (89.062)
 * Prec@1 89.300
current lr 1.00000e-02
Grad=  tensor(7.2201, device='cuda:0')
Epoch: [190][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.2581 (0.2581) ([0.107]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [190][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3268 (0.2715) ([0.176]+[0.151])	Prec@1 94.531 (95.924)
Epoch: [190][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2796 (0.2731) ([0.129]+[0.151])	Prec@1 96.094 (95.861)
Epoch: [190][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3280 (0.2798) ([0.177]+[0.151])	Prec@1 92.188 (95.627)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3569 (0.3569) ([0.206]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 89.190
current lr 1.00000e-02
Grad=  tensor(10.5222, device='cuda:0')
Epoch: [191][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.2934 (0.2934) ([0.143]+[0.151])	Prec@1 93.750 (93.750)
Epoch: [191][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2536 (0.2875) ([0.103]+[0.151])	Prec@1 96.875 (95.320)
Epoch: [191][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2842 (0.2860) ([0.133]+[0.151])	Prec@1 96.875 (95.484)
Epoch: [191][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3214 (0.2867) ([0.171]+[0.151])	Prec@1 94.531 (95.422)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.5479 (0.5479) ([0.397]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 90.530
current lr 1.00000e-02
Grad=  tensor(2.4450, device='cuda:0')
Epoch: [192][0/391]	Time 0.341 (0.341)	Data 0.220 (0.220)	Loss 0.1984 (0.1984) ([0.048]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [192][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2307 (0.2714) ([0.080]+[0.151])	Prec@1 97.656 (95.846)
Epoch: [192][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.3227 (0.2786) ([0.172]+[0.151])	Prec@1 91.406 (95.561)
Epoch: [192][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3236 (0.2813) ([0.173]+[0.151])	Prec@1 93.750 (95.460)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.3801 (0.3801) ([0.229]+[0.151])	Prec@1 92.969 (92.969)
 * Prec@1 89.420
current lr 1.00000e-02
Grad=  tensor(2.8168, device='cuda:0')
Epoch: [193][0/391]	Time 0.350 (0.350)	Data 0.227 (0.227)	Loss 0.2245 (0.2245) ([0.074]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [193][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2438 (0.2784) ([0.093]+[0.151])	Prec@1 96.875 (95.537)
Epoch: [193][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2828 (0.2803) ([0.132]+[0.151])	Prec@1 96.094 (95.472)
Epoch: [193][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2574 (0.2817) ([0.106]+[0.151])	Prec@1 96.094 (95.440)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4182 (0.4182) ([0.267]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 89.320
current lr 1.00000e-02
Grad=  tensor(5.4398, device='cuda:0')
Epoch: [194][0/391]	Time 0.349 (0.349)	Data 0.226 (0.226)	Loss 0.2481 (0.2481) ([0.097]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [194][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2563 (0.2662) ([0.105]+[0.151])	Prec@1 95.312 (96.248)
Epoch: [194][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2727 (0.2782) ([0.122]+[0.151])	Prec@1 96.094 (95.701)
Epoch: [194][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2905 (0.2824) ([0.140]+[0.151])	Prec@1 96.875 (95.497)
Test: [0/79]	Time 0.269 (0.269)	Loss 0.4225 (0.4225) ([0.271]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 89.870
current lr 1.00000e-02
Grad=  tensor(4.5038, device='cuda:0')
Epoch: [195][0/391]	Time 0.353 (0.353)	Data 0.227 (0.227)	Loss 0.2291 (0.2291) ([0.078]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [195][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2349 (0.2672) ([0.084]+[0.151])	Prec@1 97.656 (96.194)
Epoch: [195][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3493 (0.2732) ([0.198]+[0.151])	Prec@1 92.188 (95.911)
Epoch: [195][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3211 (0.2778) ([0.170]+[0.151])	Prec@1 93.750 (95.712)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4784 (0.4784) ([0.327]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 88.490
current lr 1.00000e-02
Grad=  tensor(9.7174, device='cuda:0')
Epoch: [196][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.3395 (0.3395) ([0.188]+[0.151])	Prec@1 95.312 (95.312)
Epoch: [196][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2321 (0.2695) ([0.081]+[0.151])	Prec@1 96.094 (96.001)
Epoch: [196][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3233 (0.2752) ([0.172]+[0.151])	Prec@1 95.312 (95.763)
Epoch: [196][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2705 (0.2815) ([0.119]+[0.151])	Prec@1 96.094 (95.538)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4742 (0.4742) ([0.323]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 89.190
current lr 1.00000e-02
Grad=  tensor(4.2652, device='cuda:0')
Epoch: [197][0/391]	Time 0.351 (0.351)	Data 0.226 (0.226)	Loss 0.2312 (0.2312) ([0.080]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [197][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3134 (0.2779) ([0.162]+[0.151])	Prec@1 94.531 (95.730)
Epoch: [197][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3495 (0.2783) ([0.198]+[0.151])	Prec@1 94.531 (95.643)
Epoch: [197][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3869 (0.2816) ([0.236]+[0.151])	Prec@1 93.750 (95.528)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3584 (0.3584) ([0.207]+[0.151])	Prec@1 92.969 (92.969)
 * Prec@1 89.580
current lr 1.00000e-02
Grad=  tensor(11.1030, device='cuda:0')
Epoch: [198][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.3015 (0.3015) ([0.150]+[0.151])	Prec@1 94.531 (94.531)
Epoch: [198][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2535 (0.2718) ([0.102]+[0.151])	Prec@1 96.094 (95.730)
Epoch: [198][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3841 (0.2746) ([0.233]+[0.151])	Prec@1 91.406 (95.616)
Epoch: [198][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2321 (0.2803) ([0.081]+[0.152])	Prec@1 96.094 (95.445)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4956 (0.4956) ([0.344]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(9.3023, device='cuda:0')
Epoch: [199][0/391]	Time 0.357 (0.357)	Data 0.232 (0.232)	Loss 0.2385 (0.2385) ([0.087]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [199][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3417 (0.2845) ([0.190]+[0.152])	Prec@1 94.531 (95.336)
Epoch: [199][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2602 (0.2827) ([0.108]+[0.152])	Prec@1 96.875 (95.491)
Epoch: [199][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3119 (0.2835) ([0.160]+[0.152])	Prec@1 94.531 (95.466)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3471 (0.3471) ([0.195]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 89.150
current lr 1.00000e-02
Grad=  tensor(10.1267, device='cuda:0')
Epoch: [200][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.3214 (0.3214) ([0.170]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [200][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2571 (0.2723) ([0.105]+[0.152])	Prec@1 96.094 (95.730)
Epoch: [200][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2740 (0.2716) ([0.122]+[0.152])	Prec@1 94.531 (95.732)
Epoch: [200][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2772 (0.2791) ([0.126]+[0.152])	Prec@1 95.312 (95.502)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3883 (0.3883) ([0.237]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(8.7348, device='cuda:0')
Epoch: [201][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.2657 (0.2657) ([0.114]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [201][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2363 (0.2726) ([0.085]+[0.152])	Prec@1 96.875 (95.877)
Epoch: [201][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.3330 (0.2753) ([0.181]+[0.152])	Prec@1 90.625 (95.655)
Epoch: [201][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3262 (0.2803) ([0.174]+[0.152])	Prec@1 95.312 (95.499)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4810 (0.4810) ([0.329]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 88.840
current lr 1.00000e-02
Grad=  tensor(11.1759, device='cuda:0')
Epoch: [202][0/391]	Time 0.342 (0.342)	Data 0.220 (0.220)	Loss 0.3168 (0.3168) ([0.165]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [202][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2764 (0.2797) ([0.125]+[0.152])	Prec@1 93.750 (95.413)
Epoch: [202][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2953 (0.2788) ([0.143]+[0.152])	Prec@1 96.094 (95.499)
Epoch: [202][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2775 (0.2786) ([0.126]+[0.152])	Prec@1 94.531 (95.567)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4404 (0.4404) ([0.288]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.760
current lr 1.00000e-02
Grad=  tensor(7.7300, device='cuda:0')
Epoch: [203][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.2901 (0.2901) ([0.138]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [203][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2603 (0.2688) ([0.108]+[0.152])	Prec@1 96.875 (95.985)
Epoch: [203][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2783 (0.2693) ([0.127]+[0.152])	Prec@1 96.094 (95.931)
Epoch: [203][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3769 (0.2756) ([0.225]+[0.152])	Prec@1 92.969 (95.808)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4031 (0.4031) ([0.251]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 89.310
current lr 1.00000e-02
Grad=  tensor(7.0724, device='cuda:0')
Epoch: [204][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.2498 (0.2498) ([0.098]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [204][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2591 (0.2618) ([0.107]+[0.152])	Prec@1 96.094 (96.202)
Epoch: [204][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3127 (0.2664) ([0.161]+[0.152])	Prec@1 92.969 (95.989)
Epoch: [204][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2826 (0.2753) ([0.131]+[0.152])	Prec@1 96.875 (95.637)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3462 (0.3462) ([0.194]+[0.152])	Prec@1 95.312 (95.312)
 * Prec@1 89.930
current lr 1.00000e-02
Grad=  tensor(1.4665, device='cuda:0')
Epoch: [205][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.1941 (0.1941) ([0.042]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [205][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2565 (0.2539) ([0.105]+[0.152])	Prec@1 96.094 (96.597)
Epoch: [205][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.3054 (0.2708) ([0.154]+[0.152])	Prec@1 94.531 (95.915)
Epoch: [205][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2433 (0.2732) ([0.091]+[0.152])	Prec@1 95.312 (95.868)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4681 (0.4681) ([0.316]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 89.320
current lr 1.00000e-02
Grad=  tensor(8.8399, device='cuda:0')
Epoch: [206][0/391]	Time 0.353 (0.353)	Data 0.230 (0.230)	Loss 0.2629 (0.2629) ([0.111]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [206][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2857 (0.2668) ([0.134]+[0.152])	Prec@1 96.094 (95.854)
Epoch: [206][200/391]	Time 0.117 (0.114)	Data 0.000 (0.001)	Loss 0.2436 (0.2717) ([0.092]+[0.152])	Prec@1 97.656 (95.763)
Epoch: [206][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2748 (0.2741) ([0.123]+[0.152])	Prec@1 96.875 (95.728)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4323 (0.4323) ([0.280]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(12.5221, device='cuda:0')
Epoch: [207][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.2895 (0.2895) ([0.138]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [207][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2686 (0.2667) ([0.117]+[0.152])	Prec@1 97.656 (95.877)
Epoch: [207][200/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.3161 (0.2738) ([0.164]+[0.152])	Prec@1 94.531 (95.608)
Epoch: [207][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3415 (0.2791) ([0.190]+[0.152])	Prec@1 92.188 (95.497)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3341 (0.3341) ([0.182]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 89.730
current lr 1.00000e-02
Grad=  tensor(9.4300, device='cuda:0')
Epoch: [208][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.2994 (0.2994) ([0.147]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [208][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.2881 (0.2611) ([0.136]+[0.152])	Prec@1 93.750 (96.287)
Epoch: [208][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2496 (0.2703) ([0.098]+[0.152])	Prec@1 96.094 (95.946)
Epoch: [208][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3005 (0.2732) ([0.149]+[0.152])	Prec@1 95.312 (95.889)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4434 (0.4434) ([0.291]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 89.170
current lr 1.00000e-02
Grad=  tensor(15.2988, device='cuda:0')
Epoch: [209][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.2936 (0.2936) ([0.142]+[0.152])	Prec@1 93.750 (93.750)
Epoch: [209][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2036 (0.2586) ([0.052]+[0.152])	Prec@1 98.438 (96.380)
Epoch: [209][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2329 (0.2714) ([0.081]+[0.152])	Prec@1 96.094 (95.826)
Epoch: [209][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3082 (0.2756) ([0.156]+[0.152])	Prec@1 95.312 (95.751)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4519 (0.4519) ([0.300]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 88.200
current lr 1.00000e-02
Grad=  tensor(9.8740, device='cuda:0')
Epoch: [210][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.2790 (0.2790) ([0.127]+[0.152])	Prec@1 93.750 (93.750)
Epoch: [210][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2934 (0.2774) ([0.141]+[0.152])	Prec@1 95.312 (95.545)
Epoch: [210][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2907 (0.2790) ([0.138]+[0.152])	Prec@1 94.531 (95.530)
Epoch: [210][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2668 (0.2820) ([0.114]+[0.152])	Prec@1 96.094 (95.411)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3562 (0.3562) ([0.204]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 89.960
current lr 1.00000e-02
Grad=  tensor(9.7065, device='cuda:0')
Epoch: [211][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.3035 (0.3035) ([0.151]+[0.152])	Prec@1 93.750 (93.750)
Epoch: [211][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2065 (0.2645) ([0.054]+[0.152])	Prec@1 97.656 (96.055)
Epoch: [211][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2462 (0.2719) ([0.094]+[0.152])	Prec@1 96.875 (95.814)
Epoch: [211][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2509 (0.2761) ([0.099]+[0.152])	Prec@1 96.094 (95.653)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.3540 (0.3540) ([0.202]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 89.360
current lr 1.00000e-02
Grad=  tensor(17.1929, device='cuda:0')
Epoch: [212][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.4384 (0.4384) ([0.286]+[0.152])	Prec@1 89.844 (89.844)
Epoch: [212][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2153 (0.2689) ([0.063]+[0.152])	Prec@1 97.656 (96.109)
Epoch: [212][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3340 (0.2671) ([0.182]+[0.152])	Prec@1 92.969 (96.121)
Epoch: [212][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3568 (0.2726) ([0.205]+[0.152])	Prec@1 92.188 (95.865)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.3938 (0.3938) ([0.242]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 89.720
current lr 1.00000e-02
Grad=  tensor(7.2107, device='cuda:0')
Epoch: [213][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.2407 (0.2407) ([0.089]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [213][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.3508 (0.2654) ([0.199]+[0.152])	Prec@1 93.750 (96.171)
Epoch: [213][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3528 (0.2677) ([0.201]+[0.152])	Prec@1 91.406 (96.063)
Epoch: [213][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2353 (0.2727) ([0.083]+[0.152])	Prec@1 97.656 (95.917)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.5404 (0.5404) ([0.388]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 88.970
current lr 1.00000e-02
Grad=  tensor(9.9573, device='cuda:0')
Epoch: [214][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.2485 (0.2485) ([0.096]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [214][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2103 (0.2766) ([0.058]+[0.152])	Prec@1 97.656 (95.753)
Epoch: [214][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2465 (0.2796) ([0.094]+[0.152])	Prec@1 98.438 (95.616)
Epoch: [214][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2554 (0.2784) ([0.103]+[0.152])	Prec@1 96.875 (95.660)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4796 (0.4796) ([0.327]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(2.6110, device='cuda:0')
Epoch: [215][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.1967 (0.1967) ([0.044]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [215][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3214 (0.2650) ([0.169]+[0.152])	Prec@1 93.750 (96.040)
Epoch: [215][200/391]	Time 0.116 (0.114)	Data 0.000 (0.001)	Loss 0.2184 (0.2638) ([0.066]+[0.152])	Prec@1 99.219 (96.226)
Epoch: [215][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2953 (0.2666) ([0.143]+[0.152])	Prec@1 95.312 (96.065)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4012 (0.4012) ([0.249]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 89.440
current lr 1.00000e-02
Grad=  tensor(7.5104, device='cuda:0')
Epoch: [216][0/391]	Time 0.342 (0.342)	Data 0.219 (0.219)	Loss 0.2907 (0.2907) ([0.138]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [216][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.3004 (0.2768) ([0.148]+[0.152])	Prec@1 96.094 (95.761)
Epoch: [216][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2692 (0.2695) ([0.117]+[0.152])	Prec@1 96.875 (96.020)
Epoch: [216][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3176 (0.2740) ([0.165]+[0.152])	Prec@1 96.094 (95.821)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4182 (0.4182) ([0.266]+[0.152])	Prec@1 90.625 (90.625)
 * Prec@1 89.230
current lr 1.00000e-02
Grad=  tensor(9.7662, device='cuda:0')
Epoch: [217][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.2576 (0.2576) ([0.105]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [217][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2424 (0.2711) ([0.090]+[0.152])	Prec@1 96.094 (95.978)
Epoch: [217][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2852 (0.2701) ([0.133]+[0.152])	Prec@1 96.875 (95.973)
Epoch: [217][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2904 (0.2742) ([0.138]+[0.152])	Prec@1 96.875 (95.780)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3792 (0.3792) ([0.227]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(11.1888, device='cuda:0')
Epoch: [218][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.2540 (0.2540) ([0.102]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [218][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2952 (0.2691) ([0.143]+[0.152])	Prec@1 94.531 (95.939)
Epoch: [218][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2930 (0.2702) ([0.141]+[0.152])	Prec@1 93.750 (95.915)
Epoch: [218][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2707 (0.2718) ([0.119]+[0.152])	Prec@1 96.094 (95.915)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3643 (0.3643) ([0.212]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(10.5672, device='cuda:0')
Epoch: [219][0/391]	Time 0.339 (0.339)	Data 0.216 (0.216)	Loss 0.2667 (0.2667) ([0.115]+[0.152])	Prec@1 94.531 (94.531)
Epoch: [219][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1992 (0.2681) ([0.047]+[0.152])	Prec@1 99.219 (96.140)
Epoch: [219][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2788 (0.2714) ([0.127]+[0.152])	Prec@1 95.312 (95.977)
Epoch: [219][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2716 (0.2749) ([0.119]+[0.152])	Prec@1 96.094 (95.852)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.4134 (0.4134) ([0.261]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 90.000
current lr 1.00000e-02
Grad=  tensor(7.5562, device='cuda:0')
Epoch: [220][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.2523 (0.2523) ([0.100]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [220][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3064 (0.2640) ([0.154]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [220][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2790 (0.2699) ([0.127]+[0.152])	Prec@1 95.312 (95.969)
Epoch: [220][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3274 (0.2786) ([0.175]+[0.152])	Prec@1 93.750 (95.671)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4341 (0.4341) ([0.282]+[0.152])	Prec@1 89.844 (89.844)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(9.0555, device='cuda:0')
Epoch: [221][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.3054 (0.3054) ([0.153]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [221][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2473 (0.2631) ([0.095]+[0.152])	Prec@1 95.312 (96.117)
Epoch: [221][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3167 (0.2685) ([0.164]+[0.152])	Prec@1 95.312 (95.899)
Epoch: [221][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.2467 (0.2714) ([0.094]+[0.152])	Prec@1 96.875 (95.847)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4098 (0.4098) ([0.257]+[0.152])	Prec@1 90.625 (90.625)
 * Prec@1 89.930
current lr 1.00000e-02
Grad=  tensor(8.7155, device='cuda:0')
Epoch: [222][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.2552 (0.2552) ([0.103]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [222][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2785 (0.2715) ([0.126]+[0.152])	Prec@1 97.656 (96.009)
Epoch: [222][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2595 (0.2753) ([0.107]+[0.152])	Prec@1 95.312 (95.849)
Epoch: [222][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2749 (0.2777) ([0.122]+[0.152])	Prec@1 96.094 (95.668)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.4906 (0.4906) ([0.338]+[0.153])	Prec@1 88.281 (88.281)
 * Prec@1 88.570
current lr 1.00000e-02
Grad=  tensor(14.6912, device='cuda:0')
Epoch: [223][0/391]	Time 0.342 (0.342)	Data 0.218 (0.218)	Loss 0.3256 (0.3256) ([0.173]+[0.153])	Prec@1 95.312 (95.312)
Epoch: [223][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3563 (0.2705) ([0.204]+[0.152])	Prec@1 92.188 (95.931)
Epoch: [223][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.2862 (0.2698) ([0.134]+[0.152])	Prec@1 94.531 (95.965)
Epoch: [223][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2805 (0.2718) ([0.128]+[0.152])	Prec@1 94.531 (95.852)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.5995 (0.5995) ([0.447]+[0.153])	Prec@1 87.500 (87.500)
 * Prec@1 88.680
current lr 1.00000e-02
Grad=  tensor(8.1718, device='cuda:0')
Epoch: [224][0/391]	Time 0.338 (0.338)	Data 0.214 (0.214)	Loss 0.2353 (0.2353) ([0.083]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [224][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2544 (0.2675) ([0.102]+[0.153])	Prec@1 96.875 (96.202)
Epoch: [224][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2986 (0.2680) ([0.146]+[0.152])	Prec@1 92.969 (96.086)
Epoch: [224][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2379 (0.2727) ([0.085]+[0.153])	Prec@1 96.094 (95.860)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4357 (0.4357) ([0.283]+[0.153])	Prec@1 88.281 (88.281)
 * Prec@1 88.760
current lr 1.00000e-02
Grad=  tensor(7.9044, device='cuda:0')
Epoch: [225][0/391]	Time 0.351 (0.351)	Data 0.225 (0.225)	Loss 0.3224 (0.3224) ([0.170]+[0.153])	Prec@1 94.531 (94.531)
Epoch: [225][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.3142 (0.2636) ([0.162]+[0.153])	Prec@1 94.531 (96.225)
Epoch: [225][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2501 (0.2722) ([0.097]+[0.153])	Prec@1 96.875 (95.931)
Epoch: [225][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2596 (0.2724) ([0.107]+[0.153])	Prec@1 98.438 (95.909)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.4796 (0.4796) ([0.327]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 89.360
current lr 1.00000e-02
Grad=  tensor(5.9506, device='cuda:0')
Epoch: [226][0/391]	Time 0.341 (0.341)	Data 0.218 (0.218)	Loss 0.2538 (0.2538) ([0.101]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [226][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2488 (0.2707) ([0.096]+[0.153])	Prec@1 96.094 (95.947)
Epoch: [226][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3130 (0.2797) ([0.160]+[0.153])	Prec@1 94.531 (95.620)
Epoch: [226][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2473 (0.2758) ([0.095]+[0.153])	Prec@1 96.875 (95.707)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4375 (0.4375) ([0.285]+[0.153])	Prec@1 90.625 (90.625)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(2.5153, device='cuda:0')
Epoch: [227][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.2030 (0.2030) ([0.050]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [227][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2872 (0.2718) ([0.135]+[0.153])	Prec@1 95.312 (96.055)
Epoch: [227][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3385 (0.2684) ([0.186]+[0.153])	Prec@1 92.969 (96.102)
Epoch: [227][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3945 (0.2692) ([0.242]+[0.153])	Prec@1 92.188 (96.050)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4235 (0.4235) ([0.271]+[0.153])	Prec@1 88.281 (88.281)
 * Prec@1 89.340
current lr 1.00000e-02
Grad=  tensor(9.2723, device='cuda:0')
Epoch: [228][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.2651 (0.2651) ([0.112]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [228][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3194 (0.2695) ([0.167]+[0.153])	Prec@1 93.750 (95.970)
Epoch: [228][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2823 (0.2730) ([0.130]+[0.153])	Prec@1 95.312 (95.853)
Epoch: [228][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2755 (0.2725) ([0.123]+[0.153])	Prec@1 96.094 (95.876)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3914 (0.3914) ([0.239]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(3.3821, device='cuda:0')
Epoch: [229][0/391]	Time 0.347 (0.347)	Data 0.221 (0.221)	Loss 0.2201 (0.2201) ([0.067]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [229][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.3198 (0.2620) ([0.167]+[0.152])	Prec@1 94.531 (96.279)
Epoch: [229][200/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.2679 (0.2631) ([0.115]+[0.152])	Prec@1 96.094 (96.230)
Epoch: [229][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2911 (0.2679) ([0.139]+[0.153])	Prec@1 92.969 (96.063)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.3858 (0.3858) ([0.233]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 89.330
current lr 1.00000e-02
Grad=  tensor(9.2648, device='cuda:0')
Epoch: [230][0/391]	Time 0.337 (0.337)	Data 0.213 (0.213)	Loss 0.3392 (0.3392) ([0.186]+[0.153])	Prec@1 92.969 (92.969)
Epoch: [230][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.3389 (0.2630) ([0.186]+[0.153])	Prec@1 91.406 (96.264)
Epoch: [230][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2766 (0.2635) ([0.124]+[0.152])	Prec@1 95.312 (96.245)
Epoch: [230][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2716 (0.2673) ([0.119]+[0.152])	Prec@1 93.750 (96.073)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3511 (0.3511) ([0.198]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 89.010
current lr 1.00000e-02
Grad=  tensor(6.9123, device='cuda:0')
Epoch: [231][0/391]	Time 0.345 (0.345)	Data 0.214 (0.214)	Loss 0.2631 (0.2631) ([0.110]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [231][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2872 (0.2764) ([0.135]+[0.153])	Prec@1 96.094 (95.800)
Epoch: [231][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2868 (0.2757) ([0.134]+[0.153])	Prec@1 96.875 (95.849)
Epoch: [231][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3042 (0.2773) ([0.152]+[0.153])	Prec@1 93.750 (95.775)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3705 (0.3705) ([0.218]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(10.0059, device='cuda:0')
Epoch: [232][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.3163 (0.3163) ([0.164]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [232][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2993 (0.2677) ([0.147]+[0.152])	Prec@1 95.312 (96.001)
Epoch: [232][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3547 (0.2677) ([0.202]+[0.152])	Prec@1 94.531 (96.043)
Epoch: [232][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2772 (0.2765) ([0.125]+[0.153])	Prec@1 96.875 (95.741)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.5397 (0.5397) ([0.387]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 88.470
current lr 1.00000e-02
Grad=  tensor(7.0703, device='cuda:0')
Epoch: [233][0/391]	Time 0.347 (0.347)	Data 0.221 (0.221)	Loss 0.2580 (0.2580) ([0.105]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [233][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2743 (0.2595) ([0.122]+[0.152])	Prec@1 96.875 (96.411)
Epoch: [233][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3472 (0.2654) ([0.195]+[0.152])	Prec@1 93.750 (96.206)
Epoch: [233][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.4064 (0.2698) ([0.254]+[0.152])	Prec@1 91.406 (96.013)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4304 (0.4304) ([0.278]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(7.4869, device='cuda:0')
Epoch: [234][0/391]	Time 0.339 (0.339)	Data 0.219 (0.219)	Loss 0.2619 (0.2619) ([0.109]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [234][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.3075 (0.2736) ([0.155]+[0.153])	Prec@1 96.094 (95.900)
Epoch: [234][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2633 (0.2767) ([0.111]+[0.153])	Prec@1 96.875 (95.740)
Epoch: [234][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2572 (0.2761) ([0.105]+[0.153])	Prec@1 96.875 (95.819)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4361 (0.4361) ([0.284]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 88.910
current lr 1.00000e-02
Grad=  tensor(14.4170, device='cuda:0')
Epoch: [235][0/391]	Time 0.303 (0.303)	Data 0.184 (0.184)	Loss 0.3699 (0.3699) ([0.217]+[0.153])	Prec@1 91.406 (91.406)
Epoch: [235][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3363 (0.2687) ([0.184]+[0.153])	Prec@1 95.312 (96.047)
Epoch: [235][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2871 (0.2689) ([0.135]+[0.153])	Prec@1 96.094 (96.024)
Epoch: [235][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3134 (0.2735) ([0.161]+[0.153])	Prec@1 95.312 (95.832)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.4805 (0.4805) ([0.328]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(9.0536, device='cuda:0')
Epoch: [236][0/391]	Time 0.289 (0.289)	Data 0.171 (0.171)	Loss 0.3120 (0.3120) ([0.159]+[0.153])	Prec@1 94.531 (94.531)
Epoch: [236][100/391]	Time 0.114 (0.114)	Data 0.000 (0.002)	Loss 0.2822 (0.2656) ([0.130]+[0.153])	Prec@1 95.312 (96.125)
Epoch: [236][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2115 (0.2655) ([0.059]+[0.153])	Prec@1 98.438 (96.078)
Epoch: [236][300/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2804 (0.2724) ([0.128]+[0.153])	Prec@1 96.094 (95.824)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.4612 (0.4612) ([0.308]+[0.153])	Prec@1 88.281 (88.281)
 * Prec@1 89.800
current lr 1.00000e-02
Grad=  tensor(5.5037, device='cuda:0')
Epoch: [237][0/391]	Time 0.301 (0.301)	Data 0.177 (0.177)	Loss 0.2423 (0.2423) ([0.089]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [237][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2336 (0.2610) ([0.081]+[0.153])	Prec@1 98.438 (96.225)
Epoch: [237][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3852 (0.2621) ([0.233]+[0.153])	Prec@1 93.750 (96.199)
Epoch: [237][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2739 (0.2652) ([0.121]+[0.153])	Prec@1 95.312 (96.112)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.4112 (0.4112) ([0.259]+[0.153])	Prec@1 90.625 (90.625)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(8.6598, device='cuda:0')
Epoch: [238][0/391]	Time 0.299 (0.299)	Data 0.175 (0.175)	Loss 0.2850 (0.2850) ([0.132]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [238][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2342 (0.2638) ([0.082]+[0.152])	Prec@1 97.656 (96.210)
Epoch: [238][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3161 (0.2676) ([0.164]+[0.152])	Prec@1 93.750 (96.016)
Epoch: [238][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2393 (0.2713) ([0.087]+[0.153])	Prec@1 96.094 (95.928)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4073 (0.4073) ([0.255]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(15.1974, device='cuda:0')
Epoch: [239][0/391]	Time 0.338 (0.338)	Data 0.216 (0.216)	Loss 0.2730 (0.2730) ([0.120]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [239][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.3007 (0.2664) ([0.148]+[0.153])	Prec@1 92.969 (96.055)
Epoch: [239][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3338 (0.2694) ([0.181]+[0.153])	Prec@1 92.969 (96.024)
Epoch: [239][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2356 (0.2731) ([0.083]+[0.153])	Prec@1 98.438 (95.907)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4724 (0.4724) ([0.320]+[0.153])	Prec@1 89.062 (89.062)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(7.0003, device='cuda:0')
Epoch: [240][0/391]	Time 0.340 (0.340)	Data 0.219 (0.219)	Loss 0.2471 (0.2471) ([0.094]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [240][100/391]	Time 0.108 (0.112)	Data 0.000 (0.002)	Loss 0.2337 (0.2688) ([0.081]+[0.153])	Prec@1 97.656 (96.148)
Epoch: [240][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3857 (0.2700) ([0.233]+[0.153])	Prec@1 95.312 (96.004)
Epoch: [240][300/391]	Time 0.114 (0.111)	Data 0.000 (0.001)	Loss 0.2712 (0.2726) ([0.118]+[0.153])	Prec@1 96.875 (95.930)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.3379 (0.3379) ([0.185]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 90.810
current lr 1.00000e-02
Grad=  tensor(6.0447, device='cuda:0')
Epoch: [241][0/391]	Time 0.348 (0.348)	Data 0.226 (0.226)	Loss 0.2268 (0.2268) ([0.074]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [241][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2270 (0.2543) ([0.074]+[0.153])	Prec@1 96.875 (96.643)
Epoch: [241][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1952 (0.2594) ([0.043]+[0.153])	Prec@1 100.000 (96.420)
Epoch: [241][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.3219 (0.2681) ([0.169]+[0.153])	Prec@1 93.750 (96.083)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4050 (0.4050) ([0.252]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 89.670
current lr 1.00000e-02
Grad=  tensor(4.4077, device='cuda:0')
Epoch: [242][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.2612 (0.2612) ([0.109]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [242][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2814 (0.2781) ([0.129]+[0.153])	Prec@1 96.875 (95.715)
Epoch: [242][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2462 (0.2732) ([0.093]+[0.153])	Prec@1 97.656 (95.954)
Epoch: [242][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3118 (0.2723) ([0.159]+[0.153])	Prec@1 94.531 (95.956)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3488 (0.3488) ([0.196]+[0.153])	Prec@1 95.312 (95.312)
 * Prec@1 89.890
current lr 1.00000e-02
Grad=  tensor(11.5784, device='cuda:0')
Epoch: [243][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.2522 (0.2522) ([0.099]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [243][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1973 (0.2586) ([0.045]+[0.153])	Prec@1 98.438 (96.388)
Epoch: [243][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2568 (0.2647) ([0.104]+[0.153])	Prec@1 96.875 (96.187)
Epoch: [243][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2797 (0.2661) ([0.127]+[0.153])	Prec@1 97.656 (96.109)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.5239 (0.5239) ([0.371]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(5.4900, device='cuda:0')
Epoch: [244][0/391]	Time 0.351 (0.351)	Data 0.225 (0.225)	Loss 0.2494 (0.2494) ([0.097]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [244][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2484 (0.2584) ([0.096]+[0.153])	Prec@1 96.094 (96.125)
Epoch: [244][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2215 (0.2693) ([0.069]+[0.153])	Prec@1 97.656 (95.876)
Epoch: [244][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2547 (0.2734) ([0.102]+[0.153])	Prec@1 96.094 (95.811)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3573 (0.3573) ([0.204]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(7.1956, device='cuda:0')
Epoch: [245][0/391]	Time 0.338 (0.338)	Data 0.214 (0.214)	Loss 0.2712 (0.2712) ([0.118]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [245][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2434 (0.2601) ([0.091]+[0.153])	Prec@1 96.094 (96.341)
Epoch: [245][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3209 (0.2667) ([0.168]+[0.153])	Prec@1 94.531 (96.016)
Epoch: [245][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2985 (0.2698) ([0.146]+[0.153])	Prec@1 94.531 (95.886)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4346 (0.4346) ([0.282]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 89.570
current lr 1.00000e-02
Grad=  tensor(4.7860, device='cuda:0')
Epoch: [246][0/391]	Time 0.339 (0.339)	Data 0.215 (0.215)	Loss 0.2265 (0.2265) ([0.074]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [246][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3336 (0.2585) ([0.181]+[0.153])	Prec@1 92.969 (96.527)
Epoch: [246][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3437 (0.2571) ([0.191]+[0.153])	Prec@1 92.969 (96.420)
Epoch: [246][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3470 (0.2669) ([0.194]+[0.153])	Prec@1 91.406 (96.052)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4606 (0.4606) ([0.308]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 89.660
current lr 1.00000e-02
Grad=  tensor(8.4546, device='cuda:0')
Epoch: [247][0/391]	Time 0.337 (0.337)	Data 0.213 (0.213)	Loss 0.2567 (0.2567) ([0.104]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [247][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2121 (0.2696) ([0.059]+[0.153])	Prec@1 98.438 (96.101)
Epoch: [247][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2997 (0.2726) ([0.147]+[0.153])	Prec@1 94.531 (95.965)
Epoch: [247][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2564 (0.2722) ([0.103]+[0.153])	Prec@1 96.875 (95.951)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4702 (0.4702) ([0.317]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 89.230
current lr 1.00000e-02
Grad=  tensor(11.5604, device='cuda:0')
Epoch: [248][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.3003 (0.3003) ([0.147]+[0.153])	Prec@1 93.750 (93.750)
Epoch: [248][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.3354 (0.2642) ([0.182]+[0.153])	Prec@1 93.750 (96.233)
Epoch: [248][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2162 (0.2608) ([0.063]+[0.153])	Prec@1 96.875 (96.358)
Epoch: [248][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2464 (0.2639) ([0.094]+[0.153])	Prec@1 95.312 (96.239)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4998 (0.4998) ([0.347]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 89.330
current lr 1.00000e-02
Grad=  tensor(12.8168, device='cuda:0')
Epoch: [249][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.3084 (0.3084) ([0.156]+[0.153])	Prec@1 95.312 (95.312)
Epoch: [249][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.2146 (0.2673) ([0.062]+[0.153])	Prec@1 98.438 (96.078)
Epoch: [249][200/391]	Time 0.116 (0.115)	Data 0.001 (0.001)	Loss 0.3130 (0.2717) ([0.160]+[0.153])	Prec@1 95.312 (95.872)
Epoch: [249][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2417 (0.2730) ([0.089]+[0.153])	Prec@1 96.094 (95.863)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4323 (0.4323) ([0.279]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 90.420
current lr 1.00000e-03
Grad=  tensor(9.4682, device='cuda:0')
Epoch: [250][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.2650 (0.2650) ([0.112]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [250][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1817 (0.2369) ([0.030]+[0.152])	Prec@1 99.219 (97.123)
Epoch: [250][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1888 (0.2247) ([0.037]+[0.151])	Prec@1 98.438 (97.625)
Epoch: [250][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2040 (0.2188) ([0.053]+[0.151])	Prec@1 99.219 (97.843)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4044 (0.4044) ([0.253]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 92.820
current lr 1.00000e-03
Grad=  tensor(4.9340, device='cuda:0')
Epoch: [251][0/391]	Time 0.342 (0.342)	Data 0.221 (0.221)	Loss 0.2083 (0.2083) ([0.057]+[0.151])	Prec@1 98.438 (98.438)
Epoch: [251][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.1762 (0.1932) ([0.025]+[0.151])	Prec@1 100.000 (98.871)
Epoch: [251][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.1995 (0.1927) ([0.049]+[0.151])	Prec@1 97.656 (98.842)
Epoch: [251][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1725 (0.1913) ([0.022]+[0.151])	Prec@1 100.000 (98.892)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3958 (0.3958) ([0.245]+[0.151])	Prec@1 91.406 (91.406)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(2.1516, device='cuda:0')
Epoch: [252][0/391]	Time 0.360 (0.360)	Data 0.235 (0.235)	Loss 0.1841 (0.1841) ([0.034]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [252][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1655 (0.1851) ([0.015]+[0.150])	Prec@1 100.000 (99.095)
Epoch: [252][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2139 (0.1842) ([0.064]+[0.150])	Prec@1 97.656 (99.098)
Epoch: [252][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1799 (0.1841) ([0.030]+[0.150])	Prec@1 100.000 (99.089)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.3749 (0.3749) ([0.225]+[0.150])	Prec@1 91.406 (91.406)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(4.7437, device='cuda:0')
Epoch: [253][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.2092 (0.2092) ([0.059]+[0.150])	Prec@1 98.438 (98.438)
Epoch: [253][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1869 (0.1783) ([0.037]+[0.150])	Prec@1 98.438 (99.265)
Epoch: [253][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2068 (0.1801) ([0.057]+[0.150])	Prec@1 98.438 (99.203)
Epoch: [253][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1686 (0.1799) ([0.019]+[0.150])	Prec@1 100.000 (99.234)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4020 (0.4020) ([0.253]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.9996, device='cuda:0')
Epoch: [254][0/391]	Time 0.352 (0.352)	Data 0.228 (0.228)	Loss 0.1699 (0.1699) ([0.020]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [254][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1880 (0.1771) ([0.039]+[0.149])	Prec@1 98.438 (99.257)
Epoch: [254][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2320 (0.1764) ([0.083]+[0.149])	Prec@1 96.094 (99.308)
Epoch: [254][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1968 (0.1760) ([0.048]+[0.149])	Prec@1 99.219 (99.336)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4034 (0.4034) ([0.254]+[0.149])	Prec@1 90.625 (90.625)
 * Prec@1 93.410
current lr 1.00000e-03
Grad=  tensor(1.8090, device='cuda:0')
Epoch: [255][0/391]	Time 0.349 (0.349)	Data 0.223 (0.223)	Loss 0.1769 (0.1769) ([0.028]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [255][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1622 (0.1743) ([0.013]+[0.149])	Prec@1 100.000 (99.281)
Epoch: [255][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1669 (0.1737) ([0.018]+[0.149])	Prec@1 100.000 (99.355)
Epoch: [255][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1662 (0.1728) ([0.018]+[0.149])	Prec@1 99.219 (99.393)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4121 (0.4121) ([0.264]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.6178, device='cuda:0')
Epoch: [256][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.1634 (0.1634) ([0.015]+[0.148])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1807 (0.1704) ([0.032]+[0.148])	Prec@1 99.219 (99.520)
Epoch: [256][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1633 (0.1711) ([0.015]+[0.148])	Prec@1 99.219 (99.436)
Epoch: [256][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1639 (0.1709) ([0.016]+[0.148])	Prec@1 100.000 (99.434)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4092 (0.4092) ([0.261]+[0.148])	Prec@1 90.625 (90.625)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(2.0114, device='cuda:0')
Epoch: [257][0/391]	Time 0.349 (0.349)	Data 0.223 (0.223)	Loss 0.1688 (0.1688) ([0.021]+[0.148])	Prec@1 99.219 (99.219)
Epoch: [257][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1630 (0.1702) ([0.015]+[0.148])	Prec@1 100.000 (99.459)
Epoch: [257][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1652 (0.1688) ([0.017]+[0.148])	Prec@1 100.000 (99.487)
Epoch: [257][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1706 (0.1688) ([0.023]+[0.148])	Prec@1 99.219 (99.481)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3972 (0.3972) ([0.250]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 93.520
current lr 1.00000e-03
Grad=  tensor(0.4552, device='cuda:0')
Epoch: [258][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.1575 (0.1575) ([0.010]+[0.148])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1807 (0.1675) ([0.033]+[0.147])	Prec@1 99.219 (99.598)
Epoch: [258][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1674 (0.1671) ([0.020]+[0.147])	Prec@1 99.219 (99.553)
Epoch: [258][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1555 (0.1675) ([0.008]+[0.147])	Prec@1 100.000 (99.522)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3868 (0.3868) ([0.240]+[0.147])	Prec@1 92.188 (92.188)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(0.8280, device='cuda:0')
Epoch: [259][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.1613 (0.1613) ([0.014]+[0.147])	Prec@1 100.000 (100.000)
Epoch: [259][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1664 (0.1651) ([0.019]+[0.147])	Prec@1 99.219 (99.629)
Epoch: [259][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1701 (0.1650) ([0.023]+[0.147])	Prec@1 99.219 (99.607)
Epoch: [259][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1779 (0.1643) ([0.031]+[0.147])	Prec@1 99.219 (99.629)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4126 (0.4126) ([0.266]+[0.147])	Prec@1 91.406 (91.406)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(6.7772, device='cuda:0')
Epoch: [260][0/391]	Time 0.343 (0.343)	Data 0.218 (0.218)	Loss 0.1896 (0.1896) ([0.043]+[0.147])	Prec@1 98.438 (98.438)
Epoch: [260][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1525 (0.1643) ([0.006]+[0.147])	Prec@1 100.000 (99.590)
Epoch: [260][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1658 (0.1635) ([0.019]+[0.146])	Prec@1 100.000 (99.604)
Epoch: [260][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1782 (0.1636) ([0.032]+[0.146])	Prec@1 99.219 (99.608)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.3700 (0.3700) ([0.224]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.6805, device='cuda:0')
Epoch: [261][0/391]	Time 0.337 (0.337)	Data 0.213 (0.213)	Loss 0.1563 (0.1563) ([0.010]+[0.146])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1596 (0.1624) ([0.013]+[0.146])	Prec@1 100.000 (99.729)
Epoch: [261][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1703 (0.1629) ([0.024]+[0.146])	Prec@1 99.219 (99.670)
Epoch: [261][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1698 (0.1628) ([0.024]+[0.146])	Prec@1 99.219 (99.673)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.3735 (0.3735) ([0.228]+[0.146])	Prec@1 92.969 (92.969)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(0.4712, device='cuda:0')
Epoch: [262][0/391]	Time 0.339 (0.339)	Data 0.215 (0.215)	Loss 0.1510 (0.1510) ([0.005]+[0.146])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1564 (0.1605) ([0.011]+[0.146])	Prec@1 100.000 (99.698)
Epoch: [262][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1633 (0.1611) ([0.018]+[0.146])	Prec@1 99.219 (99.674)
Epoch: [262][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1570 (0.1612) ([0.012]+[0.145])	Prec@1 100.000 (99.676)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.3763 (0.3763) ([0.231]+[0.145])	Prec@1 91.406 (91.406)
 * Prec@1 93.580
current lr 1.00000e-03
Grad=  tensor(2.1607, device='cuda:0')
Epoch: [263][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.1677 (0.1677) ([0.022]+[0.145])	Prec@1 99.219 (99.219)
Epoch: [263][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1604 (0.1616) ([0.015]+[0.145])	Prec@1 100.000 (99.698)
Epoch: [263][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2126 (0.1606) ([0.068]+[0.145])	Prec@1 98.438 (99.736)
Epoch: [263][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1543 (0.1606) ([0.009]+[0.145])	Prec@1 100.000 (99.689)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3760 (0.3760) ([0.231]+[0.145])	Prec@1 92.188 (92.188)
 * Prec@1 93.570
current lr 1.00000e-03
Grad=  tensor(1.5652, device='cuda:0')
Epoch: [264][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.1645 (0.1645) ([0.020]+[0.145])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1516 (0.1583) ([0.007]+[0.145])	Prec@1 100.000 (99.675)
Epoch: [264][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1669 (0.1582) ([0.022]+[0.145])	Prec@1 99.219 (99.697)
Epoch: [264][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1514 (0.1583) ([0.007]+[0.145])	Prec@1 100.000 (99.709)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.3530 (0.3530) ([0.209]+[0.144])	Prec@1 91.406 (91.406)
 * Prec@1 93.460
current lr 1.00000e-03
Grad=  tensor(1.1877, device='cuda:0')
Epoch: [265][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.1563 (0.1563) ([0.012]+[0.144])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1495 (0.1583) ([0.005]+[0.144])	Prec@1 100.000 (99.706)
Epoch: [265][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1635 (0.1594) ([0.019]+[0.144])	Prec@1 99.219 (99.650)
Epoch: [265][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1490 (0.1590) ([0.005]+[0.144])	Prec@1 100.000 (99.665)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3652 (0.3652) ([0.221]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.3027, device='cuda:0')
Epoch: [266][0/391]	Time 0.340 (0.340)	Data 0.218 (0.218)	Loss 0.1506 (0.1506) ([0.007]+[0.144])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1523 (0.1580) ([0.008]+[0.144])	Prec@1 100.000 (99.698)
Epoch: [266][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1545 (0.1581) ([0.011]+[0.144])	Prec@1 100.000 (99.689)
Epoch: [266][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1544 (0.1581) ([0.011]+[0.144])	Prec@1 100.000 (99.686)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.3879 (0.3879) ([0.244]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(1.4185, device='cuda:0')
Epoch: [267][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.1599 (0.1599) ([0.016]+[0.144])	Prec@1 99.219 (99.219)
Epoch: [267][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1537 (0.1581) ([0.010]+[0.143])	Prec@1 99.219 (99.660)
Epoch: [267][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1485 (0.1575) ([0.005]+[0.143])	Prec@1 100.000 (99.685)
Epoch: [267][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1559 (0.1572) ([0.013]+[0.143])	Prec@1 100.000 (99.699)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3797 (0.3797) ([0.237]+[0.143])	Prec@1 91.406 (91.406)
 * Prec@1 93.560
current lr 1.00000e-03
Grad=  tensor(2.9047, device='cuda:0')
Epoch: [268][0/391]	Time 0.346 (0.346)	Data 0.226 (0.226)	Loss 0.1705 (0.1705) ([0.027]+[0.143])	Prec@1 98.438 (98.438)
Epoch: [268][100/391]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.1508 (0.1563) ([0.008]+[0.143])	Prec@1 100.000 (99.752)
Epoch: [268][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1530 (0.1562) ([0.010]+[0.143])	Prec@1 100.000 (99.755)
Epoch: [268][300/391]	Time 0.114 (0.111)	Data 0.000 (0.001)	Loss 0.1540 (0.1560) ([0.011]+[0.143])	Prec@1 99.219 (99.748)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.3722 (0.3722) ([0.229]+[0.143])	Prec@1 91.406 (91.406)
 * Prec@1 93.380
current lr 1.00000e-03
Grad=  tensor(0.4746, device='cuda:0')
Epoch: [269][0/391]	Time 0.344 (0.344)	Data 0.221 (0.221)	Loss 0.1518 (0.1518) ([0.009]+[0.143])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.115 (0.114)	Data 0.000 (0.002)	Loss 0.1590 (0.1554) ([0.016]+[0.143])	Prec@1 100.000 (99.760)
Epoch: [269][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1482 (0.1551) ([0.006]+[0.143])	Prec@1 100.000 (99.794)
Epoch: [269][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1680 (0.1556) ([0.026]+[0.142])	Prec@1 98.438 (99.746)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4122 (0.4122) ([0.270]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(0.1780, device='cuda:0')
Epoch: [270][0/391]	Time 0.348 (0.348)	Data 0.225 (0.225)	Loss 0.1476 (0.1476) ([0.005]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1519 (0.1541) ([0.010]+[0.142])	Prec@1 99.219 (99.807)
Epoch: [270][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.1541 (0.1542) ([0.012]+[0.142])	Prec@1 99.219 (99.778)
Epoch: [270][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1534 (0.1546) ([0.011]+[0.142])	Prec@1 100.000 (99.746)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3669 (0.3669) ([0.225]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(3.3161, device='cuda:0')
Epoch: [271][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.1684 (0.1684) ([0.026]+[0.142])	Prec@1 99.219 (99.219)
Epoch: [271][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1536 (0.1533) ([0.012]+[0.142])	Prec@1 100.000 (99.814)
Epoch: [271][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1681 (0.1534) ([0.026]+[0.142])	Prec@1 99.219 (99.778)
Epoch: [271][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1481 (0.1533) ([0.006]+[0.142])	Prec@1 100.000 (99.769)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3658 (0.3658) ([0.224]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 93.340
current lr 1.00000e-03
Grad=  tensor(0.9882, device='cuda:0')
Epoch: [272][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.1511 (0.1511) ([0.010]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1597 (0.1523) ([0.018]+[0.141])	Prec@1 100.000 (99.807)
Epoch: [272][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1494 (0.1528) ([0.008]+[0.141])	Prec@1 100.000 (99.802)
Epoch: [272][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1509 (0.1527) ([0.010]+[0.141])	Prec@1 99.219 (99.785)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3725 (0.3725) ([0.231]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(0.2982, device='cuda:0')
Epoch: [273][0/391]	Time 0.340 (0.340)	Data 0.217 (0.217)	Loss 0.1478 (0.1478) ([0.007]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1700 (0.1518) ([0.029]+[0.141])	Prec@1 99.219 (99.791)
Epoch: [273][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1491 (0.1517) ([0.008]+[0.141])	Prec@1 100.000 (99.817)
Epoch: [273][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1845 (0.1522) ([0.044]+[0.141])	Prec@1 99.219 (99.798)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.3886 (0.3886) ([0.248]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(0.1142, device='cuda:0')
Epoch: [274][0/391]	Time 0.354 (0.354)	Data 0.230 (0.230)	Loss 0.1432 (0.1432) ([0.002]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1463 (0.1530) ([0.006]+[0.141])	Prec@1 100.000 (99.745)
Epoch: [274][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1536 (0.1523) ([0.013]+[0.141])	Prec@1 100.000 (99.771)
Epoch: [274][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1580 (0.1518) ([0.018]+[0.140])	Prec@1 99.219 (99.800)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3881 (0.3881) ([0.248]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 93.510
current lr 1.00000e-03
Grad=  tensor(1.2690, device='cuda:0')
Epoch: [275][0/391]	Time 0.342 (0.342)	Data 0.218 (0.218)	Loss 0.1540 (0.1540) ([0.014]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1514 (0.1509) ([0.011]+[0.140])	Prec@1 100.000 (99.791)
Epoch: [275][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1454 (0.1508) ([0.005]+[0.140])	Prec@1 100.000 (99.790)
Epoch: [275][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1431 (0.1507) ([0.003]+[0.140])	Prec@1 100.000 (99.811)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.4034 (0.4034) ([0.263]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 93.550
current lr 1.00000e-03
Grad=  tensor(2.2467, device='cuda:0')
Epoch: [276][0/391]	Time 0.337 (0.337)	Data 0.213 (0.213)	Loss 0.1553 (0.1553) ([0.015]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1485 (0.1503) ([0.009]+[0.140])	Prec@1 100.000 (99.791)
Epoch: [276][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1709 (0.1507) ([0.031]+[0.140])	Prec@1 98.438 (99.810)
Epoch: [276][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1485 (0.1505) ([0.009]+[0.140])	Prec@1 100.000 (99.811)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4177 (0.4177) ([0.278]+[0.140])	Prec@1 90.625 (90.625)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(0.3971, device='cuda:0')
Epoch: [277][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.1465 (0.1465) ([0.007]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1430 (0.1493) ([0.003]+[0.139])	Prec@1 100.000 (99.876)
Epoch: [277][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1430 (0.1493) ([0.004]+[0.139])	Prec@1 100.000 (99.868)
Epoch: [277][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1516 (0.1494) ([0.012]+[0.139])	Prec@1 99.219 (99.839)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4087 (0.4087) ([0.269]+[0.139])	Prec@1 92.188 (92.188)
 * Prec@1 93.550
current lr 1.00000e-03
Grad=  tensor(1.2291, device='cuda:0')
Epoch: [278][0/391]	Time 0.336 (0.336)	Data 0.212 (0.212)	Loss 0.1451 (0.1451) ([0.006]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1464 (0.1493) ([0.007]+[0.139])	Prec@1 100.000 (99.768)
Epoch: [278][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1441 (0.1493) ([0.005]+[0.139])	Prec@1 100.000 (99.798)
Epoch: [278][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1452 (0.1491) ([0.006]+[0.139])	Prec@1 100.000 (99.808)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4019 (0.4019) ([0.263]+[0.139])	Prec@1 90.625 (90.625)
 * Prec@1 93.570
current lr 1.00000e-03
Grad=  tensor(0.4288, device='cuda:0')
Epoch: [279][0/391]	Time 0.341 (0.341)	Data 0.220 (0.220)	Loss 0.1438 (0.1438) ([0.005]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1437 (0.1470) ([0.005]+[0.139])	Prec@1 100.000 (99.876)
Epoch: [279][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1448 (0.1474) ([0.006]+[0.139])	Prec@1 100.000 (99.868)
Epoch: [279][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1441 (0.1474) ([0.006]+[0.139])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4161 (0.4161) ([0.278]+[0.138])	Prec@1 90.625 (90.625)
 * Prec@1 93.600
current lr 1.00000e-03
Grad=  tensor(0.2001, device='cuda:0')
Epoch: [280][0/391]	Time 0.344 (0.344)	Data 0.219 (0.219)	Loss 0.1433 (0.1433) ([0.005]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1659 (0.1474) ([0.028]+[0.138])	Prec@1 99.219 (99.869)
Epoch: [280][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1426 (0.1478) ([0.004]+[0.138])	Prec@1 100.000 (99.837)
Epoch: [280][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1439 (0.1473) ([0.006]+[0.138])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3889 (0.3889) ([0.251]+[0.138])	Prec@1 91.406 (91.406)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(0.3208, device='cuda:0')
Epoch: [281][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.1445 (0.1445) ([0.006]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.1502 (0.1476) ([0.012]+[0.138])	Prec@1 99.219 (99.783)
Epoch: [281][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1480 (0.1474) ([0.010]+[0.138])	Prec@1 100.000 (99.806)
Epoch: [281][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1447 (0.1470) ([0.007]+[0.138])	Prec@1 100.000 (99.836)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3821 (0.3821) ([0.244]+[0.138])	Prec@1 91.406 (91.406)
 * Prec@1 93.670
current lr 1.00000e-03
Grad=  tensor(1.4652, device='cuda:0')
Epoch: [282][0/391]	Time 0.335 (0.335)	Data 0.211 (0.211)	Loss 0.1496 (0.1496) ([0.012]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.1449 (0.1463) ([0.007]+[0.138])	Prec@1 100.000 (99.892)
Epoch: [282][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1432 (0.1462) ([0.006]+[0.137])	Prec@1 100.000 (99.887)
Epoch: [282][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1418 (0.1463) ([0.004]+[0.137])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3650 (0.3650) ([0.228]+[0.137])	Prec@1 92.188 (92.188)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(0.4499, device='cuda:0')
Epoch: [283][0/391]	Time 0.341 (0.341)	Data 0.218 (0.218)	Loss 0.1441 (0.1441) ([0.007]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1411 (0.1452) ([0.004]+[0.137])	Prec@1 100.000 (99.907)
Epoch: [283][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1408 (0.1454) ([0.004]+[0.137])	Prec@1 100.000 (99.883)
Epoch: [283][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1489 (0.1457) ([0.012]+[0.137])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3988 (0.3988) ([0.262]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 93.590
current lr 1.00000e-03
Grad=  tensor(0.5745, device='cuda:0')
Epoch: [284][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.1441 (0.1441) ([0.007]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [284][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1594 (0.1449) ([0.023]+[0.137])	Prec@1 99.219 (99.884)
Epoch: [284][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1409 (0.1456) ([0.004]+[0.137])	Prec@1 100.000 (99.856)
Epoch: [284][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1419 (0.1456) ([0.005]+[0.137])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3775 (0.3775) ([0.241]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(1.4812, device='cuda:0')
Epoch: [285][0/391]	Time 0.344 (0.344)	Data 0.219 (0.219)	Loss 0.1466 (0.1466) ([0.010]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1393 (0.1452) ([0.003]+[0.136])	Prec@1 100.000 (99.853)
Epoch: [285][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.1419 (0.1454) ([0.006]+[0.136])	Prec@1 100.000 (99.856)
Epoch: [285][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1411 (0.1452) ([0.005]+[0.136])	Prec@1 100.000 (99.855)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3942 (0.3942) ([0.258]+[0.136])	Prec@1 90.625 (90.625)
 * Prec@1 93.600
current lr 1.00000e-03
Grad=  tensor(2.6611, device='cuda:0')
Epoch: [286][0/391]	Time 0.348 (0.348)	Data 0.214 (0.214)	Loss 0.1646 (0.1646) ([0.028]+[0.136])	Prec@1 99.219 (99.219)
Epoch: [286][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1393 (0.1446) ([0.003]+[0.136])	Prec@1 100.000 (99.838)
Epoch: [286][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1451 (0.1445) ([0.009]+[0.136])	Prec@1 99.219 (99.856)
Epoch: [286][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1404 (0.1447) ([0.005]+[0.136])	Prec@1 100.000 (99.852)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4078 (0.4078) ([0.272]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 93.590
current lr 1.00000e-03
Grad=  tensor(0.7240, device='cuda:0')
Epoch: [287][0/391]	Time 0.338 (0.338)	Data 0.214 (0.214)	Loss 0.1418 (0.1418) ([0.006]+[0.136])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1429 (0.1450) ([0.007]+[0.136])	Prec@1 100.000 (99.791)
Epoch: [287][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1430 (0.1448) ([0.007]+[0.136])	Prec@1 100.000 (99.817)
Epoch: [287][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1407 (0.1442) ([0.005]+[0.135])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.3891 (0.3891) ([0.254]+[0.135])	Prec@1 90.625 (90.625)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(0.1339, device='cuda:0')
Epoch: [288][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.1378 (0.1378) ([0.002]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1379 (0.1439) ([0.003]+[0.135])	Prec@1 100.000 (99.869)
Epoch: [288][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1553 (0.1434) ([0.020]+[0.135])	Prec@1 99.219 (99.895)
Epoch: [288][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1432 (0.1434) ([0.008]+[0.135])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3633 (0.3633) ([0.228]+[0.135])	Prec@1 91.406 (91.406)
 * Prec@1 93.610
current lr 1.00000e-03
Grad=  tensor(0.1516, device='cuda:0')
Epoch: [289][0/391]	Time 0.340 (0.340)	Data 0.220 (0.220)	Loss 0.1373 (0.1373) ([0.002]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.1462 (0.1422) ([0.011]+[0.135])	Prec@1 99.219 (99.861)
Epoch: [289][200/391]	Time 0.107 (0.110)	Data 0.000 (0.001)	Loss 0.1411 (0.1432) ([0.006]+[0.135])	Prec@1 100.000 (99.837)
Epoch: [289][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1371 (0.1432) ([0.002]+[0.135])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3634 (0.3634) ([0.229]+[0.135])	Prec@1 91.406 (91.406)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(0.4795, device='cuda:0')
Epoch: [290][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.1418 (0.1418) ([0.007]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1395 (0.1429) ([0.005]+[0.135])	Prec@1 100.000 (99.869)
Epoch: [290][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1407 (0.1429) ([0.006]+[0.134])	Prec@1 100.000 (99.845)
Epoch: [290][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1435 (0.1426) ([0.009]+[0.134])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.3872 (0.3872) ([0.253]+[0.134])	Prec@1 91.406 (91.406)
 * Prec@1 93.590
current lr 1.00000e-03
Grad=  tensor(3.5104, device='cuda:0')
Epoch: [291][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.1537 (0.1537) ([0.019]+[0.134])	Prec@1 99.219 (99.219)
Epoch: [291][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1595 (0.1431) ([0.025]+[0.134])	Prec@1 99.219 (99.869)
Epoch: [291][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1371 (0.1427) ([0.003]+[0.134])	Prec@1 100.000 (99.860)
Epoch: [291][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1373 (0.1423) ([0.003]+[0.134])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3717 (0.3717) ([0.238]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 93.510
current lr 1.00000e-03
Grad=  tensor(0.1407, device='cuda:0')
Epoch: [292][0/391]	Time 0.342 (0.342)	Data 0.220 (0.220)	Loss 0.1365 (0.1365) ([0.003]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.116 (0.111)	Data 0.000 (0.002)	Loss 0.1396 (0.1409) ([0.006]+[0.134])	Prec@1 100.000 (99.869)
Epoch: [292][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.1510 (0.1411) ([0.017]+[0.134])	Prec@1 100.000 (99.883)
Epoch: [292][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.1457 (0.1413) ([0.012]+[0.134])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.261 (0.261)	Loss 0.3808 (0.3808) ([0.247]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(1.1499, device='cuda:0')
Epoch: [293][0/391]	Time 0.351 (0.351)	Data 0.224 (0.224)	Loss 0.1418 (0.1418) ([0.008]+[0.134])	Prec@1 99.219 (99.219)
Epoch: [293][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1359 (0.1401) ([0.002]+[0.133])	Prec@1 100.000 (99.907)
Epoch: [293][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1372 (0.1410) ([0.004]+[0.133])	Prec@1 100.000 (99.887)
Epoch: [293][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1534 (0.1411) ([0.020]+[0.133])	Prec@1 99.219 (99.894)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3598 (0.3598) ([0.227]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 93.620
current lr 1.00000e-03
Grad=  tensor(0.3062, device='cuda:0')
Epoch: [294][0/391]	Time 0.333 (0.333)	Data 0.209 (0.209)	Loss 0.1380 (0.1380) ([0.005]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1388 (0.1406) ([0.006]+[0.133])	Prec@1 100.000 (99.915)
Epoch: [294][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1415 (0.1401) ([0.009]+[0.133])	Prec@1 100.000 (99.911)
Epoch: [294][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.1389 (0.1399) ([0.006]+[0.133])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.3505 (0.3505) ([0.218]+[0.133])	Prec@1 91.406 (91.406)
 * Prec@1 93.520
current lr 1.00000e-03
Grad=  tensor(0.1675, device='cuda:0')
Epoch: [295][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.1362 (0.1362) ([0.003]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1357 (0.1396) ([0.003]+[0.133])	Prec@1 100.000 (99.923)
Epoch: [295][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1374 (0.1396) ([0.005]+[0.133])	Prec@1 100.000 (99.907)
Epoch: [295][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1376 (0.1399) ([0.005]+[0.133])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3525 (0.3525) ([0.220]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(0.4115, device='cuda:0')
Epoch: [296][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.1385 (0.1385) ([0.006]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.1348 (0.1391) ([0.002]+[0.132])	Prec@1 100.000 (99.923)
Epoch: [296][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1352 (0.1394) ([0.003]+[0.132])	Prec@1 100.000 (99.895)
Epoch: [296][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1389 (0.1398) ([0.007]+[0.132])	Prec@1 100.000 (99.875)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3680 (0.3680) ([0.236]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(0.5844, device='cuda:0')
Epoch: [297][0/391]	Time 0.337 (0.337)	Data 0.213 (0.213)	Loss 0.1376 (0.1376) ([0.005]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1399 (0.1407) ([0.008]+[0.132])	Prec@1 100.000 (99.814)
Epoch: [297][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1419 (0.1401) ([0.010]+[0.132])	Prec@1 99.219 (99.848)
Epoch: [297][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1372 (0.1397) ([0.005]+[0.132])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3494 (0.3494) ([0.218]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 93.700
current lr 1.00000e-03
Grad=  tensor(1.2530, device='cuda:0')
Epoch: [298][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.1398 (0.1398) ([0.008]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.1366 (0.1394) ([0.005]+[0.132])	Prec@1 100.000 (99.876)
Epoch: [298][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1351 (0.1391) ([0.004]+[0.132])	Prec@1 100.000 (99.883)
Epoch: [298][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1344 (0.1390) ([0.003]+[0.131])	Prec@1 100.000 (99.875)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3619 (0.3619) ([0.230]+[0.131])	Prec@1 90.625 (90.625)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.1357, device='cuda:0')
Epoch: [299][0/391]	Time 0.342 (0.342)	Data 0.217 (0.217)	Loss 0.1335 (0.1335) ([0.002]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1447 (0.1384) ([0.013]+[0.131])	Prec@1 99.219 (99.907)
Epoch: [299][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1336 (0.1384) ([0.002]+[0.131])	Prec@1 100.000 (99.895)
Epoch: [299][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1332 (0.1382) ([0.002]+[0.131])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3535 (0.3535) ([0.222]+[0.131])	Prec@1 90.625 (90.625)
 * Prec@1 93.660

 Elapsed time for training  4:00:38.051795

 sparsity of   [0.0, 0.9259259104728699, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.7777777910232544, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.4444444477558136, 0.0, 0.0, 0.6666666865348816, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.6296296119689941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9259259104728699, 0.0, 0.0, 0.0, 0.8888888955116272, 0.0, 0.8148148059844971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5925925970077515, 0.9629629850387573, 0.9259259104728699, 0.9629629850387573, 0.0, 0.0, 0.0, 0.8518518805503845, 0.5925925970077515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9259259104728699]

 sparsity of   [0.984375, 0.96875, 0.96875, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.96875, 0.984375, 0.96875, 0.984375, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.96875, 0.984375, 0.96875, 0.0, 0.328125, 0.984375, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.328125, 0.96875, 0.984375, 0.984375, 0.984375, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.984375, 0.96875, 0.984375, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.984375, 0.0]

 sparsity of   [0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9947916865348816, 0.9965277910232544, 0.0, 0.71875, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.71875, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.71875, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.71875, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.0, 0.71875, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.71875, 0.9982638955116272, 0.9982638955116272, 0.9947916865348816, 0.71875, 0.9965277910232544, 0.0, 0.0]

 sparsity of   [0.0, 0.96875, 0.703125, 0.984375, 0.96875, 0.0, 0.703125, 0.0, 0.921875, 0.0, 0.96875, 0.0, 0.96875, 0.984375, 0.953125, 0.0, 0.0, 0.375, 0.0, 0.703125, 0.96875, 0.703125, 0.0, 0.703125, 0.703125, 0.0, 0.703125, 0.953125, 0.0, 0.984375, 0.0, 0.703125, 0.96875, 0.0, 0.0, 0.0, 0.703125, 0.703125, 0.0, 0.96875, 0.96875, 0.953125, 0.0, 0.609375, 0.703125, 0.0, 0.421875, 0.96875, 0.96875, 0.0, 0.4375, 0.703125, 0.703125, 0.953125, 0.125, 0.0, 0.46875, 0.0, 0.96875, 0.96875, 0.703125, 0.0, 0.0, 0.703125, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.953125, 0.109375, 0.0, 0.703125, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9375, 0.0, 0.0, 0.0, 0.984375, 0.703125, 0.96875, 0.0, 0.703125, 0.0, 0.0, 0.0, 0.15625, 0.21875, 0.0, 0.03125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.703125, 0.96875, 0.703125, 0.0, 0.015625, 0.0, 0.703125, 0.0, 0.0, 0.96875, 0.96875, 0.109375, 0.0, 0.0, 0.703125, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.703125, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.109375, 0.96875, 0.703125, 0.703125, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0625, 0.0, 0.96875, 0.0, 0.0, 0.921875, 0.0, 0.96875, 0.53125, 0.96875, 0.703125, 0.0, 0.96875, 0.234375, 0.109375, 0.0, 0.703125, 0.703125, 0.984375, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.703125, 0.8125, 0.0, 0.703125, 0.0, 0.0, 0.96875, 0.046875, 0.96875, 0.0, 0.703125, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.703125, 0.96875, 0.96875, 0.703125, 0.96875, 0.0, 0.0, 0.703125, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.953125, 0.984375, 0.0, 0.96875, 0.96875, 0.703125, 0.0625, 0.953125, 0.71875, 0.0, 0.984375, 0.96875, 0.96875, 0.96875, 0.0, 0.703125, 0.984375, 0.96875, 0.703125, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.703125, 0.96875, 0.28125, 0.03125, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.6875, 0.296875, 0.0, 0.0, 0.0, 0.703125, 0.96875, 0.96875, 0.0, 0.0, 0.953125, 0.703125, 0.96875, 0.0, 0.0, 0.703125, 0.984375, 0.0, 0.96875, 0.0, 0.984375, 0.703125, 0.0, 0.0625, 0.0, 0.0]

 sparsity of   [0.328125, 0.96875, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.984375, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.328125, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.328125, 0.984375, 0.0, 0.96875, 0.0, 0.328125, 0.0, 0.0, 0.328125, 0.96875, 0.984375, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.328125, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.96875, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.984375, 0.96875, 0.96875, 0.984375, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.3359375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.3359375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.984375, 0.9921875, 0.99609375]

 sparsity of   [0.0, 0.640625, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.640625, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.640625, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.640625, 0.6180555820465088, 0.9982638955116272, 0.640625, 0.9965277910232544, 0.640625, 0.0, 0.640625, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9947916865348816, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544]

 sparsity of   [0.0, 0.96875, 0.0, 0.0, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.53125, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.984375, 0.0, 0.0, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.96875, 0.53125, 0.0, 0.53125, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.53125, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.96875, 0.0, 0.53125, 0.53125, 0.0, 0.53125, 0.96875, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.96875, 0.96875, 0.53125, 0.53125, 0.0, 0.0, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.96875, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.96875, 0.53125, 0.96875, 0.53125, 0.53125, 0.0, 0.0, 0.96875, 0.53125, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.21875, 0.0, 0.0, 0.53125, 0.53125, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.53125, 0.53125, 0.0, 0.0, 0.53125, 0.53125, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.96875, 0.96875, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.53125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.53125, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.53125, 0.96875, 0.0, 0.0, 0.0, 0.53125, 0.53125, 0.53125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53125, 0.0]

 sparsity of   [0.01953125, 0.1015625, 0.0, 0.1015625, 0.109375, 0.9921875, 0.3828125, 0.03515625, 0.34375, 0.03515625, 0.0, 0.9921875, 0.0, 0.1015625, 0.04296875, 0.97265625, 0.875, 0.99609375, 0.9921875, 0.015625, 0.1328125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.046875, 0.109375, 0.203125, 0.6796875, 0.0, 0.0234375, 0.08984375, 0.87890625, 0.02734375, 0.04296875, 0.0, 0.0859375, 0.1171875, 0.9921875, 0.12890625, 0.05859375, 0.09375, 0.9140625, 0.9921875, 0.0, 0.03515625, 0.0, 0.0, 0.02734375, 0.0, 0.9921875, 0.03515625, 0.98828125, 0.03125, 0.01171875, 0.9921875, 0.9921875, 0.0, 0.0, 0.109375, 0.0390625]

 sparsity of   [0.9947916865348816, 0.01909722201526165, 0.0, 0.0850694477558136, 0.9982638955116272, 0.010416666977107525, 0.0677083358168602, 0.9982638955116272, 0.9965277910232544, 0.0572916679084301, 0.9982638955116272, 0.013888888992369175, 0.01215277798473835, 0.0, 0.0677083358168602, 0.5520833134651184, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.02951388992369175, 0.9965277910232544, 0.0, 0.0590277798473835, 0.9045138955116272, 0.0381944440305233, 0.65625, 0.9982638955116272, 0.0, 0.02951388992369175, 0.010416666977107525, 0.0, 0.0, 0.2413194477558136, 0.9965277910232544, 0.0347222238779068, 0.1701388955116272, 0.0, 0.01909722201526165, 0.3645833432674408, 0.9965277910232544, 0.0173611119389534, 0.0225694440305233, 0.1614583283662796, 0.8055555820465088, 0.9982638955116272, 0.0, 0.9947916865348816, 0.03125, 0.9982638955116272, 0.0, 0.0173611119389534, 0.0, 0.02083333395421505, 0.8489583134651184, 0.5989583134651184, 0.0, 0.0, 0.0746527761220932, 0.0, 0.2395833283662796, 0.0, 0.046875, 0.015625]

 sparsity of   [0.65625, 0.0, 0.0, 0.671875, 0.609375, 0.65625, 0.03125, 0.015625, 0.0, 0.09375, 0.671875, 0.40625, 0.6875, 0.0, 0.671875, 0.6875, 0.046875, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.015625, 0.0, 0.671875, 0.0625, 0.0, 0.0, 0.0, 0.015625, 0.15625, 0.6875, 0.0, 0.546875, 0.0, 0.671875, 0.6875, 0.5625, 0.0, 0.6875, 0.578125, 0.671875, 0.65625, 0.0, 0.234375, 0.0, 0.5, 0.0, 0.625, 0.015625, 0.03125, 0.0625, 0.09375, 0.0, 0.6875, 0.0, 0.078125, 0.0, 0.6875, 0.671875, 0.5625, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.09375, 0.6875, 0.0, 0.671875, 0.0, 0.6875, 0.0, 0.046875, 0.671875, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.03125, 0.6875, 0.0, 0.0, 0.25, 0.140625, 0.984375, 0.65625, 0.6875, 0.6875, 0.6875, 0.6875, 0.328125, 0.015625, 0.0, 0.0, 0.0625, 0.953125, 0.0, 0.0, 0.6875, 0.078125, 0.6875, 0.0625, 0.0, 0.6875, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.015625, 0.0, 0.1875, 0.6875, 0.03125, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.1875, 0.6875, 0.671875, 0.0, 0.0, 0.6875, 0.0, 0.96875, 0.0, 0.984375, 0.6875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.671875, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.65625, 0.0625, 0.640625, 0.078125, 0.0, 0.09375, 0.0, 0.0625, 0.0, 0.6875, 0.6875, 0.34375, 0.6875, 0.6875, 0.6875, 0.0, 0.078125, 0.03125, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.046875, 0.09375, 0.078125, 0.6875, 0.6875, 0.6875, 0.0, 0.203125, 0.234375, 0.6875, 0.65625, 0.0, 0.0, 0.6875, 0.6875, 0.0625, 0.0, 0.5, 0.0, 0.65625, 0.0, 0.515625, 0.78125, 0.0, 0.1875, 0.0, 0.09375, 0.265625, 0.6875, 0.65625, 0.6875, 0.0, 0.6875, 0.6875, 0.0, 0.03125, 0.0, 0.0, 0.6875, 0.65625, 0.6875, 0.046875, 0.0, 0.6875, 0.6875, 0.6875, 0.09375, 0.65625, 0.6875, 0.0, 0.671875, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.96875, 0.6875, 0.0, 0.078125, 0.6875, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.3125, 0.0, 0.6875, 0.0, 0.015625]

 sparsity of   [0.9921875, 0.9921875, 0.99609375, 0.0390625, 0.0390625, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.015625, 0.0390625, 0.0, 0.046875, 0.0, 0.0, 0.1171875, 0.0390625, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0390625, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.0, 0.0390625, 0.9921875, 0.9921875, 0.0, 0.0, 0.3984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0390625, 0.0, 0.0, 0.99609375, 0.05859375, 0.9921875, 0.9921875, 0.0, 0.08203125, 0.99609375, 0.98828125, 0.0390625, 0.17578125, 0.0, 0.99609375, 0.04296875, 0.0390625, 0.98828125, 0.98828125, 0.99609375, 0.9921875, 0.0, 0.0390625, 0.9921875, 0.0546875, 0.9921875, 0.9921875, 0.99609375, 0.25390625, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0390625, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0390625, 0.0, 0.9921875, 0.05859375]

 sparsity of   [0.8315972089767456, 0.0, 0.0164930559694767, 0.9982638955116272, 0.0815972238779068, 0.01215277798473835, 0.0, 0.0, 0.0251736119389534, 0.086805559694767, 0.506944477558136, 0.173611119389534, 0.0390625, 0.0, 0.0416666679084301, 0.0164930559694767, 0.0, 0.8611111044883728, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0355902798473835, 0.0, 0.0729166641831398, 0.0, 0.0416666679084301, 0.9036458134651184, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0381944440305233, 0.0, 0.0, 0.0, 0.0243055559694767, 0.0, 0.7395833134651184, 0.0, 0.0, 0.0243055559694767, 0.0529513880610466, 0.0, 0.9982638955116272, 0.0164930559694767, 0.9982638955116272, 0.014756944961845875, 0.0, 0.071180559694767, 0.0, 0.1493055522441864, 0.796006977558136, 0.0373263880610466, 0.0, 0.546875, 0.0303819440305233, 0.0, 0.02604166604578495, 0.02864583395421505, 0.0, 0.01128472201526165, 0.0, 0.02690972201526165, 0.0, 0.0, 0.0, 0.02864583395421505, 0.0, 0.8107638955116272, 0.0451388880610466, 0.0, 0.9418402910232544, 0.0494791679084301, 0.0, 0.0407986119389534, 0.0234375, 0.0, 0.010416666977107525, 0.0, 0.02777777798473835, 0.999131977558136, 0.0, 0.02864583395421505, 0.0321180559694767, 0.546875, 0.063368059694767, 0.063368059694767, 0.999131977558136, 0.071180559694767, 0.02864583395421505, 0.0, 0.7847222089767456, 0.546875, 0.0, 0.0, 0.9357638955116272, 0.0442708320915699, 0.9973958134651184, 0.0225694440305233, 0.0, 0.0, 0.0364583320915699, 0.0, 0.0, 0.999131977558136, 0.02604166604578495, 0.0, 0.0373263880610466, 0.0, 0.9982638955116272, 0.09375, 0.0, 0.0, 0.9305555820465088, 0.010416666977107525, 0.9982638955116272, 0.0, 0.0173611119389534, 0.0, 0.0, 0.0, 0.0512152798473835]

 sparsity of   [0.0, 0.0, 0.984375, 0.984375, 0.0, 0.1328125, 0.0, 0.984375, 0.0, 0.9921875, 0.4375, 0.984375, 0.0, 0.9921875, 0.984375, 0.4140625, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.4375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.4375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0625, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.53125, 0.9921875, 0.109375, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.7109375, 0.0, 0.0, 0.9921875, 0.984375, 0.9765625, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.984375, 0.984375, 0.0, 0.984375, 0.171875, 0.9921875, 0.9765625, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.9921875, 0.984375, 0.9921875, 0.9765625, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.4453125, 0.984375, 0.984375, 0.4375, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.375, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.28125, 0.984375, 0.0, 0.984375, 0.4375, 0.0, 0.984375, 0.984375, 0.9921875, 0.0, 0.984375, 0.9921875, 0.46875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.9765625, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.359375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.7734375, 0.5546875, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.53125, 0.9921875, 0.984375, 0.0, 0.0, 0.984375, 0.9765625, 0.578125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.9765625, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2890625, 0.0, 0.9921875, 0.984375, 0.90625, 0.984375, 0.0, 0.984375, 0.3359375, 0.0, 0.546875, 0.0, 0.9921875, 0.4375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.609375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.1640625, 0.9765625, 0.0, 0.984375, 0.984375, 0.0, 0.9921875, 0.984375, 0.9765625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.984375, 0.0, 0.0, 0.4375, 0.984375, 0.0, 0.984375, 0.0625, 0.9765625, 0.4375, 0.9765625, 0.0, 0.984375, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.3125, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9765625, 0.984375, 0.0, 0.0859375, 0.0, 0.984375, 0.984375, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.984375, 0.0, 0.6171875, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.4375, 0.5078125, 0.4375, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.4375, 0.9921875, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.984375, 0.0, 0.640625, 0.9765625, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.9765625, 0.984375, 0.984375, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.984375, 0.9921875, 0.0, 0.4375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.3203125, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.9921875, 0.4375, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.4296875, 0.9765625, 0.0, 0.0, 0.96875, 0.9921875, 0.9765625, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.9765625, 0.9765625, 0.9921875, 0.0, 0.0, 0.703125, 0.0, 0.0, 0.984375, 0.0, 0.21875, 0.0, 0.984375, 0.0, 0.3359375, 0.9921875, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.9921875, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.9921875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.96875, 0.984375, 0.984375, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0390625, 0.9296875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.63671875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0390625, 0.0, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0390625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0390625, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0]

 sparsity of   [0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.490234375, 0.0, 0.490234375, 0.99609375, 0.490234375, 0.490234375, 0.490234375, 0.490234375, 0.0, 0.490234375, 0.998046875, 0.99609375, 0.490234375, 0.490234375, 0.0, 0.0, 0.490234375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.490234375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.490234375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.490234375, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.490234375, 0.0, 0.490234375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.490234375, 0.998046875, 0.490234375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.490234375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.4765625, 0.0, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.490234375, 0.490234375, 0.99609375, 0.0, 0.0, 0.75390625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.490234375, 0.994140625, 0.0, 0.462890625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.490234375, 0.0, 0.998046875, 0.998046875, 0.0, 0.490234375, 0.99609375, 0.0, 0.998046875]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.4296875, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.4296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9982638955116272]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.515625, 0.515625, 0.9921875, 0.0, 0.984375, 0.984375, 0.34375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.515625, 0.0, 0.984375, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.515625, 0.0, 0.984375, 0.984375, 0.0, 0.9765625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.515625, 0.0, 0.984375, 0.0, 0.0, 0.515625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8984375, 0.3671875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.671875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.984375, 0.171875, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.9921875, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.9921875, 0.296875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.2109375, 0.0, 0.515625, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.0859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.9921875, 0.0, 0.1171875, 0.0, 0.9921875, 0.515625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1484375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.984375, 0.15625, 0.0, 0.0, 0.0, 0.515625, 0.984375, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.0703125, 0.0, 0.515625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.828125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.8515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.7109375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.984375, 0.9765625, 0.984375, 0.0, 0.5078125, 0.9921875, 0.90625, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.515625, 0.0, 0.515625, 0.515625, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.90625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.515625, 0.0, 0.984375, 0.0625, 0.0, 0.671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.984375, 0.0, 0.9765625, 0.9765625, 0.0, 0.515625, 0.0, 0.9765625, 0.0, 0.515625, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0859375, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1953125, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.1875, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.515625, 0.9921875, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.984375, 0.515625, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.671875, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.9921875, 0.0, 0.578125, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.1484375, 0.0, 0.515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.03125, 0.0, 0.998046875, 0.025390625, 0.0, 0.1328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13671875, 0.0, 0.0, 0.025390625, 0.0, 0.99609375, 0.0, 0.21875, 0.998046875, 0.0, 0.177734375, 0.0, 0.0, 0.0, 0.017578125, 0.15234375, 0.0, 0.0390625, 0.025390625, 0.0, 0.0, 0.9609375, 0.943359375, 0.02734375, 0.037109375, 0.0, 0.408203125, 0.0, 0.0, 0.0, 0.994140625, 0.208984375, 0.0, 0.833984375, 0.0, 0.0, 0.0, 0.064453125, 0.0, 0.0390625, 0.03515625, 0.994140625, 0.048828125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.169921875, 0.99609375, 0.99609375, 0.0, 0.111328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.03515625, 0.134765625, 0.994140625, 0.0, 0.994140625, 0.021484375, 0.0, 0.0, 0.0, 0.10546875, 0.015625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.07421875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.154296875, 0.99609375, 0.21875, 0.0, 0.0, 0.0, 0.201171875, 0.029296875, 0.99609375, 0.935546875, 0.998046875, 0.0, 0.03515625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375]

 sparsity of   [0.0651041641831398, 0.3819444477558136, 0.2005208283662796, 0.0, 0.0225694440305233, 0.0, 0.0, 0.0, 0.0, 0.1449652761220932, 0.0, 0.9973958134651184, 0.0, 0.0, 0.7821180820465088, 0.0911458358168602, 0.1206597238779068, 0.0173611119389534, 0.0, 0.02170138992369175, 0.0, 0.0303819440305233, 0.0, 0.0, 0.0234375, 0.02170138992369175, 0.0, 0.0, 0.1380208283662796, 0.0460069440305233, 0.0, 0.0, 0.0, 0.0694444477558136, 0.0390625, 0.02777777798473835, 0.0, 0.02170138992369175, 0.0, 0.01215277798473835, 0.0, 0.0347222238779068, 0.0, 0.8463541865348816, 0.780381977558136, 0.013020833022892475, 0.0, 0.1484375, 0.928819477558136, 0.0251736119389534, 0.0546875, 0.02690972201526165, 0.014756944961845875, 0.0, 0.0, 0.0, 0.5963541865348816, 0.0, 0.0, 0.0, 0.0338541679084301, 0.9973958134651184, 0.0338541679084301, 0.1310763955116272, 0.0, 0.0225694440305233, 0.8489583134651184, 0.0902777761220932, 0.0338541679084301, 0.0, 0.0, 0.999131977558136, 0.0164930559694767, 0.01822916604578495, 0.0590277798473835, 0.9973958134651184, 0.0234375, 0.5598958134651184, 0.0381944440305233, 0.1944444477558136, 0.009548611007630825, 0.5173611044883728, 0.0, 0.0928819477558136, 0.0, 0.2109375, 0.999131977558136, 0.0173611119389534, 0.0572916679084301, 0.9982638955116272, 0.0, 0.1154513880610466, 0.02604166604578495, 0.0347222238779068, 0.0407986119389534, 0.086805559694767, 0.0338541679084301, 0.0668402761220932, 0.0, 0.0, 0.0, 0.3671875, 0.0, 0.0, 0.9973958134651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02604166604578495, 0.0, 0.0234375, 0.01909722201526165, 0.0, 0.0, 0.0538194440305233, 0.01996527798473835, 0.1050347238779068, 0.0, 0.0164930559694767, 0.5442708134651184, 0.1032986119389534, 0.0, 0.02170138992369175, 0.02170138992369175]

 sparsity of   [0.5546875, 0.0, 0.0, 0.5546875, 0.0, 0.5546875, 0.0, 0.5546875, 0.5546875, 0.5546875, 0.5546875, 0.984375, 0.0, 0.0, 0.0, 0.65625, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.6796875, 0.0, 0.0, 0.484375, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1171875, 0.0, 0.0, 0.0390625, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5234375, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.546875, 0.0, 0.0, 0.9921875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.4765625, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.5546875, 0.984375, 0.5546875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.984375, 0.0, 0.5546875, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.5546875, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.5546875, 0.9765625, 0.984375, 0.0, 0.03125, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.5546875, 0.5546875, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.2265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.5546875, 0.984375, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.984375, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.5546875, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.984375, 0.5546875, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5546875, 0.0, 0.5546875, 0.0, 0.0]

 sparsity of   [0.0, 0.93359375, 0.0625, 0.0, 0.0, 0.06640625, 0.806640625, 0.037109375, 0.068359375, 0.0, 0.0, 0.02734375, 0.0, 0.99609375, 0.0, 0.009765625, 0.0, 0.994140625, 0.998046875, 0.99609375, 0.017578125, 0.0546875, 0.0, 0.00390625, 0.0, 0.0, 0.1328125, 0.0, 0.07421875, 0.060546875, 0.0, 0.0, 0.017578125, 0.029296875, 0.0, 0.0, 0.998046875, 0.05078125, 0.025390625, 0.0, 0.04296875, 0.94140625, 0.06640625, 0.02734375, 0.0, 0.0, 0.025390625, 0.310546875, 0.046875, 0.01953125, 0.99609375, 0.056640625, 0.037109375, 0.0, 0.048828125, 0.00390625, 0.0, 0.99609375, 0.083984375, 0.0, 0.947265625, 0.01953125, 0.08984375, 0.99609375, 0.83984375, 0.99609375, 0.0, 0.0, 0.015625, 0.03125, 0.0, 0.998046875, 0.02734375, 0.0, 0.0, 0.01171875, 0.994140625, 0.001953125, 0.017578125, 0.994140625, 0.0, 0.994140625, 0.068359375, 0.046875, 0.0, 0.05078125, 0.126953125, 0.0, 0.94921875, 0.0234375, 0.015625, 0.0, 0.0, 0.99609375, 0.263671875, 0.0078125, 0.1015625, 0.0, 0.0, 0.0, 0.03125, 0.021484375, 0.0, 0.02734375, 0.92578125, 0.05078125, 0.251953125, 0.02734375, 0.919921875, 0.0625, 0.880859375, 0.01953125, 0.0078125, 0.04296875, 0.0, 0.001953125, 0.0078125, 0.859375, 0.0, 0.0, 0.005859375, 0.060546875, 0.0, 0.0, 0.0703125, 0.013671875, 0.99609375, 0.759765625]

 sparsity of   [0.6440972089767456, 0.0, 0.819444477558136, 0.0434027798473835, 0.9982638955116272, 0.0173611119389534, 0.165798619389534, 0.0373263880610466, 0.9982638955116272, 0.0164930559694767, 0.0303819440305233, 0.0911458358168602, 0.440972238779068, 0.9973958134651184, 0.013888888992369175, 0.0, 0.0303819440305233, 0.0234375, 0.0590277798473835, 0.999131977558136, 0.0746527761220932, 0.9982638955116272, 0.0, 0.0, 0.01215277798473835, 0.1197916641831398, 0.4418402910232544, 0.0, 0.4322916567325592, 0.0321180559694767, 0.0894097238779068, 0.1328125, 0.01822916604578495, 0.9583333134651184, 0.7135416865348816, 0.9982638955116272, 0.0078125, 0.2239583283662796, 0.9348958134651184, 0.0164930559694767, 0.9982638955116272, 0.9982638955116272, 0.02170138992369175, 0.9973958134651184, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0720486119389534, 0.02170138992369175, 0.03125, 0.999131977558136, 0.0, 0.3220486044883728, 0.02777777798473835, 0.1467013955116272, 0.0234375, 0.999131977558136, 0.1119791641831398, 0.9982638955116272, 0.9982638955116272, 0.02170138992369175, 0.9982638955116272, 0.0546875, 0.0, 0.0329861119389534, 0.0876736119389534, 0.9001736044883728, 0.9982638955116272, 0.0173611119389534, 0.9973958134651184, 0.0251736119389534, 0.999131977558136, 0.0, 0.0173611119389534, 0.1935763955116272, 0.9982638955116272, 0.0173611119389534, 0.0164930559694767, 0.0, 0.0520833320915699, 0.999131977558136, 0.03125, 0.0902777761220932, 0.0512152798473835, 0.0407986119389534, 0.999131977558136, 0.02170138992369175, 0.9270833134651184, 0.4852430522441864, 0.9973958134651184, 0.0425347238779068, 0.0555555559694767, 0.7864583134651184, 0.02170138992369175, 0.02690972201526165, 0.9713541865348816, 0.9982638955116272, 0.261284738779068, 0.0303819440305233, 0.0607638880610466, 0.2595486044883728, 0.01822916604578495, 0.0373263880610466, 0.0920138880610466, 0.0234375, 0.02690972201526165, 0.01822916604578495, 0.013888888992369175, 0.0651041641831398, 0.8229166865348816, 0.4835069477558136, 0.0, 0.0685763880610466, 0.9982638955116272, 0.0243055559694767, 0.9982638955116272, 0.0173611119389534, 0.5034722089767456, 0.0, 0.1449652761220932, 0.0, 0.0399305559694767, 0.02170138992369175, 0.9982638955116272, 0.2873263955116272, 0.9401041865348816, 0.02604166604578495, 0.0486111119389534]

 sparsity of   [0.8359375, 0.0, 0.8515625, 0.8203125, 0.0, 0.734375, 0.8359375, 0.8359375, 0.8359375, 0.125, 0.828125, 0.0, 0.8125, 0.828125, 0.0, 0.9921875, 0.0, 0.8828125, 0.0, 0.0, 0.8828125, 0.8359375, 0.8125, 0.8515625, 0.8828125, 0.828125, 0.8828125, 0.8046875, 0.1171875, 0.859375, 0.09375, 0.8203125, 0.8828125, 0.8125, 0.875, 0.8359375, 0.0, 0.8828125, 0.8828125, 0.0, 0.8046875, 0.828125, 0.8359375, 0.0, 0.8203125, 0.0, 0.0, 0.828125, 0.8671875, 0.8203125, 0.0, 0.96875, 0.1328125, 0.0, 0.0, 0.828125, 0.8828125, 0.984375, 0.109375, 0.8203125, 0.921875, 0.8203125, 0.8359375, 0.8828125, 0.8203125, 0.8203125, 0.8671875, 0.8125, 0.828125, 0.8203125, 0.0078125, 0.0, 0.828125, 0.875, 0.609375, 0.7734375, 0.0, 0.828125, 0.0, 0.8828125, 0.6484375, 0.7578125, 0.0, 0.796875, 0.84375, 0.0, 0.8828125, 0.0, 0.0, 0.0, 0.8515625, 0.8359375, 0.0, 0.8828125, 0.0, 0.8828125, 0.828125, 0.0, 0.0625, 0.859375, 0.0, 0.8828125, 0.1171875, 0.859375, 0.828125, 0.8828125, 0.0, 0.421875, 0.8828125, 0.8671875, 0.6015625, 0.828125, 0.0, 0.8828125, 0.8203125, 0.828125, 0.8359375, 0.8125, 0.8359375, 0.828125, 0.828125, 0.0, 0.8828125, 0.0390625, 0.8125, 0.828125, 0.8515625, 0.8125, 0.9921875, 0.828125, 0.828125, 0.109375, 0.828125, 0.390625, 0.34375, 0.0, 0.8515625, 0.8046875, 0.8359375, 0.796875, 0.875, 0.8359375, 0.0, 0.8515625, 0.109375, 0.875, 0.9921875, 0.8125, 0.8203125, 0.8203125, 0.8828125, 0.8828125, 0.8828125, 0.828125, 0.828125, 0.8203125, 0.8515625, 0.0, 0.8828125, 0.8359375, 0.8359375, 0.0, 0.796875, 0.6796875, 0.0, 0.0, 0.0703125, 0.8203125, 0.828125, 0.8125, 0.8828125, 0.8828125, 0.8046875, 0.0, 0.0, 0.15625, 0.828125, 0.8828125, 0.8828125, 0.8203125, 0.8359375, 0.828125, 0.8828125, 0.8828125, 0.8359375, 0.0859375, 0.0, 0.0, 0.8828125, 0.1171875, 0.0, 0.609375, 0.8671875, 0.875, 0.0, 0.8828125, 0.984375, 0.8203125, 0.0703125, 0.84375, 0.8125, 0.5234375, 0.8046875, 0.6640625, 0.8828125, 0.0, 0.796875, 0.828125, 0.8828125, 0.0, 0.8046875, 0.828125, 0.8359375, 0.4140625, 0.828125, 0.0, 0.875, 0.0, 0.8203125, 0.8125, 0.0, 0.8203125, 0.828125, 0.8359375, 0.8671875, 0.8828125, 0.8828125, 0.0, 0.8203125, 0.859375, 0.8203125, 0.2890625, 0.8203125, 0.0, 0.84375, 0.09375, 0.0703125, 0.078125, 0.8359375, 0.0546875, 0.0, 0.0, 0.8359375, 0.84375, 0.171875, 0.828125, 0.875, 0.8125, 0.875, 0.0625, 0.765625, 0.8359375, 0.8046875, 0.8359375, 0.8828125, 0.8359375, 0.875, 0.8125, 0.8828125, 0.9921875, 0.03125, 0.8046875, 0.8828125, 0.0, 0.7578125, 0.0546875, 0.8125, 0.8359375, 0.8828125, 0.8125, 0.8828125, 0.828125, 0.8828125, 0.8671875, 0.8125, 0.8671875, 0.875, 0.828125, 0.0, 0.7890625, 0.109375, 0.828125, 0.859375, 0.875, 0.0, 0.8359375, 0.0, 0.8828125, 0.8203125, 0.046875, 0.8359375, 0.828125, 0.84375, 0.84375, 0.8359375, 0.0, 0.8828125, 0.84375, 0.875, 0.8046875, 0.84375, 0.0546875, 0.0, 0.8828125, 0.84375, 0.8828125, 0.8828125, 0.0, 0.8828125, 0.8515625, 0.84375, 0.8828125, 0.78125, 0.8203125, 0.0703125, 0.8828125, 0.1953125, 0.828125, 0.8359375, 0.8359375, 0.84375, 0.1171875, 0.8828125, 0.8828125, 0.0, 0.40625, 0.8359375, 0.8828125, 0.828125, 0.0, 0.0, 0.8125, 0.0, 0.0, 0.0, 0.9921875, 0.296875, 0.828125, 0.8828125, 0.109375, 0.84375, 0.8828125, 0.8515625, 0.8359375, 0.0, 0.0703125, 0.8828125, 0.8828125, 0.8828125, 0.0, 0.8828125, 0.0859375, 0.8515625, 0.8203125, 0.84375, 0.828125, 0.046875, 0.0, 0.859375, 0.0, 0.8046875, 0.8125, 0.8828125, 0.8671875, 0.875, 0.8828125, 0.8203125, 0.15625, 0.0, 0.8515625, 0.0, 0.0, 0.875, 0.8828125, 0.8359375, 0.8203125, 0.8828125, 0.875, 0.3671875, 0.8359375, 0.8828125, 0.84375, 0.859375, 0.0, 0.8828125, 0.828125, 0.0, 0.8515625, 0.0, 0.0, 0.8046875, 0.84375, 0.84375, 0.8359375, 0.046875, 0.828125, 0.8515625, 0.125, 0.828125, 0.8203125, 0.0, 0.7578125, 0.8125, 0.828125, 0.8203125, 0.828125, 0.8359375, 0.828125, 0.0, 0.828125, 0.8515625, 0.984375, 0.8125, 0.8203125, 0.8828125, 0.0, 0.8828125, 0.0, 0.3515625, 0.6484375, 0.0, 0.8828125, 0.0, 0.0, 0.8828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8515625, 0.8203125, 0.8125, 0.0, 0.828125, 0.0, 0.8828125, 0.8828125, 0.8828125, 0.8828125, 0.8828125, 0.0, 0.8359375, 0.8359375, 0.8203125, 0.8828125, 0.0, 0.8828125, 0.0, 0.8203125, 0.0, 0.8828125, 0.8828125, 0.828125, 0.0, 0.8828125, 0.0, 0.859375, 0.84375, 0.8828125, 0.8828125, 0.8828125, 0.828125, 0.0, 0.828125, 0.8515625, 0.140625, 0.828125, 0.8359375, 0.8125, 0.1015625, 0.8359375, 0.828125, 0.8671875, 0.8828125, 0.0, 0.265625, 0.828125, 0.8828125, 0.8203125, 0.3515625, 0.84375, 0.0, 0.84375, 0.0, 0.8671875, 0.0, 0.8828125, 0.828125, 0.8203125, 0.828125, 0.828125, 0.8828125, 0.765625, 0.0, 0.8203125, 0.8828125, 0.78125, 0.8828125, 0.8828125, 0.0, 0.828125, 0.8828125, 0.8359375, 0.8671875, 0.8828125, 0.828125, 0.84375, 0.8203125, 0.0, 0.84375, 0.84375]

 sparsity of   [0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.998046875, 0.015625, 0.99609375, 0.015625, 0.015625, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.0, 0.0, 0.015625, 0.99609375, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.015625, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.015625, 0.015625, 0.0, 0.99609375, 0.0, 0.015625, 0.0, 0.99609375, 0.0, 0.998046875, 0.015625, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.015625, 0.0, 0.015625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.013671875, 0.0, 0.99609375, 0.015625, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.015625, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.015625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.015625, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.01171875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.013671875, 0.013671875, 0.99609375, 0.0, 0.015625, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.015625, 0.015625, 0.99609375, 0.99609375, 0.994140625, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.015625, 0.99609375, 0.015625, 0.99609375, 0.998046875, 0.99609375, 0.015625, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.015625, 0.013671875, 0.0078125, 0.015625, 0.013671875, 0.0, 0.015625, 0.998046875, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375]

 sparsity of   [0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.1323784738779068, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4045138955116272, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1271701455116272, 0.999131977558136, 0.292534738779068, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.578125, 0.9995659589767456, 0.578125, 0.57421875, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.9995659589767456, 0.0, 0.578125, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.578125, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.578125, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.578125, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.2955729067325592, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.578125, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.578125, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.578125, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.5755208134651184, 0.999131977558136, 0.9995659589767456, 0.2756076455116272, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.5746527910232544, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.578125, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.578125, 0.0, 0.999131977558136, 0.0, 0.5746527910232544, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.578125, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.578125, 0.999131977558136, 0.9995659589767456, 0.578125, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.578125]

 sparsity of   [0.0, 0.0, 0.0, 0.734375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.734375, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.0, 0.98828125, 0.9921875, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.9921875, 0.9921875, 0.0, 0.734375, 0.734375, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.0, 0.734375, 0.58984375, 0.9921875, 0.734375, 0.8828125, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.734375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.734375, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.35546875, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.9921875, 0.734375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.734375, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.734375, 0.9921875, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.69140625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.734375, 0.99609375, 0.98828125, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.734375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.734375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.734375, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.734375, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.734375, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.734375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.734375, 0.734375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.734375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.98828125, 0.98828125, 0.0, 0.0, 0.734375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.99609375, 0.0, 0.98828125, 0.0, 0.9921875, 0.0, 0.734375, 0.99609375, 0.8828125, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.734375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.734375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.734375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.734375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.734375, 0.0, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.40625, 0.0, 0.0, 0.20703125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.890625, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.734375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.55859375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.0, 0.49609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.734375, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.734375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.3671875, 0.734375, 0.0, 0.76953125, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.98828125, 0.35546875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.98828125, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.734375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.734375, 0.734375, 0.9921875, 0.9921875, 0.0, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.734375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.734375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.734375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.734375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.734375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.44140625, 0.0, 0.0, 0.0, 0.9921875]

 sparsity of   [0.0, 0.013671875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.01171875, 0.998046875, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.0859375, 0.0, 0.0, 0.9921875, 0.09375, 0.99609375, 0.99609375, 0.0078125, 0.994140625, 0.99609375, 0.0, 0.138671875, 0.99609375, 0.0, 0.0, 0.0078125, 0.0, 0.99609375, 0.0, 0.013671875, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.05859375, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.994140625, 0.140625, 0.10546875, 0.0, 0.0, 0.009765625, 0.998046875, 0.99609375, 0.0, 0.0, 0.013671875, 0.212890625, 0.0, 0.99609375, 0.99609375, 0.0, 0.005859375, 0.74609375, 0.99609375, 0.005859375, 0.39453125, 0.21484375, 0.99609375, 0.0, 0.14453125, 0.373046875, 0.095703125, 0.064453125, 0.0, 0.0, 0.099609375, 0.0, 0.99609375, 0.896484375, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.271484375, 0.125, 0.0, 0.998046875, 0.0703125, 0.0, 0.46875, 0.99609375, 0.251953125, 0.0, 0.0, 0.056640625, 0.0, 0.193359375, 0.033203125, 0.99609375, 0.064453125, 0.0, 0.001953125, 0.0, 0.0, 0.55078125, 0.01171875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.009765625, 0.041015625, 0.994140625, 0.99609375, 0.015625, 0.35546875, 0.0, 0.0, 0.470703125, 0.00390625, 0.0, 0.0, 0.99609375, 0.1015625, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.337890625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.068359375, 0.99609375, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.640625, 0.00390625, 0.0, 0.998046875, 0.0, 0.07421875, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.01171875, 0.0, 0.328125, 0.0, 0.255859375, 0.015625, 0.201171875, 0.033203125, 0.994140625, 0.0, 0.0, 0.001953125, 0.994140625, 0.0, 0.998046875, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.17578125, 0.0, 0.005859375, 0.00390625, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.34375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.28125, 0.0625, 0.203125, 0.0, 0.0, 0.0, 0.99609375, 0.056640625, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.01171875, 0.828125, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.876953125, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.060546875, 0.142578125, 0.998046875, 0.0, 0.99609375, 0.0, 0.130859375, 0.998046875, 0.0, 0.0, 0.99609375, 0.998046875, 0.013671875, 0.99609375, 0.0, 0.62890625, 0.12890625, 0.99609375, 0.99609375, 0.11328125, 0.0, 0.0, 0.01171875, 0.0, 0.0703125, 0.0, 0.99609375, 0.056640625, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.9921875, 0.994140625, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.076171875, 0.0, 0.99609375, 0.001953125, 0.99609375, 0.0, 0.0, 0.0, 0.267578125, 0.0, 0.015625, 0.99609375, 0.0, 0.19921875, 0.0, 0.998046875, 0.0546875, 0.24609375, 0.0703125, 0.994140625, 0.99609375, 0.994140625, 0.0, 0.044921875, 0.0, 0.99609375, 0.041015625, 0.994140625, 0.99609375, 0.013671875, 0.994140625, 0.07421875, 0.0, 0.99609375, 0.48828125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.005859375, 0.994140625, 0.16015625, 0.0, 0.005859375, 0.0, 0.3359375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.111328125, 0.271484375, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.892578125, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.013671875, 0.99609375, 0.0, 0.0, 0.263671875, 0.99609375, 0.0, 0.369140625, 0.0, 0.0, 0.001953125, 0.25390625, 0.0, 0.0, 0.99609375, 0.0, 0.845703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.568359375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.33984375, 0.3984375, 0.0, 0.0, 0.1953125, 0.0, 0.0, 0.994140625, 0.052734375, 0.0, 0.0, 0.99609375, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.0859375, 0.0, 0.87109375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.99609375, 0.220703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.015625, 0.998046875, 0.087890625, 0.07421875, 0.07421875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0703125, 0.458984375, 0.998046875, 0.009765625, 0.998046875, 0.0, 0.0, 0.998046875, 0.994140625, 0.0, 0.99609375, 0.0, 0.001953125, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.056640625, 0.998046875, 0.0, 0.998046875, 0.994140625, 0.78125, 0.0703125, 0.0, 0.04296875, 0.994140625, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.015625, 0.99609375, 0.0, 0.349609375, 0.0, 0.0, 0.08203125, 0.0703125, 0.0, 0.2109375, 0.994140625, 0.0, 0.005859375, 0.0, 0.0, 0.4296875, 0.0, 0.0, 0.46484375, 0.9140625, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.005859375, 0.99609375, 0.154296875, 0.224609375, 0.0, 0.87109375, 0.0, 0.0, 0.0, 0.0, 0.123046875, 0.0, 0.841796875, 0.994140625, 0.0, 0.0, 0.015625, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.009765625, 0.0, 0.99609375, 0.0, 0.998046875, 0.125, 0.9921875, 0.130859375, 0.98828125, 0.0, 0.140625, 0.109375, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.12890625, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0234375, 0.080078125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.015625, 0.0, 0.00390625, 0.0, 0.998046875, 0.0, 0.0546875, 0.998046875, 0.115234375, 0.015625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.994140625, 0.998046875, 0.99609375, 0.087890625, 0.87109375, 0.0, 0.333984375, 0.015625, 0.453125, 0.99609375, 0.0, 0.150390625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.994140625, 0.994140625, 0.126953125, 0.994140625, 0.0078125, 0.994140625, 0.0, 0.99609375, 0.0, 0.3671875, 0.99609375, 0.0, 0.994140625, 0.99609375, 0.0, 0.0, 0.998046875, 0.9921875, 0.0, 0.00390625, 0.998046875, 0.0625, 0.232421875, 0.99609375, 0.34765625, 0.146484375, 0.0, 0.0, 0.0, 0.998046875, 0.037109375, 0.0, 0.0, 0.861328125, 0.0, 0.0, 0.146484375, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.3046875, 0.1015625, 0.0, 0.013671875, 0.119140625, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.005859375, 0.0, 0.0078125, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.06640625, 0.076171875, 0.99609375, 0.0, 0.224609375, 0.994140625, 0.994140625, 0.0, 0.861328125, 0.0, 0.33203125, 0.11328125, 0.00390625, 0.998046875, 0.1875, 0.998046875, 0.03515625, 0.794921875, 0.015625, 0.99609375, 0.0, 0.0, 0.123046875, 0.994140625, 0.908203125, 0.99609375, 0.064453125, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.998046875, 0.107421875, 0.99609375, 0.013671875, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.408203125, 0.0, 0.794921875, 0.0, 0.0, 0.0546875, 0.09375, 0.0, 0.0, 0.99609375, 0.0, 0.0625, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.802734375, 0.99609375, 0.0078125, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.107421875, 0.0703125, 0.0, 0.0, 0.0, 0.287109375, 0.076171875, 0.0, 0.0, 0.22265625, 0.99609375, 0.99609375, 0.29296875, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.103515625, 0.7734375, 0.01171875, 0.0, 0.88671875, 0.10546875, 0.994140625, 0.0, 0.41796875, 0.078125, 0.00390625, 0.0, 0.365234375, 0.99609375, 0.0, 0.0, 0.0, 0.244140625, 0.08203125, 0.998046875, 0.99609375, 0.005859375, 0.99609375, 0.826171875, 0.994140625, 0.0, 0.052734375, 0.0, 0.00390625, 0.99609375, 0.900390625, 0.99609375, 0.0, 0.99609375, 0.013671875, 0.072265625, 0.0, 0.02734375, 0.0859375, 0.0, 0.013671875, 0.994140625, 0.994140625, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.04296875, 0.015625, 0.0, 0.001953125, 0.009765625, 0.0, 0.1328125, 0.22265625, 0.994140625, 0.998046875, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.15234375, 0.0, 0.99609375, 0.994140625, 0.998046875, 0.083984375, 0.0078125, 0.0, 0.056640625, 0.998046875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.048828125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.326171875, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.998046875, 0.0, 0.0, 0.994140625, 0.99609375, 0.064453125, 0.99609375, 0.99609375, 0.998046875, 0.80859375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.994140625, 0.0, 0.34765625, 0.4453125, 0.072265625, 0.0, 0.0, 0.083984375, 0.0, 0.314453125, 0.0, 0.0, 0.005859375, 0.0390625, 0.5546875, 0.998046875, 0.998046875, 0.47265625, 0.99609375, 0.0, 0.99609375, 0.98046875, 0.341796875, 0.154296875, 0.005859375, 0.015625, 0.0, 0.083984375, 0.08984375, 0.0, 0.99609375, 0.255859375, 0.99609375, 0.8046875, 0.0, 0.01171875, 0.041015625, 0.166015625, 0.33984375, 0.0, 0.99609375, 0.0, 0.3671875, 0.99609375, 0.0, 0.431640625, 0.0, 0.99609375, 0.99609375, 0.251953125, 0.0, 0.0, 0.013671875, 0.0, 0.791015625, 0.013671875, 0.287109375, 0.0, 0.138671875, 0.0, 0.998046875, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.123046875, 0.001953125, 0.99609375, 0.994140625, 0.998046875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.044921875, 0.0, 0.02734375, 0.99609375, 0.05859375, 0.130859375, 0.0625, 0.0, 0.0, 0.99609375, 0.05859375, 0.0, 0.0546875, 0.0, 0.005859375, 0.99609375, 0.0, 0.0, 0.208984375, 0.83203125, 0.69921875, 0.998046875, 0.2421875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.203125, 0.013671875, 0.998046875, 0.994140625, 0.25390625, 0.0, 0.38671875, 0.330078125, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.04296875, 0.99609375, 0.0, 0.998046875, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.0, 0.076171875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.01171875, 0.798828125]

 sparsity of   [0.998046875, 0.0693359375, 0.9970703125, 0.998046875, 0.4521484375, 0.0, 0.998046875, 0.0, 0.998046875, 0.0244140625, 0.1494140625, 0.0, 0.1201171875, 0.05078125, 0.998046875, 0.998046875, 0.0712890625, 0.998046875, 0.0947265625, 0.9970703125, 0.1025390625, 0.0, 0.4755859375, 0.4833984375, 0.998046875, 0.17578125, 0.123046875, 0.9970703125, 0.998046875, 0.0, 0.9970703125, 0.0595703125, 0.0, 0.998046875, 0.0, 0.998046875, 0.0869140625, 0.998046875, 0.0224609375, 0.0, 0.4833984375, 0.1328125, 0.041015625, 0.44140625, 0.998046875, 0.998046875, 0.0, 0.0, 0.2080078125, 0.220703125, 0.4267578125, 0.998046875, 0.998046875, 0.0546875, 0.9990234375, 0.0, 0.9970703125, 0.998046875, 0.0, 0.068359375, 0.998046875, 0.0380859375, 0.0, 0.484375, 0.998046875, 0.07421875, 0.45703125, 0.0, 0.4609375, 0.0, 0.0, 0.0361328125, 0.1005859375, 0.0, 0.0703125, 0.779296875, 0.9970703125, 0.0849609375, 0.484375, 0.998046875, 0.484375, 0.9990234375, 0.998046875, 0.3125, 0.033203125, 0.0, 0.48046875, 0.0576171875, 0.0322265625, 0.103515625, 0.0751953125, 0.08203125, 0.998046875, 0.998046875, 0.267578125, 0.0, 0.998046875, 0.0, 0.0, 0.9990234375, 0.998046875, 0.48046875, 0.4716796875, 0.998046875, 0.896484375, 0.1474609375, 0.9970703125, 0.9970703125, 0.0322265625, 0.998046875, 0.228515625, 0.02734375, 0.19921875, 0.103515625, 0.330078125, 0.19140625, 0.0380859375, 0.0, 0.404296875, 0.1875, 0.111328125, 0.998046875, 0.9990234375, 0.998046875, 0.0, 0.0546875, 0.998046875, 0.0478515625, 0.998046875, 0.9990234375, 0.0908203125, 0.228515625, 0.998046875, 0.466796875, 0.998046875, 0.998046875, 0.484375, 0.0400390625, 0.998046875, 0.4833984375, 0.998046875, 0.1298828125, 0.0, 0.998046875, 0.9990234375, 0.0, 0.9990234375, 0.0419921875, 0.0673828125, 0.9990234375, 0.046875, 0.0, 0.9990234375, 0.998046875, 0.9970703125, 0.0322265625, 0.07421875, 0.99609375, 0.998046875, 0.05859375, 0.201171875, 0.0, 0.998046875, 0.484375, 0.0966796875, 0.998046875, 0.0, 0.9970703125, 0.0, 0.484375, 0.408203125, 0.998046875, 0.24609375, 0.2509765625, 0.1572265625, 0.0, 0.9970703125, 0.0927734375, 0.0, 0.9990234375, 0.8818359375, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.48046875, 0.0439453125, 0.4248046875, 0.9990234375, 0.3681640625, 0.275390625, 0.0498046875, 0.0673828125, 0.998046875, 0.998046875, 0.0, 0.80859375, 0.021484375, 0.0, 0.173828125, 0.3984375, 0.482421875, 0.02734375, 0.4814453125, 0.9990234375, 0.4833984375, 0.4140625, 0.0, 0.0, 0.9990234375, 0.998046875, 0.998046875, 0.0673828125, 0.05078125, 0.4833984375, 0.453125, 0.9990234375, 0.9970703125, 0.0, 0.998046875, 0.806640625, 0.998046875, 0.8671875, 0.0, 0.484375, 0.0380859375, 0.0625, 0.37109375, 0.9990234375, 0.998046875, 0.0380859375, 0.0, 0.0, 0.4833984375, 0.0390625, 0.9970703125, 0.4013671875, 0.998046875, 0.427734375, 0.150390625, 0.0556640625, 0.7333984375, 0.470703125, 0.3857421875, 0.462890625, 0.46484375, 0.998046875, 0.482421875, 0.1025390625, 0.0, 0.0625, 0.998046875, 0.4599609375, 0.126953125, 0.9990234375]

 sparsity of   [0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.35546875, 0.9995659589767456, 0.3363715410232544, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.0772569477558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.5807291865348816, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.5325520634651184, 0.1527777761220932, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.1046006977558136, 0.999131977558136, 0.1197916641831398, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.0, 0.59765625, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3350694477558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9986979365348816, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0920138880610466, 0.0, 0.9995659589767456, 0.1809895783662796, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.3324652910232544, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136]

 sparsity of   [0.0, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.8671875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.8671875, 0.0, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.8671875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.8671875, 0.8671875, 0.8671875, 0.0, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.0, 0.9921875, 0.8671875, 0.8671875, 0.8671875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.8671875, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.0, 0.8671875, 0.99609375, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.8671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.8671875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.8671875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.8671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.8671875, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.8671875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.8671875, 0.99609375, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.99609375, 0.0, 0.0, 0.8671875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.8671875, 0.0, 0.8671875, 0.8671875, 0.8671875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.8671875, 0.0, 0.8671875, 0.0, 0.8671875, 0.8671875, 0.0, 0.8671875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.8671875, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.8671875, 0.8671875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.0, 0.8671875, 0.8671875, 0.8671875, 0.9921875, 0.86328125, 0.0, 0.0, 0.0, 0.9921875, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.8671875, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.0, 0.9921875, 0.8671875, 0.0, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.8671875, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.8671875, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.8671875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.8671875, 0.99609375, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.8671875, 0.9921875, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.8671875, 0.9921875, 0.8671875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.8671875, 0.8671875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.98828125, 0.8671875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.86328125, 0.8671875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.8671875, 0.8671875, 0.0, 0.0, 0.8671875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.8671875, 0.0, 0.99609375, 0.8671875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.8671875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.8671875, 0.9921875]

 sparsity of   [0.998046875, 0.9990234375, 0.0654296875, 0.998046875, 0.9990234375, 0.0927734375, 0.0, 0.38671875, 0.9990234375, 0.9970703125, 0.0, 0.373046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.9970703125, 0.3720703125, 0.0, 0.0, 0.1748046875, 0.9990234375, 0.0, 0.0, 0.0, 0.998046875, 0.1171875, 0.0859375, 0.9990234375, 0.998046875, 0.38671875, 0.998046875, 0.0634765625, 0.0, 0.9990234375, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.998046875, 0.130859375, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.8642578125, 0.0, 0.998046875, 0.0546875, 0.0, 0.0, 0.998046875, 0.0, 0.4365234375, 0.0, 0.998046875, 0.068359375, 0.0, 0.369140625, 0.38671875, 0.0, 0.998046875, 0.998046875, 0.0, 0.9970703125, 0.998046875, 0.0, 0.0810546875, 0.9990234375, 0.9990234375, 0.9990234375, 0.38671875, 0.0, 0.38671875, 0.0, 0.06640625, 0.998046875, 0.0, 0.0, 0.998046875, 0.998046875, 0.05078125, 0.0, 0.0, 0.998046875, 0.375, 0.0, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.90234375, 0.0, 0.0, 0.3056640625, 0.998046875, 0.423828125, 0.3046875, 0.9990234375, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.2451171875, 0.0966796875, 0.0771484375, 0.9990234375, 0.3662109375, 0.9990234375, 0.0458984375, 0.0, 0.0, 0.1533203125, 0.0, 0.2197265625, 0.9990234375, 0.0302734375, 0.9970703125, 0.9990234375, 0.044921875, 0.0, 0.998046875, 0.0, 0.3447265625, 0.998046875, 0.3447265625, 0.998046875, 0.998046875, 0.341796875, 0.998046875, 0.056640625, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.3095703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9296875, 0.04296875, 0.0, 0.0, 0.1025390625, 0.9990234375, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.333984375, 0.3642578125, 0.9990234375, 0.9990234375, 0.0537109375, 0.38671875, 0.912109375, 0.373046875, 0.0, 0.998046875, 0.998046875, 0.37890625, 0.0, 0.380859375, 0.998046875, 0.32421875, 0.998046875, 0.1220703125, 0.9306640625, 0.0, 0.0, 0.0, 0.998046875, 0.1357421875, 0.9970703125, 0.38671875, 0.9990234375, 0.9970703125, 0.0, 0.998046875, 0.3662109375, 0.998046875, 0.38671875, 0.998046875, 0.998046875, 0.0, 0.38671875, 0.9990234375, 0.9970703125, 0.0, 0.0869140625, 0.0, 0.326171875, 0.998046875, 0.0771484375, 0.3115234375, 0.9990234375, 0.9990234375, 0.998046875, 0.0, 0.9990234375, 0.1669921875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.07421875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.3125, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.9990234375, 0.0, 0.0, 0.0146484375, 0.14453125, 0.9990234375, 0.3857421875, 0.9990234375, 0.12109375, 0.998046875, 0.0, 0.0, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.3857421875, 0.0, 0.0322265625, 0.0, 0.36328125]

 sparsity of   [0.8185763955116272, 0.0972222238779068, 0.0, 0.0, 0.0086805559694767, 0.01605902798473835, 0.0, 0.01171875, 0.0394965298473835, 0.01519097201526165, 0.02560763992369175, 0.0164930559694767, 0.0698784738779068, 0.1115451380610466, 0.0, 0.0373263880610466, 0.014756944961845875, 0.1375868022441864, 0.0520833320915699, 0.0876736119389534, 0.02994791604578495, 0.0598958320915699, 0.0052083334885537624, 0.0785590261220932, 0.999131977558136, 0.1302083283662796, 0.0321180559694767, 0.01215277798473835, 0.0, 0.193142369389534, 0.010416666977107525, 0.01953125, 0.0403645820915699, 0.02560763992369175, 0.772569477558136, 0.77734375, 0.9743923544883728, 0.013020833022892475, 0.0481770820915699, 0.01909722201526165, 0.0407986119389534, 0.009548611007630825, 0.0, 0.0173611119389534, 0.0, 0.0455729179084301, 0.0, 0.803819477558136, 0.0, 0.0499131940305233, 0.999131977558136, 0.0381944440305233, 0.02387152798473835, 0.014756944961845875, 0.0125868059694767, 0.0425347238779068, 0.130642369389534, 0.9986979365348816, 0.0, 0.1072048619389534, 0.663194477558136, 0.02734375, 0.0998263880610466, 0.0611979179084301, 0.082899309694767, 0.0, 0.0164930559694767, 0.3671875, 0.0225694440305233, 0.232204869389534, 0.0, 0.9995659589767456, 0.0, 0.010850694961845875, 0.0, 0.0, 0.0516493059694767, 0.9513888955116272, 0.0, 0.02213541604578495, 0.01128472201526165, 0.5646701455116272, 0.0425347238779068, 0.14453125, 0.0, 0.0264756940305233, 0.9270833134651184, 0.014322916977107525, 0.01519097201526165, 0.0, 0.02387152798473835, 0.01171875, 0.0, 0.0473090298473835, 0.01822916604578495, 0.9986979365348816, 0.0807291641831398, 0.9986979365348816, 0.0364583320915699, 0.02387152798473835, 0.02777777798473835, 0.8224826455116272, 0.0, 0.0, 0.0, 0.0, 0.9466145634651184, 0.1241319477558136, 0.0846354141831398, 0.1714409738779068, 0.1766493022441864, 0.01519097201526165, 0.0321180559694767, 0.05859375, 0.02560763992369175, 0.0, 0.6015625, 0.02734375, 0.0642361119389534, 0.0, 0.02387152798473835, 0.01215277798473835, 0.0902777761220932, 0.0, 0.014756944961845875, 0.009114583022892475, 0.01822916604578495, 0.0, 0.1467013955116272, 0.0, 0.01519097201526165, 0.0, 0.0234375, 0.9995659589767456, 0.8932291865348816, 0.015625, 0.1115451380610466, 0.02604166604578495, 0.0, 0.0, 0.0, 0.01519097201526165, 0.009548611007630825, 0.0, 0.0342881940305233, 0.0, 0.0316840298473835, 0.8793402910232544, 0.0, 0.03081597201526165, 0.01128472201526165, 0.02560763992369175, 0.0203993059694767, 0.901475727558136, 0.010416666977107525, 0.01128472201526165, 0.014322916977107525, 0.0, 0.0373263880610466, 0.4861111044883728, 0.6623263955116272, 0.0, 0.0243055559694767, 0.0, 0.02951388992369175, 0.0, 0.12109375, 0.0564236119389534, 0.12109375, 0.7630208134651184, 0.0616319440305233, 0.0551215298473835, 0.9544270634651184, 0.01996527798473835, 0.93359375, 0.0, 0.9995659589767456, 0.0282118059694767, 0.3207465410232544, 0.0316840298473835, 0.0, 0.9253472089767456, 0.09375, 0.02170138992369175, 0.0, 0.999131977558136, 0.0993923619389534, 0.0, 0.0, 0.0212673619389534, 0.0529513880610466, 0.0, 0.0334201380610466, 0.0, 0.02864583395421505, 0.1922743022441864, 0.014756944961845875, 0.7005208134651184, 0.1189236119389534, 0.013020833022892475, 0.02734375, 0.014756944961845875, 0.0551215298473835, 0.01519097201526165, 0.0494791679084301, 0.0733506977558136, 0.013888888992369175, 0.0, 0.03515625, 0.03515625, 0.02560763992369175, 0.0, 0.1115451380610466, 0.9032118320465088, 0.1232638880610466, 0.0334201380610466, 0.3806423544883728, 0.0, 0.0559895820915699, 0.0347222238779068, 0.0282118059694767, 0.0, 0.0, 0.010850694961845875, 0.0212673619389534, 0.9118923544883728, 0.1293402761220932, 0.014756944961845875, 0.745225727558136, 0.01822916604578495, 0.0, 0.006076388992369175, 0.02994791604578495, 0.8806423544883728, 0.01996527798473835, 0.0164930559694767, 0.9995659589767456, 0.0772569477558136, 0.0225694440305233, 0.01519097201526165, 0.5230034589767456, 0.0911458358168602, 0.013888888992369175, 0.0442708320915699, 0.0, 0.9995659589767456, 0.0212673619389534, 0.01128472201526165, 0.0360243059694767, 0.0963541641831398, 0.0815972238779068, 0.02300347201526165, 0.01692708395421505, 0.9357638955116272, 0.0086805559694767, 0.0572916679084301]

 sparsity of   [0.0, 0.78515625, 0.0, 0.18359375, 0.0, 0.23046875, 0.0, 0.0, 0.0, 0.77734375, 0.78515625, 0.9921875, 0.9921875, 0.75390625, 0.0, 0.78125, 0.0, 0.0, 0.0, 0.85546875, 0.640625, 0.9921875, 0.05859375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.16015625, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.05859375, 0.78515625, 0.9921875, 0.0, 0.0, 0.3515625, 0.0, 0.9921875, 0.0703125, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.046875, 0.1953125, 0.0, 0.99609375, 0.05078125, 0.0, 0.0, 0.8125, 0.26171875, 0.0, 0.01171875, 0.9921875, 0.0, 0.98828125, 0.046875, 0.9921875, 0.78515625, 0.1953125, 0.0, 0.16015625, 0.21484375, 0.9921875, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.13671875, 0.0, 0.0, 0.78515625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.08984375, 0.859375, 0.0, 0.9921875, 0.98828125, 0.78515625, 0.78515625, 0.0, 0.15234375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.93359375, 0.02734375, 0.10546875, 0.0, 0.9921875, 0.0, 0.0, 0.78515625, 0.9921875, 0.0, 0.078125, 0.0, 0.9140625, 0.0, 0.77734375, 0.36328125, 0.98828125, 0.0859375, 0.78515625, 0.98828125, 0.78515625, 0.0, 0.30859375, 0.0078125, 0.03125, 0.0, 0.78515625, 0.91015625, 0.0, 0.0, 0.1953125, 0.8046875, 0.9921875, 0.76171875, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.78515625, 0.9921875, 0.0, 0.78515625, 0.08203125, 0.1328125, 0.22265625, 0.9921875, 0.0625, 0.0, 0.0, 0.0, 0.19140625, 0.0, 0.4296875, 0.0390625, 0.0, 0.76171875, 0.0, 0.48828125, 0.78515625, 0.0, 0.08984375, 0.02734375, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.78515625, 0.99609375, 0.0625, 0.74609375, 0.98828125, 0.0, 0.0, 0.78515625, 0.0, 0.0, 0.0, 0.06640625, 0.0, 0.0, 0.0, 0.0, 0.08203125, 0.0, 0.0, 0.63671875, 0.0, 0.02734375, 0.39453125, 0.0, 0.0, 0.78515625, 0.13671875, 0.98828125, 0.0, 0.80078125, 0.0, 0.77734375, 0.01953125, 0.00390625, 0.0, 0.26171875, 0.0, 0.09765625, 0.20703125, 0.0, 0.0, 0.796875, 0.0, 0.0, 0.0, 0.0, 0.71875, 0.08984375, 0.1328125, 0.78515625, 0.0, 0.78515625, 0.78515625, 0.0, 0.78515625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.84375, 0.11328125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.73046875, 0.0, 0.0, 0.9921875, 0.0, 0.1015625, 0.9921875, 0.0, 0.78515625, 0.9921875, 0.9921875, 0.07421875, 0.046875, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0703125, 0.78515625, 0.98828125, 0.0, 0.0, 0.0, 0.78515625, 0.78515625, 0.98828125, 0.9921875, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.78515625, 0.99609375, 0.0, 0.0, 0.0, 0.78515625, 0.0, 0.86328125, 0.9921875, 0.0, 0.0546875, 0.0, 0.08203125, 0.11328125, 0.99609375, 0.82421875, 0.9921875, 0.9921875, 0.1875, 0.0, 0.03515625, 0.0, 0.72265625, 0.9921875, 0.0, 0.0, 0.77734375, 0.9921875, 0.9921875, 0.0, 0.0, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.7265625, 0.9921875, 0.0, 0.1015625, 0.0, 0.78515625, 0.0, 0.78515625, 0.0, 0.0, 0.765625, 0.9921875, 0.0, 0.0, 0.73046875, 0.78515625, 0.0, 0.0, 0.0, 0.66796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.78125, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.81640625, 0.11328125, 0.0, 0.0, 0.76171875, 0.98828125, 0.0, 0.9921875, 0.0, 0.0, 0.78515625, 0.78515625, 0.0, 0.0859375, 0.78515625, 0.0, 0.9921875, 0.0859375, 0.0, 0.0, 0.0, 0.0, 0.78515625, 0.62109375, 0.0, 0.0, 0.0, 0.78515625, 0.0, 0.765625, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.23046875, 0.7890625, 0.9921875, 0.0, 0.0, 0.0, 0.05859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.78515625, 0.84765625, 0.0, 0.0, 0.0, 0.0, 0.2890625, 0.0, 0.7578125, 0.0, 0.71484375, 0.0, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.08203125, 0.0, 0.06640625, 0.0, 0.0, 0.78515625, 0.78515625, 0.76953125, 0.9921875, 0.78515625, 0.0, 0.21484375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.76171875, 0.765625, 0.9921875, 0.9921875, 0.78515625, 0.0, 0.0, 0.78515625, 0.98828125, 0.0, 0.69921875, 0.78515625, 0.7265625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.7734375, 0.9921875, 0.83984375, 0.0, 0.98828125, 0.9921875, 0.01171875, 0.22265625, 0.78515625, 0.78515625, 0.0, 0.0, 0.203125, 0.0, 0.0, 0.0, 0.44921875, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.78515625, 0.7109375, 0.99609375, 0.0, 0.7109375, 0.0625, 0.0, 0.15234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58984375, 0.0, 0.1484375, 0.875, 0.0, 0.9921875, 0.0, 0.78515625, 0.99609375, 0.80078125, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.7578125, 0.0, 0.87890625, 0.0, 0.9921875, 0.10546875, 0.0, 0.0, 0.78515625, 0.76171875, 0.99609375, 0.0, 0.09765625, 0.0, 0.78515625, 0.9921875, 0.0546875, 0.0, 0.78515625, 0.0, 0.76171875, 0.7734375, 0.78515625, 0.78515625, 0.0, 0.0, 0.1953125, 0.9921875, 0.9921875, 0.0546875, 0.78515625, 0.99609375, 0.78515625, 0.78515625, 0.0, 0.0625, 0.0234375, 0.0, 0.0, 0.9921875, 0.0, 0.78515625, 0.0, 0.9921875, 0.0, 0.15625, 0.9921875, 0.1796875, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.06640625, 0.6171875, 0.11328125, 0.0, 0.06640625, 0.28125, 0.3828125, 0.078125, 0.0, 0.015625, 0.9921875, 0.0, 0.0, 0.9921875, 0.73828125, 0.9921875, 0.9921875, 0.140625, 0.0, 0.9921875, 0.78515625, 0.03125, 0.078125, 0.0, 0.18359375, 0.0, 0.0, 0.3125, 0.0, 0.1796875, 0.0, 0.0, 0.0, 0.95703125, 0.0, 0.0, 0.08203125, 0.93359375, 0.9921875, 0.9921875, 0.66796875, 0.25390625, 0.0, 0.9921875, 0.0, 0.78515625, 0.0, 0.0, 0.6484375, 0.0, 0.06640625, 0.0, 0.45703125, 0.87890625, 0.0, 0.78515625, 0.08203125, 0.99609375, 0.78515625, 0.78515625, 0.78125, 0.9921875, 0.0, 0.0, 0.0, 0.765625, 0.9921875, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.78515625, 0.0, 0.04296875, 0.9921875, 0.78515625, 0.73828125, 0.9921875, 0.9921875, 0.0, 0.78515625, 0.0, 0.0, 0.78515625, 0.0, 0.78515625, 0.0, 0.26953125, 0.9921875, 0.0, 0.78515625, 0.0, 0.78515625, 0.0, 0.99609375, 0.0, 0.0, 0.05859375, 0.0, 0.78515625, 0.77734375, 0.0, 0.0, 0.0, 0.9921875, 0.78515625, 0.0, 0.0, 0.234375, 0.765625, 0.0625, 0.1015625, 0.78515625, 0.0, 0.0, 0.0, 0.78515625, 0.0, 0.98828125, 0.26171875, 0.0, 0.0, 0.0, 0.08203125, 0.0, 0.0, 0.0, 0.9921875, 0.1953125, 0.2890625, 0.78125, 0.0, 0.78515625, 0.0, 0.9921875, 0.0, 0.78515625, 0.0, 0.625, 0.0, 0.05078125, 0.9921875, 0.9921875, 0.0, 0.12890625, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.02734375, 0.0, 0.0, 0.78515625, 0.09375, 0.23828125, 0.05078125, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.921875, 0.0, 0.0, 0.046875, 0.74609375, 0.1171875, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78515625, 0.27734375, 0.765625, 0.0, 0.0, 0.0703125, 0.2578125, 0.0, 0.99609375, 0.828125, 0.78515625, 0.0, 0.7734375, 0.98828125, 0.77734375, 0.0, 0.78125, 0.99609375, 0.0, 0.0, 0.0, 0.34765625, 0.9921875, 0.0, 0.99609375, 0.78515625, 0.78515625, 0.984375, 0.9921875, 0.0, 0.99609375, 0.78515625, 0.02734375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.76953125, 0.0, 0.9921875, 0.0, 0.78125, 0.9921875, 0.0, 0.15625, 0.9921875, 0.02734375, 0.0, 0.14453125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.125, 0.77734375, 0.0, 0.91015625, 0.78515625, 0.1015625, 0.0, 0.0, 0.0, 0.7265625, 0.0, 0.046875, 0.09765625, 0.0, 0.0, 0.7734375, 0.0, 0.0703125, 0.6875, 0.7265625, 0.0703125, 0.78515625, 0.78515625, 0.75390625, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.78515625, 0.78515625, 0.0, 0.0, 0.0, 0.16015625, 0.0, 0.05078125, 0.0, 0.0, 0.0, 0.04296875, 0.0625, 0.40625, 0.0, 0.78515625, 0.0, 0.78515625, 0.0, 0.9921875, 0.0, 0.0, 0.67578125, 0.1015625, 0.99609375, 0.828125, 0.31640625, 0.05078125, 0.99609375, 0.9921875, 0.05078125, 0.0, 0.78515625, 0.0, 0.0, 0.0, 0.78515625, 0.125, 0.0, 0.0, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.78515625, 0.9921875, 0.0546875, 0.9921875, 0.0, 0.78515625, 0.9921875, 0.5703125, 0.0546875, 0.0, 0.0, 0.05078125, 0.7578125, 0.0078125, 0.70703125, 0.9921875, 0.0, 0.0, 0.9921875, 0.21875, 0.0, 0.78515625, 0.0, 0.9921875, 0.78515625, 0.125, 0.78515625, 0.0390625, 0.78515625, 0.0, 0.98828125, 0.2734375, 0.0, 0.0546875, 0.0, 0.86328125, 0.78515625, 0.08984375, 0.0, 0.0, 0.77734375, 0.0, 0.9921875, 0.78515625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.78515625, 0.78515625, 0.41796875, 0.0, 0.9921875, 0.734375, 0.0, 0.0, 0.0, 0.05859375, 0.02734375, 0.78515625, 0.0, 0.4296875, 0.0625, 0.06640625, 0.78515625, 0.05078125, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.78515625, 0.0859375, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.08203125, 0.69921875, 0.0, 0.99609375, 0.0, 0.0859375, 0.0, 0.76953125, 0.0, 0.70703125, 0.0, 0.99609375, 0.78515625, 0.78515625, 0.0, 0.98828125, 0.78515625, 0.0, 0.0, 0.0, 0.0, 0.05078125, 0.9921875, 0.98828125, 0.0, 0.8046875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0546875, 0.0, 0.0, 0.9921875, 0.0, 0.01953125, 0.7578125, 0.1875]

 sparsity of   [0.0732421875, 0.0224609375, 0.0576171875, 0.1162109375, 0.9990234375, 0.0244140625, 0.9599609375, 0.0830078125, 0.0146484375, 0.095703125, 0.83203125, 0.0693359375, 0.0966796875, 0.8125, 0.0732421875, 0.798828125, 0.4150390625, 0.03515625, 0.2265625, 0.6748046875, 0.029296875, 0.0244140625, 0.8349609375, 0.1357421875, 0.0908203125, 0.0068359375, 0.0380859375, 0.6484375, 0.0244140625, 0.2509765625, 0.0107421875, 0.05859375, 0.015625, 0.0693359375, 0.029296875, 0.0, 0.8515625, 0.0087890625, 0.888671875, 0.03125, 0.41015625, 0.01953125, 0.8095703125, 0.21875, 0.0419921875, 0.01953125, 0.05078125, 0.0615234375, 0.1103515625, 0.0146484375, 0.125, 0.0224609375, 0.5498046875, 0.0, 0.876953125, 0.1162109375, 0.19921875, 0.0, 0.96875, 0.0390625, 0.0947265625, 0.9970703125, 0.0361328125, 0.0478515625, 0.5419921875, 0.0283203125, 0.9677734375, 0.0, 0.0771484375, 0.0166015625, 0.0224609375, 0.0205078125, 0.076171875, 0.02734375, 0.0166015625, 0.9228515625, 0.9716796875, 0.0205078125, 0.0205078125, 0.7177734375, 0.0712890625, 0.138671875, 0.6845703125, 0.0263671875, 0.0322265625, 0.03515625, 0.0, 0.041015625, 0.71875, 0.103515625, 0.998046875, 0.0234375, 0.671875, 0.0283203125, 0.0205078125, 0.9443359375, 0.06640625, 0.9990234375, 0.037109375, 0.09765625, 0.0751953125, 0.1337890625, 0.041015625, 0.8466796875, 0.9990234375, 0.083984375, 0.03125, 0.0126953125, 0.0791015625, 0.03515625, 0.0078125, 0.0341796875, 0.0458984375, 0.1015625, 0.0185546875, 0.1025390625, 0.3916015625, 0.0, 0.01953125, 0.0849609375, 0.0458984375, 0.0107421875, 0.009765625, 0.0810546875, 0.8125, 0.142578125, 0.029296875, 0.9990234375, 0.083984375, 0.0205078125, 0.0185546875, 0.0, 0.015625, 0.015625, 0.0, 0.2421875, 0.82421875, 0.7470703125, 0.033203125, 0.8193359375, 0.0166015625, 0.138671875, 0.4638671875, 0.45703125, 0.0732421875, 0.7255859375, 0.017578125, 0.11328125, 0.7744140625, 0.1689453125, 0.8349609375, 0.0, 0.0751953125, 0.9990234375, 0.2626953125, 0.0, 0.0234375, 0.037109375, 0.0673828125, 0.0341796875, 0.0185546875, 0.009765625, 0.0302734375, 0.62890625, 0.064453125, 0.0185546875, 0.013671875, 0.9990234375, 0.69921875, 0.1142578125, 0.736328125, 0.0341796875, 0.0029296875, 0.0, 0.958984375, 0.0224609375, 0.0068359375, 0.068359375, 0.013671875, 0.34765625, 0.080078125, 0.037109375, 0.8505859375, 0.052734375, 0.0419921875, 0.03515625, 0.2080078125, 0.0888671875, 0.8896484375, 0.06640625, 0.7060546875, 0.06640625, 0.9970703125, 0.013671875, 0.029296875, 0.060546875, 0.0029296875, 0.0107421875, 0.01171875, 0.9970703125, 0.7587890625, 0.0244140625, 0.9990234375, 0.025390625, 0.0, 0.1611328125, 0.0283203125, 0.013671875, 0.07421875, 0.2060546875, 0.9208984375, 0.9970703125, 0.0390625, 0.9970703125, 0.3408203125, 0.0, 0.0224609375, 0.912109375, 0.0361328125, 0.0556640625, 0.0810546875, 0.0322265625, 0.0126953125, 0.0244140625, 0.0224609375, 0.0166015625, 0.1357421875, 0.451171875, 0.734375, 0.0205078125, 0.0048828125, 0.05078125, 0.0205078125, 0.6923828125, 0.0703125, 0.8291015625, 0.9111328125, 0.724609375, 0.0322265625, 0.087890625, 0.8076171875, 0.0732421875, 0.1630859375, 0.013671875, 0.0, 0.0, 0.0390625, 0.0361328125, 0.0341796875, 0.0, 0.0400390625, 0.0654296875, 0.0, 0.10546875, 0.095703125, 0.009765625]

 sparsity of   [0.03125, 0.007378472480922937, 0.0950520858168602, 0.0316840298473835, 0.0377604179084301, 0.0, 0.086805559694767, 0.0, 0.106336809694767, 0.0421006940305233, 0.02734375, 0.03125, 0.0490451380610466, 0.0451388880610466, 0.1666666716337204, 0.0850694477558136, 0.0551215298473835, 0.0030381944961845875, 0.3020833432674408, 0.4986979067325592, 0.1171875, 0.0325520820915699, 0.0421006940305233, 0.0017361111240461469, 0.009548611007630825, 0.02734375, 0.0564236119389534, 0.0316840298473835, 0.0243055559694767, 0.138454869389534, 0.1723090261220932, 0.0442708320915699, 0.0321180559694767, 0.0794270858168602, 0.0, 0.0386284738779068, 0.0004340277810115367, 0.1549479216337204, 0.1649305522441864, 0.2452256977558136, 0.1245659738779068, 0.0434027798473835, 0.0, 0.0004340277810115367, 0.0590277798473835, 0.0403645820915699, 0.0303819440305233, 0.0325520820915699, 0.0546875, 0.0421006940305233, 0.1098090261220932, 0.0559895820915699, 0.0798611119389534, 0.02083333395421505, 0.0638020858168602, 0.0499131940305233, 0.009548611007630825, 0.1831597238779068, 0.4201388955116272, 0.0677083358168602, 0.0407986119389534, 0.4735243022441864, 0.487847238779068, 0.0577256940305233, 0.1050347238779068, 0.098524309694767, 0.0694444477558136, 0.0, 0.0026041667442768812, 0.02864583395421505, 0.0568576380610466, 0.0329861119389534, 0.0203993059694767, 0.014322916977107525, 0.00390625, 0.014756944961845875, 0.0542534738779068, 0.013454861007630825, 0.0338541679084301, 0.2296006977558136, 0.0416666679084301, 0.0763888880610466, 0.0460069440305233, 0.0026041667442768812, 0.7808159589767456, 0.1202256977558136, 0.0911458358168602, 0.0716145858168602, 0.014322916977107525, 0.909288227558136, 0.0707465261220932, 0.0325520820915699, 0.114149309694767, 0.1375868022441864, 0.013454861007630825, 0.0017361111240461469, 0.02170138992369175, 0.02170138992369175, 0.0, 0.0490451380610466, 0.0164930559694767, 0.0494791679084301, 0.0212673619389534, 0.0403645820915699, 0.02473958395421505, 0.0338541679084301, 0.5672743320465088, 0.5711805820465088, 0.0399305559694767, 0.0394965298473835, 0.0203993059694767, 0.082899309694767, 0.0355902798473835, 0.1128472238779068, 0.06640625, 0.01996527798473835, 0.00824652798473835, 0.1028645858168602, 0.5047743320465088, 0.1653645783662796, 0.3294270932674408, 0.02994791604578495, 0.0377604179084301, 0.03515625, 0.0473090298473835, 0.01171875, 0.0004340277810115367, 0.0303819440305233, 0.0325520820915699, 0.1015625, 0.1362847238779068, 0.0455729179084301, 0.0, 0.02170138992369175, 0.005642361007630825, 0.02690972201526165, 0.0004340277810115367, 0.118055559694767, 0.0737847238779068, 0.0321180559694767, 0.0004340277810115367, 0.1419270783662796, 0.0125868059694767, 0.01519097201526165, 0.0941840261220932, 0.0, 0.4661458432674408, 0.0, 0.0, 0.0004340277810115367, 0.006076388992369175, 0.0572916679084301, 0.0, 0.05859375, 0.063368059694767, 0.2152777761220932, 0.0542534738779068, 0.0533854179084301, 0.0394965298473835, 0.0377604179084301, 0.0078125, 0.082899309694767, 0.9557291865348816, 0.0, 0.013888888992369175, 0.2161458283662796, 0.0451388880610466, 0.0, 0.078125, 0.014322916977107525, 0.0425347238779068, 0.0477430559694767, 0.02213541604578495, 0.0520833320915699, 0.0716145858168602, 0.01171875, 0.0017361111240461469, 0.0282118059694767, 0.0941840261220932, 0.0729166641831398, 0.1236979141831398, 0.109375, 0.1614583283662796, 0.014322916977107525, 0.0047743055038154125, 0.0078125, 0.1015625, 0.0729166641831398, 0.9205729365348816, 0.0486111119389534, 0.0368923619389534, 0.0685763880610466, 0.3624131977558136, 0.02604166604578495, 0.02213541604578495, 0.0013020833721384406, 0.0, 0.009548611007630825, 0.0625, 0.0004340277810115367, 0.013020833022892475, 0.0798611119389534, 0.0863715261220932, 0.7434895634651184, 0.7907986044883728, 0.01215277798473835, 0.0407986119389534, 0.1158854141831398, 0.02170138992369175, 0.1323784738779068, 0.0, 0.1167534738779068, 0.1427951455116272, 0.0303819440305233, 0.0863715261220932, 0.2352430522441864, 0.0004340277810115367, 0.3107638955116272, 0.1011284738779068, 0.0729166641831398, 0.0577256940305233, 0.02604166604578495, 0.0503472238779068, 0.0863715261220932, 0.1953125, 0.0490451380610466, 0.6927083134651184, 0.3363715410232544, 0.874131977558136, 0.0915798619389534, 0.2044270783662796, 0.02864583395421505, 0.3654513955116272, 0.0186631940305233, 0.0026041667442768812, 0.1514756977558136, 0.1263020783662796, 0.0251736119389534, 0.02604166604578495, 0.7868923544883728, 0.0646701380610466, 0.0086805559694767, 0.0698784738779068, 0.0368923619389534, 0.0629340261220932, 0.0694444477558136, 0.0837673619389534, 0.2547743022441864, 0.0225694440305233, 0.0, 0.0004340277810115367, 0.01692708395421505, 0.01822916604578495, 0.0004340277810115367, 0.02083333395421505, 0.009982638992369175]

 sparsity of   [0.921875, 0.9609375, 0.0, 0.9609375, 0.0, 0.0234375, 0.9296875, 0.90234375, 0.0, 0.06640625, 0.9609375, 0.00390625, 0.92578125, 0.015625, 0.015625, 0.9609375, 0.8828125, 0.921875, 0.0859375, 0.94921875, 0.95703125, 0.9453125, 0.9609375, 0.95703125, 0.8828125, 0.8984375, 0.91796875, 0.9609375, 0.09765625, 0.9609375, 0.9609375, 0.0859375, 0.94921875, 0.0, 0.93359375, 0.9765625, 0.7421875, 0.9921875, 0.94921875, 0.91796875, 0.91015625, 0.94140625, 0.0, 0.046875, 0.92578125, 0.08984375, 0.95703125, 0.01953125, 0.9609375, 0.8515625, 0.9296875, 0.921875, 0.9375, 0.05078125, 0.9921875, 0.078125, 0.9609375, 0.86328125, 0.8984375, 0.01953125, 0.01953125, 0.9921875, 0.96484375, 0.9609375, 0.91015625, 0.9609375, 0.9609375, 0.9609375, 0.93359375, 0.0, 0.9609375, 0.9921875, 0.99609375, 0.00390625, 0.90234375, 0.86328125, 0.91015625, 0.0, 0.97265625, 0.64453125, 0.82421875, 0.9609375, 0.87109375, 0.9453125, 0.921875, 0.9609375, 0.28125, 0.9921875, 0.96484375, 0.0078125, 0.87890625, 0.921875, 0.9609375, 0.9609375, 0.9609375, 0.765625, 0.0703125, 0.921875, 0.9375, 0.29296875, 0.94921875, 0.9921875, 0.9609375, 0.9921875, 0.81640625, 0.91796875, 0.0, 0.1875, 0.9609375, 0.9921875, 0.9609375, 0.8515625, 0.0, 0.94921875, 0.94921875, 0.9609375, 0.9609375, 0.8515625, 0.9609375, 0.9609375, 0.046875, 0.9609375, 0.0234375, 0.9609375, 0.953125, 0.9609375, 0.01171875, 0.96484375, 0.9921875, 0.94140625, 0.9609375, 0.9453125, 0.00390625, 0.9921875, 0.07421875, 0.01171875, 0.0390625, 0.0, 0.953125, 0.0390625, 0.9609375, 0.17578125, 0.921875, 0.875, 0.90234375, 0.99609375, 0.9609375, 0.328125, 0.9609375, 0.9609375, 0.9296875, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.92578125, 0.9609375, 0.82421875, 0.953125, 0.01171875, 0.9609375, 0.9609375, 0.9609375, 0.921875, 0.9609375, 0.9453125, 0.9609375, 0.9921875, 0.9921875, 0.8515625, 0.0546875, 0.11328125, 0.01953125, 0.9609375, 0.87890625, 0.9453125, 0.9921875, 0.06640625, 0.9453125, 0.9140625, 0.12890625, 0.04296875, 0.94921875, 0.0, 0.9609375, 0.19921875, 0.9296875, 0.109375, 0.02734375, 0.9609375, 0.0, 0.88671875, 0.9609375, 0.9609375, 0.94921875, 0.890625, 0.9609375, 0.1953125, 0.0, 0.87109375, 0.9609375, 0.83984375, 0.2109375, 0.94140625, 0.16015625, 0.09765625, 0.421875, 0.9609375, 0.9609375, 0.81640625, 0.9609375, 0.0546875, 0.9609375, 0.9375, 0.93359375, 0.9375, 0.08984375, 0.12109375, 0.0, 0.125, 0.08984375, 0.9609375, 0.99609375, 0.8671875, 0.94140625, 0.0, 0.9609375, 0.9609375, 0.9609375, 0.890625, 0.88671875, 0.8203125, 0.9609375, 0.046875, 0.01171875, 0.0, 0.01171875, 0.94921875, 0.9921875, 0.9609375, 0.0, 0.94140625, 0.875, 0.86328125, 0.9609375, 0.0546875, 0.99609375, 0.9609375, 0.9609375, 0.94140625, 0.89453125, 0.9609375, 0.90625, 0.74609375, 0.94921875, 0.9296875, 0.0625, 0.0, 0.0078125, 0.94921875, 0.8515625, 0.9609375, 0.9375, 0.86328125, 0.91015625, 0.9609375, 0.01171875, 0.93359375, 0.9453125, 0.0, 0.8828125, 0.9609375, 0.0, 0.03515625, 0.9609375, 0.9609375, 0.109375, 0.07421875, 0.9609375, 0.93359375, 0.91796875, 0.92578125, 0.8046875, 0.015625, 0.9609375, 0.9921875, 0.9921875, 0.92578125, 0.99609375, 0.0078125, 0.9296875, 0.9609375, 0.99609375, 0.875, 0.9921875, 0.9609375, 0.9921875, 0.03125, 0.015625, 0.0, 0.91015625, 0.9296875, 0.9609375, 0.9609375, 0.9609375, 0.98828125, 0.05078125, 0.9609375, 0.9609375, 0.01953125, 0.9609375, 0.94921875, 0.00390625, 0.02734375, 0.9921875, 0.91015625, 0.71875, 0.73828125, 0.0, 0.9609375, 0.9453125, 0.0, 0.94921875, 0.9609375, 0.9609375, 0.078125, 0.9609375, 0.015625, 0.79296875, 0.95703125, 0.94921875, 0.0, 0.9609375, 0.91796875, 0.00390625, 0.9609375, 0.94921875, 0.9609375, 0.890625, 0.890625, 0.0, 0.94921875, 0.9609375, 0.9609375, 0.87890625, 0.9921875, 0.87109375, 0.15625, 0.0, 0.01171875, 0.0703125, 0.1171875, 0.875, 0.99609375, 0.95703125, 0.0, 0.9609375, 0.9609375, 0.9921875, 0.9140625, 0.93359375, 0.08984375, 0.91015625, 0.33984375, 0.9609375, 0.90625, 0.95703125, 0.109375, 0.83984375, 0.0078125, 0.9453125, 0.0546875, 0.99609375, 0.84375, 0.9453125, 0.9609375, 0.0, 0.02734375, 0.03125, 0.92578125, 0.9609375, 0.0234375, 0.94140625, 0.0390625, 0.89453125, 0.06640625, 0.9921875, 0.91796875, 0.9609375, 0.98828125, 0.9921875, 0.0, 0.9609375, 0.89453125, 0.9921875, 0.02734375, 0.92578125, 0.82421875, 0.94921875, 0.9609375, 0.77734375, 0.06640625, 0.01953125, 0.9609375, 0.29296875, 0.93359375, 0.80078125, 0.859375, 0.0, 0.921875, 0.94921875, 0.05078125, 0.953125, 0.0, 0.92578125, 0.9921875, 0.734375, 0.9609375, 0.125, 0.05078125, 0.87109375, 0.9609375, 0.01953125, 0.875, 0.9296875, 0.02734375, 0.9609375, 0.9609375, 0.77734375, 0.9609375, 0.04296875, 0.9609375, 0.94140625, 0.98828125, 0.91796875, 0.0, 0.95703125, 0.09765625, 0.9296875, 0.02734375, 0.0, 0.05078125, 0.00390625, 0.9921875, 0.9609375, 0.78125, 0.96484375, 0.9453125, 0.04296875, 0.02734375, 0.9921875, 0.98828125, 0.9609375, 0.94921875, 0.9921875, 0.9296875, 0.0390625, 0.8984375, 0.9921875, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.0, 0.95703125, 0.01171875, 0.8125, 0.94921875, 0.9609375, 0.921875, 0.12890625, 0.9921875, 0.9921875, 0.921875, 0.1953125, 0.8984375, 0.859375, 0.87890625, 0.9140625, 0.9609375, 0.01171875, 0.953125, 0.9609375, 0.99609375, 0.9609375, 0.9609375, 0.9609375, 0.9609375, 0.9921875, 0.0, 0.0, 0.0078125, 0.0, 0.765625, 0.83203125, 0.95703125, 0.91796875, 0.04296875, 0.98828125, 0.0625, 0.04296875, 0.94921875, 0.71484375, 0.9375, 0.02734375, 0.01171875, 0.9609375, 0.9609375, 0.00390625, 0.9609375, 0.9140625, 0.9609375, 0.9921875, 0.93359375, 0.92578125, 0.01171875, 0.01171875, 0.9921875, 0.94921875, 0.9375, 0.92578125, 0.0546875, 0.87109375, 0.58203125, 0.9609375, 0.03125, 0.05859375, 0.09375, 0.8984375, 0.9921875, 0.94140625, 0.0, 0.9375, 0.9609375, 0.98046875, 0.0, 0.91015625, 0.93359375, 0.9609375, 0.9609375, 0.1328125, 0.88671875, 0.9609375, 0.015625, 0.92578125, 0.9921875, 0.9609375, 0.94921875, 0.9609375, 0.84765625, 0.0234375, 0.99609375, 0.89453125, 0.9375, 0.9296875, 0.078125, 0.83203125, 0.9609375, 0.90625, 0.99609375, 0.9609375, 0.9609375, 0.953125, 0.9921875, 0.953125, 0.10546875, 0.8984375, 0.9609375, 0.95703125, 0.9921875, 0.0390625, 0.05859375, 0.01171875, 0.015625, 0.0, 0.0546875, 0.93359375, 0.953125, 0.9921875, 0.99609375, 0.90234375, 0.9609375, 0.953125, 0.8359375, 0.1171875, 0.0546875, 0.95703125, 0.9921875, 0.98046875, 0.87890625, 0.9609375, 0.95703125, 0.015625, 0.0, 0.94921875, 0.9296875, 0.9921875, 0.0859375, 0.9609375, 0.94140625, 0.94921875, 0.9609375, 0.8984375, 0.95703125, 0.9609375, 0.98828125, 0.94140625, 0.83984375, 0.87890625, 0.0, 0.96484375, 0.0, 0.078125, 0.9296875, 0.92578125, 0.91796875, 0.75, 0.9921875, 0.9921875, 0.98828125, 0.11328125, 0.9609375, 0.99609375, 0.0, 0.125, 0.0, 0.94921875, 0.84765625, 0.0, 0.90234375, 0.9609375, 0.953125, 0.0, 0.99609375, 0.953125, 0.84375, 0.9609375, 0.9609375, 0.07421875, 0.94921875, 0.9921875, 0.9609375, 0.84375, 0.94921875, 0.953125, 0.05859375, 0.88671875, 0.01171875, 0.9375, 0.9609375, 0.9921875, 0.0, 0.92578125, 0.9921875, 0.0, 0.91796875, 0.93359375, 0.75, 0.0078125, 0.015625, 0.0, 0.9453125, 0.9609375, 0.9609375, 0.98828125, 0.9296875, 0.0, 0.09375, 0.02734375, 0.890625, 0.9609375, 0.95703125, 0.23046875, 0.45703125, 0.89453125, 0.98046875, 0.01171875, 0.9609375, 0.34375, 0.9609375, 0.03125, 0.9609375, 0.0, 0.9609375, 0.9296875, 0.95703125, 0.8515625, 0.94921875, 0.94140625, 0.0, 0.9296875, 0.03125, 0.921875, 0.9609375, 0.0, 0.921875, 0.9609375, 0.79296875, 0.89453125, 0.94921875, 0.0, 0.0, 0.94921875, 0.9609375, 0.81640625, 0.9453125, 0.890625, 0.85546875, 0.95703125, 0.9609375, 0.9609375, 0.9609375, 0.9375, 0.08203125, 0.9921875, 0.94140625, 0.98828125, 0.0, 0.9140625, 0.9609375, 0.9609375, 0.9921875, 0.0, 0.9375, 0.9609375, 0.9296875, 0.09375, 0.9609375, 0.9921875, 0.95703125, 0.7890625, 0.81640625, 0.02734375, 0.13671875, 0.9609375, 0.9921875, 0.9609375, 0.9609375, 0.01171875, 0.0546875, 0.9609375, 0.046875, 0.92578125, 0.87109375, 0.86328125, 0.953125, 0.91015625, 0.14453125, 0.0, 0.91796875, 0.09375, 0.9609375, 0.95703125, 0.92578125, 0.8984375, 0.0625, 0.0625, 0.95703125, 0.9765625, 0.9609375, 0.89453125, 0.00390625, 0.0859375, 0.90625, 0.87890625, 0.9921875, 0.81640625, 0.99609375, 0.0, 0.90234375, 0.015625, 0.984375, 0.94921875, 0.9375, 0.02734375, 0.95703125, 0.78515625, 0.95703125, 0.98828125, 0.09375, 0.91015625, 0.9609375, 0.04296875, 0.9609375, 0.12109375, 0.9296875, 0.96875, 0.92578125, 0.9609375, 0.953125, 0.9609375, 0.9609375, 0.08203125, 0.9765625, 0.95703125, 0.99609375, 0.9609375, 0.9609375, 0.94140625, 0.94140625, 0.0703125, 0.93359375, 0.9609375, 0.171875, 0.9296875, 0.0859375, 0.9921875, 0.9453125, 0.9453125, 0.9609375, 0.9921875, 0.9921875, 0.94921875, 0.0390625, 0.890625, 0.0, 0.07421875, 0.08203125, 0.9140625, 0.9609375, 0.88671875, 0.8984375, 0.95703125, 0.9140625, 0.96484375, 0.9296875, 0.9921875, 0.0546875, 0.87890625, 0.8828125, 0.9296875, 0.109375, 0.95703125, 0.9453125, 0.875, 0.91015625, 0.9609375, 0.890625, 0.02734375, 0.890625, 0.04296875, 0.94140625, 0.9609375, 0.9609375, 0.9609375, 0.98046875, 0.30078125, 0.9609375, 0.95703125, 0.9609375, 0.95703125, 0.921875, 0.08984375, 0.9375, 0.9453125, 0.8671875, 0.0390625, 0.99609375, 0.0, 0.9375, 0.140625, 0.98828125, 0.04296875, 0.9140625, 0.8984375, 0.9609375, 0.00390625, 0.0234375, 0.8984375, 0.13671875, 0.9609375, 0.94921875, 0.9921875, 0.9609375, 0.9609375, 0.0234375, 0.96484375, 0.9921875, 0.875, 0.8984375, 0.8671875, 0.9609375, 0.02734375, 0.953125, 0.9609375, 0.05078125, 0.9921875, 0.87109375, 0.9609375, 0.90625, 0.9609375, 0.90625, 0.78515625, 0.05859375, 0.8046875, 0.9609375, 0.94921875, 0.95703125, 0.0, 0.9609375, 0.99609375, 0.9609375, 0.9921875, 0.01171875, 0.953125, 0.9609375, 0.01953125, 0.921875, 0.78125, 0.953125, 0.9609375, 0.95703125, 0.9453125, 0.9609375, 0.91015625, 0.91796875, 0.00390625, 0.9296875, 0.94921875, 0.8984375, 0.9609375, 0.921875, 0.984375, 0.9921875, 0.9140625, 0.9609375, 0.72265625, 0.99609375, 0.8671875, 0.08984375, 0.9921875, 0.95703125, 0.9609375, 0.89453125, 0.33203125, 0.87109375, 0.96875, 0.0, 0.875, 0.9609375, 0.83203125, 0.9921875, 0.9609375, 0.9609375, 0.9609375, 0.98828125, 0.91796875, 0.83203125, 0.109375, 0.7578125, 0.91796875, 0.96875, 0.9609375, 0.9375, 0.0390625, 0.01953125, 0.9921875, 0.88671875, 0.0, 0.91796875, 0.9140625, 0.92578125, 0.9609375, 0.94921875, 0.0, 0.984375, 0.90234375, 0.19140625, 0.87890625, 0.92578125, 0.0, 0.84765625, 0.953125, 0.0390625, 0.078125, 0.91015625, 0.9609375, 0.9609375, 0.9609375, 0.0, 0.953125, 0.9609375, 0.86328125, 0.9609375, 0.94140625, 0.9609375, 0.890625, 0.921875, 0.9609375, 0.9453125, 0.03515625, 0.9609375, 0.9609375, 0.94921875, 0.9609375, 0.9609375, 0.0, 0.921875, 0.8515625, 0.0, 0.0234375, 0.93359375, 0.0, 0.87109375, 0.078125, 0.0, 0.0, 0.06640625, 0.10546875, 0.21484375, 0.01171875, 0.74609375, 0.0234375, 0.9609375, 0.95703125, 0.90625, 0.9921875, 0.046875, 0.01953125, 0.9921875, 0.9609375, 0.953125, 0.9609375, 0.9921875]

 sparsity of   [0.0048828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.0537109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.0263671875, 0.998046875, 0.0078125, 0.748046875, 0.9970703125, 0.0, 0.0, 0.998046875, 0.046875, 0.93359375, 0.0439453125, 0.025390625, 0.8994140625, 0.0, 0.0, 0.0, 0.931640625, 0.0, 0.146484375, 0.0, 0.0, 0.23828125, 0.1591796875, 0.0244140625, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0244140625, 0.0126953125, 0.0, 0.0283203125, 0.0283203125, 0.0, 0.0419921875, 0.0, 0.0, 0.0, 0.0, 0.0322265625, 0.0, 0.0, 0.0361328125, 0.046875, 0.0, 0.998046875, 0.0, 0.7861328125, 0.01171875, 0.0, 0.865234375, 0.0546875, 0.0390625, 0.0, 0.0634765625, 0.0, 0.8701171875, 0.0, 0.0, 0.0576171875, 0.0, 0.04296875, 0.0, 0.85546875, 0.0, 0.0341796875, 0.0, 0.0419921875, 0.9443359375, 0.04296875, 0.9990234375, 0.998046875, 0.0, 0.013671875, 0.0, 0.998046875, 0.0185546875, 0.0224609375, 0.0, 0.0, 0.0546875, 0.015625, 0.0, 0.0185546875, 0.0, 0.0, 0.0, 0.2646484375, 0.0537109375, 0.025390625, 0.0322265625, 0.0, 0.0, 0.0, 0.0263671875, 0.0, 0.9091796875, 0.0, 0.0537109375, 0.0, 0.0, 0.9580078125, 0.0224609375, 0.8955078125, 0.0205078125, 0.125, 0.81640625, 0.9970703125, 0.0087890625, 0.04296875, 0.0224609375, 0.0, 0.998046875, 0.9970703125, 0.15625, 0.0, 0.068359375, 0.9970703125, 0.0400390625, 0.0, 0.0, 0.9970703125, 0.0, 0.0, 0.103515625, 0.0234375, 0.0517578125, 0.9697265625, 0.0166015625, 0.0, 0.0, 0.939453125, 0.9990234375, 0.060546875, 0.02734375, 0.0517578125, 0.1240234375, 0.0908203125, 0.0, 0.0546875, 0.2958984375, 0.0107421875, 0.0, 0.287109375, 0.029296875, 0.9990234375, 0.0166015625, 0.0, 0.923828125, 0.1103515625, 0.0, 0.197265625, 0.0, 0.0, 0.0, 0.0087890625, 0.009765625, 0.0, 0.0, 0.0, 0.017578125, 0.0107421875, 0.0, 0.0400390625, 0.0, 0.02734375, 0.9248046875, 0.029296875, 0.080078125, 0.0810546875, 0.0, 0.0087890625, 0.0283203125, 0.998046875, 0.0, 0.998046875, 0.130859375, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.095703125, 0.1318359375, 0.0185546875, 0.1357421875, 0.0849609375, 0.0, 0.048828125, 0.2724609375, 0.0, 0.017578125, 0.0498046875, 0.1474609375, 0.9970703125, 0.6181640625, 0.0859375, 0.0419921875, 0.0, 0.1162109375, 0.0888671875, 0.7724609375, 0.0947265625, 0.021484375, 0.998046875, 0.0224609375, 0.0, 0.0, 0.0166015625, 0.013671875, 0.0185546875, 0.2412109375, 0.0185546875, 0.9052734375, 0.015625, 0.841796875, 0.0, 0.0, 0.998046875, 0.912109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0048828125, 0.0, 0.0654296875, 0.0, 0.9228515625, 0.0048828125, 0.00390625, 0.0]

 sparsity of   [0.0, 0.0, 0.7074652910232544, 0.0, 0.03081597201526165, 0.999131977558136, 0.118055559694767, 0.506944477558136, 0.0, 0.56640625, 0.0555555559694767, 0.0477430559694767, 0.0329861119389534, 0.0412326380610466, 0.01996527798473835, 0.009114583022892475, 0.1553819477558136, 0.9986979365348816, 0.0399305559694767, 0.007378472480922937, 0.6762152910232544, 0.0186631940305233, 0.0, 0.12890625, 0.0, 0.5512152910232544, 0.013454861007630825, 0.0473090298473835, 0.0251736119389534, 0.0, 0.00390625, 0.2447916716337204, 0.0173611119389534, 0.02170138992369175, 0.02734375, 0.0, 0.02951388992369175, 0.565538227558136, 0.1258680522441864, 0.0234375, 0.0, 0.0338541679084301, 0.9986979365348816, 0.0, 0.0, 0.0, 0.0538194440305233, 0.0, 0.010416666977107525, 0.010850694961845875, 0.0863715261220932, 0.010416666977107525, 0.013888888992369175, 0.0, 0.0364583320915699, 0.0785590261220932, 0.0008680555620230734, 0.02473958395421505, 0.009982638992369175, 0.0, 0.094618059694767, 0.0, 0.8168402910232544, 0.0668402761220932, 0.0347222238779068, 0.1935763955116272, 0.0646701380610466, 0.9986979365348816, 0.1154513880610466, 0.0, 0.140625, 0.87890625, 0.0, 0.0, 0.0, 0.9995659589767456, 0.7235243320465088, 0.2083333283662796, 0.0, 0.0355902798473835, 0.0377604179084301, 0.0, 0.236111119389534, 0.0321180559694767, 0.0, 0.010416666977107525, 0.0625, 0.8046875, 0.015625, 0.02951388992369175, 0.0, 0.02951388992369175, 0.0, 0.0403645820915699, 0.0, 0.0, 0.014756944961845875, 0.0538194440305233, 0.02473958395421505, 0.6584201455116272, 0.0, 0.0303819440305233, 0.0746527761220932, 0.0, 0.0125868059694767, 0.0, 0.1545138955116272, 0.0303819440305233, 0.01605902798473835, 0.0264756940305233, 0.5724826455116272, 0.02083333395421505, 0.0, 0.0759548619389534, 0.0, 0.0368923619389534, 0.0, 0.1922743022441864, 0.181423619389534, 0.0, 0.02213541604578495, 0.9118923544883728, 0.0768229141831398, 0.06640625, 0.3424479067325592, 0.0538194440305233, 0.0, 0.0225694440305233, 0.0, 0.0, 0.0442708320915699, 0.0, 0.0, 0.03081597201526165, 0.0525173619389534, 0.0473090298473835, 0.5043402910232544, 0.0520833320915699, 0.0646701380610466, 0.013454861007630825, 0.0, 0.999131977558136, 0.0, 0.0855034738779068, 0.8767361044883728, 0.0625, 0.02473958395421505, 0.01215277798473835, 0.002170138992369175, 0.0438368059694767, 0.881944477558136, 0.9995659589767456, 0.0, 0.013020833022892475, 0.0, 0.015625, 0.0282118059694767, 0.999131977558136, 0.0694444477558136, 0.0368923619389534, 0.0577256940305233, 0.8532986044883728, 0.999131977558136, 0.9995659589767456, 0.0373263880610466, 0.0581597238779068, 0.126736119389534, 0.1688368022441864, 0.0373263880610466, 0.0225694440305233, 0.0264756940305233, 0.999131977558136, 0.014322916977107525, 0.0473090298473835, 0.0251736119389534, 0.02083333395421505, 0.6427951455116272, 0.0316840298473835, 0.17578125, 0.0729166641831398, 0.0416666679084301, 0.0959201380610466, 0.0685763880610466, 0.3624131977558136, 0.6857638955116272, 0.0, 0.9058159589767456, 0.02473958395421505, 0.013020833022892475, 0.02213541604578495, 0.3333333432674408, 0.0677083358168602, 0.0, 0.0164930559694767, 0.9995659589767456, 0.015625, 0.0, 0.0, 0.0455729179084301, 0.975694477558136, 0.03081597201526165, 0.0, 0.9986979365348816, 0.02994791604578495, 0.0, 0.0325520820915699, 0.0989583358168602, 0.999131977558136, 0.0, 0.0959201380610466, 0.9995659589767456, 0.0, 0.5442708134651184, 0.0, 0.94921875, 0.04296875, 0.204861119389534, 0.0967881977558136, 0.2170138955116272, 0.0186631940305233, 0.0, 0.0967881977558136, 0.007378472480922937, 0.1588541716337204, 0.014322916977107525, 0.0503472238779068, 0.9379340410232544, 0.090711809694767, 0.02734375, 0.0264756940305233, 0.01519097201526165, 0.06640625, 0.02473958395421505, 0.01171875, 0.83203125, 0.8307291865348816, 0.0, 0.1701388955116272, 0.0, 0.8515625, 0.0, 0.009982638992369175, 0.0794270858168602, 0.0017361111240461469, 0.1028645858168602, 0.1549479216337204, 0.1640625, 0.0, 0.046875, 0.0, 0.01692708395421505, 0.0442708320915699, 0.0, 0.0251736119389534, 0.0512152798473835, 0.9995659589767456]

 sparsity of   [0.0, 0.765625, 0.0, 0.75390625, 0.0, 0.98828125, 0.0, 0.765625, 0.0, 0.109375, 0.9921875, 0.765625, 0.99609375, 0.984375, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.9921875, 0.765625, 0.73828125, 0.75390625, 0.89453125, 0.99609375, 0.0, 0.8359375, 0.0, 0.0, 0.765625, 0.74609375, 0.0, 0.75, 0.0, 0.0, 0.85546875, 0.0, 0.9921875, 0.66796875, 0.9921875, 0.765625, 0.765625, 0.0703125, 0.0, 0.765625, 0.99609375, 0.9140625, 0.125, 0.0, 0.765625, 0.99609375, 0.9921875, 0.08203125, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.765625, 0.83984375, 0.0, 0.765625, 0.75, 0.73046875, 0.765625, 0.0, 0.10546875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.765625, 0.0, 0.74609375, 0.765625, 0.0, 0.75, 0.765625, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.765625, 0.765625, 0.35546875, 0.9921875, 0.0, 0.0, 0.765625, 0.0, 0.99609375, 0.99609375, 0.734375, 0.9921875, 0.0, 0.07421875, 0.0, 0.0, 0.765625, 0.99609375, 0.765625, 0.09765625, 0.0, 0.99609375, 0.0, 0.75, 0.765625, 0.9921875, 0.9921875, 0.75, 0.20703125, 0.75, 0.0, 0.80078125, 0.75390625, 0.765625, 0.75, 0.98828125, 0.9921875, 0.0, 0.765625, 0.9921875, 0.765625, 0.98828125, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.77734375, 0.0, 0.0, 0.765625, 0.75390625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.02734375, 0.75, 0.0, 0.0, 0.9921875, 0.75390625, 0.99609375, 0.7578125, 0.765625, 0.73828125, 0.0, 0.9921875, 0.75390625, 0.0, 0.99609375, 0.12109375, 0.0, 0.0, 0.9921875, 0.7578125, 0.76171875, 0.0, 0.765625, 0.9921875, 0.05078125, 0.45703125, 0.75, 0.734375, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.765625, 0.765625, 0.0, 0.9921875, 0.75390625, 0.0, 0.99609375, 0.0, 0.75390625, 0.890625, 0.765625, 0.0, 0.765625, 0.99609375, 0.9921875, 0.765625, 0.765625, 0.0, 0.9921875, 0.765625, 0.74609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.7265625, 0.0, 0.2109375, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0703125, 0.9921875, 0.765625, 0.0625, 0.73828125, 0.765625, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.765625, 0.765625, 0.0, 0.0, 0.48046875, 0.7578125, 0.359375, 0.0, 0.9921875, 0.74609375, 0.765625, 0.9921875, 0.6015625, 0.765625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.765625, 0.74609375, 0.99609375, 0.0, 0.765625, 0.0, 0.76171875, 0.12890625, 0.99609375, 0.9921875, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.0390625, 0.0, 0.74609375, 0.75, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.8515625, 0.9921875, 0.0, 0.98828125, 0.0, 0.078125, 0.9921875, 0.734375, 0.98828125, 0.0, 0.765625, 0.99609375, 0.765625, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.765625, 0.765625, 0.9921875, 0.9921875, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.0, 0.9921875, 0.765625, 0.0, 0.0, 0.765625, 0.75, 0.9921875, 0.765625, 0.9921875, 0.0, 0.765625, 0.765625, 0.75390625, 0.0, 0.0, 0.73828125, 0.9921875, 0.0, 0.765625, 0.9921875, 0.9921875, 0.765625, 0.765625, 0.734375, 0.9921875, 0.0, 0.0, 0.73046875, 0.0, 0.0, 0.9921875, 0.99609375, 0.765625, 0.07421875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.76171875, 0.765625, 0.734375, 0.765625, 0.0, 0.08984375, 0.0703125, 0.0, 0.0, 0.75, 0.078125, 0.0, 0.75, 0.73046875, 0.0, 0.1015625, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.734375, 0.765625, 0.0, 0.71484375, 0.765625, 0.765625, 0.0, 0.85546875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.07421875, 0.765625, 0.7578125, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.98828125, 0.171875, 0.765625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.765625, 0.04296875, 0.9921875, 0.765625, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.73828125, 0.0, 0.765625, 0.765625, 0.765625, 0.99609375, 0.765625, 0.0, 0.7578125, 0.9921875, 0.765625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.765625, 0.75390625, 0.73828125, 0.99609375, 0.0, 0.0, 0.7421875, 0.9921875, 0.0, 0.99609375, 0.0, 0.78515625, 0.0, 0.0, 0.99609375, 0.765625, 0.734375, 0.75390625, 0.0, 0.0, 0.74609375, 0.046875, 0.9921875, 0.9921875, 0.85546875, 0.0, 0.15234375, 0.99609375, 0.76171875, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.109375, 0.765625, 0.765625, 0.0, 0.0, 0.99609375, 0.0, 0.05859375, 0.765625, 0.765625, 0.98828125, 0.0, 0.0, 0.14453125, 0.0, 0.765625, 0.76171875, 0.75390625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.76171875, 0.765625, 0.0, 0.765625, 0.0, 0.1484375, 0.0, 0.765625, 0.9921875, 0.7265625, 0.9921875, 0.0, 0.75, 0.7578125, 0.9921875, 0.99609375, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.765625, 0.74609375, 0.99609375, 0.0, 0.0, 0.0, 0.31640625, 0.84765625, 0.99609375, 0.765625, 0.74609375, 0.0, 0.75390625, 0.9921875, 0.765625, 0.75390625, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.7578125, 0.7578125, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.7578125, 0.7578125, 0.08203125, 0.9921875, 0.765625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.78125, 0.0, 0.765625, 0.765625, 0.921875, 0.765625, 0.04296875, 0.75390625, 0.17578125, 0.765625, 0.765625, 0.734375, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.99609375, 0.09375, 0.7109375, 0.765625, 0.4296875, 0.765625, 0.9921875, 0.75, 0.75, 0.98828125, 0.04296875, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.9921875, 0.765625, 0.0, 0.1328125, 0.87109375, 0.9921875, 0.9921875, 0.7578125, 0.984375, 0.0, 0.9921875, 0.0, 0.765625, 0.0, 0.0, 0.9921875, 0.765625, 0.765625, 0.0, 0.76171875, 0.99609375, 0.0, 0.74609375, 0.0, 0.765625, 0.75390625, 0.7578125, 0.99609375, 0.98828125, 0.0, 0.0, 0.0, 0.99609375, 0.734375, 0.765625, 0.75390625, 0.99609375, 0.0, 0.765625, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.328125, 0.73828125, 0.765625, 0.765625, 0.9921875, 0.99609375, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.98828125, 0.0, 0.0, 0.75390625, 0.0, 0.765625, 0.0, 0.75390625, 0.0, 0.32421875, 0.765625, 0.765625, 0.7265625, 0.765625, 0.72265625, 0.0, 0.0, 0.1796875, 0.0, 0.75390625, 0.765625, 0.9921875, 0.765625, 0.98828125, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.75, 0.9921875, 0.9921875, 0.0, 0.66015625, 0.765625, 0.75, 0.76171875, 0.0, 0.0, 0.765625, 0.99609375, 0.9921875, 0.765625, 0.0, 0.765625, 0.0, 0.9921875, 0.0, 0.99609375, 0.765625, 0.76171875, 0.0, 0.765625, 0.02734375, 0.765625, 0.0, 0.08984375, 0.765625, 0.765625, 0.76171875, 0.9921875, 0.765625, 0.0, 0.0, 0.0, 0.74609375, 0.9921875, 0.0625, 0.765625, 0.75, 0.9921875, 0.98828125, 0.99609375, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.9921875, 0.765625, 0.99609375, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75390625, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.82421875, 0.5390625, 0.9921875, 0.9921875, 0.75, 0.0859375, 0.76171875, 0.7578125, 0.75390625, 0.0, 0.765625, 0.99609375, 0.1015625, 0.765625, 0.765625, 0.9921875, 0.9921875, 0.7578125, 0.74609375, 0.765625, 0.765625, 0.765625, 0.99609375, 0.74609375, 0.9921875, 0.74609375, 0.74609375, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.75, 0.765625, 0.9921875, 0.0, 0.7421875, 0.9921875, 0.015625, 0.75390625, 0.01953125, 0.98828125, 0.0, 0.01953125, 0.0, 0.40234375, 0.0, 0.0, 0.9921875, 0.74609375, 0.0, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.9921875, 0.0, 0.0, 0.75390625, 0.9921875, 0.765625, 0.0, 0.9921875, 0.0, 0.765625, 0.08203125, 0.0, 0.88671875, 0.765625, 0.9921875, 0.765625, 0.078125, 0.0, 0.75, 0.8125, 0.765625, 0.74609375, 0.75, 0.75390625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.9921875, 0.0, 0.75, 0.0, 0.0, 0.765625, 0.75390625, 0.99609375, 0.0, 0.0, 0.7421875, 0.0, 0.765625, 0.0, 0.984375, 0.73046875, 0.0, 0.6953125, 0.765625, 0.765625, 0.9921875, 0.99609375, 0.1171875, 0.9921875, 0.765625, 0.99609375, 0.765625, 0.0, 0.0, 0.7578125, 0.0, 0.76171875, 0.9921875, 0.765625, 0.0, 0.765625, 0.9921875, 0.0, 0.0, 0.0, 0.75390625, 0.765625, 0.7578125, 0.0, 0.0, 0.76171875, 0.35546875, 0.9921875, 0.0, 0.765625, 0.7421875, 0.765625, 0.9921875, 0.0, 0.75, 0.75390625, 0.765625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.75, 0.9921875, 0.765625, 0.734375, 0.765625, 0.7109375, 0.98828125, 0.26953125, 0.15625, 0.734375, 0.765625, 0.9921875, 0.7265625, 0.99609375, 0.0, 0.73828125, 0.74609375, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.734375, 0.0, 0.0, 0.9921875, 0.765625, 0.1953125, 0.9921875, 0.0, 0.7578125, 0.99609375, 0.74609375, 0.9921875, 0.7578125, 0.765625, 0.765625, 0.765625, 0.99609375, 0.765625, 0.765625, 0.7578125, 0.765625, 0.98828125, 0.0703125, 0.765625, 0.9921875, 0.0, 0.0, 0.7421875, 0.98828125, 0.640625, 0.765625, 0.75, 0.73828125, 0.765625, 0.765625, 0.0, 0.75390625, 0.765625, 0.0, 0.4140625, 0.765625, 0.0, 0.0, 0.0, 0.734375, 0.0, 0.74609375, 0.765625, 0.9921875, 0.765625, 0.765625, 0.23828125, 0.9921875, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0546875, 0.98828125, 0.0, 0.98828125, 0.0, 0.98828125, 0.765625, 0.0, 0.0, 0.765625, 0.609375, 0.0, 0.9921875, 0.0, 0.0, 0.765625, 0.75390625, 0.765625, 0.75390625, 0.9921875]

 sparsity of   [0.0, 0.998046875, 0.0, 0.009765625, 0.013671875, 0.056640625, 0.0400390625, 0.02734375, 0.7802734375, 0.0, 0.0, 0.7978515625, 0.0, 0.134765625, 0.9970703125, 0.0, 0.998046875, 0.0, 0.0703125, 0.0, 0.998046875, 0.8681640625, 0.0, 0.93359375, 0.0439453125, 0.9970703125, 0.9970703125, 0.0615234375, 0.068359375, 0.77734375, 0.912109375, 0.998046875, 0.017578125, 0.02734375, 0.0, 0.1259765625, 0.18359375, 0.0400390625, 0.048828125, 0.041015625, 0.140625, 0.0, 0.2431640625, 0.0673828125, 0.9970703125, 0.998046875, 0.998046875, 0.0, 0.99609375, 0.6845703125, 0.9970703125, 0.1162109375, 0.0234375, 0.998046875, 0.0947265625, 0.998046875, 0.0419921875, 0.0, 0.1240234375, 0.998046875, 0.0439453125, 0.0, 0.0, 0.0, 0.0673828125, 0.0712890625, 0.638671875, 0.2431640625, 0.9326171875, 0.064453125, 0.173828125, 0.0, 0.998046875, 0.0517578125, 0.9990234375, 0.998046875, 0.0380859375, 0.884765625, 0.998046875, 0.828125, 0.1728515625, 0.0, 0.0, 0.0625, 0.8798828125, 0.759765625, 0.1005859375, 0.99609375, 0.998046875, 0.060546875, 0.0, 0.0, 0.0, 0.1162109375, 0.150390625, 0.7802734375, 0.0224609375, 0.0, 0.9970703125, 0.0185546875, 0.7158203125, 0.21875, 0.1669921875, 0.0, 0.0302734375, 0.0458984375, 0.7646484375, 0.048828125, 0.99609375, 0.0, 0.998046875, 0.044921875, 0.20703125, 0.0, 0.2744140625, 0.140625, 0.1142578125, 0.998046875, 0.11328125, 0.8974609375, 0.0205078125, 0.0, 0.9970703125, 0.1884765625, 0.2109375, 0.998046875, 0.0, 0.03125, 0.0, 0.0, 0.0361328125, 0.998046875, 0.248046875, 0.0087890625, 0.0107421875, 0.0390625, 0.0869140625, 0.025390625, 0.953125, 0.998046875, 0.0, 0.919921875, 0.0, 0.0234375, 0.109375, 0.0, 0.2333984375, 0.91015625, 0.1298828125, 0.0, 0.861328125, 0.0, 0.998046875, 0.1396484375, 0.1748046875, 0.2744140625, 0.998046875, 0.09375, 0.8583984375, 0.1806640625, 0.927734375, 0.0, 0.998046875, 0.240234375, 0.0244140625, 0.80078125, 0.048828125, 0.0615234375, 0.998046875, 0.0, 0.056640625, 0.0126953125, 0.0380859375, 0.998046875, 0.0, 0.041015625, 0.2900390625, 0.1259765625, 0.390625, 0.0, 0.998046875, 0.0732421875, 0.1064453125, 0.9970703125, 0.021484375, 0.06640625, 0.0, 0.998046875, 0.8896484375, 0.0322265625, 0.3125, 0.94140625, 0.09375, 0.0, 0.0, 0.0615234375, 0.890625, 0.09375, 0.86328125, 0.0, 0.0615234375, 0.0, 0.07421875, 0.0, 0.001953125, 0.19921875, 0.2001953125, 0.041015625, 0.1494140625, 0.1357421875, 0.0908203125, 0.0390625, 0.123046875, 0.0458984375, 0.9951171875, 0.10546875, 0.1259765625, 0.998046875, 0.0771484375, 0.0, 0.0, 0.998046875, 0.390625, 0.0, 0.0625, 0.0, 0.255859375, 0.205078125, 0.03515625, 0.0, 0.2744140625, 0.220703125, 0.849609375, 0.0, 0.998046875, 0.1865234375, 0.9970703125, 0.0, 0.0, 0.998046875, 0.8310546875, 0.9970703125, 0.8828125, 0.0, 0.998046875, 0.9970703125, 0.8916015625, 0.0, 0.025390625, 0.998046875, 0.998046875, 0.0205078125, 0.0, 0.9990234375, 0.017578125, 0.9013671875]

 sparsity of   [0.0455729179084301, 0.0564236119389534, 0.0434027798473835, 0.3359375, 0.0486111119389534, 0.1692708283662796, 0.0776909738779068, 0.0421006940305233, 0.0546875, 0.05078125, 0.1184895858168602, 0.01128472201526165, 0.7239583134651184, 0.2473958283662796, 0.01692708395421505, 0.0, 0.0425347238779068, 0.0594618059694767, 0.075086809694767, 0.0490451380610466, 0.0, 0.0386284738779068, 0.257378488779068, 0.02560763992369175, 0.0455729179084301, 0.999131977558136, 0.0455729179084301, 0.009982638992369175, 0.6987847089767456, 0.0668402761220932, 0.01822916604578495, 0.0516493059694767, 0.0125868059694767, 0.02777777798473835, 0.0598958320915699, 0.0290798619389534, 0.0, 0.02777777798473835, 0.09375, 0.0729166641831398, 0.228298619389534, 0.7013888955116272, 0.7230902910232544, 0.1202256977558136, 0.0, 0.0494791679084301, 0.3446180522441864, 0.0, 0.013454861007630825, 0.0768229141831398, 0.0551215298473835, 0.9418402910232544, 0.0, 0.0616319440305233, 0.7864583134651184, 0.0503472238779068, 0.0598958320915699, 0.7864583134651184, 0.0855034738779068, 0.0681423619389534, 0.0603298619389534, 0.8298611044883728, 0.02300347201526165, 0.0902777761220932, 0.02690972201526165, 0.0, 0.1028645858168602, 0.9157986044883728, 0.0086805559694767, 0.0, 0.0, 0.0, 0.0, 0.7261284589767456, 0.4652777910232544, 0.9995659589767456, 0.0703125, 0.078125, 0.109375, 0.125, 0.0399305559694767, 0.02213541604578495, 0.009114583022892475, 0.0, 0.0850694477558136, 0.0412326380610466, 0.561631977558136, 0.0577256940305233, 0.0, 0.0876736119389534, 0.2005208283662796, 0.0876736119389534, 0.0902777761220932, 0.014322916977107525, 0.0, 0.007378472480922937, 0.6532118320465088, 0.0846354141831398, 0.0911458358168602, 0.9986979365348816, 0.0490451380610466, 0.0859375, 0.5915798544883728, 0.0368923619389534, 0.0533854179084301, 0.0568576380610466, 0.0164930559694767, 0.0568576380610466, 0.1671006977558136, 0.078993059694767, 0.2413194477558136, 0.0607638880610466, 0.706163227558136, 0.2986111044883728, 0.12890625, 0.0004340277810115367, 0.0611979179084301, 0.0225694440305233, 0.02083333395421505, 0.067274309694767, 0.0447048619389534, 0.05078125, 0.0325520820915699, 0.02083333395421505, 0.2109375, 0.00824652798473835, 0.0620659738779068, 0.02300347201526165, 0.0616319440305233, 0.0724826380610466, 0.0447048619389534, 0.08203125, 0.03515625, 0.0, 0.1232638880610466, 0.0243055559694767, 0.01519097201526165, 0.0, 0.0290798619389534, 0.0334201380610466, 0.0902777761220932, 0.6180555820465088, 0.0737847238779068, 0.1072048619389534, 0.0460069440305233, 0.02604166604578495, 0.8580729365348816, 0.4114583432674408, 0.0486111119389534, 0.999131977558136, 0.0, 0.0416666679084301, 0.0381944440305233, 0.5941840410232544, 0.0, 0.0373263880610466, 0.0872395858168602, 0.01909722201526165, 0.0703125, 0.7473958134651184, 0.1011284738779068, 0.2026909738779068, 0.0525173619389534, 0.9509548544883728, 0.0655381977558136, 0.046875, 0.7947048544883728, 0.0, 0.01953125, 0.0, 0.0668402761220932, 0.01171875, 0.4049479067325592, 0.0516493059694767, 0.0546875, 0.659288227558136, 0.0, 0.014322916977107525, 0.1232638880610466, 0.0746527761220932, 0.0989583358168602, 0.0477430559694767, 0.0998263880610466, 0.08203125, 0.0638020858168602, 0.02734375, 0.0802951380610466, 0.4379340410232544, 0.1519097238779068, 0.2222222238779068, 0.0668402761220932, 0.0, 0.01779513992369175, 0.8172743320465088, 0.7734375, 0.16015625, 0.08203125, 0.0720486119389534, 0.082899309694767, 0.094618059694767, 0.1128472238779068, 0.1536458283662796, 0.01519097201526165, 0.1080729141831398, 0.1801215261220932, 0.4231770932674408, 0.01692708395421505, 0.014322916977107525, 0.1714409738779068, 0.0802951380610466, 0.0807291641831398, 0.067274309694767, 0.0, 0.0516493059694767, 0.01822916604578495, 0.0164930559694767, 0.014756944961845875, 0.013888888992369175, 0.2378472238779068, 0.1380208283662796, 0.009114583022892475, 0.0733506977558136, 0.02777777798473835, 0.0798611119389534, 0.690538227558136, 0.02213541604578495, 0.0086805559694767, 0.05078125, 0.0694444477558136, 0.0425347238779068, 0.37109375, 0.1475694477558136, 0.0525173619389534, 0.0, 0.013454861007630825, 0.0798611119389534, 0.4014756977558136, 0.1549479216337204, 0.1197916641831398, 0.02387152798473835, 0.1050347238779068, 0.1961805522441864, 0.0928819477558136, 0.02170138992369175, 0.8346354365348816, 0.01692708395421505, 0.0486111119389534, 0.3602430522441864, 0.55859375, 0.1011284738779068, 0.0594618059694767, 0.5941840410232544, 0.0, 0.05859375, 0.0212673619389534, 0.0]

 sparsity of   [0.0, 0.61328125, 0.0, 0.6953125, 0.796875, 0.9921875, 0.76171875, 0.890625, 0.890625, 0.58984375, 0.890625, 0.546875, 0.98828125, 0.578125, 0.53125, 0.88671875, 0.0, 0.890625, 0.890625, 0.9921875, 0.55859375, 0.83203125, 0.890625, 0.78515625, 0.890625, 0.890625, 0.890625, 0.0, 0.06640625, 0.890625, 0.71484375, 0.890625, 0.890625, 0.68359375, 0.890625, 0.2578125, 0.890625, 0.9921875, 0.64453125, 0.890625, 0.890625, 0.0, 0.890625, 0.890625, 0.6875, 0.05078125, 0.890625, 0.66796875, 0.0, 0.70703125, 0.890625, 0.9921875, 0.78515625, 0.890625, 0.890625, 0.99609375, 0.37109375, 0.81640625, 0.88671875, 0.890625, 0.8515625, 0.62109375, 0.890625, 0.890625, 0.99609375, 0.890625, 0.05859375, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.1796875, 0.890625, 0.3671875, 0.75, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.75390625, 0.70703125, 0.890625, 0.7421875, 0.01171875, 0.0, 0.61328125, 0.6875, 0.890625, 0.890625, 0.9921875, 0.609375, 0.71875, 0.625, 0.9921875, 0.0, 0.890625, 0.6953125, 0.890625, 0.99609375, 0.9921875, 0.890625, 0.9921875, 0.890625, 0.05078125, 0.0, 0.0, 0.63671875, 0.99609375, 0.890625, 0.890625, 0.0, 0.9921875, 0.890625, 0.890625, 0.67578125, 0.890625, 0.9921875, 0.8046875, 0.99609375, 0.7265625, 0.890625, 0.7421875, 0.76171875, 0.6640625, 0.80859375, 0.9921875, 0.76171875, 0.0, 0.67578125, 0.6484375, 0.6640625, 0.99609375, 0.67578125, 0.890625, 0.890625, 0.0, 0.671875, 0.890625, 0.890625, 0.890625, 0.6875, 0.55078125, 0.5390625, 0.734375, 0.58984375, 0.0, 0.75, 0.890625, 0.890625, 0.890625, 0.69140625, 0.58984375, 0.671875, 0.7578125, 0.890625, 0.0, 0.890625, 0.890625, 0.890625, 0.62109375, 0.609375, 0.0, 0.890625, 0.890625, 0.890625, 0.890625, 0.10546875, 0.0, 0.0, 0.69140625, 0.671875, 0.890625, 0.890625, 0.890625, 0.69140625, 0.29296875, 0.671875, 0.9921875, 0.72265625, 0.76171875, 0.6796875, 0.0, 0.76171875, 0.890625, 0.99609375, 0.53515625, 0.890625, 0.68359375, 0.0, 0.1953125, 0.890625, 0.890625, 0.890625, 0.0, 0.7265625, 0.9921875, 0.6484375, 0.890625, 0.890625, 0.0703125, 0.9921875, 0.890625, 0.890625, 0.6171875, 0.57421875, 0.890625, 0.6875, 0.890625, 0.5859375, 0.0, 0.6015625, 0.890625, 0.6015625, 0.0, 0.890625, 0.890625, 0.0, 0.890625, 0.609375, 0.890625, 0.9921875, 0.99609375, 0.67578125, 0.7578125, 0.890625, 0.890625, 0.76171875, 0.63671875, 0.78125, 0.890625, 0.7109375, 0.0, 0.890625, 0.05078125, 0.55859375, 0.0, 0.890625, 0.74609375, 0.890625, 0.82421875, 0.69921875, 0.1640625, 0.76171875, 0.0, 0.890625, 0.890625, 0.6875, 0.94140625, 0.6796875, 0.890625, 0.890625, 0.99609375, 0.7421875, 0.0625, 0.0, 0.0234375, 0.890625, 0.5, 0.54296875, 0.72265625, 0.34375, 0.41796875, 0.6171875, 0.76953125, 0.0, 0.66015625, 0.88671875, 0.29296875, 0.890625, 0.64453125, 0.0, 0.890625, 0.6640625, 0.0, 0.74609375, 0.890625, 0.67578125, 0.6875, 0.734375, 0.51953125, 0.0, 0.890625, 0.0, 0.72265625, 0.08203125, 0.0, 0.52734375, 0.69921875, 0.9921875, 0.890625, 0.703125, 0.890625, 0.0, 0.890625, 0.7109375, 0.890625, 0.08203125, 0.29296875, 0.63671875, 0.98828125, 0.0, 0.61328125, 0.56640625, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.70703125, 0.7421875, 0.0, 0.01171875, 0.98828125, 0.71875, 0.09765625, 0.0, 0.05078125, 0.890625, 0.890625, 0.00390625, 0.05078125, 0.0234375, 0.78125, 0.9921875, 0.75, 0.40625, 0.0, 0.66015625, 0.9921875, 0.890625, 0.71875, 0.7109375, 0.05859375, 0.890625, 0.890625, 0.890625, 0.9453125, 0.3515625, 0.53515625, 0.890625, 0.890625, 0.73828125, 0.9921875, 0.703125, 0.25390625, 0.9921875, 0.890625, 0.890625, 0.890625, 0.0859375, 0.890625, 0.890625, 0.890625, 0.67578125, 0.890625, 0.890625, 0.890625, 0.640625, 0.890625, 0.890625, 0.0, 0.890625, 0.609375, 0.63671875, 0.734375, 0.625, 0.6796875, 0.57421875, 0.890625, 0.0, 0.890625, 0.0, 0.6796875, 0.66796875, 0.0, 0.0, 0.0, 0.890625, 0.6015625, 0.0, 0.6796875, 0.99609375, 0.890625, 0.00390625, 0.890625, 0.71875, 0.7734375, 0.9921875, 0.64453125, 0.890625, 0.703125, 0.890625, 0.63671875, 0.890625, 0.6796875, 0.890625, 0.890625, 0.0, 0.0, 0.0, 0.890625, 0.0, 0.890625, 0.0, 0.9921875, 0.0, 0.19921875, 0.99609375, 0.70703125, 0.078125, 0.0, 0.890625, 0.0, 0.890625, 0.890625, 0.890625, 0.890625, 0.99609375, 0.734375, 0.890625, 0.890625, 0.40234375, 0.5, 0.0, 0.890625, 0.890625, 0.9921875, 0.7109375, 0.8515625, 0.88671875, 0.890625, 0.890625, 0.1875, 0.6796875, 0.890625, 0.9921875, 0.765625, 0.0, 0.0, 0.890625, 0.0, 0.68359375, 0.69921875, 0.890625, 0.890625, 0.99609375, 0.890625, 0.890625, 0.890625, 0.64453125, 0.0, 0.890625, 0.890625, 0.859375, 0.0, 0.4921875, 0.99609375, 0.73046875, 0.890625, 0.890625, 0.65234375, 0.890625, 0.75390625, 0.26171875, 0.890625, 0.890625, 0.734375, 0.890625, 0.03125, 0.98828125, 0.68359375, 0.59375, 0.0, 0.890625, 0.890625, 0.04296875, 0.890625, 0.890625, 0.76953125, 0.0, 0.0, 0.1796875, 0.0, 0.796875, 0.890625, 0.71875, 0.9921875, 0.890625, 0.890625, 0.62109375, 0.0, 0.73828125, 0.890625, 0.890625, 0.890625, 0.0, 0.98828125, 0.0625, 0.609375, 0.890625, 0.0, 0.890625, 0.890625, 0.890625, 0.63671875, 0.890625, 0.484375, 0.69140625, 0.890625, 0.6640625, 0.6484375, 0.890625, 0.890625, 0.0, 0.890625, 0.890625, 0.890625, 0.89453125, 0.890625, 0.9921875, 0.37890625, 0.0, 0.515625, 0.0, 0.60546875, 0.0, 0.890625, 0.640625, 0.890625, 0.890625, 0.890625, 0.70703125, 0.890625, 0.00390625, 0.75, 0.0, 0.890625, 0.99609375, 0.21484375, 0.890625, 0.890625, 0.890625, 0.890625, 0.1875, 0.6796875, 0.66015625, 0.0, 0.0, 0.1484375, 0.9921875, 0.14453125, 0.9921875, 0.76171875, 0.67578125, 0.890625, 0.890625, 0.0, 0.0, 0.890625, 0.0, 0.0, 0.8828125, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.9921875, 0.890625, 0.890625, 0.890625, 0.72265625, 0.890625, 0.9921875, 0.0, 0.62109375, 0.7890625, 0.60546875, 0.890625, 0.890625, 0.61328125, 0.88671875, 0.7109375, 0.9921875, 0.58984375, 0.75, 0.54296875, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.59375, 0.58984375, 0.06640625, 0.890625, 0.609375, 0.890625, 0.9921875, 0.7890625, 0.5546875, 0.78515625, 0.0703125, 0.890625, 0.0, 0.4921875, 0.69140625, 0.890625, 0.890625, 0.0, 0.0, 0.890625, 0.890625, 0.890625, 0.0, 0.07421875, 0.9921875, 0.99609375, 0.703125, 0.64453125, 0.65234375, 0.890625, 0.83203125, 0.265625, 0.6484375, 0.890625, 0.890625, 0.88671875, 0.890625, 0.890625, 0.0, 0.84765625, 0.890625, 0.0, 0.890625, 0.890625, 0.88671875, 0.703125, 0.890625, 0.9921875, 0.65234375, 0.0859375, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.890625, 0.01953125, 0.890625, 0.890625, 0.6171875, 0.890625, 0.5390625, 0.0, 0.54296875, 0.7109375, 0.0, 0.0, 0.890625, 0.890625, 0.65625, 0.640625, 0.890625, 0.0, 0.7265625, 0.0, 0.890625, 0.890625, 0.6640625, 0.6171875, 0.7265625, 0.5234375, 0.0, 0.0, 0.890625, 0.76953125, 0.7265625, 0.0, 0.890625, 0.0, 0.890625, 0.890625, 0.6796875, 0.71875, 0.890625, 0.60546875, 0.45703125, 0.0, 0.9921875, 0.890625, 0.0, 0.65625, 0.59375, 0.890625, 0.9921875, 0.890625, 0.890625, 0.0234375, 0.6640625, 0.0, 0.80859375, 0.6640625, 0.6015625, 0.99609375, 0.0, 0.0, 0.890625, 0.890625, 0.69921875, 0.0390625, 0.890625, 0.69140625, 0.70703125, 0.0625, 0.59375, 0.0, 0.890625, 0.69140625, 0.765625, 0.890625, 0.671875, 0.890625, 0.9921875, 0.890625, 0.99609375, 0.2265625, 0.640625, 0.0, 0.890625, 0.890625, 0.890625, 0.70703125, 0.9921875, 0.6640625, 0.890625, 0.0, 0.62109375, 0.64453125, 0.0859375, 0.20703125, 0.6015625, 0.61328125, 0.9921875, 0.5078125, 0.22265625, 0.890625, 0.890625, 0.12109375, 0.890625, 0.0, 0.5390625, 0.890625, 0.0, 0.0625, 0.56640625, 0.890625, 0.828125, 0.0, 0.890625, 0.0, 0.890625, 0.0, 0.890625, 0.99609375, 0.9921875, 0.0, 0.890625, 0.9921875, 0.890625, 0.890625, 0.9921875, 0.796875, 0.68359375, 0.73046875, 0.890625, 0.890625, 0.75390625, 0.0, 0.890625, 0.88671875, 0.890625, 0.0, 0.890625, 0.74609375, 0.890625, 0.890625, 0.890625, 0.890625, 0.76953125, 0.671875, 0.9921875, 0.68359375, 0.890625, 0.890625, 0.890625, 0.9921875, 0.88671875, 0.63671875, 0.5234375, 0.890625, 0.890625, 0.890625, 0.890625, 0.9921875, 0.7734375, 0.1328125, 0.75, 0.3359375, 0.890625, 0.0, 0.890625, 0.078125, 0.0078125, 0.890625, 0.0390625, 0.98828125, 0.7109375, 0.890625, 0.890625, 0.890625, 0.890625, 0.6328125, 0.890625, 0.9921875, 0.08984375, 0.890625, 0.84375, 0.9921875, 0.890625, 0.0, 0.890625, 0.7421875, 0.0, 0.890625, 0.80859375, 0.0, 0.890625, 0.6328125, 0.22265625, 0.890625, 0.0, 0.7109375, 0.9921875, 0.890625, 0.890625, 0.7421875, 0.8671875, 0.66796875, 0.75, 0.0, 0.578125, 0.57421875, 0.99609375, 0.890625, 0.71875, 0.890625, 0.1015625, 0.890625, 0.7109375, 0.98828125, 0.0, 0.890625, 0.73046875, 0.0, 0.69921875, 0.484375, 0.98828125, 0.5546875, 0.0, 0.4921875, 0.70703125, 0.58984375, 0.99609375, 0.65234375, 0.9921875, 0.9921875, 0.890625, 0.890625, 0.73828125, 0.61328125, 0.0625, 0.75390625, 0.59375, 0.890625, 0.99609375, 0.70703125, 0.71484375, 0.8046875, 0.890625, 0.890625, 0.203125, 0.0, 0.84765625, 0.890625, 0.890625, 0.890625, 0.3359375, 0.67578125, 0.890625, 0.99609375, 0.0, 0.671875, 0.890625, 0.734375, 0.99609375, 0.890625, 0.7265625, 0.890625, 0.5859375, 0.890625, 0.7578125, 0.83203125, 0.0, 0.890625, 0.88671875, 0.99609375, 0.0, 0.0, 0.64453125, 0.65234375, 0.9921875, 0.890625, 0.69921875, 0.70703125, 0.2421875, 0.0, 0.890625, 0.99609375, 0.890625, 0.890625, 0.1640625, 0.9921875, 0.88671875, 0.2265625, 0.72265625, 0.71875, 0.73828125, 0.1015625, 0.890625, 0.890625, 0.890625, 0.890625, 0.7421875, 0.890625, 0.890625, 0.6328125, 0.0, 0.58203125, 0.71875, 0.77734375, 0.890625, 0.9921875, 0.6796875, 0.9921875, 0.65625, 0.890625, 0.890625, 0.890625, 0.890625, 0.67578125, 0.6171875, 0.7421875, 0.6171875, 0.703125, 0.9921875, 0.88671875, 0.99609375, 0.13671875, 0.76953125, 0.60546875, 0.9921875, 0.890625, 0.890625, 0.765625, 0.890625, 0.0, 0.0, 0.890625, 0.890625, 0.80859375, 0.890625, 0.890625, 0.640625, 0.0, 0.7265625, 0.0, 0.890625, 0.890625, 0.890625, 0.73828125, 0.9921875, 0.015625, 0.890625, 0.0, 0.98828125, 0.18359375, 0.0, 0.25390625, 0.0, 0.0234375, 0.59375, 0.890625, 0.03515625, 0.0078125, 0.9921875, 0.890625, 0.1171875, 0.44921875, 0.890625, 0.890625, 0.0, 0.71875, 0.0, 0.890625, 0.05078125, 0.00390625, 0.6640625, 0.890625, 0.890625, 0.890625, 0.9921875]

 sparsity of   [0.998046875, 0.9970703125, 0.998046875, 0.4931640625, 0.9970703125, 0.1328125, 0.998046875, 0.998046875, 0.9970703125, 0.1435546875, 0.9990234375, 0.1279296875, 0.998046875, 0.9970703125, 0.998046875, 0.8212890625, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.083984375, 0.998046875, 0.4482421875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.4521484375, 0.998046875, 0.998046875, 0.998046875, 0.3466796875, 0.1162109375, 0.1376953125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.1533203125, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.8505859375, 0.998046875, 0.9990234375, 0.1142578125, 0.4814453125, 0.9990234375, 0.998046875, 0.998046875, 0.1376953125, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.861328125, 0.9990234375, 0.998046875, 0.2890625, 0.9990234375, 0.9970703125, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.4599609375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.1220703125, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.4951171875, 0.998046875, 0.82421875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.822265625, 0.9970703125, 0.423828125, 0.998046875, 0.998046875, 0.50390625, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.4365234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.4873046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.1220703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.4423828125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.125, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.1767578125, 0.998046875, 0.998046875, 0.4541015625, 0.5126953125, 0.466796875, 0.0, 0.4560546875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.109375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.4443359375, 0.14453125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.4423828125, 0.998046875, 0.4560546875, 0.87109375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.1005859375, 0.279296875, 0.998046875, 0.2001953125, 0.9990234375, 0.998046875, 0.99609375, 0.9990234375, 0.9970703125, 0.9990234375, 0.4541015625, 0.2177734375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.4599609375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.99609375, 0.9990234375, 0.998046875, 0.4970703125, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.5009765625, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.4755859375, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.482421875, 0.99609375, 0.9990234375, 0.9990234375, 0.1259765625, 0.998046875, 0.056640625, 0.9990234375, 0.9970703125, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.4638671875, 0.998046875, 0.9990234375, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1484375, 0.0, 0.9970703125, 0.0, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.4892578125, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.4619140625, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.154296875, 0.998046875, 0.998046875, 0.9990234375, 0.4287109375, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.0, 0.1357421875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.99609375, 0.9990234375, 0.9970703125, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.5029296875, 0.9990234375, 0.46875, 0.150390625, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.1357421875, 0.9990234375, 0.5478515625, 0.9990234375, 0.9970703125, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.4775390625, 0.998046875, 0.4638671875, 0.998046875, 0.998046875, 0.134765625, 0.998046875, 0.998046875, 0.162109375, 0.998046875, 0.998046875, 0.462890625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.99609375, 0.998046875, 0.48046875, 0.1279296875, 0.1171875, 0.998046875, 0.998046875, 0.998046875, 0.1552734375, 0.9990234375, 0.998046875, 0.4599609375, 0.998046875, 0.1171875, 0.1318359375, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.12109375, 0.48046875, 0.9990234375, 0.0400390625, 0.9970703125, 0.9990234375, 0.998046875, 0.9970703125, 0.1181640625, 0.998046875, 0.9990234375, 0.998046875, 0.119140625, 0.998046875, 0.998046875, 0.4697265625, 0.998046875, 0.998046875, 0.458984375, 0.998046875, 0.998046875, 0.458984375, 0.9990234375, 0.9970703125, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.99609375, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.4482421875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375]

 sparsity of   [0.9700520634651184, 0.9995659589767456, 0.9377170205116272, 0.0473090298473835, 0.9995659589767456, 0.993272602558136, 0.1126302108168602, 0.1332465261220932, 0.029296875, 0.0418836809694767, 0.1534288227558136, 0.9995659589767456, 0.0640190988779068, 0.9997829794883728, 0.0303819440305233, 0.112196184694767, 0.0980902761220932, 0.0894097238779068, 0.9989149570465088, 0.1050347238779068, 0.0492621548473835, 0.0347222238779068, 0.0451388880610466, 0.0310329869389534, 0.0447048619389534, 0.142361119389534, 0.9995659589767456, 0.0481770820915699, 0.1100260391831398, 0.2250434011220932, 0.0438368059694767, 0.03059895895421505, 0.0384114570915699, 0.9876301884651184, 0.1605902761220932, 0.0327690988779068, 0.9203559160232544, 0.0460069440305233, 0.0596788190305233, 0.8044704794883728, 0.1039496511220932, 0.9995659589767456, 0.0457899309694767, 0.9995659589767456, 0.0603298619389534, 0.927734375, 0.9995659589767456, 0.0390625, 0.9995659589767456, 0.0609809048473835, 0.9631076455116272, 0.9398871660232544, 0.1178385391831398, 0.9995659589767456, 0.0674913227558136, 0.063368059694767, 0.952256977558136, 0.03081597201526165, 0.0475260429084301, 0.9995659589767456, 0.8901909589767456, 0.9995659589767456, 0.9995659589767456, 0.0401475690305233, 0.0403645820915699, 0.7838541865348816, 0.7970920205116272, 0.0388454869389534, 0.013671875, 0.0622829869389534, 0.9995659589767456, 0.1966145783662796, 0.9379340410232544, 0.9995659589767456, 0.9995659589767456, 0.2510850727558136, 0.1126302108168602, 0.0559895820915699, 0.060546875, 0.0779079869389534, 0.1195746511220932, 0.9993489384651184, 0.0533854179084301, 0.9997829794883728, 0.064453125, 0.9995659589767456, 0.0661892369389534, 0.9993489384651184, 0.9995659589767456, 0.1675347238779068, 0.1336805522441864, 0.0412326380610466, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9448784589767456, 0.232204869389534, 0.17578125, 0.9995659589767456, 0.0920138880610466, 0.928819477558136, 0.0718315988779068, 0.932725727558136, 0.9993489384651184, 0.9956597089767456, 0.9390190839767456, 0.0863715261220932, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.1072048619389534, 0.9995659589767456, 0.0473090298473835, 0.080946184694767, 0.0321180559694767, 0.9995659589767456, 0.015625, 0.08203125, 0.0581597238779068, 0.05078125, 0.0551215298473835, 0.9995659589767456, 0.0401475690305233, 0.8433159589767456, 0.9995659589767456, 0.048828125, 0.0325520820915699, 0.0321180559694767, 0.2272135466337204, 0.7953559160232544, 0.9997829794883728, 0.0, 0.0234375, 0.123046875, 0.8656684160232544, 0.0544704869389534, 0.370659738779068, 0.9251301884651184, 0.029296875, 0.0303819440305233, 0.0475260429084301, 0.954210102558136, 0.0579427070915699, 0.9993489384651184, 0.0518663190305233, 0.0607638880610466, 0.064453125, 0.8322482705116272, 0.9329426884651184, 0.9995659589767456, 0.0366753488779068, 0.9635416865348816, 0.2233072966337204, 0.9602864384651184, 0.03059895895421505, 0.063368059694767, 0.0909288227558136, 0.8810763955116272, 0.9997829794883728, 0.9993489384651184, 0.0674913227558136, 0.9995659589767456, 0.2059461772441864, 0.0345052070915699, 0.12890625, 0.0401475690305233, 0.0505642369389534, 0.0987413227558136, 0.8637152910232544, 0.0505642369389534, 0.8305121660232544, 0.0373263880610466, 0.9995659589767456, 0.0373263880610466, 0.9995659589767456, 0.9995659589767456, 0.0473090298473835, 0.01888020895421505, 0.0355902798473835, 0.0403645820915699, 0.1731770783662796, 0.9995659589767456, 0.9995659589767456, 0.938585102558136, 0.9995659589767456, 0.8795573115348816, 0.0980902761220932, 0.9995659589767456, 0.0314670130610466, 0.0405815988779068, 0.9995659589767456, 0.9995659589767456, 0.963975727558136, 0.0492621548473835, 0.0301649309694767, 0.076171875, 0.02690972201526165, 0.090711809694767, 0.0368923619389534, 0.0542534738779068, 0.1985677033662796, 0.9995659589767456, 0.9995659589767456, 0.889756977558136, 0.9995659589767456, 0.0316840298473835, 0.2389322966337204, 0.0416666679084301, 0.0342881940305233, 0.9995659589767456, 0.9995659589767456, 0.0032552082557231188, 0.03081597201526165, 0.8893229365348816, 0.0753038227558136, 0.9995659589767456, 0.9390190839767456, 0.9997829794883728, 0.8315972089767456, 0.0579427070915699, 0.9995659589767456, 0.0353732630610466, 0.9665798544883728, 0.0345052070915699, 0.1056857630610466, 0.1061197891831398, 0.0614149309694767, 0.9945746660232544, 0.9364149570465088, 0.0462239570915699, 0.9995659589767456, 0.02690972201526165, 0.232421875, 0.9995659589767456, 0.0826822891831398, 0.0577256940305233, 0.0631510391831398, 0.02495659701526165, 0.02799479104578495, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.080946184694767, 0.052734375, 0.9147135615348816, 0.9995659589767456, 0.7858073115348816, 0.9908854365348816, 0.9357638955116272, 0.0301649309694767, 0.0466579869389534, 0.0594618059694767, 0.9995659589767456, 0.0948350727558136, 0.0562065988779068, 0.9995659589767456, 0.0368923619389534, 0.8528645634651184, 0.934678852558136, 0.991319477558136, 0.0342881940305233, 0.0646701380610466, 0.0787760391831398, 0.0338541679084301, 0.9290364384651184, 0.0779079869389534, 0.9995659589767456, 0.9995659589767456, 0.0529513880610466, 0.071180559694767, 0.9375, 0.1184895858168602, 0.8949652910232544, 0.1265190988779068, 0.7884114384651184, 0.9965277910232544, 0.9995659589767456, 0.0852864608168602, 0.9997829794883728, 0.0407986119389534, 0.0282118059694767, 0.9995659589767456, 0.9945746660232544, 0.0611979179084301, 0.9997829794883728, 0.0455729179084301, 0.0436197929084301, 0.1536458283662796, 0.9995659589767456, 0.9995659589767456, 0.8578559160232544, 0.0423177070915699, 0.060546875, 0.9995659589767456, 0.0805121511220932, 0.0338541679084301, 0.9995659589767456, 0.232204869389534, 0.8407118320465088, 0.0336371548473835, 0.8580729365348816, 0.9926215410232544, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.090711809694767, 0.0353732630610466, 0.02278645895421505, 0.0638020858168602, 0.02973090298473835, 0.9934895634651184, 0.0401475690305233, 0.9997829794883728, 0.9993489384651184, 0.0618489570915699, 0.841796875, 0.9993489384651184, 0.0436197929084301, 0.8539496660232544, 0.0746527761220932, 0.9995659589767456, 0.0310329869389534, 0.0453559048473835, 0.082899309694767, 0.0125868059694767, 0.1243489608168602, 0.02886284701526165, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.8363715410232544, 0.9997829794883728, 0.0225694440305233, 0.9995659589767456, 0.9596354365348816, 0.9995659589767456, 0.0559895820915699, 0.0581597238779068, 0.9995659589767456, 0.0329861119389534, 0.9947916865348816, 0.0618489570915699, 0.0700954869389534, 0.0881076380610466, 0.1208767369389534, 0.9993489384651184, 0.951171875, 0.0390625, 0.0679253488779068, 0.1338975727558136, 0.9928385615348816, 0.0377604179084301, 0.0262586809694767, 0.944444477558136, 0.991319477558136, 0.9995659589767456, 0.2113715261220932, 0.1770833283662796, 0.082899309694767, 0.9947916865348816, 0.1154513880610466, 0.9995659589767456, 0.9995659589767456, 0.0642361119389534, 0.0690104141831398, 0.0453559048473835, 0.0457899309694767, 0.0438368059694767, 0.0720486119389534, 0.0321180559694767, 0.950303852558136, 0.9995659589767456, 0.0373263880610466, 0.9995659589767456, 0.9995659589767456, 0.0930989608168602, 0.0290798619389534, 0.9945746660232544, 0.0418836809694767, 0.7821180820465088, 0.090711809694767, 0.0651041641831398, 0.1032986119389534, 0.1052517369389534, 0.1994357705116272, 0.0373263880610466, 0.0373263880610466, 0.9986979365348816, 0.0967881977558136, 0.0642361119389534, 0.9995659589767456, 0.090711809694767, 0.0980902761220932, 0.90234375, 0.7923176884651184, 0.0724826380610466, 0.02777777798473835, 0.8344184160232544, 0.9995659589767456, 0.9995659589767456, 0.02387152798473835, 0.0464409738779068, 0.0483940988779068, 0.9995659589767456, 0.9993489384651184, 0.9958767294883728, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.109375, 0.03081597201526165, 0.9995659589767456, 0.994140625, 0.953125, 0.0203993059694767, 0.0857204869389534, 0.0316840298473835, 0.7858073115348816, 0.0473090298473835, 0.0394965298473835, 0.1169704869389534, 0.1369357705116272, 0.9997829794883728, 0.993272602558136, 0.9995659589767456, 0.9344618320465088, 0.9995659589767456, 0.0531684048473835, 0.2601996660232544, 0.8561198115348816, 0.1167534738779068, 0.052734375, 0.0516493059694767, 0.2599826455116272, 0.2424045205116272, 0.0323350690305233, 0.9995659589767456, 0.9995659589767456, 0.8519965410232544, 0.8689236044883728, 0.03081597201526165, 0.0661892369389534, 0.0941840261220932, 0.02886284701526165, 0.9153645634651184, 0.9993489384651184, 0.9995659589767456, 0.0334201380610466, 0.0681423619389534, 0.9995659589767456, 0.1252170205116272, 0.0881076380610466, 0.9898003339767456, 0.9997829794883728, 0.845703125, 0.9613715410232544, 0.0271267369389534, 0.0444878488779068, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.9516059160232544, 0.9997829794883728, 0.9995659589767456, 0.0631510391831398, 0.048828125, 0.9641926884651184, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.02408854104578495, 0.0744357630610466, 0.1358506977558136, 0.9995659589767456, 0.1410590261220932, 0.9418402910232544, 0.9995659589767456, 0.2463107705116272, 0.02408854104578495, 0.0703125, 0.9997829794883728, 0.04296875, 0.9995659589767456, 0.9431423544883728, 0.02799479104578495, 0.0831163227558136, 0.067274309694767, 0.0475260429084301, 0.0833333358168602, 0.9704861044883728, 0.0390625, 0.0575086809694767, 0.8919270634651184, 0.01692708395421505, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9108073115348816, 0.0883246511220932, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.1176215261220932, 0.0579427070915699, 0.0648871511220932, 0.9381510615348816, 0.8782551884651184, 0.9943576455116272, 0.0347222238779068, 0.02495659701526165, 0.9364149570465088]

 sparsity of   [0.927734375, 0.0, 0.021484375, 0.078125, 0.10546875, 0.005859375, 0.029296875, 0.01171875, 0.08984375, 0.826171875, 0.66796875, 0.658203125, 0.814453125, 0.015625, 0.751953125, 0.673828125, 0.0234375, 0.01171875, 0.912109375, 0.66796875, 0.0546875, 0.62109375, 0.0078125, 0.66015625, 0.130859375, 0.013671875, 0.83984375, 0.99609375, 0.001953125, 0.076171875, 0.046875, 0.154296875, 0.99609375, 0.060546875, 0.8359375, 0.041015625, 0.998046875, 0.2109375, 0.62890625, 0.021484375, 0.822265625, 0.875, 0.580078125, 0.80859375, 0.078125, 0.6953125, 0.064453125, 0.724609375, 0.998046875, 0.03515625, 0.982421875, 0.67578125, 0.033203125, 0.806640625, 0.0078125, 0.99609375, 0.6875, 0.630859375, 0.875, 0.71484375, 0.04296875, 0.787109375, 0.63671875, 0.8359375, 0.1484375, 0.80078125, 0.634765625, 0.83203125, 0.00390625, 0.765625, 0.017578125, 0.982421875, 0.04296875, 0.849609375, 0.982421875, 0.0859375, 0.69140625, 0.025390625, 0.646484375, 0.080078125, 0.03515625, 0.787109375, 0.001953125, 0.099609375, 0.048828125, 0.6953125, 0.0, 0.646484375, 0.0859375, 0.912109375, 0.994140625, 0.998046875, 0.140625, 0.931640625, 0.12109375, 0.861328125, 0.111328125, 0.005859375, 0.998046875, 0.125, 0.091796875, 0.009765625, 0.681640625, 0.017578125, 0.541015625, 0.064453125, 0.0390625, 0.6328125, 0.634765625, 0.634765625, 0.025390625, 0.0, 0.826171875, 0.580078125, 0.654296875, 0.064453125, 0.8828125, 0.982421875, 0.669921875, 0.041015625, 0.626953125, 0.73828125, 0.009765625, 0.033203125, 0.998046875, 0.857421875, 0.10546875, 0.998046875, 0.013671875, 0.4921875, 0.64453125, 0.626953125, 0.03125, 0.63671875, 0.03125, 0.68359375, 0.796875, 0.994140625, 0.10546875, 0.0625, 0.82421875, 0.958984375, 0.90234375, 0.0234375, 0.01953125, 0.93359375, 0.685546875, 0.623046875, 0.021484375, 0.650390625, 0.91015625, 0.671875, 0.048828125, 0.982421875, 0.02734375, 0.123046875, 0.07421875, 0.02734375, 0.99609375, 0.7421875, 0.6328125, 0.033203125, 0.767578125, 0.0234375, 0.67578125, 0.26171875, 0.81640625, 0.126953125, 0.8984375, 0.99609375, 0.01953125, 0.6328125, 0.0, 0.833984375, 0.013671875, 0.0, 0.00390625, 0.01953125, 0.796875, 0.943359375, 0.0390625, 0.7734375, 0.685546875, 0.0, 0.626953125, 0.962890625, 0.900390625, 0.98046875, 0.755859375, 0.01953125, 0.6171875, 0.09765625, 0.8359375, 0.93359375, 0.640625, 0.037109375, 0.669921875, 0.841796875, 0.0234375, 0.03125, 0.017578125, 0.888671875, 0.046875, 0.982421875, 0.6640625, 0.02734375, 0.048828125, 0.630859375, 0.904296875, 0.888671875, 0.037109375, 0.134765625, 0.71875, 0.830078125, 0.87109375, 0.05078125, 0.025390625, 0.009765625, 0.625, 0.638671875, 0.8203125, 0.6484375, 0.0859375, 0.7265625, 0.021484375, 0.796875, 0.251953125, 0.99609375, 0.6640625, 0.033203125, 0.044921875, 0.65625, 0.640625, 0.091796875, 0.021484375, 0.828125, 0.037109375, 0.03125, 0.87890625, 0.025390625, 0.12890625, 0.013671875, 0.896484375, 0.025390625, 0.66015625, 0.83203125, 0.037109375, 0.0234375, 0.015625, 0.0, 0.935546875, 0.0703125, 0.8671875, 0.876953125, 0.013671875, 0.658203125, 0.955078125, 0.857421875, 0.71875, 0.05078125, 0.982421875, 0.267578125, 0.884765625, 0.982421875, 0.044921875, 0.880859375, 0.6328125, 0.01171875, 0.029296875, 0.63671875, 0.998046875, 0.90234375, 0.6875, 0.033203125, 0.728515625, 0.02734375, 0.638671875, 0.015625, 0.63671875, 0.068359375, 0.015625, 0.009765625, 0.662109375, 0.095703125, 0.62109375, 0.013671875, 0.705078125, 0.681640625, 0.677734375, 0.033203125, 0.705078125, 0.673828125, 0.529296875, 0.849609375, 0.662109375, 0.01953125, 0.634765625, 0.841796875, 0.916015625, 0.017578125, 0.7578125, 0.83984375, 0.927734375, 0.982421875, 0.94921875, 0.982421875, 0.93359375, 0.623046875, 0.708984375, 0.998046875, 0.99609375, 0.033203125, 0.79296875, 0.67578125, 0.048828125, 0.9296875, 0.958984375, 0.013671875, 0.99609375, 0.138671875, 0.13671875, 0.625, 0.947265625, 0.982421875, 0.955078125, 0.982421875, 0.048828125, 0.017578125, 0.689453125, 0.71484375, 0.982421875, 0.09375, 0.955078125, 0.0859375, 0.90625, 0.6484375, 0.037109375, 0.021484375, 0.998046875, 0.900390625, 0.783203125, 0.033203125, 0.646484375, 0.6328125, 0.66796875, 0.009765625, 0.021484375, 0.99609375, 0.044921875, 0.85546875, 0.0, 0.927734375, 0.1015625, 0.0546875, 0.78515625, 0.662109375, 0.05078125, 0.01953125, 0.13671875, 0.0234375, 0.63671875, 0.845703125, 0.822265625, 0.646484375, 0.03515625, 0.021484375, 0.86328125, 0.03515625, 0.017578125, 0.0, 0.90625, 0.775390625, 0.01171875, 0.171875, 0.634765625, 0.830078125, 0.67578125, 0.90234375, 0.0859375, 0.0234375, 0.951171875, 0.025390625, 0.84765625, 0.642578125, 0.982421875, 0.154296875, 0.025390625, 0.951171875, 0.876953125, 0.990234375, 0.677734375, 0.84765625, 0.83984375, 0.779296875, 0.744140625, 0.982421875, 0.04296875, 0.64453125, 0.083984375, 0.01953125, 0.08984375, 0.099609375, 0.037109375, 0.076171875, 0.771484375, 0.044921875, 0.94921875, 0.076171875, 0.01953125, 0.2421875, 0.029296875, 0.923828125, 0.833984375, 0.02734375, 0.634765625, 0.794921875, 0.671875, 0.708984375, 0.3046875, 0.998046875, 0.779296875, 0.626953125, 0.681640625, 0.048828125, 0.982421875, 0.013671875, 0.0703125, 0.666015625, 0.06640625, 0.67578125, 0.048828125, 0.65234375, 0.0703125, 0.021484375, 0.015625, 0.681640625, 0.998046875, 0.11328125, 0.06640625, 0.08984375, 0.08203125, 0.6796875, 0.052734375, 0.732421875, 0.02734375, 0.0234375, 0.890625, 0.798828125, 0.111328125, 0.6640625, 0.0, 0.03125, 0.02734375, 0.63671875, 0.091796875, 0.11328125, 0.65625, 0.15625, 0.625, 0.01953125, 0.734375, 0.025390625, 0.01171875, 0.9609375, 0.025390625, 0.033203125, 0.994140625, 0.6953125, 0.626953125, 0.048828125, 0.68359375, 0.693359375, 0.041015625, 0.0546875, 0.04296875, 0.927734375, 0.115234375, 0.01171875, 0.685546875, 0.03125, 0.779296875, 0.982421875, 0.9140625, 0.013671875, 0.0234375, 0.025390625, 0.84765625, 0.041015625, 0.869140625, 0.640625, 0.638671875, 0.015625, 0.75, 0.037109375, 0.05859375, 0.826171875, 0.00390625, 0.873046875, 0.884765625, 0.033203125, 0.64453125, 0.6796875, 0.943359375, 0.634765625, 0.6484375, 0.681640625, 0.01953125, 0.056640625, 0.5703125, 0.029296875, 0.115234375, 0.015625, 0.935546875, 0.09765625, 0.935546875, 0.982421875, 0.0234375, 0.111328125, 0.009765625, 0.91015625, 0.029296875, 0.677734375, 0.890625, 0.982421875, 0.9140625, 0.01171875, 0.87109375, 0.912109375, 0.0, 0.998046875, 0.0234375, 0.119140625, 0.66796875, 0.8828125, 0.759765625, 0.861328125, 0.037109375, 0.015625, 0.03125, 0.001953125, 0.8203125, 0.94921875, 0.03125, 0.013671875, 0.08203125, 0.0703125, 0.048828125, 0.01953125, 0.791015625, 0.01953125, 0.845703125, 0.017578125, 0.716796875, 0.083984375, 0.658203125, 0.982421875, 0.78515625, 0.0234375, 0.6484375, 0.005859375, 0.01171875, 0.02734375, 0.93359375, 0.025390625, 0.62890625, 0.11328125, 0.033203125, 0.0546875, 0.04296875, 0.060546875, 0.013671875, 0.861328125, 0.99609375, 0.873046875, 0.669921875, 0.013671875, 0.65234375, 0.982421875, 0.625, 0.87890625, 0.03125, 0.12890625, 0.888671875, 0.041015625, 0.060546875, 0.96484375, 0.875, 0.03515625, 0.0546875, 0.01171875, 0.849609375, 0.091796875, 0.66015625, 0.6640625, 0.6953125, 0.078125, 0.935546875, 0.015625, 0.775390625, 0.037109375, 0.056640625, 0.662109375, 0.076171875, 0.03125, 0.1015625, 0.001953125, 0.021484375, 0.935546875, 0.931640625, 0.970703125, 0.88671875, 0.07421875, 0.8984375, 0.044921875, 0.662109375, 0.99609375, 0.1484375, 0.65234375, 0.982421875, 0.625, 0.041015625, 0.015625, 0.634765625, 0.923828125, 0.009765625, 0.015625, 0.009765625, 0.982421875, 0.99609375, 0.9140625, 0.091796875, 0.982421875, 0.66796875, 0.0546875, 0.025390625, 0.0546875, 0.318359375, 0.0078125, 0.875, 0.001953125, 0.630859375, 0.06640625, 0.626953125, 0.630859375, 0.63671875, 0.025390625, 0.998046875, 0.0234375, 0.23828125, 0.708984375, 0.998046875, 0.642578125, 0.890625, 0.08203125, 0.8984375, 0.802734375, 0.04296875, 0.994140625, 0.994140625, 0.544921875, 0.83984375, 0.025390625, 0.0390625, 0.02734375, 0.03515625, 0.064453125, 0.189453125, 0.78515625, 0.05078125, 0.626953125, 0.576171875, 0.1328125, 0.01171875, 0.0703125, 0.033203125, 0.94140625, 0.033203125, 0.951171875, 0.919921875, 0.80078125, 0.810546875, 0.021484375, 0.021484375, 0.224609375, 0.669921875, 0.005859375, 0.060546875, 0.05859375, 0.982421875, 0.029296875, 0.041015625, 0.982421875, 0.013671875, 0.673828125, 0.900390625, 0.65234375, 0.73828125, 0.662109375, 0.0625, 0.998046875, 0.986328125, 0.876953125, 0.037109375, 0.6796875, 0.63671875, 0.013671875, 0.623046875, 0.06640625, 0.982421875, 0.904296875, 0.6171875, 0.82421875, 0.095703125, 0.037109375, 0.7421875, 0.046875, 0.021484375, 0.669921875, 0.0546875, 0.9375, 0.638671875, 0.076171875, 0.03125, 0.009765625, 0.982421875, 0.0625, 0.0078125, 0.625, 0.6171875, 0.009765625, 0.021484375, 0.677734375, 0.01953125, 0.96484375, 0.021484375, 0.04296875, 0.849609375, 0.859375, 0.01171875, 0.0546875, 0.982421875, 0.03515625, 0.1015625, 0.671875, 0.0625, 0.931640625, 0.826171875, 0.982421875, 0.8203125, 0.08203125, 0.63671875, 0.73046875, 0.6328125, 0.0078125, 0.66796875, 0.017578125, 0.615234375, 0.046875, 0.63671875, 0.068359375, 0.146484375, 0.443359375, 0.103515625, 0.962890625, 0.998046875, 0.060546875, 0.6640625, 0.38671875, 0.62109375, 0.037109375, 0.939453125, 0.05078125, 0.650390625, 0.998046875, 0.640625, 0.0625, 0.982421875, 0.16796875, 0.990234375, 0.033203125, 0.037109375, 0.666015625, 0.626953125, 0.6328125, 0.982421875, 0.029296875, 0.052734375, 0.626953125, 0.1015625, 0.287109375, 0.017578125, 0.65625, 0.6953125, 0.076171875, 0.767578125, 0.03515625, 0.978515625, 0.9375, 0.04296875, 0.966796875, 0.0078125, 0.10546875, 0.021484375, 0.765625, 0.021484375, 0.056640625, 0.09765625, 0.634765625, 0.080078125, 0.005859375, 0.0546875, 0.638671875, 0.037109375, 0.859375, 0.873046875, 0.6328125, 0.791015625, 0.84765625, 0.048828125, 0.68359375, 0.908203125, 0.650390625, 0.041015625, 0.857421875, 0.025390625, 0.009765625, 0.982421875, 0.982421875, 0.630859375, 0.66015625, 0.837890625, 0.736328125, 0.794921875, 0.017578125, 0.626953125, 0.740234375, 0.81640625, 0.99609375, 0.01953125, 0.0234375, 0.630859375, 0.046875, 0.04296875, 0.015625, 0.86328125, 0.07421875, 0.673828125, 0.86328125, 0.982421875, 0.01953125, 0.048828125, 0.03515625, 0.017578125, 0.060546875, 0.01953125, 0.013671875, 0.03515625, 0.927734375, 0.009765625, 0.88671875, 0.96484375, 0.08984375, 0.0078125, 0.017578125, 0.0234375, 0.673828125, 0.99609375, 0.0546875, 0.763671875, 0.095703125, 0.650390625, 0.994140625, 0.94921875, 0.0078125, 0.890625, 0.541015625, 0.068359375, 0.015625, 0.693359375, 0.998046875, 0.7890625, 0.982421875, 0.671875, 0.017578125, 0.62890625, 0.884765625, 0.982421875, 0.982421875, 0.724609375, 0.046875, 0.982421875, 0.982421875, 0.892578125, 0.021484375, 0.8515625, 0.005859375, 0.982421875, 0.01953125, 0.01171875, 0.89453125, 0.685546875, 0.025390625, 0.84375, 0.669921875, 0.830078125, 0.03515625, 0.85546875, 0.798828125, 0.97265625, 0.982421875, 0.669921875, 0.833984375, 0.68359375, 0.078125, 0.6328125, 0.67578125, 0.0234375, 0.880859375, 0.8359375, 0.021484375, 0.630859375, 0.025390625, 0.716796875, 0.126953125, 0.640625, 0.87109375, 0.048828125, 0.005859375, 0.03515625, 0.62109375, 0.021484375, 0.646484375, 0.947265625, 0.662109375, 0.119140625, 0.0390625, 0.63671875, 0.015625, 0.794921875, 0.982421875, 0.65234375, 0.037109375, 0.640625, 0.048828125, 0.814453125, 0.0625, 0.03125, 0.013671875, 0.62109375, 0.02734375, 0.017578125, 0.255859375, 0.84375, 0.94921875, 0.02734375, 0.015625, 0.814453125, 0.072265625, 0.025390625, 0.998046875, 0.830078125, 0.02734375, 0.685546875, 0.771484375, 0.025390625, 0.982421875, 0.009765625, 0.701171875, 0.02734375, 0.6953125, 0.634765625, 0.640625, 0.0, 0.1328125, 0.107421875, 0.896484375, 0.982421875, 0.849609375, 0.015625, 0.849609375, 0.634765625, 0.056640625, 0.626953125, 0.99609375, 0.623046875, 0.66796875, 0.634765625, 0.0234375, 0.03125, 0.22265625, 0.01953125, 0.03125, 0.029296875, 0.01953125, 0.802734375, 0.041015625, 0.654296875, 0.931640625, 0.677734375, 0.982421875, 0.994140625, 0.779296875, 0.083984375, 0.009765625, 0.671875, 0.015625, 0.04296875, 0.634765625, 0.642578125, 0.69921875, 0.8828125, 0.037109375, 0.0078125, 0.833984375, 0.8671875, 0.01171875, 0.626953125, 0.04296875, 0.982421875, 0.982421875, 0.896484375, 0.99609375, 0.0078125, 0.931640625, 0.6796875, 0.66015625, 0.025390625, 0.703125, 0.6328125, 0.013671875, 0.09375, 0.076171875, 0.04296875, 0.68359375, 0.796875, 0.654296875, 0.982421875, 0.09375, 0.7109375, 0.017578125, 0.017578125, 0.654296875, 0.0546875, 0.662109375, 0.0390625, 0.056640625, 0.01953125, 0.90625, 0.67578125, 0.662109375, 0.86328125, 0.6328125, 0.875, 0.078125, 0.64453125, 0.001953125, 0.00390625, 0.94140625, 0.7109375, 0.041015625, 0.701171875, 0.826171875, 0.015625, 0.02734375, 0.6796875, 0.642578125, 0.640625, 0.03125, 0.037109375, 0.025390625, 0.037109375, 0.888671875, 0.66015625, 0.03125, 0.99609375, 0.66796875, 0.982421875, 0.99609375, 0.765625, 0.79296875, 0.912109375, 0.998046875, 0.02734375, 0.76171875, 0.912109375, 0.01953125, 0.626953125, 0.154296875, 0.982421875, 0.09375, 0.205078125, 0.998046875, 0.02734375, 0.033203125, 0.732421875, 0.70703125, 0.935546875, 0.017578125, 0.8203125, 0.6953125, 0.896484375, 0.083984375, 0.654296875, 0.00390625, 0.01953125, 0.703125, 0.8125, 0.0, 0.029296875, 0.03125, 0.173828125, 0.021484375, 0.029296875, 0.982421875, 0.66796875, 0.392578125, 0.73046875, 0.01953125, 0.015625, 0.99609375, 0.998046875, 0.03515625, 0.234375, 0.052734375, 0.67578125, 0.74609375, 0.638671875, 0.51171875, 0.99609375, 0.798828125, 0.001953125, 0.6328125, 0.720703125, 0.041015625, 0.998046875, 0.02734375, 0.02734375, 0.8515625, 0.87109375, 0.189453125, 0.650390625, 0.8359375, 0.916015625, 0.1015625, 0.09375, 0.037109375, 0.013671875, 0.98046875, 0.8984375, 0.783203125, 0.017578125, 0.68359375, 0.69140625, 0.67578125, 0.982421875, 0.908203125, 0.029296875, 0.041015625, 0.955078125, 0.884765625, 0.0234375, 0.876953125, 0.6875, 0.01171875, 0.9140625, 0.802734375, 0.982421875, 0.673828125, 0.005859375, 0.982421875, 0.982421875, 0.828125, 0.541015625, 0.916015625, 0.865234375, 0.994140625, 0.939453125, 0.0078125, 0.982421875, 0.7265625, 0.662109375, 0.818359375, 0.06640625, 0.818359375, 0.95703125, 0.064453125, 0.017578125, 0.037109375, 0.62890625, 0.673828125, 0.84375, 0.951171875, 0.646484375, 0.828125, 0.0, 0.7421875, 0.80859375, 0.07421875, 0.68359375, 0.640625, 0.69921875, 0.265625, 0.6328125, 0.11328125, 0.494140625, 0.046875, 0.634765625, 0.046875, 0.029296875, 0.0625, 0.63671875, 0.72265625, 0.6875, 0.6328125, 0.947265625, 0.0546875, 0.998046875, 0.638671875, 0.953125, 0.767578125, 0.037109375, 0.873046875, 0.861328125, 0.671875, 0.10546875, 0.625, 0.037109375, 0.0703125, 0.044921875, 0.9921875, 0.0546875, 0.923828125, 0.6484375, 0.64453125, 0.857421875, 0.6484375, 0.884765625, 0.078125, 0.962890625, 0.982421875, 0.916015625, 0.01171875, 0.708984375, 0.02734375, 0.998046875, 0.998046875, 0.982421875, 0.67578125, 0.669921875, 0.16015625, 0.982421875, 0.998046875, 0.02734375, 0.955078125, 0.00390625, 0.92578125, 0.01171875, 0.6484375, 0.763671875, 0.1640625, 0.0234375, 0.05078125, 0.6953125, 0.8125, 0.05078125, 0.830078125, 0.994140625, 0.953125, 0.998046875, 0.76171875, 0.658203125, 0.150390625, 0.06640625, 0.03125, 0.080078125, 0.615234375, 0.103515625, 0.85546875, 0.05859375, 0.6796875, 0.84765625, 0.638671875, 0.642578125, 0.029296875, 0.671875, 0.677734375, 0.630859375, 0.02734375, 0.94921875, 0.91796875, 0.93359375, 0.630859375, 0.044921875, 0.931640625, 0.03125, 0.78125, 0.80859375, 0.638671875, 0.705078125, 0.982421875, 0.99609375, 0.998046875, 0.017578125, 0.025390625, 0.994140625, 0.775390625, 0.501953125, 0.041015625, 0.982421875, 0.802734375, 0.99609375, 0.99609375, 0.021484375, 0.068359375, 0.64453125, 0.669921875, 0.84765625, 0.99609375, 0.662109375, 0.87109375, 0.982421875, 0.775390625, 0.0390625, 0.798828125, 0.888671875, 0.0859375, 0.6328125, 0.982421875, 0.982421875, 0.048828125, 0.69140625, 0.92578125, 0.09375, 0.060546875, 0.015625, 0.982421875, 0.00390625, 0.0390625, 0.0390625, 0.0234375, 0.876953125, 0.091796875, 0.939453125, 0.76171875, 0.056640625, 0.982421875, 0.98046875, 0.02734375, 0.091796875, 0.048828125, 0.029296875, 0.029296875, 0.982421875, 0.029296875, 0.64453125, 0.625, 0.779296875, 0.677734375, 0.78125, 0.92578125, 0.044921875, 0.10546875, 0.01171875, 0.927734375, 0.04296875, 0.017578125, 0.19140625, 0.998046875, 0.927734375, 0.642578125, 0.095703125, 0.7265625, 0.642578125, 0.025390625, 0.025390625, 0.869140625, 0.982421875, 0.08203125, 0.662109375, 0.66796875, 0.796875, 0.68359375, 0.037109375, 0.955078125, 0.890625, 0.982421875, 0.2265625, 0.0234375, 0.0390625, 0.736328125, 0.158203125, 0.689453125, 0.029296875, 0.8203125, 0.99609375, 0.669921875, 0.021484375, 0.640625, 0.982421875, 0.650390625, 0.08984375, 0.982421875, 0.654296875, 0.9375, 0.83984375, 0.810546875, 0.01953125, 0.982421875, 0.001953125, 0.0625, 0.68359375, 0.9296875, 0.01171875, 0.037109375, 0.775390625, 0.056640625, 0.916015625, 0.654296875, 0.05078125, 0.87109375, 0.736328125, 0.78515625, 0.634765625, 0.794921875, 0.982421875, 0.11328125, 0.099609375, 0.037109375, 0.01171875, 0.056640625, 0.095703125, 0.73828125, 0.939453125, 0.998046875, 0.001953125, 0.91796875, 0.62890625, 0.81640625, 0.037109375, 0.625, 0.625, 0.982421875, 0.65234375, 0.013671875, 0.0546875, 0.677734375, 0.642578125, 0.67578125, 0.013671875, 0.99609375, 0.068359375, 0.03515625, 0.8671875, 0.958984375, 0.75, 0.666015625, 0.63671875, 0.025390625, 0.017578125, 0.986328125, 0.892578125, 0.064453125, 0.94921875, 0.103515625, 0.681640625, 0.796875, 0.998046875, 0.087890625, 0.84765625, 0.982421875, 0.021484375, 0.814453125, 0.015625, 0.03125, 0.7421875, 0.658203125, 0.041015625, 0.015625, 0.021484375, 0.103515625, 0.818359375, 0.8828125, 0.927734375, 0.033203125, 0.634765625, 0.982421875, 0.0234375, 0.0234375, 0.1484375, 0.640625, 0.828125, 0.017578125, 0.666015625, 0.01953125, 0.828125, 0.724609375, 0.041015625, 0.8359375, 0.640625, 0.640625, 0.642578125, 0.982421875, 0.0234375, 0.056640625, 0.919921875, 0.9140625, 0.025390625, 0.0390625, 0.04296875, 0.673828125, 0.99609375, 0.0390625, 0.865234375, 0.99609375, 0.037109375, 0.646484375, 0.921875, 0.671875, 0.0390625, 0.078125, 0.03515625, 0.140625, 0.041015625, 0.892578125, 0.05859375, 0.298828125, 0.021484375, 0.052734375, 0.9296875, 0.6640625, 0.01171875, 0.396484375, 0.0859375, 0.669921875, 0.2578125, 0.2109375, 0.044921875, 0.873046875, 0.982421875, 0.677734375, 0.685546875, 0.6484375, 0.998046875, 0.0078125, 0.990234375, 0.640625, 0.05859375, 0.640625, 0.642578125, 0.982421875, 0.0625, 0.9765625, 0.009765625, 0.951171875, 0.046875, 0.958984375, 0.044921875, 0.876953125, 0.7109375, 0.734375, 0.640625, 0.01171875, 0.72265625, 0.087890625, 0.83203125, 0.109375, 0.642578125, 0.67578125, 0.830078125, 0.982421875, 0.056640625, 0.009765625, 0.626953125, 0.0234375, 0.64453125, 0.080078125, 0.8359375, 0.490234375, 0.798828125, 0.76171875, 0.982421875, 0.02734375, 0.68359375, 0.916015625, 0.013671875, 0.01171875, 0.015625, 0.8515625, 0.97265625, 0.705078125, 0.01953125, 0.08203125, 0.689453125, 0.6796875, 0.982421875, 0.853515625, 0.900390625, 0.00390625, 0.03515625, 0.662109375, 0.857421875, 0.0546875, 0.65625, 0.033203125, 0.8828125, 0.6875, 0.021484375, 0.634765625, 0.03125, 0.99609375, 0.64453125, 0.037109375, 0.01953125, 0.86328125, 0.09375, 0.736328125, 0.103515625, 0.654296875, 0.982421875, 0.6484375, 0.10546875, 0.638671875, 0.982421875, 0.919921875, 0.619140625, 0.962890625, 0.69140625, 0.04296875, 0.072265625, 0.908203125, 0.03125, 0.82421875, 0.2265625, 0.650390625, 0.876953125, 0.982421875, 0.0546875, 0.05859375, 0.12890625, 0.033203125, 0.892578125, 0.384765625, 0.0390625, 0.013671875, 0.02734375, 0.998046875, 0.056640625, 0.677734375, 0.69921875, 0.99609375, 0.6796875, 0.83203125, 0.69140625, 0.029296875, 0.021484375, 0.673828125, 0.716796875, 0.828125, 0.056640625, 0.982421875, 0.900390625, 0.03515625, 0.693359375, 0.890625, 0.048828125, 0.89453125, 0.982421875, 0.6875, 0.982421875, 0.92578125, 0.927734375, 0.017578125, 0.64453125, 0.025390625, 0.76171875, 0.701171875, 0.896484375, 0.744140625, 0.888671875, 0.03125, 0.302734375, 0.62109375, 0.037109375, 0.0703125, 0.650390625, 0.017578125, 0.0234375, 0.263671875, 0.048828125, 0.8515625, 0.64453125, 0.798828125, 0.25390625, 0.677734375, 0.595703125, 0.998046875, 0.91015625, 0.0234375, 0.017578125, 0.931640625, 0.640625, 0.044921875, 0.982421875, 0.0234375, 0.666015625, 0.0234375, 0.798828125, 0.150390625, 0.0859375, 0.908203125, 0.021484375, 0.046875, 0.982421875, 0.03125, 0.982421875, 0.6328125, 0.93359375, 0.06640625, 0.0234375, 0.03515625, 0.05078125, 0.01171875, 0.01171875, 0.833984375, 0.884765625, 0.6875, 0.63671875, 0.994140625, 0.998046875, 0.802734375, 0.03125, 0.009765625, 0.642578125, 0.83203125, 0.123046875, 0.025390625, 0.982421875, 0.029296875, 0.841796875, 0.6875, 0.125, 0.029296875, 0.111328125, 0.6796875, 0.6953125, 0.619140625, 0.080078125, 0.03515625, 0.044921875, 0.91796875, 0.7421875, 0.029296875, 0.716796875, 0.73828125, 0.998046875, 0.044921875, 0.927734375, 0.01171875, 0.99609375, 0.982421875, 0.017578125, 0.037109375, 0.96484375, 0.626953125, 0.078125, 0.9453125, 0.86328125, 0.9375, 0.6875, 0.91796875, 0.982421875, 0.0234375, 0.716796875, 0.982421875, 0.638671875, 0.630859375, 0.6953125, 0.89453125, 0.001953125, 0.982421875, 0.8203125, 0.0234375, 0.03515625, 0.646484375, 0.697265625, 0.05078125, 0.63671875, 0.05078125, 0.982421875, 0.685546875, 0.05078125, 0.150390625, 0.025390625, 0.01171875, 0.931640625, 0.01953125, 0.099609375, 0.00390625, 0.025390625, 0.001953125, 0.0390625, 0.6953125, 0.6796875, 0.6328125, 0.751953125, 0.8359375, 0.90625, 0.044921875, 0.693359375, 0.6328125, 0.8515625, 0.84375, 0.982421875, 0.0546875, 0.0234375, 0.0859375, 0.015625, 0.712890625, 0.0, 0.154296875, 0.953125, 0.06640625, 0.181640625, 0.982421875, 0.982421875, 0.0, 0.11328125, 0.62890625, 0.015625, 0.052734375, 0.52734375, 0.068359375, 0.001953125, 0.6484375, 0.046875, 0.857421875, 0.9140625, 0.779296875, 0.087890625, 0.076171875, 0.671875, 0.845703125, 0.029296875, 0.01171875, 0.869140625, 0.02734375, 0.3046875, 0.068359375, 0.0, 0.064453125, 0.080078125, 0.927734375, 0.017578125, 0.060546875, 0.99609375, 0.033203125, 0.66796875, 0.013671875, 0.845703125, 0.01953125, 0.16796875, 0.66796875, 0.8359375, 0.623046875, 0.93359375, 0.072265625, 0.982421875, 0.99609375, 0.01953125, 0.9375, 0.048828125, 0.078125, 0.91015625, 0.666015625, 0.982421875, 0.060546875, 0.671875, 0.7890625, 0.064453125, 0.677734375, 0.982421875, 0.08203125, 0.904296875, 0.02734375, 0.068359375, 0.03515625, 0.65234375, 0.982421875, 0.169921875, 0.076171875, 0.662109375, 0.630859375, 0.08203125, 0.02734375, 0.845703125, 0.8828125, 0.919921875, 0.99609375, 0.669921875, 0.08984375, 0.166015625, 0.8125, 0.68359375, 0.0390625, 0.029296875, 0.01953125, 0.982421875, 0.02734375, 0.025390625, 0.685546875, 0.025390625, 0.037109375, 0.63671875, 0.974609375, 0.12890625, 0.876953125, 0.05859375, 0.90625, 0.013671875, 0.853515625, 0.822265625, 0.61328125, 0.04296875, 0.693359375, 0.876953125, 0.982421875, 0.998046875, 0.025390625, 0.640625, 0.671875, 0.681640625, 0.806640625, 0.015625, 0.84375, 0.716796875, 0.849609375, 0.693359375, 0.951171875, 0.982421875, 0.8359375, 0.76953125, 0.640625, 0.681640625, 0.798828125, 0.640625, 0.087890625, 0.87109375, 0.650390625, 0.8828125, 0.8125, 0.638671875, 0.666015625, 0.037109375, 0.080078125, 0.96484375, 0.654296875, 0.03515625, 0.662109375, 0.96875, 0.08984375, 0.087890625, 0.029296875, 0.666015625, 0.875, 0.982421875, 0.19140625, 0.072265625, 0.99609375, 0.71875, 0.982421875, 0.064453125, 0.044921875, 0.865234375, 0.021484375, 0.982421875, 0.044921875, 0.7109375, 0.01171875, 0.69140625, 0.146484375, 0.982421875, 0.724609375, 0.626953125, 0.01953125, 0.92578125, 0.044921875, 0.025390625, 0.642578125, 0.025390625, 0.837890625, 0.9453125, 0.029296875, 0.89453125, 0.869140625, 0.998046875, 0.625, 0.015625, 0.03125, 0.080078125, 0.771484375, 0.03125, 0.693359375, 0.02734375, 0.04296875, 0.025390625, 0.685546875, 0.029296875, 0.021484375, 0.052734375, 0.017578125, 0.0234375, 0.87109375, 0.013671875, 0.064453125, 0.92578125, 0.931640625, 0.330078125, 0.982421875, 0.982421875, 0.638671875, 0.7890625, 0.98046875, 0.0390625, 0.70703125, 0.8828125, 0.6796875, 0.037109375, 0.759765625, 0.939453125, 0.62890625, 0.0234375, 0.001953125, 0.0859375, 0.994140625, 0.7265625, 0.62109375, 0.064453125, 0.6953125, 0.189453125, 0.044921875, 0.033203125, 0.005859375, 0.05078125, 0.046875, 0.7578125, 0.001953125, 0.017578125, 0.044921875]

 sparsity of   [0.0146484375, 0.0771484375, 0.73046875, 0.7958984375, 0.0986328125, 0.02734375, 0.0341796875, 0.0234375, 0.0361328125, 0.0, 0.08203125, 0.2109375, 0.623046875, 0.1640625, 0.02734375, 0.001953125, 0.001953125, 0.1884765625, 0.0634765625, 0.0, 0.0283203125, 0.0068359375, 0.015625, 0.09375, 0.25390625, 0.998046875, 0.00390625, 0.0107421875, 0.0185546875, 0.029296875, 0.138671875, 0.0087890625, 0.0263671875, 0.0244140625, 0.00390625, 0.908203125, 0.029296875, 0.9970703125, 0.041015625, 0.0390625, 0.1357421875, 0.12109375, 0.03515625, 0.1103515625, 0.0, 0.00390625, 0.8857421875, 0.5361328125, 0.0703125, 0.0283203125, 0.044921875, 0.0908203125, 0.224609375, 0.1748046875, 0.087890625, 0.921875, 0.0302734375, 0.0439453125, 0.0458984375, 0.037109375, 0.873046875, 0.10546875, 0.01171875, 0.0283203125, 0.126953125, 0.068359375, 0.056640625, 0.0048828125, 0.0419921875, 0.3994140625, 0.2236328125, 0.0087890625, 0.9521484375, 0.033203125, 0.00390625, 0.0146484375, 0.1796875, 0.05078125, 0.083984375, 0.029296875, 0.0, 0.005859375, 0.0380859375, 0.0654296875, 0.3037109375, 0.0244140625, 0.1728515625, 0.0830078125, 0.798828125, 0.1376953125, 0.0244140625, 0.1005859375, 0.029296875, 0.716796875, 0.3193359375, 0.05859375, 0.021484375, 0.0791015625, 0.8212890625, 0.0244140625, 0.0478515625, 0.0146484375, 0.005859375, 0.0224609375, 0.9970703125, 0.0634765625, 0.1376953125, 0.041015625, 0.0576171875, 0.0068359375, 0.18359375, 0.02734375, 0.0615234375, 0.0087890625, 0.10546875, 0.056640625, 0.0419921875, 0.0, 0.009765625, 0.8994140625, 0.00390625, 0.0830078125, 0.021484375, 0.0224609375, 0.1064453125, 0.017578125, 0.009765625, 0.0517578125, 0.8515625, 0.0029296875, 0.0048828125, 0.00390625, 0.037109375, 0.0068359375, 0.0244140625, 0.0517578125, 0.1123046875, 0.013671875, 0.0361328125, 0.4755859375, 0.1142578125, 0.021484375, 0.0693359375, 0.01171875, 0.072265625, 0.025390625, 0.0888671875, 0.0068359375, 0.0556640625, 0.013671875, 0.0078125, 0.0029296875, 0.1494140625, 0.0849609375, 0.0078125, 0.9970703125, 0.869140625, 0.0009765625, 0.1083984375, 0.4619140625, 0.001953125, 0.0546875, 0.041015625, 0.9970703125, 0.09765625, 0.1787109375, 0.033203125, 0.998046875, 0.1103515625, 0.8369140625, 0.1513671875, 0.009765625, 0.0224609375, 0.01171875, 0.23828125, 0.0, 0.037109375, 0.0810546875, 0.1796875, 0.9990234375, 0.0615234375, 0.1005859375, 0.0146484375, 0.998046875, 0.052734375, 0.9970703125, 0.017578125, 0.0, 0.1025390625, 0.86328125, 0.0322265625, 0.05078125, 0.0859375, 0.02734375, 0.09375, 0.0185546875, 0.068359375, 0.0400390625, 0.013671875, 0.091796875, 0.09765625, 0.0283203125, 0.173828125, 0.0126953125, 0.0009765625, 0.998046875, 0.017578125, 0.3017578125, 0.43359375, 0.07421875, 0.220703125, 0.9970703125, 0.0126953125, 0.029296875, 0.9619140625, 0.02734375, 0.021484375, 0.0107421875, 0.0029296875, 0.0439453125, 0.2138671875, 0.0048828125, 0.623046875, 0.029296875, 0.01171875, 0.1953125, 0.9970703125, 0.9990234375, 0.37890625, 0.0517578125, 0.861328125, 0.0029296875, 0.048828125, 0.0361328125, 0.9970703125, 0.0400390625, 0.0087890625, 0.2919921875, 0.6494140625, 0.0869140625, 0.0, 0.0673828125, 0.0302734375, 0.02734375, 0.00390625, 0.07421875, 0.0205078125, 0.1689453125, 0.138671875, 0.9306640625, 0.09765625, 0.208984375, 0.2705078125, 0.021484375, 0.0244140625, 0.10546875, 0.896484375, 0.0029296875, 0.1103515625, 0.0732421875, 0.0009765625, 0.912109375, 0.0751953125, 0.0, 0.0322265625, 0.0048828125, 0.013671875, 0.9267578125, 0.01953125, 0.0390625, 0.080078125, 0.02734375, 0.08984375, 0.13671875, 0.005859375, 0.0556640625, 0.068359375, 0.775390625, 0.0048828125, 0.0224609375, 0.814453125, 0.0966796875, 0.009765625, 0.009765625, 0.095703125, 0.0166015625, 0.0078125, 0.0068359375, 0.322265625, 0.943359375, 0.044921875, 0.0029296875, 0.0078125, 0.0087890625, 0.10546875, 0.0498046875, 0.02734375, 0.1083984375, 0.1240234375, 0.02734375, 0.12109375, 0.0224609375, 0.0595703125, 0.013671875, 0.998046875, 0.0048828125, 0.888671875, 0.0166015625, 0.1220703125, 0.041015625, 0.0166015625, 0.037109375, 0.00390625, 0.041015625, 0.23046875, 0.0234375, 0.44140625, 0.0244140625, 0.0751953125, 0.9853515625, 0.9970703125, 0.009765625, 0.0263671875, 0.0439453125, 0.080078125, 0.0087890625, 0.0, 0.029296875, 0.1298828125, 0.0244140625, 0.0234375, 0.0771484375, 0.0400390625, 0.998046875, 0.0341796875, 0.044921875, 0.005859375, 0.935546875, 0.14453125, 0.013671875, 0.0048828125, 0.59765625, 0.0, 0.0087890625, 0.0, 0.0234375, 0.017578125, 0.9521484375, 0.958984375, 0.0146484375, 0.0283203125, 0.1142578125, 0.998046875, 0.0, 0.0048828125, 0.0, 0.08984375, 0.0654296875, 0.009765625, 0.013671875, 0.0078125, 0.01953125, 0.005859375, 0.0830078125, 0.0263671875, 0.06640625, 0.0126953125, 0.021484375, 0.0166015625, 0.0185546875, 0.84375, 0.0068359375, 0.0283203125, 0.1591796875, 0.005859375, 0.00390625, 0.1162109375, 0.0244140625, 0.998046875, 0.9482421875, 0.1884765625, 0.0400390625, 0.017578125, 0.0146484375, 0.0029296875, 0.001953125, 0.0166015625, 0.2236328125, 0.212890625, 0.015625, 0.1796875, 0.0908203125, 0.12109375, 0.1875, 0.2900390625, 0.0, 0.0322265625, 0.0126953125, 0.091796875, 0.998046875, 0.4912109375, 0.0888671875, 0.865234375, 0.140625, 0.1689453125, 0.0263671875, 0.025390625, 0.0283203125, 0.0244140625, 0.0087890625, 0.029296875, 0.3994140625, 0.03515625, 0.013671875, 0.05859375, 0.005859375, 0.0771484375, 0.12109375, 0.0126953125, 0.056640625, 0.2333984375, 0.1181640625, 0.0087890625, 0.0419921875, 0.0107421875, 0.025390625, 0.021484375, 0.015625, 0.0166015625, 0.0146484375, 0.9990234375, 0.4267578125, 0.859375, 0.015625, 0.0166015625, 0.0078125, 0.150390625, 0.091796875, 0.0185546875, 0.041015625, 0.1328125, 0.0361328125, 0.044921875, 0.9697265625, 0.0078125, 0.0380859375, 0.078125, 0.943359375, 0.939453125, 0.00390625, 0.736328125, 0.0205078125, 0.1494140625, 0.1064453125, 0.587890625, 0.021484375, 0.09765625, 0.0283203125, 0.0078125, 0.02734375, 0.2548828125, 0.95703125, 0.103515625, 0.0859375, 0.0234375, 0.998046875, 0.0302734375, 0.1787109375, 0.048828125, 0.814453125, 0.0068359375, 0.013671875, 0.0380859375, 0.947265625, 0.9970703125, 0.00390625, 0.0234375, 0.001953125, 0.0166015625, 0.126953125, 0.0126953125, 0.048828125, 0.0322265625, 0.01171875, 0.0732421875, 0.03515625, 0.0107421875, 0.1025390625, 0.0400390625, 0.0537109375, 0.5380859375, 0.0244140625, 0.1787109375, 0.90234375, 0.7275390625, 0.005859375, 0.1279296875, 0.1904296875, 0.291015625, 0.0146484375, 0.009765625, 0.1357421875, 0.01953125, 0.0087890625, 0.0126953125, 0.05078125, 0.025390625, 0.025390625, 0.0771484375, 0.9970703125, 0.0927734375, 0.1982421875, 0.0673828125, 0.0234375, 0.029296875, 0.0, 0.998046875, 0.0, 0.091796875, 0.1240234375, 0.0048828125, 0.0537109375, 0.0126953125, 0.0, 0.0205078125, 0.013671875, 0.01171875, 0.939453125, 0.0, 0.234375, 0.9267578125, 0.01171875, 0.0146484375, 0.318359375, 0.109375, 0.0029296875, 0.0166015625, 0.0732421875, 0.080078125, 0.9990234375, 0.021484375, 0.11328125, 0.005859375, 0.046875, 0.9990234375, 0.1015625, 0.9091796875, 0.96484375, 0.009765625, 0.9970703125, 0.04296875, 0.1123046875, 0.1005859375, 0.0078125, 0.0341796875, 0.0830078125, 0.1181640625, 0.9970703125, 0.11328125, 0.017578125, 0.974609375, 0.0234375, 0.05859375, 0.0029296875, 0.119140625, 0.0576171875, 0.927734375, 0.0048828125, 0.02734375, 0.021484375, 0.04296875, 0.029296875, 0.611328125, 0.17578125, 0.0107421875, 0.998046875, 0.021484375, 0.0, 0.06640625, 0.017578125, 0.0771484375, 0.0419921875, 0.0068359375, 0.0341796875, 0.0068359375, 0.87890625, 0.0224609375, 0.017578125, 0.0, 0.033203125, 0.0556640625, 0.0654296875, 0.0126953125, 0.0869140625, 0.0087890625, 0.998046875, 0.53125, 0.0537109375, 0.1240234375, 0.3251953125, 0.0, 0.0400390625, 0.0712890625, 0.1015625, 0.1123046875, 0.9267578125, 0.9970703125, 0.302734375, 0.025390625, 0.0380859375, 0.9970703125, 0.091796875, 0.0107421875, 0.1298828125, 0.2392578125, 0.8125, 0.099609375, 0.0322265625, 0.009765625, 0.005859375, 0.015625, 0.015625, 0.0048828125, 0.1142578125, 0.203125, 0.0185546875, 0.0322265625, 0.0, 0.0234375, 0.0029296875, 0.576171875, 0.1357421875, 0.099609375, 0.0224609375, 0.1025390625, 0.998046875, 0.998046875, 0.0263671875, 0.8759765625, 0.9169921875, 0.072265625, 0.0625, 0.0068359375, 0.0986328125, 0.01171875, 0.9072265625, 0.0166015625, 0.0068359375, 0.0, 0.025390625, 0.947265625, 0.0263671875, 0.08984375, 0.0400390625, 0.0771484375, 0.05078125, 0.15234375, 0.0595703125, 0.1044921875, 0.1123046875, 0.5751953125, 0.9970703125, 0.060546875, 0.9189453125, 0.05078125, 0.826171875, 0.0166015625, 0.13671875, 0.1025390625, 0.001953125, 0.1240234375, 0.0380859375, 0.0283203125, 0.025390625, 0.8828125, 0.0966796875, 0.1728515625, 0.0439453125, 0.0068359375, 0.7490234375, 0.0078125, 0.8291015625, 0.037109375, 0.0341796875, 0.0048828125, 0.0380859375, 0.0419921875, 0.0380859375, 0.0078125, 0.103515625, 0.0, 0.0, 0.9248046875, 0.0849609375, 0.09765625, 0.3720703125, 0.005859375, 0.009765625, 0.1845703125, 0.0107421875, 0.283203125, 0.0390625, 0.07421875, 0.0380859375, 0.005859375, 0.029296875, 0.0048828125, 0.1142578125, 0.0, 0.0224609375, 0.0107421875, 0.0029296875, 0.048828125, 0.041015625, 0.337890625, 0.9970703125, 0.0166015625, 0.0048828125, 0.05078125, 0.08203125, 0.1123046875, 0.06640625, 0.021484375, 0.0078125, 0.0, 0.0224609375, 0.0400390625, 0.04296875, 0.0888671875, 0.01953125, 0.0283203125, 0.21875, 0.048828125, 0.0361328125, 0.052734375, 0.0615234375, 0.0048828125, 0.1357421875, 0.9189453125, 0.029296875, 0.0048828125, 0.1171875, 0.0322265625, 0.1259765625, 0.02734375, 0.03125, 0.00390625, 0.0, 0.041015625, 0.9970703125, 0.0302734375, 0.314453125, 0.0146484375, 0.998046875, 0.0146484375, 0.185546875, 0.0439453125, 0.0048828125, 0.173828125, 0.10546875, 0.0302734375, 0.0166015625, 0.9541015625, 0.880859375, 0.1005859375, 0.888671875, 0.04296875, 0.0009765625, 0.05859375, 0.9970703125, 0.1748046875, 0.0, 0.0087890625, 0.029296875, 0.09375, 0.0185546875, 0.0, 0.033203125, 0.998046875, 0.75390625, 0.0234375, 0.1455078125, 0.083984375, 0.009765625, 0.0283203125, 0.1298828125, 0.056640625, 0.0009765625, 0.021484375, 0.9970703125, 0.021484375, 0.0068359375, 0.017578125, 0.138671875, 0.0029296875, 0.73046875, 0.1025390625, 0.0185546875, 0.0263671875, 0.791015625, 0.009765625, 0.9990234375, 0.0224609375, 0.3935546875, 0.1494140625, 0.0478515625, 0.01953125, 0.009765625, 0.9990234375, 0.1650390625, 0.9970703125, 0.0146484375, 0.3994140625, 0.0537109375, 0.099609375, 0.021484375, 0.1220703125, 0.9345703125, 0.083984375, 0.4052734375, 0.037109375, 0.103515625, 0.0517578125, 0.0146484375, 0.859375, 0.015625, 0.013671875, 0.0048828125, 0.005859375, 0.0791015625, 0.0068359375, 0.998046875, 0.1171875, 0.65625, 0.052734375, 0.02734375, 0.009765625, 0.009765625, 0.998046875, 0.0498046875, 0.013671875, 0.1357421875, 0.0078125, 0.0146484375, 0.1279296875, 0.0703125, 0.0439453125, 0.03515625, 0.0009765625, 0.0166015625, 0.1025390625, 0.01171875, 0.0498046875, 0.0224609375, 0.169921875, 0.5107421875, 0.025390625, 0.021484375, 0.0341796875, 0.96875, 0.9599609375, 0.0244140625, 0.6611328125, 0.02734375, 0.0244140625, 0.01171875, 0.025390625, 0.3583984375, 0.384765625, 0.0185546875, 0.0078125, 0.90234375, 0.7509765625, 0.107421875, 0.0380859375, 0.998046875, 0.140625, 0.91015625, 0.0068359375, 0.07421875, 0.0048828125, 0.0, 0.0400390625, 0.1533203125, 0.1357421875, 0.021484375, 0.0078125, 0.0, 0.01171875, 0.056640625, 0.033203125, 0.0, 0.9970703125, 0.03125, 0.1337890625, 0.748046875, 0.134765625, 0.0107421875, 0.998046875, 0.0126953125, 0.0087890625, 0.0205078125, 0.08203125, 0.0087890625, 0.00390625, 0.0595703125, 0.0146484375, 0.0068359375, 0.0068359375, 0.0, 0.119140625, 0.044921875, 0.0390625, 0.056640625, 0.0068359375, 0.0556640625, 0.0107421875, 0.0234375, 0.005859375, 0.677734375, 0.0029296875, 0.1640625, 0.005859375, 0.158203125, 0.0234375, 0.083984375, 0.0439453125, 0.0908203125, 0.0166015625, 0.0126953125, 0.0283203125, 0.001953125, 0.9375, 0.4169921875, 0.01953125, 0.8935546875, 0.0302734375, 0.013671875, 0.4794921875, 0.0048828125, 0.533203125, 0.005859375, 0.0126953125, 0.0126953125, 0.1171875, 0.025390625, 0.0947265625, 0.0234375, 0.013671875, 0.02734375, 0.021484375, 0.087890625, 0.0078125, 0.005859375, 0.1220703125, 0.0439453125, 0.00390625, 0.12109375, 0.0810546875, 0.02734375, 0.00390625, 0.0439453125, 0.0068359375, 0.0869140625, 0.8662109375, 0.0, 0.1611328125, 0.0078125, 0.1025390625, 0.095703125, 0.00390625, 0.037109375, 0.0205078125, 0.0048828125, 0.052734375, 0.0068359375, 0.0, 0.125, 0.1005859375, 0.0087890625, 0.0107421875, 0.037109375, 0.0068359375, 0.0654296875, 0.087890625, 0.044921875, 0.0751953125, 0.173828125, 0.28515625, 0.037109375, 0.0283203125, 0.0478515625, 0.1279296875, 0.0390625, 0.01953125, 0.001953125, 0.0009765625, 0.015625, 0.05859375, 0.005859375, 0.0732421875, 0.8212890625, 0.71484375, 0.248046875, 0.1201171875, 0.0146484375, 0.0068359375, 0.0029296875, 0.0078125, 0.0029296875, 0.0615234375, 0.0302734375, 0.908203125, 0.009765625, 0.013671875, 0.060546875, 0.0537109375, 0.05859375, 0.0, 0.005859375, 0.8125, 0.0302734375, 0.8349609375, 0.01953125, 0.0927734375, 0.0, 0.921875, 0.0400390625, 0.0146484375, 0.83984375, 0.9248046875, 0.0068359375, 0.9609375, 0.11328125, 0.015625, 0.1259765625, 0.0, 0.0146484375, 0.001953125, 0.0751953125, 0.3203125, 0.009765625, 0.8818359375, 0.013671875, 0.017578125, 0.033203125, 0.0234375, 0.0390625, 0.025390625, 0.009765625, 0.0234375, 0.0107421875, 0.107421875, 0.017578125, 0.0595703125, 0.0078125, 0.884765625, 0.921875, 0.0087890625, 0.08203125, 0.10546875, 0.0703125, 0.0234375, 0.046875, 0.01953125, 0.09375, 0.0859375, 0.333984375, 0.0068359375, 0.0263671875, 0.017578125, 0.03515625, 0.033203125, 0.12890625, 0.01171875, 0.1123046875, 0.0, 0.0185546875, 0.3984375, 0.013671875, 0.923828125, 0.0322265625, 0.0146484375, 0.56640625, 0.134765625, 0.0849609375, 0.005859375, 0.625, 0.0029296875, 0.017578125, 0.8974609375, 0.0068359375, 0.9248046875, 0.01953125, 0.2490234375, 0.9970703125, 0.998046875, 0.0595703125, 0.0234375, 0.1005859375, 0.0302734375, 0.224609375, 0.0517578125, 0.041015625, 0.021484375, 0.103515625, 0.1064453125, 0.0, 0.9970703125, 0.0029296875, 0.1103515625, 0.0107421875, 0.1552734375, 0.0458984375, 0.1884765625, 0.0537109375, 0.03125, 0.791015625, 0.015625, 0.056640625, 0.3408203125, 0.0029296875, 0.01171875, 0.9033203125, 0.0029296875, 0.0205078125, 0.1923828125, 0.087890625, 0.0322265625, 0.013671875, 0.0185546875, 0.1181640625, 0.083984375, 0.884765625, 0.248046875, 0.06640625, 0.65625, 0.080078125, 0.3046875, 0.0068359375, 0.1103515625, 0.125, 0.048828125, 0.998046875, 0.2275390625, 0.931640625, 0.029296875, 0.0478515625, 0.0400390625, 0.0439453125, 0.0, 0.0048828125, 0.0234375, 0.0224609375, 0.0068359375, 0.0986328125, 0.0615234375, 0.041015625, 0.0751953125, 0.009765625, 0.869140625, 0.0146484375, 0.0078125, 0.0068359375, 0.1220703125, 0.0087890625, 0.0078125, 0.0009765625, 0.01171875, 0.0, 0.0693359375, 0.0244140625, 0.0107421875, 0.0361328125, 0.9365234375, 0.0224609375, 0.9970703125, 0.2099609375, 0.0322265625, 0.3134765625, 0.060546875, 0.0029296875, 0.001953125, 0.0126953125, 0.03515625, 0.998046875, 0.13671875, 0.029296875, 0.0126953125, 0.009765625, 0.1357421875, 0.0654296875, 0.0556640625, 0.0693359375, 0.0849609375, 0.0107421875, 0.111328125, 0.78125, 0.00390625, 0.005859375, 0.0205078125, 0.2685546875, 0.009765625, 0.90234375, 0.1259765625, 0.046875, 0.0947265625, 0.0322265625, 0.013671875, 0.2607421875, 0.0078125, 0.0068359375, 0.1005859375, 0.0078125, 0.9970703125, 0.07421875, 0.5576171875, 0.0048828125, 0.8642578125, 0.02734375, 0.04296875, 0.091796875, 0.0048828125, 0.0126953125, 0.0029296875, 0.01953125, 0.9228515625, 0.013671875, 0.1181640625, 0.869140625, 0.37890625, 0.1123046875, 0.115234375, 0.0283203125, 0.009765625, 0.1796875, 0.0791015625, 0.0478515625, 0.0810546875, 0.0078125, 0.0341796875, 0.0068359375, 0.0029296875, 0.0341796875, 0.041015625, 0.09375, 0.0, 0.033203125, 0.0048828125, 0.193359375, 0.0, 0.025390625, 0.1103515625, 0.0810546875, 0.8447265625, 0.0048828125, 0.123046875, 0.0107421875, 0.1044921875, 0.0166015625, 0.009765625, 0.03125, 0.0224609375, 0.001953125, 0.0849609375, 0.07421875, 0.9970703125, 0.0224609375, 0.998046875, 0.013671875, 0.00390625, 0.7333984375, 0.1376953125, 0.0166015625, 0.02734375, 0.0, 0.0322265625, 0.1259765625, 0.09765625, 0.01171875, 0.00390625, 0.0087890625, 0.052734375, 0.3173828125, 0.0341796875, 0.048828125, 0.0625, 0.0244140625, 0.0244140625, 0.0048828125, 0.013671875, 0.025390625, 0.0361328125, 0.0126953125, 0.0419921875, 0.0107421875, 0.009765625, 0.0126953125, 0.580078125, 0.0146484375, 0.0078125, 0.1259765625, 0.013671875, 0.0048828125, 0.0546875, 0.0283203125, 0.0, 0.220703125, 0.1357421875, 0.1123046875, 0.029296875, 0.998046875, 0.1083984375, 0.029296875, 0.0439453125, 0.025390625, 0.1005859375, 0.0244140625, 0.0546875, 0.248046875, 0.0, 0.0009765625, 0.572265625, 0.0107421875, 0.0029296875, 0.05859375, 0.0986328125, 0.1123046875, 0.0, 0.0068359375, 0.0205078125, 0.017578125, 0.03515625, 0.998046875, 0.146484375, 0.00390625, 0.03515625, 0.6171875, 0.0546875, 0.9970703125, 0.021484375, 0.0, 0.091796875, 0.005859375, 0.9970703125, 0.0, 0.9306640625, 0.9970703125, 0.888671875, 0.0009765625, 0.017578125, 0.01953125, 0.068359375, 0.025390625, 0.0703125, 0.0341796875, 0.0478515625, 0.06640625, 0.0009765625, 0.0107421875, 0.9658203125, 0.0634765625, 0.017578125, 0.109375, 0.0498046875, 0.9365234375, 0.2998046875, 0.3232421875, 0.001953125, 0.091796875, 0.0322265625, 0.0068359375, 0.052734375, 0.046875, 0.0126953125, 0.08984375, 0.0, 0.0234375, 0.095703125, 0.0810546875, 0.0078125, 0.009765625, 0.0263671875, 0.2197265625, 0.224609375, 0.0126953125, 0.6025390625, 0.83203125, 0.0, 0.0556640625, 0.025390625, 0.2744140625, 0.0908203125, 0.052734375, 0.9970703125, 0.0146484375, 0.0634765625, 0.0078125, 0.01171875, 0.0224609375, 0.15234375, 0.0029296875, 0.0107421875, 0.015625, 0.0009765625, 0.1123046875, 0.8603515625, 0.001953125, 0.1357421875, 0.9267578125, 0.0322265625, 0.13671875, 0.005859375, 0.0302734375, 0.0048828125, 0.8505859375, 0.0126953125, 0.0673828125, 0.013671875, 0.81640625, 0.1318359375, 0.44140625, 0.0078125, 0.16015625, 0.0029296875, 0.0615234375, 0.041015625, 0.0498046875, 0.9970703125, 0.013671875, 0.1943359375, 0.0126953125, 0.0078125, 0.0927734375, 0.998046875, 0.005859375, 0.0029296875, 0.005859375, 0.0234375, 0.0185546875, 0.0908203125, 0.0888671875, 0.0791015625, 0.025390625, 0.09375, 0.0791015625, 0.0966796875, 0.07421875, 0.947265625, 0.81640625, 0.1171875, 0.0146484375, 0.248046875, 0.0205078125, 0.4052734375, 0.0498046875, 0.0498046875, 0.033203125, 0.0146484375, 0.0, 0.0166015625, 0.0, 0.7763671875, 0.5927734375, 0.0029296875, 0.103515625, 0.9970703125, 0.017578125, 0.0185546875, 0.0, 0.0166015625, 0.107421875, 0.087890625, 0.03125, 0.0078125, 0.0185546875, 0.0146484375, 0.0224609375, 0.8984375, 0.009765625, 0.05078125, 0.0458984375, 0.03515625, 0.9970703125, 0.0048828125, 0.05078125, 0.0400390625, 0.29296875, 0.0205078125, 0.0078125, 0.009765625, 0.046875, 0.0224609375, 0.126953125, 0.015625, 0.287109375, 0.0703125, 0.0166015625, 0.0146484375, 0.06640625, 0.0078125, 0.009765625, 0.0185546875, 0.08984375, 0.998046875, 0.0712890625, 0.9453125, 0.05078125, 0.998046875, 0.0048828125, 0.0205078125, 0.9072265625, 0.0986328125, 0.6533203125, 0.04296875, 0.005859375, 0.0068359375, 0.0029296875, 0.0712890625, 0.0634765625, 0.0185546875, 0.041015625, 0.0361328125, 0.9345703125, 0.1142578125, 0.064453125, 0.0205078125, 0.076171875, 0.0458984375, 0.125, 0.0263671875, 0.0, 0.109375, 0.0068359375, 0.9482421875, 0.271484375, 0.025390625, 0.146484375, 0.1123046875, 0.015625, 0.0390625, 0.0078125, 0.0107421875, 0.025390625, 0.025390625, 0.0849609375, 0.0078125, 0.072265625, 0.0849609375, 0.0390625, 0.0439453125, 0.025390625, 0.9970703125, 0.056640625, 0.03515625, 0.0224609375, 0.02734375, 0.998046875, 0.998046875, 0.162109375, 0.009765625, 0.166015625, 0.0205078125, 0.009765625, 0.0478515625, 0.0244140625, 0.0029296875, 0.0048828125, 0.021484375, 0.0615234375, 0.07421875, 0.0234375, 0.0595703125, 0.9970703125, 0.09765625, 0.03515625, 0.056640625, 0.1484375, 0.033203125, 0.078125, 0.01171875, 0.7099609375, 0.01953125, 0.021484375, 0.814453125, 0.01953125, 0.9970703125, 0.19921875, 0.66796875, 0.015625, 0.029296875, 0.0576171875, 0.005859375, 0.013671875, 0.001953125, 0.0048828125, 0.0029296875, 0.0341796875, 0.0205078125, 0.0029296875, 0.0107421875, 0.248046875, 0.001953125, 0.8642578125, 0.0810546875, 0.009765625, 0.11328125, 0.015625, 0.9072265625, 0.857421875, 0.0, 0.0673828125, 0.02734375, 0.1865234375, 0.998046875, 0.0068359375, 0.1748046875, 0.0849609375, 0.03125, 0.001953125, 0.8984375, 0.109375, 0.0, 0.0068359375, 0.0126953125, 0.009765625, 0.1357421875, 0.998046875, 0.111328125, 0.0029296875, 0.9970703125, 0.0927734375, 0.837890625, 0.03515625, 0.0966796875, 0.0029296875, 0.0361328125, 0.20703125, 0.2236328125, 0.0302734375, 0.05078125, 0.08984375, 0.9970703125, 0.037109375, 0.9970703125, 0.125, 0.0, 0.009765625, 0.0126953125, 0.2783203125, 0.8369140625, 0.025390625, 0.0234375, 0.025390625, 0.095703125, 0.0380859375, 0.099609375, 0.2353515625, 0.060546875, 0.015625, 0.017578125, 0.998046875, 0.185546875, 0.021484375, 0.0322265625, 0.005859375, 0.0, 0.1103515625, 0.0, 0.044921875, 0.140625, 0.9990234375, 0.052734375, 0.9443359375, 0.130859375, 0.4873046875, 0.0185546875, 0.1083984375, 0.833984375, 0.0263671875, 0.0322265625, 0.0986328125, 0.0224609375, 0.021484375, 0.0556640625, 0.1220703125, 0.142578125, 0.998046875, 0.0185546875, 0.140625, 0.0244140625, 0.0517578125, 0.0361328125, 0.0390625, 0.099609375, 0.0703125, 0.015625, 0.0947265625, 0.88671875, 0.029296875, 0.068359375, 0.2587890625, 0.0, 0.9970703125, 0.1123046875, 0.087890625, 0.966796875, 0.1591796875, 0.0634765625, 0.998046875, 0.033203125, 0.0498046875, 0.115234375, 0.095703125, 0.0, 0.1240234375, 0.3134765625, 0.810546875, 0.0537109375, 0.0166015625, 0.1962890625, 0.1298828125, 0.0380859375, 0.0380859375, 0.0068359375, 0.00390625, 0.0341796875, 0.90234375, 0.3251953125, 0.08203125, 0.0712890625, 0.01171875, 0.0087890625, 0.1787109375, 0.041015625, 0.931640625, 0.0068359375, 0.0302734375, 0.005859375, 0.0048828125, 0.0390625, 0.029296875, 0.041015625, 0.81640625, 0.0771484375, 0.068359375, 0.015625, 0.03515625, 0.0732421875, 0.0888671875, 0.0, 0.0244140625, 0.0078125, 0.646484375, 0.140625, 0.0087890625, 0.63671875, 0.0087890625, 0.0380859375, 0.0, 0.9990234375, 0.0888671875, 0.0244140625, 0.0078125, 0.0205078125, 0.021484375, 0.0048828125, 0.7197265625, 0.0068359375, 0.048828125, 0.0078125, 0.01171875, 0.1611328125, 0.0439453125, 0.2060546875, 0.107421875, 0.0849609375, 0.0947265625, 0.1123046875, 0.00390625, 0.0048828125, 0.0546875, 0.0283203125, 0.015625, 0.0068359375, 0.8271484375, 0.001953125, 0.22265625, 0.0166015625, 0.001953125, 0.0419921875, 0.0185546875, 0.0615234375, 0.16796875, 0.005859375, 0.9970703125, 0.033203125, 0.009765625, 0.017578125, 0.0166015625, 0.0927734375, 0.1708984375, 0.0224609375, 0.0234375, 0.1123046875, 0.091796875, 0.9990234375, 0.845703125, 0.0634765625, 0.0068359375, 0.0078125, 0.1083984375, 0.099609375, 0.8720703125, 0.02734375, 0.0166015625, 0.86328125, 0.0859375, 0.005859375, 0.1318359375, 0.638671875, 0.767578125, 0.2412109375, 0.0, 0.046875, 0.9150390625, 0.03125, 0.076171875, 0.9970703125, 0.005859375, 0.017578125, 0.0576171875, 0.1025390625, 0.0908203125, 0.0, 0.01953125, 0.1259765625, 0.302734375, 0.052734375, 0.0263671875, 0.099609375, 0.0, 0.095703125, 0.9970703125, 0.0048828125, 0.9970703125, 0.9970703125, 0.77734375, 0.0068359375, 0.10546875, 0.0224609375, 0.103515625, 0.0458984375, 0.015625, 0.0078125, 0.126953125, 0.013671875, 0.14453125, 0.001953125, 0.0205078125, 0.0185546875, 0.0107421875, 0.0166015625, 0.0205078125, 0.0517578125, 0.0205078125, 0.0, 0.0234375, 0.390625, 0.529296875, 0.0537109375, 0.828125, 0.9990234375, 0.0048828125, 0.0361328125, 0.1796875, 0.048828125, 0.0146484375, 0.0546875, 0.01171875, 0.0009765625, 0.1298828125, 0.0478515625, 0.06640625, 0.0341796875, 0.005859375, 0.1162109375, 0.001953125, 0.0439453125, 0.037109375, 0.126953125, 0.015625, 0.0380859375, 0.1328125, 0.115234375, 0.013671875, 0.0400390625, 0.7861328125, 0.0078125, 0.0859375, 0.0361328125, 0.0048828125, 0.1005859375, 0.998046875, 0.0185546875, 0.0029296875, 0.0, 0.025390625, 0.9970703125, 0.1142578125, 0.0302734375, 0.0146484375, 0.044921875, 0.0869140625, 0.0615234375, 0.48828125, 0.0205078125, 0.05078125, 0.021484375, 0.1005859375, 0.099609375, 0.1376953125, 0.060546875, 0.0341796875, 0.046875, 0.0, 0.05859375, 0.07421875, 0.0478515625, 0.0048828125, 0.0859375, 0.0146484375, 0.998046875, 0.033203125, 0.0048828125, 0.1015625, 0.11328125, 0.0205078125, 0.0, 0.033203125, 0.138671875, 0.005859375, 0.0205078125, 0.1142578125, 0.0322265625, 0.0205078125, 0.091796875, 0.0126953125, 0.0556640625, 0.00390625, 0.04296875, 0.1181640625, 0.150390625, 0.0029296875, 0.033203125, 0.087890625, 0.0, 0.0078125, 0.0029296875, 0.0498046875, 0.0244140625, 0.8486328125, 0.087890625, 0.0322265625, 0.001953125, 0.0009765625, 0.005859375, 0.1845703125, 0.0947265625, 0.0185546875, 0.0283203125, 0.0087890625, 0.0400390625, 0.173828125, 0.0, 0.0478515625, 0.787109375, 0.123046875, 0.0205078125, 0.90625, 0.005859375, 0.029296875, 0.0078125, 0.0361328125, 0.93359375, 0.9970703125, 0.025390625, 0.076171875, 0.0029296875, 0.005859375, 0.4697265625, 0.0322265625, 0.228515625, 0.1943359375, 0.0341796875, 0.0302734375, 0.0146484375, 0.0048828125, 0.0126953125, 0.078125, 0.0166015625, 0.0771484375, 0.05078125, 0.033203125, 0.220703125, 0.1220703125, 0.322265625, 0.9970703125, 0.9990234375, 0.0888671875, 0.01171875, 0.005859375, 0.1640625, 0.0517578125, 0.0029296875, 0.0126953125, 0.6416015625, 0.1279296875, 0.0009765625, 0.015625, 0.013671875, 0.0068359375, 0.029296875, 0.0029296875, 0.0166015625, 0.0087890625, 0.0087890625, 0.2646484375, 0.0322265625, 0.0126953125, 0.0888671875, 0.013671875, 0.0234375, 0.9990234375, 0.0048828125, 0.0322265625, 0.0166015625, 0.0029296875, 0.7490234375, 0.9990234375, 0.015625, 0.0361328125, 0.7314453125, 0.0615234375, 0.0126953125, 0.037109375, 0.0888671875, 0.0]

 sparsity of   [0.03466796875, 0.0234375, 0.0205078125, 0.52685546875, 0.91748046875, 0.0654296875, 0.03857421875, 0.0146484375, 0.623046875, 0.6669921875, 0.0380859375, 0.99951171875, 0.09375, 0.0576171875, 0.99853515625, 0.0263671875, 0.07763671875, 0.0263671875, 0.0947265625, 0.2529296875, 0.525390625, 0.099609375, 0.53173828125, 0.78125, 0.03466796875, 0.05712890625, 0.123046875, 0.98876953125, 0.02197265625, 0.0380859375, 0.5498046875, 0.00537109375, 0.029296875, 0.52294921875, 0.8125, 0.07373046875, 0.0205078125, 0.05859375, 0.80078125, 0.42333984375, 0.06689453125, 0.09375, 0.02392578125, 0.52392578125, 0.017578125, 0.21728515625, 0.0244140625, 0.033203125, 0.10400390625, 0.08251953125, 0.0947265625, 0.03564453125, 0.0390625, 0.61572265625, 0.12451171875, 0.15625, 0.03076171875, 0.5263671875, 0.05322265625, 0.05859375, 0.06982421875, 0.01171875, 0.400390625, 0.02587890625, 0.04150390625, 0.08740234375, 0.06982421875, 0.56103515625, 0.4697265625, 0.04833984375, 0.46044921875, 0.02587890625, 0.04052734375, 0.61962890625, 0.05126953125, 0.08935546875, 0.41015625, 0.0, 0.01806640625, 0.48583984375, 0.54296875, 0.27099609375, 0.052734375, 0.42431640625, 0.29052734375, 0.01318359375, 0.09521484375, 0.99853515625, 0.85693359375, 0.08251953125, 0.0810546875, 0.4599609375, 0.0322265625, 0.1103515625, 0.85498046875, 0.88818359375, 0.07373046875, 0.13330078125, 0.06591796875, 0.9990234375, 0.0244140625, 0.20654296875, 0.2451171875, 0.6845703125, 0.03076171875, 0.52880859375, 0.42431640625, 0.27685546875, 0.56884765625, 0.01611328125, 0.521484375, 0.076171875, 0.06689453125, 0.11083984375, 0.01318359375, 0.9580078125, 0.04541015625, 0.09228515625, 0.05029296875, 0.0224609375, 0.02099609375, 0.01708984375, 0.56494140625, 0.06591796875, 0.02880859375, 0.99853515625, 0.06884765625, 0.06005859375, 0.54736328125, 0.0517578125, 0.0322265625, 0.02001953125, 0.06005859375, 0.03173828125, 0.1591796875, 0.02099609375, 0.51611328125, 0.767578125, 0.029296875, 0.0517578125, 0.69921875, 0.05908203125, 0.7060546875, 0.037109375, 0.0830078125, 0.99853515625, 0.14599609375, 0.7802734375, 0.11328125, 0.0361328125, 0.6318359375, 0.025390625, 0.02197265625, 0.2138671875, 0.8310546875, 0.01318359375, 0.634765625, 0.0986328125, 0.21875, 0.9990234375, 0.1103515625, 0.5703125, 0.109375, 0.61181640625, 0.0888671875, 0.9990234375, 0.12451171875, 0.1337890625, 0.51708984375, 0.5234375, 0.3759765625, 0.15771484375, 0.0703125, 0.05322265625, 0.89990234375, 0.0625, 0.55712890625, 0.83837890625, 0.64501953125, 0.06787109375, 0.06298828125, 0.1123046875, 0.0224609375, 0.03955078125, 0.0146484375, 0.171875, 0.0595703125, 0.04736328125, 0.04443359375, 0.11474609375, 0.05517578125, 0.02001953125, 0.02392578125, 0.50830078125, 0.4697265625, 0.0537109375, 0.03857421875, 0.01513671875, 0.02587890625, 0.01513671875, 0.0205078125, 0.046875, 0.5009765625, 0.05078125, 0.8916015625, 0.03466796875, 0.1669921875, 0.33251953125, 0.02587890625, 0.50732421875, 0.94287109375, 0.07666015625, 0.1328125, 0.02294921875, 0.02734375, 0.50537109375, 0.0615234375, 0.0126953125, 0.02392578125, 0.0615234375, 0.09033203125, 0.62548828125, 0.0, 0.517578125, 0.05419921875, 0.58447265625, 0.06396484375, 0.50537109375, 0.1708984375, 0.07861328125, 0.0146484375, 0.7978515625, 0.0595703125, 0.49853515625, 0.037109375, 0.99951171875, 0.77099609375, 0.11279296875, 0.0654296875, 0.06005859375, 0.5908203125, 0.005859375, 0.99951171875, 0.3330078125, 0.02880859375, 0.01416015625, 0.1025390625, 0.0, 0.51611328125, 0.04248046875, 0.0830078125, 0.02587890625, 0.111328125, 0.0087890625, 0.11962890625, 0.16015625, 0.05615234375, 0.01220703125, 0.6357421875, 0.0166015625, 0.23681640625, 0.7138671875, 0.0185546875, 0.1044921875, 0.01220703125, 0.03515625, 0.08544921875, 0.50439453125, 0.841796875, 0.08544921875, 0.30859375, 0.0576171875, 0.0546875, 0.0185546875, 0.9990234375, 0.4033203125, 0.048828125, 0.05517578125, 0.78076171875, 0.23828125, 0.025390625, 0.21630859375, 0.0185546875, 0.06298828125, 0.037109375, 0.18017578125, 0.01025390625, 0.08056640625, 0.02294921875, 0.01513671875, 0.02880859375, 0.88916015625, 0.27490234375, 0.646484375, 0.068359375, 0.05810546875, 0.03173828125, 0.87841796875, 0.04052734375, 0.1025390625, 0.115234375, 0.5517578125, 0.044921875, 0.0673828125, 0.111328125, 0.1015625, 0.01904296875, 0.072265625, 0.01318359375, 0.0029296875, 0.0478515625, 0.7890625, 0.1552734375, 0.83154296875, 0.0185546875, 0.701171875, 0.15234375, 0.02392578125, 0.0078125, 0.99853515625, 0.01611328125, 0.1396484375, 0.015625, 0.423828125, 0.19287109375, 0.0, 0.0234375, 0.046875, 0.0537109375, 0.09716796875, 0.0390625, 0.06298828125, 0.09326171875, 0.1611328125, 0.06005859375, 0.013671875, 0.4716796875, 0.06494140625, 0.0166015625, 0.52783203125, 0.16845703125, 0.59814453125, 0.51953125, 0.37158203125, 0.0068359375, 0.13623046875, 0.029296875, 0.07861328125, 0.0771484375, 0.06494140625, 0.068359375, 0.03173828125, 0.01953125, 0.43798828125, 0.08056640625, 0.09716796875, 0.02099609375, 0.55615234375, 0.5234375, 0.0625, 0.92578125, 0.072265625, 0.0244140625, 0.23291015625, 0.04345703125, 0.0654296875, 0.05126953125, 0.03759765625, 0.03125, 0.013671875, 0.02001953125, 0.025390625, 0.0234375, 0.1103515625, 0.9990234375, 0.50830078125, 0.0322265625, 0.076171875, 0.08740234375, 0.111328125, 0.5625, 0.037109375, 0.28564453125, 0.0419921875, 0.09716796875, 0.1728515625, 0.1513671875, 0.169921875, 0.16259765625, 0.53125, 0.06787109375, 0.05908203125, 0.02099609375, 0.0205078125, 0.53466796875, 0.04248046875, 0.443359375, 0.06982421875, 0.04541015625, 0.10888671875, 0.4873046875, 0.56494140625, 0.03173828125, 0.07958984375, 0.07275390625, 0.01513671875, 0.0625, 0.03662109375, 0.33642578125, 0.521484375, 0.03515625, 0.064453125, 0.1728515625, 0.1005859375, 0.04443359375, 0.1923828125, 0.0283203125, 0.99853515625, 0.99853515625, 0.02783203125, 0.9775390625, 0.05810546875, 0.08154296875, 0.5107421875, 0.041015625, 0.0556640625, 0.72412109375, 0.029296875, 0.03759765625, 0.13623046875, 0.0087890625, 0.10400390625, 0.0400390625, 0.53466796875, 0.0908203125, 0.11279296875, 0.0283203125, 0.0, 0.01171875, 0.55712890625, 0.01806640625, 0.02490234375, 0.0498046875, 0.9990234375, 0.04931640625, 0.03076171875, 0.05419921875, 0.677734375, 0.01171875, 0.0419921875, 0.1591796875, 0.16845703125, 0.025390625, 0.12890625, 0.7431640625, 0.03857421875, 0.5166015625, 0.0595703125, 0.47607421875, 0.01806640625, 0.21630859375, 0.08837890625, 0.46435546875, 0.01904296875, 0.02734375, 0.033203125, 0.56982421875, 0.138671875, 0.04736328125, 0.22900390625, 0.0, 0.216796875, 0.1083984375, 0.99853515625, 0.02294921875, 0.0634765625, 0.4677734375, 0.03857421875, 0.474609375, 0.9990234375, 0.45263671875, 0.82421875, 0.09521484375, 0.02783203125, 0.029296875, 0.068359375, 0.01416015625, 0.998046875, 0.01708984375, 0.51416015625, 0.50439453125, 0.00537109375, 0.376953125, 0.705078125, 0.0205078125, 0.00732421875, 0.08740234375, 0.4931640625, 0.04638671875, 0.033203125, 0.0146484375, 0.04736328125, 0.1611328125, 0.42822265625, 0.0166015625, 0.046875, 0.4501953125, 0.09228515625, 0.025390625, 0.056640625, 0.0224609375, 0.05126953125]

 sparsity of   [0.0032552082557231188, 0.01801215298473835, 0.0362413190305233, 0.02473958395421505, 0.015625, 0.0, 0.00021701389050576836, 0.0184461809694767, 0.0427517369389534, 0.00021701389050576836, 0.021484375, 0.0271267369389534, 0.0, 0.0, 0.0, 0.02582465298473835, 0.0006510416860692203, 0.0375434048473835, 0.0015190972480922937, 0.0028211805038154125, 0.015407986007630825, 0.0, 0.0, 0.0581597238779068, 0.0106336809694767, 0.009982638992369175, 0.0251736119389534, 0.0, 0.011067708022892475, 0.0314670130610466, 0.0015190972480922937, 0.0, 0.0, 0.0, 0.008463541977107525, 0.0, 0.0, 0.00021701389050576836, 0.290581613779068, 0.0, 0.0, 0.3888888955116272, 0.01692708395421505, 0.01974826492369175, 0.0, 0.0, 0.0125868059694767, 0.0377604179084301, 0.0, 0.0243055559694767, 0.0, 0.010850694961845875, 0.0, 0.00824652798473835, 0.0, 0.02886284701526165, 0.1032986119389534, 0.0, 0.01519097201526165, 0.0, 0.0026041667442768812, 0.0008680555620230734, 0.0321180559694767, 0.0, 0.0640190988779068, 0.009548611007630825, 0.00021701389050576836, 0.0, 0.0323350690305233, 0.0, 0.0186631940305233, 0.02582465298473835, 0.0301649309694767, 0.0, 0.0047743055038154125, 0.1321614533662796, 0.01019965298473835, 0.0, 0.0455729179084301, 0.0, 0.0, 0.009114583022892475, 0.0, 0.0, 0.0006510416860692203, 0.0340711809694767, 0.0, 0.00021701389050576836, 0.05078125, 0.0342881940305233, 0.0533854179084301, 0.01410590298473835, 0.00021701389050576836, 0.0470920130610466, 0.0514322929084301, 0.02387152798473835, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0512152798473835, 0.01627604104578495, 0.0, 0.0470920130610466, 0.0431857630610466, 0.0125868059694767, 0.0015190972480922937, 0.0, 0.0, 0.0, 0.0, 0.01584201492369175, 0.021484375, 0.0013020833721384406, 0.02864583395421505, 0.009114583022892475, 0.0, 0.0049913194961845875, 0.0, 0.0, 0.0349392369389534, 0.1085069477558136, 0.009114583022892475, 0.2962239682674408, 0.0, 0.0086805559694767, 0.0010850694961845875, 0.0, 0.00021701389050576836, 0.008463541977107525, 0.00629340298473835, 0.0, 0.01171875, 0.00021701389050576836, 0.02473958395421505, 0.0, 0.1013454869389534, 0.0, 0.00933159701526165, 0.3645833432674408, 0.02105034701526165, 0.0479600690305233, 0.02018229104578495, 0.0052083334885537624, 0.0, 0.02777777798473835, 0.0, 0.0, 0.0390625, 0.0, 0.0223524309694767, 0.0575086809694767, 0.0186631940305233, 0.001953125, 0.0, 0.8313801884651184, 0.0, 0.0, 0.015407986007630825, 0.012803819961845875, 0.0026041667442768812, 0.0071614584885537624, 0.041015625, 0.013671875, 0.001953125, 0.0321180559694767, 0.0, 0.0, 0.0, 0.01801215298473835, 0.0047743055038154125, 0.02408854104578495, 0.0, 0.0, 0.02300347201526165, 0.048828125, 0.00434027798473835, 0.009982638992369175, 0.0316840298473835, 0.00021701389050576836, 0.02994791604578495, 0.0004340277810115367, 0.0718315988779068, 0.0, 0.0, 0.0462239570915699, 0.0, 0.0225694440305233, 0.0, 0.0, 0.013020833022892475, 0.0, 0.01584201492369175, 0.01323784701526165, 0.1241319477558136, 0.11328125, 0.02213541604578495, 0.00021701389050576836, 0.0, 0.01888020895421505, 0.0622829869389534, 0.0, 0.0, 0.0, 0.01888020895421505, 0.0, 0.0006510416860692203, 0.0, 0.0, 0.0028211805038154125, 0.0, 0.010416666977107525, 0.0, 0.0, 0.02756076492369175, 0.1091579869389534, 0.0, 0.0, 0.014756944961845875, 0.1640625, 0.03059895895421505, 0.710069477558136, 0.0603298619389534, 0.0010850694961845875, 0.0, 0.00021701389050576836, 0.0, 0.005642361007630825, 0.0, 0.0423177070915699, 0.0, 0.0, 0.012369791977107525, 0.0243055559694767, 0.0010850694961845875, 0.0303819440305233, 0.00021701389050576836, 0.0290798619389534, 0.0, 0.009114583022892475, 0.0, 0.0, 0.02278645895421505, 0.01323784701526165, 0.00021701389050576836, 0.0388454869389534, 0.0483940988779068, 0.005859375, 0.008897569961845875, 0.0, 0.02105034701526165, 0.0, 0.0, 0.0, 0.02604166604578495, 0.0, 0.0006510416860692203, 0.0, 0.0, 0.0, 0.01909722201526165, 0.0989583358168602, 0.01714409701526165, 0.0310329869389534, 0.03125, 0.0440538190305233, 0.0071614584885537624, 0.0557725690305233, 0.02973090298473835, 0.0, 0.00021701389050576836, 0.0028211805038154125, 0.01019965298473835, 0.0, 0.0052083334885537624, 0.0, 0.0004340277810115367, 0.0460069440305233, 0.0, 0.0015190972480922937, 0.0421006940305233, 0.01779513992369175, 0.0, 0.0, 0.0, 0.0026041667442768812, 0.0234375, 0.00021701389050576836, 0.001953125, 0.0010850694961845875, 0.0225694440305233, 0.0, 0.011501736007630825, 0.0, 0.006076388992369175, 0.0703125, 0.0, 0.00629340298473835, 0.02734375, 0.0991753488779068, 0.0, 0.0, 0.02734375, 0.0681423619389534, 0.0173611119389534, 0.00021701389050576836, 0.0414496548473835, 0.0006510416860692203, 0.5455729365348816, 0.0340711809694767, 0.0067274305038154125, 0.00021701389050576836, 0.0, 0.0368923619389534, 0.15625, 0.0358072929084301, 0.01215277798473835, 0.0325520820915699, 0.0707465261220932, 0.1595052033662796, 0.008897569961845875, 0.02799479104578495, 0.0069444444961845875, 0.0, 0.0032552082557231188, 0.0, 0.0, 0.014322916977107525, 0.01996527798473835, 0.0, 0.0, 0.0399305559694767, 0.0006510416860692203, 0.00021701389050576836, 0.0173611119389534, 0.0, 0.007595486007630825, 0.02951388992369175, 0.0, 0.0, 0.0661892369389534, 0.0023871527519077063, 0.008897569961845875, 0.0008680555620230734, 0.014756944961845875, 0.013888888992369175, 0.00933159701526165, 0.02669270895421505, 0.014973958022892475, 0.0006510416860692203, 0.0006510416860692203, 0.0496961809694767, 0.0329861119389534, 0.0, 0.1228298619389534, 0.0, 0.02669270895421505, 0.0, 0.0, 0.0067274305038154125, 0.0, 0.0212673619389534, 0.0394965298473835, 0.0, 0.0, 0.0015190972480922937, 0.0, 0.0264756940305233, 0.79296875, 0.0004340277810115367, 0.0, 0.0069444444961845875, 0.0032552082557231188, 0.0047743055038154125, 0.0592447929084301, 0.0243055559694767, 0.02278645895421505, 0.6223958134651184, 0.006076388992369175, 0.0375434048473835, 0.0, 0.00021701389050576836, 0.0462239570915699, 0.001953125, 0.02669270895421505, 0.0, 0.0245225690305233, 0.01128472201526165, 0.0023871527519077063, 0.0036892362404614687, 0.0, 0.0, 0.0, 0.052734375, 0.00021701389050576836, 0.0, 0.005859375, 0.0, 0.0, 0.0032552082557231188, 0.9997829794883728, 0.0, 0.00021701389050576836, 0.0321180559694767, 0.0026041667442768812, 0.012803819961845875, 0.0, 0.0414496548473835, 0.0447048619389534, 0.00021701389050576836, 0.0336371548473835, 0.02408854104578495, 0.00021701389050576836, 0.0, 0.0030381944961845875, 0.0, 0.0145399309694767, 0.0008680555620230734, 0.0212673619389534, 0.01974826492369175, 0.0, 0.02473958395421505, 0.0, 0.0, 0.0345052070915699, 0.0, 0.0015190972480922937, 0.00021701389050576836, 0.0358072929084301, 0.0212673619389534, 0.0598958320915699, 0.011501736007630825, 0.0145399309694767, 0.011067708022892475, 0.01974826492369175, 0.005425347480922937, 0.0049913194961845875, 0.02387152798473835, 0.0737847238779068, 0.0518663190305233, 0.0026041667442768812, 0.01410590298473835, 0.012803819961845875, 0.03081597201526165, 0.014322916977107525, 0.01822916604578495, 0.021484375, 0.013888888992369175, 0.02560763992369175, 0.0, 0.0, 0.0364583320915699, 0.0338541679084301, 0.1456163227558136, 0.009548611007630825, 0.0, 0.010850694961845875, 0.0017361111240461469, 0.0086805559694767, 0.017578125, 0.01974826492369175, 0.0028211805038154125, 0.0045572915114462376, 0.0, 0.0, 0.01714409701526165, 0.02191840298473835, 0.009548611007630825, 0.02083333395421505, 0.011501736007630825, 0.0, 0.0, 0.0418836809694767, 0.0319010429084301, 0.01019965298473835, 0.0071614584885537624, 0.0, 0.0184461809694767, 0.012803819961845875, 0.0, 0.046875, 0.0423177070915699, 0.0, 0.006076388992369175, 0.010416666977107525, 0.0125868059694767, 0.00021701389050576836, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0, 0.9997829794883728, 0.0023871527519077063, 0.0251736119389534, 0.0, 0.0, 0.0709635391831398, 0.0, 0.0, 0.0, 0.02300347201526165, 0.0, 0.0028211805038154125, 0.0310329869389534, 0.00629340298473835, 0.0167100690305233, 0.0]

 sparsity of   [0.0, 0.01171875, 0.00390625, 0.00390625, 0.0, 0.015625, 0.009765625, 0.0, 0.001953125, 0.001953125, 0.162109375, 0.0, 0.00390625, 0.0, 0.0, 0.0078125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.001953125, 0.0, 0.001953125, 0.005859375, 0.01171875, 0.001953125, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.015625, 0.009765625, 0.013671875, 0.041015625, 0.091796875, 0.009765625, 0.06640625, 0.00390625, 0.005859375, 0.009765625, 0.0, 0.0, 0.0, 0.064453125, 0.00390625, 0.0, 0.0, 0.04296875, 0.001953125, 0.05078125, 0.0, 0.001953125, 0.0, 0.0, 0.00390625, 0.009765625, 0.033203125, 0.048828125, 0.1015625, 0.01171875, 0.0, 0.013671875, 0.0, 0.005859375, 0.001953125, 0.00390625, 0.005859375, 0.00390625, 0.0078125, 0.0, 0.015625, 0.0, 0.01171875, 0.994140625, 0.0, 0.0, 0.0, 0.021484375, 0.00390625, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.990234375, 0.994140625, 0.04296875, 0.0, 0.01953125, 0.005859375, 0.0, 0.0, 0.0, 0.005859375, 0.001953125, 0.0, 0.005859375, 0.0, 0.001953125, 0.02734375, 0.0, 0.38671875, 0.0, 0.0, 0.021484375, 0.00390625, 0.041015625, 0.044921875, 0.041015625, 0.001953125, 0.0, 0.001953125, 0.0, 0.0625, 0.0, 0.001953125, 0.0, 0.0, 0.013671875, 0.001953125, 0.00390625, 0.0, 0.0, 0.01171875, 0.01953125, 0.0, 0.0, 0.0, 0.056640625, 0.0, 0.0, 0.083984375, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.08984375, 0.046875, 0.005859375, 0.01171875, 0.0, 0.0, 0.0, 0.04296875, 0.0, 0.0, 0.0, 0.005859375, 0.03515625, 0.935546875, 0.01953125, 0.05859375, 0.0, 0.0, 0.0078125, 0.0, 0.01953125, 0.009765625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0078125, 0.0, 0.087890625, 0.017578125, 0.0, 0.00390625, 0.99609375, 0.005859375, 0.0, 0.990234375, 0.0234375, 0.005859375, 0.0, 0.001953125, 0.0, 0.037109375, 0.001953125, 0.0, 0.0, 0.00390625, 0.00390625, 0.0, 0.0, 0.0, 0.005859375, 0.009765625, 0.009765625, 0.904296875, 0.005859375, 0.998046875, 0.0, 0.0, 0.001953125, 0.015625, 0.0703125, 0.001953125, 0.01953125, 0.00390625, 0.0, 0.015625, 0.02734375, 0.005859375, 0.0078125, 0.01953125, 0.001953125, 0.0, 0.19921875, 0.0, 0.001953125, 0.0, 0.0, 0.03515625, 0.431640625, 0.0, 0.0, 0.0, 0.00390625, 0.001953125, 0.001953125, 0.98828125, 0.001953125, 0.0, 0.013671875, 0.060546875, 0.0, 0.013671875, 0.001953125, 0.005859375, 0.033203125, 0.0, 0.0, 0.009765625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.005859375, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0078125, 0.009765625, 0.13671875, 0.0625, 0.017578125, 0.025390625, 0.0, 0.76953125, 0.033203125, 0.0, 0.005859375, 0.0234375, 0.015625, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.017578125, 0.00390625, 0.0, 0.103515625, 0.01171875, 0.0, 0.0078125, 0.001953125, 0.0, 0.005859375, 0.0, 0.009765625, 0.0, 0.013671875, 0.05078125, 0.0, 0.001953125, 0.0, 0.001953125, 0.009765625, 0.0078125, 0.00390625, 0.0, 0.0, 0.0, 0.07421875, 0.009765625, 0.00390625, 0.0, 0.0, 0.052734375, 0.0, 0.0, 0.029296875, 0.001953125, 0.001953125, 0.001953125, 0.0234375, 0.015625, 0.0, 0.0, 0.001953125, 0.037109375, 0.048828125, 0.0, 0.0, 0.0, 0.001953125, 0.001953125, 0.009765625, 0.037109375, 0.01171875, 0.05078125, 0.009765625, 0.009765625, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.0, 0.009765625, 0.005859375, 0.0234375, 0.966796875, 0.0, 0.03125, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.00390625, 0.0, 0.0, 0.056640625, 0.0, 0.017578125, 0.0078125, 0.994140625, 0.732421875, 0.0, 0.02734375, 0.0078125, 0.013671875, 0.005859375, 0.0078125, 0.005859375, 0.0, 0.052734375, 0.0, 0.060546875, 0.01171875, 0.00390625, 0.0546875, 0.0, 0.037109375, 0.0, 0.0, 0.005859375, 0.0, 0.015625, 0.001953125, 0.0, 0.03125, 0.994140625, 0.001953125, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0078125, 0.009765625, 0.009765625, 0.01171875, 0.0078125, 0.009765625, 0.0, 0.013671875, 0.0, 0.00390625, 0.005859375, 0.015625, 0.0, 0.0, 0.0234375, 0.173828125, 0.998046875, 0.009765625, 0.00390625, 0.001953125, 0.005859375, 0.03515625, 0.0, 0.00390625, 0.0, 0.001953125, 0.0, 0.0, 0.01953125, 0.0, 0.068359375, 0.005859375, 0.021484375, 0.005859375, 0.0078125, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0234375, 0.013671875, 0.015625, 0.001953125, 0.9921875, 0.0, 0.078125, 0.01171875, 0.001953125, 0.0, 0.01171875, 0.0, 0.0, 0.0078125, 0.0, 0.005859375, 0.001953125, 0.001953125, 0.0, 0.0, 0.005859375, 0.0, 0.03125, 0.001953125, 0.0, 0.013671875, 0.013671875, 0.99609375, 0.056640625, 0.0, 0.0, 0.001953125, 0.005859375, 0.001953125, 0.005859375, 0.0, 0.009765625, 0.03515625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.05859375, 0.0, 0.001953125, 0.0078125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.0, 0.001953125, 0.05078125, 0.0078125, 0.0078125, 0.009765625, 0.017578125, 0.015625, 0.0, 0.001953125, 0.4296875, 0.0, 0.0, 0.0234375, 0.0, 0.775390625, 0.001953125, 0.0234375, 0.0, 0.0, 0.0078125, 0.029296875, 0.001953125, 0.0, 0.0234375, 0.017578125, 0.009765625, 0.001953125, 0.0, 0.0, 0.01953125, 0.0078125, 0.017578125, 0.01953125, 0.07421875, 0.03125, 0.005859375, 0.01171875, 0.01171875, 0.00390625, 0.0078125, 0.02734375, 0.005859375, 0.005859375, 0.0, 0.0, 0.009765625, 0.0, 0.23828125, 0.001953125, 0.9375, 0.0, 0.005859375, 0.0, 0.017578125, 0.966796875, 0.044921875, 0.0, 0.01953125, 0.015625, 0.0, 0.001953125, 0.017578125, 0.0, 0.029296875, 0.0, 0.0, 0.099609375, 0.01171875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.013671875, 0.0, 0.01171875, 0.0, 0.0, 0.009765625, 0.0, 0.025390625, 0.001953125, 0.005859375, 0.0078125, 0.1875, 0.0, 0.009765625, 0.0, 0.00390625, 0.0, 0.0234375, 0.001953125, 0.111328125, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.001953125, 0.0, 0.005859375, 0.0, 0.0, 0.001953125, 0.0, 0.005859375, 0.005859375, 0.0078125, 0.0, 0.0, 0.001953125, 0.0, 0.005859375, 0.0, 0.0, 0.015625, 0.0, 0.169921875, 0.0, 0.013671875, 0.01953125, 0.005859375, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.01953125, 0.0078125, 0.005859375, 0.125, 0.0, 0.0, 0.01171875, 0.0234375, 0.263671875, 0.01171875, 0.001953125, 0.009765625, 0.0, 0.00390625, 0.005859375, 0.005859375, 0.0, 0.693359375, 0.0, 0.0, 0.0, 0.025390625, 0.013671875, 0.0, 0.052734375, 0.001953125, 0.0, 0.001953125, 0.021484375, 0.0, 0.0, 0.005859375, 0.0, 0.04296875, 0.017578125, 0.025390625, 0.044921875, 0.01953125, 0.0, 0.0078125, 0.009765625, 0.998046875, 0.0, 0.001953125, 0.0, 0.021484375, 0.013671875, 0.009765625, 0.001953125, 0.001953125, 0.001953125, 0.0, 0.0078125, 0.00390625, 0.017578125, 0.00390625, 0.00390625, 0.0, 0.00390625, 0.0, 0.00390625, 0.0, 0.037109375, 0.00390625, 0.0, 0.0234375, 0.0, 0.009765625, 0.001953125, 0.048828125, 0.009765625, 0.0, 0.021484375, 0.0, 0.0, 0.041015625, 0.001953125, 0.017578125, 0.005859375, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.00390625, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.001953125, 0.16796875, 0.0, 0.998046875, 0.00390625, 0.005859375, 0.0, 0.0, 0.0, 0.02734375, 0.037109375, 0.013671875, 0.017578125, 0.0, 0.0, 0.013671875, 0.001953125, 0.001953125, 0.0, 0.07421875, 0.04296875, 0.009765625, 0.0, 0.009765625, 0.0, 0.00390625, 0.005859375, 0.0, 0.00390625, 0.0, 0.001953125, 0.00390625, 0.06640625, 0.0, 0.01953125, 0.08984375, 0.00390625, 0.0, 0.001953125, 0.0, 0.017578125, 0.005859375, 0.0, 0.009765625, 0.005859375, 0.0, 0.0, 0.01171875, 0.0, 0.01171875, 0.0, 0.009765625, 0.00390625, 0.0, 0.033203125, 0.009765625, 0.005859375, 0.00390625, 0.0, 0.0, 0.0, 0.00390625, 0.00390625, 0.009765625, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.00390625, 0.052734375, 0.037109375, 0.0078125, 0.0, 0.005859375, 0.0078125, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01171875, 0.96484375, 0.0, 0.01171875, 0.017578125, 0.0, 0.0078125, 0.005859375, 0.015625, 0.0078125, 0.005859375, 0.0, 0.00390625, 0.0, 0.005859375, 0.892578125, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.00390625, 0.0, 0.994140625, 0.0, 0.908203125, 0.041015625, 0.00390625, 0.0, 0.00390625, 0.103515625, 0.861328125, 0.005859375, 0.046875, 0.0, 0.01171875, 0.0, 0.056640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146484375, 0.0, 0.0, 0.0078125, 0.09765625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.162109375, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.005859375, 0.0, 0.033203125, 0.0, 0.03125, 0.0, 0.01171875, 0.0, 0.005859375, 0.0, 0.0390625, 0.0, 0.0, 0.0, 0.052734375, 0.00390625, 0.025390625, 0.0, 0.0, 0.00390625, 0.0, 0.0390625, 0.00390625, 0.01171875, 0.05078125, 0.0, 0.01171875, 0.009765625, 0.041015625, 0.0, 0.0, 0.041015625, 0.005859375, 0.0, 0.001953125, 0.037109375, 0.0, 0.009765625, 0.001953125, 0.794921875, 0.171875, 0.0, 0.01171875, 0.0, 0.0, 0.00390625, 0.0, 0.001953125, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.005859375, 0.005859375, 0.029296875, 0.03515625, 0.001953125, 0.009765625, 0.0, 0.0078125, 0.0, 0.0, 0.005859375, 0.001953125, 0.02734375, 0.017578125, 0.03125, 0.013671875, 0.001953125, 0.001953125, 0.0234375, 0.0, 0.015625, 0.0546875, 0.001953125, 0.005859375, 0.0, 0.015625, 0.0, 0.001953125, 0.00390625, 0.001953125, 0.025390625, 0.001953125, 0.013671875, 0.626953125, 0.0, 0.0078125, 0.13671875, 0.009765625, 0.0, 0.0, 0.02734375, 0.0, 0.001953125, 0.0, 0.015625, 0.005859375, 0.001953125, 0.005859375, 0.021484375, 0.001953125, 0.013671875, 0.015625, 0.0, 0.033203125, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.966796875, 0.0078125, 0.0, 0.001953125, 0.005859375, 0.0, 0.0, 0.046875, 0.001953125, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.75390625, 0.0078125, 0.01953125, 0.0, 0.0, 0.0, 0.0234375, 0.0078125, 0.783203125, 0.0, 0.03125, 0.0078125, 0.076171875, 0.0703125, 0.021484375, 0.060546875, 0.0, 0.00390625, 0.0, 0.00390625, 0.994140625, 0.033203125, 0.0, 0.0, 0.009765625, 0.00390625, 0.0078125, 0.0, 0.0, 0.00390625, 0.0, 0.048828125, 0.0078125, 0.0, 0.0, 0.01171875, 0.0, 0.01953125, 0.087890625, 0.0, 0.0, 0.01171875, 0.005859375, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.013671875, 0.03515625, 0.0, 0.0, 0.04296875, 0.0, 0.005859375, 0.0, 0.05078125, 0.009765625, 0.0, 0.109375, 0.0078125, 0.001953125, 0.0078125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.068359375, 0.0, 0.009765625, 0.0, 0.013671875, 0.00390625, 0.005859375, 0.0, 0.001953125, 0.0078125, 0.001953125, 0.0, 0.001953125, 0.01171875, 0.0, 0.0, 0.970703125, 0.0, 0.033203125, 0.0, 0.001953125, 0.123046875, 0.015625, 0.0, 0.03125, 0.0078125, 0.009765625, 0.0, 0.001953125, 0.0, 0.0, 0.9921875, 0.005859375, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.013671875, 0.025390625, 0.001953125, 0.0, 0.169921875, 0.0, 0.001953125, 0.001953125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0234375, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.001953125, 0.005859375, 0.0234375, 0.0, 0.001953125, 0.005859375, 0.015625, 0.001953125, 0.0, 0.0, 0.0, 0.00390625, 0.01171875, 0.0, 0.013671875, 0.0, 0.0, 0.888671875, 0.005859375, 0.005859375, 0.05078125, 0.0, 0.0, 0.783203125, 0.005859375, 0.00390625, 0.001953125, 0.0, 0.0, 0.04296875, 0.0, 0.001953125, 0.001953125, 0.00390625, 0.033203125, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.044921875, 0.0, 0.0, 0.0, 0.01171875, 0.0078125, 0.0, 0.015625, 0.021484375, 0.0, 0.0, 0.0078125, 0.00390625, 0.0, 0.00390625, 0.0078125, 0.001953125, 0.001953125, 0.015625, 0.001953125, 0.0, 0.0, 0.0078125, 0.0078125, 0.005859375, 0.001953125, 0.025390625, 0.0, 0.00390625, 0.0078125, 0.00390625, 0.0, 0.037109375, 0.046875, 0.064453125, 0.009765625, 0.009765625, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0234375, 0.01171875, 0.03125, 0.00390625, 0.001953125, 0.994140625, 0.0, 0.984375, 0.0, 0.0234375, 0.01171875, 0.021484375, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.017578125, 0.0, 0.0, 0.00390625, 0.001953125, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.048828125, 0.025390625, 0.0, 0.033203125, 0.005859375, 0.00390625, 0.001953125, 0.0, 0.017578125, 0.0, 0.029296875, 0.0, 0.0859375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.04296875, 0.01953125, 0.01171875, 0.025390625, 0.0, 0.03515625, 0.001953125, 0.00390625, 0.005859375, 0.0, 0.005859375, 0.009765625, 0.005859375, 0.0, 0.02734375, 0.0, 0.01171875, 0.0, 0.005859375, 0.025390625, 0.0, 0.056640625, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.00390625, 0.0859375, 0.00390625, 0.00390625, 0.0, 0.0, 0.01171875, 0.0, 0.056640625, 0.0, 0.013671875, 0.001953125, 0.0, 0.009765625, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.001953125, 0.26953125, 0.015625, 0.00390625, 0.0, 0.00390625, 0.005859375, 0.001953125, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.00390625, 0.0, 0.0078125, 0.0234375, 0.0, 0.033203125, 0.0, 0.009765625, 0.009765625, 0.0, 0.005859375, 0.009765625, 0.005859375, 0.001953125, 0.072265625, 0.025390625, 0.962890625, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.005859375, 0.0, 0.0, 0.01171875, 0.015625, 0.0, 0.01953125, 0.0, 0.0, 0.0, 0.009765625, 0.001953125, 0.0, 0.04296875, 0.0234375, 0.00390625, 0.029296875, 0.0, 0.009765625, 0.0, 0.0, 0.380859375, 0.0, 0.01953125, 0.044921875, 0.0, 0.02734375, 0.0078125, 0.0, 0.0, 0.00390625, 0.01171875, 0.0, 0.0078125, 0.052734375, 0.0, 0.001953125, 0.02734375, 0.0, 0.001953125, 0.0, 0.017578125, 0.009765625, 0.0, 0.0, 0.005859375, 0.546875, 0.015625, 0.0, 0.708984375, 0.0, 0.0, 0.00390625, 0.005859375, 0.001953125, 0.00390625, 0.0078125, 0.16796875, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.005859375, 0.00390625, 0.009765625, 0.00390625, 0.00390625, 0.07421875, 0.0, 0.005859375, 0.0, 0.712890625, 0.00390625, 0.0, 0.0, 0.01171875, 0.015625, 0.009765625, 0.99609375, 0.0, 0.001953125, 0.0, 0.009765625, 0.00390625, 0.0, 0.005859375, 0.0, 0.009765625, 0.0, 0.0, 0.0625, 0.0, 0.0390625, 0.2265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.005859375, 0.0234375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0078125, 0.0, 0.009765625, 0.0, 0.009765625, 0.013671875, 0.0, 0.00390625, 0.025390625, 0.0234375, 0.0, 0.0, 0.015625, 0.001953125, 0.0, 0.0, 0.01171875, 0.01171875, 0.046875, 0.037109375, 0.005859375, 0.0, 0.0, 0.001953125, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.005859375, 0.056640625, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.890625, 0.017578125, 0.0, 0.0, 0.013671875, 0.01953125, 0.0078125, 0.013671875, 0.189453125, 0.00390625, 0.71875, 0.0, 0.00390625, 0.00390625, 0.0234375, 0.0, 0.126953125, 0.009765625, 0.01953125, 0.03125, 0.0, 0.0, 0.0078125, 0.01171875, 0.0, 0.001953125, 0.001953125, 0.001953125, 0.0, 0.009765625, 0.0, 0.009765625, 0.001953125, 0.013671875, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.048828125, 0.0, 0.01171875, 0.0, 0.01171875, 0.00390625, 0.0, 0.0, 0.01171875, 0.029296875, 0.001953125, 0.015625, 0.0, 0.01953125, 0.005859375, 0.0, 0.263671875, 0.00390625, 0.005859375, 0.0, 0.001953125, 0.005859375, 0.0078125, 0.005859375, 0.00390625, 0.0, 0.0, 0.041015625, 0.0, 0.0, 0.005859375, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.001953125, 0.0078125, 0.005859375, 0.00390625, 0.0, 0.01171875, 0.0, 0.0, 0.025390625, 0.05859375, 0.0078125, 0.017578125, 0.00390625, 0.0, 0.001953125, 0.0, 0.005859375, 0.0078125, 0.0, 0.001953125, 0.00390625, 0.01953125, 0.0, 0.001953125, 0.013671875, 0.0, 0.0, 0.02734375, 0.013671875, 0.0, 0.00390625, 0.009765625, 0.005859375, 0.0, 0.00390625, 0.0, 0.0078125, 0.0, 0.994140625, 0.15234375, 0.00390625, 0.0, 0.0, 0.00390625, 0.017578125, 0.00390625, 0.001953125, 0.0, 0.0234375, 0.001953125, 0.0, 0.00390625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.013671875, 0.00390625, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.001953125, 0.001953125, 0.03515625, 0.009765625, 0.01953125, 0.029296875, 0.005859375, 0.033203125, 0.0, 0.009765625, 0.0, 0.0, 0.076171875, 0.0, 0.005859375, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.025390625, 0.02734375, 0.001953125, 0.046875, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.001953125, 0.044921875, 0.03515625, 0.005859375, 0.0, 0.005859375, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.015625, 0.005859375, 0.27734375, 0.0, 0.048828125, 0.0, 0.0, 0.01953125, 0.029296875, 0.01953125, 0.00390625, 0.0, 0.0, 0.005859375, 0.0, 0.013671875, 0.009765625, 0.021484375, 0.0, 0.029296875, 0.005859375, 0.0, 0.0, 0.005859375, 0.001953125, 0.0, 0.001953125, 0.0, 0.01953125, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.00390625, 0.02734375, 0.052734375, 0.0078125, 0.005859375, 0.0625, 0.056640625, 0.01953125, 0.0, 0.021484375, 0.00390625, 0.005859375, 0.0, 0.0, 0.001953125, 0.044921875, 0.0, 0.02734375, 0.0, 0.005859375, 0.060546875, 0.0, 0.119140625, 0.0625, 0.0, 0.0, 0.0, 0.021484375, 0.00390625, 0.005859375, 0.0, 0.03125, 0.00390625, 0.0, 0.0, 0.0859375, 0.00390625, 0.0, 0.01953125, 0.0234375, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.060546875, 0.0, 0.041015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.01953125, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.03515625, 0.0, 0.005859375, 0.158203125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.916015625, 0.01953125, 0.00390625, 0.0, 0.0078125, 0.029296875, 0.001953125, 0.015625, 0.0, 0.015625, 0.04296875, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.26171875, 0.009765625, 0.0, 0.01171875, 0.0, 0.009765625, 0.0078125, 0.0, 0.009765625, 0.01171875, 0.0, 0.0, 0.037109375, 0.017578125, 0.0, 0.001953125, 0.013671875, 0.0234375, 0.00390625, 0.0, 0.005859375, 0.0, 0.0, 0.0, 0.01171875, 0.001953125, 0.0078125, 0.0, 0.01171875, 0.005859375, 0.017578125, 0.0, 0.001953125, 0.0, 0.0, 0.126953125, 0.0234375, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.00390625, 0.00390625, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.052734375, 0.0, 0.0, 0.01953125, 0.0, 0.00390625, 0.0, 0.009765625, 0.0, 0.0703125, 0.0, 0.00390625, 0.005859375, 0.052734375, 0.0, 0.03515625, 0.001953125, 0.099609375, 0.0, 0.017578125, 0.0, 0.001953125, 0.013671875, 0.0, 0.994140625, 0.017578125, 0.0, 0.00390625, 0.0078125, 0.009765625, 0.0, 0.0, 0.0078125, 0.0, 0.00390625, 0.00390625, 0.0, 0.005859375, 0.0, 0.013671875, 0.00390625, 0.05078125, 0.00390625, 0.005859375, 0.029296875, 0.0, 0.111328125, 0.0, 0.013671875, 0.001953125, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.00390625, 0.005859375, 0.0, 0.470703125, 0.0, 0.03515625, 0.1640625, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.009765625, 0.013671875, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.001953125, 0.009765625, 0.001953125, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0078125, 0.0, 0.005859375, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.07421875, 0.05859375, 0.005859375, 0.0, 0.0, 0.05859375, 0.79296875, 0.0078125, 0.0, 0.01953125, 0.0, 0.02734375, 0.005859375, 0.009765625, 0.001953125, 0.216796875, 0.0, 0.005859375, 0.0, 0.0078125, 0.0078125, 0.0, 0.001953125, 0.0, 0.0078125, 0.021484375, 0.0, 0.00390625, 0.0, 0.0078125, 0.1640625, 0.00390625, 0.123046875, 0.009765625, 0.0078125, 0.060546875, 0.017578125, 0.001953125, 0.001953125, 0.0, 0.029296875, 0.00390625]

 sparsity of   [0.0546875, 0.01025390625, 0.046875, 0.0009765625, 0.037109375, 0.0224609375, 0.00048828125, 0.02880859375, 0.0126953125, 0.03466796875, 0.00146484375, 0.02001953125, 0.01513671875, 0.037109375, 0.04345703125, 0.06005859375, 0.01513671875, 0.0166015625, 0.0, 0.01123046875, 0.04296875, 0.04248046875, 0.19775390625, 0.04052734375, 0.0263671875, 0.02392578125, 0.0009765625, 0.02587890625, 0.95361328125, 0.01171875, 0.13623046875, 0.07421875, 0.0, 0.0830078125, 0.0458984375, 0.0234375, 0.0302734375, 0.013671875, 0.0126953125, 0.00048828125, 0.0283203125, 0.0, 0.02783203125, 0.05859375, 0.92919921875, 0.0322265625, 0.02001953125, 0.01025390625, 0.02490234375, 0.02783203125, 0.029296875, 0.10009765625, 0.05419921875, 0.01171875, 0.0419921875, 0.0361328125, 0.0146484375, 0.0, 0.0439453125, 0.0146484375, 0.18701171875, 0.02587890625, 0.00048828125, 0.02001953125, 0.01806640625, 0.03125, 0.80322265625, 0.04541015625, 0.00830078125, 0.06494140625, 0.06591796875, 0.01123046875, 0.0244140625, 0.03076171875, 0.0498046875, 0.029296875, 0.07763671875, 0.08203125, 0.0224609375, 0.0859375, 0.0419921875, 0.0146484375, 0.046875, 0.00439453125, 0.0, 0.076171875, 0.01416015625, 0.03076171875, 0.05029296875, 0.0849609375, 0.02294921875, 0.0615234375, 0.02197265625, 0.0, 0.041015625, 0.02783203125, 0.01416015625, 0.0, 0.00048828125, 0.01513671875, 0.01025390625, 0.03466796875, 0.00634765625, 0.015625, 0.06494140625, 0.01171875, 0.0341796875, 0.064453125, 0.02490234375, 0.0, 0.01123046875, 0.0205078125, 0.0029296875, 0.0361328125, 0.0126953125, 0.0, 0.0244140625, 0.03564453125, 0.0458984375, 0.05859375, 0.08642578125, 0.00146484375, 0.083984375, 0.04736328125, 0.0712890625, 0.0, 0.01416015625, 0.00244140625, 0.01611328125, 0.02783203125, 0.0107421875, 0.005859375, 0.05810546875, 0.07568359375, 0.0693359375, 0.05126953125, 0.0146484375, 0.0, 0.05908203125, 0.0283203125, 0.00927734375, 0.05029296875, 0.37646484375, 0.0107421875, 0.0, 0.041015625, 0.0380859375, 0.01025390625, 0.0, 0.05419921875, 0.0, 0.00390625, 0.02197265625, 0.2646484375, 0.03466796875, 0.01416015625, 0.0029296875, 0.61865234375, 0.0, 0.0048828125, 0.01953125, 0.02490234375, 0.0498046875, 0.03857421875, 0.0234375, 0.00048828125, 0.10693359375, 0.0703125, 0.09375, 0.0283203125, 0.07470703125, 0.0146484375, 0.0, 0.0380859375, 0.0, 0.0625, 0.0869140625, 0.0625, 0.0087890625, 0.02783203125, 0.01513671875, 0.01318359375, 0.02197265625, 0.0283203125, 0.01708984375, 0.02001953125, 0.07421875, 0.55126953125, 0.02587890625, 0.0703125, 0.02685546875, 0.03955078125, 0.00537109375, 0.00830078125, 0.14111328125, 0.9677734375, 0.0, 0.07763671875, 0.005859375, 0.00146484375, 0.0009765625, 0.04345703125, 0.10009765625, 0.0263671875, 0.0380859375, 0.92138671875, 0.09228515625, 0.05126953125, 0.01416015625, 0.00732421875, 0.01611328125, 0.02392578125, 0.0, 0.13037109375, 0.0341796875, 0.70556640625, 0.0, 0.0, 0.0068359375, 0.10107421875, 0.0458984375, 0.66650390625, 0.23828125, 0.015625, 0.03857421875, 0.0224609375, 0.08837890625, 0.03369140625, 0.0048828125, 0.01025390625, 0.693359375, 0.0, 0.0126953125, 0.03955078125, 0.05029296875, 0.16015625, 0.03662109375, 0.08642578125, 0.0126953125, 0.02392578125, 0.01904296875, 0.03173828125, 0.0341796875, 0.0166015625, 0.61279296875, 0.10546875, 0.0087890625, 0.08447265625, 0.00732421875, 0.04248046875, 0.03857421875, 0.02392578125, 0.00048828125, 0.01904296875, 0.01904296875, 0.03173828125, 0.0, 0.0439453125, 0.0068359375, 0.015625, 0.01123046875, 0.0615234375, 0.0224609375, 0.02978515625, 0.09423828125, 0.01806640625, 0.0, 0.408203125, 0.0791015625, 0.02685546875, 0.64208984375, 0.01708984375, 0.02099609375, 0.0341796875, 0.00146484375, 0.0, 0.01708984375, 0.1298828125, 0.5068359375, 0.7841796875, 0.0185546875, 0.01416015625, 0.04248046875, 0.0185546875, 0.42822265625, 0.0068359375, 0.052734375, 0.0380859375, 0.033203125, 0.0087890625, 0.07958984375, 0.455078125, 0.05224609375, 0.08740234375, 0.0009765625, 0.091796875, 0.0009765625, 0.0458984375, 0.0966796875, 0.0009765625, 0.02490234375, 0.02685546875, 0.0625, 0.02587890625, 0.00341796875, 0.970703125, 0.041015625, 0.0166015625, 0.029296875, 0.00927734375, 0.0107421875, 0.0439453125, 0.6962890625, 0.00048828125, 0.029296875, 0.02197265625, 0.3232421875, 0.021484375, 0.05224609375, 0.04150390625, 0.0703125, 0.1240234375, 0.0224609375, 0.078125, 0.0380859375, 0.037109375, 0.1533203125, 0.0126953125, 0.03173828125, 0.001953125, 0.03759765625, 0.07568359375, 0.0, 0.0283203125, 0.0185546875, 0.1572265625, 0.0263671875, 0.00439453125, 0.08837890625, 0.04345703125, 0.857421875, 0.00732421875, 0.06787109375, 0.01513671875, 0.033203125, 0.04052734375, 0.00439453125, 0.34375, 0.07421875, 0.01904296875, 0.04931640625, 0.0302734375, 0.01953125, 0.1787109375, 0.3330078125, 0.02978515625, 0.02197265625, 0.015625, 0.109375, 0.01513671875, 0.00048828125, 0.02734375, 0.02392578125, 0.02001953125, 0.07275390625, 0.00537109375, 0.126953125, 0.998046875, 0.05712890625, 0.0166015625, 0.021484375, 0.04052734375, 0.02783203125, 0.02099609375, 0.017578125, 0.78466796875, 0.0, 0.6650390625, 0.03271484375, 0.05029296875, 0.01953125, 0.00048828125, 0.01953125, 0.0341796875, 0.04833984375, 0.0361328125, 0.041015625, 0.04638671875, 0.03955078125, 0.048828125, 0.01220703125, 0.0078125, 0.02587890625, 0.0390625, 0.01708984375, 0.205078125, 0.02783203125, 0.029296875, 0.03466796875, 0.0, 0.0419921875, 0.0634765625, 0.03515625, 0.02783203125, 0.0390625, 0.0322265625, 0.02587890625, 0.05517578125, 0.01953125, 0.01123046875, 0.03515625, 0.01904296875, 0.060546875, 0.09130859375, 0.037109375, 0.19580078125, 0.0419921875, 0.04150390625, 0.08837890625, 0.017578125, 0.0458984375, 0.00537109375, 0.0, 0.0205078125, 0.0478515625, 0.0244140625, 0.00927734375, 0.0, 0.0625, 0.08056640625, 0.0166015625, 0.0673828125, 0.0419921875, 0.025390625, 0.072265625, 0.04052734375, 0.00927734375, 0.05029296875, 0.04931640625, 0.0205078125, 0.046875, 0.064453125, 0.0126953125, 0.99755859375, 0.0009765625, 0.005859375, 0.0908203125, 0.0283203125, 0.0517578125, 0.01416015625, 0.00927734375, 0.02294921875, 0.0322265625, 0.0, 0.0693359375, 0.04296875, 0.029296875, 0.00048828125, 0.015625, 0.02001953125, 0.0390625, 0.05029296875, 0.0732421875, 0.03955078125, 0.02001953125, 0.0185546875, 0.0, 0.0478515625, 0.01025390625, 0.0546875, 0.0732421875, 0.00146484375, 0.86279296875, 0.9296875, 0.60986328125, 0.0205078125, 0.1279296875, 0.0009765625, 0.025390625, 0.02587890625, 0.025390625, 0.115234375, 0.044921875, 0.03466796875, 0.0517578125, 0.19140625, 0.0361328125, 0.01220703125, 0.02001953125, 0.03515625, 0.02587890625, 0.025390625, 0.03955078125, 0.0, 0.06884765625, 0.03125, 0.04833984375, 0.046875, 0.01123046875, 0.02197265625, 0.0439453125, 0.04736328125, 0.015625, 0.0283203125, 0.0, 0.02294921875, 0.017578125, 0.0439453125, 0.0537109375, 0.03759765625, 0.3876953125, 0.03955078125]

 sparsity of   [0.0407986119389534, 0.0, 0.0842013880610466, 0.0327690988779068, 0.0004340277810115367, 0.03081597201526165, 0.0583767369389534, 0.0594618059694767, 0.00021701389050576836, 0.00021701389050576836, 0.00021701389050576836, 0.1126302108168602, 0.0579427070915699, 0.0, 0.0, 0.014973958022892475, 0.01519097201526165, 0.0, 0.0034722222480922937, 0.0206163190305233, 0.9982638955116272, 0.0466579869389534, 0.9906684160232544, 0.0399305559694767, 0.02560763992369175, 0.0015190972480922937, 0.0052083334885537624, 0.00021701389050576836, 0.0, 0.0798611119389534, 0.0, 0.02864583395421505, 0.1315104216337204, 0.02191840298473835, 0.0049913194961845875, 0.0381944440305233, 0.0173611119389534, 0.0010850694961845875, 0.0, 0.00390625, 0.00021701389050576836, 0.0225694440305233, 0.02387152798473835, 0.00021701389050576836, 0.0125868059694767, 0.0, 0.0, 0.004123263992369175, 0.0, 0.0570746548473835, 0.0509982630610466, 0.0, 0.0696614608168602, 0.01996527798473835, 0.0030381944961845875, 0.006076388992369175, 0.1338975727558136, 0.1145833358168602, 0.0394965298473835, 0.1497395783662796, 0.0, 0.0340711809694767, 0.0757378488779068, 0.0, 0.0, 0.0798611119389534, 0.0336371548473835, 0.01779513992369175, 0.212673619389534, 0.0, 0.0815972238779068, 0.1369357705116272, 0.00629340298473835, 0.014322916977107525, 0.3216145932674408, 0.0, 0.0852864608168602, 0.0, 0.0321180559694767, 0.115234375, 0.0481770820915699, 0.0006510416860692203, 0.0, 0.0, 0.0017361111240461469, 0.0, 0.01714409701526165, 0.0390625, 0.0036892362404614687, 0.00021701389050576836, 0.0078125, 0.0611979179084301, 0.002170138992369175, 0.0008680555620230734, 0.0, 0.01410590298473835, 0.03059895895421505, 0.0909288227558136, 0.010416666977107525, 0.0329861119389534, 0.51171875, 0.0594618059694767, 0.01323784701526165, 0.0, 0.0028211805038154125, 0.0, 0.01410590298473835, 0.010850694961845875, 0.0915798619389534, 0.0067274305038154125, 0.0145399309694767, 0.0440538190305233, 0.0282118059694767, 0.9696180820465088, 0.0342881940305233, 0.0052083334885537624, 0.00021701389050576836, 0.0203993059694767, 0.0, 0.02170138992369175, 0.03125, 0.0, 0.02387152798473835, 0.00434027798473835, 0.0687934011220932, 0.00021701389050576836, 0.9995659589767456, 0.0677083358168602, 0.0399305559694767, 0.134765625, 0.0282118059694767, 0.1603732705116272, 0.0004340277810115367, 0.0, 0.1924913227558136, 0.0186631940305233, 0.015625, 0.0598958320915699, 0.0, 0.037109375, 0.00390625, 0.1334635466337204, 0.1032986119389534, 0.01953125, 0.0017361111240461469, 0.0505642369389534, 0.0603298619389534, 0.0004340277810115367, 0.008029513992369175, 0.0, 0.0264756940305233, 0.0023871527519077063, 0.0792100727558136, 0.00021701389050576836, 0.0028211805038154125, 0.0616319440305233, 0.0164930559694767, 0.1629774272441864, 0.00021701389050576836, 0.017578125, 0.0086805559694767, 0.0010850694961845875, 0.9993489384651184, 0.071180559694767, 0.02734375, 0.0006510416860692203, 0.0859375, 0.8515625, 0.0401475690305233, 0.288628488779068, 0.007595486007630825, 0.009765625, 0.00824652798473835, 0.0483940988779068, 0.03081597201526165, 0.91796875, 0.0726996511220932, 0.005642361007630825, 0.0065104165114462376, 0.146267369389534, 0.0805121511220932, 0.0, 0.03059895895421505, 0.0, 0.999131977558136, 0.00021701389050576836, 0.015407986007630825, 0.0894097238779068, 0.02191840298473835, 0.0464409738779068, 0.0, 0.01410590298473835, 0.0, 0.0271267369389534, 0.0036892362404614687, 0.091796875, 0.00021701389050576836, 0.088758684694767, 0.0, 0.1022135391831398, 0.3396267294883728, 0.0, 0.005425347480922937, 0.0282118059694767, 0.02083333395421505, 0.0243055559694767, 0.0069444444961845875, 0.8426649570465088, 0.1378038227558136, 0.0015190972480922937, 0.0987413227558136, 0.2452256977558136, 0.0930989608168602, 0.0941840261220932, 0.01519097201526165, 0.0, 0.01171875, 0.0470920130610466, 0.0, 0.00390625, 0.01779513992369175, 0.00021701389050576836, 0.075086809694767, 0.0542534738779068, 0.0167100690305233, 0.013020833022892475, 0.0034722222480922937, 0.0414496548473835, 0.0, 0.0833333358168602, 0.0106336809694767, 0.01692708395421505, 0.0052083334885537624, 0.0, 0.0193142369389534, 0.274956613779068, 0.5138888955116272, 0.0362413190305233, 0.6959635615348816, 0.0412326380610466, 0.0004340277810115367, 0.0015190972480922937, 0.0494791679084301, 0.02365451492369175, 0.1098090261220932, 0.00434027798473835, 0.00021701389050576836, 0.0401475690305233, 0.0023871527519077063, 0.103515625, 0.0377604179084301, 0.0004340277810115367, 0.4385850727558136, 0.1091579869389534, 0.02973090298473835, 0.02191840298473835, 0.00021701389050576836, 0.01128472201526165, 0.00021701389050576836, 0.0390625, 0.2658420205116272, 0.0, 0.0386284738779068, 0.0, 0.02191840298473835, 0.0444878488779068, 0.0015190972480922937, 0.010416666977107525, 0.1447482705116272, 0.0008680555620230734, 0.134548619389534, 0.00390625, 0.0, 0.013888888992369175, 0.01909722201526165, 0.0792100727558136, 0.0451388880610466, 0.0460069440305233, 0.0, 0.006076388992369175, 0.0362413190305233, 0.0651041641831398, 0.1612413227558136, 0.0, 0.0718315988779068, 0.0, 0.02582465298473835, 0.189236119389534, 0.0685763880610466, 0.0004340277810115367, 0.006076388992369175, 0.0364583320915699, 0.00021701389050576836, 0.1959635466337204, 0.0026041667442768812, 0.1751302033662796, 0.01714409701526165, 0.0, 0.0145399309694767, 0.00629340298473835, 0.00021701389050576836, 0.00021701389050576836, 0.02018229104578495, 0.03125, 0.001953125, 0.0755208358168602, 0.0, 0.007378472480922937, 0.0405815988779068, 0.087890625, 0.0525173619389534, 0.00021701389050576836, 0.0028211805038154125, 0.015407986007630825, 0.00021701389050576836, 0.0529513880610466, 0.0855034738779068, 0.0355902798473835, 0.0, 0.008029513992369175, 0.00021701389050576836, 0.0716145858168602, 0.099609375, 0.0, 0.2810329794883728, 0.9995659589767456, 0.0225694440305233, 0.01692708395421505, 0.0555555559694767, 0.0264756940305233, 0.9995659589767456, 0.01215277798473835, 0.0, 0.9989149570465088, 0.0, 0.1158854141831398, 0.0030381944961845875, 0.013454861007630825, 0.0, 0.0538194440305233, 0.0, 0.0010850694961845875, 0.1410590261220932, 0.0345052070915699, 0.0, 0.014322916977107525, 0.037109375, 0.419487863779068, 0.012803819961845875, 0.0, 0.975694477558136, 0.0581597238779068, 0.253472238779068, 0.0379774309694767, 0.0568576380610466, 0.041015625, 0.0, 0.0106336809694767, 0.0614149309694767, 0.0, 0.1254340261220932, 0.008029513992369175, 0.04296875, 0.01974826492369175, 0.0, 0.4939236044883728, 0.790147602558136, 0.1584201455116272, 0.0, 0.01171875, 0.107421875, 0.319878488779068, 0.9976128339767456, 0.00434027798473835, 0.013671875, 0.002170138992369175, 0.0, 0.0223524309694767, 0.0, 0.0, 0.0, 0.007595486007630825, 0.02669270895421505, 0.0010850694961845875, 0.0421006940305233, 0.0846354141831398, 0.0579427070915699, 0.0013020833721384406, 0.0, 0.0542534738779068, 0.0, 0.00021701389050576836, 0.007595486007630825, 0.006076388992369175, 0.02951388992369175, 0.01627604104578495, 0.015625, 0.1844618022441864, 0.011935763992369175, 0.01128472201526165, 0.00021701389050576836, 0.0542534738779068, 0.1408420205116272, 0.00021701389050576836, 0.0, 0.01323784701526165, 0.0, 0.0, 0.02994791604578495, 0.1918402761220932, 0.0334201380610466, 0.071180559694767, 0.0086805559694767, 0.02365451492369175, 0.0553385429084301, 0.00824652798473835, 0.0030381944961845875, 0.00021701389050576836, 0.01584201492369175, 0.02170138992369175, 0.8591579794883728, 0.06640625, 0.0, 0.0648871511220932, 0.0, 0.011935763992369175, 0.3743489682674408, 0.0486111119389534, 0.0078125, 0.0010850694961845875, 0.00021701389050576836, 0.001953125, 0.00434027798473835, 0.103515625, 0.01605902798473835, 0.53515625, 0.0390625, 0.0414496548473835, 0.060546875, 0.1644965261220932, 0.0106336809694767, 0.02191840298473835, 0.03515625, 0.0, 0.0, 0.2081163227558136, 0.9924045205116272, 0.0065104165114462376, 0.0894097238779068, 0.0004340277810115367, 0.0030381944961845875, 0.0071614584885537624, 0.0538194440305233, 0.0436197929084301, 0.02604166604578495, 0.0529513880610466, 0.9993489384651184, 0.0477430559694767, 0.0, 0.0, 0.0431857630610466, 0.0872395858168602, 0.1729600727558136, 0.0, 0.02582465298473835, 0.037109375, 0.3953993022441864, 0.0006510416860692203, 0.0, 0.0284288190305233, 0.00434027798473835, 0.0447048619389534, 0.0023871527519077063, 0.0, 0.0360243059694767, 0.02213541604578495, 0.0, 0.0381944440305233, 0.0, 0.0, 0.0047743055038154125, 0.012803819961845875, 0.0520833320915699, 0.0635850727558136, 0.0036892362404614687, 0.0390625, 0.0314670130610466, 0.00021701389050576836, 0.3851996660232544, 0.0034722222480922937, 0.0418836809694767, 0.02191840298473835, 0.2662760317325592, 0.7658420205116272, 0.02669270895421505, 0.00021701389050576836, 0.0, 0.1764322966337204, 0.0008680555620230734, 0.00021701389050576836, 0.5629340410232544, 0.0657552108168602, 0.0049913194961845875, 0.0010850694961845875, 0.0, 0.4965277910232544, 0.015407986007630825, 0.02495659701526165, 0.005642361007630825, 0.0010850694961845875, 0.00021701389050576836, 0.0]

 sparsity of   [0.0234375, 0.99609375, 0.20703125, 0.1328125, 0.619140625, 0.03125, 0.78125, 0.037109375, 0.0546875, 0.19921875, 0.99609375, 0.00390625, 0.033203125, 0.076171875, 0.201171875, 0.623046875, 0.025390625, 0.05078125, 0.802734375, 0.021484375, 0.013671875, 0.666015625, 0.1328125, 0.27734375, 0.12890625, 0.03125, 0.1953125, 0.7890625, 0.033203125, 0.998046875, 0.09765625, 0.021484375, 0.013671875, 0.072265625, 0.05859375, 0.0078125, 0.029296875, 0.994140625, 0.021484375, 0.748046875, 0.240234375, 0.01171875, 0.033203125, 0.728515625, 0.025390625, 0.02734375, 0.064453125, 0.037109375, 0.037109375, 0.068359375, 0.04296875, 0.326171875, 0.744140625, 0.0625, 0.068359375, 0.017578125, 0.099609375, 0.267578125, 0.033203125, 0.013671875, 0.033203125, 0.009765625, 0.109375, 0.806640625, 0.009765625, 0.025390625, 0.86328125, 0.640625, 0.4453125, 0.025390625, 0.021484375, 0.015625, 0.03515625, 0.083984375, 0.005859375, 0.09765625, 0.05859375, 0.669921875, 0.01171875, 0.123046875, 0.09765625, 0.7578125, 0.994140625, 0.0390625, 0.10546875, 0.02734375, 0.947265625, 0.43359375, 0.04296875, 0.015625, 0.814453125, 0.107421875, 0.021484375, 0.064453125, 0.017578125, 0.02734375, 0.06640625, 0.013671875, 0.01171875, 0.060546875, 0.0546875, 0.091796875, 0.041015625, 0.07421875, 0.013671875, 0.111328125, 0.013671875, 0.0390625, 0.14453125, 0.017578125, 0.029296875, 0.05859375, 0.76953125, 0.015625, 0.005859375, 0.111328125, 0.19140625, 0.1640625, 0.005859375, 0.046875, 0.01171875, 0.00390625, 0.228515625, 0.10546875, 0.09375, 0.037109375, 0.02734375, 0.064453125, 0.001953125, 0.03125, 0.021484375, 0.052734375, 0.02734375, 0.091796875, 0.013671875, 0.107421875, 0.017578125, 0.044921875, 0.05859375, 0.572265625, 0.001953125, 0.07421875, 0.63671875, 0.02734375, 0.705078125, 0.056640625, 0.21484375, 0.060546875, 0.041015625, 0.037109375, 0.08984375, 0.6328125, 0.220703125, 0.0625, 0.998046875, 0.7421875, 0.033203125, 0.0078125, 0.072265625, 0.994140625, 0.04296875, 0.25, 0.017578125, 0.04296875, 0.994140625, 0.078125, 0.55859375, 0.1015625, 0.01171875, 0.802734375, 0.126953125, 0.380859375, 0.193359375, 0.033203125, 0.712890625, 0.794921875, 0.009765625, 0.111328125, 0.455078125, 0.083984375, 0.025390625, 0.03125, 0.01953125, 0.015625, 0.0078125, 0.935546875, 0.080078125, 0.13671875, 0.63671875, 0.568359375, 0.09375, 0.02734375, 0.046875, 0.001953125, 0.029296875, 0.998046875, 0.15625, 0.0, 0.02734375, 0.04296875, 0.181640625, 0.009765625, 0.04296875, 0.0234375, 0.052734375, 0.1171875, 0.046875, 0.017578125, 0.21875, 0.744140625, 0.99609375, 0.005859375, 0.0390625, 0.02734375, 0.037109375, 0.0078125, 0.05859375, 0.083984375, 0.017578125, 0.072265625, 0.994140625, 0.28125, 0.029296875, 0.951171875, 0.0390625, 0.005859375, 0.310546875, 0.05078125, 0.02734375, 0.17578125, 0.080078125, 0.0390625, 0.015625, 0.091796875, 0.01171875, 0.03125, 0.01953125, 0.048828125, 0.05859375, 0.208984375, 0.025390625, 0.03515625, 0.03125, 0.009765625, 0.078125, 0.73046875, 0.029296875, 0.068359375, 0.587890625, 0.025390625, 0.0, 0.02734375, 0.765625, 0.12109375, 0.169921875, 0.181640625, 0.0234375, 0.0078125, 0.6484375, 0.0625, 0.001953125, 0.029296875, 0.01171875, 0.322265625, 0.109375, 0.02734375, 0.0546875, 0.166015625, 0.876953125, 0.03515625, 0.248046875, 0.099609375, 0.734375, 0.05859375, 0.1015625, 0.216796875, 0.0234375, 0.005859375, 0.025390625, 0.087890625, 0.93359375, 0.0234375, 0.119140625, 0.0234375, 0.03125, 0.025390625, 0.998046875, 0.013671875, 0.013671875, 0.125, 0.01953125, 0.41796875, 0.701171875, 0.103515625, 0.056640625, 0.994140625, 0.650390625, 0.021484375, 0.013671875, 0.046875, 0.056640625, 0.310546875, 0.025390625, 0.06640625, 0.0078125, 0.052734375, 0.1484375, 0.037109375, 0.087890625, 0.17578125, 0.056640625, 0.06640625, 0.01171875, 0.994140625, 0.1328125, 0.021484375, 0.015625, 0.017578125, 0.087890625, 0.99609375, 0.154296875, 0.0390625, 0.03515625, 0.056640625, 0.0546875, 0.99609375, 0.0078125, 0.150390625, 0.078125, 0.2265625, 0.197265625, 0.05078125, 0.0546875, 0.75, 0.994140625, 0.078125, 0.06640625, 0.017578125, 0.037109375, 0.99609375, 0.044921875, 0.61328125, 0.0234375, 0.0625, 0.0078125, 0.041015625, 0.041015625, 0.025390625, 0.01953125, 0.083984375, 0.994140625, 0.013671875, 0.04296875, 0.01171875, 0.001953125, 0.03515625, 0.01171875, 0.044921875, 0.056640625, 0.01171875, 0.0234375, 0.115234375, 0.009765625, 0.005859375, 0.01953125, 0.083984375, 0.240234375, 0.146484375, 0.060546875, 0.041015625, 0.033203125, 0.08984375, 0.1015625, 0.056640625, 0.076171875, 0.017578125, 0.033203125, 0.0703125, 0.693359375, 0.0234375, 0.013671875, 0.70703125, 0.013671875, 0.083984375, 0.041015625, 0.17578125, 0.048828125, 0.044921875, 0.025390625, 0.01171875, 0.998046875, 0.6484375, 0.75390625, 0.115234375, 0.12109375, 0.107421875, 0.013671875, 0.0625, 0.083984375, 0.03125, 0.994140625, 0.01171875, 0.0390625, 0.017578125, 0.05078125, 0.072265625, 0.015625, 0.033203125, 0.015625, 0.025390625, 0.0234375, 0.994140625, 0.025390625, 0.0859375, 0.017578125, 0.185546875, 0.130859375, 0.08203125, 0.0546875, 0.056640625, 0.05078125, 0.03125, 0.134765625, 0.083984375, 0.16015625, 0.00390625, 0.033203125, 0.794921875, 0.044921875, 0.998046875, 0.01953125, 0.12109375, 0.685546875, 0.0078125, 0.197265625, 0.013671875, 0.05078125, 0.02734375, 0.994140625, 0.92578125, 0.02734375, 0.052734375, 0.001953125, 0.091796875, 0.00390625, 0.0390625, 0.541015625, 0.044921875, 0.994140625, 0.71875, 0.162109375, 0.041015625, 0.083984375, 0.1015625, 0.02734375, 0.359375, 0.025390625, 0.0625, 0.107421875, 0.09375, 0.92578125, 0.005859375, 0.28515625, 0.041015625, 0.99609375, 0.0390625, 0.083984375, 0.994140625, 0.02734375, 0.947265625, 0.03125, 0.072265625, 0.060546875, 0.0078125, 0.99609375, 0.056640625, 0.0625, 0.169921875, 0.037109375, 0.033203125, 0.056640625, 0.02734375, 0.0546875, 0.06640625, 0.154296875, 0.07421875, 0.072265625, 0.025390625, 0.083984375, 0.02734375, 0.791015625, 0.12109375, 0.005859375, 0.056640625, 0.833984375, 0.046875, 0.15625, 0.103515625, 0.001953125, 0.23046875, 0.080078125, 0.052734375, 0.779296875, 0.005859375, 0.01171875, 0.03515625, 0.80859375, 0.16015625, 0.03515625, 0.3125, 0.943359375, 0.015625, 0.03125, 0.99609375, 0.1328125, 0.021484375, 0.119140625, 0.90234375, 0.25390625, 0.068359375, 0.13671875, 0.18359375, 0.01171875, 0.28515625, 0.0703125, 0.0390625, 0.65234375, 0.048828125, 0.107421875, 0.994140625, 0.140625, 0.03515625, 0.02734375, 0.044921875, 0.01171875, 0.474609375, 0.01953125, 0.015625, 0.04296875, 0.015625, 0.052734375, 0.01171875, 0.068359375, 0.228515625, 0.052734375, 0.68359375, 0.951171875, 0.078125, 0.03125, 0.037109375, 0.134765625, 0.779296875, 0.017578125, 0.029296875, 0.07421875, 0.064453125, 0.072265625, 0.587890625, 0.033203125, 0.07421875, 0.029296875, 0.994140625, 0.001953125, 0.029296875, 0.208984375, 0.830078125, 0.13671875, 0.060546875, 0.35546875, 0.08984375, 0.03515625, 0.21875, 0.08984375, 0.068359375, 0.12109375, 0.080078125, 0.8671875, 0.642578125, 0.046875, 0.1171875, 0.09765625, 0.662109375, 0.056640625, 0.0390625, 0.021484375, 0.8828125, 0.279296875, 0.134765625, 0.0546875, 0.005859375, 0.283203125, 0.046875, 0.126953125, 0.009765625, 0.265625, 0.296875, 0.998046875, 0.994140625, 0.01171875, 0.025390625, 0.0703125, 0.037109375, 0.19140625, 0.162109375, 0.697265625, 0.169921875, 0.0859375, 0.443359375, 0.013671875, 0.025390625, 0.572265625, 0.076171875, 0.072265625, 0.267578125, 0.150390625, 0.744140625, 0.0234375, 0.05078125, 0.18359375, 0.05078125, 0.994140625, 0.02734375, 0.515625, 0.0234375, 0.2109375, 0.05859375, 0.025390625, 0.5078125, 0.033203125, 0.103515625, 0.998046875, 0.08984375, 0.009765625, 0.029296875, 0.0234375, 0.015625, 0.017578125, 0.736328125, 0.044921875, 0.177734375, 0.046875, 0.458984375, 0.005859375, 0.0, 0.111328125, 0.1171875, 0.015625, 0.296875, 0.03125, 0.33203125, 0.01953125, 0.013671875, 0.54296875, 0.994140625, 0.0234375, 0.330078125, 0.015625, 0.044921875, 0.08984375, 0.103515625, 0.177734375, 0.755859375, 0.0703125, 0.05859375, 0.05859375, 0.025390625, 0.60546875, 0.015625, 0.005859375, 0.18359375, 0.02734375, 0.001953125, 0.015625, 0.03515625, 0.763671875, 0.09375, 0.02734375, 0.021484375, 0.943359375, 0.99609375, 0.029296875, 0.07421875, 0.03125, 0.015625, 0.072265625, 0.04296875, 0.03125, 0.0546875, 0.005859375, 0.021484375, 0.052734375, 0.0859375, 0.19921875, 0.552734375, 0.169921875, 0.041015625, 0.060546875, 0.017578125, 0.025390625, 0.046875, 0.015625, 0.03515625, 0.087890625, 0.03125, 0.05078125, 0.03515625, 0.025390625, 0.7734375, 0.001953125, 0.203125, 0.0078125, 0.060546875, 0.05078125, 0.61328125, 0.087890625, 0.015625, 0.005859375, 0.134765625, 0.169921875, 0.037109375, 0.025390625, 0.0390625, 0.017578125, 0.08203125, 0.033203125, 0.044921875, 0.01953125, 0.041015625, 0.1015625, 0.044921875, 0.0390625, 0.0703125, 0.0234375, 0.0625, 0.041015625, 0.134765625, 0.05859375, 0.041015625, 0.07421875, 0.14453125, 0.009765625, 0.0234375, 0.44140625, 0.0078125, 0.0625, 0.0625, 0.634765625, 0.041015625, 0.029296875, 0.595703125, 0.083984375, 0.048828125, 0.52734375, 0.994140625, 0.1171875, 0.095703125, 0.048828125, 0.439453125, 0.009765625, 0.095703125, 0.998046875, 0.25390625, 0.1015625, 0.43359375, 0.015625, 0.083984375, 0.001953125, 0.1484375, 0.029296875, 0.04296875, 0.953125, 0.033203125, 0.009765625, 0.408203125, 0.068359375, 0.029296875, 0.65625, 0.859375, 0.869140625, 0.0234375, 0.029296875, 0.04296875, 0.02734375, 0.01171875, 0.078125, 0.1484375, 0.052734375, 0.09375, 0.013671875, 0.03515625, 0.029296875, 0.00390625, 0.0234375, 0.044921875, 0.0234375, 0.072265625, 0.458984375, 0.025390625, 0.99609375, 0.111328125, 0.16015625, 0.04296875, 0.01953125, 0.197265625, 0.900390625, 0.013671875, 0.076171875, 0.0234375, 0.048828125, 0.85546875, 0.009765625, 0.01171875, 0.037109375, 0.041015625, 0.025390625, 0.09375, 0.02734375, 0.16796875, 0.0078125, 0.2578125, 0.009765625, 0.037109375, 0.306640625, 0.806640625, 0.046875, 0.68359375, 0.017578125, 0.00390625, 0.048828125, 0.01171875, 0.171875, 0.0859375, 0.744140625, 0.05078125, 0.015625, 0.0390625, 0.37890625, 0.04296875, 0.029296875, 0.021484375, 0.015625, 0.994140625, 0.078125, 0.99609375, 0.12109375, 0.15234375, 0.046875, 0.037109375, 0.09375, 0.04296875, 0.810546875, 0.021484375, 0.03515625, 0.041015625, 0.013671875, 0.0234375, 0.255859375, 0.021484375, 0.11328125, 0.146484375, 0.146484375, 0.1171875, 0.044921875, 0.99609375, 0.07421875, 0.123046875, 0.884765625, 0.0390625, 0.017578125, 0.037109375, 0.015625, 0.005859375, 0.033203125, 0.17578125, 0.052734375, 0.740234375, 0.072265625, 0.015625, 0.02734375, 0.060546875, 0.05859375, 0.01171875, 0.994140625, 0.119140625, 0.037109375, 0.634765625, 0.08984375, 0.009765625, 0.15234375, 0.05078125, 0.09375, 0.8515625, 0.03125, 0.03125, 0.0234375, 0.046875, 0.091796875, 0.0078125, 0.046875, 0.99609375, 0.0390625, 0.4140625, 0.037109375, 0.001953125, 0.013671875, 0.05859375, 0.103515625, 0.087890625, 0.302734375, 0.634765625, 0.77734375, 0.068359375, 0.0859375, 0.013671875, 0.033203125, 0.298828125, 0.14453125, 0.015625, 0.998046875, 0.05859375, 0.10546875, 0.009765625, 0.037109375, 0.49609375, 0.443359375, 0.837890625, 0.0078125, 0.1484375, 0.044921875, 0.013671875, 0.033203125, 0.033203125, 0.0, 0.01171875, 0.060546875, 0.068359375, 0.021484375, 0.494140625, 0.724609375, 0.017578125, 0.048828125, 0.021484375, 0.02734375, 0.052734375, 0.048828125, 0.078125, 0.017578125, 0.185546875, 0.087890625, 0.05078125, 0.99609375, 0.056640625, 0.005859375, 0.029296875, 0.833984375, 0.076171875, 0.05859375, 0.884765625, 0.708984375, 0.009765625, 0.072265625, 0.056640625, 0.06640625, 0.66796875, 0.021484375, 0.07421875, 0.013671875, 0.0625, 0.2265625, 0.0078125, 0.015625, 0.068359375, 0.13671875, 0.013671875, 0.142578125, 0.05078125, 0.05078125, 0.166015625, 0.015625, 0.0, 0.107421875, 0.111328125, 0.736328125, 0.07421875, 0.017578125, 0.033203125, 0.068359375, 0.083984375, 0.06640625, 0.052734375, 0.115234375, 0.03125, 0.029296875, 0.076171875, 0.068359375, 0.197265625, 0.0625, 0.087890625, 0.009765625, 0.064453125, 0.166015625, 0.072265625, 0.025390625, 0.103515625, 0.126953125, 0.6640625, 0.041015625, 0.013671875, 0.0234375, 0.0546875, 0.083984375, 0.107421875, 0.123046875, 0.0625, 0.07421875, 0.01953125, 0.046875, 0.7421875, 0.009765625, 0.060546875, 0.169921875, 0.03125, 0.00390625, 0.486328125, 0.029296875, 0.08203125, 0.078125, 0.001953125, 0.001953125, 0.033203125, 0.0234375, 0.021484375, 0.20703125, 0.052734375, 0.064453125, 0.01171875, 0.013671875, 0.052734375, 0.1953125, 0.017578125, 0.01171875, 0.025390625, 0.072265625, 0.130859375, 0.19921875, 0.115234375, 0.01171875, 0.060546875, 0.021484375, 0.048828125, 0.8125, 0.03515625, 0.015625, 0.06640625, 0.146484375, 0.017578125, 0.009765625, 0.021484375, 0.037109375, 0.998046875, 0.126953125, 0.0390625, 0.0546875, 0.013671875, 0.015625, 0.994140625, 0.203125, 0.041015625, 0.04296875, 0.0234375, 0.03515625, 0.021484375, 0.01953125, 0.0, 0.08984375, 0.130859375, 0.021484375, 0.03515625, 0.017578125, 0.001953125, 0.021484375, 0.123046875, 0.056640625, 0.63671875, 0.041015625, 0.021484375, 0.044921875, 0.009765625, 0.017578125, 0.908203125, 0.0625, 0.095703125, 0.02734375, 0.009765625, 0.33984375, 0.041015625, 0.05078125, 0.05078125, 0.072265625, 0.162109375, 0.798828125, 0.044921875, 0.0546875, 0.015625, 0.18359375, 0.03125, 0.02734375, 0.126953125, 0.99609375, 0.08984375, 0.0234375, 0.068359375, 0.05078125, 0.34765625, 0.109375, 0.0234375, 0.033203125, 0.13671875, 0.064453125, 0.078125, 0.037109375, 0.037109375, 0.013671875, 0.0078125, 0.078125, 0.0390625, 0.55859375, 0.576171875, 0.033203125, 0.025390625, 0.01953125, 0.068359375, 0.064453125, 0.87109375, 0.064453125, 0.1015625, 0.005859375, 0.03125, 0.005859375, 0.994140625, 0.072265625, 0.08203125, 0.13671875, 0.451171875, 0.025390625, 0.181640625, 0.126953125, 0.009765625, 0.041015625, 0.19921875, 0.201171875, 0.078125, 0.01953125, 0.041015625, 0.08984375, 0.994140625, 0.00390625, 0.029296875, 0.009765625, 0.029296875, 0.005859375, 0.083984375, 0.064453125, 0.998046875, 0.41796875, 0.1953125, 0.568359375, 0.994140625, 0.076171875, 0.61328125, 0.046875, 0.017578125, 0.1171875, 0.060546875, 0.041015625, 0.013671875, 0.064453125, 0.953125, 0.76171875, 0.029296875, 0.62109375, 0.994140625, 0.041015625, 0.0234375, 0.1171875, 0.033203125, 0.107421875, 0.005859375, 0.0625, 0.064453125, 0.779296875, 0.1171875, 0.365234375, 0.021484375, 0.16015625, 0.6328125, 0.029296875, 0.025390625, 0.01171875, 0.99609375, 0.04296875, 0.0546875, 0.076171875, 0.0234375, 0.01171875, 0.091796875, 0.033203125, 0.044921875, 0.994140625, 0.015625, 0.078125, 0.203125, 0.029296875, 0.015625, 0.919921875, 0.630859375, 0.78125, 0.056640625, 0.015625, 0.044921875, 0.033203125, 0.01953125, 0.048828125, 0.001953125, 0.064453125, 0.03515625, 0.859375, 0.01171875, 0.015625, 0.6640625, 0.05859375, 0.287109375, 0.068359375, 0.99609375, 0.09375, 0.662109375, 0.01953125, 0.126953125, 0.037109375, 0.01953125, 0.0546875, 0.01953125, 0.119140625, 0.0859375, 0.021484375, 0.20703125, 0.654296875, 0.044921875, 0.025390625, 0.083984375, 0.017578125, 0.068359375, 0.826171875, 0.5234375, 0.94921875, 0.01953125, 0.34765625, 0.01953125, 0.310546875, 0.109375, 0.02734375, 0.02734375, 0.07421875, 0.017578125, 0.392578125, 0.08203125, 0.044921875, 0.3359375, 0.021484375, 0.658203125, 0.02734375, 0.646484375, 0.99609375, 0.05859375, 0.03515625, 0.001953125, 0.0703125, 0.26171875, 0.00390625, 0.279296875, 0.134765625, 0.013671875, 0.021484375, 0.041015625, 0.01171875, 0.02734375, 0.994140625, 0.046875, 0.025390625, 0.076171875, 0.01171875, 0.265625, 0.02734375, 0.119140625, 0.0703125, 0.00390625, 0.083984375, 0.013671875, 0.013671875, 0.2109375, 0.138671875, 0.01953125, 0.796875, 0.044921875, 0.060546875, 0.0546875, 0.05859375, 0.02734375, 0.052734375, 0.029296875, 0.013671875, 0.9453125, 0.85546875, 0.03515625, 0.947265625, 0.107421875, 0.080078125, 0.015625, 0.93359375, 0.015625, 0.029296875, 0.033203125, 0.578125, 0.8359375, 0.1328125, 0.998046875, 0.029296875, 0.013671875, 0.16015625, 0.041015625, 0.01953125, 0.1484375, 0.05859375, 0.205078125, 0.029296875, 0.69921875, 0.091796875, 0.998046875, 0.234375, 0.142578125, 0.041015625, 0.224609375, 0.09375, 0.0625, 0.046875, 0.013671875, 0.654296875, 0.75, 0.013671875, 0.02734375, 0.466796875, 0.03515625, 0.146484375, 0.888671875, 0.015625, 0.01171875, 0.017578125, 0.01171875, 0.029296875, 0.015625, 0.09375, 0.005859375, 0.03125, 0.005859375, 0.263671875, 0.021484375, 0.060546875, 0.0625, 0.02734375, 0.513671875, 0.044921875, 0.689453125, 0.005859375, 0.46484375, 0.009765625, 0.02734375, 0.04296875, 0.6640625, 0.046875, 0.845703125, 0.18359375, 0.046875, 0.328125, 0.03125, 0.099609375, 0.1484375, 0.009765625, 0.072265625, 0.03125, 0.01171875, 0.21875, 0.041015625, 0.064453125, 0.03125, 0.021484375, 0.03515625, 0.052734375, 0.029296875, 0.0234375, 0.07421875, 0.013671875, 0.0234375, 0.095703125, 0.09765625, 0.080078125, 0.1640625, 0.10546875, 0.0546875, 0.044921875, 0.087890625, 0.1015625, 0.765625, 0.998046875, 0.091796875, 0.037109375, 0.064453125, 0.275390625, 0.0234375, 0.03125, 0.4609375, 0.216796875, 0.02734375, 0.015625, 0.029296875, 0.01171875, 0.0078125, 0.06640625, 0.1171875, 0.16796875, 0.083984375, 0.013671875, 0.099609375, 0.033203125, 0.001953125, 0.048828125, 0.01171875, 0.6796875, 0.46484375, 0.16796875, 0.271484375, 0.134765625, 0.30859375, 0.103515625, 0.0234375, 0.03515625, 0.0234375, 0.138671875, 0.009765625, 0.064453125, 0.033203125, 0.994140625, 0.0234375, 0.029296875, 0.03125, 0.646484375, 0.08984375, 0.025390625, 0.01171875, 0.017578125, 0.033203125, 0.08203125, 0.0625, 0.478515625, 0.220703125, 0.03515625, 0.041015625, 0.69140625, 0.0546875, 0.017578125, 0.0234375, 0.1328125, 0.02734375, 0.03515625, 0.14453125, 0.0234375, 0.05859375, 0.994140625, 0.037109375, 0.15234375, 0.001953125, 0.033203125, 0.04296875, 0.029296875, 0.03125, 0.0390625, 0.0390625, 0.0078125, 0.095703125, 0.693359375, 0.208984375, 0.056640625, 0.0390625, 0.025390625, 0.078125, 0.01171875, 0.048828125, 0.033203125, 0.0, 0.03125, 0.048828125, 0.013671875, 0.876953125, 0.052734375, 0.01171875, 0.01953125, 0.044921875, 0.015625, 0.00390625, 0.12890625, 0.068359375, 0.076171875, 0.8671875, 0.001953125, 0.03125, 0.021484375, 0.029296875, 0.033203125, 0.095703125, 0.037109375, 0.03125, 0.017578125, 0.525390625, 0.1953125, 0.0078125, 0.021484375, 0.03125, 0.029296875, 0.125, 0.115234375, 0.068359375, 0.01171875, 0.033203125, 0.029296875, 0.171875, 0.01171875, 0.0625, 0.037109375, 0.0234375, 0.189453125, 0.5234375, 0.052734375, 0.013671875, 0.0546875, 0.896484375, 0.302734375, 0.0390625, 0.029296875, 0.517578125, 0.138671875, 0.0859375, 0.1875, 0.001953125, 0.017578125, 0.037109375, 0.068359375, 0.634765625, 0.009765625, 0.158203125, 0.03515625, 0.03125, 0.0390625, 0.037109375, 0.611328125, 0.04296875, 0.025390625, 0.13671875, 0.16796875, 0.861328125, 0.0234375, 0.626953125, 0.55078125, 0.037109375, 0.708984375, 0.6796875, 0.0234375, 0.517578125, 0.115234375, 0.009765625, 0.130859375, 0.068359375, 0.783203125, 0.033203125, 0.62890625, 0.05859375, 0.080078125, 0.025390625, 0.05859375, 0.21875, 0.923828125, 0.017578125, 0.03515625, 0.013671875, 0.00390625, 0.025390625, 0.044921875, 0.630859375, 0.068359375, 0.0078125, 0.06640625, 0.025390625, 0.001953125, 0.015625, 0.080078125, 0.017578125, 0.0625, 0.48046875, 0.01953125, 0.013671875, 0.08203125, 0.01953125, 0.736328125, 0.080078125, 0.04296875, 0.03515625, 0.173828125, 0.0078125, 0.09375, 0.021484375, 0.015625, 0.91015625, 0.103515625, 0.044921875, 0.0390625, 0.076171875, 0.037109375, 0.005859375, 0.072265625, 0.0234375, 0.62890625, 0.01171875, 0.04296875, 0.05078125, 0.365234375, 0.365234375, 0.001953125, 0.2734375, 0.111328125, 0.263671875, 0.5546875, 0.09765625, 0.642578125, 0.025390625, 0.0078125, 0.037109375, 0.046875, 0.052734375, 0.248046875, 0.103515625, 0.998046875, 0.021484375, 0.005859375, 0.185546875, 0.052734375, 0.1484375, 0.09765625, 0.02734375, 0.52734375, 0.16796875, 0.30859375, 0.046875, 0.048828125, 0.994140625, 0.056640625, 0.111328125, 0.283203125, 0.0625, 0.072265625, 0.341796875, 0.01171875, 0.12109375, 0.59765625, 0.052734375, 0.017578125, 0.09375, 0.05859375, 0.01171875, 0.037109375, 0.6796875, 0.15625, 0.994140625, 0.009765625, 0.017578125, 0.16796875, 0.021484375, 0.9375, 0.033203125, 0.029296875, 0.009765625, 0.85546875, 0.81640625, 0.724609375, 0.009765625, 0.1171875, 0.076171875, 0.02734375, 0.078125, 0.46484375, 0.771484375, 0.029296875, 0.08984375, 0.208984375, 0.037109375, 0.23046875, 0.11328125, 0.03125, 0.017578125, 0.017578125, 0.0625, 0.998046875, 0.037109375, 0.037109375, 0.07421875, 0.06640625, 0.173828125, 0.068359375, 0.048828125, 0.0234375, 0.16015625, 0.099609375, 0.00390625, 0.01171875, 0.064453125, 0.99609375, 0.2734375, 0.12109375, 0.994140625, 0.787109375, 0.658203125, 0.015625, 0.0, 0.53125, 0.14453125, 0.013671875, 0.01953125, 0.0859375, 0.048828125, 0.103515625, 0.080078125, 0.02734375, 0.07421875, 0.18359375, 0.03515625, 0.01171875, 0.056640625, 0.072265625, 0.001953125, 0.99609375, 0.033203125, 0.68359375, 0.05859375, 0.99609375, 0.0859375, 0.16015625, 0.59375, 0.873046875, 0.349609375, 0.994140625, 0.015625, 0.060546875, 0.017578125, 0.037109375, 0.57421875, 0.080078125, 0.0390625, 0.033203125, 0.13671875, 0.818359375, 0.0390625, 0.0078125, 0.0625, 0.998046875, 0.01171875, 0.02734375, 0.072265625, 0.7578125, 0.025390625, 0.056640625, 0.06640625, 0.044921875, 0.041015625, 0.01171875, 0.0390625, 0.099609375, 0.107421875, 0.056640625, 0.052734375, 0.10546875, 0.998046875, 0.029296875, 0.892578125, 0.998046875, 0.07421875, 0.142578125, 0.005859375, 0.060546875, 0.994140625, 0.015625, 0.0703125, 0.048828125, 0.84765625, 0.0390625, 0.021484375, 0.60546875, 0.021484375, 0.0390625, 0.544921875, 0.01953125, 0.037109375, 0.025390625, 0.021484375, 0.017578125, 0.0390625, 0.015625, 0.048828125, 0.140625, 0.8046875, 0.015625, 0.009765625, 0.994140625, 0.044921875, 0.1796875, 0.083984375, 0.177734375, 0.080078125, 0.03515625, 0.01171875, 0.021484375, 0.05078125, 0.94140625, 0.06640625, 0.005859375, 0.013671875, 0.0625, 0.01171875, 0.09765625, 0.052734375, 0.091796875, 0.0234375, 0.353515625, 0.275390625, 0.994140625, 0.060546875, 0.0546875, 0.02734375, 0.03515625, 0.01953125, 0.095703125, 0.23046875, 0.037109375, 0.076171875, 0.07421875, 0.01953125, 0.033203125, 0.013671875, 0.1875, 0.07421875, 0.169921875, 0.03125, 0.041015625, 0.10546875, 0.607421875, 0.044921875, 0.09375, 0.052734375, 0.916015625, 0.013671875, 0.04296875, 0.189453125, 0.056640625, 0.021484375, 0.06640625, 0.642578125, 0.0234375, 0.064453125, 0.025390625, 0.99609375, 0.216796875, 0.072265625, 0.08984375, 0.025390625, 0.01953125, 0.43359375, 0.015625, 0.1953125, 0.01953125, 0.935546875, 0.025390625, 0.02734375, 0.021484375, 0.056640625, 0.99609375, 0.037109375, 0.203125, 0.033203125, 0.009765625, 0.068359375, 0.12109375, 0.07421875, 0.994140625, 0.037109375, 0.037109375, 0.03125, 0.017578125, 0.0234375, 0.09765625, 0.05078125, 0.017578125, 0.08203125, 0.13671875, 0.029296875, 0.748046875, 0.6484375, 0.041015625, 0.04296875, 0.013671875, 0.009765625, 0.09375, 0.005859375, 0.177734375, 0.01953125, 0.08203125, 0.5, 0.095703125, 0.048828125, 0.150390625, 0.0234375, 0.029296875, 0.05859375, 0.02734375, 0.044921875, 0.458984375, 0.056640625, 0.029296875, 0.0625, 0.03125, 0.994140625, 0.033203125, 0.048828125, 0.029296875, 0.02734375, 0.03125, 0.08203125, 0.013671875, 0.134765625, 0.04296875, 0.994140625, 0.025390625, 0.107421875, 0.02734375, 0.08203125, 0.0625, 0.615234375, 0.138671875, 0.048828125, 0.09375, 0.1171875, 0.033203125, 0.0078125, 0.02734375, 0.021484375, 0.001953125, 0.044921875, 0.767578125, 0.029296875, 0.01953125, 0.62890625, 0.037109375, 0.943359375, 0.041015625, 0.01953125, 0.02734375, 0.34375, 0.23046875, 0.158203125, 0.025390625, 0.994140625, 0.091796875, 0.01953125, 0.76953125, 0.994140625, 0.033203125, 0.525390625, 0.025390625, 0.26171875, 0.01171875, 0.7421875, 0.0625, 0.02734375, 0.02734375, 0.0625, 0.03515625, 0.205078125, 0.0078125, 0.1015625, 0.71875, 0.029296875, 0.0390625, 0.71875, 0.99609375, 0.072265625, 0.767578125, 0.0, 0.09375, 0.021484375, 0.037109375, 0.017578125, 0.021484375, 0.046875, 0.04296875, 0.123046875, 0.44140625, 0.00390625, 0.025390625, 0.080078125, 0.185546875, 0.03125, 0.041015625, 0.060546875, 0.01953125, 0.037109375, 0.5234375, 0.03515625, 0.087890625, 0.07421875, 0.03515625, 0.01953125, 0.017578125, 0.10546875, 0.580078125, 0.890625, 0.162109375, 0.025390625, 0.7109375, 0.013671875, 0.0546875, 0.009765625, 0.046875, 0.04296875, 0.033203125, 0.00390625, 0.994140625, 0.013671875, 0.07421875, 0.02734375, 0.05859375, 0.046875, 0.6484375, 0.056640625]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 6648389.02216351 (unstructured) 0 (structured)

max weight is  tensor([3.0669e-01, 2.6106e-09, 2.4819e-01, 4.8502e-01, 6.4784e-09, 2.4851e-09,
        4.5325e-01, 2.1013e-09, 1.8061e-01, 2.1137e-01, 3.5202e-09, 1.0843e-09,
        3.7408e-01, 4.0183e-01, 1.0045e-08, 2.2841e-01, 2.2464e-01, 2.6394e-09,
        4.4146e-09, 1.5090e-01, 7.0084e-04, 1.2437e-01, 1.0721e-01, 2.6106e-09,
        9.7902e-02, 2.7148e-02, 3.8966e-02, 3.2897e-01, 3.0649e-02, 2.6106e-09,
        6.8253e-02, 3.6182e-01, 1.3171e-01, 2.1046e-01, 1.0427e-01, 2.5375e-02,
        6.6403e-09, 1.0614e-01, 3.8678e-01, 5.4117e-02, 3.1909e-09, 3.2171e-01,
        2.5465e-09, 3.4272e-01, 1.2799e-01, 2.7722e-01, 6.1560e-03, 1.2619e-01,
        2.9543e-09, 5.8972e-09, 6.6403e-09, 4.4716e-09, 9.2108e-01, 2.2901e-02,
        2.1960e-01, 2.6106e-09, 2.6106e-09, 1.3663e-01, 6.6116e-03, 1.8423e-01,
        3.8040e-02, 2.6106e-09, 4.6445e-09, 2.2928e-09], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3222e-08, 3.4366e-08, 2.9447e-08, 8.8303e-02, 1.2585e-08, 1.3222e-08,
        3.2271e-01, 1.4225e-01, 3.8150e-08, 1.7374e-08, 5.9135e-08, 3.9954e-08,
        1.2540e-01, 4.4920e-08, 7.9956e-09, 1.3625e-01, 1.2732e-01, 1.1642e-01,
        1.3222e-08, 1.8928e-01, 1.3222e-08, 2.3393e-08, 2.8759e-08, 2.9447e-08,
        1.0396e-01, 1.3249e-08, 1.3222e-08, 1.9749e-08, 1.4075e-08, 2.3393e-08,
        3.4366e-08, 1.4561e-01, 1.2063e-08, 3.0760e-02, 1.1169e-08, 1.3222e-08,
        1.3920e-01, 1.2958e-01, 1.9749e-08, 2.9447e-08, 2.1463e-08, 4.9878e-08,
        1.9749e-08, 1.9749e-08, 3.4366e-08, 9.8014e-09, 1.3222e-08, 1.2177e-08,
        1.2063e-08, 4.1394e-02, 1.2513e-01, 1.2585e-08, 5.9135e-08, 2.3393e-08,
        1.3222e-08, 2.9447e-08, 8.4499e-09, 1.1695e-08, 3.9217e-08, 4.4920e-08,
        3.9217e-08, 1.7623e-08, 1.3222e-08, 5.4164e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.0016e-07, 1.6748e-07, 1.4942e-07, 8.2100e-07, 3.6047e-07, 1.4942e-07,
        8.1207e-02, 2.5627e-07, 2.0370e-07, 2.1167e-01, 8.3470e-07, 1.4942e-07,
        4.3913e-07, 3.6047e-07, 8.0063e-07, 3.7815e-07, 1.6748e-07, 1.0159e-07,
        8.5524e-08, 2.5356e-07, 4.3913e-07, 6.6055e-07, 2.9886e-07, 2.5497e-07,
        2.5252e-01, 4.1817e-02, 4.3853e-07, 4.3506e-02, 2.0370e-07, 1.1842e-07,
        3.0190e-02, 4.3853e-07, 8.3142e-07, 1.0658e-07, 3.6043e-01, 8.3142e-07,
        6.8565e-07, 5.5136e-07, 5.6168e-07, 5.4567e-07, 1.5870e-07, 8.3142e-07,
        8.3470e-07, 4.4231e-07, 4.3853e-07, 2.4655e-01, 1.4942e-07, 3.4854e-07,
        2.5356e-07, 8.3142e-07, 1.4942e-07, 8.3142e-07, 6.1037e-02, 2.9886e-07,
        1.7870e-01, 1.4942e-07, 5.4567e-07, 2.5626e-07, 1.8724e-07, 2.9637e-07,
        2.5356e-07, 1.4942e-07, 4.4568e-02, 3.4103e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.4934e-02, 4.0108e-08, 2.5481e-08, 1.0254e-07, 5.6697e-08, 5.9861e-08,
        2.5572e-08, 1.2573e-01, 1.1264e-07, 2.9799e-02, 1.4289e-08, 5.9696e-02,
        5.0996e-08, 1.2586e-10, 6.8122e-08, 2.1081e-02, 9.8285e-03, 5.7088e-08,
        2.0283e-02, 4.7233e-08, 5.2844e-08, 5.3303e-08, 1.0298e-02, 8.1108e-08,
        7.4307e-08, 1.7632e-02, 2.1723e-08, 6.7465e-08, 1.5728e-01, 1.6495e-08,
        1.2154e-02, 6.2158e-08, 2.3114e-08, 4.0637e-02, 3.3719e-02, 1.9190e-02,
        6.8665e-08, 1.0440e-07, 5.5978e-02, 5.0996e-08, 2.2708e-08, 7.9827e-08,
        2.3320e-02, 1.2687e-07, 3.9909e-08, 2.8157e-02, 5.7969e-08, 1.4704e-08,
        5.0840e-08, 2.2620e-02, 5.8894e-08, 1.0403e-07, 3.7091e-08, 7.2639e-08,
        2.5430e-08, 2.2980e-02, 3.7118e-08, 2.6155e-02, 1.3722e-08, 6.9525e-08,
        5.3002e-08, 1.8331e-02, 5.6429e-02, 2.0437e-08, 2.3127e-08, 1.1854e-02,
        1.2953e-02, 5.2844e-08, 2.3387e-02, 4.7143e-02, 2.9672e-08, 7.4453e-08,
        5.5164e-02, 3.3222e-08, 1.0761e-02, 1.0388e-02, 3.4356e-02, 6.0196e-03,
        1.5819e-08, 9.8765e-08, 1.4318e-01, 1.3420e-02, 3.7754e-02, 1.6495e-08,
        3.6207e-08, 2.6471e-08, 8.9457e-02, 3.4151e-08, 5.7843e-02, 1.3166e-01,
        2.0804e-03, 1.2687e-07, 1.2687e-07, 3.6118e-02, 2.7232e-08, 2.6471e-08,
        4.1659e-02, 1.3727e-01, 1.2182e-01, 3.4345e-02, 3.4895e-08, 6.0710e-08,
        5.7718e-02, 5.6103e-08, 5.2844e-08, 6.7918e-08, 8.8864e-02, 6.0001e-08,
        7.7296e-02, 5.7693e-08, 1.3468e-02, 8.9838e-03, 2.4040e-08, 6.4518e-08,
        3.0562e-08, 3.1413e-02, 5.6238e-02, 5.2899e-08, 3.0770e-02, 1.5247e-08,
        7.6660e-08, 7.9045e-02, 5.8273e-08, 7.9308e-02, 1.3880e-01, 6.8178e-08,
        7.3082e-08, 6.6603e-02, 8.2098e-02, 2.0245e-02, 1.2786e-01, 5.2844e-08,
        1.5377e-01, 3.0214e-02, 7.2639e-08, 1.4704e-08, 3.2524e-08, 2.3766e-08,
        3.4895e-08, 6.7288e-02, 2.2645e-02, 2.9564e-08, 3.4301e-02, 2.2192e-08,
        4.6718e-02, 2.6471e-08, 1.6585e-02, 3.5719e-02, 1.1264e-07, 1.1416e-01,
        2.4040e-08, 7.9990e-08, 7.0984e-08, 5.2257e-08, 1.4139e-02, 6.4518e-08,
        5.1036e-08, 6.1405e-08, 2.1657e-01, 1.5033e-08, 4.7577e-08, 1.7577e-08,
        4.2043e-08, 2.1266e-02, 3.3606e-02, 7.7727e-02, 2.3249e-07, 7.9500e-02,
        6.8105e-08, 2.2708e-08, 4.6788e-02, 7.0504e-08, 3.2212e-03, 2.7515e-02,
        5.2844e-08, 4.6521e-08, 6.4518e-08, 3.8971e-02, 1.5887e-08, 1.4282e-02,
        1.7342e-01, 2.5246e-02, 5.7675e-08, 6.8800e-08, 3.4389e-08, 3.6164e-08,
        5.8273e-08, 3.0070e-08, 2.2708e-08, 7.5773e-02, 4.7904e-02, 7.6532e-08,
        2.7635e-02, 2.8418e-02, 6.4518e-08, 3.8470e-02, 1.4245e-01, 4.9056e-08,
        1.0689e-07, 8.4151e-02, 4.5207e-08, 7.2639e-08, 2.6471e-08, 4.7733e-08,
        4.8666e-08, 2.9970e-08, 5.7422e-02, 1.0689e-07, 2.3275e-08, 4.5208e-08,
        3.2927e-08, 6.9118e-02, 1.7122e-08, 3.8234e-08, 2.4040e-08, 3.6331e-08,
        2.0834e-02, 6.0317e-02, 4.0568e-02, 2.4040e-08, 2.4457e-02, 2.5644e-08,
        5.2844e-08, 3.8633e-08, 7.0976e-08, 2.4934e-02, 3.8235e-08, 5.0996e-08,
        6.1613e-02, 3.4485e-02, 1.0605e-07, 2.8910e-08, 5.1382e-02, 2.1353e-02,
        1.6907e-02, 3.2660e-08, 2.3275e-08, 1.3722e-08, 8.9425e-02, 2.9797e-02,
        6.7465e-08, 1.7530e-08, 5.8273e-08, 1.0152e-02, 1.5703e-01, 6.0670e-08,
        1.5897e-08, 1.8586e-01, 3.6164e-08, 6.2144e-02, 2.0916e-08, 1.1031e-08,
        9.6578e-03, 6.0690e-08, 4.3863e-02, 1.8093e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.2048e-08, 1.2779e-08, 1.7164e-01, 7.9667e-09, 2.0267e-08, 1.4303e-02,
        1.2511e-01, 1.0282e-01, 1.2779e-08, 4.8443e-02, 5.3181e-10, 1.0476e-02,
        1.2779e-08, 1.7304e-08, 1.8799e-08, 1.0121e-01, 1.2248e-01, 1.2846e-01,
        4.8495e-02, 3.0365e-02, 1.2779e-08, 1.6074e-01, 1.0436e-01, 8.2320e-02,
        1.1096e-01, 1.1009e-01, 6.6807e-02, 1.2779e-08, 1.7484e-01, 1.2779e-08,
        1.5266e-01, 1.0073e-01, 1.2779e-08, 5.2277e-02, 7.1564e-02, 3.7849e-03,
        1.2388e-01, 3.7977e-02, 9.5684e-02, 1.2779e-08, 2.3492e-08, 1.8799e-08,
        8.0642e-02, 2.3492e-08, 1.4448e-02, 2.6683e-02, 1.5546e-01, 1.7304e-08,
        2.2997e-08, 2.2725e-01, 8.4445e-02, 1.0651e-01, 9.5202e-02, 1.2779e-08,
        1.1869e-01, 7.3659e-02, 1.1937e-01, 2.0775e-02, 1.8799e-08, 1.8799e-08,
        8.3798e-02, 2.5798e-02, 1.0302e-01, 1.2944e-08, 1.8799e-08, 1.0335e-01,
        1.7182e-01, 1.8799e-08, 1.0643e-01, 1.7486e-01, 2.9050e-09, 3.7268e-02,
        3.3105e-02, 1.4844e-02, 1.7748e-01, 6.7583e-03, 2.4241e-02, 1.0570e-01,
        1.7304e-08, 1.2779e-08, 9.6945e-02, 1.2861e-01, 9.8241e-09, 1.3819e-08,
        1.3850e-01, 1.2779e-08, 1.4912e-01, 2.3657e-08, 6.6996e-02, 4.1423e-02,
        1.8133e-08, 1.2779e-08, 2.3839e-08, 3.9670e-02, 1.7304e-08, 4.2664e-08,
        1.0296e-01, 1.0193e-01, 7.0931e-02, 1.0297e-01, 4.5389e-08, 1.3262e-08,
        6.9562e-02, 3.4239e-02, 1.8799e-08, 1.7380e-01, 4.6067e-02, 6.5792e-02,
        9.4828e-02, 1.2780e-01, 1.7747e-01, 1.0438e-01, 2.3492e-08, 1.2779e-08,
        1.7304e-08, 3.1563e-06, 5.6526e-02, 5.3624e-02, 1.8231e-01, 1.7304e-08,
        6.8392e-03, 5.8393e-02, 1.2779e-08, 1.2273e-01, 2.4467e-02, 1.2779e-08,
        1.4465e-01, 4.3586e-02, 4.2127e-02, 1.3210e-02, 6.0729e-02, 1.2779e-08,
        1.2132e-01, 9.8481e-02, 1.2779e-08, 1.2779e-08, 1.7009e-01, 9.5311e-02,
        1.7304e-08, 6.2141e-02, 1.9777e-01, 1.7304e-08, 2.0263e-02, 1.7304e-08,
        3.4826e-02, 2.3493e-08, 3.4077e-02, 1.0990e-02, 1.8799e-08, 7.8961e-02,
        2.3492e-08, 8.0868e-02, 2.1221e-08, 1.0165e-01, 1.7697e-01, 4.2776e-08,
        1.8529e-01, 1.5770e-01, 9.6529e-02, 6.5866e-02, 9.5124e-02, 4.4954e-08,
        1.0644e-01, 1.8246e-01, 2.3652e-02, 5.7419e-02, 1.1404e-02, 8.1266e-02,
        2.5964e-02, 1.9340e-08, 6.5811e-03, 1.9256e-01, 1.1851e-08, 6.4697e-02,
        1.2779e-08, 1.2497e-01, 1.4465e-08, 2.6674e-02, 9.6085e-02, 2.0086e-01,
        1.6607e-01, 3.4187e-02, 2.1221e-08, 1.7304e-08, 8.2925e-02, 2.1221e-08,
        2.9778e-08, 8.2507e-02, 1.2779e-08, 9.5037e-02, 9.8640e-02, 9.8669e-02,
        7.3170e-02, 1.8579e-02, 1.2779e-08, 7.2461e-02, 7.2143e-02, 1.3819e-08,
        1.7304e-08, 1.2422e-01, 2.9959e-08, 1.2779e-08, 1.7729e-03, 1.2051e-01,
        1.2779e-08, 1.2779e-08, 7.7668e-02, 1.4208e-08, 1.2779e-08, 1.8799e-08,
        4.2664e-08, 1.3074e-02, 5.6148e-02, 1.7304e-08, 1.2779e-08, 8.1484e-02,
        1.8004e-02, 9.3645e-02, 7.5050e-02, 1.8799e-08, 2.1503e-01, 7.2642e-02,
        1.9340e-08, 1.2545e-02, 5.4572e-02, 2.2446e-02, 1.4465e-08, 2.3659e-08,
        9.2466e-02, 3.2244e-02, 3.8372e-02, 1.7304e-08, 1.3970e-02, 2.1742e-02,
        1.5863e-01, 1.5142e-01, 1.8799e-08, 1.9340e-08, 5.3926e-02, 2.5955e-02,
        1.3262e-08, 6.3049e-02, 1.2779e-08, 2.3159e-02, 1.5383e-02, 1.3408e-01,
        1.2779e-08, 1.8326e-01, 1.7304e-08, 1.3741e-01, 1.2779e-08, 1.5753e-01,
        1.6160e-01, 1.3429e-01, 9.5170e-02, 4.9739e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.5936e-07, 3.2194e-07, 7.2186e-02, 1.6151e-07, 2.5229e-02, 1.9970e-07,
        4.3411e-07, 1.2257e-07, 1.4074e-07, 2.2901e-08, 2.8301e-07, 1.3219e-02,
        7.2851e-08, 1.4917e-07, 2.0598e-07, 4.2367e-07, 7.2851e-08, 3.8642e-02,
        4.4069e-02, 8.1322e-08, 6.8024e-08, 6.0229e-08, 8.1411e-08, 3.7771e-02,
        6.8025e-08, 2.8301e-07, 1.0717e-02, 1.9970e-07, 1.6453e-07, 4.7838e-02,
        4.2241e-02, 4.2373e-07, 9.3034e-08, 8.1322e-08, 3.2194e-07, 6.8025e-08,
        6.8024e-08, 7.2404e-02, 5.0413e-02, 6.8025e-08, 5.3724e-08, 3.7786e-02,
        7.2851e-08, 2.0438e-02, 1.8855e-02, 2.9025e-02, 3.3077e-07, 2.1729e-07,
        2.9937e-02, 1.4917e-07, 6.4675e-08, 5.9247e-02, 3.7114e-07, 1.8643e-07,
        3.4610e-02, 6.7732e-08, 4.1498e-08, 4.3928e-02, 4.1498e-08, 6.4392e-02,
        1.7615e-01, 9.1285e-08, 1.3178e-07, 7.1645e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.4509e-02, 1.8910e-07, 1.2925e-07, 2.8268e-07, 5.3256e-02, 7.8264e-02,
        7.4843e-02, 1.1126e-07, 2.0816e-07, 3.7742e-07, 2.9756e-07, 6.2185e-02,
        1.3600e-07, 4.9531e-02, 1.2490e-07, 2.6129e-07, 2.8568e-02, 7.8910e-02,
        2.0816e-07, 3.4412e-07, 2.9756e-07, 6.9724e-02, 8.8133e-02, 8.8388e-08,
        2.0816e-07, 1.3600e-07, 7.6732e-02, 1.1126e-07, 3.7742e-07, 6.1188e-07,
        2.7209e-07, 1.1126e-07, 2.6129e-07, 1.3600e-07, 6.7662e-02, 3.0170e-07,
        1.1126e-07, 2.0816e-07, 8.8388e-08, 1.8910e-07, 2.2079e-07, 3.7742e-07,
        4.7123e-07, 3.2897e-02, 8.1499e-02, 2.5492e-02, 1.1126e-07, 4.5098e-07,
        6.0767e-07, 3.8012e-02, 3.4412e-07, 4.1580e-02, 7.5083e-02, 1.2925e-07,
        7.0215e-02, 1.1126e-07, 5.8922e-02, 2.0816e-07, 6.0724e-07, 6.0892e-07,
        9.0101e-02, 7.5543e-02, 4.5098e-07, 2.9756e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.1384e-02, 4.6319e-08, 6.7255e-02, 2.4273e-02, 2.3929e-08, 1.7687e-08,
        2.6757e-02, 8.3861e-03, 5.1411e-03, 4.4330e-08, 3.6929e-02, 3.9198e-02,
        1.0092e-08, 6.1753e-02, 1.9495e-08, 7.0951e-02, 1.4708e-02, 3.2324e-02,
        4.2795e-02, 4.6920e-08, 1.4890e-08, 1.1041e-02, 1.2973e-02, 3.4540e-08,
        5.0660e-08, 5.7981e-02, 1.5846e-02, 6.6860e-02, 5.7418e-02, 7.5757e-08,
        7.3760e-03, 6.6881e-03, 4.1782e-08, 5.0870e-08, 9.2490e-03, 2.9864e-02,
        2.6107e-03, 6.4647e-03, 3.7365e-08, 5.7512e-02, 1.3862e-02, 1.7462e-08,
        5.4747e-08, 5.1378e-03, 1.7637e-08, 9.7008e-03, 7.3736e-02, 6.3843e-08,
        2.6696e-02, 2.7529e-02, 1.0548e-02, 1.2365e-02, 1.4027e-02, 6.5063e-02,
        6.3839e-03, 2.6423e-02, 3.0268e-08, 2.6863e-03, 6.3591e-02, 2.8997e-02,
        8.0915e-03, 4.4176e-08, 4.4009e-02, 4.4173e-08, 2.3315e-08, 1.5265e-02,
        6.2040e-03, 3.0775e-08, 1.0836e-02, 1.0021e-02, 4.4173e-08, 6.7739e-08,
        3.7885e-02, 6.2206e-03, 1.4162e-02, 4.1422e-03, 1.6958e-02, 6.4582e-03,
        5.1534e-08, 5.0363e-02, 9.2337e-03, 1.6593e-02, 4.6709e-08, 3.8586e-02,
        1.3100e-02, 2.9723e-08, 8.4131e-03, 5.1291e-08, 4.6109e-08, 8.3096e-03,
        2.0381e-08, 5.1421e-08, 3.2475e-02, 2.6560e-08, 1.5713e-02, 5.8199e-02,
        1.0043e-01, 4.1606e-08, 1.3566e-02, 7.7010e-03, 1.0764e-07, 5.1421e-08,
        1.6583e-08, 2.5311e-08, 2.2211e-02, 8.3203e-03, 3.3836e-08, 5.4668e-08,
        1.3168e-02, 1.3841e-02, 2.0426e-02, 7.4251e-03, 2.8433e-08, 2.4674e-08,
        2.9001e-02, 2.3550e-02, 4.0400e-08, 1.2667e-02, 9.5063e-03, 2.9238e-08,
        2.4028e-08, 3.8517e-08, 5.0997e-02, 5.7840e-02, 3.1460e-02, 7.0547e-02,
        5.8576e-02, 4.9143e-02, 7.2074e-03, 2.2100e-02, 4.3933e-03, 5.3924e-02,
        9.8611e-03, 4.8820e-08, 4.2305e-02, 7.5728e-02, 1.2049e-02, 4.0855e-08,
        1.7008e-02, 5.6587e-03, 2.7434e-02, 1.7492e-08, 1.8655e-02, 4.8841e-08,
        4.5750e-08, 7.5757e-08, 6.4667e-08, 2.0146e-08, 4.0303e-02, 1.2714e-02,
        5.1421e-08, 2.3381e-08, 5.0376e-08, 3.3916e-08, 1.5211e-02, 1.5227e-02,
        1.0610e-02, 5.8029e-02, 3.3051e-02, 5.6313e-08, 6.0507e-03, 7.6793e-02,
        8.9498e-08, 2.4166e-02, 6.4618e-02, 1.4008e-02, 4.2850e-02, 7.9852e-03,
        3.7812e-08, 6.6475e-08, 5.3417e-02, 3.7433e-02, 1.9526e-08, 3.8324e-08,
        3.0775e-08, 5.8622e-08, 6.3968e-02, 6.6836e-02, 1.2364e-02, 2.1911e-02,
        1.4169e-02, 6.2454e-03, 2.7071e-02, 7.6194e-02, 1.3826e-02, 6.0028e-02,
        2.3315e-08, 2.7801e-08, 3.0775e-08, 8.6788e-02, 6.5640e-03, 5.0303e-08,
        2.9037e-08, 2.8062e-02, 4.3083e-02, 4.0476e-08, 1.0269e-02, 4.9351e-02,
        6.5390e-02, 7.4308e-03, 6.8986e-02, 6.9728e-02, 2.9819e-08, 2.2866e-02,
        4.6319e-08, 1.0764e-07, 3.7264e-08, 6.9261e-02, 6.4376e-02, 3.7344e-02,
        6.8197e-02, 4.0646e-02, 2.7070e-08, 1.0180e-02, 4.0821e-02, 9.1103e-03,
        4.3002e-08, 2.6905e-08, 6.6372e-08, 4.7403e-02, 2.5786e-02, 2.8639e-02,
        3.8917e-02, 6.4051e-08, 3.9817e-08, 1.3954e-02, 2.7239e-02, 7.0119e-09,
        2.9139e-02, 3.9166e-02, 4.4163e-08, 5.3869e-02, 4.4158e-08, 1.1636e-02,
        1.6504e-02, 1.3803e-02, 3.6582e-08, 4.6319e-08, 1.2400e-02, 1.4147e-02,
        7.6101e-02, 2.2709e-08, 2.8521e-08, 5.2394e-08, 1.8059e-02, 7.5123e-03,
        7.4119e-02, 4.4085e-02, 5.2530e-02, 4.0758e-02, 6.9748e-02, 1.1266e-02,
        1.0880e-02, 6.3422e-03, 4.7933e-08, 9.8808e-03], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.6825e-07, 7.2695e-08, 3.4359e-07, 2.1381e-07, 9.6623e-08, 3.4359e-07,
        2.7848e-07, 6.2036e-07, 5.9810e-07, 8.3116e-07, 4.7425e-02, 1.0288e-07,
        3.6094e-02, 1.5793e-07, 2.9541e-07, 2.6988e-07, 1.3581e-07, 1.3478e-07,
        1.2802e-07, 2.3670e-07, 2.2715e-07, 4.0295e-02, 1.1200e-01, 1.2452e-01,
        1.7136e-02, 6.5595e-08, 2.0681e-07, 4.0896e-02, 2.3390e-07, 1.0873e-07,
        1.4923e-07, 2.5324e-07, 2.3742e-02, 1.6540e-07, 3.1285e-07, 1.7009e-07,
        2.2715e-07, 3.7528e-07, 3.7126e-02, 6.4648e-08, 2.6603e-07, 4.6439e-07,
        2.3958e-07, 5.3911e-07, 2.8451e-07, 6.2036e-07, 2.9434e-07, 5.2275e-02,
        2.7215e-07, 3.5731e-02, 1.9875e-02, 2.2251e-07, 1.7248e-01, 3.5005e-07,
        4.1077e-07, 2.2934e-07, 2.9868e-07, 4.0205e-07, 1.4982e-07, 2.8310e-07,
        2.7845e-02, 3.6660e-02, 1.2801e-07, 2.7215e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.3159e-07, 6.2944e-07, 2.7459e-02, 9.6209e-07, 4.9477e-08, 9.1332e-07,
        9.7853e-07, 8.2322e-07, 1.6587e-07, 9.4306e-07, 8.7414e-07, 3.7881e-07,
        2.9056e-07, 2.9943e-02, 5.3316e-07, 4.6433e-07, 8.7414e-07, 1.9337e-06,
        3.7405e-04, 3.9349e-02, 4.6781e-07, 6.3159e-07, 2.8365e-02, 9.3623e-07,
        1.3362e-06, 1.3184e-06, 2.9221e-06, 1.9363e-06, 2.9687e-02, 4.6781e-07,
        2.9114e-07, 1.0487e-01, 9.7011e-02, 1.4873e-06, 4.7679e-07, 8.9918e-07,
        5.5805e-07, 1.0693e-06, 1.3818e-06, 4.5861e-07, 5.3225e-07, 8.6043e-07,
        1.3818e-06, 2.9056e-07, 4.7137e-07, 1.9337e-06, 1.1814e-01, 2.3826e-07,
        8.1987e-07, 2.8713e-08, 3.3867e-02, 3.4352e-07, 9.7127e-02, 5.8107e-07,
        5.7112e-07, 3.3938e-07, 2.2091e-02, 1.6571e-01, 6.8088e-07, 1.6300e-01,
        3.8896e-07, 3.2292e-02, 3.6980e-07, 9.1332e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.6326e-08, 9.0378e-03, 5.9274e-08, 4.6998e-08, 3.8659e-08, 5.6814e-08,
        5.4448e-08, 9.9074e-08, 4.9031e-02, 4.1642e-08, 7.1047e-08, 3.6775e-08,
        4.7108e-08, 2.6590e-02, 8.0265e-08, 3.1126e-08, 7.0399e-08, 9.6508e-03,
        1.0399e-07, 3.9372e-08, 7.9887e-03, 9.1748e-03, 8.7470e-03, 6.9820e-08,
        7.0438e-08, 6.9404e-08, 1.2682e-07, 7.7507e-08, 5.3368e-02, 1.2742e-01,
        3.2425e-02, 1.1632e-07, 4.2099e-08, 1.3649e-08, 9.0565e-08, 4.3766e-08,
        1.2598e-02, 6.2616e-08, 5.1089e-08, 1.0453e-07, 9.2622e-08, 9.3276e-08,
        9.3148e-08, 1.1479e-07, 3.6046e-08, 1.3250e-02, 8.5240e-08, 6.1805e-02,
        5.8632e-08, 3.4669e-02, 7.4530e-08, 5.8272e-08, 3.5215e-08, 7.0977e-08,
        8.3406e-08, 2.6297e-02, 9.5264e-08, 6.7571e-03, 6.5686e-08, 4.7952e-02,
        1.3331e-08, 5.7698e-08, 4.7084e-08, 2.0584e-08, 1.8850e-01, 9.4148e-03,
        6.4867e-02, 1.2362e-07, 9.7108e-03, 1.5311e-02, 4.4369e-08, 6.4257e-08,
        3.8720e-02, 4.8776e-08, 1.6901e-02, 4.4473e-08, 4.5954e-03, 7.2741e-08,
        3.3729e-08, 5.3430e-08, 1.2866e-01, 1.3366e-02, 2.7109e-02, 5.2178e-02,
        1.5772e-02, 3.1677e-08, 1.9632e-03, 1.1209e-08, 1.6061e-02, 7.4633e-02,
        1.2925e-07, 4.3449e-02, 5.4034e-02, 7.0719e-08, 2.8842e-08, 8.0830e-08,
        3.2668e-08, 7.3730e-02, 1.2739e-01, 4.5950e-08, 3.8333e-08, 1.2078e-07,
        6.1168e-08, 8.4283e-08, 2.8967e-08, 3.4717e-08, 2.2530e-08, 1.4521e-07,
        7.3534e-08, 5.4827e-03, 1.9889e-02, 1.0926e-07, 4.1082e-08, 8.2570e-03,
        3.9044e-08, 2.8508e-08, 6.6812e-08, 1.7356e-08, 4.4244e-08, 4.8800e-02,
        5.8555e-08, 3.0159e-08, 1.8239e-02, 5.7693e-08, 8.4004e-08, 5.8670e-02,
        8.7660e-08, 4.7480e-02, 4.1490e-08, 4.3850e-08, 8.6902e-08, 7.6773e-02,
        1.2541e-01, 4.5559e-02, 5.3565e-08, 2.7838e-02, 4.5392e-08, 5.7464e-08,
        8.0258e-08, 1.4899e-01, 2.5767e-02, 3.3100e-08, 1.5642e-06, 4.7108e-08,
        3.6534e-02, 1.6625e-08, 5.8338e-08, 1.1070e-02, 2.2412e-02, 1.6522e-03,
        3.1677e-08, 3.5617e-08, 5.9454e-08, 2.7303e-03, 1.7954e-02, 7.4768e-08,
        9.1792e-03, 2.0839e-08, 8.9589e-03, 4.1663e-08, 6.4942e-08, 7.2463e-08,
        6.2827e-08, 2.1118e-02, 1.2529e-07, 1.4130e-01, 5.1578e-08, 5.5486e-02,
        4.7104e-08, 3.3729e-08, 4.5254e-08, 3.3806e-08, 2.2496e-08, 8.4715e-08,
        2.0299e-02, 6.0103e-08, 6.9381e-08, 5.7943e-02, 3.0289e-08, 2.2467e-02,
        1.2748e-02, 2.9838e-02, 6.2282e-08, 7.4050e-08, 7.3858e-08, 6.6330e-08,
        2.9466e-08, 2.5467e-08, 3.3729e-08, 1.5118e-02, 6.1388e-08, 7.3599e-08,
        6.9227e-08, 1.0495e-07, 7.9485e-08, 3.3936e-02, 4.1406e-08, 9.8563e-08,
        3.5545e-08, 9.7743e-02, 5.0477e-08, 8.2379e-02, 4.8790e-08, 3.1728e-02,
        3.3729e-08, 2.9709e-08, 2.6924e-02, 4.8029e-08, 4.7380e-02, 4.4138e-08,
        9.7282e-08, 4.1856e-08, 4.6892e-08, 2.9538e-08, 1.7100e-02, 5.5260e-08,
        9.8072e-08, 2.0783e-02, 7.2326e-08, 6.1505e-02, 1.9766e-02, 3.7035e-08,
        2.4040e-08, 7.3869e-08, 3.5924e-08, 6.2875e-03, 6.1759e-08, 1.2112e-08,
        6.2878e-08, 8.9060e-08, 6.1910e-08, 6.6158e-08, 5.6145e-02, 2.2469e-08,
        2.0673e-02, 1.6938e-02, 2.9466e-08, 1.2493e-07, 1.2672e-01, 1.3145e-02,
        8.1442e-02, 4.4473e-08, 4.7108e-08, 9.0781e-08, 6.6938e-08, 2.2998e-08,
        5.1815e-02, 2.8318e-02, 7.5863e-08, 3.4108e-04, 7.5004e-02, 3.9661e-08,
        2.2065e-02, 8.1656e-08, 5.6229e-08, 1.0278e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.7611e-07, 3.3121e-07, 1.0223e-07, 1.9747e-07, 6.2589e-08, 1.1823e-02,
        1.9103e-07, 3.3121e-07, 1.0223e-07, 4.9886e-02, 4.8298e-07, 3.1260e-07,
        2.4212e-07, 2.8225e-07, 5.7354e-02, 3.7333e-02, 2.3045e-07, 2.3772e-07,
        6.6495e-02, 2.1062e-07, 3.7819e-02, 1.3153e-07, 3.7611e-07, 6.1191e-02,
        2.5172e-07, 6.7659e-03, 1.2511e-01, 1.3153e-07, 5.2653e-08, 6.1129e-02,
        1.6624e-07, 2.5172e-07, 1.1874e-02, 1.3197e-02, 6.4635e-02, 2.6340e-02,
        9.7193e-08, 1.5902e-07, 5.8190e-02, 5.4522e-03, 1.3153e-07, 5.7839e-02,
        5.9591e-02, 2.1354e-07, 3.3121e-07, 1.5151e-07, 4.9638e-08, 1.0223e-07,
        3.8698e-06, 3.5343e-07, 1.7446e-07, 2.1354e-07, 6.3696e-08, 9.9496e-02,
        1.0117e-07, 3.0355e-07, 8.8827e-08, 1.8657e-03, 6.7147e-02, 1.9032e-07,
        5.0026e-02, 2.7952e-07, 5.9316e-02, 6.6071e-02, 2.5172e-07, 7.8815e-08,
        5.6392e-08, 5.0344e-02, 6.0245e-02, 1.0117e-07, 9.4691e-08, 8.0004e-08,
        1.0513e-07, 6.8665e-02, 1.3771e-07, 1.0223e-07, 1.1592e-07, 3.1646e-07,
        2.2153e-07, 3.6656e-02, 4.6853e-07, 2.3198e-07, 1.9747e-07, 1.2119e-07,
        1.1592e-07, 9.0220e-08, 1.0513e-07, 1.3529e-01, 2.1354e-07, 3.0355e-07,
        2.2671e-07, 6.7909e-08, 1.6624e-07, 8.0732e-08, 1.8749e-07, 8.8827e-08,
        1.4779e-07, 4.5429e-02, 4.8233e-02, 1.3153e-07, 3.3620e-02, 2.2628e-07,
        1.3975e-07, 5.7475e-02, 6.5982e-02, 1.2119e-07, 1.9300e-07, 2.3919e-02,
        3.6581e-02, 1.0117e-07, 1.0117e-07, 5.9339e-02, 2.7746e-07, 5.6281e-02,
        4.8740e-07, 1.6624e-07, 1.1521e-07, 4.3552e-02, 6.3696e-08, 1.9032e-07,
        5.2653e-08, 2.1012e-07, 3.9007e-02, 1.3153e-07, 1.1496e-07, 4.3880e-02,
        3.3121e-07, 1.6488e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.8523e-06, 3.9269e-02, 4.1992e-07, 1.5509e-06, 4.3676e-07, 1.0041e-06,
        4.3514e-02, 3.7640e-02, 8.9676e-07, 2.1431e-06, 1.0014e-06, 1.1193e-06,
        4.3676e-07, 3.6405e-02, 8.6019e-07, 1.9131e-06, 9.3111e-02, 1.1563e-06,
        3.1330e-02, 3.8264e-02, 1.0790e-06, 3.7332e-02, 3.9165e-02, 4.2905e-02,
        2.1036e-06, 3.6461e-02, 1.3024e-06, 9.0370e-02, 2.0681e-06, 1.0254e-02,
        1.7015e-06, 9.5436e-07, 3.8397e-02, 4.3359e-02, 4.9387e-07, 1.1586e-01,
        3.8064e-02, 1.3019e-06, 3.6470e-02, 3.9233e-02, 2.9661e-02, 2.8033e-06,
        4.1279e-02, 3.9081e-07, 4.9221e-02, 3.9715e-02, 1.3819e-06, 7.6840e-07,
        4.2526e-02, 1.0265e-06, 2.2926e-06, 1.0790e-06, 2.0212e-06, 3.4912e-02,
        2.7874e-06, 1.3248e-02, 1.1318e-06, 1.1355e-06, 5.4984e-07, 1.0032e-01,
        9.0216e-07, 3.5557e-07, 3.6971e-02, 2.0213e-06, 3.2983e-07, 3.1825e-02,
        1.1785e-06, 4.0657e-02, 9.4709e-07, 3.7410e-02, 1.2756e-02, 4.9379e-02,
        2.2926e-06, 1.1168e-01, 6.0732e-07, 8.1197e-07, 3.5846e-02, 9.5436e-07,
        1.4219e-06, 3.1697e-02, 1.1049e-06, 1.5210e-06, 4.2798e-02, 7.3907e-07,
        3.3766e-02, 1.1355e-06, 2.8477e-07, 9.4657e-02, 3.2983e-07, 9.5479e-07,
        4.9387e-07, 1.3024e-06, 5.6930e-07, 1.6710e-07, 2.0066e-06, 4.6680e-07,
        4.1832e-02, 1.0814e-06, 9.0216e-07, 9.1182e-02, 4.1965e-02, 1.1563e-06,
        2.9951e-07, 1.4985e-06, 1.0041e-06, 6.9434e-02, 8.8650e-02, 3.9082e-07,
        4.0487e-02, 4.4078e-02, 1.6001e-07, 1.6932e-06, 4.4352e-02, 9.1687e-07,
        1.2184e-01, 2.0545e-06, 9.1687e-07, 4.1102e-02, 3.6201e-02, 8.2638e-07,
        1.1339e-06, 9.6687e-07, 3.6913e-02, 2.1036e-06, 3.3766e-02, 4.0593e-02,
        3.6518e-02, 1.1132e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.6338e-02, 5.7853e-02, 1.0034e-07, 2.2551e-07, 1.0832e-02, 1.0654e-07,
        2.3073e-02, 9.2505e-08, 4.2032e-03, 6.8969e-08, 4.7296e-08, 2.2551e-07,
        7.8724e-02, 4.7296e-08, 8.8013e-08, 1.4327e-07, 1.0396e-02, 2.0844e-01,
        6.2420e-08, 6.0242e-08, 7.3432e-03, 9.9792e-08, 9.6366e-02, 1.2259e-07,
        1.2320e-02, 2.2551e-07, 1.1811e-02, 9.7314e-08, 4.8735e-02, 1.4326e-02,
        3.8840e-08, 1.8008e-02, 1.1242e-02, 4.1993e-08, 1.1158e-07, 8.8013e-08,
        1.0296e-01, 8.7163e-03, 1.0563e-02, 7.3334e-08, 1.1368e-01, 2.7782e-08,
        7.7715e-03, 3.6981e-02, 1.1974e-02, 2.2936e-07, 6.0241e-08, 1.2548e-07,
        3.6583e-08, 4.0781e-08, 4.0781e-08, 1.6098e-07, 1.8203e-02, 1.4925e-02,
        6.4133e-02, 9.5010e-02, 1.3840e-07, 7.6605e-08, 1.3789e-02, 1.0822e-02,
        6.5746e-08, 5.6987e-08, 1.0226e-07, 7.6055e-02, 7.6605e-08, 8.9974e-02,
        1.0265e-02, 1.2259e-07, 8.7804e-03, 7.2342e-08, 1.6098e-07, 6.2420e-08,
        2.7747e-02, 5.6718e-08, 6.8158e-08, 5.1841e-08, 1.0226e-07, 1.6989e-02,
        8.8013e-08, 9.7256e-03, 4.0145e-08, 6.5839e-02, 2.6796e-08, 2.2551e-07,
        3.8840e-08, 1.0226e-07, 2.8977e-08, 1.6966e-01, 9.8015e-02, 6.9082e-02,
        1.3840e-07, 7.3087e-03, 7.1984e-02, 2.0553e-02, 5.6718e-08, 4.0781e-08,
        2.0610e-02, 4.7296e-08, 4.7394e-08, 1.1616e-02, 1.6060e-01, 5.3620e-02,
        1.3772e-02, 4.2561e-08, 1.1960e-02, 7.5513e-02, 2.2488e-02, 2.0328e-02,
        9.2505e-08, 4.2919e-08, 5.6718e-08, 1.2890e-07, 1.7596e-01, 1.2890e-07,
        2.1259e-02, 8.8630e-08, 1.0774e-07, 7.6148e-08, 1.1954e-02, 5.4520e-08,
        1.6510e-02, 1.0411e-02, 1.0961e-02, 2.1119e-02, 1.0606e-02, 1.2917e-02,
        9.2505e-08, 1.4327e-07, 4.7296e-08, 2.5090e-02, 1.2852e-02, 1.2890e-07,
        1.0744e-02, 9.6762e-02, 5.0171e-08, 8.7577e-02, 1.3840e-07, 4.2919e-08,
        1.1086e-02, 4.2919e-08, 1.1525e-07, 1.2936e-02, 1.3287e-07, 2.8977e-08,
        5.1841e-08, 2.2863e-03, 1.3287e-07, 6.8969e-08, 1.3840e-07, 2.9919e-02,
        7.4869e-03, 1.5582e-02, 6.4081e-02, 1.6880e-02, 6.3004e-03, 8.8675e-03,
        6.0266e-08, 3.4922e-08, 8.9227e-02, 2.4732e-02, 9.6820e-02, 2.4351e-02,
        7.8553e-02, 1.4327e-07, 8.8013e-08, 1.0226e-07, 5.4767e-02, 8.6683e-08,
        1.0034e-07, 2.5461e-02, 7.1829e-02, 4.2880e-02, 4.3992e-08, 1.3840e-07,
        4.0781e-08, 3.0813e-02, 2.3978e-02, 8.8290e-03, 6.4103e-02, 1.1194e-07,
        9.2505e-08, 1.2548e-07, 2.6796e-08, 1.3374e-02, 1.2641e-03, 4.0781e-08,
        1.1755e-02, 9.6966e-08, 1.4327e-07, 3.8840e-08, 5.0171e-08, 1.0862e-01,
        1.2380e-02, 9.3234e-08, 8.6683e-08, 1.2548e-07, 7.4032e-02, 4.4405e-02,
        5.0171e-08, 9.0745e-03, 4.8515e-02, 1.2548e-07, 1.0034e-07, 9.2505e-08,
        5.0171e-08, 5.0171e-08, 2.5082e-02, 5.5290e-02, 1.5123e-02, 5.0171e-08,
        1.4186e-07, 1.1777e-02, 1.3671e-02, 9.1650e-03, 3.2181e-02, 2.9958e-02,
        3.0644e-02, 4.9262e-08, 1.6323e-02, 7.6084e-08, 4.0781e-08, 1.2338e-07,
        9.2505e-08, 4.5640e-02, 8.8013e-08, 1.3840e-07, 3.4760e-02, 1.2548e-07,
        4.9843e-02, 1.0271e-07, 2.8391e-08, 3.2849e-02, 3.1274e-08, 1.4007e-02,
        1.2840e-01, 1.3287e-07, 9.7756e-02, 1.5673e-02, 1.4327e-07, 1.3282e-02,
        3.5404e-02, 1.2871e-02, 1.5351e-02, 1.5662e-07, 1.4327e-07, 1.2259e-07,
        9.2505e-08, 1.0034e-07, 7.8298e-02, 4.3992e-08, 6.2420e-08, 1.6936e-02,
        1.9547e-07, 5.6718e-08, 5.4840e-08, 1.9547e-07, 1.5316e-02, 1.0861e-02,
        8.4849e-02, 1.0518e-01, 2.9544e-02, 9.2505e-08, 3.1656e-08, 2.2551e-07,
        1.1533e-02, 4.4530e-02, 5.0967e-08, 2.2551e-07, 2.0612e-02, 3.6583e-08,
        1.2890e-07, 4.2189e-08, 1.4327e-07, 3.5652e-08, 2.8773e-02, 1.0774e-07,
        1.2259e-07, 2.0216e-07, 4.7296e-08, 6.8690e-08, 2.0868e-02, 6.6290e-08,
        2.4289e-02, 4.5557e-02, 1.7174e-02, 1.3287e-07, 7.7371e-02, 3.9083e-02,
        6.2954e-02, 1.1729e-02, 7.6605e-08, 7.0024e-03, 3.8463e-02, 6.8690e-08,
        6.7775e-02, 1.3287e-07, 2.4276e-02, 1.9889e-02, 1.7632e-02, 2.4580e-02,
        4.2919e-08, 1.6492e-02, 1.3880e-01, 8.8013e-08, 6.6479e-02, 2.9260e-02,
        7.4063e-02, 1.2259e-07, 7.3527e-02, 7.0581e-03, 2.0173e-07, 5.8112e-03,
        3.2388e-02, 1.0549e-07, 1.4327e-07, 2.2551e-07, 6.0266e-08, 4.9102e-02,
        2.1845e-02, 2.2551e-07, 1.2985e-02, 2.2551e-07, 1.0924e-02, 1.0932e-02,
        8.1314e-02, 1.4112e-01, 1.1224e-02, 3.8840e-08, 5.8294e-08, 9.2505e-08,
        6.6340e-02, 1.0654e-07, 1.5784e-01, 2.2551e-07, 6.6290e-08, 5.1841e-08,
        3.2831e-02, 8.8013e-08, 1.3405e-02, 8.3873e-02, 2.5269e-02, 1.4434e-08,
        5.2601e-08, 9.4349e-08, 1.9274e-01, 1.4830e-07, 6.0242e-08, 1.0034e-07,
        1.1361e-02, 5.3607e-02, 4.0801e-03, 4.0781e-08, 1.4327e-07, 2.6895e-02,
        6.4408e-08, 4.0638e-08, 8.1207e-02, 5.0171e-08, 1.0654e-07, 1.1444e-01,
        5.3354e-03, 6.2783e-08, 1.4327e-07, 3.3313e-08, 8.8013e-08, 1.2689e-02,
        1.3093e-02, 4.0781e-08, 5.6718e-08, 1.4193e-07, 1.7636e-08, 8.0352e-03,
        3.8826e-02, 1.5940e-02, 5.6718e-08, 6.0241e-08, 7.6976e-02, 3.6583e-08,
        1.0719e-02, 9.6406e-08, 8.5209e-08, 8.5180e-03, 1.4790e-02, 8.8540e-08,
        9.3142e-03, 2.5047e-02, 8.8013e-08, 4.1889e-02, 4.0638e-08, 1.0034e-07,
        8.8013e-08, 8.8013e-08, 1.4327e-07, 1.0383e-02, 9.3224e-02, 2.9613e-02,
        4.5562e-02, 3.0390e-02, 7.6605e-08, 3.0084e-02, 5.6718e-08, 1.5189e-02,
        6.0266e-08, 1.1376e-07, 1.0238e-01, 8.8910e-08, 3.0751e-02, 8.8013e-08,
        7.6582e-02, 1.0034e-07, 1.0507e-02, 1.0034e-07, 7.7604e-02, 1.3840e-07,
        4.9438e-02, 1.0772e-02, 9.3234e-08, 2.2551e-07, 1.6939e-02, 7.6605e-08,
        8.5812e-02, 3.8888e-02, 2.2551e-07, 4.3780e-02, 1.4327e-07, 8.8013e-08,
        1.2760e-02, 1.6158e-01, 6.6290e-08, 2.2427e-09, 1.2240e-07, 1.0774e-07,
        5.6718e-08, 1.2890e-07, 1.4261e-02, 8.8013e-08, 7.7269e-08, 1.2259e-07,
        1.0736e-02, 9.6187e-03, 6.1656e-08, 2.6796e-08, 4.2189e-08, 1.0529e-07,
        6.6290e-08, 4.8612e-02, 9.8937e-02, 1.4276e-02, 7.7269e-08, 1.0226e-07,
        6.2200e-08, 8.3974e-03, 1.0307e-02, 9.2505e-08, 1.4314e-01, 6.8054e-03,
        5.6987e-08, 1.1335e-02, 6.8158e-08, 1.4255e-02, 1.1158e-07, 1.0840e-02,
        1.4327e-07, 2.6796e-08, 6.2573e-02, 1.8486e-02, 9.7071e-02, 1.0183e-07,
        9.6238e-03, 6.6198e-03, 1.6339e-02, 1.2618e-02, 3.8770e-02, 1.3840e-07,
        2.0173e-07, 8.8013e-08, 4.0781e-08, 1.6294e-02, 1.3347e-02, 4.0618e-02,
        1.7896e-02, 3.0291e-02, 1.0266e-02, 3.6583e-08, 1.6534e-02, 7.6605e-08,
        6.8969e-08, 1.5738e-01, 1.3237e-02, 1.6792e-07, 1.4327e-07, 1.3633e-02,
        4.8717e-08, 3.6565e-02, 6.2420e-08, 1.3374e-02, 1.3287e-02, 4.0781e-08,
        6.2420e-08, 5.0171e-08, 1.2993e-02, 3.1656e-08, 1.5082e-02, 2.2569e-02,
        4.2919e-08, 3.1656e-08, 9.4600e-03, 8.8630e-08, 5.0171e-08, 1.0034e-07,
        1.4862e-02, 1.2823e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.9378e-02, 8.4770e-03, 1.9784e-07, 1.2715e-07, 4.5041e-02, 6.1480e-08,
        2.4267e-02, 1.2715e-07, 2.7777e-02, 4.0816e-08, 1.7476e-07, 9.4299e-09,
        5.5878e-02, 5.3715e-08, 2.0618e-07, 1.5722e-07, 2.5845e-02, 3.9313e-03,
        1.8268e-07, 6.1480e-08, 7.2354e-02, 3.8315e-03, 5.0212e-03, 1.2715e-07,
        3.6337e-02, 1.1932e-07, 8.4653e-02, 1.4919e-02, 2.1311e-03, 9.9499e-02,
        8.9311e-08, 4.0153e-02, 4.4089e-02, 1.2715e-07, 5.3715e-08, 2.8751e-07,
        8.8918e-03, 2.5519e-02, 3.1212e-02, 1.2715e-07, 2.9585e-03, 1.8268e-07,
        2.5373e-02, 8.9879e-02, 5.1114e-02, 1.8268e-07, 6.1480e-08, 1.2585e-07,
        1.7476e-07, 1.8268e-07, 8.9193e-08, 7.1511e-08, 7.0264e-02, 7.0941e-02,
        7.6657e-03, 6.2983e-03, 2.8751e-07, 2.1679e-07, 6.6799e-02, 5.5488e-02,
        1.8268e-07, 6.1480e-08, 8.1166e-08, 2.7639e-03, 1.2715e-07, 5.2649e-03,
        3.0744e-02, 1.8268e-07, 2.7715e-02, 1.5722e-07, 2.9783e-07, 1.4329e-07,
        6.1860e-02, 1.4329e-07, 3.8876e-07, 4.0427e-08, 5.3715e-08, 7.0352e-02,
        4.2637e-08, 1.0114e-01, 1.5220e-07, 4.6579e-03, 5.3715e-08, 4.8877e-08,
        5.3715e-08, 2.1332e-07, 1.2715e-07, 1.1446e-02, 5.6941e-03, 6.1227e-03,
        7.1511e-08, 2.8925e-02, 5.6370e-03, 8.3016e-02, 9.2864e-08, 1.4329e-07,
        8.8135e-02, 5.3715e-08, 5.3715e-08, 5.2710e-02, 7.0035e-03, 2.7906e-03,
        8.5063e-02, 1.5722e-07, 5.0986e-02, 6.6090e-03, 1.1215e-01, 1.0927e-01,
        1.1932e-07, 1.5220e-07, 5.3715e-08, 7.1511e-08, 1.1491e-02, 4.2637e-08,
        1.3025e-01, 2.9903e-07, 4.0427e-08, 7.1511e-08, 6.1390e-02, 2.2850e-02,
        7.1387e-02, 9.5485e-02, 7.8363e-02, 4.3463e-02, 6.5261e-02, 6.0758e-02,
        5.3715e-08, 5.3715e-08, 5.3715e-08, 8.6753e-02, 4.7696e-02, 9.5577e-08,
        3.6144e-02, 5.0261e-03, 5.3715e-08, 1.0006e-02, 2.1679e-07, 4.8877e-08,
        2.0064e-02, 5.3715e-08, 1.1741e-02, 7.1961e-02, 7.9445e-08, 8.0307e-08,
        2.3967e-08, 2.7617e-02, 2.8337e-07, 7.1511e-08, 3.2002e-07, 4.5736e-02,
        3.0174e-02, 5.0784e-02, 5.8144e-03, 5.6432e-02, 1.8282e-02, 4.7451e-02,
        4.8877e-08, 5.3715e-08, 5.4388e-03, 1.3115e-07, 5.0255e-03, 6.7992e-02,
        3.6828e-02, 2.8751e-07, 8.4219e-08, 2.9511e-07, 3.6123e-02, 7.1511e-08,
        2.2721e-07, 2.4616e-02, 5.0159e-02, 4.3810e-02, 1.1635e-07, 5.3715e-08,
        8.9311e-08, 6.3291e-02, 6.9485e-02, 5.3969e-02, 2.0188e-03, 4.0427e-08,
        2.1679e-07, 2.7504e-07, 4.8877e-08, 6.0993e-02, 5.3715e-08, 3.7007e-07,
        7.1377e-02, 2.3797e-07, 4.8877e-08, 4.8877e-08, 5.3715e-08, 1.0897e-02,
        1.1302e-01, 4.0816e-08, 1.5220e-07, 2.1332e-07, 4.9600e-03, 5.9879e-02,
        1.4329e-07, 7.2062e-02, 5.7381e-03, 1.4619e-07, 5.3715e-08, 1.0038e-07,
        7.1511e-08, 1.8268e-07, 3.5819e-02, 4.2892e-03, 8.0232e-02, 1.4329e-07,
        4.2637e-08, 7.5396e-02, 1.0152e-01, 5.4409e-02, 5.6022e-02, 3.4364e-02,
        5.2354e-02, 7.1511e-08, 5.1472e-02, 5.3715e-08, 2.8751e-07, 1.4619e-07,
        2.7504e-07, 7.5755e-02, 6.1480e-08, 1.5220e-07, 2.9270e-02, 1.4329e-07,
        5.0184e-03, 6.5319e-08, 7.4504e-03, 4.7037e-02, 2.1679e-07, 6.5327e-02,
        1.2518e-02, 1.2715e-07, 9.6810e-03, 1.8755e-02, 1.8268e-07, 8.8326e-02,
        1.2624e-01, 8.3610e-02, 1.1724e-01, 4.8877e-08, 1.5220e-07, 1.9784e-07,
        1.4329e-07, 4.8877e-08, 4.8831e-03, 4.2637e-08, 1.0038e-07, 1.0654e-01,
        4.0816e-08, 4.0426e-08, 3.8043e-07, 5.3715e-08, 6.5607e-02, 5.7998e-02,
        8.1633e-03, 1.6781e-07, 9.4924e-03, 1.2715e-07, 6.1480e-08, 5.3715e-08,
        5.6554e-02, 3.9724e-02, 2.4314e-02, 2.1679e-07, 5.9103e-02, 2.2721e-07,
        6.5319e-08, 2.9154e-07, 5.3715e-08, 6.1480e-08, 2.6716e-02, 2.9783e-07,
        6.5319e-08, 5.3715e-08, 6.5319e-08, 1.5220e-07, 6.2467e-02, 7.1511e-08,
        1.1283e-01, 8.1347e-02, 1.0632e-01, 1.4329e-07, 1.0962e-02, 3.3394e-02,
        4.5436e-03, 4.6234e-02, 7.1511e-08, 2.9094e-02, 1.8176e-07, 7.3497e-08,
        5.5824e-03, 4.8877e-08, 8.4476e-02, 7.6766e-02, 4.9812e-02, 2.5850e-02,
        6.1480e-08, 6.0070e-02, 1.1786e-02, 1.6631e-07, 4.0708e-03, 2.6208e-02,
        3.4690e-03, 7.3497e-08, 4.1025e-03, 6.6872e-02, 1.8268e-07, 4.9405e-02,
        5.0452e-02, 2.1332e-07, 2.1679e-07, 2.0112e-07, 1.5220e-07, 2.8712e-03,
        2.7922e-02, 5.3715e-08, 6.8112e-02, 1.5386e-07, 5.9895e-02, 2.4063e-02,
        1.1647e-02, 7.4502e-03, 7.5757e-02, 1.4329e-07, 6.1480e-08, 2.9519e-07,
        8.4919e-02, 5.3715e-08, 8.9906e-03, 7.1511e-08, 6.1480e-08, 4.8877e-08,
        2.9217e-02, 5.3715e-08, 6.9876e-02, 5.5521e-03, 3.1843e-02, 2.1679e-07,
        1.8268e-07, 2.1332e-07, 7.0470e-03, 5.3715e-08, 6.1480e-08, 2.9436e-07,
        4.8556e-02, 2.1514e-03, 2.6242e-02, 1.2715e-07, 1.9784e-07, 2.9975e-02,
        6.1480e-08, 5.3715e-08, 3.5066e-03, 1.8268e-07, 6.1480e-08, 9.7417e-03,
        3.6825e-02, 1.0524e-02, 7.1511e-08, 5.4702e-03, 5.3715e-08, 6.3370e-02,
        3.3800e-02, 2.1679e-07, 7.3497e-08, 2.4329e-02, 2.1679e-07, 4.1825e-02,
        3.3632e-03, 4.1774e-02, 5.3715e-08, 1.1932e-07, 6.1478e-03, 3.8876e-07,
        6.3235e-02, 8.9311e-08, 4.0427e-08, 4.9317e-02, 9.0813e-02, 2.1332e-07,
        4.5285e-02, 5.2705e-02, 2.9783e-07, 2.2742e-07, 2.9928e-08, 2.1679e-07,
        5.3715e-08, 3.8050e-07, 5.3715e-08, 6.1717e-02, 2.7520e-02, 3.3000e-02,
        4.6097e-03, 6.3561e-02, 1.1932e-07, 1.8479e-02, 2.9783e-07, 6.4683e-02,
        2.1332e-07, 6.5319e-08, 5.4592e-03, 7.2051e-03, 3.7477e-02, 4.8877e-08,
        1.2014e-02, 2.1332e-07, 4.7099e-02, 1.2715e-07, 6.7745e-03, 1.4619e-07,
        3.0281e-02, 8.5551e-02, 6.1480e-08, 1.8268e-07, 6.3006e-02, 2.1679e-07,
        7.5487e-03, 5.3511e-02, 1.1932e-07, 3.0633e-02, 1.8268e-07, 4.8877e-08,
        7.0454e-02, 7.0395e-03, 2.8751e-07, 3.8876e-07, 1.9808e-02, 1.8317e-08,
        8.1166e-08, 2.1679e-07, 7.3469e-02, 7.1511e-08, 2.1332e-07, 9.2864e-08,
        6.0553e-02, 2.7430e-02, 6.1480e-08, 5.3715e-08, 5.3715e-08, 1.4619e-07,
        7.3497e-08, 2.5926e-02, 5.3370e-03, 6.4505e-02, 2.9500e-07, 2.8751e-07,
        2.9783e-07, 3.4434e-02, 5.6627e-02, 6.1480e-08, 6.6760e-03, 4.2885e-02,
        4.8877e-08, 4.2297e-02, 1.0038e-07, 5.2456e-02, 7.3497e-08, 5.3700e-02,
        3.1833e-08, 3.2002e-07, 1.7169e-02, 5.5394e-02, 7.2948e-03, 1.4619e-07,
        4.3901e-02, 3.0216e-02, 8.3037e-02, 6.4872e-02, 7.4933e-02, 1.4329e-07,
        1.8268e-07, 4.8877e-08, 2.1679e-07, 8.1117e-02, 4.5145e-02, 5.2369e-02,
        2.3485e-02, 4.3591e-02, 8.2493e-02, 7.7948e-09, 9.6164e-02, 7.1511e-08,
        1.1801e-07, 7.3807e-03, 5.2255e-02, 6.5319e-08, 4.2637e-08, 5.7767e-02,
        6.1480e-08, 6.3116e-02, 1.1932e-07, 5.0381e-02, 8.6242e-02, 1.2585e-07,
        2.8751e-07, 4.0816e-08, 6.5236e-02, 4.0816e-08, 8.3806e-02, 6.3764e-02,
        2.1679e-07, 1.8268e-07, 5.0210e-02, 2.3797e-07, 1.9784e-07, 5.3715e-08,
        8.1762e-02, 4.7829e-03], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.5528e-02, 3.1859e-07, 3.1859e-07, 1.2582e-07, 2.3045e-02, 8.9771e-08,
        2.9291e-02, 8.9771e-08, 2.4768e-07, 8.7737e-08, 1.3656e-07, 2.6348e-07,
        2.1266e-07, 4.1473e-02, 7.0452e-08, 4.7223e-07, 4.5678e-07, 3.8934e-07,
        2.4768e-07, 8.6890e-02, 3.8180e-02, 8.9771e-08, 3.6398e-02, 8.7738e-08,
        4.3447e-07, 2.1266e-07, 8.8586e-07, 4.5678e-07, 1.4965e-07, 6.2445e-02,
        4.0576e-02, 3.8934e-07, 3.4125e-07, 2.4768e-07, 3.0254e-07, 1.6083e-07,
        1.4158e-07, 2.1266e-07, 4.8869e-02, 8.8603e-07, 5.4240e-02, 2.8311e-02,
        4.3449e-07, 5.5425e-07, 7.5590e-02, 2.1266e-07, 2.9799e-07, 1.2582e-07,
        5.5361e-02, 8.8786e-02, 2.1673e-07, 4.4921e-02, 2.6942e-02, 1.6083e-07,
        5.4244e-02, 1.6923e-07, 7.9851e-02, 2.3969e-02, 4.4130e-02, 1.9008e-08,
        6.2912e-02, 5.9917e-02, 5.8747e-02, 2.4819e-02, 4.3447e-07, 6.9998e-02,
        3.4125e-07, 4.9451e-02, 2.1266e-07, 5.8028e-08, 4.5676e-07, 1.7078e-07,
        6.9685e-07, 3.1859e-07, 1.0413e-01, 3.2125e-07, 8.1711e-02, 1.4158e-07,
        8.9771e-08, 5.0043e-02, 6.5761e-03, 3.8836e-02, 7.9284e-02, 1.9525e-07,
        4.3667e-02, 3.1859e-07, 2.2077e-07, 5.6359e-02, 4.7049e-07, 5.1886e-02,
        3.8934e-07, 1.6923e-07, 6.9193e-02, 1.2293e-07, 1.5194e-07, 1.2582e-07,
        5.0625e-02, 5.0737e-02, 6.9685e-07, 2.6348e-07, 8.9771e-08, 3.4924e-02,
        2.6348e-07, 3.2848e-07, 2.4768e-07, 2.7455e-02, 6.9685e-07, 2.1266e-07,
        3.8934e-07, 3.4409e-07, 1.6923e-07, 4.2931e-02, 4.9144e-02, 1.5194e-07,
        6.7169e-02, 6.8703e-07, 4.9827e-02, 6.9229e-02, 8.9771e-08, 1.5194e-07,
        7.6043e-02, 6.8699e-07, 8.8602e-07, 3.3570e-02, 8.9771e-08, 8.9771e-08,
        4.5954e-02, 1.3656e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.1199e-02, 5.0883e-02, 8.3136e-02, 1.6059e-01, 1.2647e-06, 5.2660e-03,
        1.2705e-01, 1.1697e-06, 4.3596e-03, 1.6834e-07, 9.0907e-08, 2.3853e-07,
        5.2667e-07, 9.2193e-02, 1.1494e-01, 8.5226e-02, 2.2872e-07, 1.5151e-07,
        2.4698e-07, 1.3038e-06, 1.0378e-01, 1.1697e-06, 9.5292e-02, 3.9526e-07,
        1.1697e-06, 1.1790e-01, 2.5403e-02, 1.6834e-07, 3.0070e-07, 1.6834e-07,
        5.2667e-07, 5.2667e-07, 2.2872e-07, 1.5506e-01, 3.3690e-07, 1.1218e-01,
        1.1073e-01, 4.0597e-03, 1.0343e-01, 1.1560e-01, 5.5368e-03, 7.1558e-07,
        1.0691e-01, 4.4455e-03, 5.1249e-03, 1.0065e-01, 1.2647e-06, 5.1498e-03,
        2.3817e-07, 3.9852e-02, 5.2667e-07, 1.4117e-01, 2.1178e-07, 6.4021e-07,
        3.5810e-07, 1.0467e-01, 5.2667e-07, 5.9974e-02, 8.4253e-02, 5.2294e-03,
        7.1558e-07, 5.6741e-07, 7.1558e-07, 1.6834e-07, 1.0544e-01, 1.2939e-01,
        1.0016e-01, 2.8515e-07, 2.1178e-07, 5.9710e-07, 2.2872e-07, 1.2969e-01,
        5.6741e-07, 2.1178e-07, 1.0917e-01, 2.1178e-07, 2.4443e-07, 4.6974e-03,
        7.4134e-02, 2.1178e-07, 1.2248e-01, 2.1178e-07, 1.0112e-01, 1.4893e-01,
        7.1558e-07, 1.0960e-01, 1.5003e-01, 1.6834e-07, 8.1023e-02, 9.5149e-02,
        5.2667e-07, 3.2128e-07, 1.2647e-06, 6.7786e-02, 5.2667e-07, 6.4771e-03,
        7.1558e-07, 2.1178e-07, 2.1178e-07, 1.2010e-01, 3.2128e-07, 4.1800e-07,
        1.6834e-07, 4.6819e-07, 2.3817e-07, 4.0873e-07, 5.7278e-07, 5.8352e-02,
        1.2986e-01, 1.5099e-01, 1.0032e-01, 1.1143e-01, 4.1801e-07, 1.3966e-01,
        3.0071e-07, 4.2493e-03, 3.1867e-07, 5.1233e-07, 5.6725e-07, 4.6819e-07,
        1.2550e-01, 6.8074e-03, 6.6649e-07, 9.5811e-03, 1.4940e-06, 1.1916e-06,
        1.0731e-01, 3.3690e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.2014e-03, 1.1208e-02, 3.1808e-03, 3.1341e-02, 5.5421e-02, 6.0839e-08,
        3.2768e-02, 5.5423e-02, 2.1328e-02, 1.3314e-07, 9.4310e-08, 3.3792e-08,
        5.8963e-02, 9.4310e-08, 1.0535e-07, 5.9778e-08, 4.8055e-02, 4.1511e-03,
        4.8497e-02, 3.7778e-08, 5.4133e-02, 2.8878e-08, 7.3745e-03, 3.2098e-08,
        7.1098e-02, 6.0839e-08, 1.9744e-02, 1.7438e-02, 7.7349e-02, 5.4959e-02,
        7.0345e-02, 2.5231e-02, 7.6066e-02, 2.7224e-03, 2.1783e-01, 9.0879e-03,
        1.3617e-02, 5.4136e-02, 6.7372e-02, 4.8651e-08, 7.8397e-02, 2.0156e-01,
        3.4591e-02, 1.0148e-02, 5.8743e-02, 8.4226e-02, 5.5368e-02, 2.9892e-03,
        3.5571e-03, 2.8523e-07, 4.9304e-08, 1.2582e-07, 6.3580e-03, 6.1171e-02,
        1.3431e-02, 1.0373e-02, 3.8946e-02, 1.2086e-07, 2.7162e-02, 5.9998e-02,
        1.5289e-07, 5.5252e-02, 7.6734e-02, 5.2436e-03, 3.7253e-03, 9.0930e-03,
        2.2532e-02, 5.3257e-08, 5.6746e-08, 7.6554e-02, 2.1766e-07, 4.4816e-08,
        4.0233e-02, 2.5449e-07, 5.5618e-08, 2.6984e-03, 1.2295e-01, 1.3947e-02,
        2.6310e-02, 4.7275e-02, 5.5618e-08, 1.4430e-01, 1.4572e-07, 9.0810e-02,
        8.4616e-02, 3.8785e-02, 4.7773e-02, 1.4359e-02, 4.1756e-02, 1.5534e-02,
        1.0546e-01, 1.1705e-01, 9.9068e-03, 9.4566e-03, 5.3257e-08, 8.4516e-02,
        3.3873e-02, 6.0839e-08, 7.7271e-02, 3.1732e-02, 1.2653e-02, 5.1230e-03,
        1.8134e-02, 6.7386e-02, 7.2461e-02, 1.0671e-01, 1.1425e-02, 4.9104e-02,
        1.4381e-07, 3.3718e-03, 2.9420e-03, 1.1134e-07, 1.2929e-02, 1.4159e-07,
        3.7452e-02, 8.5711e-03, 1.5289e-07, 4.4966e-08, 4.5650e-03, 2.0991e-02,
        7.6442e-03, 3.3418e-02, 6.7264e-02, 6.7797e-02, 2.0692e-03, 2.5653e-02,
        7.6013e-08, 8.4890e-08, 1.5289e-07, 7.1673e-03, 4.0941e-02, 5.4504e-02,
        8.4162e-02, 8.3656e-03, 2.4920e-01, 1.1135e-02, 1.0328e-07, 1.4120e-07,
        8.3685e-02, 2.1877e-07, 1.0286e-02, 8.2213e-03, 1.5667e-01, 9.5555e-02,
        5.1814e-02, 2.0198e-08, 1.3314e-07, 8.5028e-08, 9.4310e-08, 2.9447e-02,
        1.5306e-01, 9.0514e-02, 7.9240e-03, 4.8422e-03, 9.6607e-03, 2.8939e-02,
        2.9603e-03, 4.4966e-08, 7.4185e-03, 1.0450e-07, 9.0137e-03, 6.0607e-02,
        5.5320e-02, 6.0839e-08, 4.4998e-08, 4.3534e-02, 3.3069e-03, 5.1816e-08,
        5.2022e-08, 6.6560e-02, 7.5222e-03, 4.0879e-02, 2.8332e-03, 1.3135e-01,
        6.3832e-02, 8.6824e-03, 2.3655e-02, 4.0610e-02, 8.3413e-03, 1.0328e-07,
        1.6617e-01, 3.1203e-08, 6.3166e-08, 9.1076e-02, 6.1761e-02, 4.5425e-08,
        6.7543e-02, 5.3732e-08, 3.2686e-03, 3.5590e-02, 6.6355e-02, 1.6334e-02,
        3.6994e-02, 1.4159e-07, 8.5028e-08, 5.2382e-02, 1.1786e-07, 9.0039e-03,
        2.0531e-07, 5.0263e-02, 5.4199e-03, 4.8470e-02, 3.8008e-02, 6.9098e-08,
        3.0137e-03, 5.5189e-02, 1.5865e-01, 5.3657e-03, 5.5151e-02, 4.4275e-02,
        3.1943e-03, 5.6393e-02, 6.6589e-03, 5.4954e-02, 4.3938e-02, 6.0595e-02,
        3.0217e-03, 4.8431e-08, 7.1990e-02, 4.8776e-08, 3.4238e-02, 3.0650e-03,
        3.3054e-03, 9.5059e-03, 6.0839e-08, 5.1816e-08, 5.1575e-02, 1.4159e-07,
        1.0981e-02, 3.1711e-08, 2.8918e-08, 1.6365e-01, 2.8965e-08, 1.3098e-01,
        1.6407e-02, 7.9543e-02, 1.0298e-02, 9.1617e-08, 2.1920e-07, 5.1958e-02,
        1.0617e-02, 7.4608e-03, 1.5937e-02, 5.3606e-02, 7.2268e-02, 8.2794e-02,
        5.2571e-02, 9.0253e-03, 3.3211e-02, 6.7130e-02, 4.8431e-08, 2.8935e-02,
        4.5425e-08, 1.3751e-01, 4.4998e-08, 2.8541e-03, 1.6607e-01, 3.1662e-02,
        1.0759e-02, 7.5169e-08, 5.3403e-02, 9.4310e-08, 6.0839e-08, 6.8557e-02,
        1.6481e-01, 5.9847e-02, 2.0219e-08, 9.4310e-08, 7.1062e-08, 3.4242e-03,
        5.3770e-02, 2.0857e-02, 5.9393e-02, 1.0055e-01, 1.4619e-07, 6.0839e-08,
        6.9633e-02, 6.0839e-08, 5.3257e-08, 2.9018e-03, 7.1176e-02, 2.7926e-03,
        1.0244e-02, 7.3388e-02, 4.3641e-02, 4.2362e-02, 1.9076e-02, 7.5619e-02,
        6.7349e-03, 7.6821e-03, 4.7857e-02, 5.0590e-02, 1.1357e-07, 2.8702e-03,
        7.4005e-03, 4.8431e-08, 1.1045e-02, 1.9259e-02, 7.6190e-02, 5.4003e-02,
        1.1791e-07, 8.1528e-02, 1.7187e-02, 1.4111e-01, 6.2912e-03, 1.7594e-02,
        4.9621e-03, 1.3494e-01, 6.5591e-03, 1.2828e-02, 7.5482e-08, 5.4941e-02,
        6.2536e-02, 1.3207e-01, 3.0192e-02, 1.0486e-07, 3.2098e-08, 8.3751e-02,
        6.7992e-02, 1.1134e-07, 5.8007e-02, 7.6013e-08, 4.8496e-02, 1.1127e-02,
        1.5294e-02, 2.0211e-02, 5.3787e-02, 1.1797e-01, 2.8103e-03, 3.9888e-02,
        7.3992e-02, 1.0328e-07, 1.1134e-02, 6.0640e-02, 4.9989e-02, 1.1134e-07,
        5.2166e-02, 1.9891e-09, 7.4501e-02, 6.2526e-03, 1.2695e-02, 4.0541e-08,
        1.0657e-07, 1.6181e-07, 1.1633e-02, 4.4998e-08, 2.8545e-07, 7.6013e-08,
        3.6666e-03, 3.2006e-08, 2.8658e-03, 2.1908e-02, 3.7725e-02, 2.2877e-02,
        3.2846e-03, 1.7801e-08, 4.3941e-03, 1.1153e-01, 1.3152e-07, 1.2503e-02,
        3.1491e-08, 3.9326e-08, 6.7455e-02, 7.0729e-08, 6.7744e-03, 6.7014e-02,
        9.7000e-02, 3.1130e-08, 4.0270e-02, 9.7045e-04, 7.7388e-02, 6.4824e-02,
        9.2503e-02, 6.0233e-02, 8.5028e-08, 2.0146e-02, 9.5437e-03, 7.6013e-08,
        1.2878e-01, 3.2870e-03, 4.1023e-08, 5.2556e-02, 7.8310e-02, 8.4890e-08,
        3.1664e-02, 6.8163e-03, 1.4105e-01, 1.5955e-07, 5.9339e-02, 1.7635e-07,
        6.0839e-08, 1.1365e-02, 6.0839e-08, 5.9573e-02, 5.2675e-02, 1.6026e-02,
        8.3284e-02, 1.3177e-02, 3.3337e-03, 1.5361e-07, 1.4572e-07, 6.9394e-02,
        1.0404e-07, 5.9045e-08, 7.8557e-03, 3.7937e-08, 1.7547e-02, 8.6320e-08,
        1.8174e-02, 8.4890e-08, 1.4233e-03, 1.4977e-07, 3.7110e-02, 2.1877e-07,
        1.5935e-01, 3.8465e-02, 1.2613e-01, 7.2612e-02, 6.5070e-02, 1.8975e-07,
        9.1100e-03, 5.6369e-02, 1.4159e-07, 3.9824e-02, 3.1510e-03, 1.7097e-01,
        1.3598e-01, 1.0633e-02, 1.5599e-01, 8.6320e-08, 1.7718e-07, 7.4419e-02,
        1.5569e-01, 6.3000e-02, 1.2220e-02, 2.8513e-07, 1.3355e-01, 5.3095e-08,
        5.6954e-02, 4.9661e-02, 3.3164e-03, 7.3213e-08, 6.0839e-08, 4.2496e-08,
        3.8568e-08, 1.9814e-01, 9.2188e-03, 3.3923e-02, 4.8431e-08, 4.0541e-08,
        6.5892e-08, 6.3595e-02, 3.7866e-02, 1.1009e-07, 8.6437e-03, 5.1995e-02,
        1.5289e-07, 4.3355e-08, 4.0869e-08, 8.0174e-02, 2.8486e-03, 5.5324e-02,
        8.8633e-02, 5.7188e-02, 2.0607e-08, 4.5446e-02, 8.8798e-03, 2.1766e-07,
        7.7152e-08, 3.6104e-02, 3.2105e-02, 7.9299e-03, 1.0446e-02, 1.1644e-08,
        1.1134e-07, 6.8269e-03, 6.6332e-08, 6.1407e-02, 7.7468e-03, 1.1314e-07,
        2.7022e-02, 5.8136e-02, 7.5597e-02, 2.1877e-07, 8.8411e-02, 3.4999e-02,
        3.8780e-02, 9.6462e-03, 1.0039e-01, 1.0731e-07, 7.6972e-08, 6.7957e-02,
        1.0404e-07, 3.5083e-02, 5.5618e-08, 4.9754e-03, 5.8825e-02, 5.0598e-08,
        1.8361e-01, 6.9098e-08, 5.7484e-02, 1.0328e-07, 5.6952e-03, 4.0176e-02,
        4.8709e-03, 7.5897e-02, 5.5495e-02, 5.4221e-02, 3.1787e-03, 3.1835e-02,
        3.5742e-02, 8.0456e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.6964e-07, 1.1315e-01, 8.2055e-07, 4.4277e-07, 2.3311e-03, 2.8759e-07,
        1.3580e-01, 4.7346e-02, 2.4138e-03, 5.5228e-02, 3.7953e-02, 2.4865e-03,
        3.4788e-02, 4.0209e-07, 3.8412e-02, 2.1676e-01, 6.5033e-07, 2.5428e-03,
        3.9384e-07, 2.4586e-03, 3.7035e-07, 7.0428e-07, 3.7968e-02, 4.4725e-07,
        4.3812e-02, 4.0324e-02, 3.6804e-02, 2.8759e-07, 4.4725e-07, 4.4834e-02,
        4.7894e-07, 6.0180e-07, 3.7725e-02, 1.2898e-01, 9.6147e-07, 4.9461e-07,
        3.2139e-07, 3.3301e-07, 2.0271e-03, 6.4208e-07, 2.0424e-03, 1.6467e-01,
        3.5531e-02, 4.6808e-07, 2.1463e-07, 1.9984e-01, 1.0360e-06, 5.7359e-02,
        3.5784e-02, 2.7283e-03, 3.3622e-07, 2.0888e-03, 2.9924e-07, 8.1500e-07,
        5.4663e-07, 1.4482e-07, 4.4940e-07, 2.5030e-02, 5.4294e-02, 3.7160e-02,
        3.6794e-02, 1.5134e-07, 3.3622e-07, 4.6431e-07, 5.5575e-07, 3.4772e-02,
        1.0834e-06, 1.3262e-02, 2.1655e-01, 1.2127e-01, 4.6460e-02, 1.2055e-01,
        1.2596e-01, 1.2708e-01, 1.9042e-02, 5.2073e-03, 3.6301e-07, 3.2813e-03,
        3.6107e-02, 3.8943e-02, 4.7632e-07, 3.2289e-02, 2.6711e-03, 4.0441e-02,
        8.8364e-07, 4.4725e-07, 9.5317e-07, 2.1993e-01, 4.0981e-07, 1.7765e-07,
        4.4987e-02, 1.1709e-01, 4.0769e-02, 7.9747e-07, 4.5774e-07, 3.7460e-02,
        4.5552e-02, 2.1340e-01, 2.7504e-07, 4.1263e-02, 4.0467e-07, 1.5134e-07,
        4.3117e-02, 1.0996e-02, 4.4804e-02, 3.4195e-02, 3.7752e-02, 2.8218e-07,
        2.6606e-07, 2.3170e-07, 5.9405e-03, 3.0695e-02, 3.2363e-02, 4.4940e-07,
        6.1174e-07, 2.0948e-07, 1.2123e-07, 1.1851e-06, 4.2801e-02, 3.7609e-07,
        3.6802e-02, 2.1767e-07, 4.7444e-02, 5.0496e-02, 8.8955e-02, 5.0359e-02,
        1.7056e-01, 2.7209e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.4292e-06, 8.7896e-07, 4.1806e-07, 1.8046e-01, 2.7140e-06, 1.4554e-01,
        2.5670e-03, 9.0572e-02, 9.0179e-02, 1.9711e-06, 9.8324e-02, 1.2597e-06,
        1.0683e-01, 1.0401e-01, 3.2613e-06, 1.5430e-06, 5.6776e-07, 2.0089e-06,
        8.3503e-02, 1.7078e-06, 2.0587e-03, 1.4287e-06, 2.0356e-03, 8.4708e-02,
        2.0136e-06, 2.3379e-06, 2.5393e-03, 9.7575e-02, 1.9711e-06, 8.0878e-07,
        1.1553e-01, 3.2861e-02, 7.6824e-02, 1.5705e-06, 1.2858e-06, 1.1306e-06,
        1.1311e-01, 7.7607e-07, 1.3532e-01, 4.2063e-07, 1.1431e-01, 3.3819e-06,
        9.4915e-02, 4.6111e-07, 4.3686e-06, 1.0344e-06, 3.5605e-02, 1.3946e-06,
        5.9350e-07, 1.5884e-06, 6.9768e-07, 1.3718e-06, 1.9315e-06, 1.1854e-01,
        2.4661e-02, 1.2189e-01, 9.1119e-07, 2.3523e-02, 1.1222e-01, 1.0393e-01,
        4.6465e-07, 2.0163e-06, 2.3382e-06, 1.6500e-06, 7.9171e-02, 9.5270e-07,
        1.4799e-06, 9.8755e-07, 1.1219e-06, 1.0273e-01, 2.2664e-03, 9.0222e-08,
        3.7735e-06, 1.8414e-06, 8.7550e-07, 2.4245e-06, 6.0075e-07, 1.0856e-06,
        9.4192e-07, 1.6498e-06, 2.7928e-06, 1.0548e-06, 1.7403e-02, 1.1742e-06,
        1.0435e-01, 1.6248e-06, 9.0222e-08, 6.6905e-07, 4.3434e-07, 2.4665e-07,
        9.0373e-02, 1.0717e-06, 1.1561e-06, 8.8587e-07, 2.5378e-06, 1.7524e-06,
        9.3286e-07, 7.1781e-07, 7.2405e-02, 1.2148e-01, 1.3581e-01, 2.9539e-08,
        5.8816e-02, 1.0764e-01, 1.5705e-06, 7.6104e-02, 2.3733e-03, 7.6789e-02,
        3.1643e-03, 1.0379e-01, 2.5657e-03, 1.0798e-01, 6.0075e-07, 9.4436e-02,
        1.3817e-06, 1.6137e-06, 3.7503e-03, 1.9843e-03, 2.2169e-06, 7.1354e-07,
        5.6651e-07, 8.7432e-02, 1.0364e-06, 2.3241e-06, 1.6976e-06, 8.7959e-02,
        1.9368e-06, 2.4110e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2098e-07, 2.1700e-02, 1.8447e-07, 4.5139e-08, 5.3416e-02, 6.3349e-08,
        5.7138e-02, 1.1654e-07, 3.1056e-08, 2.3319e-07, 1.1787e-07, 3.3430e-08,
        5.9247e-02, 1.9955e-02, 7.2461e-03, 1.8435e-07, 3.5911e-08, 1.4984e-07,
        1.2180e-02, 1.5668e-01, 2.1411e-02, 1.5975e-07, 8.5663e-03, 9.7410e-02,
        1.3427e-07, 2.2113e-07, 1.1280e-02, 2.4205e-02, 4.6968e-02, 3.3370e-02,
        4.9630e-02, 4.6165e-03, 1.7656e-02, 1.7755e-07, 3.5941e-02, 1.4494e-01,
        1.0509e-02, 6.1452e-02, 7.1253e-03, 2.5588e-01, 4.2448e-02, 2.1033e-02,
        1.4882e-01, 1.1237e-02, 8.8893e-03, 8.6418e-02, 5.5229e-02, 2.0820e-07,
        2.5420e-07, 1.6915e-07, 3.0971e-02, 1.6915e-07, 7.5576e-03, 4.7818e-03,
        1.2545e-02, 3.5094e-03, 1.4748e-07, 6.6076e-08, 9.9887e-03, 3.0804e-02,
        1.7628e-07, 3.6833e-08, 1.5271e-02, 7.7289e-02, 3.1997e-07, 2.1297e-01,
        2.0755e-01, 3.7462e-02, 4.3730e-02, 2.8709e-02, 2.8308e-08, 1.4487e-01,
        5.5778e-08, 1.5004e-01, 4.4232e-02, 1.5006e-07, 3.3857e-02, 1.1578e-02,
        1.8930e-02, 1.9849e-02, 2.8232e-02, 5.0003e-02, 8.4283e-08, 8.8155e-03,
        5.2927e-02, 6.3339e-08, 9.7697e-08, 1.3571e-02, 8.2392e-03, 1.5061e-01,
        1.0418e-02, 6.4174e-03, 9.7487e-03, 9.3664e-03, 5.4859e-02, 7.2448e-03,
        9.6663e-03, 3.8028e-08, 1.3118e-01, 3.8837e-02, 8.4235e-03, 1.6852e-07,
        2.0198e-02, 4.5110e-03, 3.8547e-02, 1.4272e-01, 8.6559e-03, 4.8745e-02,
        2.9228e-02, 1.7518e-07, 1.6991e-07, 1.1184e-07, 9.1863e-03, 6.2769e-02,
        4.3329e-02, 7.7432e-02, 7.6087e-08, 3.7378e-08, 1.5378e-02, 4.1008e-08,
        5.7410e-03, 9.2335e-03, 1.7076e-02, 3.3166e-02, 1.9315e-02, 2.9106e-03,
        4.2468e-08, 1.8536e-01, 2.2497e-08, 6.1998e-03, 4.4889e-08, 6.5213e-02,
        3.9377e-02, 6.7750e-02, 3.8499e-02, 8.7734e-03, 1.0910e-07, 6.1450e-03,
        8.0829e-03, 2.9797e-07, 8.8377e-08, 8.2596e-03, 2.2598e-02, 9.3907e-03,
        5.8700e-02, 2.0770e-07, 1.1184e-07, 1.6430e-02, 5.2875e-06, 2.2588e-01,
        1.8173e-01, 4.3469e-02, 6.8148e-03, 6.0243e-08, 9.1814e-02, 1.4701e-02,
        1.6290e-07, 1.4431e-07, 5.6014e-03, 4.2281e-08, 6.9758e-02, 2.4878e-02,
        3.6375e-02, 1.4600e-07, 9.8198e-08, 8.9845e-08, 5.6992e-02, 1.3034e-07,
        1.0879e-02, 3.2230e-02, 7.9187e-03, 4.6641e-02, 1.7377e-07, 2.0911e-02,
        2.2574e-02, 8.5233e-03, 3.1183e-02, 2.4582e-02, 4.6980e-03, 2.1273e-01,
        2.9341e-02, 3.8132e-08, 1.6915e-07, 2.3621e-02, 8.6557e-02, 2.0225e-01,
        1.7972e-02, 1.5025e-07, 2.4696e-07, 3.6166e-02, 9.7597e-03, 2.7327e-02,
        4.8399e-02, 4.7907e-08, 7.9368e-08, 3.5744e-02, 1.0485e-07, 7.4384e-03,
        2.8845e-02, 7.4358e-03, 2.2490e-02, 4.8974e-02, 6.4801e-02, 3.6884e-02,
        1.3498e-07, 4.1771e-02, 3.3926e-02, 6.2880e-03, 4.9706e-02, 9.4428e-03,
        2.1077e-07, 7.0982e-03, 8.6511e-03, 9.7056e-03, 6.9027e-02, 3.7877e-02,
        3.2991e-03, 1.5934e-07, 3.0376e-02, 1.4231e-01, 4.7007e-02, 1.7833e-07,
        1.4412e-07, 1.5240e-07, 1.5025e-07, 1.8863e-01, 1.3850e-02, 1.4225e-01,
        6.6830e-02, 3.3132e-02, 7.9774e-08, 2.6595e-02, 7.9657e-08, 7.3545e-03,
        1.2141e-02, 3.1228e-02, 7.7752e-03, 5.1088e-03, 1.2662e-07, 1.5293e-02,
        1.2053e-02, 1.0106e-02, 1.0970e-02, 3.9646e-02, 6.5990e-03, 6.0108e-03,
        4.8473e-08, 1.6898e-01, 7.7091e-02, 4.5594e-02, 2.3103e-07, 1.7036e-02,
        5.6751e-08, 1.3829e-02, 1.1895e-01, 1.6310e-07, 6.6833e-03, 3.8842e-02,
        1.0338e-01, 2.0655e-02, 1.4096e-01, 5.5407e-08, 5.5407e-08, 1.4457e-01,
        4.5622e-02, 1.8793e-01, 1.0344e-07, 5.0088e-02, 6.1238e-02, 1.6983e-07,
        4.9897e-02, 5.6996e-08, 7.5791e-08, 1.5091e-02, 9.2349e-08, 5.0250e-02,
        8.6582e-03, 1.2664e-07, 1.8693e-01, 1.4836e-07, 8.0294e-03, 1.2994e-07,
        9.5369e-03, 5.4879e-02, 1.2460e-02, 1.6147e-07, 1.6174e-02, 3.2484e-02,
        7.4537e-02, 6.6034e-03, 5.2542e-02, 3.8791e-02, 7.3581e-02, 1.4123e-07,
        3.6324e-02, 1.1603e-07, 1.2716e-02, 9.2616e-08, 1.5094e-02, 9.8832e-08,
        1.5025e-07, 6.7428e-03, 1.0693e-02, 4.9170e-03, 2.9432e-03, 1.3437e-02,
        3.7793e-08, 6.0532e-02, 3.3193e-03, 4.8637e-03, 9.2197e-04, 1.5572e-07,
        7.5835e-02, 5.4076e-02, 7.2933e-02, 3.1804e-02, 1.3085e-01, 2.4559e-07,
        2.1633e-02, 2.0666e-01, 2.6335e-02, 7.6087e-08, 3.0207e-02, 1.6155e-01,
        1.1395e-02, 2.9289e-02, 2.8375e-02, 6.5917e-02, 2.0449e-07, 2.3181e-02,
        1.2626e-01, 1.4955e-01, 9.8693e-03, 7.1729e-03, 5.9999e-03, 9.8198e-08,
        9.3255e-02, 3.4167e-02, 6.7156e-03, 2.3607e-02, 3.1306e-02, 1.4228e-01,
        7.4334e-02, 1.9762e-07, 1.0151e-02, 1.9499e-01, 6.9107e-02, 1.5621e-07,
        2.9161e-08, 4.0906e-02, 3.7740e-02, 3.5015e-02, 7.0035e-02, 2.1499e-02,
        1.8944e-07, 8.2279e-08, 4.1808e-02, 1.8499e-02, 5.0575e-04, 1.1421e-02,
        8.4062e-08, 3.6897e-08, 3.0827e-02, 1.4431e-07, 7.7030e-02, 1.3390e-02,
        9.2138e-03, 4.5142e-02, 6.3662e-08, 9.2381e-08, 3.9087e-02, 4.0439e-02,
        4.6588e-02, 2.0960e-02, 4.8840e-08, 7.1660e-03, 7.4775e-03, 1.5934e-07,
        2.8440e-02, 1.4976e-06, 1.8920e-03, 3.1648e-02, 4.4335e-02, 1.3074e-01,
        1.8037e-03, 1.0680e-07, 1.0869e-02, 4.5497e-02, 1.7487e-07, 7.8610e-08,
        1.5934e-07, 1.4603e-02, 3.3467e-07, 1.6108e-07, 3.9006e-02, 5.6268e-02,
        8.6951e-03, 9.5652e-03, 2.2359e-07, 3.0412e-02, 4.8953e-03, 5.4871e-02,
        2.1383e-01, 2.3096e-07, 3.6819e-02, 6.3347e-08, 2.3334e-07, 5.3110e-02,
        1.6364e-02, 1.2670e-07, 2.3957e-02, 5.9695e-02, 6.4587e-02, 9.1697e-08,
        3.9161e-02, 5.0339e-02, 3.9846e-02, 3.9200e-02, 4.9269e-02, 1.8435e-07,
        6.9419e-03, 5.7429e-02, 7.9368e-08, 1.8939e-01, 1.4137e-07, 3.4190e-02,
        2.6382e-02, 8.3864e-03, 1.1202e-02, 7.6087e-08, 7.2711e-08, 3.1652e-03,
        3.0607e-02, 1.5648e-02, 1.0356e-02, 6.3349e-08, 9.1175e-03, 6.3479e-02,
        4.1154e-02, 1.3290e-01, 2.2001e-07, 1.2682e-02, 6.7218e-02, 7.5041e-02,
        1.0493e-03, 9.2474e-03, 7.2533e-03, 6.8802e-03, 6.6652e-08, 1.7628e-07,
        7.6081e-08, 2.4387e-02, 5.0750e-02, 6.6076e-08, 1.1166e-02, 1.9146e-02,
        5.0367e-08, 6.6115e-08, 2.4683e-02, 3.7863e-02, 1.5108e-07, 2.3139e-02,
        6.2348e-03, 2.2767e-02, 5.4618e-02, 4.7100e-02, 6.0551e-02, 5.7196e-08,
        4.6195e-08, 4.8755e-02, 7.7216e-03, 1.7143e-07, 1.3313e-02, 4.7257e-02,
        4.2468e-08, 1.0516e-07, 3.9405e-02, 6.8655e-03, 4.8427e-02, 9.0967e-08,
        6.1306e-02, 3.4031e-02, 4.4895e-02, 2.0244e-03, 4.3521e-02, 1.6854e-01,
        1.5976e-01, 9.3475e-03, 2.5042e-02, 3.7378e-08, 1.5766e-01, 2.7569e-02,
        5.3099e-02, 2.1220e-02, 4.6591e-02, 1.0878e-07, 1.4555e-02, 4.3770e-02,
        7.6434e-03, 6.5666e-02, 3.9839e-02, 2.3307e-07, 8.5517e-03, 3.7366e-02,
        3.1506e-02, 3.5528e-02, 2.4782e-02, 2.9330e-08, 1.7289e-07, 2.0752e-07,
        1.0287e-02, 4.7857e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2745e-01, 6.6350e-07, 8.4198e-07, 2.7443e-02, 2.9358e-02, 6.7830e-07,
        4.8117e-07, 2.9399e-07, 5.6496e-07, 1.2009e-01, 5.4413e-02, 2.7083e-07,
        1.0947e-03, 3.2067e-07, 4.7599e-02, 5.3803e-08, 3.2297e-02, 7.9970e-07,
        2.2456e-07, 4.5780e-07, 5.0562e-07, 4.4992e-08, 4.6160e-02, 4.6353e-07,
        1.1459e-03, 1.0315e-03, 3.0512e-07, 1.1806e-03, 6.2585e-07, 1.1682e-06,
        1.0962e-01, 4.3648e-02, 3.9763e-07, 9.3821e-07, 4.2037e-02, 2.3356e-02,
        1.8817e-07, 5.1567e-07, 7.8364e-07, 8.5047e-02, 5.0475e-07, 6.6762e-07,
        1.1682e-06, 3.1991e-07, 1.0398e-03, 3.8347e-03, 4.2226e-07, 5.1282e-07,
        5.1567e-07, 3.4202e-07, 4.9277e-07, 6.2087e-07, 6.7830e-07, 1.8165e-02,
        6.8000e-07, 3.1991e-07, 1.2465e-01, 5.5085e-07, 1.2091e-06, 3.5760e-02,
        4.2544e-07, 4.8227e-07, 1.5490e-07, 6.6756e-07, 4.8227e-07, 4.9277e-07,
        9.6896e-02, 1.1381e-02, 4.8227e-07, 4.6352e-07, 4.2420e-02, 1.7502e-06,
        6.6692e-07, 2.7194e-03, 1.2850e-03, 4.8117e-07, 5.0475e-07, 7.4359e-07,
        6.2570e-07, 4.2544e-07, 1.1182e-03, 7.6765e-07, 8.9319e-07, 8.6983e-07,
        3.9509e-02, 8.9319e-07, 5.3803e-08, 1.1381e-03, 3.8358e-07, 7.6881e-07,
        1.3351e-07, 2.8186e-02, 5.2181e-02, 4.0230e-07, 3.0659e-07, 2.9399e-07,
        6.5681e-07, 9.1684e-02, 3.5211e-02, 2.2171e-02, 5.1567e-07, 7.1471e-07,
        1.1406e-01, 4.1493e-07, 3.6932e-07, 4.4660e-07, 1.0794e-06, 4.3127e-07,
        5.1681e-07, 8.2607e-07, 5.8075e-07, 6.9141e-07, 4.6937e-07, 8.8524e-07,
        1.4691e-03, 7.9030e-07, 9.3821e-07, 5.8075e-07, 1.2720e-03, 1.0004e-02,
        2.5178e-07, 1.2091e-06, 9.5524e-04, 4.8586e-03, 5.1567e-07, 4.4688e-07,
        1.2592e-06, 5.8075e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.2629e-07, 9.8246e-02, 1.1318e-06, 1.4462e-06, 1.5729e-06, 6.2294e-07,
        1.1917e-06, 1.1917e-06, 6.2629e-07, 1.2087e-06, 1.0419e-06, 8.6978e-07,
        7.2272e-07, 1.7516e-06, 6.9110e-07, 5.5231e-05, 6.4278e-07, 2.5353e-06,
        2.3276e-06, 2.7582e-06, 7.1379e-07, 1.2103e-06, 1.2126e-01, 1.2854e-01,
        6.9302e-07, 1.0229e-06, 1.8691e-06, 4.2935e-02, 3.0506e-06, 1.1638e-06,
        5.8009e-07, 1.0693e-06, 9.8562e-07, 1.6841e-06, 6.7840e-07, 1.2103e-06,
        1.0848e-06, 6.3894e-07, 1.1634e-06, 2.9371e-06, 1.6010e-06, 1.2103e-06,
        8.6977e-07, 1.5307e-06, 8.6478e-02, 1.2103e-06, 1.6010e-06, 1.7584e-06,
        4.1188e-07, 2.9539e-06, 5.5125e-07, 9.8141e-02, 1.4174e-06, 7.7276e-07,
        2.0628e-06, 1.6941e-06, 5.5125e-07, 8.6977e-07, 1.0224e-06, 1.6010e-06,
        4.5529e-07, 9.6340e-07, 1.5252e-06, 1.1605e-01, 1.1638e-06, 1.8959e-06,
        1.8522e-06, 1.1199e-06, 1.1917e-06, 2.9044e-06, 2.1714e-06, 1.2978e-06,
        1.0695e-01, 2.2754e-06, 1.7899e-06, 8.0045e-07, 1.1959e-06, 1.0176e-06,
        9.8708e-02, 2.2041e-06, 5.5125e-07, 1.0957e-06, 1.1950e-06, 2.8579e-06,
        1.1355e-06, 1.1119e-06, 1.0040e-06, 1.0757e-06, 1.8670e-06, 5.1425e-07,
        1.1965e-06, 3.3622e-07, 1.6015e-06, 1.6941e-06, 1.0525e-06, 3.1404e-06,
        1.1711e-06, 1.5932e-06, 1.1917e-06, 4.9412e-07, 6.9302e-07, 1.8622e-06,
        2.1986e-06, 1.4878e-06, 2.1098e-06, 5.9584e-07, 1.6412e-06, 9.8770e-07,
        1.2955e-06, 2.3659e-06, 2.4681e-06, 3.7048e-02, 2.1466e-06, 6.2629e-07,
        1.9502e-06, 3.3622e-07, 2.6054e-06, 5.7061e-07, 1.0517e-01, 1.3480e-06,
        2.9818e-02, 1.0848e-06, 4.2056e-07, 1.2103e-06, 2.3659e-06, 2.7489e-06,
        1.0345e-06, 1.0419e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.5564e-07, 8.2073e-02, 1.4670e-07, 6.3688e-08, 1.3402e-02, 1.3996e-07,
        1.0858e-07, 8.8215e-08, 1.3575e-07, 5.5300e-08, 2.2333e-07, 2.2597e-01,
        4.4598e-08, 1.4373e-07, 2.0244e-01, 1.6653e-07, 5.6272e-02, 1.1076e-07,
        3.7797e-02, 5.2165e-03, 1.2476e-07, 6.3800e-08, 2.4503e-07, 5.8355e-08,
        4.1284e-08, 2.1929e-07, 1.3250e-07, 1.4377e-07, 1.2729e-07, 8.5326e-08,
        1.7071e-07, 7.7325e-08, 1.1622e-07, 1.2931e-07, 1.6045e-07, 7.6705e-08,
        9.7067e-03, 8.1672e-08, 3.5645e-08, 6.6320e-03, 1.8560e-07, 1.4536e-07,
        1.0003e-07, 9.8997e-03, 7.7226e-08, 2.7276e-02, 5.1134e-02, 6.7382e-09,
        1.5601e-07, 9.5916e-08, 5.2237e-02, 7.6745e-08, 1.7434e-07, 1.5125e-02,
        9.2520e-03, 1.1400e-07, 3.2912e-08, 1.6984e-08, 9.9509e-08, 1.0132e-07,
        1.6307e-07, 1.0644e-07, 1.1705e-07, 8.7898e-08, 9.5625e-08, 1.9409e-07,
        1.1208e-07, 1.0002e-07, 1.4335e-07, 1.2028e-07, 1.2497e-07, 2.0529e-02,
        1.2517e-07, 9.3066e-08, 9.1313e-08, 1.3996e-07, 3.6647e-02, 4.5274e-08,
        1.1046e-02, 9.8694e-08, 1.1454e-07, 1.8309e-07, 7.8912e-03, 2.7130e-07,
        5.1377e-08, 5.0209e-02, 7.2849e-08, 1.0808e-02, 4.9215e-03, 6.1888e-03,
        5.7835e-08, 7.1028e-08, 4.8245e-03, 1.0406e-07, 5.5270e-02, 2.0565e-07,
        1.7009e-07, 2.1048e-01, 1.0929e-07, 6.5664e-08, 9.2601e-03, 4.2423e-08,
        9.9104e-08, 4.7730e-08, 7.3191e-08, 8.5451e-08, 4.1049e-03, 1.0299e-07,
        9.0522e-08, 4.5955e-08, 1.0810e-07, 2.3175e-07, 8.8342e-03, 4.0625e-08,
        1.4497e-07, 1.6411e-07, 7.0005e-08, 8.1638e-08, 6.1751e-08, 1.5177e-07,
        1.4252e-07, 1.9765e-04, 1.4073e-07, 1.2143e-07, 1.1040e-07, 1.7941e-07,
        8.5522e-08, 1.1149e-07, 2.2366e-07, 1.0357e-07, 1.0451e-07, 1.2845e-07,
        5.0006e-08, 1.2541e-07, 1.0537e-07, 7.9974e-03, 7.6669e-08, 7.0329e-08,
        1.1998e-07, 1.3996e-07, 2.9115e-08, 1.6667e-07, 1.6502e-01, 1.0036e-07,
        8.8666e-08, 1.9400e-07, 2.3175e-07, 1.5160e-07, 1.0623e-07, 1.2621e-07,
        8.2951e-08, 1.3604e-07, 3.0296e-08, 1.0348e-07, 1.1084e-07, 6.3505e-08,
        2.1664e-07, 5.9659e-02, 9.1377e-08, 5.4166e-08, 5.6716e-08, 4.0190e-03,
        1.3326e-07, 6.3688e-08, 1.4630e-01, 3.5437e-02, 1.4215e-07, 1.3583e-07,
        1.0124e-07, 1.2414e-07, 1.0389e-07, 4.1874e-08, 2.1929e-07, 1.0469e-02,
        2.1557e-01, 1.6086e-07, 9.5071e-08, 3.4423e-08, 1.0452e-07, 9.0726e-08,
        6.1516e-08, 7.6668e-08, 7.3631e-08, 1.2772e-07, 1.6012e-07, 1.8773e-07,
        3.8315e-02, 9.6420e-02, 2.8435e-08, 1.0906e-07, 6.1957e-02, 6.0051e-08,
        1.0939e-07, 2.2156e-07, 1.2298e-01, 1.0847e-07, 5.4116e-08, 1.3088e-07,
        1.4014e-07, 1.5152e-07, 5.0233e-08, 1.3419e-07, 1.7766e-07, 1.1487e-07,
        4.5296e-08, 5.2374e-02, 1.4024e-07, 1.1683e-07, 6.4472e-08, 2.1957e-02,
        1.6313e-07, 8.3349e-08, 1.7549e-07, 1.6449e-07, 7.3979e-08, 3.3556e-02,
        1.9481e-07, 1.8987e-02, 1.6699e-07, 1.1043e-07, 7.3413e-02, 1.6261e-07,
        6.1283e-08, 2.0860e-07, 1.5602e-07, 1.6451e-07, 1.5785e-08, 3.8882e-03,
        6.6251e-08, 5.2351e-08, 7.0580e-08, 1.1898e-07, 7.6745e-08, 3.9987e-03,
        1.7888e-07, 8.1626e-08, 8.8780e-08, 1.4177e-07, 2.2179e-07, 1.1920e-07,
        5.9463e-03, 7.6293e-03, 9.4281e-08, 1.3851e-07, 1.1142e-07, 1.3523e-07,
        1.4828e-07, 1.5950e-07, 1.0265e-07, 1.7633e-07, 1.9290e-07, 5.9513e-08,
        2.8947e-07, 1.4468e-07, 6.5032e-08, 7.3698e-08, 8.2271e-08, 1.8711e-07,
        9.4997e-08, 8.3148e-08, 1.5527e-07, 2.3009e-07, 3.7971e-08, 7.1329e-02,
        1.1866e-07, 1.0398e-07, 6.4018e-08, 1.4128e-07, 1.8864e-07, 1.1212e-07,
        1.1602e-07, 8.5672e-08, 9.4843e-08, 1.3289e-07, 1.3603e-07, 4.9921e-08,
        1.3955e-07, 5.5001e-08, 4.7673e-03, 7.6668e-08, 1.4552e-07, 1.5389e-07,
        6.5121e-08, 8.6489e-08, 3.3181e-03, 1.0499e-07, 1.3274e-02, 4.4614e-08,
        1.7292e-07, 7.9839e-08, 1.4566e-07, 1.2902e-07, 7.0325e-08, 7.0008e-08,
        1.4492e-07, 8.6665e-02, 5.7522e-08, 7.8118e-08, 8.9683e-08, 1.0607e-07,
        1.5602e-07, 2.0671e-07, 1.1210e-02, 6.8288e-08, 1.7123e-07, 4.8529e-08,
        1.0465e-07, 7.4449e-03, 7.2842e-08, 1.9749e-07, 7.7860e-08, 9.3034e-08,
        1.2755e-07, 1.4667e-07, 1.2581e-07, 2.8905e-08, 1.2222e-07, 1.1549e-07,
        8.9055e-08, 1.0279e-07, 7.1271e-08, 1.5403e-07, 4.8177e-08, 3.1237e-08,
        8.9474e-03, 1.3802e-07, 6.4859e-08, 7.5834e-08, 8.1638e-08, 2.7543e-02,
        2.0022e-02, 6.5945e-08, 8.2882e-03, 5.3763e-02, 6.1061e-02, 2.2118e-07,
        1.1586e-07, 1.9623e-07, 5.1210e-08, 1.5072e-07, 2.0823e-07, 1.6480e-07,
        7.4676e-08, 8.8726e-08, 9.6396e-03, 1.4577e-07, 7.7052e-08, 1.6984e-08,
        4.9028e-08, 8.5359e-02, 8.7331e-08, 1.1257e-07, 4.7678e-08, 9.3791e-08,
        5.6292e-08, 2.3177e-07, 1.3515e-07, 6.5839e-02, 1.4505e-07, 9.3037e-03,
        6.1134e-08, 1.4505e-07, 6.3567e-08, 8.6194e-08, 9.4213e-08, 1.5065e-07,
        1.5363e-07, 6.3688e-08, 1.9925e-02, 8.8462e-08, 1.1100e-02, 5.4591e-02,
        1.4273e-07, 1.5025e-07, 1.6306e-07, 1.1218e-07, 6.4905e-08, 2.9342e-07,
        1.7610e-07, 1.4671e-07, 4.9318e-08, 1.0702e-07, 1.0846e-07, 8.9884e-03,
        5.3493e-08, 7.4192e-08, 1.1700e-02, 5.3747e-08, 5.4862e-02, 1.1065e-01,
        8.1638e-08, 1.0373e-07, 8.2523e-08, 1.2402e-07, 1.1686e-07, 4.8881e-08,
        1.1403e-07, 7.6162e-08, 7.6397e-08, 1.4525e-07, 1.0491e-02, 2.3551e-07,
        1.3233e-07, 8.1638e-08, 1.0239e-07, 1.0175e-07, 7.7376e-08, 4.2583e-08,
        1.3749e-02, 8.5522e-08, 7.1816e-08, 6.1288e-08, 1.2309e-07, 4.2822e-08,
        4.1129e-08, 3.2390e-02, 6.0578e-08, 3.4047e-02, 7.1802e-08, 1.0810e-07,
        5.5364e-03, 6.0882e-08, 1.5567e-01, 4.5762e-02, 5.2855e-08, 1.0325e-02,
        3.6481e-03, 4.7237e-02, 5.5449e-03, 1.8147e-01, 1.1279e-07, 1.1736e-07,
        1.7659e-07, 1.5707e-01, 1.3729e-07, 2.0818e-01, 3.8908e-08, 8.2118e-08,
        1.8237e-07, 1.0323e-07, 3.7223e-08, 7.5722e-02, 1.4538e-07, 7.8598e-08,
        1.3022e-07, 8.7758e-08, 5.5255e-03, 9.3851e-08, 1.2213e-01, 2.8951e-07,
        1.6184e-01, 8.2443e-08, 1.8264e-07, 1.7839e-07, 6.9976e-03, 7.1952e-08,
        1.5928e-01, 6.3046e-08, 7.2872e-08, 7.1977e-08, 6.1180e-08, 8.4266e-08,
        9.6025e-08, 4.9979e-02, 1.1132e-07, 1.1352e-07, 1.9130e-07, 5.4116e-08,
        7.6744e-08, 1.8916e-07, 1.3416e-07, 2.9772e-08, 1.1007e-07, 4.9373e-08,
        2.8253e-08, 1.3377e-02, 1.1801e-07, 2.2232e-07, 6.9190e-08, 9.2224e-08,
        1.5657e-07, 1.2675e-07, 3.3288e-02, 1.7825e-07, 2.6296e-03, 6.7915e-08,
        4.7575e-02, 4.0513e-08, 1.0731e-07, 2.2159e-07, 1.3998e-07, 1.1712e-07,
        4.5589e-08, 1.3352e-07, 4.3526e-02, 7.0050e-08, 7.0327e-08, 1.1556e-07,
        4.5229e-08, 4.5850e-08, 4.1002e-02, 4.9007e-08, 4.9018e-08, 4.4276e-08,
        9.9682e-08, 8.2237e-08, 1.2240e-07, 8.2360e-08, 7.0005e-08, 2.3226e-03,
        1.0425e-07, 1.0427e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0746e-07, 1.6511e-07, 3.3272e-07, 1.2299e-07, 8.9989e-02, 2.5438e-07,
        1.0754e-01, 1.4307e-07, 2.5177e-07, 5.9537e-07, 4.7609e-07, 3.6548e-07,
        6.9294e-02, 5.9529e-07, 4.3368e-07, 4.3368e-07, 2.1684e-07, 8.0618e-08,
        1.0803e-01, 3.5913e-07, 2.8666e-07, 7.5657e-07, 4.5443e-07, 8.2510e-02,
        1.8623e-07, 3.6548e-07, 1.0746e-07, 3.4967e-07, 1.2299e-07, 1.0746e-07,
        1.0980e-01, 2.8666e-07, 2.5438e-07, 3.2275e-07, 3.9573e-07, 9.6474e-02,
        9.8791e-02, 1.7866e-07, 1.3067e-07, 8.2084e-02, 5.9529e-07, 1.2234e-01,
        4.3368e-07, 1.1105e-01, 1.2299e-07, 8.0619e-08, 2.5177e-07, 1.1381e-01,
        1.0321e-01, 1.0746e-07, 1.3067e-07, 1.1021e-01, 4.3368e-07, 9.9962e-02,
        5.7341e-07, 1.2544e-01, 4.2673e-07, 1.2299e-07, 4.7609e-07, 5.7341e-07,
        7.8224e-02, 9.7779e-08, 4.3368e-07, 9.8446e-02, 1.2299e-07, 1.1836e-01,
        1.3067e-07, 7.2501e-02, 1.0746e-07, 9.2491e-02, 1.8623e-07, 6.0139e-07,
        9.7779e-08, 2.5177e-07, 4.9943e-07, 1.0689e-01, 1.8621e-07, 4.2673e-07,
        2.5177e-07, 1.0746e-07, 3.6548e-07, 1.4307e-07, 1.4703e-07, 9.7779e-08,
        1.0746e-07, 4.0987e-02, 1.2299e-07, 6.6696e-02, 5.7512e-02, 8.4562e-02,
        1.4703e-07, 1.1959e-07, 1.0571e-01, 5.8987e-07, 1.2299e-07, 1.1073e-01,
        2.5438e-07, 2.9247e-07, 4.2673e-07, 8.0617e-08, 9.2245e-02, 2.0081e-07,
        5.9534e-07, 3.6548e-07, 5.9529e-07, 9.3649e-02, 2.3877e-07, 9.8715e-02,
        3.6548e-07, 6.6616e-02, 8.0618e-08, 1.0746e-07, 4.3368e-07, 4.3368e-07,
        4.1248e-07, 4.3368e-07, 1.0126e-01, 1.0746e-07, 8.0603e-08, 5.7334e-07,
        4.7609e-07, 1.0746e-07, 9.3777e-02, 1.7866e-07, 1.2732e-07, 9.2739e-02,
        3.3272e-07, 7.3493e-02, 1.0324e-01, 4.3368e-07, 1.2257e-07, 4.3368e-07,
        1.0746e-07, 4.7609e-07, 3.4967e-07, 1.2299e-07, 1.2299e-07, 3.9573e-07,
        4.5443e-07, 4.3368e-07, 3.6130e-07, 1.1959e-07, 1.0746e-07, 1.7815e-02,
        1.1959e-07, 2.8666e-07, 1.0746e-07, 1.0746e-07, 4.3368e-07, 4.5443e-07,
        5.8722e-07, 1.2299e-07, 1.0354e-01, 9.9621e-02, 4.2673e-07, 1.2723e-07,
        9.1327e-02, 1.4703e-07, 4.5825e-02, 3.1452e-07, 5.7329e-07, 1.3579e-01,
        1.0541e-01, 2.3286e-07, 1.6367e-07, 4.2673e-07, 1.0746e-07, 9.5104e-02,
        1.2732e-07, 3.4967e-07, 1.0746e-07, 1.0976e-01, 1.0746e-07, 4.3368e-07,
        1.2299e-07, 5.5020e-07, 8.5298e-08, 1.0746e-07, 1.6616e-07, 1.4307e-07,
        3.6548e-07, 2.5177e-07, 1.4307e-07, 2.7321e-07, 4.3368e-07, 4.3368e-07,
        1.2299e-07, 4.2673e-07, 3.6548e-07, 1.4703e-07, 1.4307e-07, 8.5239e-02,
        8.5298e-08, 1.2299e-07, 3.6548e-07, 2.3877e-07, 1.2770e-07, 1.0839e-01,
        1.0231e-01, 1.2330e-01, 1.0746e-07, 9.4688e-02, 1.0746e-07, 4.3368e-07,
        3.5913e-07, 1.0746e-07, 1.0763e-01, 2.3877e-07, 1.0748e-01, 1.3468e-01,
        7.5613e-07, 9.8895e-02, 1.1889e-01, 1.2299e-07, 5.9609e-07, 2.3286e-07,
        1.3067e-07, 3.8520e-07, 1.3513e-01, 4.3368e-07, 7.2383e-02, 3.6548e-07,
        3.6548e-07, 1.0617e-01, 9.5167e-02, 4.5443e-07, 3.9573e-07, 2.3877e-07,
        3.6548e-07, 2.8666e-07, 1.8623e-07, 8.5298e-08, 2.9247e-07, 9.7779e-08,
        8.3193e-02, 1.2299e-07, 1.6367e-07, 9.4435e-02, 1.4307e-07, 8.5298e-08,
        1.4703e-07, 3.4967e-07, 8.0621e-08, 1.0848e-01, 6.4023e-07, 1.3484e-07,
        1.0746e-07, 9.9691e-02, 9.9457e-02, 1.0280e-01, 1.2027e-01, 2.3286e-07,
        8.9848e-02, 2.5177e-07, 1.2299e-07, 3.9573e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.4138e-02, 1.0377e-06, 1.7945e-06, 8.8162e-02, 2.3412e-06, 3.2966e-07,
        1.6630e-06, 1.0770e-06, 1.2788e-06, 1.2788e-06, 9.0942e-07, 2.6765e-06,
        1.7889e-06, 7.5846e-02, 4.2944e-06, 2.2439e-06, 1.5452e-06, 1.7979e-06,
        6.8357e-07, 2.2439e-06, 2.6765e-06, 3.3585e-06, 2.2343e-06, 1.0770e-06,
        6.4714e-02, 1.7889e-06, 2.2439e-06, 7.2485e-02, 1.5574e-06, 4.8391e-07,
        1.7979e-06, 1.2193e-06, 6.8357e-07, 1.0377e-06, 1.7889e-06, 2.2343e-06,
        6.5699e-02, 1.5452e-06, 7.1014e-07, 1.9564e-06, 3.4036e-06, 6.1069e-07,
        2.6765e-06, 1.0770e-06, 1.1229e-06, 2.6765e-06, 6.1069e-07, 2.4863e-06,
        1.2112e-06, 1.8885e-06, 6.8859e-02, 4.1672e-07, 4.3552e-07, 2.3119e-06,
        2.1343e-06, 1.5574e-06, 5.3693e-07, 1.6027e-06, 1.0377e-06, 1.7979e-06,
        9.7979e-07, 2.2439e-06, 5.3031e-07, 8.2531e-02, 1.8885e-06, 1.4607e-06,
        5.3031e-07, 3.3307e-06, 6.1069e-07, 1.7620e-06, 1.0377e-06, 3.4010e-06,
        1.7889e-06, 3.3188e-06, 1.0770e-06, 6.8357e-07, 5.3693e-07, 1.5452e-06,
        1.7889e-06, 5.5660e-07, 4.9036e-02, 6.3519e-02, 3.4036e-06, 7.3864e-02,
        1.5452e-06, 5.3031e-07, 7.8129e-07, 6.2522e-02, 1.5452e-06, 1.4607e-06,
        4.4065e-06, 3.4036e-06, 2.3119e-06, 1.7889e-06, 3.3307e-06, 1.5452e-06,
        3.3188e-06, 6.0234e-02, 2.2343e-06, 6.8906e-02, 7.1309e-02, 1.0176e-06,
        5.3031e-07, 7.1301e-02, 1.0377e-06, 5.3693e-07, 2.5394e-07, 6.1069e-07,
        1.7889e-06, 2.2343e-06, 3.4036e-06, 1.7945e-06, 7.0631e-02, 2.2343e-06,
        2.3119e-06, 6.8357e-07, 5.9901e-02, 6.8993e-07, 1.4255e-07, 4.3552e-07,
        8.3375e-07, 7.1520e-02, 1.5452e-06, 8.3375e-07, 6.1069e-07, 5.3693e-07,
        2.1343e-06, 1.5452e-06, 5.3693e-07, 7.2751e-02, 8.1685e-07, 1.2788e-06,
        2.6765e-06, 5.3031e-07, 1.0770e-06, 1.7889e-06, 6.5807e-02, 8.1992e-02,
        3.3585e-06, 6.9580e-02, 2.2343e-06, 1.0917e-06, 6.2413e-02, 1.4501e-06,
        1.7979e-06, 1.0377e-06, 6.9809e-02, 6.0786e-02, 5.7933e-02, 2.6765e-06,
        8.5334e-07, 2.2343e-06, 5.9578e-02, 1.5452e-06, 5.6995e-02, 8.3440e-02,
        1.7979e-06, 1.7979e-06, 1.0770e-06, 5.4169e-02, 6.1069e-07, 1.0377e-06,
        7.9459e-02, 1.4607e-06, 1.0176e-06, 1.0770e-06, 1.7889e-06, 2.1343e-06,
        1.5452e-06, 6.4557e-02, 6.1112e-02, 9.0942e-07, 1.2387e-07, 1.7979e-06,
        1.0377e-06, 1.1384e-06, 1.7979e-06, 2.2343e-06, 3.4036e-06, 9.0698e-07,
        2.2343e-06, 1.7889e-06, 7.0219e-02, 6.1069e-07, 3.3188e-06, 6.2585e-02,
        8.3375e-07, 5.3066e-07, 5.0651e-02, 4.7985e-02, 1.7979e-06, 2.6765e-06,
        6.1069e-07, 3.4036e-06, 1.4501e-06, 1.5452e-06, 5.3693e-07, 1.7889e-06,
        1.7889e-06, 6.1069e-07, 6.3152e-02, 6.1069e-07, 8.3375e-07, 2.1343e-06,
        5.3693e-07, 7.3576e-07, 5.3693e-07, 6.1069e-07, 1.7979e-06, 8.1901e-07,
        7.1066e-02, 1.4748e-06, 1.4607e-06, 1.7979e-06, 1.5452e-06, 3.3307e-06,
        8.3404e-02, 1.7979e-06, 6.6747e-02, 2.1601e-06, 2.9026e-07, 3.2966e-07,
        6.1069e-07, 3.4036e-06, 6.1069e-07, 3.3597e-06, 3.3307e-06, 1.5452e-06,
        5.3031e-07, 6.8903e-02, 5.3583e-02, 5.3031e-07, 7.2763e-02, 1.5452e-06,
        6.1069e-07, 8.3375e-07, 7.5182e-02, 1.0176e-06, 7.3576e-07, 3.2309e-06,
        2.2439e-06, 3.3144e-06, 6.1069e-07, 7.2010e-02, 5.3693e-07, 1.0377e-06,
        7.6364e-02, 1.7979e-06, 5.3693e-07, 6.2074e-07, 1.4607e-06, 4.2927e-06,
        6.1069e-07, 1.4755e-06, 5.9793e-02, 3.3188e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.7890e-02, 2.9980e-03, 1.5800e-01,  ..., 2.6975e-02, 2.4492e-02,
        1.4264e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.7599e-02, 4.1140e-07, 3.9676e-02,  ..., 1.9386e-02, 5.3772e-07,
        4.0655e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2058e-06, 1.4810e-06, 7.3284e-07, 2.7750e-07, 1.0719e-06, 3.8834e-02,
        2.0713e-06, 2.6153e-02, 6.1790e-07, 1.1311e-06, 1.4757e-06, 1.7578e-02,
        1.4757e-06, 1.5358e-06, 7.3284e-07, 1.1311e-06, 1.0719e-06, 7.1276e-07,
        6.1731e-07, 1.3063e-06, 8.4189e-07, 3.4991e-02, 1.3188e-07, 7.0935e-07,
        2.5957e-07, 1.1040e-06, 1.3064e-06, 6.1731e-07, 1.6055e-06, 3.6332e-02,
        9.9253e-07, 1.4031e-06, 3.1824e-02, 2.0826e-06, 3.8046e-02, 3.0833e-07,
        1.2648e-06, 1.3580e-06, 8.7119e-07, 3.2703e-02, 9.3059e-07, 1.4029e-06,
        8.5677e-07, 8.5677e-07, 7.0935e-07, 7.8902e-07, 4.3675e-02, 2.8079e-02,
        9.0283e-07, 1.3063e-06, 1.3858e-06, 7.3284e-07, 1.3939e-06, 1.3939e-06,
        1.1146e-06, 6.6570e-07, 8.1592e-07, 6.9817e-07, 3.3986e-02, 9.4595e-07,
        4.6330e-07, 8.8884e-07, 3.3363e-02, 2.7750e-07, 4.3023e-07, 5.8385e-07,
        7.2487e-07, 2.8834e-02, 1.0719e-06, 2.5502e-02, 3.0205e-02, 1.3380e-06,
        1.4029e-06, 1.3389e-01, 4.2067e-07, 1.0719e-06, 1.3939e-06, 1.3857e-06,
        2.4086e-07, 6.9817e-07, 2.1981e-07, 7.8630e-07, 7.3284e-07, 5.8385e-07,
        5.3993e-07, 6.3129e-03, 1.1311e-06, 8.1592e-07, 5.8312e-07, 4.3023e-07,
        8.0748e-07, 1.5907e-06, 1.1311e-06, 4.6952e-07, 2.1404e-06, 3.7119e-02,
        4.6780e-07, 2.2333e-02, 1.2829e-01, 2.4101e-07, 1.0275e-06, 6.8336e-07,
        4.3708e-07, 1.0719e-06, 4.3110e-07, 1.3939e-06, 5.8385e-07, 1.5103e-06,
        5.8312e-07, 4.1815e-07, 1.4757e-06, 7.5723e-07, 4.7906e-07, 1.3857e-06,
        2.8446e-07, 1.4757e-06, 4.0130e-07, 9.3232e-03, 4.3488e-07, 1.4029e-06,
        8.1256e-07, 7.3720e-07, 4.9731e-07, 8.9860e-07, 1.7567e-02, 5.8312e-07,
        2.7750e-07, 1.0069e-06, 4.6330e-07, 2.4101e-07, 3.8967e-07, 5.8385e-07,
        3.0778e-07, 6.5266e-07, 1.3939e-06, 1.3939e-06, 2.2107e-07, 9.0402e-07,
        1.3580e-06, 4.9730e-07, 1.3063e-06, 5.8385e-07, 3.5772e-02, 1.4441e-06,
        4.9731e-07, 2.5730e-02, 2.4101e-07, 5.8385e-07, 5.8385e-07, 7.8630e-07,
        3.4966e-07, 2.2603e-02, 1.4011e-07, 1.8367e-06, 1.6055e-06, 6.6144e-07,
        4.6641e-07, 1.2015e-06, 1.0275e-06, 6.7087e-07, 7.9730e-07, 3.9935e-02,
        2.3035e-07, 2.3039e-07, 1.2015e-06, 4.3023e-07, 3.5315e-02, 8.3060e-07,
        3.7313e-02, 2.3034e-07, 1.4756e-06, 5.6332e-07, 5.8385e-07, 1.4757e-06,
        5.8784e-07, 3.1499e-02, 1.2015e-06, 7.5723e-07, 4.0311e-02, 1.4011e-07,
        9.5031e-07, 7.3720e-07, 1.0719e-06, 2.4450e-07, 3.8564e-07, 2.7750e-07,
        4.3133e-07, 7.7720e-07, 5.3996e-07, 2.4101e-07, 6.7274e-07, 2.7514e-06,
        1.6055e-06, 1.6547e-06, 9.3059e-07, 1.3857e-06, 2.7869e-02, 1.3063e-06,
        7.4416e-07, 3.3441e-02, 5.3993e-07, 8.8884e-07, 4.4971e-07, 6.3102e-07,
        9.3059e-07, 4.9731e-07, 7.8713e-07, 1.3857e-06, 3.6684e-02, 4.5569e-02,
        4.8805e-07, 1.1311e-06, 7.5723e-07, 6.0244e-07, 4.6641e-07, 2.7862e-06,
        7.5723e-07, 2.6756e-07, 1.3939e-06, 3.8650e-02, 1.4757e-06, 1.4011e-07,
        1.3939e-06, 1.9581e-06, 3.2117e-02, 6.9817e-07, 3.4968e-07, 3.4969e-07,
        9.0886e-07, 9.5087e-07, 6.9817e-07, 2.4279e-07, 2.6882e-02, 3.3572e-02,
        6.5266e-07, 5.8312e-07, 1.6055e-06, 9.7818e-07, 2.0713e-06, 4.3132e-07,
        6.5266e-07, 2.4279e-07, 7.0524e-07, 1.2068e-06, 1.0719e-06, 1.0275e-06,
        1.3045e-06, 2.1422e-06, 3.8969e-07, 6.6319e-07, 2.4735e-02, 1.8815e-06,
        2.1663e-06, 5.2269e-07, 1.0719e-06, 2.4101e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.4558e-06, 7.5687e-07, 1.9049e-06, 7.5687e-07, 1.3036e-06, 5.6014e-02,
        4.9317e-06, 2.4558e-06, 8.3364e-07, 1.6154e-06, 8.3364e-07, 1.4458e-06,
        1.6230e-06, 5.1742e-02, 8.6657e-07, 3.0666e-07, 7.5687e-07, 1.3718e-06,
        1.9049e-06, 1.7886e-06, 7.5916e-07, 4.3897e-02, 8.3308e-07, 1.0803e-06,
        7.5687e-07, 1.0803e-06, 8.3364e-07, 8.3308e-07, 1.6230e-06, 1.3718e-06,
        1.0658e-06, 3.0666e-07, 1.3718e-06, 5.6667e-07, 2.6183e-06, 2.4558e-06,
        4.0601e-06, 5.7711e-02, 1.3769e-06, 4.2828e-02, 5.3222e-07, 1.9049e-06,
        5.7596e-02, 4.7514e-02, 8.3308e-07, 3.6898e-06, 1.8058e-06, 7.5687e-07,
        4.9317e-06, 3.8504e-06, 1.6230e-06, 1.2207e-06, 2.3257e-06, 5.3014e-07,
        2.4558e-06, 8.3895e-07, 2.0805e-06, 7.5915e-07, 1.9049e-06, 4.5730e-06,
        6.6168e-07, 1.3036e-06, 1.8382e-06, 1.0562e-06, 8.6657e-07, 1.8382e-06,
        3.5601e-06, 1.1870e-06, 7.0073e-02, 9.5425e-07, 8.3364e-07, 1.0749e-06,
        7.1345e-07, 9.1337e-07, 7.7032e-07, 2.6183e-06, 4.6565e-02, 3.6898e-06,
        2.4558e-06, 1.0749e-06, 3.8505e-06, 4.1256e-02, 1.0749e-06, 5.8308e-02,
        4.0650e-07, 8.1864e-07, 4.0601e-06, 4.9464e-07, 5.6667e-07, 1.8382e-06,
        3.6898e-06, 1.0658e-06, 1.2578e-06, 4.0601e-06, 2.3542e-06, 8.3364e-07,
        8.3364e-07, 3.0676e-02, 1.2578e-06, 4.9072e-06, 5.3544e-02, 3.7913e-06,
        2.4814e-06, 1.8382e-06, 1.0749e-06, 1.4458e-06, 5.6807e-07, 8.6657e-07,
        5.8857e-02, 8.1864e-07, 3.4967e-02, 1.6649e-06, 2.7777e-02, 5.9300e-07,
        2.5589e-06, 5.8055e-02, 1.4458e-06, 8.3895e-07, 1.0421e-06, 2.4327e-06,
        8.1864e-07, 5.3670e-02, 1.6154e-06, 2.4558e-06, 2.9560e-02, 1.3718e-06,
        1.1870e-06, 8.1864e-07, 5.6667e-07, 1.1770e-06, 2.1873e-06, 1.6230e-06,
        3.0666e-07, 5.0387e-02, 2.6183e-06, 3.4117e-02, 1.6154e-06, 1.7852e-06,
        1.1870e-06, 1.2574e-06, 8.1864e-07, 2.1873e-06, 5.0109e-07, 5.3222e-07,
        1.9049e-06, 1.1943e-06, 2.0199e-06, 3.8504e-06, 9.7944e-07, 2.4801e-06,
        1.6230e-06, 7.5687e-07, 4.0601e-06, 3.6898e-06, 7.5687e-07, 4.4693e-02,
        1.4458e-06, 9.5425e-07, 1.2578e-06, 8.1864e-07, 1.2578e-06, 5.9611e-02,
        2.4327e-06, 2.6183e-06, 6.4149e-07, 1.0421e-06, 1.4458e-06, 5.6687e-07,
        1.7683e-06, 7.6523e-07, 1.3769e-06, 2.6183e-06, 2.4558e-06, 1.3036e-06,
        3.0666e-07, 5.6660e-02, 3.8504e-06, 1.1673e-07, 7.5687e-07, 6.0375e-07,
        7.5687e-07, 1.0803e-06, 9.7944e-07, 3.7322e-02, 2.3542e-06, 1.8382e-06,
        9.7944e-07, 9.7944e-07, 4.0601e-06, 2.1437e-06, 1.9389e-06, 2.6183e-06,
        1.8828e-06, 1.1870e-06, 2.3257e-06, 1.9049e-06, 1.3769e-06, 2.6183e-06,
        8.3308e-07, 7.5687e-07, 1.6154e-06, 2.0844e-06, 3.0172e-02, 8.6657e-07,
        1.6154e-06, 1.7542e-06, 1.2578e-06, 5.6667e-07, 8.3364e-07, 1.6230e-06,
        7.5687e-07, 2.6183e-06, 2.3257e-06, 1.8382e-06, 9.5425e-07, 1.2574e-06,
        6.1460e-02, 5.6687e-07, 3.3212e-06, 1.4458e-06, 1.6649e-06, 5.6687e-07,
        5.3014e-07, 2.4558e-06, 3.7916e-06, 5.0916e-02, 1.3769e-06, 3.9916e-02,
        4.2371e-02, 1.0559e-06, 2.1709e-06, 5.4960e-07, 7.1345e-07, 8.3364e-07,
        3.8504e-06, 3.7908e-06, 1.8290e-06, 8.6657e-07, 2.4558e-06, 1.6230e-06,
        6.4149e-07, 2.6183e-06, 2.6183e-06, 1.8382e-06, 3.6898e-06, 2.6183e-06,
        7.5687e-07, 7.6523e-07, 1.9049e-06, 2.0199e-06, 5.6916e-07, 6.3308e-02,
        2.5255e-02, 6.3446e-02, 7.1345e-07, 9.5425e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.4067e-02, 4.9044e-08, 6.0758e-02,  ..., 3.1362e-02, 1.0933e-07,
        4.9257e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.2666e-07, 7.2347e-07, 6.9615e-07, 4.3654e-07, 3.8942e-07, 1.1388e-06,
        5.9508e-02, 3.5558e-07, 3.8942e-07, 3.9341e-07, 3.3647e-02, 7.9031e-07,
        2.5506e-07, 1.3007e-06, 1.9172e-06, 5.0038e-02, 1.1388e-06, 1.2150e-01,
        2.8754e-02, 3.2791e-02, 3.6066e-02, 6.2666e-07, 4.8105e-07, 4.6706e-02,
        4.3587e-02, 1.5475e-06, 3.2879e-07, 3.3144e-02, 2.1417e-01, 3.8702e-02,
        9.2307e-07, 1.3000e-06, 1.7597e-06, 4.0351e-07, 3.6442e-07, 7.1032e-07,
        4.1085e-07, 5.6617e-07, 5.3884e-02, 3.8942e-07, 2.7197e-07, 3.2859e-02,
        4.6821e-07, 3.6917e-07, 4.3654e-07, 4.2229e-07, 1.9944e-01, 1.7597e-06,
        5.3828e-03, 2.3410e-02, 7.1032e-07, 3.3388e-07, 3.0394e-02, 2.4929e-07,
        1.5079e-06, 3.9996e-02, 2.8508e-02, 4.9903e-07, 3.6773e-02, 1.7597e-06,
        4.3133e-02, 3.6442e-07, 7.5162e-07, 3.8542e-02, 1.5092e-06, 9.7892e-07,
        1.4945e-01, 8.4679e-07, 4.3654e-07, 3.3806e-02, 9.5516e-07, 7.9030e-07,
        3.0818e-02, 9.6269e-07, 2.8658e-07, 1.9387e-06, 5.5993e-07, 2.6503e-07,
        3.1449e-02, 1.8265e-06, 3.5417e-02, 1.3007e-06, 1.0413e-06, 1.8681e-02,
        4.1312e-02, 1.6989e-06, 1.0413e-06, 1.0019e-06, 4.6562e-02, 2.5740e-02,
        3.6442e-07, 5.7371e-07, 4.4903e-02, 5.1312e-07, 4.1474e-02, 1.2701e-06,
        4.7714e-07, 6.9360e-07, 4.8105e-07, 1.5092e-06, 3.3388e-07, 4.1161e-02,
        3.9726e-02, 1.0226e-06, 9.5858e-07, 6.3951e-07, 1.6989e-06, 2.5506e-07,
        2.4927e-07, 1.3007e-06, 3.9683e-02, 9.2307e-07, 2.0766e-02, 6.8167e-07,
        3.3388e-07, 6.9615e-07, 4.2657e-07, 5.7393e-07, 4.9022e-07, 6.9615e-07,
        3.8218e-02, 2.7457e-02, 9.0982e-07, 2.7684e-02, 4.0302e-07, 1.2701e-06,
        4.9679e-07, 6.7809e-07, 2.5506e-07, 9.6718e-07, 3.5711e-02, 2.2964e-07,
        3.8237e-02, 9.6269e-07, 7.1991e-07, 1.6989e-06, 4.3654e-07, 3.6442e-07,
        1.6989e-06, 6.1258e-07, 1.3007e-06, 2.9985e-02, 5.1312e-07, 4.4596e-02,
        2.0192e-06, 1.3007e-06, 6.7809e-07, 3.8409e-02, 4.2937e-02, 4.0394e-02,
        4.4992e-02, 4.2469e-02, 7.7647e-07, 4.0639e-07, 3.7809e-02, 4.4635e-02,
        9.4725e-07, 1.2900e-06, 1.1388e-06, 3.9047e-07, 1.2861e-01, 1.0413e-06,
        7.7647e-07, 4.1125e-02, 3.3058e-07, 3.3058e-07, 1.6989e-06, 7.9030e-07,
        5.2374e-07, 4.2657e-07, 1.3007e-06, 1.3200e-06, 1.3269e-06, 9.5516e-07,
        4.7956e-02, 3.6442e-07, 5.8367e-07, 4.8923e-07, 3.4467e-02, 6.7777e-07,
        1.3007e-06, 1.6989e-06, 3.6442e-07, 1.0019e-06, 8.1072e-07, 1.5570e-02,
        2.1292e-02, 4.4740e-02, 6.9360e-07, 7.7904e-07, 9.6401e-07, 7.1991e-07,
        6.9853e-07, 1.3007e-06, 3.6338e-02, 3.6442e-07, 7.9030e-07, 1.3775e-06,
        3.8942e-07, 5.1312e-07, 7.1991e-07, 4.0793e-02, 3.6917e-07, 2.6503e-07,
        5.7393e-07, 4.9029e-02, 6.6725e-07, 3.9660e-02, 1.6989e-06, 5.0406e-07,
        5.0878e-07, 5.4629e-07, 1.9409e-06, 4.2657e-07, 4.7714e-07, 4.2046e-02,
        1.2701e-06, 1.3058e-06, 7.8424e-07, 2.5506e-07, 5.2374e-07, 9.4096e-07,
        9.5858e-07, 3.0226e-02, 8.4679e-07, 7.9030e-07, 1.5411e-06, 3.1088e-02,
        1.3365e-03, 1.6989e-06, 7.1032e-07, 7.1032e-07, 4.4614e-02, 1.8818e-06,
        1.1388e-06, 2.0236e-06, 3.1731e-02, 3.6796e-02, 4.0302e-07, 5.4629e-07,
        3.8942e-07, 5.1312e-07, 8.3208e-07, 4.3654e-07, 4.3654e-07, 3.3843e-02,
        3.0512e-02, 4.7714e-07, 1.0413e-06, 2.0008e-06, 8.4679e-07, 6.2666e-07,
        2.8221e-02, 1.6989e-06, 4.1259e-02, 1.3058e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.2549e-06, 2.8018e-06, 4.5077e-06, 1.2737e-01, 4.6497e-06, 9.6341e-07,
        1.0144e-01, 1.4030e-06, 2.5968e-06, 5.2821e-06, 1.9613e-06, 3.5458e-06,
        3.4470e-06, 2.4446e-06, 9.3375e-02, 5.9097e-06, 1.2541e-06, 2.4943e-06,
        4.3418e-06, 3.9079e-06, 1.2275e-06, 4.3611e-06, 9.4610e-07, 1.1654e-06,
        2.5345e-06, 1.9251e-06, 3.4471e-06, 4.7698e-06, 9.0315e-02, 2.1647e-06,
        2.3338e-06, 1.4112e-06, 6.9310e-06, 2.9265e-06, 1.8283e-06, 1.9323e-06,
        3.6839e-06, 3.3830e-06, 9.9323e-07, 4.8574e-06, 2.9951e-06, 2.9572e-06,
        3.2873e-02, 2.2123e-06, 9.8490e-02, 5.9837e-06, 9.0033e-02, 3.2549e-06,
        2.6025e-02, 4.4926e-06, 1.0736e-06, 4.0016e-06, 2.2123e-06, 1.8283e-06,
        2.9572e-06, 5.0956e-06, 3.5578e-06, 1.5077e-06, 7.9770e-02, 4.3779e-06,
        3.4199e-06, 5.2407e-06, 2.1065e-06, 3.1056e-06, 1.9865e-06, 8.9888e-02,
        4.8574e-06, 3.5090e-06, 1.2541e-06, 3.7103e-06, 9.1626e-02, 2.7282e-06,
        1.1888e-01, 3.6428e-06, 9.4285e-02, 1.1465e-01, 1.8079e-06, 1.7302e-06,
        1.4248e-06, 2.8660e-06, 3.2358e-06, 2.8254e-06, 3.5430e-06, 5.0047e-06,
        1.0524e-01, 9.9470e-07, 3.6839e-06, 3.5090e-06, 1.4698e-06, 7.9072e-02,
        2.8660e-06, 1.4698e-06, 7.2815e-02, 1.2510e-06, 3.3830e-06, 3.8893e-06,
        5.6316e-07, 1.4475e-06, 2.1684e-06, 5.5124e-06, 1.4112e-06, 3.2549e-06,
        3.2964e-02, 1.0411e-01, 1.5077e-06, 1.0455e-01, 1.2508e-06, 3.1105e-06,
        1.9501e-06, 3.8352e-06, 5.7641e-06, 4.0368e-06, 3.3988e-06, 1.0241e-06,
        1.4020e-06, 7.6427e-02, 4.1019e-06, 2.8233e-06, 9.0065e-07, 8.5824e-02,
        1.4030e-06, 4.3480e-06, 2.2271e-06, 8.1412e-02, 3.2876e-06, 3.5090e-06,
        6.7731e-06, 8.2532e-02, 1.0065e-06, 9.6428e-02, 9.0065e-07, 1.1573e-01,
        3.5205e-06, 1.8289e-07, 3.5430e-06, 3.8347e-07, 5.8018e-06, 5.9334e-06,
        1.0013e-01, 8.0704e-02, 2.6215e-02, 2.9505e-06, 3.5236e-06, 8.4212e-02,
        4.2614e-06, 7.8951e-02, 5.9097e-06, 2.6318e-06, 1.1575e-01, 5.2821e-06,
        2.4460e-06, 2.5968e-06, 1.5367e-06, 9.3898e-07, 3.8521e-06, 2.9572e-06,
        4.1417e-06, 8.6411e-02, 4.1964e-06, 2.6671e-06, 1.0660e-06, 7.5266e-02,
        2.5907e-06, 9.1907e-02, 5.9837e-06, 7.1656e-02, 4.6958e-06, 1.2771e-06,
        4.9264e-06, 3.2549e-06, 2.8852e-06, 3.1056e-06, 1.5226e-06, 1.8283e-06,
        2.0898e-06, 9.5422e-02, 1.1017e-06, 1.5685e-06, 2.5968e-06, 7.0790e-06,
        7.7505e-02, 2.6318e-06, 4.2614e-06, 8.9333e-07, 1.0345e-01, 2.4446e-06,
        4.1657e-06, 2.1550e-02, 8.6226e-02, 5.1046e-06, 5.6654e-06, 7.8378e-02,
        2.7442e-06, 9.1606e-02, 5.2063e-06, 5.4202e-06, 3.3750e-06, 1.0660e-06,
        1.4625e-06, 5.3446e-06, 1.2882e-06, 3.3830e-06, 3.8521e-06, 1.9624e-06,
        5.1240e-06, 1.9335e-06, 1.6675e-06, 9.1707e-02, 2.8539e-06, 1.2541e-06,
        5.6654e-06, 8.2971e-02, 1.2993e-06, 4.4260e-06, 4.4776e-06, 3.5090e-06,
        1.5226e-06, 7.6270e-02, 1.7302e-06, 1.8283e-06, 3.5616e-06, 6.9353e-02,
        7.1967e-02, 2.9572e-06, 4.1417e-06, 3.6839e-06, 7.9585e-06, 1.2541e-06,
        1.2882e-06, 2.7747e-06, 8.4962e-02, 3.1446e-06, 2.2123e-06, 1.5334e-06,
        8.6045e-06, 2.2123e-06, 9.8441e-06, 1.5077e-06, 2.1929e-06, 3.6315e-06,
        3.6839e-06, 2.0645e-06, 2.6318e-06, 4.3006e-06, 8.5800e-02, 1.4089e-07,
        2.8101e-06, 1.6675e-06, 1.4906e-06, 1.4698e-06, 3.6428e-06, 2.7178e-06,
        6.2206e-06, 1.7302e-06, 2.9572e-06, 1.9920e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.2698e-02, 5.9207e-08, 5.1630e-02,  ..., 6.9974e-08, 4.2353e-07,
        2.1249e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.8729e-07, 6.0808e-07, 3.2379e-08, 2.2045e-06, 1.8721e-06, 7.4754e-07,
        1.4487e-06, 8.4269e-07, 4.3296e-07, 9.0120e-07, 1.5623e-06, 8.4851e-07,
        1.4738e-06, 1.4173e-06, 1.0287e-06, 1.2740e-06, 2.1342e-06, 3.1220e-06,
        2.3037e-06, 1.0542e-06, 7.4652e-07, 5.1073e-07, 7.9444e-07, 2.4442e-06,
        2.0736e-06, 6.3433e-07, 5.2913e-07, 2.4391e-07, 4.8827e-07, 1.5542e-06,
        2.3037e-06, 1.3344e-06, 9.0120e-07, 1.1348e-06, 1.8793e-07, 1.3427e-01,
        7.1684e-07, 2.8890e-06, 2.8051e-07, 9.0121e-07, 2.2045e-06, 1.9554e-06,
        9.0120e-07, 1.2313e-06, 2.0939e-06, 8.4045e-07, 6.5062e-07, 1.8700e-06,
        1.2619e-06, 8.3206e-07, 5.3914e-07, 1.3490e-06, 1.1222e-06, 3.2092e-02,
        1.3326e-06, 5.3932e-07, 6.4678e-07, 1.1427e-01, 2.8051e-07, 1.3967e-06,
        3.0515e-06, 7.4037e-07, 7.8858e-07, 1.4890e-06, 6.5062e-07, 2.5223e-06,
        2.0419e-06, 8.8762e-02, 2.3037e-06, 1.6258e-06, 4.1830e-07, 4.7520e-07,
        3.9361e-06, 5.1073e-07, 1.6101e-06, 1.1222e-06, 9.0748e-07, 1.4053e-06,
        1.7251e-06, 9.3540e-07, 1.2973e-06, 1.6772e-06, 2.4391e-07, 1.2223e-06,
        6.3083e-07, 1.3391e-06, 3.8084e-02, 2.8884e-06, 1.8385e-06, 2.0939e-06,
        5.4504e-07, 7.6931e-07, 1.5143e-06, 2.3370e-06, 8.8238e-07, 1.3053e-06,
        6.3185e-07, 4.2493e-06, 3.0515e-06, 1.1725e-06, 3.9123e-07, 5.3932e-07,
        1.4557e-06, 2.8051e-07, 3.8200e-06, 1.2619e-06, 7.8858e-07, 1.1891e-06,
        6.0808e-07, 1.6903e-06, 2.3370e-06, 9.5541e-07, 2.6902e-06, 1.2408e-06,
        1.4144e-06, 2.8331e-06, 2.2045e-06, 8.0216e-02, 9.0120e-07, 1.7535e-06,
        6.5003e-07, 8.2415e-07, 2.1218e-06, 9.6531e-07, 2.4442e-06, 1.7578e-06,
        9.0120e-07, 4.9536e-07, 6.3433e-07, 1.0542e-06, 4.4382e-07, 7.1834e-02,
        2.1175e-06, 2.5156e-06, 8.5709e-02, 2.3037e-06, 1.5282e-06, 1.9991e-06,
        2.0939e-06, 3.4327e-07, 2.3902e-06, 9.6827e-07, 2.2045e-06, 8.5562e-07,
        7.2848e-07, 8.0544e-07, 1.4092e-06, 2.5388e-06, 2.1900e-06, 1.6101e-06,
        7.8858e-07, 1.2704e-01, 4.0082e-07, 4.9842e-07, 1.3101e-06, 2.1704e-02,
        7.1684e-07, 2.3769e-06, 2.5497e-06, 1.3993e-06, 6.6059e-07, 1.2296e-06,
        9.0120e-07, 3.9123e-07, 7.9444e-07, 8.2415e-07, 1.1175e-06, 2.7235e-06,
        1.2313e-06, 6.3433e-07, 7.1913e-07, 1.6835e-06, 9.0120e-07, 1.0352e-01,
        3.2379e-08, 8.4045e-07, 2.5661e-06, 3.7389e-07, 1.4042e-06, 2.2045e-06,
        1.7758e-06, 6.3185e-07, 7.1684e-07, 2.0222e-06, 3.9291e-07, 1.3169e-06,
        2.4442e-06, 1.2121e-06, 5.9228e-07, 1.0568e-06, 1.5281e-06, 1.3169e-06,
        1.2358e-06, 8.3206e-07, 3.9361e-06, 4.8488e-07, 1.5623e-06, 2.1245e-06,
        3.7898e-07, 1.4824e-06, 1.5358e-06, 1.4557e-06, 8.4816e-08, 3.7898e-07,
        3.3567e-02, 9.8962e-07, 6.6338e-07, 2.4193e-06, 1.2619e-06, 1.3378e-06,
        1.1222e-06, 2.3302e-06, 2.0222e-06, 1.4824e-06, 1.2740e-06, 1.1675e-01,
        3.0515e-06, 2.5532e-06, 3.7898e-07, 2.3216e-06, 3.9122e-07, 6.0808e-07,
        2.3769e-06, 8.0544e-07, 1.3199e-06, 1.0542e-06, 1.7818e-06, 2.2045e-06,
        2.7019e-06, 1.5871e-06, 9.0120e-07, 8.2706e-07, 1.9704e-06, 7.1684e-07,
        2.9230e-06, 7.1684e-07, 2.6147e-06, 1.8385e-06, 1.9348e-06, 3.7116e-06,
        1.3326e-06, 1.5542e-06, 4.0874e-07, 1.2079e-06, 7.4906e-02, 1.3358e-01,
        2.8051e-07, 2.0850e-06, 1.7578e-06, 2.9709e-02, 2.7897e-06, 9.7761e-07,
        3.1055e-02, 1.1072e-06, 1.1175e-06, 2.4442e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.7767e-06, 1.1492e-06, 3.8373e-06, 3.9644e-06, 3.0035e-06, 1.1485e-01,
        3.0532e-06, 4.3923e-06, 5.4005e-06, 2.3973e-06, 3.2711e-06, 2.3846e-06,
        5.4097e-06, 1.5898e-06, 1.1272e-06, 5.2671e-06, 5.4097e-06, 1.6902e-06,
        4.7733e-06, 3.0263e-06, 8.2637e-06, 4.7733e-06, 2.9549e-06, 3.4273e-06,
        2.8872e-06, 2.4618e-06, 6.3085e-06, 5.0078e-06, 3.8373e-06, 4.3486e-06,
        5.9410e-06, 6.3347e-06, 2.9549e-06, 3.0035e-06, 8.8653e-06, 5.4876e-06,
        6.4833e-06, 2.0707e-06, 3.2479e-06, 4.9262e-07, 6.4833e-06, 3.7302e-06,
        1.3386e-01, 5.0457e-06, 3.4305e-06, 2.7381e-06, 5.3243e-06, 2.9479e-06,
        1.1766e-06, 6.5400e-06, 1.8965e-06, 4.2564e-06, 1.2005e-06, 5.5997e-06,
        2.8618e-06, 2.5516e-06, 2.7444e-06, 6.2461e-06, 2.6381e-06, 2.3846e-06,
        5.5198e-06, 5.8786e-06, 3.0263e-06, 3.6518e-06, 1.1220e-06, 1.0795e-05,
        5.4968e-06, 1.0723e-01, 3.0314e-06, 6.1992e-06, 3.2214e-06, 4.3053e-06,
        6.8290e-06, 5.7482e-06, 1.8191e-06, 2.3973e-06, 5.6467e-06, 3.4305e-06,
        4.1454e-06, 2.6799e-06, 2.3886e-06, 2.2574e-06, 2.8798e-06, 4.3278e-06,
        1.2228e-05, 3.6505e-06, 3.5510e-06, 6.0265e-06, 6.6170e-06, 3.0523e-06,
        3.0818e-06, 5.4097e-06, 1.4021e-06, 1.0869e-06, 2.1622e-06, 4.0849e-06,
        5.7482e-06, 3.3305e-06, 8.2275e-02, 3.7512e-06, 8.5109e-06, 6.2700e-06,
        2.1276e-06, 2.7911e-06, 1.9815e-06, 3.7159e-06, 6.7184e-06, 4.7733e-06,
        2.9504e-06, 4.6805e-06, 4.7807e-06, 6.7314e-06, 4.2408e-06, 3.3073e-06,
        1.9269e-06, 3.2363e-06, 5.8815e-06, 7.5347e-06, 3.0263e-06, 2.5637e-06,
        5.9935e-06, 4.3824e-06, 3.0402e-06, 2.5689e-06, 4.2408e-06, 3.2363e-06,
        1.5302e-06, 4.1826e-06, 2.3973e-06, 2.9260e-06, 2.8872e-06, 2.4181e-06,
        1.4939e-01, 6.7184e-06, 6.4833e-06, 5.1697e-06, 2.9549e-06, 6.0265e-06,
        3.5152e-06, 5.9595e-06, 7.5030e-06, 2.1276e-06, 3.1686e-06, 5.7587e-06,
        2.3969e-06, 9.6308e-02, 3.0263e-06, 3.8538e-06, 1.8679e-06, 4.6307e-06,
        5.8432e-06, 4.2237e-06, 1.4232e-01, 5.6835e-07, 1.3769e-06, 7.7198e-06,
        2.1276e-06, 2.7816e-06, 2.5719e-06, 5.9935e-06, 2.8251e-06, 2.0801e-06,
        2.7972e-06, 1.0051e-01, 4.7260e-06, 2.0801e-06, 2.3795e-06, 5.9165e-06,
        6.7506e-06, 2.9549e-06, 3.3573e-06, 1.9269e-06, 2.1255e-06, 4.3053e-06,
        5.0078e-06, 5.8012e-06, 4.3278e-06, 6.0812e-06, 3.6994e-06, 7.5145e-06,
        2.6799e-06, 6.7184e-06, 3.0532e-06, 4.7733e-06, 8.0466e-06, 2.6748e-06,
        2.4736e-06, 2.7381e-06, 3.5240e-06, 3.7520e-06, 4.3817e-06, 3.3573e-06,
        5.9935e-06, 5.8432e-06, 1.5380e-06, 1.3685e-06, 1.2571e-01, 2.0630e-06,
        5.0078e-06, 7.4295e-06, 4.6831e-06, 4.6805e-06, 3.0402e-06, 4.7156e-06,
        1.4662e-06, 3.6249e-06, 2.8245e-06, 1.8965e-06, 2.8965e-06, 2.0970e-06,
        3.3640e-06, 5.6467e-06, 4.5268e-06, 4.1001e-06, 6.0478e-06, 2.0707e-06,
        3.5675e-06, 3.6243e-06, 2.7381e-06, 3.0964e-06, 5.8815e-06, 3.7512e-06,
        2.8250e-06, 1.5553e-06, 5.9935e-06, 8.1774e-06, 6.9519e-06, 5.9935e-06,
        1.2228e-05, 2.8798e-06, 2.1255e-06, 2.6725e-06, 5.9935e-06, 9.4453e-06,
        3.4171e-06, 6.0593e-06, 3.1325e-06, 5.6687e-06, 2.1255e-06, 2.7816e-06,
        2.5361e-06, 3.8307e-06, 2.2021e-06, 9.8565e-06, 3.0818e-06, 2.6147e-06,
        4.7733e-06, 5.9935e-06, 5.7004e-06, 4.5606e-06, 3.2541e-06, 3.4638e-06,
        4.4300e-06, 2.7935e-06, 3.5510e-06, 2.4569e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.1154e-07, 2.9608e-07, 3.8788e-07,  ..., 1.9548e-07, 1.4664e-07,
        2.8302e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.6158e-07, 1.0256e-01, 1.2996e-01, 1.0751e-01, 9.6469e-02, 8.0947e-02,
        1.1883e-01, 1.1236e-01, 8.0387e-02, 1.5037e-06, 1.0782e-06, 8.3553e-02,
        4.9244e-02, 8.9517e-02, 8.8733e-02, 4.2479e-02, 6.0485e-07, 8.9351e-02,
        1.0109e-06, 9.3579e-07, 1.4949e-06, 3.2375e-07, 5.2890e-07, 1.0368e-01,
        4.3617e-02, 1.0743e-06, 1.7747e-06, 1.5718e-06, 1.2153e-06, 2.6220e-06,
        1.4949e-06, 1.0775e-01, 1.0704e-01, 4.4530e-02, 1.7760e-06, 8.6717e-02,
        5.6716e-07, 9.9778e-02, 1.0688e-01, 3.7772e-07, 2.1474e-06, 1.7892e-06,
        9.9403e-07, 1.0008e-01, 1.0685e-01, 9.9300e-02, 9.0951e-02, 1.5874e-01,
        1.8210e-06, 1.2104e-06, 8.5549e-02, 1.1966e-06, 3.2375e-07, 9.8324e-02,
        8.9139e-07, 8.2112e-02, 1.0323e-01, 4.4407e-02, 5.4633e-02, 6.1344e-07,
        1.2167e-01, 1.2104e-01, 1.0328e-06, 9.6620e-07, 9.1992e-02, 1.0607e-06,
        5.4959e-02, 1.6573e-06, 1.0072e-06, 1.1091e-01, 3.7772e-07, 1.5349e-06,
        1.1917e-06, 4.6716e-02, 1.0453e-06, 8.7698e-02, 3.7199e-06, 8.0959e-02,
        1.0038e-01, 6.1344e-07, 9.5383e-02, 2.6826e-06, 1.2403e-01, 1.0956e-06,
        1.0627e-01, 6.1518e-07, 9.9875e-02, 3.6559e-07, 5.4386e-07, 2.1522e-07,
        2.1691e-06, 7.7544e-07, 7.8566e-02, 2.4067e-06, 4.8771e-02, 1.4058e-06,
        6.3921e-07, 1.3207e-06, 4.7403e-02, 9.6529e-02, 7.1907e-07, 2.0398e-06,
        1.1088e-01, 4.0415e-07, 1.0469e-01, 9.6915e-02, 9.8803e-02, 1.1756e-06,
        5.5592e-07, 4.0415e-07, 6.4802e-07, 9.7605e-02, 5.4713e-02, 8.4972e-02,
        6.3921e-07, 1.2784e-01, 1.6441e-06, 1.0175e-01, 7.7205e-07, 1.0950e-01,
        8.3388e-02, 1.0072e-06, 6.3921e-07, 1.0805e-06, 8.9118e-07, 5.7844e-07,
        3.7772e-07, 1.6108e-06, 3.5444e-07, 1.7661e-06, 2.6220e-06, 9.3073e-02,
        4.0091e-07, 1.3015e-06, 9.0032e-07, 8.1474e-02, 1.4215e-06, 5.7069e-07,
        1.5581e-06, 1.0663e-01, 4.7453e-02, 1.1965e-06, 9.3050e-02, 8.1556e-02,
        6.1345e-07, 8.4576e-07, 3.2375e-07, 1.2419e-06, 2.6220e-06, 4.6750e-02,
        8.3861e-02, 8.8839e-07, 1.2313e-07, 7.1849e-07, 6.1518e-07, 1.0956e-06,
        7.7205e-07, 1.0956e-06, 8.5744e-02, 8.6554e-07, 1.0743e-06, 2.2455e-06,
        5.4320e-02, 1.1504e-06, 1.1966e-06, 3.6542e-06, 3.2375e-07, 1.2058e-01,
        7.7205e-07, 2.0533e-06, 1.1634e-01, 1.0328e-06, 1.0737e-01, 5.0433e-02,
        1.0065e-01, 2.6220e-06, 5.0451e-07, 1.0319e-01, 1.0026e-01, 4.8007e-02,
        1.3207e-06, 2.1474e-06, 1.2793e-01, 3.2375e-07, 1.1080e-01, 1.2153e-06,
        9.1831e-07, 9.6298e-07, 1.2153e-06, 2.6220e-06, 9.3465e-02, 5.7921e-07,
        1.1026e-06, 4.0091e-07, 1.2074e-01, 1.0743e-06, 1.8210e-06, 4.8249e-02,
        6.6906e-02, 5.9049e-07, 9.3936e-02, 4.8428e-02, 4.6686e-02, 8.4791e-02,
        4.3860e-07, 2.6220e-06, 1.5403e-06, 1.6073e-06, 2.2489e-06, 6.3007e-02,
        9.6298e-07, 5.2890e-07, 9.3073e-02, 3.7772e-07, 6.4802e-07, 9.7779e-07,
        1.3148e-06, 3.4102e-07, 8.6196e-07, 1.6441e-06, 1.1040e-01, 6.1345e-07,
        1.1504e-06, 5.2324e-07, 2.1474e-06, 1.1966e-06, 2.9855e-07, 3.2375e-07,
        1.2228e-01, 8.3883e-02, 1.5780e-06, 9.7136e-07, 2.7873e-06, 9.7136e-07,
        2.6070e-06, 8.2846e-07, 1.5349e-06, 3.2375e-07, 9.3960e-02, 9.2625e-02,
        4.0091e-07, 2.8399e-07, 1.0814e-01, 1.0694e-01, 1.5154e-01, 9.9012e-02,
        8.9677e-02, 9.1831e-07, 2.4723e-06, 1.4953e-01, 1.1015e-06, 5.3549e-02,
        1.2935e-06, 8.1970e-07, 2.7079e-06, 9.1472e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.8420e-02, 7.9474e-02, 4.6007e-06, 9.6866e-02, 5.0060e-06, 3.8666e-06,
        5.6328e-06, 1.9395e-06, 7.5935e-02, 2.8090e-06, 5.6328e-06, 4.9114e-06,
        1.6927e-06, 1.0022e-06, 2.0776e-06, 3.5237e-06, 1.6799e-06, 3.1994e-06,
        3.7715e-06, 3.3385e-06, 2.7578e-06, 5.6328e-06, 9.6830e-02, 1.8177e-06,
        8.2836e-02, 2.8090e-06, 7.5391e-06, 2.6163e-06, 2.8455e-06, 3.8969e-02,
        3.1517e-06, 2.9912e-06, 3.7125e-06, 8.9518e-07, 2.3491e-06, 7.9239e-02,
        7.0371e-06, 1.9395e-06, 6.1883e-06, 4.7248e-06, 8.0565e-02, 4.3218e-06,
        4.8309e-06, 8.6060e-02, 7.5547e-02, 8.3459e-02, 2.7853e-06, 8.7360e-02,
        5.0084e-06, 1.5067e-06, 5.6328e-06, 1.7201e-06, 2.9093e-06, 8.2799e-02,
        2.0930e-06, 2.0940e-06, 1.6082e-06, 6.1919e-06, 3.2272e-06, 9.1971e-02,
        3.7125e-06, 1.1323e-01, 5.4262e-06, 2.6163e-06, 1.0863e-06, 1.6799e-06,
        5.2314e-06, 2.4038e-06, 5.0497e-06, 7.6269e-02, 3.5224e-06, 1.6578e-06,
        9.1829e-02, 3.1119e-02, 9.9287e-02, 4.6926e-07, 1.6814e-06, 3.8378e-06,
        7.9212e-02, 2.6700e-06, 1.1787e-06, 1.1318e-01, 3.4012e-06, 1.7203e-06,
        6.1134e-02, 3.1517e-06, 4.4665e-06, 2.8024e-06, 7.5243e-07, 1.4818e-06,
        7.9922e-02, 6.8287e-06, 9.8444e-02, 1.8177e-06, 8.8418e-02, 1.1235e-01,
        2.2525e-06, 1.8177e-06, 1.5537e-06, 1.0600e-06, 1.0076e-01, 5.0755e-07,
        2.7853e-06, 8.7816e-02, 3.2272e-06, 1.1604e-01, 1.0005e-06, 1.5245e-06,
        9.6297e-07, 1.8177e-06, 2.8090e-06, 2.6484e-06, 6.9454e-02, 2.5442e-06,
        8.6296e-02, 5.7988e-06, 7.5706e-02, 3.4349e-06, 5.3525e-06, 1.0973e-01,
        1.8177e-06, 1.4836e-06, 3.1147e-06, 3.5253e-06, 5.1757e-06, 3.7125e-06,
        9.0783e-02, 7.5391e-06, 1.1758e-01, 3.9877e-02, 4.8309e-06, 7.6615e-02,
        7.4799e-02, 2.4076e-06, 8.3866e-07, 3.2474e-06, 1.9395e-06, 1.6578e-06,
        2.4303e-06, 5.2513e-06, 8.5780e-02, 4.8287e-06, 1.1869e-01, 2.7834e-06,
        3.6624e-06, 3.2272e-06, 2.0104e-06, 3.4002e-06, 2.2000e-06, 2.4661e-06,
        1.9219e-06, 2.0969e-06, 3.7556e-02, 1.1369e-06, 1.0279e-01, 2.9093e-06,
        2.7853e-06, 1.1232e-06, 6.0250e-06, 8.0376e-06, 2.2000e-06, 2.2370e-06,
        1.1232e-06, 2.0370e-06, 1.2699e-06, 2.4076e-06, 6.4460e-07, 1.2435e-06,
        2.5611e-06, 4.4839e-06, 2.1181e-06, 1.9951e-06, 1.7935e-06, 1.2435e-06,
        2.5833e-06, 1.4989e-06, 2.9093e-06, 4.0751e-06, 4.2368e-06, 3.5030e-06,
        5.3129e-06, 5.7750e-06, 2.4303e-06, 4.6515e-06, 4.2368e-06, 3.6758e-02,
        2.4303e-06, 1.9868e-06, 7.0783e-06, 2.9093e-06, 2.6434e-06, 7.0371e-06,
        8.5053e-02, 4.1877e-06, 1.1111e-06, 2.9093e-06, 9.8004e-02, 8.7856e-02,
        4.0134e-06, 3.6624e-06, 2.4038e-06, 3.2922e-02, 5.8182e-06, 4.8309e-06,
        4.0422e-02, 1.1982e-05, 2.8375e-06, 3.5827e-06, 9.6179e-02, 5.9606e-06,
        1.7675e-07, 1.0224e-01, 5.8757e-06, 9.0224e-02, 5.1016e-06, 2.4303e-06,
        4.5484e-06, 5.3525e-06, 2.7298e-06, 7.5391e-06, 8.0673e-02, 5.7936e-06,
        3.8137e-07, 5.2077e-06, 2.2000e-06, 2.0595e-06, 2.6163e-06, 1.5245e-06,
        3.8137e-07, 3.4351e-06, 2.9847e-06, 4.0751e-06, 3.7470e-06, 1.5747e-06,
        2.5817e-06, 9.2587e-06, 7.6197e-02, 3.4351e-06, 8.5339e-02, 2.3062e-06,
        3.8471e-02, 3.3462e-06, 2.7415e-06, 2.2235e-06, 8.9923e-06, 1.9485e-06,
        5.4262e-06, 7.2878e-02, 2.4951e-06, 9.8033e-02, 2.3491e-06, 1.9380e-06,
        8.1003e-02, 2.9386e-06, 2.4076e-06, 1.6577e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3590e-02, 1.7800e-07, 1.1254e-01,  ..., 3.2138e-07, 2.1508e-07,
        4.6926e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.3114e-01, 1.4258e-06, 5.6854e-02, 8.7492e-07, 6.2365e-07, 1.1227e-06,
        7.1467e-07, 5.0506e-07, 9.5662e-07, 3.1828e-02, 1.0379e-01, 9.0571e-07,
        1.0843e-01, 2.2593e-06, 8.9632e-07, 3.9267e-02, 7.1630e-07, 1.2955e-01,
        1.5874e-06, 4.7455e-02, 1.0756e-07, 6.7072e-07, 1.1471e-01, 3.1805e-07,
        9.5662e-07, 9.0263e-07, 1.5178e-06, 1.9274e-06, 2.2107e-06, 7.1467e-07,
        9.0263e-07, 8.1319e-07, 1.2507e-06, 6.7393e-07, 1.3152e-01, 2.2593e-06,
        7.4210e-07, 7.0372e-07, 6.4137e-07, 1.4794e-06, 1.6234e-06, 1.0472e-01,
        1.1424e-06, 2.8654e-07, 3.8089e-07, 8.1319e-07, 1.2311e-06, 1.2449e-01,
        1.7309e-07, 4.8234e-07, 1.4429e-06, 6.3468e-07, 1.8529e-06, 1.4712e-06,
        8.4193e-07, 8.2866e-07, 1.0702e-06, 1.0819e-01, 4.0089e-07, 7.7437e-07,
        1.5505e-06, 5.7031e-02, 1.1108e-01, 5.4145e-02, 5.0506e-07, 2.2593e-06,
        1.1597e-06, 9.0263e-07, 1.1413e-06, 3.1591e-07, 1.2346e-06, 5.2209e-02,
        2.1952e-06, 9.5662e-07, 1.1971e-06, 8.9155e-07, 1.3772e-06, 1.8338e-06,
        8.1337e-07, 1.1413e-06, 6.4137e-07, 5.9367e-02, 4.4844e-02, 1.0384e-06,
        3.2805e-06, 7.1467e-07, 8.4382e-07, 5.0052e-07, 2.2719e-06, 7.0006e-07,
        1.0224e-01, 4.9264e-02, 9.4457e-02, 1.5505e-06, 5.8910e-07, 9.0571e-07,
        9.0743e-07, 9.4337e-02, 5.8910e-07, 2.1952e-06, 7.7787e-07, 9.2346e-07,
        2.2593e-06, 1.2004e-01, 1.8338e-06, 2.1952e-06, 9.0570e-07, 5.0506e-07,
        2.7251e-06, 1.3050e-01, 5.8910e-07, 1.1816e-06, 7.4210e-07, 1.1971e-01,
        4.4846e-07, 1.2346e-06, 2.4617e-06, 2.2593e-06, 6.4137e-07, 8.4382e-07,
        6.7393e-07, 4.8072e-02, 8.5516e-07, 1.2346e-06, 1.1242e-06, 4.0573e-07,
        4.0492e-02, 6.7136e-07, 4.8628e-02, 4.0876e-02, 1.1242e-06, 1.0756e-07,
        5.1552e-07, 1.2346e-06, 5.3605e-07, 7.3726e-07, 5.8910e-07, 2.2733e-06,
        9.3899e-07, 1.7048e-06, 5.9915e-02, 1.8338e-06, 5.1394e-02, 9.5662e-07,
        5.8910e-07, 1.2713e-01, 1.8338e-06, 6.7073e-07, 4.8412e-07, 5.1230e-02,
        6.7136e-07, 1.0358e-01, 4.4391e-07, 1.2346e-06, 6.7136e-07, 5.3264e-07,
        1.4712e-06, 2.1278e-06, 6.7136e-07, 9.2346e-07, 9.0263e-07, 6.6511e-02,
        1.7048e-06, 1.0384e-06, 1.8338e-06, 4.8234e-07, 7.1467e-07, 2.2593e-06,
        1.4258e-06, 5.0273e-02, 9.5662e-07, 5.8910e-07, 6.7136e-07, 4.4846e-07,
        4.3148e-02, 4.4846e-07, 3.1628e-06, 1.3567e-06, 1.6234e-06, 1.0843e-01,
        1.4712e-06, 8.1636e-07, 1.2507e-06, 7.4210e-07, 6.7073e-07, 2.9264e-07,
        5.0779e-02, 2.1952e-06, 4.7030e-07, 5.8910e-07, 1.5505e-06, 1.0702e-06,
        1.8338e-06, 5.1542e-02, 9.6411e-02, 7.8558e-07, 6.7136e-07, 1.3772e-06,
        4.8234e-07, 8.1383e-02, 1.2346e-06, 5.3721e-02, 5.1552e-07, 9.4818e-02,
        2.8013e-06, 4.8234e-07, 7.1467e-07, 1.8338e-06, 7.4210e-07, 6.7393e-07,
        6.4137e-07, 1.2346e-06, 9.0263e-07, 1.0749e-06, 8.3483e-07, 5.7663e-07,
        2.1278e-06, 6.4137e-07, 9.1883e-07, 9.6826e-02, 1.7557e-01, 6.0065e-07,
        1.6234e-06, 5.6374e-02, 1.8338e-06, 5.2428e-02, 2.3396e-06, 3.2788e-07,
        7.1467e-07, 4.6717e-02, 4.4392e-07, 4.0821e-07, 4.0190e-07, 9.9629e-02,
        2.2719e-06, 4.8234e-07, 2.1879e-06, 3.2956e-02, 1.1247e-01, 1.0756e-07,
        9.9999e-07, 5.5581e-07, 9.9999e-07, 5.7819e-02, 1.4712e-06, 1.3816e-06,
        9.9528e-07, 4.6116e-02, 4.8234e-07, 1.1501e-06, 1.0756e-07, 8.4039e-07,
        1.1287e-01, 5.4873e-07, 5.3537e-07, 1.1413e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.7176e-06, 3.4238e-06, 1.8041e-06, 2.6989e-06, 3.6911e-06, 1.3092e-06,
        1.0391e-06, 2.6070e-06, 5.3490e-06, 3.0189e-06, 3.2246e-06, 2.8663e-06,
        2.3603e-06, 1.5986e-06, 2.2246e-06, 8.0655e-02, 4.5965e-06, 2.7551e-06,
        3.7964e-06, 1.8704e-06, 7.5413e-02, 3.4334e-06, 4.1375e-06, 3.5491e-06,
        4.9552e-06, 2.1791e-06, 4.5477e-06, 3.9527e-06, 4.0555e-06, 2.5236e-06,
        4.8910e-06, 2.2929e-06, 3.3056e-06, 6.4924e-06, 5.5764e-06, 1.9471e-06,
        2.8104e-02, 3.6954e-06, 1.4729e-06, 2.0337e-06, 2.0337e-06, 5.3253e-06,
        1.3731e-06, 2.1850e-06, 9.0186e-02, 1.1612e-06, 1.9061e-06, 4.1964e-02,
        3.4238e-06, 7.6035e-07, 8.8082e-07, 1.2360e-06, 7.7318e-02, 5.2550e-06,
        1.4576e-06, 4.5477e-06, 3.1144e-06, 3.6911e-06, 9.4067e-07, 3.4389e-06,
        2.4788e-06, 1.5647e-06, 4.0119e-06, 1.1796e-06, 5.7371e-06, 7.4128e-02,
        4.3866e-06, 1.9411e-07, 5.0295e-06, 5.0995e-02, 6.8782e-02, 2.6390e-02,
        1.4414e-02, 1.9061e-06, 9.7803e-07, 2.1912e-06, 2.1804e-06, 3.2841e-06,
        9.3202e-07, 7.2243e-06, 1.0363e-06, 1.8870e-06, 2.6659e-06, 6.8005e-02,
        5.4654e-06, 4.5477e-06, 2.0168e-06, 3.0504e-06, 6.8191e-02, 2.1979e-06,
        4.7829e-06, 3.7816e-06, 3.5491e-06, 2.9991e-06, 8.1289e-02, 2.7079e-06,
        7.3562e-06, 2.1979e-06, 3.8150e-06, 3.7816e-06, 6.2495e-06, 8.0940e-06,
        4.5186e-06, 2.1805e-06, 4.3077e-06, 9.6271e-07, 7.3856e-06, 2.3603e-06,
        1.5312e-06, 2.4433e-06, 4.9021e-06, 1.0391e-06, 6.2495e-06, 7.1965e-06,
        1.2091e-06, 5.5088e-06, 2.2929e-06, 3.2466e-06, 5.4244e-06, 3.0552e-06,
        2.2571e-06, 9.7803e-07, 2.2099e-06, 2.5236e-06, 3.2246e-06, 2.5490e-06,
        2.4433e-06, 1.8704e-06, 4.2467e-07, 3.5685e-06, 9.8575e-07, 1.5312e-06,
        1.7387e-06, 9.9783e-02, 4.4878e-06, 9.2561e-07, 3.8685e-06, 6.4651e-02,
        3.4991e-06, 1.7339e-06, 1.7339e-06, 3.2768e-06, 6.3475e-06, 8.8828e-07,
        9.6271e-07, 6.4440e-06, 1.4551e-06, 4.9080e-06, 4.3826e-06, 2.2572e-06,
        8.3259e-02, 1.4646e-06, 5.4302e-06, 1.5799e-06, 8.9960e-02, 2.0730e-06,
        2.2601e-06, 5.2550e-06, 3.1704e-06, 3.3876e-06, 3.1134e-06, 4.0221e-06,
        9.7803e-07, 1.9471e-06, 1.9801e-06, 4.0705e-06, 1.9059e-06, 9.2610e-02,
        1.7396e-06, 7.4860e-02, 1.9471e-06, 3.2377e-06, 1.7387e-06, 4.2141e-06,
        2.3991e-06, 3.2768e-06, 8.0907e-02, 2.9740e-06, 1.5986e-06, 2.3518e-06,
        2.8035e-06, 3.0552e-06, 3.6448e-06, 3.6586e-06, 7.1034e-06, 2.6768e-06,
        1.5922e-06, 8.8828e-07, 1.5138e-06, 1.7704e-06, 3.1589e-06, 2.6465e-02,
        2.4433e-06, 1.5543e-06, 3.2974e-06, 9.1295e-07, 2.9202e-06, 4.8368e-06,
        4.9643e-06, 2.2349e-06, 7.3562e-06, 2.4330e-06, 2.8902e-06, 1.3155e-06,
        1.2253e-06, 3.7879e-06, 3.4238e-06, 4.6841e-06, 1.4589e-06, 4.2080e-06,
        2.1791e-06, 4.4595e-06, 7.7920e-02, 2.8553e-06, 8.6763e-07, 3.7054e-06,
        5.4199e-06, 1.5535e-06, 1.4589e-06, 3.2544e-06, 1.7176e-06, 4.9080e-06,
        4.7829e-06, 2.1791e-06, 1.8498e-06, 3.7967e-06, 8.3284e-06, 3.3771e-06,
        3.6911e-06, 3.0072e-06, 1.0183e-06, 3.2466e-06, 2.4193e-06, 8.6080e-02,
        3.4238e-06, 3.0518e-06, 2.1958e-06, 3.1466e-06, 2.8482e-06, 2.5490e-06,
        5.4302e-06, 7.6035e-07, 7.1965e-06, 2.8035e-06, 2.7119e-06, 1.3296e-06,
        3.7795e-06, 2.5826e-06, 1.9059e-06, 2.7447e-06, 2.6321e-06, 9.7803e-07,
        8.0712e-02, 5.3427e-06, 5.0895e-06, 9.3562e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.5771e-02, 3.3414e-07, 2.4182e-02,  ..., 3.8378e-07, 2.7383e-07,
        1.3952e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.9496e-07, 5.9496e-07, 1.7455e-07, 8.7276e-07, 2.0737e-07, 1.7706e-06,
        1.2438e-06, 4.3680e-07, 2.9832e-07, 1.5573e-06, 1.7948e-06, 1.1718e-06,
        8.2950e-07, 1.0340e-06, 3.2978e-07, 2.9832e-07, 1.8090e-06, 1.8629e-07,
        7.6703e-07, 1.2438e-06, 6.9054e-07, 1.2992e-06, 7.1627e-07, 4.9482e-07,
        5.6543e-07, 1.7455e-07, 8.7289e-07, 1.8104e-06, 1.8151e-06, 7.1627e-07,
        1.1740e-06, 5.4286e-07, 4.9482e-07, 7.6703e-07, 4.3300e-07, 2.3498e-07,
        1.8104e-06, 7.1627e-07, 1.2438e-06, 1.7706e-06, 3.9564e-07, 1.5722e-07,
        3.9341e-07, 1.1718e-06, 4.9482e-07, 1.7455e-07, 3.6570e-07, 1.1838e-06,
        1.3172e-06, 4.9482e-07, 5.1475e-07, 7.6703e-07, 6.1971e-07, 1.1718e-06,
        1.8104e-06, 4.9482e-07, 7.6703e-07, 4.9482e-07, 6.9054e-07, 6.8989e-07,
        1.2992e-06, 1.2438e-06, 6.8989e-07, 1.7310e-06, 3.9564e-07, 1.8104e-06,
        1.0340e-06, 7.1627e-07, 6.1992e-07, 2.5351e-07, 2.9113e-07, 1.2992e-06,
        6.5363e-07, 5.4712e-07, 1.1718e-06, 7.1627e-07, 1.5573e-06, 4.3300e-07,
        5.6543e-07, 1.0340e-06, 7.1627e-07, 6.1992e-07, 3.6676e-07, 1.1718e-06,
        8.4008e-07, 4.2516e-07, 5.9496e-07, 1.5722e-07, 2.9113e-07, 1.8104e-06,
        6.5992e-07, 1.2438e-06, 5.2120e-07, 2.9113e-07, 5.9496e-07, 5.6292e-07,
        1.7455e-07, 1.7751e-06, 3.6570e-07, 1.4401e-06, 5.1290e-07, 5.1717e-07,
        1.7455e-07, 1.8104e-06, 5.6543e-07, 1.8104e-06, 1.8088e-06, 5.1475e-07,
        6.9054e-07, 2.7830e-07, 8.7289e-07, 3.8073e-07, 2.3498e-07, 2.9832e-07,
        5.1717e-07, 1.2438e-06, 9.6403e-07, 3.2978e-07, 1.1718e-06, 1.8104e-06,
        8.7289e-07, 3.9341e-07, 7.6703e-07, 7.6703e-07, 8.4008e-07, 1.8091e-06,
        1.0826e-06, 1.3158e-07, 2.6413e-07, 1.2438e-06, 5.4712e-07, 1.1838e-06,
        4.3680e-07, 4.3680e-07, 5.7946e-07, 4.3300e-07, 4.3680e-07, 1.8104e-06,
        6.1971e-07, 1.5722e-07, 4.3300e-07, 5.6543e-07, 7.6703e-07, 7.6703e-07,
        2.9832e-07, 4.9482e-07, 1.1052e-06, 8.5736e-07, 5.9496e-07, 8.7276e-07,
        6.0602e-07, 6.9054e-07, 1.4401e-06, 6.1971e-07, 1.8629e-07, 8.7437e-07,
        1.7706e-06, 2.5351e-07, 3.5868e-07, 3.9341e-07, 2.3871e-07, 5.1717e-07,
        7.7158e-07, 8.7289e-07, 2.6108e-07, 1.7455e-07, 3.9564e-07, 8.7289e-07,
        4.2415e-07, 4.3300e-07, 1.7956e-06, 5.9496e-07, 1.1838e-06, 1.1718e-06,
        5.6543e-07, 4.3680e-07, 3.9341e-07, 1.2438e-06, 5.9496e-07, 2.6413e-07,
        1.7706e-06, 7.0719e-07, 4.5468e-07, 8.4945e-07, 5.6543e-07, 3.9564e-07,
        8.7289e-07, 1.8104e-06, 8.2950e-07, 1.8104e-06, 7.1627e-07, 5.9496e-07,
        4.9482e-07, 3.6570e-07, 1.5722e-07, 8.5736e-07, 7.1627e-07, 1.6532e-06,
        2.9113e-07, 1.4401e-06, 5.1290e-07, 1.4401e-06, 6.1971e-07, 1.8104e-06,
        8.2950e-07, 1.1718e-06, 1.3172e-06, 4.9482e-07, 8.7276e-07, 5.1717e-07,
        5.1717e-07, 1.0910e-01, 5.1717e-07, 1.5627e-06, 4.3300e-07, 6.5992e-07,
        2.3498e-07, 3.6570e-07, 1.8104e-06, 7.0683e-07, 5.6292e-07, 5.4713e-07,
        9.0586e-07, 1.4401e-06, 2.0770e-07, 6.5992e-07, 5.9496e-07, 2.6108e-07,
        4.0145e-07, 6.9054e-07, 6.5363e-07, 1.2438e-06, 5.6543e-07, 5.9496e-07,
        1.8104e-06, 3.6570e-07, 1.7706e-06, 5.7946e-07, 1.1052e-06, 6.9188e-07,
        4.3300e-07, 7.1627e-07, 7.6703e-07, 3.7275e-07, 9.0187e-07, 2.9113e-07,
        6.0395e-07, 1.2438e-06, 1.2438e-06, 3.6570e-07, 2.9113e-07, 9.0586e-07,
        3.7044e-07, 4.9482e-07, 3.7535e-07, 1.8057e-06, 5.6543e-07, 4.4753e-07,
        1.8104e-06, 7.7263e-07, 2.9113e-07, 6.5363e-07, 8.7289e-07, 5.3998e-07,
        7.1627e-07, 7.4154e-07, 2.9113e-07, 3.9564e-07, 1.1052e-06, 1.5722e-07,
        3.8073e-07, 4.3300e-07, 3.9341e-07, 7.5658e-07, 1.8104e-06, 1.7455e-07,
        8.7276e-07, 1.1250e-06, 5.6543e-07, 1.2992e-06, 5.6543e-07, 7.6703e-07,
        1.2438e-06, 5.1290e-07, 8.7276e-07, 8.2950e-07, 9.7085e-07, 1.2438e-06,
        4.5468e-07, 3.9341e-07, 4.5287e-07, 3.6570e-07, 1.4401e-06, 1.5722e-07,
        5.9496e-07, 5.9496e-07, 9.1572e-07, 8.7276e-07, 5.9496e-07, 7.5538e-07,
        3.9341e-07, 2.9113e-07, 3.6570e-07, 2.5351e-07, 5.6543e-07, 6.5992e-07,
        5.6543e-07, 3.6570e-07, 3.9564e-07, 3.2615e-07, 4.3680e-07, 6.1971e-07,
        1.8104e-06, 5.1717e-07, 7.5659e-07, 7.0721e-07, 1.8109e-06, 8.7276e-07,
        1.7706e-06, 3.5603e-07, 1.8093e-06, 6.8989e-07, 6.1971e-07, 3.9593e-07,
        2.9113e-07, 1.7455e-07, 3.1371e-07, 3.9564e-07, 1.8104e-06, 7.0722e-07,
        6.1971e-07, 1.2438e-06, 3.9564e-07, 8.6840e-07, 2.5351e-07, 1.5144e-06,
        6.8431e-02, 5.9496e-07, 5.3182e-02, 7.6703e-07, 1.8104e-06, 1.8109e-06,
        5.4712e-07, 1.7455e-07, 6.5363e-07, 1.8104e-06, 7.6033e-07, 7.7263e-07,
        3.9593e-07, 1.0826e-06, 1.8104e-06, 1.7455e-07, 9.7081e-07, 4.9482e-07,
        3.6570e-07, 5.2829e-07, 1.5573e-06, 1.3518e-07, 1.8104e-06, 1.7706e-06,
        3.3377e-07, 9.0586e-07, 3.8135e-07, 1.2975e-06, 1.8104e-06, 3.6570e-07,
        3.7044e-07, 5.6543e-07, 1.1718e-06, 1.8104e-06, 1.7706e-06, 4.0145e-07,
        2.0771e-07, 7.1627e-07, 1.1838e-06, 1.1479e-01, 6.5546e-07, 3.6570e-07,
        1.1838e-06, 3.9341e-07, 1.1838e-06, 6.9054e-07, 8.4536e-07, 7.5658e-07,
        1.8104e-06, 8.4008e-07, 8.4008e-07, 1.4401e-06, 7.0868e-07, 1.7455e-07,
        2.7189e-07, 8.7276e-07, 1.5722e-07, 1.7706e-06, 4.9482e-07, 1.2438e-06,
        7.7263e-07, 7.0686e-07, 2.3498e-07, 6.1395e-07, 2.9113e-07, 6.7300e-08,
        5.9496e-07, 3.8073e-07, 6.1971e-07, 1.1838e-06, 1.7310e-06, 1.3338e-07,
        2.5351e-07, 9.2069e-07, 8.6840e-07, 1.2438e-06, 2.8536e-07, 1.8629e-07,
        1.8104e-06, 1.1250e-06, 4.9482e-07, 5.6543e-07, 3.7275e-07, 6.9054e-07,
        5.1717e-07, 5.6543e-07, 2.7830e-07, 6.1971e-07, 1.7706e-06, 4.3680e-07,
        1.1052e-06, 1.0826e-06, 1.7455e-07, 9.0586e-07, 4.3680e-07, 1.2438e-06,
        4.9482e-07, 8.5732e-07, 3.2978e-07, 6.5363e-07, 3.4586e-07, 1.7455e-07,
        2.6108e-07, 5.7946e-07, 3.9564e-07, 4.9482e-07, 5.1717e-07, 1.5573e-06,
        4.9482e-07, 1.1838e-06, 4.9482e-07, 3.9341e-07, 1.6813e-06, 1.8104e-06,
        2.3498e-07, 5.1717e-07, 1.3172e-06, 4.9482e-07, 9.0586e-07, 4.3680e-07,
        4.9482e-07, 7.5659e-07, 1.8629e-07, 1.7455e-07, 3.9341e-07, 2.6413e-07,
        5.6292e-07, 8.7276e-07, 3.8073e-07, 5.9496e-07, 5.9496e-07, 1.8104e-06,
        3.9341e-07, 8.5732e-07, 1.5573e-06, 1.1718e-06, 1.7310e-06, 1.7455e-07,
        8.7276e-07, 5.6543e-07, 3.9593e-07, 6.5363e-07, 4.3300e-07, 3.6570e-07,
        5.1717e-07, 3.7535e-07, 3.2978e-07, 1.7706e-06, 1.8109e-06, 3.8135e-07,
        1.8109e-06, 5.6895e-07, 6.9054e-07, 1.7455e-07, 3.9564e-07, 1.8094e-06,
        1.0826e-06, 7.1627e-07, 6.9054e-07, 5.4712e-07, 4.3300e-07, 1.2438e-06,
        1.5722e-07, 6.1971e-07, 1.1838e-06, 1.1675e-06, 4.9482e-07, 1.1052e-06,
        5.7946e-07, 1.4401e-06, 1.7455e-07, 1.0801e-06, 6.1971e-07, 1.4401e-06,
        4.9482e-07, 1.8057e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.1001e-06, 3.3960e-06, 7.7999e-06, 1.1197e-05, 1.0394e-05, 7.9143e-06,
        1.0337e-05, 6.3182e-06, 1.2888e-06, 3.0744e-06, 4.1393e-06, 8.5685e-07,
        6.5211e-06, 4.7552e-06, 6.2012e-06, 3.4496e-06, 7.5345e-06, 2.6935e-06,
        2.4340e-06, 2.6378e-06, 4.3754e-06, 3.8697e-06, 2.1780e-06, 7.7999e-06,
        4.0137e-06, 5.6469e-06, 3.2684e-06, 5.3027e-06, 5.2858e-06, 2.9325e-06,
        4.6141e-06, 6.3182e-06, 5.6742e-06, 4.8021e-06, 4.1393e-06, 8.3813e-06,
        4.2215e-06, 4.0137e-06, 5.3323e-06, 3.5925e-06, 2.2675e-06, 4.1393e-06,
        5.6742e-06, 4.1302e-06, 3.0702e-06, 4.8666e-06, 2.4363e-06, 4.9114e-06,
        8.5685e-07, 9.4605e-06, 2.3776e-06, 7.7999e-06, 5.2858e-06, 2.6378e-06,
        3.0702e-06, 4.4550e-06, 5.2181e-06, 8.3813e-06, 4.8666e-06, 4.1393e-06,
        5.9493e-06, 1.3959e-06, 3.3960e-06, 3.0691e-06, 4.5723e-06, 2.2064e-06,
        5.7513e-06, 3.8081e-06, 4.9114e-06, 4.8279e-06, 2.0807e-06, 4.0723e-06,
        8.3813e-06, 6.0367e-06, 4.1393e-06, 3.8686e-06, 9.7308e-06, 2.6739e-06,
        4.4550e-06, 2.8507e-06, 8.8858e-07, 3.5925e-06, 8.4790e-06, 1.4820e-06,
        2.7872e-06, 3.3960e-06, 4.5723e-06, 4.1393e-06, 3.2204e-06, 1.8336e-06,
        8.4790e-06, 5.2181e-06, 7.7999e-06, 2.5418e-06, 3.2748e-06, 8.3813e-06,
        6.3182e-06, 2.2064e-06, 4.0769e-06, 3.2684e-06, 1.0892e-05, 4.2215e-06,
        7.7999e-06, 5.3832e-06, 3.2684e-06, 3.1672e-06, 3.2684e-06, 5.1355e-06,
        1.7444e-06, 5.8195e-06, 8.4790e-06, 1.3125e-06, 4.0137e-06, 1.4657e-06,
        2.7467e-06, 4.1393e-06, 4.1613e-06, 2.8507e-06, 7.7999e-06, 4.4320e-06,
        5.4649e-06, 2.0807e-06, 1.4657e-06, 2.4513e-06, 2.3109e-06, 3.0744e-06,
        2.8507e-06, 3.0691e-06, 4.7487e-06, 2.2064e-06, 6.0407e-07, 1.0968e-02,
        4.8408e-06, 7.4274e-06, 6.6032e-06, 4.8332e-06, 5.0253e-06, 4.8666e-06,
        5.2248e-06, 3.0691e-06, 2.5418e-06, 4.2945e-06, 4.0137e-06, 4.1393e-06,
        8.4790e-06, 3.2684e-06, 4.4550e-06, 2.8629e-06, 7.7999e-06, 1.7671e-06,
        2.5418e-06, 4.1001e-06, 3.5925e-06, 4.1001e-06, 8.3813e-06, 1.4657e-06,
        4.2025e-06, 5.9493e-06, 1.4821e-06, 5.6469e-06, 7.2567e-06, 5.3130e-06,
        3.5925e-06, 8.3813e-06, 7.4274e-06, 2.2064e-06, 5.6469e-06, 4.1001e-06,
        2.8507e-06, 3.9360e-06, 2.4513e-06, 2.9325e-06, 3.2003e-06, 8.3813e-06,
        6.0675e-06, 4.1302e-06, 4.9114e-06, 4.4129e-06, 4.8439e-06, 4.8279e-06,
        1.8336e-06, 3.3960e-06, 2.3109e-06, 5.9344e-06, 4.1302e-06, 2.9325e-06,
        4.3554e-06, 1.3125e-06, 3.0691e-06, 2.8515e-06, 2.9927e-06, 1.6510e-06,
        4.1001e-06, 6.4105e-06, 3.8081e-06, 1.3125e-06, 2.8507e-06, 2.0510e-06,
        8.3813e-06, 5.3027e-06, 3.5925e-06, 4.1393e-06, 3.5752e-06, 6.6081e-06,
        2.9325e-06, 8.3813e-06, 2.2064e-06, 9.8978e-06, 3.8697e-06, 3.3960e-06,
        3.2684e-06, 6.6081e-06, 4.2025e-06, 2.1420e-06, 2.6818e-06, 3.3960e-06,
        8.3813e-06, 1.3374e-05, 6.2040e-06, 1.3125e-06, 3.9107e-06, 3.8697e-06,
        2.9325e-06, 5.2248e-06, 1.0045e-05, 3.1672e-06, 3.7980e-06, 3.2003e-06,
        7.7999e-06, 3.2003e-06, 8.0863e-06, 4.3553e-06, 2.9325e-06, 4.1302e-06,
        2.8507e-06, 1.5002e-05, 1.4657e-06, 7.2950e-06, 2.7467e-06, 1.0795e-05,
        8.5685e-07, 3.3960e-06, 1.7444e-06, 2.2064e-06, 5.6742e-06, 2.4513e-06,
        6.2416e-06, 2.2064e-06, 4.1001e-06, 7.7999e-06, 3.0691e-06, 8.3813e-06,
        2.6739e-06, 3.0702e-06, 2.0510e-06, 1.4657e-06, 3.2684e-06, 8.3813e-06,
        2.8507e-06, 4.8666e-06, 6.3182e-06, 3.0675e-06, 2.6935e-06, 2.6818e-06,
        2.7467e-06, 7.7999e-06, 2.6818e-06, 3.2748e-06, 1.7444e-06, 4.6118e-06,
        2.6935e-06, 8.3813e-06, 5.8193e-06, 2.3737e-06, 5.2858e-06, 2.2064e-06,
        4.1393e-06, 5.4649e-06, 2.2064e-06, 6.0407e-07, 2.7467e-06, 6.2012e-06,
        5.3073e-06, 7.7999e-06, 4.4550e-06, 5.5609e-07, 3.7980e-06, 5.3908e-06,
        5.6469e-06, 4.1302e-06, 3.5752e-06, 4.5723e-06, 5.6469e-06, 2.7035e-06,
        5.6469e-06, 1.3125e-06, 1.4998e-06, 4.1302e-06, 2.2064e-06, 2.8507e-06,
        3.8594e-06, 2.8629e-06, 3.3960e-06, 6.8255e-06, 4.1393e-06, 2.0807e-06,
        4.3554e-06, 4.8439e-06, 2.2064e-06, 4.2957e-06, 8.3813e-06, 1.0045e-05,
        4.8439e-06, 1.4820e-06, 1.0045e-05, 4.4550e-06, 2.5179e-06, 1.8336e-06,
        3.3444e-06, 2.8507e-06, 4.9885e-06, 4.1302e-06, 1.0394e-05, 8.4790e-06,
        7.8974e-07, 2.6935e-06, 7.2567e-06, 4.6193e-06, 3.5752e-06, 8.4790e-06,
        2.3109e-06, 4.3754e-06, 1.4820e-06, 1.5613e-06, 3.5925e-06, 4.1001e-06,
        3.2684e-06, 4.4550e-06, 1.0337e-05, 2.7467e-06, 4.8439e-06, 3.5752e-06,
        7.8974e-07, 4.7908e-06, 2.8507e-06, 7.2567e-06, 5.7136e-06, 3.5425e-06,
        7.7999e-06, 4.2215e-06, 1.8336e-06, 3.3960e-06, 1.0337e-05, 4.2025e-06,
        7.7868e-06, 6.3182e-06, 1.0795e-05, 2.9325e-06, 1.8336e-06, 2.8507e-06,
        3.9107e-06, 2.2064e-06, 4.2945e-06, 4.1302e-06, 1.4657e-06, 7.7868e-06,
        4.1613e-06, 2.6935e-06, 4.5723e-06, 4.9983e-06, 4.9114e-06, 5.2181e-06,
        3.5925e-06, 2.6935e-06, 3.3960e-06, 5.3073e-06, 7.6028e-06, 3.5425e-06,
        3.2684e-06, 3.2530e-06, 2.2064e-06, 3.1672e-06, 2.2064e-06, 4.2025e-06,
        2.9325e-06, 5.4272e-06, 4.0159e-06, 4.9114e-06, 3.8594e-06, 1.0394e-05,
        3.8449e-06, 4.1393e-06, 2.2914e-06, 2.8507e-06, 6.2040e-06, 2.2064e-06,
        5.2858e-06, 8.3813e-06, 6.2040e-06, 2.4340e-06, 2.4513e-06, 5.3130e-06,
        5.3027e-06, 2.6935e-06, 1.4657e-06, 5.2555e-06, 3.5925e-06, 1.1009e-05,
        4.1393e-06, 8.5685e-07, 4.6580e-06, 4.3754e-06, 1.0394e-05, 3.3960e-06,
        4.7487e-06, 4.0201e-06, 5.7136e-06, 1.0394e-05, 2.2064e-06, 5.4649e-06,
        7.8992e-06, 9.7308e-06, 8.4790e-06, 6.0407e-07, 3.5752e-06, 2.9325e-06,
        4.8666e-06, 3.5752e-06, 2.6739e-06, 2.9325e-06, 6.6032e-06, 4.0769e-06,
        6.3182e-06, 2.5418e-06, 3.8686e-06, 2.9325e-06, 2.7467e-06, 6.0367e-06,
        8.5685e-07, 2.8507e-06, 6.6032e-06, 4.5723e-06, 9.7308e-06, 4.3554e-06,
        2.7467e-06, 2.5697e-06, 7.7999e-06, 1.0795e-05, 3.2684e-06, 5.1355e-06,
        2.4279e-06, 9.7308e-06, 3.1672e-06, 1.8336e-06, 6.0407e-07, 6.6032e-06,
        4.1001e-06, 8.3813e-06, 4.5723e-06, 3.8594e-06, 5.3832e-06, 4.1393e-06,
        8.3813e-06, 6.0407e-07, 3.2748e-06, 4.3917e-06, 3.8594e-06, 4.1001e-06,
        4.1302e-06, 6.8255e-06, 2.7467e-06, 3.5425e-06, 4.0201e-06, 2.2064e-06,
        4.1393e-06, 1.8336e-06, 8.3005e-07, 4.1302e-06, 7.0940e-06, 2.2064e-06,
        2.2064e-06, 5.8195e-06, 2.2036e-06, 2.3109e-06, 7.7999e-06, 4.0159e-06,
        5.9460e-06, 4.2025e-06, 2.8629e-06, 3.8697e-06, 4.1001e-06, 2.4513e-06,
        2.6739e-06, 2.6935e-06, 2.8507e-06, 3.2684e-06, 1.7444e-06, 3.5925e-06,
        5.1355e-06, 2.0510e-06, 3.5925e-06, 4.1302e-06, 3.0702e-06, 3.5925e-06,
        1.0795e-05, 3.8697e-06, 8.3813e-06, 2.1420e-06, 3.2003e-06, 8.3813e-06,
        4.0159e-06, 3.8594e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.9141e-07, 6.9641e-07, 3.0435e-07,  ..., 7.6357e-07, 4.4050e-07,
        5.5780e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5314e-06, 2.0303e-06, 1.5260e-06,  ..., 1.6123e-06, 1.1765e-06,
        8.1984e-03], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.6657e-06, 2.4410e-06, 2.1186e-06, 2.3122e-06, 9.6973e-07, 2.3870e-06,
        4.5231e-06, 2.7684e-06, 1.2037e-06, 1.9952e-06, 5.7245e-06, 1.6503e-07,
        1.7535e-06, 1.6492e-06, 3.2376e-06, 3.1490e-06, 1.9135e-06, 2.2228e-06,
        1.8543e-06, 2.7461e-06, 1.0114e-06, 1.6657e-06, 2.9506e-06, 4.8411e-07,
        5.3990e-06, 1.8835e-06, 2.0896e-06, 9.6973e-07, 2.8365e-06, 1.5748e-06,
        2.4158e-06, 2.7099e-06, 5.0713e-06, 2.5351e-06, 3.0291e-06, 3.0547e-06,
        2.3037e-06, 4.1171e-06, 6.0828e-06, 3.5145e-06, 2.0445e-06, 1.1739e-06,
        2.7682e-06, 1.5059e-06, 2.3088e-06, 1.8702e-06, 4.3059e-06, 1.8491e-06,
        5.2162e-06, 1.4607e-06, 3.9425e-06, 2.8855e-06, 2.5460e-06, 2.9995e-06,
        3.5222e-06, 4.8454e-06, 4.7607e-06, 2.5012e-06, 3.2920e-06, 2.1961e-06,
        3.4582e-06, 5.1836e-06, 1.9952e-06, 3.4824e-06, 1.2876e-06, 2.2245e-06,
        4.7165e-06, 3.3081e-06, 6.1117e-06, 4.5656e-06, 1.2131e-06, 1.2576e-06,
        1.4254e-06, 1.7019e-06, 2.0458e-06, 2.9995e-06, 2.1662e-06, 1.9480e-01,
        1.5078e-06, 1.6352e-06, 1.3727e-06, 4.8219e-06, 4.8454e-06, 4.3405e-06,
        2.5964e-06, 2.9445e-06, 5.2162e-06, 2.0693e-06, 1.5631e-06, 5.1836e-06,
        3.3858e-06, 1.8725e-06, 2.8039e-06, 4.3405e-06, 1.5631e-06, 8.5031e-07,
        9.7725e-07, 1.9135e-06, 3.0969e-06, 2.2747e-06, 2.2245e-06, 9.9492e-07,
        4.5656e-06, 2.4410e-06, 3.9577e-06, 4.6122e-06, 1.2034e-06, 4.5656e-06,
        1.5468e-06, 4.0081e-06, 1.5490e-06, 2.8276e-06, 2.7924e-06, 3.2920e-06,
        4.3695e-06, 6.2075e-07, 3.4582e-06, 1.4452e-06, 5.7245e-06, 3.0926e-06,
        5.2162e-06, 4.3695e-06, 4.5656e-06, 3.2933e-06, 2.7110e-06, 1.6528e-06,
        1.5965e-06, 4.7632e-06, 6.0436e-06, 6.5636e-06, 2.0458e-06, 2.7099e-06,
        2.6516e-06, 2.7950e-06, 1.2876e-06, 1.7863e-06, 4.8452e-06, 2.5880e-07,
        1.2738e-06, 2.8037e-06, 1.9183e-06, 5.7245e-06, 1.9381e-06, 2.5012e-06,
        1.0114e-06, 4.4001e-06, 4.1966e-06, 1.3189e-07, 3.6446e-06, 3.9112e-06,
        1.5078e-06, 3.4001e-06, 3.1239e-06, 3.4425e-06, 5.5591e-07, 2.9466e-06,
        2.1186e-06, 1.3406e-06, 1.2291e-06, 4.9011e-06, 2.3195e-06, 1.5631e-06,
        2.8798e-06, 1.2836e-06, 5.2232e-06, 1.7019e-06, 1.6714e-06, 8.2947e-07,
        2.7367e-06, 2.8374e-06, 1.7877e-06, 4.1494e-06, 1.9791e-06, 2.0372e-06,
        1.6554e-06, 1.2258e-06, 2.6420e-06, 1.2017e-06, 4.8411e-07, 2.7099e-06,
        3.2443e-06, 2.3762e-06, 1.0976e-06, 1.5078e-06, 4.7383e-06, 1.5907e-06,
        1.7010e-06, 1.0156e-06, 3.8406e-06, 2.2492e-06, 5.7245e-06, 4.2062e-06,
        3.1457e-06, 7.3584e-06, 2.5470e-06, 2.2100e-06, 2.4410e-06, 4.3050e-06,
        1.0689e-06, 4.0128e-06, 1.1693e-06, 4.0963e-06, 2.8129e-06, 1.0021e-06,
        2.5012e-06, 2.4670e-06, 2.5964e-06, 5.5591e-07, 4.7112e-06, 2.5949e-06,
        3.0291e-06, 3.2933e-06, 3.0808e-06, 8.2159e-06, 4.0963e-06, 1.7988e-06,
        2.6516e-06, 6.5636e-06, 2.0652e-06, 2.9011e-06, 4.3405e-06, 1.4558e-06,
        2.2628e-01, 3.4211e-06, 3.7120e-06, 5.5524e-07, 1.3874e-06, 3.3851e-06,
        2.8798e-06, 6.5636e-06, 4.3695e-06, 1.4558e-06, 5.3472e-06, 1.4300e-06,
        4.4649e-06, 1.6503e-07, 1.9952e-06, 2.8798e-06, 2.0163e-06, 1.6827e-06,
        4.8411e-07, 2.2232e-06, 8.7101e-07, 1.6105e-06, 2.4410e-06, 2.9995e-06,
        8.2332e-07, 2.8271e-01, 9.4205e-07, 1.8760e-06, 1.2576e-06, 1.0753e-06,
        3.7405e-06, 2.9168e-06, 4.1971e-06, 2.9070e-06, 2.9895e-06, 2.5510e-06,
        3.9425e-06, 5.0545e-06, 3.0774e-06, 1.1946e-06, 4.1741e-06, 8.7667e-07,
        3.9970e-06, 4.7112e-06, 3.0808e-06, 1.5631e-06, 1.6657e-06, 1.9135e-06,
        4.5656e-06, 4.1098e-06, 2.2196e-06, 3.7153e-06, 3.1283e-06, 1.9381e-06,
        1.1978e-06, 4.7112e-06, 1.9952e-06, 2.4410e-06, 4.3722e-06, 5.3446e-07,
        7.6857e-07, 7.2831e-06, 2.2519e-06, 3.6446e-06, 9.7725e-07, 4.3914e-06,
        2.8539e-06, 7.8006e-06, 1.9743e-06, 1.6105e-06, 4.5656e-06, 1.5078e-06,
        2.7380e-06, 6.4194e-06, 2.3088e-06, 3.0015e-06, 4.1741e-06, 2.2526e-06,
        6.0828e-06, 1.9872e-06, 6.5636e-06, 5.1836e-06, 3.0480e-06, 1.0487e-06,
        2.8719e-06, 1.0114e-06, 2.5949e-06, 2.3243e-06, 4.1493e-06, 1.3341e-06,
        4.8411e-07, 2.1674e-06, 4.3695e-06, 1.1946e-06, 1.6554e-06, 1.3189e-06,
        1.6657e-06, 3.0356e-06, 2.9995e-06, 2.8569e-06, 1.8725e-06, 2.0860e-06,
        2.3408e-06, 1.9400e-01, 3.9975e-06, 2.4670e-06, 1.9743e-06, 1.8952e-06,
        1.7229e-06, 1.0137e-06, 2.4901e-06, 1.7634e-06, 2.0445e-06, 1.8474e-06,
        1.5304e-06, 8.8696e-07, 6.5636e-06, 4.8462e-06, 2.3356e-06, 1.3341e-06,
        2.0782e-06, 2.4410e-06, 1.2258e-06, 2.5617e-06, 3.1770e-06, 2.2370e-06,
        7.3574e-06, 4.3801e-06, 1.7010e-06, 2.4670e-06, 4.3801e-06, 3.7044e-06,
        6.3618e-06, 1.5512e-06, 4.2326e-06, 3.3219e-06, 1.4258e-06, 1.6936e-06,
        1.5512e-06, 3.9577e-06, 1.2873e-06, 1.2291e-06, 8.2715e-06, 1.5965e-06,
        1.2883e-06, 3.0981e-06, 4.7112e-06, 3.6568e-06, 4.3026e-06, 2.4901e-06,
        1.7863e-06, 7.7087e-07, 3.1585e-06, 1.5647e-06, 6.7373e-06, 3.5794e-06,
        2.5868e-06, 4.3405e-06, 2.2243e-06, 4.1499e-06, 1.7863e-06, 2.0400e-06,
        1.1306e-06, 2.6551e-06, 4.7112e-06, 3.5935e-06, 2.4406e-06, 1.7029e-06,
        2.4989e-06, 2.7099e-06, 2.6158e-06, 6.5636e-06, 7.3223e-06, 1.6657e-06,
        1.0733e-06, 2.0959e-06, 4.8454e-06, 2.5617e-06, 1.5896e-06, 5.5579e-07,
        2.0091e-06, 2.6883e-06, 3.1045e-06, 4.5656e-06, 3.4177e-06, 7.9044e-06,
        1.3874e-06, 1.0829e-06, 2.8746e-06, 1.2876e-06, 3.6446e-06, 4.1971e-06,
        7.6163e-06, 1.6105e-06, 1.7019e-06, 3.2443e-06, 3.9577e-06, 1.1978e-06,
        8.2332e-07, 7.6857e-07, 5.1836e-06, 4.7419e-06, 1.7863e-06, 2.5013e-06,
        1.6573e-06, 2.7234e-06, 9.2410e-07, 2.6557e-06, 8.2947e-07, 7.8006e-06,
        2.0896e-06, 3.7638e-06, 4.8454e-06, 4.1971e-06, 2.1281e-06, 2.8950e-01,
        7.8865e-07, 1.2061e-06, 6.5636e-06, 1.4799e-06, 1.3874e-06, 2.9260e-06,
        2.4026e-06, 4.1490e-06, 3.1910e-06, 3.8406e-06, 1.6657e-06, 2.9541e-06,
        3.1288e-06, 2.9520e-06, 2.0697e-06, 1.2460e-06, 1.3189e-07, 8.2947e-07,
        2.0946e-06, 5.7594e-06, 4.8804e-06, 1.7863e-06, 4.0051e-06, 4.4001e-06,
        1.4603e-06, 5.7245e-06, 9.6640e-07, 3.1283e-06, 2.1629e-06, 2.1341e-06,
        2.0188e-06, 1.5140e-06, 1.5171e-01, 3.1000e-06, 1.9135e-06, 3.9741e-06,
        3.4582e-06, 1.7229e-06, 3.7014e-06, 3.9429e-06, 2.8539e-06, 8.3036e-07,
        2.1791e-06, 1.3341e-06, 3.9047e-06, 2.4995e-06, 6.5595e-06, 1.9743e-06,
        1.7980e-06, 1.0689e-06, 6.5636e-06, 5.0548e-06, 3.2900e-06, 2.8719e-06,
        1.3341e-06, 9.4183e-07, 1.6657e-06, 4.9612e-06, 4.4491e-06, 3.2608e-06,
        5.7102e-06, 1.7171e-06, 3.5794e-06, 2.2370e-06, 4.6319e-06, 4.1499e-06,
        4.3695e-06, 2.9995e-06, 3.2444e-06, 2.2319e-06, 5.3717e-06, 2.5751e-06,
        2.8798e-06, 1.7185e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0573e-05, 1.2202e-05, 8.7874e-06, 5.6002e-06, 5.8135e-06, 1.5746e-05,
        6.9311e-07, 6.7634e-06, 4.2129e-06, 1.7390e-05, 9.4813e-06, 1.2179e-05,
        2.2741e-05, 1.7637e-05, 2.3150e-05, 1.0260e-05, 1.3167e-05, 6.2961e-06,
        1.3277e-05, 1.2238e-05, 8.3865e-06, 2.1811e-06, 6.3571e-06, 1.2672e-05,
        8.5222e-06, 1.9917e-05, 1.2116e-05, 1.8268e-05, 1.9221e-05, 1.3592e-05,
        1.5267e-05, 1.5331e-05, 1.2563e-05, 8.1927e-06, 1.2989e-05, 1.2707e-05,
        1.4997e-05, 2.0708e-05, 3.4332e-06, 8.6539e-02, 1.0535e-05, 3.6485e-06,
        1.3405e-05, 3.8598e-06, 1.3652e-05, 1.3799e-05, 9.0703e-06, 9.3913e-06,
        6.3673e-02, 1.1098e-05, 1.7971e-05, 1.4353e-05, 3.6061e-06, 7.8839e-06,
        1.6047e-05, 6.0845e-06, 1.5802e-05, 1.6583e-05, 9.4408e-06, 6.2226e-06,
        6.6690e-06, 6.3313e-06, 7.7335e-06, 1.1434e-05, 6.9334e-06, 4.6932e-06,
        6.7650e-06, 1.0640e-05, 5.8135e-06, 1.2582e-05, 1.1317e-05, 2.8588e-06,
        1.6348e-05, 1.1500e-05, 2.5648e-05, 8.9755e-07, 1.0563e-05, 8.5253e-06,
        1.5751e-05, 2.0272e-05, 8.0413e-06, 6.6690e-06, 1.5859e-05, 1.1585e-05,
        8.2272e-06, 1.8268e-05, 1.6925e-05, 4.1582e-06, 6.7950e-06, 4.9452e-06,
        5.9217e-06, 1.2519e-05, 5.9624e-06, 2.6249e-06, 7.6949e-06, 9.4827e-06,
        4.3096e-06, 4.0974e-06, 7.7044e-06, 6.5005e-06, 1.0934e-05, 7.7578e-06,
        4.8683e-06, 9.2685e-06, 9.0936e-06, 8.0806e-06, 5.7342e-06, 1.3948e-05,
        1.2806e-05, 8.9704e-06, 1.2219e-05, 7.6821e-06, 9.9322e-06, 7.9125e-06,
        9.4202e-02, 1.5936e-05, 5.8372e-06, 1.2806e-05, 5.8372e-06, 1.6740e-05,
        7.0980e-06, 5.9075e-06, 3.8189e-06, 1.2959e-05, 4.3096e-06, 1.1248e-05,
        1.5602e-05, 5.5251e-06, 1.0052e-05, 9.4827e-06, 8.4234e-06, 8.4523e-06,
        1.6914e-05, 6.3393e-06, 1.1434e-05, 1.4261e-05, 1.3652e-05, 1.2153e-05,
        1.2432e-05, 4.8884e-06, 1.3689e-05, 2.7931e-05, 6.5405e-06, 5.3829e-06,
        5.9559e-06, 5.9075e-06, 1.1317e-05, 8.1712e-06, 6.9360e-02, 1.9462e-05,
        4.0524e-06, 1.7290e-05, 1.5799e-05, 7.9554e-06, 9.0689e-06, 9.7157e-06,
        4.6472e-06, 1.0405e-05, 1.2071e-05, 4.1997e-06, 6.3886e-06, 9.1414e-06,
        8.4202e-06, 6.6677e-06, 1.1106e-05, 1.5025e-05, 1.1096e-05, 1.3652e-05,
        1.3898e-05, 7.9125e-06, 7.1410e-07, 2.3012e-05, 2.2355e-05, 7.8217e-06,
        9.5270e-06, 2.1931e-05, 1.4103e-05, 7.0066e-06, 1.1182e-05, 1.0080e-05,
        1.5691e-05, 2.3037e-05, 8.8885e-06, 1.1089e-05, 1.0116e-05, 3.0495e-06,
        1.3622e-05, 7.3286e-06, 2.0298e-05, 1.1855e-05, 1.0207e-05, 1.8817e-05,
        2.1342e-05, 1.1177e-05, 1.0633e-05, 6.1970e-06, 8.6134e-06, 4.9452e-06,
        6.6690e-06, 1.0995e-05, 1.3163e-05, 7.0980e-06, 6.3571e-06, 1.2497e-05,
        7.7044e-06, 1.1400e-05, 6.2544e-06, 1.1884e-05, 1.1855e-05, 1.1434e-05,
        9.8549e-06, 9.6803e-06, 9.6803e-06, 1.2616e-05, 1.4261e-05, 1.2006e-05,
        9.6803e-06, 1.4977e-05, 9.1257e-06, 1.6018e-05, 8.6134e-06, 5.9075e-06,
        8.3135e-06, 8.7939e-06, 1.0392e-05, 9.3193e-06, 2.3150e-05, 1.2006e-05,
        1.6738e-06, 1.8727e-05, 1.2080e-05, 4.7014e-06, 1.0169e-05, 1.5781e-05,
        1.0392e-05, 1.1642e-05, 1.5556e-05, 9.8122e-06, 4.7014e-06, 1.6472e-05,
        9.8698e-06, 9.4926e-06, 4.0122e-06, 1.1663e-05, 9.0275e-02, 2.3012e-05,
        1.4368e-05, 5.3718e-06, 3.3231e-06, 4.7014e-06, 2.3751e-06, 9.9581e-06,
        2.6598e-05, 8.5354e-06, 1.4091e-05, 5.0396e-06, 8.8957e-02, 1.0008e-01,
        7.9125e-06, 4.0294e-06, 6.7651e-06, 1.1759e-05, 7.1410e-07, 4.2080e-06,
        3.4593e-06, 1.0305e-05, 1.9228e-05, 1.2806e-05, 3.5898e-06, 1.4370e-05,
        3.3489e-06, 1.6550e-05, 6.7651e-06, 1.6584e-05, 3.9571e-06, 3.6920e-06,
        9.4827e-06, 1.7217e-06, 6.9334e-06, 1.9049e-05, 1.3671e-05, 6.7651e-06,
        1.2672e-05, 1.5993e-05, 9.4813e-06, 7.9470e-06, 8.5296e-06, 1.2279e-01,
        8.8744e-06, 1.0058e-05, 5.2902e-06, 1.2153e-05, 9.4802e-06, 1.3761e-05,
        9.4123e-06, 7.3546e-02, 6.9005e-06, 1.2202e-05, 6.3313e-06, 1.1384e-05,
        5.3592e-06, 6.8608e-06, 8.5222e-06, 1.5201e-05, 5.9075e-06, 5.3718e-06,
        4.6141e-06, 1.4006e-05, 6.0227e-06, 1.4888e-05, 7.3175e-06, 1.6608e-05,
        1.2697e-05, 5.9392e-06, 1.1423e-05, 1.2153e-05, 1.2006e-05, 6.6047e-06,
        5.0297e-06, 9.4813e-06, 6.6047e-06, 1.3265e-05, 1.5781e-05, 5.6002e-06,
        1.0155e-05, 1.4624e-05, 2.1307e-05, 5.5301e-06, 8.6444e-06, 9.5023e-02,
        7.4807e-02, 6.0540e-06, 6.6047e-06, 2.2886e-05, 4.6418e-06, 8.4095e-06,
        9.4827e-06, 1.1262e-06, 1.9510e-05, 1.4296e-05, 4.5814e-06, 1.0015e-05,
        1.0169e-05, 1.5172e-05, 7.6962e-06, 9.1352e-06, 1.1066e-05, 6.0404e-06,
        1.0421e-05, 6.8525e-06, 2.1812e-05, 1.6814e-05, 8.7125e-06, 7.0066e-06,
        1.5410e-05, 5.8372e-06, 6.0799e-06, 7.0256e-02, 6.1970e-06, 1.1002e-05,
        6.0473e-06, 1.4385e-05, 1.2652e-05, 4.0524e-06, 1.9228e-05, 2.0796e-05,
        1.3563e-05, 6.3141e-06, 1.7178e-05, 8.8571e-06, 2.3012e-05, 6.7651e-06,
        4.1997e-06, 1.1855e-05, 5.9075e-06, 6.9078e-06, 1.2519e-05, 1.2434e-05,
        1.2206e-05, 1.3921e-05, 5.8353e-06, 1.2941e-05, 5.3718e-06, 1.6819e-05,
        5.0396e-06, 8.2930e-06, 1.0881e-05, 1.2433e-05, 5.3718e-06, 8.5425e-06,
        1.4710e-05, 8.4234e-06, 6.2227e-06, 8.5307e-06, 1.3894e-05, 1.5781e-05,
        5.8135e-06, 9.7912e-06, 9.6111e-06, 4.0974e-06, 1.3447e-05, 1.2354e-05,
        9.6803e-06, 1.2959e-05, 2.7168e-05, 3.4394e-06, 8.7690e-06, 1.4981e-05,
        1.2364e-05, 1.3921e-05, 4.3130e-06, 9.4813e-06, 6.9348e-06, 1.5734e-05,
        3.6061e-06, 5.3718e-06, 6.2704e-06, 8.0313e-06, 7.3175e-06, 8.0257e-06,
        1.3405e-05, 1.0881e-05, 7.9408e-06, 7.8187e-06, 1.5357e-05, 6.3313e-06,
        1.0771e-05, 5.3710e-06, 5.2353e-06, 4.9452e-06, 1.0178e-05, 6.3313e-06,
        2.6916e-05, 2.1261e-05, 4.6639e-06, 1.1155e-05, 7.3668e-06, 1.1010e-05,
        1.3198e-05, 3.1664e-06, 5.9075e-06, 5.5650e-06, 6.9078e-06, 1.1182e-05,
        1.5781e-05, 1.2202e-05, 8.8511e-06, 5.5650e-06, 1.4780e-05, 5.6618e-06,
        5.1293e-06, 7.7334e-06, 8.5392e-06, 8.1368e-06, 1.1398e-01, 1.2314e-06,
        7.4406e-06, 2.7079e-06, 1.8577e-05, 1.1382e-05, 7.9125e-06, 1.7102e-05,
        1.1434e-05, 1.4082e-05, 9.4827e-06, 8.7026e-06, 1.5114e-05, 5.9075e-06,
        5.1165e-06, 1.1855e-05, 8.4510e-06, 2.3150e-05, 6.7650e-06, 8.2272e-06,
        2.1513e-05, 8.6444e-06, 2.0796e-05, 1.4419e-05, 6.7403e-06, 1.0389e-05,
        7.6645e-06, 2.8285e-06, 1.1872e-05, 3.5770e-06, 1.2959e-05, 7.8102e-06,
        2.1057e-05, 3.8390e-06, 8.6955e-06, 1.3761e-05, 1.2153e-05, 6.3571e-06,
        2.3150e-05, 1.3020e-01, 4.5814e-06, 1.5859e-05, 4.6953e-07, 6.6690e-06,
        4.6141e-06, 1.4296e-05, 4.5330e-06, 1.7443e-05, 5.5827e-06, 1.3652e-05,
        1.5984e-01, 5.3718e-06, 1.1349e-05, 9.4827e-06, 7.5827e-06, 1.1155e-05,
        1.8323e-05, 1.2354e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.5139e-07, 1.9362e-07, 6.1103e-07,  ..., 7.4919e-07, 1.0920e-06,
        9.2115e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.3446e-06, 5.2329e-06, 4.6360e-06, 2.8130e-06, 2.2625e-06, 8.5791e-06,
        2.8704e-06, 4.1828e-06, 1.9960e-06, 4.3534e-06, 1.8809e-06, 3.2142e-06,
        2.6984e-06, 4.5298e-06, 3.0352e-06, 7.4318e-06, 2.4899e-06, 5.0879e-06,
        5.1284e-06, 1.0789e-05, 5.8960e-06, 7.3478e-06, 3.2635e-06, 7.2959e-06,
        4.3161e-07, 3.0168e-06, 3.4185e-06, 3.0035e-06, 2.8886e-06, 4.2934e-06,
        5.8716e-06, 8.3241e-06, 3.6276e-06, 8.4289e-06, 6.1356e-06, 3.6899e-06,
        6.4020e-06, 2.5820e-06, 4.1756e-06, 2.2063e-06, 2.9507e-06, 2.1881e-06,
        1.4267e-06, 5.4184e-06, 9.8613e-06, 5.2386e-06, 7.4293e-06, 5.7040e-06,
        5.0103e-06, 4.2451e-06, 4.7777e-06, 4.5331e-06, 2.2637e-06, 2.5334e-06,
        5.8494e-06, 7.1983e-06, 5.6421e-06, 2.7644e-06, 4.8613e-06, 8.2732e-06,
        3.6814e-06, 4.0064e-06, 1.7500e-06, 4.9509e-06, 3.6655e-06, 2.1431e-06,
        3.0021e-06, 2.5682e-06, 2.3556e-06, 3.0548e-06, 5.7066e-06, 5.1965e-06,
        3.9454e-06, 2.2090e-06, 8.1414e-06, 3.2635e-06, 3.2635e-06, 1.2365e-06,
        5.4876e-06, 4.0596e-06, 9.4166e-07, 4.6367e-06, 5.5163e-06, 1.2833e-06,
        3.3135e-06, 2.8704e-06, 2.2063e-06, 3.0280e-06, 6.3015e-06, 1.5885e-06,
        2.8959e-06, 4.9243e-06, 4.5733e-06, 1.7326e-06, 2.7788e-06, 1.2710e-06,
        8.3470e-06, 2.9884e-06, 3.3244e-06, 1.6722e-06, 2.0651e-06, 2.5702e-06,
        2.8507e-06, 3.3408e-06, 1.3849e-06, 1.2762e-06, 5.0879e-06, 4.1401e-06,
        1.6799e-06, 3.3135e-06, 5.5642e-06, 9.0350e-06, 5.2229e-06, 4.7777e-06,
        8.8509e-06, 4.4161e-06, 3.7258e-06, 2.7552e-06, 5.3333e-06, 4.5282e-06,
        1.6396e-06, 3.9923e-06, 3.7557e-06, 1.3849e-06, 2.5880e-06, 8.3470e-06,
        3.0021e-06, 5.4637e-06, 3.6819e-06, 4.2995e-06, 2.6813e-06, 3.5002e-06,
        7.5590e-07, 5.0496e-06, 7.1624e-06, 7.7847e-06, 3.2585e-06, 2.5334e-06,
        1.6552e-06, 5.5285e-06, 2.2232e-06, 6.6529e-06, 3.6814e-06, 7.7409e-07,
        3.4815e-06, 4.8182e-06, 4.5331e-06, 4.3549e-06, 2.8130e-06, 1.0801e-05,
        2.7389e-06, 8.3470e-06, 7.4318e-06, 2.8499e-06, 7.2140e-06, 7.3478e-06,
        3.8567e-06, 2.1294e-06, 8.3512e-06, 9.0350e-06, 2.3928e-06, 3.1417e-06,
        3.8251e-06, 1.0789e-05, 1.8106e-06, 5.7309e-06, 6.0229e-06, 4.7842e-06,
        4.8894e-06, 7.5506e-06, 2.7208e-06, 7.1085e-06, 4.9509e-06, 4.5331e-06,
        3.3135e-06, 4.7777e-06, 6.1479e-06, 3.4840e-06, 5.7475e-06, 6.6650e-06,
        5.0496e-06, 5.3485e-06, 2.6813e-06, 1.3389e-06, 2.6591e-06, 3.1539e-06,
        2.0024e-06, 3.1539e-06, 3.2635e-06, 3.8518e-06, 4.9675e-06, 7.9560e-06,
        3.5541e-06, 3.2635e-06, 2.0251e-06, 4.1061e-06, 5.7320e-06, 5.0496e-06,
        2.3474e-06, 5.3879e-06, 1.9211e-06, 3.5224e-06, 1.4473e-06, 5.4776e-06,
        3.1487e-06, 5.7814e-06, 1.3849e-06, 7.1085e-06, 4.5270e-06, 3.6396e-06,
        7.4656e-06, 2.7208e-06, 5.8155e-06, 5.8678e-07, 2.4671e-06, 4.2125e-06,
        3.3135e-06, 2.7644e-06, 8.1934e-06, 5.4953e-06, 2.8886e-06, 2.9732e-06,
        5.0987e-06, 3.6819e-06, 2.5658e-06, 3.2585e-06, 1.2365e-06, 2.3556e-06,
        2.6813e-06, 5.6790e-06, 1.5824e-06, 3.9127e-06, 5.6720e-06, 4.7885e-06,
        1.0801e-05, 5.8716e-06, 1.0218e-06, 3.4127e-06, 6.8345e-06, 7.4656e-06,
        3.1539e-06, 5.4876e-06, 2.5841e-06, 2.4836e-06, 4.8607e-06, 3.9454e-06,
        9.0350e-06, 1.3999e-06, 2.4837e-06, 5.2229e-06, 2.4212e-06, 1.8973e-06,
        4.4388e-06, 4.4981e-06, 3.6819e-06, 1.5131e-06, 2.4836e-06, 1.3849e-06,
        9.0350e-06, 2.2615e-06, 2.4551e-06, 5.8960e-06, 2.2615e-06, 4.6443e-06,
        5.5549e-06, 3.2040e-06, 2.9415e-06, 1.9528e-06, 3.0279e-06, 6.2350e-06,
        3.6814e-06, 5.0496e-06, 1.4583e-06, 5.4726e-06, 2.4212e-06, 1.9838e-06,
        2.3556e-06, 5.4184e-06, 4.1816e-06, 2.0052e-06, 3.5663e-06, 3.4285e-06,
        4.4968e-06, 6.4133e-06, 3.2235e-06, 4.3130e-06, 6.6529e-06, 2.2232e-06,
        3.6819e-06, 2.5888e-06, 1.7500e-06, 1.9528e-06, 4.3194e-06, 3.8281e-06,
        9.8145e-07, 1.6722e-06, 3.7095e-06, 2.3098e-06, 3.3318e-06, 6.5458e-06,
        4.5331e-06, 5.8155e-06, 3.8448e-06, 3.9956e-06, 6.0274e-06, 4.7842e-06,
        3.3408e-06, 5.0501e-06, 1.5264e-06, 6.6784e-06, 3.6385e-06, 5.4184e-06,
        3.2235e-06, 3.7308e-06, 2.8821e-06, 4.0120e-06, 3.6814e-06, 2.6193e-06,
        3.5541e-06, 4.1810e-06, 5.4776e-06, 4.6293e-06, 3.6819e-06, 1.5305e-06,
        4.8443e-06, 5.2229e-06, 8.7984e-06, 2.5338e-06, 3.7557e-06, 2.8844e-06,
        4.8522e-06, 1.1389e-07, 4.8182e-06, 2.2615e-06, 6.0346e-06, 1.1596e-06,
        1.9234e-06, 2.5888e-06, 3.9559e-06, 3.7557e-06, 9.0350e-06, 4.5312e-06,
        3.4887e-06, 4.5306e-06, 7.9048e-06, 3.1539e-06, 5.1862e-06, 5.8716e-06,
        5.3273e-06, 2.8821e-06, 2.3267e-06, 3.5019e-06, 3.4285e-06, 1.5824e-06,
        4.5664e-07, 1.3849e-06, 3.0555e-06, 2.3412e-06, 4.6909e-06, 2.2063e-06,
        4.0475e-06, 5.2229e-06, 4.6443e-06, 2.3810e-06, 5.9245e-06, 2.6233e-06,
        2.2615e-06, 9.0125e-07, 3.6101e-06, 5.0496e-06, 4.2235e-06, 2.4194e-06,
        6.0816e-06, 3.6819e-06, 4.9914e-06, 1.5824e-06, 3.2635e-06, 5.5467e-06,
        5.2229e-06, 5.2324e-06, 6.6529e-06, 6.1502e-06, 3.2635e-06, 4.2960e-07,
        3.6819e-06, 3.5699e-06, 5.0496e-06, 1.8635e-06, 3.3135e-06, 5.0987e-06,
        3.8184e-06, 3.9800e-06, 6.6529e-06, 5.1892e-06, 3.6820e-06, 1.8826e-06,
        4.3834e-06, 1.8703e-06, 2.7653e-06, 4.2594e-06, 3.9978e-06, 5.8547e-06,
        4.4981e-06, 3.1685e-06, 3.8251e-06, 3.3919e-06, 3.5705e-06, 1.7164e-06,
        2.5841e-06, 3.8906e-06, 4.7741e-06, 2.0024e-06, 2.4836e-06, 8.8507e-06,
        1.2710e-06, 5.8716e-06, 3.4783e-06, 6.3510e-06, 3.7262e-06, 2.4899e-06,
        4.0314e-06, 3.3495e-06, 6.0068e-06, 3.4285e-06, 4.2877e-06, 3.0855e-06,
        3.2044e-06, 3.6010e-06, 4.5298e-06, 2.4664e-06, 7.9097e-06, 8.2416e-06,
        3.6819e-06, 7.9048e-06, 2.2090e-06, 3.6819e-06, 8.2389e-06, 5.8960e-06,
        5.2229e-06, 5.8590e-06, 3.3562e-06, 3.8251e-06, 3.4285e-06, 2.6239e-06,
        2.5405e-06, 4.6610e-06, 3.7936e-06, 3.4739e-06, 2.1492e-06, 3.4285e-06,
        4.0470e-06, 1.8219e-06, 4.6361e-06, 4.8510e-06, 2.8073e-06, 6.2530e-06,
        5.2596e-07, 3.8248e-06, 4.8443e-06, 3.8184e-06, 1.2365e-06, 1.8809e-06,
        4.6816e-06, 1.3849e-06, 8.1934e-06, 3.5663e-06, 3.9946e-06, 3.4319e-06,
        6.3178e-06, 2.8886e-06, 1.3303e-06, 5.2037e-06, 2.3430e-06, 3.6392e-06,
        5.4184e-06, 4.6762e-06, 3.2178e-06, 4.8765e-06, 4.9914e-06, 5.4363e-06,
        3.2594e-06, 3.8251e-06, 5.3333e-06, 5.3333e-06, 1.4267e-06, 4.4270e-06,
        5.2229e-06, 3.4285e-06, 8.8507e-06, 2.6801e-06, 3.6276e-06, 2.6250e-06,
        8.7984e-06, 2.1433e-06, 2.0131e-06, 3.4098e-06, 3.3035e-06, 2.5211e-06,
        3.2524e-06, 9.0350e-06, 3.2635e-06, 1.6898e-06, 3.4285e-06, 4.3460e-06,
        4.2811e-06, 3.9949e-06, 7.4722e-06, 2.2232e-06, 6.6678e-06, 3.6737e-06,
        1.9528e-06, 2.8221e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1567e-05, 1.0713e-05, 5.6021e-06, 1.0162e-05, 9.8720e-06, 7.0196e-06,
        7.1685e-06, 8.9902e-06, 7.5838e-06, 2.4942e-06, 6.4364e-06, 1.2475e-05,
        1.0527e-05, 3.1388e-06, 8.7385e-06, 7.1685e-06, 6.0038e-06, 1.5789e-05,
        8.4666e-06, 1.7023e-05, 1.7244e-05, 7.0739e-06, 5.1034e-06, 7.3167e-06,
        7.6171e-06, 8.2185e-06, 1.7603e-05, 7.0196e-06, 8.4666e-06, 1.8959e-06,
        5.3554e-06, 7.1890e-06, 5.0017e-06, 8.7658e-06, 8.8801e-06, 4.8109e-06,
        1.2235e-05, 1.2323e-05, 7.7610e-06, 1.7801e-05, 3.1047e-06, 2.0930e-05,
        1.1750e-05, 1.3209e-05, 1.2869e-05, 1.4491e-05, 9.1228e-06, 1.5597e-05,
        1.8959e-06, 5.4677e-06, 8.6430e-06, 8.2050e-06, 4.5511e-06, 8.5907e-06,
        1.9480e-05, 1.1872e-05, 5.7557e-06, 5.0017e-06, 5.1763e-06, 7.2635e-06,
        8.7658e-06, 5.7952e-06, 9.9086e-06, 6.0449e-06, 1.3478e-05, 2.1694e-05,
        1.0807e-05, 1.8676e-05, 1.0582e-05, 2.1082e-05, 3.3583e-06, 9.3772e-06,
        3.3277e-06, 5.7457e-06, 1.0263e-05, 3.0989e-06, 1.1381e-05, 1.7940e-05,
        5.6026e-06, 8.4666e-06, 7.1871e-06, 3.7544e-06, 3.6279e-06, 1.3505e-05,
        8.1128e-06, 1.0106e-05, 9.1639e-06, 9.3772e-06, 2.8098e-06, 1.0458e-05,
        9.2989e-06, 4.5016e-06, 1.9480e-05, 9.3504e-07, 1.0564e-05, 1.4975e-05,
        6.6565e-06, 4.5511e-06, 1.4565e-05, 9.7375e-06, 4.2299e-06, 5.6517e-06,
        1.5259e-05, 5.1852e-06, 7.1695e-06, 5.6517e-06, 4.0720e-06, 1.1410e-05,
        1.6324e-05, 1.9480e-05, 1.2433e-05, 2.1082e-05, 1.8061e-05, 6.3289e-06,
        8.9249e-06, 7.6171e-06, 1.0499e-05, 4.3210e-06, 1.6591e-05, 7.9827e-06,
        9.6667e-06, 7.1885e-06, 7.2922e-06, 1.8319e-05, 3.2286e-06, 2.5879e-06,
        1.1990e-05, 8.6507e-06, 6.7536e-06, 7.0986e-06, 5.6972e-06, 1.1886e-05,
        5.1889e-06, 8.6431e-06, 5.3847e-06, 1.5910e-05, 6.2166e-06, 1.1241e-05,
        7.5838e-06, 1.1221e-05, 2.2198e-06, 1.2073e-05, 1.0245e-05, 7.8311e-06,
        8.1693e-07, 1.6324e-05, 5.5644e-06, 1.8319e-05, 5.7952e-06, 5.6698e-06,
        9.2378e-06, 8.5288e-06, 2.1082e-05, 4.5986e-06, 1.2190e-05, 1.1230e-05,
        1.4470e-05, 7.5838e-06, 7.9614e-06, 5.2774e-06, 1.4095e-05, 1.2409e-05,
        4.0720e-06, 1.0821e-05, 1.4144e-05, 1.1605e-05, 6.7029e-06, 7.6171e-06,
        9.3634e-06, 1.0263e-05, 2.2198e-06, 1.1095e-05, 1.1165e-05, 5.1107e-06,
        1.8625e-05, 9.1102e-06, 1.3135e-05, 5.7952e-06, 3.0990e-06, 5.0375e-06,
        7.9280e-06, 1.1671e-05, 7.6344e-06, 7.7610e-06, 4.8109e-06, 9.6399e-06,
        4.3210e-06, 5.6039e-06, 1.0858e-05, 4.2262e-06, 9.7375e-06, 1.1786e-05,
        8.2730e-06, 1.6324e-05, 1.9661e-05, 1.1406e-05, 1.1398e-05, 9.6369e-06,
        2.3355e-05, 8.4846e-06, 1.0987e-05, 9.2815e-06, 1.5004e-05, 1.2983e-05,
        1.7358e-05, 1.3942e-05, 8.9868e-06, 5.2864e-06, 5.8131e-06, 9.3633e-06,
        4.5511e-06, 7.4465e-06, 7.0691e-06, 7.9827e-06, 9.3306e-06, 9.1777e-06,
        9.1639e-06, 5.8619e-06, 4.2930e-06, 7.4012e-06, 5.6106e-06, 2.0539e-05,
        4.5016e-06, 1.2584e-05, 8.5907e-06, 1.0777e-05, 9.3306e-06, 8.4847e-06,
        1.0573e-05, 5.0017e-06, 1.0518e-05, 7.0554e-06, 1.0518e-05, 8.5006e-06,
        6.0120e-06, 4.8109e-06, 3.8856e-06, 1.1162e-05, 7.5071e-06, 2.2198e-06,
        8.2185e-06, 8.4376e-06, 5.5161e-06, 9.4171e-06, 6.1825e-06, 3.3277e-06,
        6.0418e-06, 1.8655e-05, 1.1175e-05, 1.0229e-05, 1.1133e-05, 1.3808e-05,
        4.8109e-06, 8.9902e-06, 7.6073e-06, 7.9280e-06, 4.2520e-06, 1.3294e-05,
        1.7047e-05, 8.6114e-06, 1.0263e-05, 1.2182e-05, 7.8851e-06, 6.9926e-06,
        6.5813e-06, 4.2566e-06, 3.2286e-06, 1.3408e-05, 1.1786e-05, 6.7471e-06,
        1.0221e-05, 2.0930e-05, 5.9956e-06, 3.5177e-06, 8.9249e-06, 1.2710e-05,
        6.0584e-06, 7.6171e-06, 4.5977e-06, 8.4666e-06, 1.7165e-05, 1.1647e-05,
        4.3878e-06, 1.1581e-05, 7.8464e-06, 8.8573e-06, 8.6431e-06, 7.9595e-06,
        8.6431e-06, 1.0518e-05, 1.3074e-05, 8.5907e-06, 8.6114e-06, 1.0263e-05,
        9.2989e-06, 4.3956e-06, 9.1639e-06, 2.7995e-06, 1.0635e-05, 2.5208e-05,
        1.3455e-05, 1.5431e-05, 7.0196e-06, 1.0458e-05, 9.7375e-06, 9.3230e-06,
        8.1120e-06, 2.2198e-06, 9.3256e-06, 4.2566e-06, 1.5011e-05, 1.0458e-05,
        6.0120e-06, 5.3605e-06, 5.3875e-06, 7.2917e-06, 1.7965e-06, 2.5587e-06,
        5.3801e-06, 5.5098e-06, 1.0458e-05, 1.0010e-05, 7.4012e-06, 1.1175e-05,
        5.0375e-06, 3.5131e-06, 3.2943e-06, 1.5597e-05, 7.9594e-06, 4.3956e-06,
        8.1079e-06, 2.2563e-06, 3.6855e-06, 4.8194e-06, 1.5360e-05, 6.0405e-06,
        7.5101e-06, 1.3861e-05, 1.8895e-05, 1.2648e-05, 1.6497e-05, 4.2930e-06,
        4.3652e-06, 4.2930e-06, 1.1930e-06, 4.2937e-06, 5.8131e-06, 1.0987e-05,
        8.1128e-06, 1.2667e-05, 6.2269e-06, 2.2198e-06, 1.3764e-05, 1.3307e-05,
        2.1082e-05, 1.3573e-05, 7.1332e-06, 1.3633e-05, 1.0777e-05, 1.2980e-05,
        1.0582e-05, 4.1130e-06, 1.2648e-05, 8.6507e-06, 1.3824e-05, 1.5325e-06,
        1.1166e-05, 1.0080e-05, 8.4886e-06, 1.3633e-05, 5.0017e-06, 5.3252e-06,
        1.2869e-05, 1.8959e-06, 6.4133e-06, 9.4707e-06, 7.0207e-06, 9.9164e-06,
        6.8514e-06, 7.0250e-06, 7.0667e-06, 1.8710e-05, 1.7165e-05, 3.2286e-06,
        1.9431e-05, 6.0405e-06, 7.9827e-06, 1.6605e-05, 1.1245e-05, 7.9614e-06,
        7.4746e-06, 1.4660e-05, 2.3171e-05, 9.7375e-06, 3.2683e-06, 2.0890e-06,
        1.2004e-05, 6.7272e-06, 8.5907e-06, 1.3573e-05, 3.7532e-06, 6.8702e-06,
        4.3956e-06, 9.3632e-06, 1.4255e-05, 1.1931e-05, 6.7502e-06, 3.6855e-06,
        1.5952e-05, 3.7532e-06, 1.0582e-05, 5.0017e-06, 7.6344e-06, 5.6517e-06,
        8.5907e-06, 8.9902e-06, 2.1022e-05, 4.8194e-06, 1.3942e-05, 1.1701e-05,
        4.8109e-06, 5.0017e-06, 1.3391e-05, 6.0405e-06, 7.0760e-06, 1.1212e-05,
        9.9086e-06, 1.0608e-05, 1.1151e-05, 1.0227e-05, 1.0190e-05, 5.2774e-06,
        6.4374e-06, 1.8959e-06, 8.5869e-06, 9.8164e-06, 1.3281e-05, 1.0807e-05,
        1.0089e-05, 7.6171e-06, 8.6956e-06, 8.6918e-06, 9.4974e-06, 2.8098e-06,
        8.5402e-06, 5.0017e-06, 1.2106e-05, 8.2924e-06, 1.7448e-05, 7.6171e-06,
        1.1302e-05, 1.2475e-05, 6.7573e-06, 5.9033e-06, 1.2869e-05, 8.9902e-06,
        1.1639e-05, 1.8959e-06, 4.0438e-06, 1.0499e-05, 5.2774e-06, 6.7272e-06,
        5.1913e-06, 1.2818e-05, 1.8319e-05, 2.0579e-05, 6.2269e-06, 7.9289e-06,
        6.8514e-06, 8.1199e-06, 1.7952e-06, 1.9689e-05, 1.9480e-05, 1.6324e-05,
        7.2635e-06, 1.4134e-05, 1.3336e-05, 2.2198e-06, 7.1332e-06, 8.4307e-06,
        6.4925e-06, 2.0539e-05, 1.5371e-06, 3.6978e-06, 1.9699e-05, 5.1792e-06,
        3.5653e-06, 4.6931e-06, 2.0539e-05, 3.2943e-06, 1.8800e-05, 1.0102e-05,
        1.0263e-05, 1.8771e-05, 2.0513e-05, 7.4112e-06, 7.2237e-06, 5.0017e-06,
        3.6632e-06, 4.9425e-06, 1.2744e-05, 1.2424e-05, 1.7801e-05, 8.2953e-06,
        7.1086e-06, 1.2252e-05, 4.1130e-06, 7.2653e-06, 8.7677e-06, 1.2004e-05,
        1.4963e-05, 1.1151e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.4940e-07, 1.6584e-06, 6.9290e-07,  ..., 6.7527e-07, 1.3346e-06,
        1.1013e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2249, 1.1943, 1.1179, 1.2696, 1.2392, 1.6938, 1.7723, 1.3163, 1.2158,
        1.2942, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333432674408, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.390625, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7534722089767456, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 0.75, 1.0, 0.75, 1.0, 1.0, 0.7517361044883728, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 0.7552083134651184]

 sparsity of   [0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.765625, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.8125, 0.796875, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.765625, 1.0, 0.796875, 1.0, 1.0, 0.765625, 0.765625, 0.8125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.8125, 0.78125, 1.0, 1.0, 0.796875, 0.78125, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.8125, 0.8125, 0.8125, 0.78125, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.765625, 0.828125, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.765625, 0.78125, 0.78125, 0.765625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 0.765625, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.78125, 1.0, 0.796875, 1.0, 1.0, 0.765625, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 0.78125, 0.8125, 0.765625, 1.0, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 0.8125, 1.0, 0.78125, 1.0, 0.796875, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.78125, 0.78125, 1.0, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.8125, 0.78125, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.796875, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.78125, 0.78125, 1.0, 0.78125, 0.796875, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.765625, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.765625, 1.0, 0.765625, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.78125]

 sparsity of   [1.0, 1.0, 0.34375, 1.0, 1.0, 0.40625, 0.359375, 0.390625, 1.0, 0.359375, 1.0, 0.40625, 1.0, 1.0, 1.0, 0.375, 0.34375, 0.34375, 0.34375, 0.359375, 1.0, 0.359375, 0.375, 0.34375, 0.34375, 0.34375, 0.375, 1.0, 0.34375, 1.0, 0.359375, 0.34375, 1.0, 0.375, 0.375, 0.515625, 0.359375, 0.390625, 0.359375, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.40625, 0.390625, 0.34375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.375, 1.0, 0.375, 0.359375, 0.375, 0.40625, 1.0, 1.0, 0.359375, 0.375, 0.390625, 1.0, 1.0, 0.359375, 0.375, 1.0, 0.359375, 0.359375, 1.0, 0.390625, 0.375, 0.359375, 0.359375, 0.4375, 0.40625, 0.34375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.375, 0.421875, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.34375, 0.34375, 0.40625, 0.359375, 1.0, 1.0, 0.375, 0.390625, 1.0, 0.359375, 0.359375, 0.375, 0.359375, 0.375, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.390625, 0.359375, 1.0, 0.421875, 0.375, 1.0, 0.375, 0.359375, 1.0, 0.34375, 0.390625, 0.359375, 0.40625, 0.375, 1.0, 0.34375, 0.390625, 1.0, 1.0, 0.390625, 0.359375, 1.0, 0.375, 0.34375, 1.0, 0.390625, 1.0, 0.40625, 1.0, 0.40625, 0.421875, 1.0, 0.390625, 1.0, 0.359375, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.359375, 0.375, 0.375, 0.359375, 1.0, 0.375, 0.359375, 0.375, 0.359375, 0.375, 0.375, 0.359375, 1.0, 0.421875, 0.359375, 1.0, 0.390625, 1.0, 0.359375, 1.0, 0.40625, 0.34375, 0.375, 0.359375, 0.375, 1.0, 1.0, 0.34375, 1.0, 1.0, 0.359375, 1.0, 0.359375, 0.34375, 0.375, 0.359375, 0.40625, 1.0, 0.375, 0.375, 1.0, 1.0, 0.40625, 1.0, 1.0, 0.59375, 0.34375, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.375, 0.34375, 1.0, 1.0, 0.40625, 0.375, 0.375, 0.359375, 1.0, 0.34375, 0.375, 1.0, 0.40625, 0.390625, 0.40625, 1.0, 1.0, 0.359375, 0.359375, 0.359375, 1.0, 0.40625, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.40625, 1.0, 0.375, 1.0, 0.359375, 0.375, 0.359375, 1.0, 0.359375, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.359375, 0.453125]

 sparsity of   [1.0, 1.0, 0.3515625, 1.0, 0.34765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3515625, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.34765625, 1.0, 1.0, 0.3671875, 1.0, 1.0, 0.33984375, 0.34765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3359375, 0.34375, 1.0, 1.0, 0.35546875, 1.0, 0.37109375, 0.3671875, 0.35546875, 1.0, 1.0, 0.35546875, 1.0, 1.0, 0.33984375, 1.0, 1.0, 0.3515625, 1.0, 1.0, 0.34765625, 1.0, 0.34765625, 0.3359375, 1.0, 1.0, 1.0]

 sparsity of   [0.6388888955116272, 1.0, 1.0, 1.0, 0.6527777910232544, 0.6371527910232544, 0.6440972089767456, 1.0, 1.0, 1.0, 1.0, 0.6284722089767456, 1.0, 0.65625, 1.0, 1.0, 0.663194477558136, 0.640625, 1.0, 1.0, 1.0, 0.6545138955116272, 0.6302083134651184, 1.0, 1.0, 1.0, 0.6302083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6354166865348816, 0.6649305820465088, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.647569477558136, 0.6371527910232544, 1.0, 0.6354166865348816, 1.0, 0.6527777910232544, 1.0, 1.0, 1.0, 0.6215277910232544, 0.640625, 1.0, 1.0]

 sparsity of   [0.65625, 1.0, 0.640625, 0.65625, 1.0, 1.0, 0.65625, 0.671875, 0.671875, 1.0, 0.65625, 0.640625, 1.0, 0.65625, 1.0, 0.625, 0.65625, 0.640625, 0.65625, 1.0, 1.0, 0.671875, 0.65625, 1.0, 1.0, 0.625, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 0.671875, 1.0, 1.0, 0.671875, 0.65625, 0.671875, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 1.0, 0.65625, 0.640625, 1.0, 0.640625, 0.65625, 0.65625, 0.65625, 0.671875, 0.65625, 0.65625, 0.671875, 1.0, 0.671875, 0.640625, 0.65625, 0.65625, 1.0, 0.6875, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 0.671875, 0.671875, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.65625, 1.0, 0.65625, 0.65625, 0.640625, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.671875, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.640625, 0.65625, 0.65625, 0.65625, 0.65625, 0.671875, 0.671875, 0.671875, 0.65625, 0.65625, 1.0, 0.65625, 0.640625, 0.65625, 1.0, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.640625, 0.65625, 1.0, 0.671875, 0.640625, 1.0, 0.65625, 0.65625, 0.65625, 0.65625, 0.671875, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 0.65625, 0.671875, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.640625, 0.65625, 1.0, 0.671875, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.640625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 0.65625, 0.671875, 0.65625, 1.0, 1.0, 0.671875, 0.65625, 0.625, 1.0, 1.0, 1.0, 0.65625, 0.6875, 0.65625, 0.640625, 0.640625, 0.65625, 0.65625, 0.671875, 0.671875, 0.671875, 1.0, 0.6875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1484375, 0.14453125, 0.15234375, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.15234375, 0.1484375, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1640625, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.710069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7274305820465088, 1.0, 1.0, 1.0, 1.0, 0.7934027910232544, 0.663194477558136, 1.0, 1.0, 0.6840277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.725694477558136, 1.0, 1.0, 0.7065972089767456, 0.6805555820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.6979166865348816, 1.0, 1.0, 1.0, 0.7326388955116272, 0.6822916865348816, 1.0, 0.6996527910232544, 1.0, 0.7048611044883728, 1.0, 1.0]

 sparsity of   [1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.75, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.734375, 1.0, 0.75, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.734375, 0.734375, 0.75, 0.734375, 0.734375, 1.0, 0.734375, 1.0, 0.734375, 0.75, 1.0, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.734375, 0.734375, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.75, 0.734375, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 0.734375, 0.734375, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 0.734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.75, 0.75, 1.0, 1.0, 0.734375, 0.75, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 0.75, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.109375, 1.0, 1.0, 0.09765625, 1.0, 0.109375, 1.0, 1.0, 0.1015625, 1.0, 0.1484375, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.1328125, 0.12109375, 0.11328125, 0.11328125, 1.0, 1.0, 0.10546875, 0.140625, 1.0, 0.12109375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.23828125, 0.10546875, 1.0, 0.09765625, 1.0, 0.11328125, 0.10546875, 1.0, 1.0, 1.0, 0.109375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.11328125, 1.0, 0.109375, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0]

 sparsity of   [1.0, 0.6657986044883728, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6579861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6675347089767456, 1.0, 1.0, 0.6579861044883728, 1.0, 0.663194477558136, 0.6571180820465088, 1.0, 0.663194477558136, 0.6588541865348816, 0.6614583134651184, 1.0, 0.6623263955116272, 1.0, 0.6675347089767456, 1.0, 0.6736111044883728, 1.0, 1.0, 0.6623263955116272, 0.6614583134651184, 1.0, 0.6545138955116272, 0.6657986044883728, 1.0, 0.6649305820465088, 0.6623263955116272, 0.6640625, 1.0, 0.6614583134651184, 1.0, 0.6640625, 0.6597222089767456, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.655381977558136, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 0.6588541865348816, 1.0, 0.6605902910232544, 1.0, 0.6640625, 0.6727430820465088, 0.6597222089767456, 1.0, 0.663194477558136, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 0.6579861044883728, 1.0, 1.0, 0.6675347089767456, 1.0, 0.6614583134651184, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6657986044883728, 1.0, 1.0, 0.6579861044883728, 0.6597222089767456, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 0.6597222089767456, 1.0, 0.655381977558136, 0.6623263955116272, 1.0, 1.0, 0.6623263955116272, 1.0, 0.6588541865348816, 1.0, 1.0, 0.6649305820465088, 0.663194477558136, 1.0, 1.0, 1.0, 0.6640625, 1.0, 0.6649305820465088, 0.671875, 0.6579861044883728, 1.0]

 sparsity of   [0.5703125, 0.578125, 1.0, 1.0, 0.578125, 1.0, 0.578125, 1.0, 0.59375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 0.5859375, 1.0, 0.578125, 0.578125, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 0.5859375, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 0.5703125, 1.0, 1.0, 0.578125, 0.5859375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5703125, 0.578125, 1.0, 0.5859375, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 0.578125, 0.5703125, 0.578125, 1.0, 1.0, 0.5859375, 1.0, 1.0, 0.5703125, 0.5703125, 0.578125, 0.5859375, 1.0, 0.578125, 0.578125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5859375, 0.5703125, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.5703125, 1.0, 1.0, 0.5859375, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.59375, 0.5859375, 0.5859375, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 0.578125, 0.5703125, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 0.59375, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5859375, 0.59375, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.5859375, 0.5703125, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 0.578125, 0.578125, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.5703125, 0.5703125, 1.0, 0.578125, 0.5703125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.59375, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5859375, 1.0, 0.5703125, 0.578125, 0.5859375, 1.0, 0.5703125, 0.5703125, 0.5703125, 0.5859375, 1.0, 0.578125, 0.578125, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 0.5703125, 0.578125, 1.0, 0.5859375, 0.5703125, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 0.5703125, 0.578125, 1.0, 0.5859375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 1.0, 0.578125, 0.578125, 0.578125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5703125, 0.5859375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.59375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.5703125, 0.5703125, 0.578125, 1.0, 0.578125, 1.0, 0.5859375, 1.0, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 0.578125, 0.5859375, 1.0, 0.578125, 1.0, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.6015625, 0.578125, 1.0, 0.578125, 0.5859375, 1.0, 0.578125, 1.0, 0.5703125, 1.0, 0.578125, 1.0, 1.0, 0.5703125, 0.59375, 0.5703125, 1.0, 0.578125, 0.5859375, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.5859375, 0.578125, 0.578125, 0.5703125, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 0.5703125, 0.6015625, 1.0, 1.0, 0.5859375, 1.0, 0.578125, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.5859375, 1.0, 0.59375, 0.578125, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.5859375, 0.5703125]

 sparsity of   [0.12109375, 0.14453125, 1.0, 1.0, 0.11328125, 1.0, 0.125, 1.0, 0.125, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1328125, 0.18359375, 1.0, 1.0, 0.11328125, 0.18359375, 0.1953125, 1.0, 0.12109375, 1.0, 0.1171875, 0.1328125, 0.27734375, 0.12109375, 1.0, 0.11328125, 0.1171875, 1.0, 1.0, 1.0, 0.1953125, 0.12109375, 0.125, 1.0, 0.20703125, 1.0, 0.125, 0.10546875, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.12109375, 0.14453125, 0.15625, 1.0, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 1.0, 0.2265625, 1.0, 0.18359375, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.11328125, 1.0, 0.17578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.16796875, 0.15234375, 1.0, 0.11328125, 0.1640625, 0.12109375, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 0.14453125, 0.23046875, 0.11328125, 1.0, 0.1171875, 0.1875, 0.10546875, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.125, 0.12109375, 0.125, 0.11328125, 0.11328125, 0.11328125, 0.11328125, 0.125, 1.0, 1.0, 1.0, 0.1171875, 0.11328125, 1.0, 0.1171875, 0.2109375, 1.0, 0.14453125, 1.0, 1.0, 0.1171875, 1.0, 0.1484375, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.12109375, 0.12890625, 0.10546875, 0.1796875, 0.11328125, 0.12109375, 0.125, 1.0, 1.0, 0.17578125, 1.0, 0.1796875, 0.109375, 0.12109375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 0.10546875, 0.109375, 1.0, 1.0, 1.0, 0.12109375, 0.109375, 0.12109375, 0.26953125, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.125, 1.0, 1.0, 1.0, 0.19140625, 0.12109375, 1.0, 0.1171875, 0.19140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.1875, 0.12109375, 1.0, 1.0, 0.11328125, 0.1171875, 0.10546875, 0.109375, 0.1171875, 0.1171875, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.1171875, 1.0, 0.171875, 1.0, 0.1484375, 0.1171875, 1.0, 0.12109375, 0.13671875, 1.0, 0.12890625, 0.1171875, 1.0, 0.11328125, 0.109375, 0.11328125, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.125, 0.1171875, 0.16015625, 1.0, 0.12890625, 1.0, 1.0, 1.0, 0.13671875, 0.11328125, 0.12109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.11328125, 0.1171875, 0.12109375, 1.0, 0.16015625, 0.1171875, 0.21484375, 0.125, 1.0, 0.13671875, 1.0, 1.0, 0.17578125, 1.0, 0.109375, 0.109375, 0.10546875, 0.12890625, 1.0, 0.109375, 0.13671875, 1.0, 0.203125, 0.1171875, 0.23046875, 1.0, 0.19921875, 0.1171875, 1.0, 0.125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.265625, 0.1328125, 1.0, 0.12109375, 1.0, 0.1171875, 0.1171875, 0.140625, 0.1328125, 0.1171875, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.15625, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.11328125, 0.17578125, 0.109375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.11328125, 0.28515625, 0.12109375, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.17578125, 1.0, 1.0, 0.1640625, 0.109375, 0.15234375, 1.0, 0.203125, 1.0, 0.125, 0.12109375, 1.0, 1.0, 0.13671875, 1.0, 0.125, 0.20703125, 0.11328125, 1.0, 1.0, 0.15625, 1.0, 0.12890625, 1.0, 1.0, 0.109375, 0.10546875, 1.0, 0.11328125, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.125, 0.1171875, 0.19140625, 0.09765625, 1.0, 0.12109375, 1.0, 0.1171875, 1.0, 1.0, 0.15625, 0.171875, 0.1171875, 1.0, 0.140625, 1.0, 0.1171875, 1.0, 0.1640625, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 0.109375, 1.0, 0.171875, 0.12109375, 1.0, 0.125, 1.0, 1.0, 0.1171875, 0.17578125, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.12109375, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.1796875, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 0.1171875, 1.0, 0.16796875, 0.11328125, 1.0, 0.109375, 1.0, 0.11328125, 1.0, 0.1171875, 1.0, 1.0, 0.13671875, 0.109375, 0.16015625, 1.0, 0.1171875, 0.1171875, 0.11328125, 0.109375, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12109375, 0.11328125, 0.11328125, 0.1171875, 0.109375, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 0.16015625, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 0.11328125, 1.0, 0.12109375, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.109375, 0.125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.11328125, 0.16015625]

 sparsity of   [0.482421875, 1.0, 1.0, 1.0, 0.5, 1.0, 0.494140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 0.484375, 1.0, 0.490234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 0.490234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.486328125, 0.494140625, 1.0, 1.0, 0.47265625, 1.0, 1.0, 1.0, 0.494140625, 0.4765625, 1.0, 0.484375, 0.5, 1.0, 0.482421875, 1.0, 0.47265625, 0.498046875, 0.484375, 1.0, 0.474609375, 0.4765625, 0.474609375, 0.501953125, 1.0, 0.478515625, 1.0, 0.482421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.474609375, 1.0, 1.0, 0.48828125, 0.515625, 0.494140625, 0.474609375, 1.0, 0.482421875, 1.0, 1.0, 0.474609375, 1.0, 0.48828125, 1.0, 1.0, 0.474609375, 1.0, 1.0, 1.0, 0.470703125, 0.478515625, 1.0, 1.0, 1.0, 0.48828125, 1.0, 1.0, 1.0, 0.498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.48828125, 0.484375, 1.0, 0.474609375, 1.0, 0.484375, 0.478515625, 1.0, 1.0, 0.4765625, 1.0, 1.0, 0.494140625, 1.0, 1.0, 0.4921875, 1.0]

 sparsity of   [0.6102430820465088, 0.6215277910232544, 0.6171875, 0.608506977558136, 1.0, 0.655381977558136, 0.6111111044883728, 1.0, 0.6701388955116272, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6119791865348816, 0.6111111044883728, 1.0, 1.0, 1.0, 1.0, 0.6111111044883728, 1.0, 0.6111111044883728, 1.0, 1.0, 0.609375, 0.6197916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.600694477558136, 1.0, 0.6119791865348816, 0.609375, 0.6675347089767456, 0.6128472089767456, 0.6076388955116272, 0.6519097089767456, 1.0, 0.6119791865348816, 0.6493055820465088, 0.6493055820465088, 0.616319477558136, 1.0, 0.6449652910232544, 1.0, 0.6232638955116272, 1.0, 0.6059027910232544, 1.0, 1.0, 1.0, 0.616319477558136, 1.0, 0.6232638955116272, 0.6154513955116272, 0.6423611044883728, 1.0, 1.0, 1.0, 1.0, 0.6137152910232544, 0.6111111044883728, 0.608506977558136, 1.0, 1.0, 1.0, 1.0, 0.6059027910232544, 1.0, 1.0, 0.6059027910232544, 1.0, 1.0, 0.6527777910232544, 0.6171875, 1.0, 0.6111111044883728, 1.0, 0.6111111044883728, 0.6102430820465088, 1.0, 0.6111111044883728, 0.6032986044883728, 1.0, 0.616319477558136, 0.608506977558136, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6076388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6189236044883728, 0.616319477558136, 0.6067708134651184, 0.6102430820465088, 0.609375, 1.0, 0.6059027910232544, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.6059027910232544, 0.6484375, 1.0, 0.639756977558136, 1.0, 1.0, 0.6102430820465088, 1.0]

 sparsity of   [0.546875, 0.53125, 0.5625, 0.53125, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 1.0, 0.5390625, 1.0, 1.0, 1.0, 0.53125, 0.5625, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.5546875, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 1.0, 0.5390625, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.53125, 0.53125, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 0.546875, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 1.0, 0.5546875, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.53125, 0.546875, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.5546875, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 1.0, 0.5625, 0.5390625, 1.0, 0.5390625, 1.0, 0.53125, 0.5625, 1.0, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5546875, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.5390625, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.5390625, 0.5390625, 1.0, 0.53125, 1.0, 1.0, 0.53125, 0.546875, 1.0, 0.5390625, 1.0, 0.546875, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.53125, 0.53125, 1.0, 0.5390625, 0.53125, 0.53125, 0.5390625, 0.5390625, 0.53125, 0.5546875, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.53125, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 0.5546875, 0.5546875, 0.5390625, 1.0, 1.0, 0.546875, 1.0, 0.5390625, 1.0, 1.0, 0.53125, 1.0, 0.546875, 0.5390625, 0.53125, 0.5625, 1.0, 1.0, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.5703125, 0.53125, 0.5390625, 0.53125, 1.0, 0.5390625, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 1.0, 1.0, 0.5546875, 0.53125, 0.546875, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.53125, 1.0, 0.546875, 0.546875, 1.0, 0.53125, 0.5390625, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.546875, 0.5546875, 0.5390625, 0.546875, 0.5390625, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.5390625, 1.0, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.5625, 0.546875, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5625, 1.0, 0.5390625, 0.53125, 1.0, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.5546875, 1.0, 0.5390625, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.546875, 1.0, 1.0, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 1.0, 1.0, 0.5546875, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 0.53125, 0.53125, 0.53125, 0.5390625, 1.0, 0.546875, 0.53125, 1.0, 0.53125, 0.5703125, 0.53125, 0.5390625, 0.546875, 0.53125, 1.0, 1.0, 0.5390625, 0.5390625, 0.53125, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.5546875, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.546875, 0.53125, 1.0, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 1.0, 0.53125, 0.578125, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 0.5390625, 0.53125, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.53125, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.546875, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.546875, 0.5625, 0.53125, 0.53125, 0.53125]

 sparsity of   [1.0, 0.244140625, 1.0, 1.0, 0.333984375, 1.0, 0.240234375, 0.2421875, 0.306640625, 0.251953125, 0.26953125, 0.3125, 0.25390625, 1.0, 0.244140625, 0.248046875, 1.0, 0.310546875, 1.0, 0.326171875, 1.0, 1.0, 0.24609375, 1.0, 0.25390625, 0.24609375, 0.24609375, 1.0, 1.0, 0.25, 1.0, 1.0, 0.263671875, 0.26171875, 1.0, 1.0, 1.0, 1.0, 0.341796875, 1.0, 0.328125, 0.24609375, 0.26953125, 1.0, 1.0, 0.24609375, 1.0, 0.2421875, 0.259765625, 0.302734375, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.255859375, 0.24609375, 0.244140625, 0.244140625, 1.0, 1.0, 1.0, 1.0, 0.259765625, 1.0, 0.267578125, 0.2421875, 0.26171875, 0.25, 0.248046875, 0.251953125, 0.248046875, 0.267578125, 0.27734375, 1.0, 0.298828125, 0.25, 0.24609375, 1.0, 0.255859375, 0.310546875, 0.251953125, 1.0, 1.0, 1.0, 0.2421875, 1.0, 1.0, 0.248046875, 0.2734375, 0.25390625, 1.0, 1.0, 0.244140625, 0.251953125, 0.24609375, 1.0, 0.25390625, 1.0, 1.0, 0.24609375, 0.265625, 0.259765625, 0.25, 0.2734375, 1.0, 1.0, 1.0, 0.28125, 0.26171875, 0.2578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.25390625, 1.0, 0.24609375, 0.24609375, 0.24609375, 0.240234375, 0.24609375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.433159738779068, 1.0, 0.4383680522441864, 0.4765625, 0.4357638955116272, 0.4375, 1.0, 0.4392361044883728, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 1.0, 1.0, 1.0, 0.4383680522441864, 1.0, 0.4835069477558136, 1.0, 0.495659738779068, 0.4375, 1.0, 1.0, 0.4835069477558136, 0.4383680522441864, 1.0, 1.0, 0.4401041567325592, 0.440972238779068, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4357638955116272, 1.0, 0.4383680522441864, 1.0, 0.4375, 1.0, 0.4383680522441864, 1.0, 1.0, 1.0, 0.4401041567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4392361044883728, 0.4479166567325592, 0.4375, 1.0, 0.4375, 0.4392361044883728, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 1.0, 1.0, 0.4375, 0.4930555522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4401041567325592, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4392361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4383680522441864, 0.4383680522441864, 1.0, 0.4383680522441864, 0.4383680522441864, 1.0, 0.4383680522441864, 0.4748263955116272, 0.433159738779068, 0.4861111044883728, 0.4375, 0.5026041865348816, 0.4366319477558136, 1.0, 0.4366319477558136, 1.0, 1.0, 0.4739583432674408, 0.5008680820465088, 1.0, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4383680522441864, 1.0, 1.0]

 sparsity of   [1.0, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6171875, 0.6015625, 0.6015625, 1.0, 0.6171875, 0.6015625, 1.0, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6171875, 0.6171875, 0.609375, 1.0, 0.609375, 0.6015625, 1.0, 1.0, 0.609375, 0.6015625, 0.609375, 0.6171875, 0.609375, 0.6015625, 0.6171875, 0.6015625, 0.609375, 0.609375, 1.0, 0.6015625, 0.6015625, 0.625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 1.0, 0.6171875, 0.6015625, 0.609375, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 0.6171875, 1.0, 0.6171875, 0.609375, 0.6015625, 0.609375, 0.6171875, 1.0, 0.609375, 0.609375, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.6171875, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 1.0, 0.6171875, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 0.6015625, 0.6171875, 0.6015625, 0.625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6171875, 0.6015625, 0.6015625, 0.6171875, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.625, 0.6171875, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.609375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 0.609375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.6015625, 1.0, 0.609375, 0.609375, 0.609375, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 0.6015625, 0.6171875, 0.6015625, 1.0, 0.6015625, 0.609375, 0.609375, 0.625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.609375, 0.609375, 0.6015625, 1.0, 0.609375, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.609375, 1.0, 0.609375, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.6015625, 0.6875, 0.6171875, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.625, 0.609375, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 0.609375, 0.609375, 1.0, 0.625, 0.6171875, 1.0, 0.609375, 1.0, 0.625, 0.609375, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6015625, 0.625, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 1.0, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6328125, 0.6015625, 1.0, 0.6171875, 0.609375, 1.0, 0.6015625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 0.6171875, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 0.609375, 0.6015625, 0.6796875, 0.609375, 0.6171875, 0.6015625, 1.0, 1.0, 1.0, 0.609375, 0.6015625, 1.0, 0.625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6171875, 1.0, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.640625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.609375, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.609375, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6171875, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 0.6171875, 0.6015625]

 sparsity of   [0.125, 1.0, 1.0, 0.125, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.138671875, 0.12890625, 1.0, 0.283203125, 1.0, 0.125, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.271484375, 0.271484375, 1.0, 0.259765625, 1.0, 1.0, 0.123046875, 0.123046875, 1.0, 1.0, 0.123046875, 0.1484375, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.271484375, 0.185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 1.0, 1.0, 0.123046875, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 0.1484375, 1.0, 1.0, 0.125, 1.0, 1.0, 0.197265625, 0.255859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 0.130859375, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.123046875, 0.142578125, 0.150390625, 1.0, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.228515625, 1.0, 1.0, 1.0, 0.240234375, 0.142578125, 1.0, 1.0, 0.310546875, 0.1640625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6649305820465088, 1.0, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6657986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6684027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6692708134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6675347089767456, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 0.8203125, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 0.8203125, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.103515625, 1.0, 1.0, 0.09765625, 1.0, 0.091796875, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.099609375, 0.091796875, 1.0, 1.0, 0.103515625, 1.0, 0.09765625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.095703125, 1.0, 0.09765625, 1.0, 0.099609375, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.10546875, 0.09765625, 0.09765625, 1.0, 1.0, 0.103515625, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 0.09375, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.091796875, 1.0, 0.111328125, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.099609375, 1.0, 1.0, 0.1171875, 1.0, 0.115234375, 1.0, 1.0, 0.087890625, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.091796875, 0.103515625, 0.099609375, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 0.09765625, 0.115234375, 1.0, 0.091796875, 0.091796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08984375, 1.0, 0.09765625, 1.0, 1.0, 0.08984375, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 0.091796875, 0.09765625, 0.103515625, 1.0, 0.09375, 1.0, 1.0, 1.0]

 sparsity of   [0.7352430820465088, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7339409589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7352430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7378472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7365451455116272, 0.7356770634651184, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 0.7361111044883728, 0.7352430820465088, 1.0, 1.0, 0.7365451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7356770634651184, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7365451455116272, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7348090410232544, 0.7361111044883728, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 0.7356770634651184, 0.7348090410232544, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 0.7352430820465088, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7352430820465088, 1.0, 0.7356770634651184, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7369791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0]

 sparsity of   [0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.81640625, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.81640625, 1.0, 0.80859375, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.81640625, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.81640625, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 0.8125, 1.0, 0.8125, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 0.80859375, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.8125, 0.80859375, 1.0, 0.8125, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 0.8125, 0.80859375, 1.0]

 sparsity of   [0.13671875, 1.0, 0.109375, 0.1328125, 0.099609375, 1.0, 0.103515625, 0.10546875, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.119140625, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.09765625, 0.099609375, 1.0, 0.11328125, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.2109375, 0.115234375, 0.109375, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.109375, 0.1171875, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.09765625, 0.109375, 1.0, 0.189453125, 1.0, 1.0, 0.091796875, 0.14453125, 0.111328125, 0.12109375, 1.0, 0.111328125, 0.1015625, 0.111328125, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.111328125, 0.111328125, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.099609375, 0.123046875, 1.0, 1.0, 0.11328125, 1.0, 0.08984375, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.119140625, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 0.111328125, 0.107421875, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.095703125, 0.103515625, 0.1171875, 0.107421875, 0.099609375, 1.0, 0.09765625, 0.119140625, 1.0, 1.0, 0.10546875, 0.103515625, 0.1328125, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.134765625, 0.21484375, 1.0, 0.103515625, 1.0, 1.0, 0.150390625, 1.0, 0.1171875, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.095703125, 0.142578125, 1.0, 0.109375, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.109375, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.123046875, 0.15234375, 0.130859375, 0.103515625, 1.0, 0.1015625, 1.0, 1.0, 0.11328125, 0.111328125, 1.0, 0.1015625, 0.103515625, 1.0, 1.0, 1.0, 0.103515625, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.1015625, 0.123046875, 0.134765625, 1.0, 1.0, 0.103515625, 0.12109375, 1.0, 0.099609375, 0.111328125, 0.09765625, 0.1015625, 1.0, 1.0, 1.0, 0.103515625, 0.1015625, 0.12890625, 1.0, 0.09765625, 1.0, 0.109375, 1.0, 0.265625, 1.0, 0.095703125, 0.13671875, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 1.0, 0.103515625, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 0.10546875, 1.0, 0.09375, 1.0, 0.14453125, 1.0, 1.0, 0.111328125, 0.11328125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.1171875, 0.10546875, 0.1015625, 1.0, 0.12109375, 1.0, 1.0, 0.107421875, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 0.09765625, 0.146484375, 0.125, 0.09765625, 1.0, 0.111328125, 0.103515625, 0.09375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.123046875, 1.0, 0.09765625, 1.0, 1.0, 0.103515625, 1.0, 1.0, 0.115234375, 0.1015625, 1.0, 1.0, 1.0, 0.1171875, 0.142578125, 1.0, 0.12109375, 0.111328125, 0.10546875, 0.1171875, 0.09375, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.09765625, 0.09765625, 1.0, 0.279296875, 1.0, 1.0, 0.125, 0.125, 1.0, 1.0, 0.099609375, 1.0, 0.10546875, 0.1015625, 1.0, 1.0, 0.095703125, 0.103515625, 1.0, 0.12109375, 1.0, 0.103515625, 0.123046875, 0.09375, 0.107421875, 0.16796875, 1.0, 0.1015625, 0.126953125, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 0.107421875, 0.10546875, 1.0, 1.0, 0.107421875, 0.103515625, 1.0, 1.0, 0.1171875, 0.107421875, 0.107421875, 0.13671875, 0.130859375, 0.11328125, 1.0, 0.12890625, 0.107421875, 0.107421875, 0.13671875, 1.0, 0.09765625, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.103515625, 0.107421875, 1.0, 0.099609375, 1.0, 0.2109375, 1.0, 0.1015625, 1.0, 1.0, 0.103515625, 0.10546875, 0.111328125, 0.130859375, 0.123046875, 1.0, 0.107421875, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 1.0, 0.103515625, 0.099609375, 0.103515625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.193359375, 1.0, 1.0, 0.1328125, 1.0, 0.158203125, 1.0, 0.126953125, 0.125, 1.0, 0.12109375, 0.09375, 0.119140625, 0.11328125, 0.123046875, 0.10546875, 0.138671875, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 0.203125, 0.126953125, 0.111328125, 0.10546875, 1.0, 1.0, 0.1015625, 0.099609375, 1.0, 1.0, 0.09375, 0.10546875, 1.0, 1.0, 0.173828125, 1.0, 0.111328125, 0.248046875, 1.0, 1.0, 0.111328125, 1.0, 1.0, 0.109375, 1.0, 0.138671875, 0.115234375, 1.0, 0.109375, 0.11328125, 1.0, 1.0, 0.16015625, 1.0, 0.107421875, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.138671875, 1.0, 0.228515625, 0.099609375, 0.1484375, 0.091796875, 1.0, 0.12109375, 1.0, 1.0, 0.103515625, 0.109375, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 0.1015625, 0.095703125, 1.0, 1.0, 0.099609375, 1.0, 0.103515625, 1.0, 1.0, 0.103515625, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.107421875, 0.111328125, 1.0, 0.115234375, 1.0, 1.0, 0.115234375, 0.15234375, 0.11328125, 1.0, 0.111328125, 1.0, 1.0, 0.130859375, 0.099609375, 0.095703125, 0.115234375, 1.0, 0.107421875, 0.1328125, 1.0, 1.0, 0.111328125, 1.0, 0.146484375, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.10546875, 1.0, 0.103515625, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.130859375, 0.095703125, 0.109375, 0.123046875, 1.0, 0.095703125, 0.111328125, 0.09765625, 1.0, 0.1015625, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.09765625, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.119140625, 0.10546875, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.099609375, 0.134765625, 0.107421875, 1.0, 1.0, 0.1015625, 0.09765625, 1.0, 0.103515625, 0.115234375, 1.0, 0.099609375, 0.130859375, 0.1015625, 0.109375, 1.0, 0.103515625, 1.0, 1.0, 0.259765625, 1.0, 1.0, 1.0, 0.126953125, 0.109375, 0.1015625, 1.0, 1.0, 0.099609375, 1.0, 0.126953125, 1.0, 1.0, 0.107421875, 1.0, 0.099609375, 0.123046875, 0.115234375, 1.0, 1.0, 1.0, 0.134765625, 1.0, 1.0, 1.0, 0.119140625, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.1171875, 1.0, 0.115234375, 0.107421875, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 0.095703125, 1.0, 0.11328125, 0.111328125, 1.0, 1.0, 0.140625, 0.154296875, 1.0, 0.134765625, 1.0, 1.0, 0.130859375, 0.09765625, 0.107421875, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1015625, 0.09765625, 1.0, 1.0, 0.09765625, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.099609375, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 0.134765625, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.12109375, 0.09765625, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1015625, 0.21484375, 0.10546875, 1.0, 0.103515625, 0.099609375, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 0.103515625, 1.0, 1.0, 0.09375, 0.126953125, 1.0, 0.1328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.11328125, 0.099609375, 0.13671875, 1.0, 0.115234375, 1.0, 0.1015625, 0.119140625, 0.119140625, 1.0, 0.107421875, 1.0, 0.123046875, 0.095703125, 0.119140625, 1.0, 1.0, 0.103515625, 0.107421875, 0.138671875, 0.099609375, 1.0, 0.1015625, 1.0, 0.119140625, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 0.12109375, 1.0, 0.11328125, 0.10546875, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.103515625, 0.095703125, 1.0, 0.107421875, 1.0, 0.12109375, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.099609375, 0.10546875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.119140625, 1.0, 1.0, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 0.109375, 0.123046875, 1.0, 0.1171875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 0.09765625, 1.0, 0.16796875, 1.0, 1.0, 0.126953125, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.130859375, 0.125, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 0.09375, 0.09375, 1.0, 0.125, 1.0, 1.0, 1.0, 0.1328125, 1.0, 0.099609375, 1.0, 0.1171875, 0.111328125, 0.09375, 0.12109375, 1.0, 0.111328125, 1.0, 0.107421875, 0.11328125, 1.0, 0.115234375, 0.1171875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.51171875, 1.0, 0.521484375, 1.0, 1.0, 1.0, 0.5224609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 0.5205078125, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.509765625, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 1.0, 1.0, 0.517578125, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 0.5224609375, 0.5205078125, 1.0, 1.0, 0.5078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5400390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.517578125, 1.0, 0.5234375, 0.5107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.52734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5322265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 0.517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.517578125, 0.5146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 0.517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.796006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8051215410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7955729365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7747395634651184, 1.0, 0.815538227558136, 1.0, 1.0, 0.753038227558136, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7703993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 0.8029513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7951388955116272, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 0.8424479365348816, 1.0, 0.838975727558136, 1.0, 1.0, 0.8129340410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7747395634651184, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8059895634651184, 1.0, 0.8337673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8172743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8355034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8285590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7934027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7434895634651184, 1.0, 0.8064236044883728, 0.7621527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 0.8463541865348816, 0.7170138955116272, 1.0, 1.0]

 sparsity of   [0.87109375, 1.0, 0.87109375, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 0.86328125, 0.87109375, 1.0, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.8671875, 0.86328125, 0.87109375, 1.0, 0.87109375, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.859375, 1.0, 1.0, 0.87109375, 0.859375, 1.0, 0.8671875, 1.0, 1.0, 0.875, 0.86328125, 0.87109375, 0.8671875, 1.0, 0.875, 0.8671875, 0.859375, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 0.875, 1.0, 1.0, 0.859375, 0.87109375, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 0.87109375, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 0.875, 0.86328125, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.875, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.86328125, 0.8671875, 0.87109375, 1.0, 1.0, 0.87109375, 0.86328125, 1.0, 0.87109375, 0.859375, 0.8671875, 0.859375, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 0.859375, 1.0, 0.8671875, 1.0, 0.86328125, 0.86328125, 1.0, 1.0, 0.87109375, 0.875, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.875, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.875, 0.859375, 1.0, 1.0, 1.0, 0.87109375, 0.8671875, 0.859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 0.875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87890625, 0.86328125, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 0.859375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.875, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.859375, 0.87109375, 1.0, 0.875, 1.0, 0.86328125, 0.875, 1.0, 1.0, 0.859375, 0.8671875, 1.0, 0.8671875, 1.0, 0.859375, 0.8671875, 0.87109375, 0.859375, 0.8671875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.86328125, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 0.859375, 0.8671875, 0.87109375, 0.87109375, 1.0, 0.87109375, 0.8671875, 0.86328125, 0.8671875, 1.0, 0.8828125, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 0.87109375, 1.0, 0.86328125, 1.0, 0.859375, 1.0, 0.87109375, 1.0, 0.87109375, 0.8671875, 1.0, 0.8671875, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.87109375, 0.86328125, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 0.86328125, 0.8671875, 1.0, 0.87890625, 0.86328125, 1.0, 0.875, 0.86328125, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87109375, 1.0, 0.8671875, 0.86328125, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 0.859375, 0.87890625, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.8671875, 0.8671875, 1.0, 0.859375, 1.0, 1.0, 0.8671875, 1.0, 0.86328125, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.8671875, 0.87109375, 0.8671875, 0.875, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.8671875, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.86328125, 0.86328125, 1.0, 0.86328125, 0.87109375, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.86328125, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.87890625, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.8671875, 0.86328125, 0.8671875, 0.859375, 1.0, 0.859375, 0.8671875, 0.87109375, 1.0, 0.87109375, 0.859375, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 0.8671875, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 0.87109375, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.875, 0.86328125, 1.0, 0.859375, 0.87109375, 0.8671875, 0.87109375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.8671875, 0.87109375, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 0.87109375, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 0.875, 1.0, 0.8671875, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 0.87109375, 0.859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.87109375, 1.0, 0.875, 1.0, 0.87109375, 0.875, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 0.87109375, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 0.859375, 0.87109375, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.875, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 1.0, 1.0, 1.0, 0.859375, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.875, 0.87109375, 0.8671875, 1.0, 0.87890625, 0.8671875, 0.8671875, 0.8671875, 0.8671875, 1.0, 0.86328125, 1.0, 0.86328125, 0.86328125, 0.8671875, 0.86328125, 1.0, 0.875, 0.87109375, 1.0, 0.87109375, 0.87109375, 0.86328125, 1.0, 0.875, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.859375, 0.8671875, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.875, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.859375, 0.86328125, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.875, 0.859375, 1.0, 0.87109375, 1.0, 1.0, 0.875, 0.87109375, 1.0, 0.8671875, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.875, 1.0, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 0.8671875, 1.0, 0.875, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 0.86328125, 0.86328125, 1.0, 1.0, 0.86328125, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 1.0, 0.87109375, 0.86328125, 0.8671875, 0.87109375, 1.0, 0.86328125, 1.0, 0.8828125, 0.87109375, 1.0, 0.8671875, 0.87109375, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 1.0, 0.4638671875, 1.0, 1.0, 1.0, 1.0, 0.458984375, 1.0, 0.4501953125, 0.46484375, 0.4697265625, 0.462890625, 1.0, 1.0, 0.4541015625, 0.46484375, 1.0, 1.0, 0.4697265625, 0.453125, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4501953125, 1.0, 1.0, 0.470703125, 1.0, 1.0, 1.0, 1.0, 0.4462890625, 1.0, 0.4912109375, 0.4658203125, 1.0, 1.0, 0.466796875, 1.0, 1.0, 0.4580078125, 0.46484375, 1.0, 0.458984375, 1.0, 0.4580078125, 1.0, 1.0, 0.4599609375, 1.0, 1.0, 0.447265625, 1.0, 1.0, 0.4638671875, 1.0, 1.0, 0.4619140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 0.4619140625, 1.0, 1.0, 0.46875, 0.4599609375, 1.0, 1.0, 1.0, 0.4541015625, 0.462890625, 1.0, 1.0, 0.458984375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4580078125, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4609375, 1.0, 0.4697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46484375, 0.46875, 1.0, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4599609375, 1.0, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4677734375, 1.0, 0.4638671875, 1.0, 1.0, 1.0, 0.4619140625, 0.45703125, 0.4609375, 0.4580078125, 0.4619140625, 1.0, 1.0, 0.462890625, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 0.4453125, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4560546875, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4716796875, 0.4755859375, 0.45703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 0.4599609375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4658203125, 1.0, 1.0, 1.0, 0.4619140625, 0.5693359375, 1.0, 1.0, 1.0, 0.458984375, 1.0, 1.0, 1.0, 0.4609375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4658203125, 0.470703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4716796875, 1.0, 0.462890625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.702256977558136, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6979166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7078993320465088, 1.0, 0.7000868320465088, 1.0, 0.7018229365348816, 1.0, 0.706163227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.69921875, 1.0, 0.7000868320465088, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6987847089767456, 1.0, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 0.7000868320465088, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 0.6996527910232544, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6996527910232544, 0.7052951455116272, 0.7044270634651184, 1.0, 1.0, 0.7013888955116272, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 0.6987847089767456, 1.0, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 0.7000868320465088, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7057291865348816, 0.7000868320465088, 1.0, 1.0, 0.703125, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 0.7000868320465088, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6987847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.8046875, 1.0, 0.80078125, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.80078125, 0.80859375, 0.8046875, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80078125, 0.8046875, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80078125, 0.80859375, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.80859375, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.80859375, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.80859375, 0.80078125, 1.0, 0.80078125, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.80078125, 0.80078125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80078125, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 0.80859375, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.8046875, 0.796875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80078125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80078125, 0.8046875, 0.8046875, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 0.416015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.404296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 0.3984375, 1.0, 1.0, 1.0, 0.40625, 1.0, 1.0, 0.40625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.9322916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.932725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.932725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9314236044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9322916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9331597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.392578125, 0.3896484375, 0.388671875, 0.39453125, 0.3994140625, 0.396484375, 0.3955078125, 0.390625, 1.0, 1.0, 0.3955078125, 0.39453125, 0.392578125, 0.3955078125, 0.40234375, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39453125, 0.40234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 0.3935546875, 0.4013671875, 1.0, 0.3935546875, 1.0, 0.396484375, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 0.392578125, 0.388671875, 0.39453125, 0.3955078125, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3935546875, 1.0, 0.3994140625, 0.3935546875, 0.4033203125, 0.3935546875, 1.0, 0.3935546875, 0.396484375, 1.0, 1.0, 0.3955078125, 1.0, 0.39453125, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 0.4033203125, 1.0, 0.3916015625, 1.0, 0.390625, 0.3974609375, 1.0, 0.390625, 1.0, 0.39453125, 1.0, 0.3828125, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 0.3984375, 1.0, 1.0, 1.0, 0.404296875, 0.392578125, 1.0, 1.0, 0.392578125, 1.0, 0.392578125, 0.396484375, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3916015625, 0.388671875, 1.0, 0.392578125, 1.0, 0.392578125, 1.0, 0.3896484375, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 0.390625, 0.3984375, 1.0, 0.3994140625, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.408203125, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3935546875, 1.0, 0.3984375, 0.392578125, 0.3984375, 1.0, 1.0, 0.384765625, 0.3935546875, 0.3984375, 1.0, 1.0, 0.3974609375, 1.0, 0.396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 0.3955078125, 0.3935546875, 1.0, 0.396484375, 0.3974609375, 0.392578125, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3974609375, 1.0, 1.0, 0.396484375, 0.396484375, 0.3916015625, 0.3935546875, 0.3916015625, 1.0, 1.0, 0.396484375, 1.0, 0.3984375, 1.0, 1.0, 1.0, 0.3935546875]

 sparsity of   [0.5941840410232544, 0.596788227558136, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5959201455116272, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 0.5946180820465088, 0.5941840410232544, 0.5954861044883728, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 0.5946180820465088, 0.5950520634651184, 0.596788227558136, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.59375, 1.0, 0.5959201455116272, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5950520634651184, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 0.5950520634651184, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 0.5954861044883728, 0.5950520634651184, 1.0, 0.5959201455116272, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 0.5963541865348816, 0.5950520634651184, 1.0, 1.0, 1.0, 0.596788227558136, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 0.5959201455116272, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5963541865348816, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 0.5959201455116272, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0]

 sparsity of   [0.7734375, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.77734375, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 0.76953125, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.7734375, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 0.765625, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 0.765625, 0.7734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.77734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 0.7734375, 0.765625, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.796875, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.765625, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.7734375, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 0.76953125, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 0.765625, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.78125, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.38671875, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4033203125, 0.3876953125, 1.0, 0.3857421875, 1.0, 1.0, 0.400390625, 1.0, 0.3828125, 1.0, 0.3916015625, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.390625, 0.3876953125, 0.3896484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 0.3974609375, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 0.3935546875, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3896484375, 1.0, 0.3857421875, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 0.39453125, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3896484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 0.380859375, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 0.3916015625, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 0.38671875, 1.0, 1.0, 0.3896484375, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.3955078125, 0.3828125, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7808159589767456, 0.78125, 0.7769097089767456, 0.7816840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7790798544883728, 1.0, 1.0, 1.0, 0.7790798544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7816840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 0.78125]

 sparsity of   [0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90625, 1.0, 0.8984375, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 0.89453125, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90234375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.90234375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 0.8984375, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3779296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9951171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.48828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4931640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.880859375, 1.0, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83984375, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.408203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37890625, 1.0, 1.0, 1.0, 0.4140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4482421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4091796875, 1.0, 1.0, 1.0, 0.4443359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8740234375, 1.0, 1.0, 1.0, 0.416015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.447265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5283203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5224609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41796875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9658203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8302951455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.822265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8185763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8344184160232544, 0.8129340410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.837022602558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8426649570465088, 0.8578559160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8569878339767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8602430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8452690839767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8413628339767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.623046875, 0.62109375, 0.60888671875, 0.6171875, 0.60888671875, 0.60595703125, 0.615234375, 0.61962890625, 0.6162109375, 0.62646484375, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875]

Total parameter pruned: 22644409.005100533 (unstructured) 21123309 (structured)

Test: [0/79]	Time 0.263 (0.263)	Loss 0.3532 (0.3532) ([0.222]+[0.131])	Prec@1 90.625 (90.625)
 * Prec@1 93.660

 Total elapsed time  4:00:46.271552 
 FINETUNING


 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333432674408, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.390625, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7534722089767456, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 0.75, 1.0, 0.75, 1.0, 1.0, 0.7517361044883728, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7517361044883728, 0.7552083134651184]

 sparsity of   [0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.765625, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.8125, 0.796875, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.765625, 1.0, 0.796875, 1.0, 1.0, 0.765625, 0.765625, 0.8125, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.8125, 0.78125, 1.0, 1.0, 0.796875, 0.78125, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.8125, 0.8125, 0.8125, 0.78125, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.765625, 0.828125, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.765625, 0.78125, 0.78125, 0.765625, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.78125, 1.0, 0.78125, 1.0, 0.765625, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.78125, 1.0, 0.796875, 1.0, 1.0, 0.765625, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.765625, 0.78125, 0.8125, 0.765625, 1.0, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 0.8125, 1.0, 0.78125, 1.0, 0.796875, 0.78125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.78125, 0.78125, 1.0, 0.78125, 1.0, 1.0, 0.78125, 1.0, 0.8125, 0.78125, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.796875, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.78125, 0.78125, 1.0, 0.78125, 0.796875, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 0.796875, 1.0, 0.796875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.78125, 0.78125, 1.0, 1.0, 0.78125, 0.796875, 0.78125, 1.0, 1.0, 1.0, 0.765625, 0.796875, 1.0, 1.0, 1.0, 0.796875, 0.78125, 1.0, 1.0, 0.765625, 1.0, 0.765625, 1.0, 1.0, 0.78125, 1.0, 0.78125, 0.78125]

 sparsity of   [1.0, 1.0, 0.34375, 1.0, 1.0, 0.40625, 0.359375, 0.390625, 1.0, 0.359375, 1.0, 0.40625, 1.0, 1.0, 1.0, 0.375, 0.34375, 0.34375, 0.34375, 0.359375, 1.0, 0.359375, 0.375, 0.34375, 0.34375, 0.34375, 0.375, 1.0, 0.34375, 1.0, 0.359375, 0.34375, 1.0, 0.375, 0.375, 0.515625, 0.359375, 0.390625, 0.359375, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.40625, 0.390625, 0.34375, 1.0, 1.0, 0.34375, 0.375, 0.34375, 0.375, 1.0, 0.375, 0.359375, 0.375, 0.40625, 1.0, 1.0, 0.359375, 0.375, 0.390625, 1.0, 1.0, 0.359375, 0.375, 1.0, 0.359375, 0.359375, 1.0, 0.390625, 0.375, 0.359375, 0.359375, 0.4375, 0.40625, 0.34375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.375, 0.421875, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 0.34375, 0.34375, 0.40625, 0.359375, 1.0, 1.0, 0.375, 0.390625, 1.0, 0.359375, 0.359375, 0.375, 0.359375, 0.375, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.390625, 0.359375, 1.0, 0.421875, 0.375, 1.0, 0.375, 0.359375, 1.0, 0.34375, 0.390625, 0.359375, 0.40625, 0.375, 1.0, 0.34375, 0.390625, 1.0, 1.0, 0.390625, 0.359375, 1.0, 0.375, 0.34375, 1.0, 0.390625, 1.0, 0.40625, 1.0, 0.40625, 0.421875, 1.0, 0.390625, 1.0, 0.359375, 1.0, 0.375, 0.375, 1.0, 0.34375, 0.359375, 0.375, 0.375, 0.359375, 1.0, 0.375, 0.359375, 0.375, 0.359375, 0.375, 0.375, 0.359375, 1.0, 0.421875, 0.359375, 1.0, 0.390625, 1.0, 0.359375, 1.0, 0.40625, 0.34375, 0.375, 0.359375, 0.375, 1.0, 1.0, 0.34375, 1.0, 1.0, 0.359375, 1.0, 0.359375, 0.34375, 0.375, 0.359375, 0.40625, 1.0, 0.375, 0.375, 1.0, 1.0, 0.40625, 1.0, 1.0, 0.59375, 0.34375, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.375, 0.34375, 1.0, 1.0, 0.40625, 0.375, 0.375, 0.359375, 1.0, 0.34375, 0.375, 1.0, 0.40625, 0.390625, 0.40625, 1.0, 1.0, 0.359375, 0.359375, 0.359375, 1.0, 0.40625, 0.375, 0.359375, 0.359375, 1.0, 1.0, 0.359375, 0.40625, 1.0, 0.375, 1.0, 0.359375, 0.375, 0.359375, 1.0, 0.359375, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.359375, 0.453125]

 sparsity of   [1.0, 1.0, 0.3515625, 1.0, 0.34765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3515625, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.34765625, 1.0, 1.0, 0.3671875, 1.0, 1.0, 0.33984375, 0.34765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3359375, 0.34375, 1.0, 1.0, 0.35546875, 1.0, 0.37109375, 0.3671875, 0.35546875, 1.0, 1.0, 0.35546875, 1.0, 1.0, 0.33984375, 1.0, 1.0, 0.3515625, 1.0, 1.0, 0.34765625, 1.0, 0.34765625, 0.3359375, 1.0, 1.0, 1.0]

 sparsity of   [0.6388888955116272, 1.0, 1.0, 1.0, 0.6527777910232544, 0.6371527910232544, 0.6440972089767456, 1.0, 1.0, 1.0, 1.0, 0.6284722089767456, 1.0, 0.65625, 1.0, 1.0, 0.663194477558136, 0.640625, 1.0, 1.0, 1.0, 0.6545138955116272, 0.6302083134651184, 1.0, 1.0, 1.0, 0.6302083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6354166865348816, 0.6649305820465088, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.647569477558136, 0.6371527910232544, 1.0, 0.6354166865348816, 1.0, 0.6527777910232544, 1.0, 1.0, 1.0, 0.6215277910232544, 0.640625, 1.0, 1.0]

 sparsity of   [0.65625, 1.0, 0.640625, 0.65625, 1.0, 1.0, 0.65625, 0.671875, 0.671875, 1.0, 0.65625, 0.640625, 1.0, 0.65625, 1.0, 0.625, 0.65625, 0.640625, 0.65625, 1.0, 1.0, 0.671875, 0.65625, 1.0, 1.0, 0.625, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 0.671875, 1.0, 1.0, 0.671875, 0.65625, 0.671875, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 1.0, 0.65625, 0.640625, 1.0, 0.640625, 0.65625, 0.65625, 0.65625, 0.671875, 0.65625, 0.65625, 0.671875, 1.0, 0.671875, 0.640625, 0.65625, 0.65625, 1.0, 0.6875, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 0.671875, 0.671875, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.65625, 1.0, 0.65625, 0.65625, 0.640625, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.671875, 0.65625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.640625, 0.65625, 0.65625, 0.65625, 0.65625, 0.671875, 0.671875, 0.671875, 0.65625, 0.65625, 1.0, 0.65625, 0.640625, 0.65625, 1.0, 0.65625, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.640625, 0.65625, 1.0, 0.671875, 0.640625, 1.0, 0.65625, 0.65625, 0.65625, 0.65625, 0.671875, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 0.65625, 0.671875, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.65625, 0.65625, 0.65625, 0.640625, 0.65625, 1.0, 0.671875, 0.65625, 0.65625, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.640625, 0.65625, 1.0, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 0.65625, 1.0, 0.65625, 1.0, 0.65625, 0.671875, 0.65625, 1.0, 1.0, 0.671875, 0.65625, 0.625, 1.0, 1.0, 1.0, 0.65625, 0.6875, 0.65625, 0.640625, 0.640625, 0.65625, 0.65625, 0.671875, 0.671875, 0.671875, 1.0, 0.6875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1484375, 0.14453125, 0.15234375, 1.0, 1.0, 0.14453125, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 0.15234375, 0.1484375, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1640625, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.710069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7274305820465088, 1.0, 1.0, 1.0, 1.0, 0.7934027910232544, 0.663194477558136, 1.0, 1.0, 0.6840277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.725694477558136, 1.0, 1.0, 0.7065972089767456, 0.6805555820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.6979166865348816, 1.0, 1.0, 1.0, 0.7326388955116272, 0.6822916865348816, 1.0, 0.6996527910232544, 1.0, 0.7048611044883728, 1.0, 1.0]

 sparsity of   [1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.75, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.734375, 1.0, 0.75, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.734375, 0.734375, 0.75, 0.734375, 0.734375, 1.0, 0.734375, 1.0, 0.734375, 0.75, 1.0, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.734375, 0.734375, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 0.75, 0.734375, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 0.734375, 0.734375, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 0.734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.75, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.734375, 1.0, 0.734375, 0.734375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.75, 0.75, 1.0, 1.0, 0.734375, 0.75, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.734375, 1.0, 0.75, 0.734375, 1.0, 0.734375, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.109375, 1.0, 1.0, 0.09765625, 1.0, 0.109375, 1.0, 1.0, 0.1015625, 1.0, 0.1484375, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.1328125, 0.12109375, 0.11328125, 0.11328125, 1.0, 1.0, 0.10546875, 0.140625, 1.0, 0.12109375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.23828125, 0.10546875, 1.0, 0.09765625, 1.0, 0.11328125, 0.10546875, 1.0, 1.0, 1.0, 0.109375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.11328125, 1.0, 0.109375, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0]

 sparsity of   [1.0, 0.6657986044883728, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6579861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6675347089767456, 1.0, 1.0, 0.6579861044883728, 1.0, 0.663194477558136, 0.6571180820465088, 1.0, 0.663194477558136, 0.6588541865348816, 0.6614583134651184, 1.0, 0.6623263955116272, 1.0, 0.6675347089767456, 1.0, 0.6736111044883728, 1.0, 1.0, 0.6623263955116272, 0.6614583134651184, 1.0, 0.6545138955116272, 0.6657986044883728, 1.0, 0.6649305820465088, 0.6623263955116272, 0.6640625, 1.0, 0.6614583134651184, 1.0, 0.6640625, 0.6597222089767456, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.655381977558136, 1.0, 0.6762152910232544, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 0.6588541865348816, 1.0, 0.6605902910232544, 1.0, 0.6640625, 0.6727430820465088, 0.6597222089767456, 1.0, 0.663194477558136, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 0.6579861044883728, 1.0, 1.0, 0.6675347089767456, 1.0, 0.6614583134651184, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6657986044883728, 1.0, 1.0, 0.6579861044883728, 0.6597222089767456, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 0.6597222089767456, 1.0, 0.655381977558136, 0.6623263955116272, 1.0, 1.0, 0.6623263955116272, 1.0, 0.6588541865348816, 1.0, 1.0, 0.6649305820465088, 0.663194477558136, 1.0, 1.0, 1.0, 0.6640625, 1.0, 0.6649305820465088, 0.671875, 0.6579861044883728, 1.0]

 sparsity of   [0.5703125, 0.578125, 1.0, 1.0, 0.578125, 1.0, 0.578125, 1.0, 0.59375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 0.5859375, 1.0, 0.578125, 0.578125, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 0.5859375, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 0.5703125, 1.0, 1.0, 0.578125, 0.5859375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5703125, 0.578125, 1.0, 0.5859375, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 0.578125, 0.5703125, 0.578125, 1.0, 1.0, 0.5859375, 1.0, 1.0, 0.5703125, 0.5703125, 0.578125, 0.5859375, 1.0, 0.578125, 0.578125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5859375, 0.5703125, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.5703125, 1.0, 1.0, 0.5859375, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.59375, 0.5859375, 0.5859375, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 0.578125, 0.5703125, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 0.59375, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5859375, 0.59375, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59375, 0.5859375, 0.5703125, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 0.578125, 0.578125, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.5703125, 0.5703125, 1.0, 0.578125, 0.5703125, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.59375, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5859375, 1.0, 0.5703125, 0.578125, 0.5859375, 1.0, 0.5703125, 0.5703125, 0.5703125, 0.5859375, 1.0, 0.578125, 0.578125, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 0.5703125, 0.578125, 1.0, 0.5859375, 0.5703125, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 0.5703125, 0.578125, 1.0, 0.5859375, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.578125, 1.0, 0.578125, 1.0, 0.578125, 0.578125, 0.578125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 1.0, 0.578125, 0.5703125, 0.5859375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.578125, 0.5703125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.578125, 0.5859375, 1.0, 1.0, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 1.0, 0.5859375, 0.578125, 1.0, 0.578125, 0.5703125, 1.0, 0.59375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.578125, 0.5703125, 0.5703125, 0.578125, 1.0, 0.578125, 1.0, 0.5859375, 1.0, 1.0, 0.5703125, 1.0, 0.5859375, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 0.5703125, 1.0, 0.578125, 0.578125, 1.0, 1.0, 0.5703125, 1.0, 0.578125, 0.5859375, 1.0, 0.578125, 1.0, 1.0, 0.578125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.578125, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 0.6015625, 0.578125, 1.0, 0.578125, 0.5859375, 1.0, 0.578125, 1.0, 0.5703125, 1.0, 0.578125, 1.0, 1.0, 0.5703125, 0.59375, 0.5703125, 1.0, 0.578125, 0.5859375, 0.5703125, 0.5703125, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.578125, 0.5859375, 0.578125, 0.578125, 0.5703125, 0.578125, 1.0, 0.5703125, 1.0, 1.0, 0.5703125, 0.6015625, 1.0, 1.0, 0.5859375, 1.0, 0.578125, 1.0, 0.5859375, 0.578125, 1.0, 1.0, 1.0, 0.5859375, 1.0, 0.59375, 0.578125, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 0.5859375, 0.5703125]

 sparsity of   [0.12109375, 0.14453125, 1.0, 1.0, 0.11328125, 1.0, 0.125, 1.0, 0.125, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1328125, 0.18359375, 1.0, 1.0, 0.11328125, 0.18359375, 0.1953125, 1.0, 0.12109375, 1.0, 0.1171875, 0.1328125, 0.27734375, 0.12109375, 1.0, 0.11328125, 0.1171875, 1.0, 1.0, 1.0, 0.1953125, 0.12109375, 0.125, 1.0, 0.20703125, 1.0, 0.125, 0.10546875, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.12109375, 0.14453125, 0.15625, 1.0, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 1.0, 0.2265625, 1.0, 0.18359375, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.11328125, 1.0, 0.17578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.16796875, 0.15234375, 1.0, 0.11328125, 0.1640625, 0.12109375, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 0.14453125, 0.23046875, 0.11328125, 1.0, 0.1171875, 0.1875, 0.10546875, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.14453125, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.125, 0.12109375, 0.125, 0.11328125, 0.11328125, 0.11328125, 0.11328125, 0.125, 1.0, 1.0, 1.0, 0.1171875, 0.11328125, 1.0, 0.1171875, 0.2109375, 1.0, 0.14453125, 1.0, 1.0, 0.1171875, 1.0, 0.1484375, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.12109375, 0.12890625, 0.10546875, 0.1796875, 0.11328125, 0.12109375, 0.125, 1.0, 1.0, 0.17578125, 1.0, 0.1796875, 0.109375, 0.12109375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 0.10546875, 0.109375, 1.0, 1.0, 1.0, 0.12109375, 0.109375, 0.12109375, 0.26953125, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.125, 1.0, 1.0, 1.0, 0.19140625, 0.12109375, 1.0, 0.1171875, 0.19140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.1875, 0.12109375, 1.0, 1.0, 0.11328125, 0.1171875, 0.10546875, 0.109375, 0.1171875, 0.1171875, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.1171875, 1.0, 0.171875, 1.0, 0.1484375, 0.1171875, 1.0, 0.12109375, 0.13671875, 1.0, 0.12890625, 0.1171875, 1.0, 0.11328125, 0.109375, 0.11328125, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.125, 0.1171875, 0.16015625, 1.0, 0.12890625, 1.0, 1.0, 1.0, 0.13671875, 0.11328125, 0.12109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.11328125, 0.1171875, 0.12109375, 1.0, 0.16015625, 0.1171875, 0.21484375, 0.125, 1.0, 0.13671875, 1.0, 1.0, 0.17578125, 1.0, 0.109375, 0.109375, 0.10546875, 0.12890625, 1.0, 0.109375, 0.13671875, 1.0, 0.203125, 0.1171875, 0.23046875, 1.0, 0.19921875, 0.1171875, 1.0, 0.125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.265625, 0.1328125, 1.0, 0.12109375, 1.0, 0.1171875, 0.1171875, 0.140625, 0.1328125, 0.1171875, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.15625, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.11328125, 0.17578125, 0.109375, 1.0, 1.0, 1.0, 0.1484375, 1.0, 1.0, 1.0, 0.11328125, 0.28515625, 0.12109375, 1.0, 1.0, 0.12109375, 1.0, 1.0, 0.17578125, 1.0, 1.0, 0.1640625, 0.109375, 0.15234375, 1.0, 0.203125, 1.0, 0.125, 0.12109375, 1.0, 1.0, 0.13671875, 1.0, 0.125, 0.20703125, 0.11328125, 1.0, 1.0, 0.15625, 1.0, 0.12890625, 1.0, 1.0, 0.109375, 0.10546875, 1.0, 0.11328125, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.125, 0.1171875, 0.19140625, 0.09765625, 1.0, 0.12109375, 1.0, 0.1171875, 1.0, 1.0, 0.15625, 0.171875, 0.1171875, 1.0, 0.140625, 1.0, 0.1171875, 1.0, 0.1640625, 1.0, 0.1171875, 0.1171875, 1.0, 1.0, 0.109375, 1.0, 0.171875, 0.12109375, 1.0, 0.125, 1.0, 1.0, 0.1171875, 0.17578125, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.12109375, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.1796875, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 0.1171875, 1.0, 0.16796875, 0.11328125, 1.0, 0.109375, 1.0, 0.11328125, 1.0, 0.1171875, 1.0, 1.0, 0.13671875, 0.109375, 0.16015625, 1.0, 0.1171875, 0.1171875, 0.11328125, 0.109375, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12109375, 0.11328125, 0.11328125, 0.1171875, 0.109375, 0.1171875, 1.0, 0.12109375, 1.0, 1.0, 0.16015625, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 0.11328125, 1.0, 0.12109375, 0.11328125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.109375, 0.125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.11328125, 0.16015625]

 sparsity of   [0.482421875, 1.0, 1.0, 1.0, 0.5, 1.0, 0.494140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 0.484375, 1.0, 0.490234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 0.490234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.486328125, 0.494140625, 1.0, 1.0, 0.47265625, 1.0, 1.0, 1.0, 0.494140625, 0.4765625, 1.0, 0.484375, 0.5, 1.0, 0.482421875, 1.0, 0.47265625, 0.498046875, 0.484375, 1.0, 0.474609375, 0.4765625, 0.474609375, 0.501953125, 1.0, 0.478515625, 1.0, 0.482421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4765625, 1.0, 0.474609375, 1.0, 1.0, 0.48828125, 0.515625, 0.494140625, 0.474609375, 1.0, 0.482421875, 1.0, 1.0, 0.474609375, 1.0, 0.48828125, 1.0, 1.0, 0.474609375, 1.0, 1.0, 1.0, 0.470703125, 0.478515625, 1.0, 1.0, 1.0, 0.48828125, 1.0, 1.0, 1.0, 0.498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.48828125, 0.484375, 1.0, 0.474609375, 1.0, 0.484375, 0.478515625, 1.0, 1.0, 0.4765625, 1.0, 1.0, 0.494140625, 1.0, 1.0, 0.4921875, 1.0]

 sparsity of   [0.6102430820465088, 0.6215277910232544, 0.6171875, 0.608506977558136, 1.0, 0.655381977558136, 0.6111111044883728, 1.0, 0.6701388955116272, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6119791865348816, 0.6111111044883728, 1.0, 1.0, 1.0, 1.0, 0.6111111044883728, 1.0, 0.6111111044883728, 1.0, 1.0, 0.609375, 0.6197916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.600694477558136, 1.0, 0.6119791865348816, 0.609375, 0.6675347089767456, 0.6128472089767456, 0.6076388955116272, 0.6519097089767456, 1.0, 0.6119791865348816, 0.6493055820465088, 0.6493055820465088, 0.616319477558136, 1.0, 0.6449652910232544, 1.0, 0.6232638955116272, 1.0, 0.6059027910232544, 1.0, 1.0, 1.0, 0.616319477558136, 1.0, 0.6232638955116272, 0.6154513955116272, 0.6423611044883728, 1.0, 1.0, 1.0, 1.0, 0.6137152910232544, 0.6111111044883728, 0.608506977558136, 1.0, 1.0, 1.0, 1.0, 0.6059027910232544, 1.0, 1.0, 0.6059027910232544, 1.0, 1.0, 0.6527777910232544, 0.6171875, 1.0, 0.6111111044883728, 1.0, 0.6111111044883728, 0.6102430820465088, 1.0, 0.6111111044883728, 0.6032986044883728, 1.0, 0.616319477558136, 0.608506977558136, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6076388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6189236044883728, 0.616319477558136, 0.6067708134651184, 0.6102430820465088, 0.609375, 1.0, 0.6059027910232544, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.6059027910232544, 0.6484375, 1.0, 0.639756977558136, 1.0, 1.0, 0.6102430820465088, 1.0]

 sparsity of   [0.546875, 0.53125, 0.5625, 0.53125, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 1.0, 0.5390625, 1.0, 1.0, 1.0, 0.53125, 0.5625, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 0.5546875, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 1.0, 0.5390625, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.53125, 0.53125, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.53125, 0.53125, 0.546875, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 0.546875, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 1.0, 0.5546875, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.53125, 0.546875, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.5546875, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 1.0, 0.5625, 0.5390625, 1.0, 0.5390625, 1.0, 0.53125, 0.5625, 1.0, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5546875, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.5390625, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.5390625, 0.5390625, 1.0, 0.53125, 1.0, 1.0, 0.53125, 0.546875, 1.0, 0.5390625, 1.0, 0.546875, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.53125, 0.53125, 1.0, 0.5390625, 0.53125, 0.53125, 0.5390625, 0.5390625, 0.53125, 0.5546875, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.53125, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 0.5546875, 0.5546875, 0.5390625, 1.0, 1.0, 0.546875, 1.0, 0.5390625, 1.0, 1.0, 0.53125, 1.0, 0.546875, 0.5390625, 0.53125, 0.5625, 1.0, 1.0, 0.53125, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 0.53125, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.5703125, 0.53125, 0.5390625, 0.53125, 1.0, 0.5390625, 1.0, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 1.0, 1.0, 0.5546875, 0.53125, 0.546875, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5390625, 0.5390625, 0.5390625, 0.53125, 1.0, 0.546875, 0.546875, 1.0, 0.53125, 0.5390625, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 0.53125, 0.5390625, 0.546875, 0.5546875, 0.5390625, 0.546875, 0.5390625, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.5390625, 1.0, 0.5390625, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.5625, 0.546875, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 0.546875, 0.546875, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.53125, 0.5625, 1.0, 0.5390625, 0.53125, 1.0, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.5390625, 0.53125, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.5546875, 1.0, 0.5390625, 0.546875, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.546875, 1.0, 1.0, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 0.546875, 1.0, 1.0, 0.53125, 1.0, 1.0, 0.5546875, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 1.0, 0.53125, 1.0, 0.5390625, 0.53125, 0.53125, 0.53125, 0.5390625, 1.0, 0.546875, 0.53125, 1.0, 0.53125, 0.5703125, 0.53125, 0.5390625, 0.546875, 0.53125, 1.0, 1.0, 0.5390625, 0.5390625, 0.53125, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.5546875, 0.5703125, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.546875, 0.53125, 1.0, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 1.0, 0.53125, 0.578125, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 0.5390625, 1.0, 1.0, 0.5390625, 0.53125, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.53125, 0.5390625, 1.0, 0.53125, 0.53125, 0.53125, 1.0, 0.53125, 0.53125, 0.53125, 0.53125, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.546875, 0.5390625, 1.0, 0.53125, 1.0, 0.53125, 1.0, 0.53125, 0.53125, 0.5390625, 0.53125, 0.53125, 0.546875, 0.5625, 0.53125, 0.53125, 0.53125]

 sparsity of   [1.0, 0.244140625, 1.0, 1.0, 0.333984375, 1.0, 0.240234375, 0.2421875, 0.306640625, 0.251953125, 0.26953125, 0.3125, 0.25390625, 1.0, 0.244140625, 0.248046875, 1.0, 0.310546875, 1.0, 0.326171875, 1.0, 1.0, 0.24609375, 1.0, 0.25390625, 0.24609375, 0.24609375, 1.0, 1.0, 0.25, 1.0, 1.0, 0.263671875, 0.26171875, 1.0, 1.0, 1.0, 1.0, 0.341796875, 1.0, 0.328125, 0.24609375, 0.26953125, 1.0, 1.0, 0.24609375, 1.0, 0.2421875, 0.259765625, 0.302734375, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.255859375, 0.24609375, 0.244140625, 0.244140625, 1.0, 1.0, 1.0, 1.0, 0.259765625, 1.0, 0.267578125, 0.2421875, 0.26171875, 0.25, 0.248046875, 0.251953125, 0.248046875, 0.267578125, 0.27734375, 1.0, 0.298828125, 0.25, 0.24609375, 1.0, 0.255859375, 0.310546875, 0.251953125, 1.0, 1.0, 1.0, 0.2421875, 1.0, 1.0, 0.248046875, 0.2734375, 0.25390625, 1.0, 1.0, 0.244140625, 0.251953125, 0.24609375, 1.0, 0.25390625, 1.0, 1.0, 0.24609375, 0.265625, 0.259765625, 0.25, 0.2734375, 1.0, 1.0, 1.0, 0.28125, 0.26171875, 0.2578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25390625, 1.0, 0.25390625, 1.0, 0.24609375, 0.24609375, 0.24609375, 0.240234375, 0.24609375, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.433159738779068, 1.0, 0.4383680522441864, 0.4765625, 0.4357638955116272, 0.4375, 1.0, 0.4392361044883728, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 1.0, 1.0, 1.0, 0.4383680522441864, 1.0, 0.4835069477558136, 1.0, 0.495659738779068, 0.4375, 1.0, 1.0, 0.4835069477558136, 0.4383680522441864, 1.0, 1.0, 0.4401041567325592, 0.440972238779068, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4357638955116272, 1.0, 0.4383680522441864, 1.0, 0.4375, 1.0, 0.4383680522441864, 1.0, 1.0, 1.0, 0.4401041567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4392361044883728, 0.4479166567325592, 0.4375, 1.0, 0.4375, 0.4392361044883728, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 1.0, 1.0, 0.4375, 0.4930555522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4401041567325592, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4392361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4383680522441864, 0.4383680522441864, 1.0, 0.4383680522441864, 0.4383680522441864, 1.0, 0.4383680522441864, 0.4748263955116272, 0.433159738779068, 0.4861111044883728, 0.4375, 0.5026041865348816, 0.4366319477558136, 1.0, 0.4366319477558136, 1.0, 1.0, 0.4739583432674408, 0.5008680820465088, 1.0, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4383680522441864, 1.0, 1.0]

 sparsity of   [1.0, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6171875, 0.6015625, 0.6015625, 1.0, 0.6171875, 0.6015625, 1.0, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6171875, 0.6171875, 0.609375, 1.0, 0.609375, 0.6015625, 1.0, 1.0, 0.609375, 0.6015625, 0.609375, 0.6171875, 0.609375, 0.6015625, 0.6171875, 0.6015625, 0.609375, 0.609375, 1.0, 0.6015625, 0.6015625, 0.625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 1.0, 0.6171875, 0.6015625, 0.609375, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 0.6171875, 1.0, 0.6171875, 0.609375, 0.6015625, 0.609375, 0.6171875, 1.0, 0.609375, 0.609375, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.6171875, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 1.0, 0.6171875, 0.609375, 0.6015625, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 0.6015625, 0.6171875, 0.6015625, 0.625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6171875, 0.6015625, 0.6015625, 0.6171875, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.625, 0.6171875, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.609375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 1.0, 0.609375, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.6015625, 1.0, 0.609375, 0.609375, 0.609375, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 1.0, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 0.6015625, 0.6171875, 0.6015625, 1.0, 0.6015625, 0.609375, 0.609375, 0.625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.609375, 0.609375, 0.6015625, 1.0, 0.609375, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.609375, 1.0, 0.609375, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.6015625, 0.6875, 0.6171875, 1.0, 1.0, 0.6015625, 1.0, 0.6015625, 0.625, 0.609375, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 0.609375, 0.609375, 1.0, 0.625, 0.6171875, 1.0, 0.609375, 1.0, 0.625, 0.609375, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.609375, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.609375, 1.0, 0.6015625, 0.625, 0.6015625, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.6015625, 0.6171875, 1.0, 0.609375, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6328125, 0.6015625, 1.0, 0.6171875, 0.609375, 1.0, 0.6015625, 1.0, 0.609375, 0.6015625, 0.6015625, 0.609375, 1.0, 1.0, 0.609375, 0.6015625, 0.6171875, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.609375, 0.609375, 0.6015625, 0.6796875, 0.609375, 0.6171875, 0.6015625, 1.0, 1.0, 1.0, 0.609375, 0.6015625, 1.0, 0.625, 0.6015625, 1.0, 1.0, 0.6015625, 0.609375, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 0.6171875, 1.0, 1.0, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 1.0, 1.0, 0.6015625, 0.6015625, 0.6015625, 1.0, 0.6015625, 0.6015625, 0.609375, 0.640625, 0.6015625, 0.6015625, 0.6015625, 0.609375, 0.609375, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.609375, 1.0, 0.6015625, 0.6015625, 0.609375, 0.6015625, 0.6171875, 1.0, 0.609375, 0.609375, 0.6015625, 0.6015625, 0.6015625, 1.0, 1.0, 1.0, 0.6171875, 0.6015625]

 sparsity of   [0.125, 1.0, 1.0, 0.125, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.138671875, 0.12890625, 1.0, 0.283203125, 1.0, 0.125, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.146484375, 1.0, 0.271484375, 0.271484375, 1.0, 0.259765625, 1.0, 1.0, 0.123046875, 0.123046875, 1.0, 1.0, 0.123046875, 0.1484375, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.271484375, 0.185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 1.0, 1.0, 0.123046875, 1.0, 1.0, 0.146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 0.1484375, 1.0, 1.0, 0.125, 1.0, 1.0, 0.197265625, 0.255859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 0.130859375, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.123046875, 0.142578125, 0.150390625, 1.0, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.228515625, 1.0, 1.0, 1.0, 0.240234375, 0.142578125, 1.0, 1.0, 0.310546875, 0.1640625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6649305820465088, 1.0, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6657986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6684027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6692708134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6675347089767456, 1.0, 0.6727430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 0.8203125, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 0.8125, 1.0, 0.8203125, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8125, 1.0, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8203125, 1.0, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.103515625, 1.0, 1.0, 0.09765625, 1.0, 0.091796875, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.099609375, 0.091796875, 1.0, 1.0, 0.103515625, 1.0, 0.09765625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.095703125, 1.0, 0.09765625, 1.0, 0.099609375, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.10546875, 0.09765625, 0.09765625, 1.0, 1.0, 0.103515625, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 0.09375, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.091796875, 1.0, 0.111328125, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.099609375, 1.0, 1.0, 0.1171875, 1.0, 0.115234375, 1.0, 1.0, 0.087890625, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.091796875, 0.103515625, 0.099609375, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 0.09765625, 0.115234375, 1.0, 0.091796875, 0.091796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08984375, 1.0, 0.09765625, 1.0, 1.0, 0.08984375, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 0.091796875, 0.09765625, 0.103515625, 1.0, 0.09375, 1.0, 1.0, 1.0]

 sparsity of   [0.7352430820465088, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7339409589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7352430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7378472089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7365451455116272, 0.7356770634651184, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 0.7361111044883728, 0.7352430820465088, 1.0, 1.0, 0.7365451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7356770634651184, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7365451455116272, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7348090410232544, 0.7361111044883728, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 0.7356770634651184, 0.7348090410232544, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 0.7352430820465088, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7348090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7352430820465088, 1.0, 0.7356770634651184, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7356770634651184, 1.0, 1.0, 0.7369791865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7361111044883728, 1.0]

 sparsity of   [0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.81640625, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.81640625, 1.0, 0.80859375, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.81640625, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.81640625, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 0.8125, 1.0, 0.8125, 0.80859375, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 0.80859375, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.8125, 0.80859375, 1.0, 0.8125, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8125, 0.80859375, 0.8125, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.8125, 1.0, 1.0, 0.80859375, 0.81640625, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 0.8125, 1.0, 0.80859375, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 0.8125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80859375, 0.80859375, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.80859375, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 0.8125, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 0.80859375, 1.0, 0.8125, 0.8125, 0.80859375, 1.0]

 sparsity of   [0.13671875, 1.0, 0.109375, 0.1328125, 0.099609375, 1.0, 0.103515625, 0.10546875, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.119140625, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.09765625, 0.099609375, 1.0, 0.11328125, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.2109375, 0.115234375, 0.109375, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.109375, 0.1171875, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 0.09765625, 0.109375, 1.0, 0.189453125, 1.0, 1.0, 0.091796875, 0.14453125, 0.111328125, 0.12109375, 1.0, 0.111328125, 0.1015625, 0.111328125, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.111328125, 0.111328125, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.099609375, 0.123046875, 1.0, 1.0, 0.11328125, 1.0, 0.08984375, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.119140625, 1.0, 1.0, 1.0, 0.126953125, 1.0, 1.0, 0.111328125, 0.107421875, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.095703125, 0.103515625, 0.1171875, 0.107421875, 0.099609375, 1.0, 0.09765625, 0.119140625, 1.0, 1.0, 0.10546875, 0.103515625, 0.1328125, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.134765625, 0.21484375, 1.0, 0.103515625, 1.0, 1.0, 0.150390625, 1.0, 0.1171875, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.095703125, 0.142578125, 1.0, 0.109375, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.095703125, 0.109375, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.123046875, 0.15234375, 0.130859375, 0.103515625, 1.0, 0.1015625, 1.0, 1.0, 0.11328125, 0.111328125, 1.0, 0.1015625, 0.103515625, 1.0, 1.0, 1.0, 0.103515625, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.1015625, 0.123046875, 0.134765625, 1.0, 1.0, 0.103515625, 0.12109375, 1.0, 0.099609375, 0.111328125, 0.09765625, 0.1015625, 1.0, 1.0, 1.0, 0.103515625, 0.1015625, 0.12890625, 1.0, 0.09765625, 1.0, 0.109375, 1.0, 0.265625, 1.0, 0.095703125, 0.13671875, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 1.0, 0.103515625, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 0.10546875, 1.0, 0.09375, 1.0, 0.14453125, 1.0, 1.0, 0.111328125, 0.11328125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.126953125, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.1171875, 0.10546875, 0.1015625, 1.0, 0.12109375, 1.0, 1.0, 0.107421875, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 1.0, 0.09765625, 0.146484375, 0.125, 0.09765625, 1.0, 0.111328125, 0.103515625, 0.09375, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.123046875, 1.0, 0.09765625, 1.0, 1.0, 0.103515625, 1.0, 1.0, 0.115234375, 0.1015625, 1.0, 1.0, 1.0, 0.1171875, 0.142578125, 1.0, 0.12109375, 0.111328125, 0.10546875, 0.1171875, 0.09375, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.09765625, 0.09765625, 1.0, 0.279296875, 1.0, 1.0, 0.125, 0.125, 1.0, 1.0, 0.099609375, 1.0, 0.10546875, 0.1015625, 1.0, 1.0, 0.095703125, 0.103515625, 1.0, 0.12109375, 1.0, 0.103515625, 0.123046875, 0.09375, 0.107421875, 0.16796875, 1.0, 0.1015625, 0.126953125, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 0.107421875, 0.10546875, 1.0, 1.0, 0.107421875, 0.103515625, 1.0, 1.0, 0.1171875, 0.107421875, 0.107421875, 0.13671875, 0.130859375, 0.11328125, 1.0, 0.12890625, 0.107421875, 0.107421875, 0.13671875, 1.0, 0.09765625, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.103515625, 0.107421875, 1.0, 0.099609375, 1.0, 0.2109375, 1.0, 0.1015625, 1.0, 1.0, 0.103515625, 0.10546875, 0.111328125, 0.130859375, 0.123046875, 1.0, 0.107421875, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 1.0, 0.103515625, 0.099609375, 0.103515625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.193359375, 1.0, 1.0, 0.1328125, 1.0, 0.158203125, 1.0, 0.126953125, 0.125, 1.0, 0.12109375, 0.09375, 0.119140625, 0.11328125, 0.123046875, 0.10546875, 0.138671875, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.154296875, 1.0, 1.0, 1.0, 1.0, 0.203125, 0.126953125, 0.111328125, 0.10546875, 1.0, 1.0, 0.1015625, 0.099609375, 1.0, 1.0, 0.09375, 0.10546875, 1.0, 1.0, 0.173828125, 1.0, 0.111328125, 0.248046875, 1.0, 1.0, 0.111328125, 1.0, 1.0, 0.109375, 1.0, 0.138671875, 0.115234375, 1.0, 0.109375, 0.11328125, 1.0, 1.0, 0.16015625, 1.0, 0.107421875, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.138671875, 1.0, 0.228515625, 0.099609375, 0.1484375, 0.091796875, 1.0, 0.12109375, 1.0, 1.0, 0.103515625, 0.109375, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 0.1015625, 0.095703125, 1.0, 1.0, 0.099609375, 1.0, 0.103515625, 1.0, 1.0, 0.103515625, 1.0, 0.09765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 0.107421875, 0.111328125, 1.0, 0.115234375, 1.0, 1.0, 0.115234375, 0.15234375, 0.11328125, 1.0, 0.111328125, 1.0, 1.0, 0.130859375, 0.099609375, 0.095703125, 0.115234375, 1.0, 0.107421875, 0.1328125, 1.0, 1.0, 0.111328125, 1.0, 0.146484375, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.10546875, 1.0, 0.103515625, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.130859375, 0.095703125, 0.109375, 0.123046875, 1.0, 0.095703125, 0.111328125, 0.09765625, 1.0, 0.1015625, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.09765625, 1.0, 1.0, 0.130859375, 1.0, 1.0, 0.119140625, 0.10546875, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.099609375, 0.134765625, 0.107421875, 1.0, 1.0, 0.1015625, 0.09765625, 1.0, 0.103515625, 0.115234375, 1.0, 0.099609375, 0.130859375, 0.1015625, 0.109375, 1.0, 0.103515625, 1.0, 1.0, 0.259765625, 1.0, 1.0, 1.0, 0.126953125, 0.109375, 0.1015625, 1.0, 1.0, 0.099609375, 1.0, 0.126953125, 1.0, 1.0, 0.107421875, 1.0, 0.099609375, 0.123046875, 0.115234375, 1.0, 1.0, 1.0, 0.134765625, 1.0, 1.0, 1.0, 0.119140625, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.1171875, 1.0, 0.115234375, 0.107421875, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 0.095703125, 1.0, 0.11328125, 0.111328125, 1.0, 1.0, 0.140625, 0.154296875, 1.0, 0.134765625, 1.0, 1.0, 0.130859375, 0.09765625, 0.107421875, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1484375, 0.1015625, 0.09765625, 1.0, 1.0, 0.09765625, 0.12109375, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.099609375, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.123046875, 1.0, 1.0, 1.0, 0.134765625, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.12109375, 0.09765625, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1015625, 0.21484375, 0.10546875, 1.0, 0.103515625, 0.099609375, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.115234375, 1.0, 0.103515625, 1.0, 1.0, 0.09375, 0.126953125, 1.0, 0.1328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.11328125, 0.099609375, 0.13671875, 1.0, 0.115234375, 1.0, 0.1015625, 0.119140625, 0.119140625, 1.0, 0.107421875, 1.0, 0.123046875, 0.095703125, 0.119140625, 1.0, 1.0, 0.103515625, 0.107421875, 0.138671875, 0.099609375, 1.0, 0.1015625, 1.0, 0.119140625, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1328125, 0.12109375, 1.0, 0.11328125, 0.10546875, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.103515625, 0.095703125, 1.0, 0.107421875, 1.0, 0.12109375, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.099609375, 0.10546875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.119140625, 1.0, 1.0, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 0.109375, 0.123046875, 1.0, 0.1171875, 1.0, 0.1015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 1.0, 1.0, 0.09765625, 1.0, 0.16796875, 1.0, 1.0, 0.126953125, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.130859375, 0.125, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 0.09375, 0.09375, 1.0, 0.125, 1.0, 1.0, 1.0, 0.1328125, 1.0, 0.099609375, 1.0, 0.1171875, 0.111328125, 0.09375, 0.12109375, 1.0, 0.111328125, 1.0, 0.107421875, 0.11328125, 1.0, 0.115234375, 0.1171875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.51171875, 1.0, 0.521484375, 1.0, 1.0, 1.0, 0.5224609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 0.5205078125, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.509765625, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 1.0, 1.0, 0.517578125, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 0.5224609375, 0.5205078125, 1.0, 1.0, 0.5078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5400390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.517578125, 1.0, 0.5234375, 0.5107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.52734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5322265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 0.5185546875, 1.0, 0.517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 1.0, 1.0, 0.5166015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.517578125, 0.5146484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.515625, 1.0, 1.0, 1.0, 1.0, 0.51953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5205078125, 0.517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.521484375, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.796006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8051215410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7955729365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7747395634651184, 1.0, 0.815538227558136, 1.0, 1.0, 0.753038227558136, 0.828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7703993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 0.8029513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7951388955116272, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 0.8424479365348816, 1.0, 0.838975727558136, 1.0, 1.0, 0.8129340410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7747395634651184, 1.0, 1.0, 0.78515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8059895634651184, 1.0, 0.8337673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8172743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8355034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8285590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7934027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7434895634651184, 1.0, 0.8064236044883728, 0.7621527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 0.8463541865348816, 0.7170138955116272, 1.0, 1.0]

 sparsity of   [0.87109375, 1.0, 0.87109375, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 0.86328125, 0.87109375, 1.0, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.8671875, 0.86328125, 0.87109375, 1.0, 0.87109375, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.859375, 1.0, 1.0, 0.87109375, 0.859375, 1.0, 0.8671875, 1.0, 1.0, 0.875, 0.86328125, 0.87109375, 0.8671875, 1.0, 0.875, 0.8671875, 0.859375, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 0.875, 1.0, 1.0, 0.859375, 0.87109375, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 0.87109375, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 0.875, 0.86328125, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.875, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.86328125, 0.8671875, 0.87109375, 1.0, 1.0, 0.87109375, 0.86328125, 1.0, 0.87109375, 0.859375, 0.8671875, 0.859375, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 0.859375, 1.0, 0.8671875, 1.0, 0.86328125, 0.86328125, 1.0, 1.0, 0.87109375, 0.875, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.875, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.875, 0.859375, 1.0, 1.0, 1.0, 0.87109375, 0.8671875, 0.859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.87109375, 0.875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87890625, 0.86328125, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 0.859375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.875, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.859375, 0.87109375, 1.0, 0.875, 1.0, 0.86328125, 0.875, 1.0, 1.0, 0.859375, 0.8671875, 1.0, 0.8671875, 1.0, 0.859375, 0.8671875, 0.87109375, 0.859375, 0.8671875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.86328125, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 0.859375, 0.8671875, 0.87109375, 0.87109375, 1.0, 0.87109375, 0.8671875, 0.86328125, 0.8671875, 1.0, 0.8828125, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 0.87109375, 1.0, 0.86328125, 1.0, 0.859375, 1.0, 0.87109375, 1.0, 0.87109375, 0.8671875, 1.0, 0.8671875, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.87109375, 0.86328125, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 0.86328125, 0.8671875, 1.0, 0.87890625, 0.86328125, 1.0, 0.875, 0.86328125, 0.87109375, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 0.8671875, 0.87109375, 1.0, 0.87109375, 1.0, 0.8671875, 0.86328125, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 0.859375, 0.87890625, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.8671875, 0.8671875, 1.0, 0.859375, 1.0, 1.0, 0.8671875, 1.0, 0.86328125, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.8671875, 0.87109375, 0.8671875, 0.875, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.8671875, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.86328125, 0.86328125, 1.0, 0.86328125, 0.87109375, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 0.8671875, 0.86328125, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 1.0, 0.87890625, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.8671875, 0.86328125, 0.8671875, 0.859375, 1.0, 0.859375, 0.8671875, 0.87109375, 1.0, 0.87109375, 0.859375, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 0.8671875, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.8671875, 0.87109375, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.875, 0.86328125, 1.0, 0.859375, 0.87109375, 0.8671875, 0.87109375, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.8671875, 0.87109375, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 0.87109375, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.8671875, 0.875, 1.0, 0.8671875, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 0.87109375, 0.859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.87109375, 1.0, 0.875, 1.0, 0.87109375, 0.875, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.8671875, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 0.87109375, 0.87109375, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 0.859375, 0.87109375, 0.86328125, 0.86328125, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 0.875, 0.87109375, 0.8671875, 1.0, 0.87109375, 0.86328125, 1.0, 1.0, 0.859375, 1.0, 1.0, 1.0, 1.0, 0.859375, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.875, 0.87109375, 0.8671875, 1.0, 0.87890625, 0.8671875, 0.8671875, 0.8671875, 0.8671875, 1.0, 0.86328125, 1.0, 0.86328125, 0.86328125, 0.8671875, 0.86328125, 1.0, 0.875, 0.87109375, 1.0, 0.87109375, 0.87109375, 0.86328125, 1.0, 0.875, 0.86328125, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8671875, 1.0, 1.0, 0.859375, 0.8671875, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 0.87109375, 0.8671875, 1.0, 1.0, 1.0, 0.875, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.859375, 0.86328125, 1.0, 1.0, 0.86328125, 0.8671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.87109375, 1.0, 1.0, 1.0, 1.0, 0.875, 0.859375, 1.0, 0.87109375, 1.0, 1.0, 0.875, 0.87109375, 1.0, 0.8671875, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 0.87109375, 0.875, 1.0, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 0.87109375, 1.0, 1.0, 0.86328125, 1.0, 1.0, 0.8671875, 0.8671875, 1.0, 0.86328125, 0.87109375, 1.0, 1.0, 0.8671875, 1.0, 0.875, 0.87109375, 0.87109375, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.86328125, 0.86328125, 0.86328125, 1.0, 1.0, 0.86328125, 0.86328125, 1.0, 0.87109375, 1.0, 1.0, 1.0, 0.8671875, 1.0, 0.87109375, 1.0, 0.87109375, 0.86328125, 0.8671875, 0.87109375, 1.0, 0.86328125, 1.0, 0.8828125, 0.87109375, 1.0, 0.8671875, 0.87109375, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 1.0, 0.4638671875, 1.0, 1.0, 1.0, 1.0, 0.458984375, 1.0, 0.4501953125, 0.46484375, 0.4697265625, 0.462890625, 1.0, 1.0, 0.4541015625, 0.46484375, 1.0, 1.0, 0.4697265625, 0.453125, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4501953125, 1.0, 1.0, 0.470703125, 1.0, 1.0, 1.0, 1.0, 0.4462890625, 1.0, 0.4912109375, 0.4658203125, 1.0, 1.0, 0.466796875, 1.0, 1.0, 0.4580078125, 0.46484375, 1.0, 0.458984375, 1.0, 0.4580078125, 1.0, 1.0, 0.4599609375, 1.0, 1.0, 0.447265625, 1.0, 1.0, 0.4638671875, 1.0, 1.0, 0.4619140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 0.4619140625, 1.0, 1.0, 0.46875, 0.4599609375, 1.0, 1.0, 1.0, 0.4541015625, 0.462890625, 1.0, 1.0, 0.458984375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4580078125, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4609375, 1.0, 0.4697265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46484375, 0.46875, 1.0, 0.46484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4599609375, 1.0, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4677734375, 1.0, 0.4638671875, 1.0, 1.0, 1.0, 0.4619140625, 0.45703125, 0.4609375, 0.4580078125, 0.4619140625, 1.0, 1.0, 0.462890625, 0.4599609375, 1.0, 1.0, 1.0, 1.0, 0.4453125, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4560546875, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4716796875, 0.4755859375, 0.45703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.462890625, 1.0, 1.0, 1.0, 0.4599609375, 1.0, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4658203125, 1.0, 1.0, 1.0, 0.4619140625, 0.5693359375, 1.0, 1.0, 1.0, 0.458984375, 1.0, 1.0, 1.0, 0.4609375, 0.4609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4658203125, 0.470703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4716796875, 1.0, 0.462890625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.702256977558136, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6979166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7078993320465088, 1.0, 0.7000868320465088, 1.0, 0.7018229365348816, 1.0, 0.706163227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 0.69921875, 1.0, 0.7000868320465088, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6987847089767456, 1.0, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 0.7000868320465088, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.69921875, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 0.6996527910232544, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6996527910232544, 0.7052951455116272, 0.7044270634651184, 1.0, 1.0, 0.7013888955116272, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 0.6987847089767456, 1.0, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 0.7000868320465088, 1.0, 1.0, 1.0, 0.7009548544883728, 1.0, 1.0, 0.7057291865348816, 0.7000868320465088, 1.0, 1.0, 0.703125, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 0.6996527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 0.7000868320465088, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6987847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.8046875, 1.0, 0.80078125, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.80078125, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.80078125, 0.80859375, 0.8046875, 0.80859375, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80078125, 0.8046875, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80078125, 0.80859375, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.80859375, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.80859375, 0.8046875, 0.80859375, 0.8046875, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.80078125, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81640625, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.80859375, 0.80078125, 1.0, 0.80078125, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.80078125, 0.80078125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 0.80859375, 1.0, 0.80859375, 1.0, 0.80078125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 0.80078125, 0.80859375, 1.0, 1.0, 1.0, 1.0, 0.80859375, 0.80078125, 0.80078125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80078125, 0.80859375, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.80859375, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.80859375, 0.80859375, 1.0, 1.0, 0.8046875, 0.8046875, 0.80859375, 1.0, 0.8046875, 0.8046875, 0.796875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.80078125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.80078125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.80859375, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80859375, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.80078125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.80078125, 0.8046875, 0.8046875, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 0.416015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.404296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 0.3984375, 1.0, 1.0, 1.0, 0.40625, 1.0, 1.0, 0.40625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.9322916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.932725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.932725727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9314236044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9322916865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9331597089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 0.96484375, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.392578125, 0.3896484375, 0.388671875, 0.39453125, 0.3994140625, 0.396484375, 0.3955078125, 0.390625, 1.0, 1.0, 0.3955078125, 0.39453125, 0.392578125, 0.3955078125, 0.40234375, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39453125, 0.40234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 0.3935546875, 0.4013671875, 1.0, 0.3935546875, 1.0, 0.396484375, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 0.392578125, 0.388671875, 0.39453125, 0.3955078125, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3935546875, 1.0, 0.3994140625, 0.3935546875, 0.4033203125, 0.3935546875, 1.0, 0.3935546875, 0.396484375, 1.0, 1.0, 0.3955078125, 1.0, 0.39453125, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 0.4033203125, 1.0, 0.3916015625, 1.0, 0.390625, 0.3974609375, 1.0, 0.390625, 1.0, 0.39453125, 1.0, 0.3828125, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 0.3984375, 1.0, 1.0, 1.0, 0.404296875, 0.392578125, 1.0, 1.0, 0.392578125, 1.0, 0.392578125, 0.396484375, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3916015625, 0.388671875, 1.0, 0.392578125, 1.0, 0.392578125, 1.0, 0.3896484375, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 0.390625, 0.3984375, 1.0, 0.3994140625, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.408203125, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 0.3935546875, 1.0, 0.3984375, 0.392578125, 0.3984375, 1.0, 1.0, 0.384765625, 0.3935546875, 0.3984375, 1.0, 1.0, 0.3974609375, 1.0, 0.396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 0.3955078125, 0.3935546875, 1.0, 0.396484375, 0.3974609375, 0.392578125, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 0.3974609375, 1.0, 1.0, 0.396484375, 0.396484375, 0.3916015625, 0.3935546875, 0.3916015625, 1.0, 1.0, 0.396484375, 1.0, 0.3984375, 1.0, 1.0, 1.0, 0.3935546875]

 sparsity of   [0.5941840410232544, 0.596788227558136, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5959201455116272, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 0.5946180820465088, 0.5941840410232544, 0.5954861044883728, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 0.5946180820465088, 0.5950520634651184, 0.596788227558136, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.59375, 1.0, 0.5959201455116272, 0.596788227558136, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5950520634651184, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 0.5950520634651184, 1.0, 0.5954861044883728, 1.0, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5950520634651184, 1.0, 0.5954861044883728, 0.5950520634651184, 1.0, 0.5959201455116272, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5963541865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5946180820465088, 1.0, 1.0, 1.0, 0.5963541865348816, 0.5950520634651184, 1.0, 1.0, 1.0, 0.596788227558136, 1.0, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 0.5959201455116272, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5950520634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5954861044883728, 1.0, 0.5963541865348816, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5963541865348816, 1.0, 0.5959201455116272, 1.0, 1.0, 0.5954861044883728, 1.0, 1.0, 1.0]

 sparsity of   [0.7734375, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.77734375, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 0.76953125, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.7734375, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 0.765625, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 0.765625, 0.7734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 0.7734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 0.765625, 0.765625, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 0.77734375, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.77734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 0.7734375, 0.765625, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.796875, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.7734375, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.7734375, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.78125, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.765625, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.7734375, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 0.76953125, 0.7734375, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.765625, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 0.765625, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.76953125, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 1.0, 0.76953125, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 0.765625, 0.765625, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 0.78125, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 0.765625, 1.0, 1.0, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 0.7734375, 1.0, 0.765625, 0.7734375, 1.0, 1.0, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 1.0, 1.0, 0.76953125, 0.76953125, 0.76953125, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 0.765625, 0.76953125, 0.76953125, 1.0, 1.0, 1.0, 0.76953125, 1.0, 0.76953125, 1.0, 1.0, 0.7734375, 0.765625, 1.0, 1.0, 0.7734375, 1.0, 0.77734375, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.38671875, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4033203125, 0.3876953125, 1.0, 0.3857421875, 1.0, 1.0, 0.400390625, 1.0, 0.3828125, 1.0, 0.3916015625, 1.0, 1.0, 0.39453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.390625, 0.3876953125, 0.3896484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 0.3974609375, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 0.3935546875, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3896484375, 1.0, 0.3857421875, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 0.39453125, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3876953125, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3896484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 0.380859375, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 0.3916015625, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 0.38671875, 1.0, 1.0, 0.3896484375, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 0.3955078125, 0.3828125, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 0.3935546875, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 0.7808159589767456, 0.78125, 0.7769097089767456, 0.7816840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 1.0, 1.0, 0.7795138955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7790798544883728, 1.0, 1.0, 1.0, 0.7790798544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7816840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7808159589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.780381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.77734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7799479365348816, 1.0, 1.0, 0.78125]

 sparsity of   [0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90625, 1.0, 0.8984375, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 0.89453125, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.90234375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.8984375, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.90234375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 0.90234375, 1.0, 0.8984375, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 0.8984375, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3779296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4462890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9951171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.48828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4931640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3798828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.880859375, 1.0, 0.3984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.83984375, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.701171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7080078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.408203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.474609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37890625, 1.0, 1.0, 1.0, 0.4140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4482421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4091796875, 1.0, 1.0, 1.0, 0.4443359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.384765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8740234375, 1.0, 1.0, 1.0, 0.416015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3916015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.392578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3857421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5791015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38671875, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4013671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3837890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3994140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.447265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5283203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5224609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3818359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41796875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96533203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9658203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8302951455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.822265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8185763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8344184160232544, 0.8129340410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.837022602558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8426649570465088, 0.8578559160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8569878339767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8602430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8452690839767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8413628339767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.962890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.623046875, 0.62109375, 0.60888671875, 0.6171875, 0.60888671875, 0.60595703125, 0.615234375, 0.61962890625, 0.6162109375, 0.62646484375, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875]

Total parameter pruned: 22644409.005100533 (unstructured) 21123309 (structured)

Test: [0/79]	Time 0.258 (0.258)	Loss 0.3532 (0.3532) ([0.222]+[0.131])	Prec@1 90.625 (90.625)
 * Prec@1 93.670
current lr 1.00000e-03
Grad=  tensor(0.4356, device='cuda:0')
Epoch: [300][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [300][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0191 (0.0076) ([0.019]+[0.000])	Prec@1 99.219 (99.884)
Epoch: [300][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0074) ([0.004]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [300][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0272 (0.0073) ([0.027]+[0.000])	Prec@1 99.219 (99.896)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2448 (0.2448) ([0.245]+[0.000])	Prec@1 90.625 (90.625)
 * Prec@1 93.680
current lr 1.00000e-03
Grad=  tensor(0.9737, device='cuda:0')
Epoch: [301][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0143 (0.0077) ([0.014]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [301][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0075) ([0.003]+[0.000])	Prec@1 100.000 (99.860)
Epoch: [301][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0054 (0.0072) ([0.005]+[0.000])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2473 (0.2473) ([0.247]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.660
current lr 1.00000e-03
Grad=  tensor(0.9832, device='cuda:0')
Epoch: [302][0/391]	Time 0.302 (0.302)	Data 0.231 (0.231)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0050 (0.0073) ([0.005]+[0.000])	Prec@1 100.000 (99.861)
Epoch: [302][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0072) ([0.002]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [302][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0073) ([0.003]+[0.000])	Prec@1 100.000 (99.870)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.2468 (0.2468) ([0.247]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(0.0439, device='cuda:0')
Epoch: [303][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0049 (0.0067) ([0.005]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [303][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0068) ([0.002]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [303][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0066) ([0.003]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2306 (0.2306) ([0.231]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.600
current lr 1.00000e-03
Grad=  tensor(0.0220, device='cuda:0')
Epoch: [304][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0045 (0.0073) ([0.004]+[0.000])	Prec@1 100.000 (99.892)
Epoch: [304][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0070) ([0.004]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [304][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0070) ([0.003]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2601 (0.2601) ([0.260]+[0.000])	Prec@1 90.625 (90.625)
 * Prec@1 93.610
current lr 1.00000e-03
Grad=  tensor(0.0953, device='cuda:0')
Epoch: [305][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0056 (0.0080) ([0.006]+[0.000])	Prec@1 100.000 (99.814)
Epoch: [305][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0085 (0.0070) ([0.009]+[0.000])	Prec@1 100.000 (99.868)
Epoch: [305][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0171 (0.0068) ([0.017]+[0.000])	Prec@1 99.219 (99.875)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.2308 (0.2308) ([0.231]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.1093, device='cuda:0')
Epoch: [306][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0136 (0.0067) ([0.014]+[0.000])	Prec@1 99.219 (99.869)
Epoch: [306][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0063) ([0.003]+[0.000])	Prec@1 100.000 (99.895)
Epoch: [306][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0126 (0.0064) ([0.013]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2142 (0.2142) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.1212, device='cuda:0')
Epoch: [307][0/391]	Time 0.295 (0.295)	Data 0.225 (0.225)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0056 (0.0065) ([0.006]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [307][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0065) ([0.005]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [307][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0080 (0.0067) ([0.008]+[0.000])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2291 (0.2291) ([0.229]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(0.3601, device='cuda:0')
Epoch: [308][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0066 (0.0059) ([0.007]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [308][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0061) ([0.002]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [308][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0068) ([0.005]+[0.000])	Prec@1 100.000 (99.886)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2031 (0.2031) ([0.203]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.2909, device='cuda:0')
Epoch: [309][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [309][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0062) ([0.005]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [309][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0206 (0.0061) ([0.021]+[0.000])	Prec@1 99.219 (99.914)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2037 (0.2037) ([0.204]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.670
current lr 1.00000e-03
Grad=  tensor(0.3507, device='cuda:0')
Epoch: [310][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0057) ([0.003]+[0.000])	Prec@1 100.000 (99.899)
Epoch: [310][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [310][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0059) ([0.005]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2249 (0.2249) ([0.225]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(0.5328, device='cuda:0')
Epoch: [311][0/391]	Time 0.289 (0.289)	Data 0.218 (0.218)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0045 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [311][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0060) ([0.004]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [311][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0071 (0.0060) ([0.007]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.2403 (0.2403) ([0.240]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.610
current lr 1.00000e-03
Grad=  tensor(6.7836, device='cuda:0')
Epoch: [312][0/391]	Time 0.263 (0.263)	Data 0.192 (0.192)	Loss 0.0175 (0.0175) ([0.018]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [312][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0115 (0.0063) ([0.012]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [312][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0060) ([0.003]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [312][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0060) ([0.003]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2088 (0.2088) ([0.209]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(0.0283, device='cuda:0')
Epoch: [313][0/391]	Time 0.253 (0.253)	Data 0.182 (0.182)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0060) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [313][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0084 (0.0057) ([0.008]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [313][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2009 (0.2009) ([0.201]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.580
current lr 1.00000e-03
Grad=  tensor(0.0365, device='cuda:0')
Epoch: [314][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0110 (0.0053) ([0.011]+[0.000])	Prec@1 99.219 (99.923)
Epoch: [314][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [314][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2601 (0.2601) ([0.260]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.540
current lr 1.00000e-03
Grad=  tensor(0.4049, device='cuda:0')
Epoch: [315][0/391]	Time 0.299 (0.299)	Data 0.226 (0.226)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0075 (0.0056) ([0.007]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [315][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [315][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0076 (0.0057) ([0.008]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2269 (0.2269) ([0.227]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.0772, device='cuda:0')
Epoch: [316][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0064 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [316][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0054 (0.0056) ([0.005]+[0.000])	Prec@1 100.000 (99.918)
Epoch: [316][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0146 (0.0058) ([0.015]+[0.000])	Prec@1 99.219 (99.914)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.2674 (0.2674) ([0.267]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.570
current lr 1.00000e-03
Grad=  tensor(0.6593, device='cuda:0')
Epoch: [317][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [317][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0057 (0.0054) ([0.006]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [317][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2426 (0.2426) ([0.243]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.0415, device='cuda:0')
Epoch: [318][0/391]	Time 0.290 (0.290)	Data 0.219 (0.219)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0054) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [318][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0059) ([0.004]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [318][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0058) ([0.005]+[0.000])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2761 (0.2761) ([0.276]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.600
current lr 1.00000e-03
Grad=  tensor(0.5730, device='cuda:0')
Epoch: [319][0/391]	Time 0.291 (0.291)	Data 0.220 (0.220)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [319][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0061 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [319][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0159 (0.0060) ([0.016]+[0.000])	Prec@1 99.219 (99.926)
Epoch: [319][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0185 (0.0060) ([0.019]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2659 (0.2659) ([0.266]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.570
current lr 1.00000e-03
Grad=  tensor(0.1461, device='cuda:0')
Epoch: [320][0/391]	Time 0.265 (0.265)	Data 0.194 (0.194)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [320][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [320][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0059) ([0.002]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2705 (0.2705) ([0.271]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(0.8018, device='cuda:0')
Epoch: [321][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0075 (0.0075) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0052 (0.0059) ([0.005]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [321][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0064 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [321][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0058) ([0.001]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2546 (0.2546) ([0.255]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.590
current lr 1.00000e-03
Grad=  tensor(1.6099, device='cuda:0')
Epoch: [322][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0115 (0.0115) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [322][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0055) ([0.001]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [322][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0135 (0.0056) ([0.013]+[0.000])	Prec@1 99.219 (99.914)
Epoch: [322][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2586 (0.2586) ([0.259]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.460
current lr 1.00000e-03
Grad=  tensor(13.1746, device='cuda:0')
Epoch: [323][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0350 (0.0350) ([0.035]+[0.000])	Prec@1 97.656 (97.656)
Epoch: [323][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0083 (0.0057) ([0.008]+[0.000])	Prec@1 100.000 (99.876)
Epoch: [323][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [323][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0148 (0.0052) ([0.015]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2485 (0.2485) ([0.249]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.560
current lr 1.00000e-03
Grad=  tensor(0.0424, device='cuda:0')
Epoch: [324][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0057) ([0.002]+[0.000])	Prec@1 100.000 (99.861)
Epoch: [324][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.911)
Epoch: [324][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0059 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2358 (0.2358) ([0.236]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.1554, device='cuda:0')
Epoch: [325][0/391]	Time 0.295 (0.295)	Data 0.224 (0.224)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [325][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [325][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2182 (0.2182) ([0.218]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.580
current lr 1.00000e-03
Grad=  tensor(0.3157, device='cuda:0')
Epoch: [326][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0057) ([0.002]+[0.000])	Prec@1 100.000 (99.884)
Epoch: [326][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0193 (0.0052) ([0.019]+[0.000])	Prec@1 99.219 (99.922)
Epoch: [326][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0059 (0.0051) ([0.006]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2298 (0.2298) ([0.230]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(3.0015, device='cuda:0')
Epoch: [327][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0080 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0049 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [327][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0073 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [327][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0058 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2357 (0.2357) ([0.236]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(1.0843, device='cuda:0')
Epoch: [328][0/391]	Time 0.268 (0.268)	Data 0.197 (0.197)	Loss 0.0084 (0.0084) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0095 (0.0045) ([0.010]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [328][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [328][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.1946 (0.1946) ([0.195]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.3809, device='cuda:0')
Epoch: [329][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [329][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [329][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0060 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2061 (0.2061) ([0.206]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(16.4210, device='cuda:0')
Epoch: [330][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0249 (0.0249) ([0.025]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [330][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [330][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0077 (0.0053) ([0.008]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [330][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2140 (0.2140) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.3587, device='cuda:0')
Epoch: [331][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0070 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [331][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [331][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0076 (0.0050) ([0.008]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.1858 (0.1858) ([0.186]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.0268, device='cuda:0')
Epoch: [332][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0052 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [332][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [332][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0056 (0.0049) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2224 (0.2224) ([0.222]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(0.0497, device='cuda:0')
Epoch: [333][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [333][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [333][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2345 (0.2345) ([0.235]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.740
current lr 1.00000e-03
Grad=  tensor(0.0298, device='cuda:0')
Epoch: [334][0/391]	Time 0.296 (0.296)	Data 0.223 (0.223)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0044 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [334][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0070 (0.0048) ([0.007]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [334][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2386 (0.2386) ([0.239]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(8.5136, device='cuda:0')
Epoch: [335][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0130 (0.0130) ([0.013]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [335][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [335][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0053) ([0.002]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [335][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.2027 (0.2027) ([0.203]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(0.5975, device='cuda:0')
Epoch: [336][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0095 (0.0095) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0068 (0.0043) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [336][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [336][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0084 (0.0048) ([0.008]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1889 (0.1889) ([0.189]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.1320, device='cuda:0')
Epoch: [337][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0030 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [337][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0053) ([0.007]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [337][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2077 (0.2077) ([0.208]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.0906, device='cuda:0')
Epoch: [338][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0067 (0.0053) ([0.007]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [338][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [338][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2461 (0.2461) ([0.246]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.0403, device='cuda:0')
Epoch: [339][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [339][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [339][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2413 (0.2413) ([0.241]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(1.5592, device='cuda:0')
Epoch: [340][0/391]	Time 0.265 (0.265)	Data 0.194 (0.194)	Loss 0.0073 (0.0073) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0056 (0.0050) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [340][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0144 (0.0048) ([0.014]+[0.000])	Prec@1 99.219 (99.965)
Epoch: [340][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0048) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2558 (0.2558) ([0.256]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.0637, device='cuda:0')
Epoch: [341][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0168 (0.0046) ([0.017]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [341][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0153 (0.0051) ([0.015]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [341][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0050 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2302 (0.2302) ([0.230]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(0.0587, device='cuda:0')
Epoch: [342][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [342][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0235 (0.0051) ([0.023]+[0.000])	Prec@1 99.219 (99.914)
Epoch: [342][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0049) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2737 (0.2737) ([0.274]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(1.2092, device='cuda:0')
Epoch: [343][0/391]	Time 0.264 (0.264)	Data 0.193 (0.193)	Loss 0.0080 (0.0080) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0062) ([0.002]+[0.000])	Prec@1 100.000 (99.892)
Epoch: [343][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [343][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0052) ([0.002]+[0.000])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2569 (0.2569) ([0.257]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.620
current lr 1.00000e-03
Grad=  tensor(0.3138, device='cuda:0')
Epoch: [344][0/391]	Time 0.258 (0.258)	Data 0.187 (0.187)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [344][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [344][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2247 (0.2247) ([0.225]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(0.3994, device='cuda:0')
Epoch: [345][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0065 (0.0065) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [345][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [345][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0046) ([0.004]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2558 (0.2558) ([0.256]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.530
current lr 1.00000e-03
Grad=  tensor(0.0298, device='cuda:0')
Epoch: [346][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0044) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [346][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [346][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0071 (0.0045) ([0.007]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2230 (0.2230) ([0.223]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(0.0309, device='cuda:0')
Epoch: [347][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [347][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [347][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2290 (0.2290) ([0.229]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.580
current lr 1.00000e-03
Grad=  tensor(0.9441, device='cuda:0')
Epoch: [348][0/391]	Time 0.260 (0.260)	Data 0.189 (0.189)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0047) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [348][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [348][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0045) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2249 (0.2249) ([0.225]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(0.0297, device='cuda:0')
Epoch: [349][0/391]	Time 0.296 (0.296)	Data 0.226 (0.226)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0107 (0.0047) ([0.011]+[0.000])	Prec@1 99.219 (99.954)
Epoch: [349][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [349][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0043) ([0.001]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2121 (0.2121) ([0.212]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.650
current lr 1.00000e-04
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [350][0/391]	Time 0.302 (0.302)	Data 0.232 (0.232)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [350][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0073 (0.0045) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [350][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.2251 (0.2251) ([0.225]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.620
current lr 1.00000e-04
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [351][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [351][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0068 (0.0046) ([0.007]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [351][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0044) ([0.001]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2233 (0.2233) ([0.223]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.1795, device='cuda:0')
Epoch: [352][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [352][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [352][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2171 (0.2171) ([0.217]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.690
current lr 1.00000e-04
Grad=  tensor(0.2637, device='cuda:0')
Epoch: [353][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [353][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0129 (0.0042) ([0.013]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [353][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.2264 (0.2264) ([0.226]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.710
current lr 1.00000e-04
Grad=  tensor(0.3175, device='cuda:0')
Epoch: [354][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0048) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [354][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [354][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.2271 (0.2271) ([0.227]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.720
current lr 1.00000e-04
Grad=  tensor(0.1134, device='cuda:0')
Epoch: [355][0/391]	Time 0.267 (0.267)	Data 0.196 (0.196)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [355][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [355][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2218 (0.2218) ([0.222]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.840
current lr 1.00000e-04
Grad=  tensor(2.1277, device='cuda:0')
Epoch: [356][0/391]	Time 0.259 (0.259)	Data 0.188 (0.188)	Loss 0.0151 (0.0151) ([0.015]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [356][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [356][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [356][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2208 (0.2208) ([0.221]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.750
current lr 1.00000e-04
Grad=  tensor(0.2845, device='cuda:0')
Epoch: [357][0/391]	Time 0.252 (0.252)	Data 0.181 (0.181)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0032 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [357][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [357][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2310 (0.2310) ([0.231]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.1326, device='cuda:0')
Epoch: [358][0/391]	Time 0.250 (0.250)	Data 0.179 (0.179)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [358][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [358][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2394 (0.2394) ([0.239]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-04
Grad=  tensor(0.0057, device='cuda:0')
Epoch: [359][0/391]	Time 0.303 (0.303)	Data 0.232 (0.232)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0046) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [359][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [359][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2278 (0.2278) ([0.228]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.3125, device='cuda:0')
Epoch: [360][0/391]	Time 0.299 (0.299)	Data 0.229 (0.229)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [360][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [360][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0121 (0.0039) ([0.012]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.2221 (0.2221) ([0.222]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.0477, device='cuda:0')
Epoch: [361][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [361][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0056 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [361][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2157 (0.2157) ([0.216]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.2137, device='cuda:0')
Epoch: [362][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [362][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [362][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2413 (0.2413) ([0.241]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-04
Grad=  tensor(0.0234, device='cuda:0')
Epoch: [363][0/391]	Time 0.290 (0.290)	Data 0.218 (0.218)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [363][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [363][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0145 (0.0043) ([0.015]+[0.000])	Prec@1 99.219 (99.956)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.2373 (0.2373) ([0.237]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.600
current lr 1.00000e-04
Grad=  tensor(0.8629, device='cuda:0')
Epoch: [364][0/391]	Time 0.293 (0.293)	Data 0.223 (0.223)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0045) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [364][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0057 (0.0042) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [364][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2369 (0.2369) ([0.237]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-04
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [365][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [365][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0065 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [365][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2365 (0.2365) ([0.236]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-04
Grad=  tensor(0.0168, device='cuda:0')
Epoch: [366][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0181 (0.0040) ([0.018]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [366][200/391]	Time 0.065 (0.064)	Data 0.001 (0.001)	Loss 0.0015 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [366][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2262 (0.2262) ([0.226]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-04
Grad=  tensor(0.0225, device='cuda:0')
Epoch: [367][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0057 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [367][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [367][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2138 (0.2138) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-04
Grad=  tensor(0.1547, device='cuda:0')
Epoch: [368][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [368][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [368][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.2068 (0.2068) ([0.207]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.4163, device='cuda:0')
Epoch: [369][0/391]	Time 0.267 (0.267)	Data 0.196 (0.196)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [369][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [369][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2277 (0.2277) ([0.228]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.810
current lr 1.00000e-04
Grad=  tensor(0.5620, device='cuda:0')
Epoch: [370][0/391]	Time 0.257 (0.257)	Data 0.186 (0.186)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [370][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [370][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2258 (0.2258) ([0.226]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.760
current lr 1.00000e-04
Grad=  tensor(0.0419, device='cuda:0')
Epoch: [371][0/391]	Time 0.250 (0.250)	Data 0.179 (0.179)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [371][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [371][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.2214 (0.2214) ([0.221]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.3324, device='cuda:0')
Epoch: [372][0/391]	Time 0.253 (0.253)	Data 0.182 (0.182)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [372][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [372][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0058 (0.0036) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2109 (0.2109) ([0.211]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.2086, device='cuda:0')
Epoch: [373][0/391]	Time 0.300 (0.300)	Data 0.226 (0.226)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0010 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [373][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0092 (0.0037) ([0.009]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [373][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0061 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2125 (0.2125) ([0.213]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(0.0207, device='cuda:0')
Epoch: [374][0/391]	Time 0.304 (0.304)	Data 0.233 (0.233)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [374][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [374][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0115 (0.0038) ([0.012]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2249 (0.2249) ([0.225]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.710
current lr 1.00000e-04
Grad=  tensor(0.0260, device='cuda:0')
Epoch: [375][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [375][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [375][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0091 (0.0041) ([0.009]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2348 (0.2348) ([0.235]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-04
Grad=  tensor(0.2912, device='cuda:0')
Epoch: [376][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0039 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [376][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [376][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.2436 (0.2436) ([0.244]+[0.000])	Prec@1 90.625 (90.625)
 * Prec@1 93.790
current lr 1.00000e-04
Grad=  tensor(0.0351, device='cuda:0')
Epoch: [377][0/391]	Time 0.300 (0.300)	Data 0.230 (0.230)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0045 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [377][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [377][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2103 (0.2103) ([0.210]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(0.0327, device='cuda:0')
Epoch: [378][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [378][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0070 (0.0034) ([0.007]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [378][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2140 (0.2140) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-04
Grad=  tensor(0.0087, device='cuda:0')
Epoch: [379][0/391]	Time 0.306 (0.306)	Data 0.234 (0.234)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0040 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [379][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [379][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2278 (0.2278) ([0.228]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.840
current lr 1.00000e-04
Grad=  tensor(0.0600, device='cuda:0')
Epoch: [380][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0075 (0.0035) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [380][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [380][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.2229 (0.2229) ([0.223]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(1.2316, device='cuda:0')
Epoch: [381][0/391]	Time 0.296 (0.296)	Data 0.223 (0.223)	Loss 0.0085 (0.0085) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [381][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [381][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2166 (0.2166) ([0.217]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.740
current lr 1.00000e-04
Grad=  tensor(0.0215, device='cuda:0')
Epoch: [382][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0044) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [382][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [382][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2280 (0.2280) ([0.228]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-04
Grad=  tensor(0.0297, device='cuda:0')
Epoch: [383][0/391]	Time 0.261 (0.261)	Data 0.190 (0.190)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0049 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [383][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0078 (0.0046) ([0.008]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [383][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2246 (0.2246) ([0.225]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.720
current lr 1.00000e-04
Grad=  tensor(0.0318, device='cuda:0')
Epoch: [384][0/391]	Time 0.304 (0.304)	Data 0.233 (0.233)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0098 (0.0034) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [384][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [384][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2301 (0.2301) ([0.230]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-04
Grad=  tensor(0.0880, device='cuda:0')
Epoch: [385][0/391]	Time 0.297 (0.297)	Data 0.227 (0.227)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0094 (0.0039) ([0.009]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [385][200/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [385][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0050 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2238 (0.2238) ([0.224]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.3301, device='cuda:0')
Epoch: [386][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [386][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [386][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2109 (0.2109) ([0.211]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.850
current lr 1.00000e-04
Grad=  tensor(0.1241, device='cuda:0')
Epoch: [387][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [387][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0043 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [387][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [387][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2164 (0.2164) ([0.216]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.0804, device='cuda:0')
Epoch: [388][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [388][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [388][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0086 (0.0037) ([0.009]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2413 (0.2413) ([0.241]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.770
current lr 1.00000e-04
Grad=  tensor(0.0058, device='cuda:0')
Epoch: [389][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [389][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [389][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2249 (0.2249) ([0.225]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.690
current lr 1.00000e-04
Grad=  tensor(0.3906, device='cuda:0')
Epoch: [390][0/391]	Time 0.295 (0.295)	Data 0.224 (0.224)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [390][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [390][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2374 (0.2374) ([0.237]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.810
current lr 1.00000e-04
Grad=  tensor(0.3205, device='cuda:0')
Epoch: [391][0/391]	Time 0.348 (0.348)	Data 0.275 (0.275)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.063 (0.066)	Data 0.000 (0.003)	Loss 0.0026 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [391][200/391]	Time 0.063 (0.065)	Data 0.000 (0.001)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [391][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2165 (0.2165) ([0.216]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.760
current lr 1.00000e-04
Grad=  tensor(0.0380, device='cuda:0')
Epoch: [392][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0048 (0.0034) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [392][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2112 (0.2112) ([0.211]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.710
current lr 1.00000e-04
Grad=  tensor(0.0441, device='cuda:0')
Epoch: [393][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0044 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [393][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [393][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2103 (0.2103) ([0.210]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.830
current lr 1.00000e-04
Grad=  tensor(0.0518, device='cuda:0')
Epoch: [394][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [394][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2090 (0.2090) ([0.209]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.700
current lr 1.00000e-04
Grad=  tensor(0.4041, device='cuda:0')
Epoch: [395][0/391]	Time 0.302 (0.302)	Data 0.231 (0.231)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0072 (0.0038) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [395][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [395][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2336 (0.2336) ([0.234]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.2264, device='cuda:0')
Epoch: [396][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [396][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [396][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2165 (0.2165) ([0.217]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(1.8064, device='cuda:0')
Epoch: [397][0/391]	Time 0.288 (0.288)	Data 0.216 (0.216)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0175 (0.0040) ([0.018]+[0.000])	Prec@1 99.219 (99.954)
Epoch: [397][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [397][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.2371 (0.2371) ([0.237]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.790
current lr 1.00000e-04
Grad=  tensor(0.8584, device='cuda:0')
Epoch: [398][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0059 (0.0034) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [398][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [398][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2272 (0.2272) ([0.227]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-04
Grad=  tensor(0.0281, device='cuda:0')
Epoch: [399][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0050 (0.0031) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [399][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [399][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2294 (0.2294) ([0.229]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.690
current lr 1.00000e-05
Grad=  tensor(1.3540, device='cuda:0')
Epoch: [400][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [400][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [400][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2084 (0.2084) ([0.208]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(0.0365, device='cuda:0')
Epoch: [401][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [401][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [401][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.2226 (0.2226) ([0.223]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(0.1857, device='cuda:0')
Epoch: [402][0/391]	Time 0.290 (0.290)	Data 0.220 (0.220)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0094 (0.0034) ([0.009]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [402][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [402][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.2226 (0.2226) ([0.223]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-05
Grad=  tensor(0.0308, device='cuda:0')
Epoch: [403][0/391]	Time 0.262 (0.262)	Data 0.191 (0.191)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0060 (0.0037) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [403][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [403][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2313 (0.2313) ([0.231]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.0238, device='cuda:0')
Epoch: [404][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [404][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [404][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0079 (0.0036) ([0.008]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.2293 (0.2293) ([0.229]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-05
Grad=  tensor(0.3989, device='cuda:0')
Epoch: [405][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0065 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [405][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [405][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.2201 (0.2201) ([0.220]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.780
current lr 1.00000e-05
Grad=  tensor(0.0211, device='cuda:0')
Epoch: [406][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [406][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [406][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.2261 (0.2261) ([0.226]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.700
current lr 1.00000e-05
Grad=  tensor(0.4023, device='cuda:0')
Epoch: [407][0/391]	Time 0.255 (0.255)	Data 0.185 (0.185)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [407][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [407][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2370 (0.2370) ([0.237]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.620
current lr 1.00000e-05
Grad=  tensor(0.0211, device='cuda:0')
Epoch: [408][0/391]	Time 0.255 (0.255)	Data 0.185 (0.185)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0063 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [408][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [408][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.2257 (0.2257) ([0.226]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.870
current lr 1.00000e-05
Grad=  tensor(0.6015, device='cuda:0')
Epoch: [409][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0205 (0.0035) ([0.020]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [409][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [409][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2223 (0.2223) ([0.222]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.720
current lr 1.00000e-05
Grad=  tensor(0.0420, device='cuda:0')
Epoch: [410][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [410][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [410][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2328 (0.2328) ([0.233]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-05
Grad=  tensor(0.0882, device='cuda:0')
Epoch: [411][0/391]	Time 0.299 (0.299)	Data 0.226 (0.226)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [411][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0059 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [411][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.2238 (0.2238) ([0.224]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.610
current lr 1.00000e-05
Grad=  tensor(0.0720, device='cuda:0')
Epoch: [412][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [412][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [412][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2241 (0.2241) ([0.224]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.810
current lr 1.00000e-05
Grad=  tensor(0.9321, device='cuda:0')
Epoch: [413][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [413][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [413][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2505 (0.2505) ([0.251]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.1910, device='cuda:0')
Epoch: [414][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [414][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [414][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2369 (0.2369) ([0.237]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(2.3301, device='cuda:0')
Epoch: [415][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [415][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [415][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2188 (0.2188) ([0.219]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-05
Grad=  tensor(0.0838, device='cuda:0')
Epoch: [416][0/391]	Time 0.294 (0.294)	Data 0.221 (0.221)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [416][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [416][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2161 (0.2161) ([0.216]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.790
current lr 1.00000e-05
Grad=  tensor(0.1784, device='cuda:0')
Epoch: [417][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0045 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [417][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [417][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2320 (0.2320) ([0.232]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.680
current lr 1.00000e-05
Grad=  tensor(0.0906, device='cuda:0')
Epoch: [418][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [418][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [418][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2214 (0.2214) ([0.221]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.0127, device='cuda:0')
Epoch: [419][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [419][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [419][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2203 (0.2203) ([0.220]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(1.4229, device='cuda:0')
Epoch: [420][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0091 (0.0091) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [420][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [420][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2083 (0.2083) ([0.208]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(0.0246, device='cuda:0')
Epoch: [421][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [421][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [421][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2133 (0.2133) ([0.213]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.830
current lr 1.00000e-05
Grad=  tensor(0.4423, device='cuda:0')
Epoch: [422][0/391]	Time 0.267 (0.267)	Data 0.196 (0.196)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0071 (0.0042) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [422][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [422][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0137 (0.0040) ([0.014]+[0.000])	Prec@1 99.219 (99.964)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2259 (0.2259) ([0.226]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.690
current lr 1.00000e-05
Grad=  tensor(0.0244, device='cuda:0')
Epoch: [423][0/391]	Time 0.295 (0.295)	Data 0.222 (0.222)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [423][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [423][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.2152 (0.2152) ([0.215]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.650
current lr 1.00000e-05
Grad=  tensor(1.3646, device='cuda:0')
Epoch: [424][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0101 (0.0101) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0046 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [424][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [424][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2259 (0.2259) ([0.226]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.630
current lr 1.00000e-05
Grad=  tensor(0.7105, device='cuda:0')
Epoch: [425][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [425][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0093 (0.0038) ([0.009]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [425][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2237 (0.2237) ([0.224]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.700
current lr 1.00000e-05
Grad=  tensor(0.3095, device='cuda:0')
Epoch: [426][0/391]	Time 0.285 (0.285)	Data 0.215 (0.215)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [426][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [426][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0058 (0.0034) ([0.006]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.2285 (0.2285) ([0.229]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.690
current lr 1.00000e-05
Grad=  tensor(0.0782, device='cuda:0')
Epoch: [427][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [427][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [427][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2179 (0.2179) ([0.218]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-05
Grad=  tensor(0.0151, device='cuda:0')
Epoch: [428][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [428][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [428][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.1955 (0.1955) ([0.195]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.0976, device='cuda:0')
Epoch: [429][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0023 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [429][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [429][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2192 (0.2192) ([0.219]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.810
current lr 1.00000e-05
Grad=  tensor(0.1928, device='cuda:0')
Epoch: [430][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [430][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [430][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0067 (0.0036) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.2254 (0.2254) ([0.225]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.1316, device='cuda:0')
Epoch: [431][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0073 (0.0030) ([0.007]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [431][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [431][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.2068 (0.2068) ([0.207]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.780
current lr 1.00000e-05
Grad=  tensor(0.4226, device='cuda:0')
Epoch: [432][0/391]	Time 0.263 (0.263)	Data 0.192 (0.192)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [432][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [432][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2277 (0.2277) ([0.228]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.780
current lr 1.00000e-05
Grad=  tensor(0.0847, device='cuda:0')
Epoch: [433][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0055 (0.0037) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [433][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [433][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0091 (0.0038) ([0.009]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.2410 (0.2410) ([0.241]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.600
current lr 1.00000e-05
Grad=  tensor(0.0700, device='cuda:0')
Epoch: [434][0/391]	Time 0.270 (0.270)	Data 0.199 (0.199)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0034 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [434][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [434][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.2149 (0.2149) ([0.215]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.790
current lr 1.00000e-05
Grad=  tensor(0.0732, device='cuda:0')
Epoch: [435][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [435][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [435][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.2350 (0.2350) ([0.235]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(1.2639, device='cuda:0')
Epoch: [436][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [436][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [436][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2377 (0.2377) ([0.238]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-05
Grad=  tensor(0.2545, device='cuda:0')
Epoch: [437][0/391]	Time 0.305 (0.305)	Data 0.232 (0.232)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [437][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2246 (0.2246) ([0.225]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.770
current lr 1.00000e-05
Grad=  tensor(0.0503, device='cuda:0')
Epoch: [438][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0064 (0.0031) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [438][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [438][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2347 (0.2347) ([0.235]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.4658, device='cuda:0')
Epoch: [439][0/391]	Time 0.295 (0.295)	Data 0.224 (0.224)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0039 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [439][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [439][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0044 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.2380 (0.2380) ([0.238]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.760
current lr 1.00000e-05
Grad=  tensor(0.1384, device='cuda:0')
Epoch: [440][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [440][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [440][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2535 (0.2535) ([0.254]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.700
current lr 1.00000e-05
Grad=  tensor(0.0560, device='cuda:0')
Epoch: [441][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0043 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [441][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [441][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2168 (0.2168) ([0.217]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.760
current lr 1.00000e-05
Grad=  tensor(0.1557, device='cuda:0')
Epoch: [442][0/391]	Time 0.304 (0.304)	Data 0.232 (0.232)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [442][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [442][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2123 (0.2123) ([0.212]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-05
Grad=  tensor(0.3561, device='cuda:0')
Epoch: [443][0/391]	Time 0.267 (0.267)	Data 0.195 (0.195)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [443][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0044 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [443][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0067 (0.0036) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2280 (0.2280) ([0.228]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-05
Grad=  tensor(0.0042, device='cuda:0')
Epoch: [444][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [444][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [444][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2376 (0.2376) ([0.238]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.770
current lr 1.00000e-05
Grad=  tensor(0.0369, device='cuda:0')
Epoch: [445][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0049 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [445][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [445][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2147 (0.2147) ([0.215]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.780
current lr 1.00000e-05
Grad=  tensor(1.4138, device='cuda:0')
Epoch: [446][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0084 (0.0084) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [446][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [446][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0139 (0.0035) ([0.014]+[0.000])	Prec@1 99.219 (99.979)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2097 (0.2097) ([0.210]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.750
current lr 1.00000e-05
Grad=  tensor(0.1103, device='cuda:0')
Epoch: [447][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [447][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [447][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2217 (0.2217) ([0.222]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.800
current lr 1.00000e-05
Grad=  tensor(0.0820, device='cuda:0')
Epoch: [448][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0032 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [448][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [448][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2392 (0.2392) ([0.239]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(0.5095, device='cuda:0')
Epoch: [449][0/391]	Time 0.302 (0.302)	Data 0.231 (0.231)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0043 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [449][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [449][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2261 (0.2261) ([0.226]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(0.4104, device='cuda:0')
Epoch: [450][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [450][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [450][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2149 (0.2149) ([0.215]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.880
current lr 1.00000e-06
Grad=  tensor(0.1439, device='cuda:0')
Epoch: [451][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0070 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [451][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [451][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2206 (0.2206) ([0.221]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.790
current lr 1.00000e-06
Grad=  tensor(0.0759, device='cuda:0')
Epoch: [452][0/391]	Time 0.259 (0.259)	Data 0.188 (0.188)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [452][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [452][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0034) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2391 (0.2391) ([0.239]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.800
current lr 1.00000e-06
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [453][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0090 (0.0039) ([0.009]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [453][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [453][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2063 (0.2063) ([0.206]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.740
current lr 1.00000e-06
Grad=  tensor(0.3014, device='cuda:0')
Epoch: [454][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [454][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [454][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.2297 (0.2297) ([0.230]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(2.0902, device='cuda:0')
Epoch: [455][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0114 (0.0114) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0050 (0.0031) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [455][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [455][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2247 (0.2247) ([0.225]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.770
current lr 1.00000e-06
Grad=  tensor(0.2256, device='cuda:0')
Epoch: [456][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [456][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [456][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.2314 (0.2314) ([0.231]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(0.0430, device='cuda:0')
Epoch: [457][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0065 (0.0035) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [457][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [457][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2216 (0.2216) ([0.222]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(0.0529, device='cuda:0')
Epoch: [458][0/391]	Time 0.291 (0.291)	Data 0.220 (0.220)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0046 (0.0032) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [458][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [458][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0069 (0.0036) ([0.007]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2307 (0.2307) ([0.231]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.660
current lr 1.00000e-06
Grad=  tensor(0.0567, device='cuda:0')
Epoch: [459][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0054 (0.0034) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [459][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0077 (0.0037) ([0.008]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [459][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0099 (0.0036) ([0.010]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2386 (0.2386) ([0.239]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.770
current lr 1.00000e-06
Grad=  tensor(1.3630, device='cuda:0')
Epoch: [460][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [460][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [460][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2223 (0.2223) ([0.222]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.830
current lr 1.00000e-06
Grad=  tensor(0.4639, device='cuda:0')
Epoch: [461][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0060 (0.0035) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [461][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [461][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2247 (0.2247) ([0.225]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.2599, device='cuda:0')
Epoch: [462][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0071 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [462][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [462][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2351 (0.2351) ([0.235]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-06
Grad=  tensor(0.0839, device='cuda:0')
Epoch: [463][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0210 (0.0037) ([0.021]+[0.000])	Prec@1 98.438 (99.961)
Epoch: [463][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [463][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.2319 (0.2319) ([0.232]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-06
Grad=  tensor(0.7434, device='cuda:0')
Epoch: [464][0/391]	Time 0.292 (0.292)	Data 0.220 (0.220)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [464][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0034) ([0.007]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2368 (0.2368) ([0.237]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-06
Grad=  tensor(2.4386, device='cuda:0')
Epoch: [465][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0104 (0.0104) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0031 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [465][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0034) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [465][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0089 (0.0035) ([0.009]+[0.000])	Prec@1 99.219 (99.971)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2415 (0.2415) ([0.242]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.680
current lr 1.00000e-06
Grad=  tensor(0.0931, device='cuda:0')
Epoch: [466][0/391]	Time 0.264 (0.264)	Data 0.194 (0.194)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0094 (0.0040) ([0.009]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [466][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [466][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.2263 (0.2263) ([0.226]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.610
current lr 1.00000e-06
Grad=  tensor(0.0326, device='cuda:0')
Epoch: [467][0/391]	Time 0.253 (0.253)	Data 0.182 (0.182)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [467][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0335 (0.0043) ([0.034]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [467][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2383 (0.2383) ([0.238]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.790
current lr 1.00000e-06
Grad=  tensor(0.6117, device='cuda:0')
Epoch: [468][0/391]	Time 0.248 (0.248)	Data 0.178 (0.178)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [468][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [468][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2289 (0.2289) ([0.229]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.750
current lr 1.00000e-06
Grad=  tensor(0.0983, device='cuda:0')
Epoch: [469][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [469][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [469][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0076 (0.0037) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.2198 (0.2198) ([0.220]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(0.0102, device='cuda:0')
Epoch: [470][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [470][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [470][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.2144 (0.2144) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(0.2245, device='cuda:0')
Epoch: [471][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [471][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [471][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2223 (0.2223) ([0.222]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.750
current lr 1.00000e-06
Grad=  tensor(0.0018, device='cuda:0')
Epoch: [472][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [472][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0104 (0.0034) ([0.010]+[0.000])	Prec@1 99.219 (99.981)
Epoch: [472][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2067 (0.2067) ([0.207]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.590
current lr 1.00000e-06
Grad=  tensor(1.0143, device='cuda:0')
Epoch: [473][0/391]	Time 0.260 (0.260)	Data 0.189 (0.189)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [473][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [473][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2215 (0.2215) ([0.221]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.830
current lr 1.00000e-06
Grad=  tensor(0.0453, device='cuda:0')
Epoch: [474][0/391]	Time 0.295 (0.295)	Data 0.224 (0.224)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [474][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [474][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2123 (0.2123) ([0.212]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.810
current lr 1.00000e-06
Grad=  tensor(0.2616, device='cuda:0')
Epoch: [475][0/391]	Time 0.455 (0.455)	Data 0.384 (0.384)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.063 (0.067)	Data 0.000 (0.004)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [475][200/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0032 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [475][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.276 (0.276)	Loss 0.2136 (0.2136) ([0.214]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.0639, device='cuda:0')
Epoch: [476][0/391]	Time 0.304 (0.304)	Data 0.232 (0.232)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0043 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [476][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [476][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.2214 (0.2214) ([0.221]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.800
current lr 1.00000e-06
Grad=  tensor(0.0643, device='cuda:0')
Epoch: [477][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [477][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0057 (0.0035) ([0.006]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [477][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2325 (0.2325) ([0.233]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(0.0239, device='cuda:0')
Epoch: [478][0/391]	Time 0.271 (0.271)	Data 0.200 (0.200)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [478][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [478][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2150 (0.2150) ([0.215]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.840
current lr 1.00000e-06
Grad=  tensor(0.1308, device='cuda:0')
Epoch: [479][0/391]	Time 0.292 (0.292)	Data 0.220 (0.220)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [479][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [479][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.2329 (0.2329) ([0.233]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.740
current lr 1.00000e-06
Grad=  tensor(0.0369, device='cuda:0')
Epoch: [480][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0096 (0.0035) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [480][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [480][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2382 (0.2382) ([0.238]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.810
current lr 1.00000e-06
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [481][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0244 (0.0037) ([0.024]+[0.000])	Prec@1 99.219 (99.969)
Epoch: [481][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [481][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2203 (0.2203) ([0.220]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.880
current lr 1.00000e-06
Grad=  tensor(0.7193, device='cuda:0')
Epoch: [482][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.062 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [482][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [482][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2289 (0.2289) ([0.229]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.810
current lr 1.00000e-06
Grad=  tensor(0.0288, device='cuda:0')
Epoch: [483][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [483][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [483][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2428 (0.2428) ([0.243]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.760
current lr 1.00000e-06
Grad=  tensor(0.0186, device='cuda:0')
Epoch: [484][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [484][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [484][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2273 (0.2273) ([0.227]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.720
current lr 1.00000e-06
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [485][0/391]	Time 0.307 (0.307)	Data 0.235 (0.235)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0023 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [485][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [485][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2181 (0.2181) ([0.218]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(10.4590, device='cuda:0')
Epoch: [486][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0125 (0.0125) ([0.012]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [486][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [486][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [486][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.2340 (0.2340) ([0.234]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.730
current lr 1.00000e-06
Grad=  tensor(0.0141, device='cuda:0')
Epoch: [487][0/391]	Time 0.302 (0.302)	Data 0.231 (0.231)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [487][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [487][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2159 (0.2159) ([0.216]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.830
current lr 1.00000e-06
Grad=  tensor(0.0750, device='cuda:0')
Epoch: [488][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0051 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [488][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [488][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.2142 (0.2142) ([0.214]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.0296, device='cuda:0')
Epoch: [489][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [489][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [489][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2371 (0.2371) ([0.237]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.0548, device='cuda:0')
Epoch: [490][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [490][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [490][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.2418 (0.2418) ([0.242]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.640
current lr 1.00000e-06
Grad=  tensor(0.2313, device='cuda:0')
Epoch: [491][0/391]	Time 0.267 (0.267)	Data 0.196 (0.196)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [491][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [491][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2246 (0.2246) ([0.225]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.760
current lr 1.00000e-06
Grad=  tensor(0.0994, device='cuda:0')
Epoch: [492][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [492][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [492][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0054 (0.0034) ([0.005]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2292 (0.2292) ([0.229]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.0188, device='cuda:0')
Epoch: [493][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.065 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [493][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [493][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2129 (0.2129) ([0.213]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.760
current lr 1.00000e-06
Grad=  tensor(0.0616, device='cuda:0')
Epoch: [494][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [494][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [494][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.2272 (0.2272) ([0.227]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.740
current lr 1.00000e-06
Grad=  tensor(0.0011, device='cuda:0')
Epoch: [495][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [495][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [495][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.2406 (0.2406) ([0.241]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.640
current lr 1.00000e-06
Grad=  tensor(0.2642, device='cuda:0')
Epoch: [496][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0060 (0.0042) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [496][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [496][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.2344 (0.2344) ([0.234]+[0.000])	Prec@1 91.406 (91.406)
 * Prec@1 93.820
current lr 1.00000e-06
Grad=  tensor(0.0384, device='cuda:0')
Epoch: [497][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0058 (0.0031) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [497][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [497][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2138 (0.2138) ([0.214]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.710
current lr 1.00000e-06
Grad=  tensor(0.0178, device='cuda:0')
Epoch: [498][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [498][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [498][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2242 (0.2242) ([0.224]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.800
current lr 1.00000e-06
Grad=  tensor(0.0093, device='cuda:0')
Epoch: [499][0/391]	Time 0.270 (0.270)	Data 0.199 (0.199)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [499][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0116 (0.0036) ([0.012]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [499][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2210 (0.2210) ([0.221]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.800

 Elapsed time for training  5:31:03.353950

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5546875, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.86328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.015625, 0.79296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87890625, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.84765625, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.01171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 0.87890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0234375, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7265625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.73046875, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.734375, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.734375, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7265625, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4921875, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.484375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.662109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.017578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.623046875, 0.62109375, 0.60888671875, 0.6171875, 0.60888671875, 0.60595703125, 0.615234375, 0.61962890625, 0.6162109375, 0.62646484375, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875, 0.8857421875]
Total parameter pruned: 21307624.0 (unstructured) 21123309 (structured)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2210 (0.2210) ([0.221]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.800
Best accuracy:  93.92
