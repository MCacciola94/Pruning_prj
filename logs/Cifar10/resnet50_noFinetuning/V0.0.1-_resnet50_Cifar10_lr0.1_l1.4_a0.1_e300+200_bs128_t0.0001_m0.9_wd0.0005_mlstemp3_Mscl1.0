V0.0.1-_resnet50_Cifar10_lr0.1_l1.4_a0.1_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5254763960838318, Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.24196940660476685, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.15759095549583435, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.13501641154289246, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.3461485505104065, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.17473100125789642, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21617008745670319, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.14946585893630981, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09060623496770859, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08498729020357132, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11497705429792404, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11804789304733276, Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08379501849412918, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1424030363559723, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.19753389060497284, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.16684924066066742, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.22829987108707428, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12801074981689453, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09603530913591385, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06901206821203232, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12272872775793076, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0835055485367775, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.07954221963882446, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12703275680541992, Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.19747452437877655, Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.15407174825668335, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1602816879749298, Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.10645194351673126, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10600411146879196, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.13483507931232452, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.21709460020065308, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11353497207164764, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0660422295331955, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10686782747507095, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06808818876743317, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.05323619768023491, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.08759226649999619, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07965513318777084, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06643471866846085, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12164679169654846, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09185265004634857, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06330689787864685, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1110600158572197, Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12582343816757202, Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.09035182744264603, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07410024106502533, Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.07018566876649857, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0687103122472763, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.04065759852528572, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.03755198046565056, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.04725675657391548, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.045549724251031876, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06192586570978165, Linear(in_features=2048, out_features=100, bias=True): 0.44340774416923523}
current lr 1.00000e-01
Grad=  tensor(5543.8911, device='cuda:0')
Epoch: [0][0/391]	Time 0.356 (0.356)	Data 0.216 (0.216)	Loss 5.8665 (5.8665) ([4.627]+[1.240])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.116 (0.118)	Data 0.000 (0.002)	Loss 4.3957 (6.6613) ([2.350]+[2.045])	Prec@1 14.062 (10.736)
Epoch: [0][200/391]	Time 0.115 (0.117)	Data 0.000 (0.001)	Loss 4.1457 (5.4869) ([2.288]+[1.857])	Prec@1 5.469 (10.938)
Epoch: [0][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 3.7954 (4.9773) ([2.109]+[1.686])	Prec@1 19.531 (13.253)
Test: [0/79]	Time 0.246 (0.246)	Loss 3.5428 (3.5428) ([2.001]+[1.542])	Prec@1 25.000 (25.000)
 * Prec@1 21.260
current lr 1.00000e-01
Grad=  tensor(0.3460, device='cuda:0')
Epoch: [1][0/391]	Time 0.471 (0.471)	Data 0.283 (0.283)	Loss 3.5047 (3.5047) ([1.963]+[1.542])	Prec@1 28.125 (28.125)
Epoch: [1][100/391]	Time 0.113 (0.117)	Data 0.000 (0.003)	Loss 3.3447 (3.4832) ([1.952]+[1.393])	Prec@1 29.688 (21.875)
Epoch: [1][200/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 3.1427 (3.3682) ([1.888]+[1.255])	Prec@1 25.781 (23.927)
Epoch: [1][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 3.0779 (3.2686) ([1.951]+[1.127])	Prec@1 25.781 (25.722)
Test: [0/79]	Time 0.245 (0.245)	Loss 2.8568 (2.8568) ([1.836]+[1.021])	Prec@1 25.781 (25.781)
 * Prec@1 32.520
current lr 1.00000e-01
Grad=  tensor(0.7037, device='cuda:0')
Epoch: [2][0/391]	Time 0.338 (0.338)	Data 0.214 (0.214)	Loss 2.7641 (2.7641) ([1.743]+[1.021])	Prec@1 29.688 (29.688)
Epoch: [2][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 2.7348 (2.7652) ([1.818]+[0.917])	Prec@1 35.938 (31.474)
Epoch: [2][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 2.6327 (2.6792) ([1.813]+[0.820])	Prec@1 32.031 (33.384)
Epoch: [2][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 2.4065 (2.6058) ([1.669]+[0.738])	Prec@1 42.969 (34.668)
Test: [0/79]	Time 0.248 (0.248)	Loss 2.2263 (2.2263) ([1.548]+[0.679])	Prec@1 42.969 (42.969)
 * Prec@1 41.900
current lr 1.00000e-01
Grad=  tensor(0.9816, device='cuda:0')
Epoch: [3][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 2.1991 (2.1991) ([1.521]+[0.679])	Prec@1 44.531 (44.531)
Epoch: [3][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 2.2017 (2.2140) ([1.577]+[0.624])	Prec@1 39.062 (41.793)
Epoch: [3][200/391]	Time 0.109 (0.114)	Data 0.000 (0.001)	Loss 2.1529 (2.1648) ([1.577]+[0.575])	Prec@1 39.844 (42.778)
Epoch: [3][300/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 1.9480 (2.1155) ([1.423]+[0.525])	Prec@1 42.188 (43.950)
Test: [0/79]	Time 0.247 (0.247)	Loss 1.8346 (1.8346) ([1.347]+[0.487])	Prec@1 49.219 (49.219)
 * Prec@1 44.940
current lr 1.00000e-01
Grad=  tensor(1.0165, device='cuda:0')
Epoch: [4][0/391]	Time 0.333 (0.333)	Data 0.215 (0.215)	Loss 1.8446 (1.8446) ([1.357]+[0.487])	Prec@1 49.219 (49.219)
Epoch: [4][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 1.9580 (1.8565) ([1.506]+[0.452])	Prec@1 44.531 (48.584)
Epoch: [4][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 1.6442 (1.8234) ([1.220]+[0.424])	Prec@1 55.469 (49.662)
Epoch: [4][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.7571 (1.7921) ([1.354]+[0.403])	Prec@1 50.781 (50.566)
Test: [0/79]	Time 0.242 (0.242)	Loss 1.5939 (1.5939) ([1.214]+[0.380])	Prec@1 56.250 (56.250)
 * Prec@1 53.590
current lr 1.00000e-01
Grad=  tensor(1.2578, device='cuda:0')
Epoch: [5][0/391]	Time 0.319 (0.319)	Data 0.201 (0.201)	Loss 1.5395 (1.5395) ([1.159]+[0.380])	Prec@1 57.031 (57.031)
Epoch: [5][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 1.6333 (1.6210) ([1.271]+[0.362])	Prec@1 57.031 (54.533)
Epoch: [5][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.5360 (1.5963) ([1.194]+[0.342])	Prec@1 58.594 (55.002)
Epoch: [5][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.5430 (1.5697) ([1.211]+[0.332])	Prec@1 55.469 (55.915)
Test: [0/79]	Time 0.238 (0.238)	Loss 1.4482 (1.4482) ([1.127]+[0.321])	Prec@1 60.156 (60.156)
 * Prec@1 56.550
current lr 1.00000e-01
Grad=  tensor(1.3771, device='cuda:0')
Epoch: [6][0/391]	Time 0.344 (0.344)	Data 0.225 (0.225)	Loss 1.4179 (1.4179) ([1.097]+[0.321])	Prec@1 53.125 (53.125)
Epoch: [6][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 1.3765 (1.4617) ([1.063]+[0.314])	Prec@1 59.375 (58.988)
Epoch: [6][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 1.3510 (1.4511) ([1.024]+[0.327])	Prec@1 60.938 (59.663)
Epoch: [6][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.3876 (1.4303) ([1.086]+[0.301])	Prec@1 62.500 (60.312)
Test: [0/79]	Time 0.248 (0.248)	Loss 1.4395 (1.4395) ([1.150]+[0.289])	Prec@1 58.594 (58.594)
 * Prec@1 59.530
current lr 1.00000e-01
Grad=  tensor(1.3674, device='cuda:0')
Epoch: [7][0/391]	Time 0.317 (0.317)	Data 0.194 (0.194)	Loss 1.3810 (1.3810) ([1.092]+[0.289])	Prec@1 67.969 (67.969)
Epoch: [7][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 1.4538 (1.3128) ([1.176]+[0.278])	Prec@1 58.594 (63.436)
Epoch: [7][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 1.1605 (1.2970) ([0.889]+[0.271])	Prec@1 67.969 (63.981)
Epoch: [7][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 1.1813 (1.2790) ([0.913]+[0.268])	Prec@1 72.656 (64.535)
Test: [0/79]	Time 0.261 (0.261)	Loss 1.5754 (1.5754) ([1.314]+[0.261])	Prec@1 64.062 (64.062)
 * Prec@1 60.200
current lr 1.00000e-01
Grad=  tensor(1.1061, device='cuda:0')
Epoch: [8][0/391]	Time 0.332 (0.332)	Data 0.207 (0.207)	Loss 1.2061 (1.2061) ([0.945]+[0.261])	Prec@1 67.188 (67.188)
Epoch: [8][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 1.3138 (1.2150) ([1.053]+[0.261])	Prec@1 60.938 (65.687)
Epoch: [8][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 1.1636 (1.1971) ([0.908]+[0.256])	Prec@1 67.188 (66.721)
Epoch: [8][300/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 1.1095 (1.1981) ([0.855]+[0.255])	Prec@1 71.094 (66.850)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.5807 (1.5807) ([1.327]+[0.254])	Prec@1 53.125 (53.125)
 * Prec@1 56.480
current lr 1.00000e-01
Grad=  tensor(1.7955, device='cuda:0')
Epoch: [9][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 1.2233 (1.2233) ([0.969]+[0.254])	Prec@1 65.625 (65.625)
Epoch: [9][100/391]	Time 0.116 (0.117)	Data 0.000 (0.002)	Loss 1.1689 (1.1533) ([0.920]+[0.249])	Prec@1 69.531 (68.286)
Epoch: [9][200/391]	Time 0.109 (0.115)	Data 0.000 (0.001)	Loss 1.0427 (1.1401) ([0.781]+[0.262])	Prec@1 72.656 (68.575)
Epoch: [9][300/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 1.2648 (1.1335) ([1.015]+[0.249])	Prec@1 64.062 (68.851)
Test: [0/79]	Time 0.251 (0.251)	Loss 1.5358 (1.5358) ([1.288]+[0.248])	Prec@1 57.812 (57.812)
 * Prec@1 56.970
current lr 1.00000e-01
Grad=  tensor(1.5550, device='cuda:0')
Epoch: [10][0/391]	Time 0.359 (0.359)	Data 0.234 (0.234)	Loss 1.0378 (1.0378) ([0.790]+[0.248])	Prec@1 72.656 (72.656)
Epoch: [10][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 1.1597 (1.0882) ([0.913]+[0.247])	Prec@1 66.406 (70.220)
Epoch: [10][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 1.1187 (1.0888) ([0.875]+[0.244])	Prec@1 67.969 (70.072)
Epoch: [10][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 1.0497 (1.0857) ([0.805]+[0.245])	Prec@1 73.438 (70.396)
Test: [0/79]	Time 0.245 (0.245)	Loss 1.0520 (1.0520) ([0.807]+[0.245])	Prec@1 71.875 (71.875)
 * Prec@1 67.850
current lr 1.00000e-01
Grad=  tensor(1.6190, device='cuda:0')
Epoch: [11][0/391]	Time 0.333 (0.333)	Data 0.213 (0.213)	Loss 0.9398 (0.9398) ([0.695]+[0.245])	Prec@1 79.688 (79.688)
Epoch: [11][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.8733 (1.0383) ([0.635]+[0.239])	Prec@1 79.688 (72.184)
Epoch: [11][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.1385 (1.0397) ([0.902]+[0.236])	Prec@1 68.750 (72.038)
Epoch: [11][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.0702 (1.0403) ([0.837]+[0.233])	Prec@1 65.625 (71.865)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.1069 (1.1069) ([0.869]+[0.237])	Prec@1 73.438 (73.438)
 * Prec@1 65.530
current lr 1.00000e-01
Grad=  tensor(1.8988, device='cuda:0')
Epoch: [12][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.9745 (0.9745) ([0.737]+[0.237])	Prec@1 74.219 (74.219)
Epoch: [12][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 1.0684 (1.0030) ([0.834]+[0.235])	Prec@1 74.219 (72.734)
Epoch: [12][200/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 1.0921 (1.0088) ([0.859]+[0.233])	Prec@1 69.531 (72.652)
Epoch: [12][300/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.9137 (1.0027) ([0.682]+[0.232])	Prec@1 82.031 (72.999)
Test: [0/79]	Time 0.248 (0.248)	Loss 1.0825 (1.0825) ([0.854]+[0.228])	Prec@1 72.656 (72.656)
 * Prec@1 67.760
current lr 1.00000e-01
Grad=  tensor(1.5782, device='cuda:0')
Epoch: [13][0/391]	Time 0.334 (0.334)	Data 0.212 (0.212)	Loss 0.9830 (0.9830) ([0.755]+[0.228])	Prec@1 77.344 (77.344)
Epoch: [13][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.9477 (0.9737) ([0.719]+[0.228])	Prec@1 78.125 (74.288)
Epoch: [13][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.9049 (0.9817) ([0.678]+[0.227])	Prec@1 77.344 (73.752)
Epoch: [13][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 1.0002 (0.9790) ([0.771]+[0.229])	Prec@1 78.125 (73.770)
Test: [0/79]	Time 0.241 (0.241)	Loss 1.8598 (1.8598) ([1.634]+[0.226])	Prec@1 53.906 (53.906)
 * Prec@1 55.960
current lr 1.00000e-01
Grad=  tensor(1.9901, device='cuda:0')
Epoch: [14][0/391]	Time 0.327 (0.327)	Data 0.208 (0.208)	Loss 0.9342 (0.9342) ([0.708]+[0.226])	Prec@1 76.562 (76.562)
Epoch: [14][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.9488 (0.9483) ([0.723]+[0.226])	Prec@1 69.531 (74.729)
Epoch: [14][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.8981 (0.9508) ([0.675]+[0.223])	Prec@1 74.219 (74.852)
Epoch: [14][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 1.0463 (0.9490) ([0.822]+[0.224])	Prec@1 71.094 (74.860)
Test: [0/79]	Time 0.248 (0.248)	Loss 1.0391 (1.0391) ([0.818]+[0.221])	Prec@1 71.875 (71.875)
 * Prec@1 70.110
current lr 1.00000e-01
Grad=  tensor(2.1635, device='cuda:0')
Epoch: [15][0/391]	Time 0.313 (0.313)	Data 0.193 (0.193)	Loss 0.9439 (0.9439) ([0.723]+[0.221])	Prec@1 75.000 (75.000)
Epoch: [15][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 1.1412 (0.9110) ([0.921]+[0.220])	Prec@1 67.969 (75.936)
Epoch: [15][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.9613 (0.9048) ([0.743]+[0.219])	Prec@1 74.219 (76.193)
Epoch: [15][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 1.0267 (0.9012) ([0.809]+[0.218])	Prec@1 75.781 (76.433)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.9552 (0.9552) ([0.738]+[0.218])	Prec@1 72.656 (72.656)
 * Prec@1 73.040
current lr 1.00000e-01
Grad=  tensor(1.9094, device='cuda:0')
Epoch: [16][0/391]	Time 0.336 (0.336)	Data 0.216 (0.216)	Loss 0.8154 (0.8154) ([0.598]+[0.218])	Prec@1 79.688 (79.688)
Epoch: [16][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.8745 (0.8956) ([0.658]+[0.217])	Prec@1 78.906 (76.555)
Epoch: [16][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7934 (0.8881) ([0.578]+[0.216])	Prec@1 75.781 (76.784)
Epoch: [16][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.9627 (0.8877) ([0.748]+[0.214])	Prec@1 72.656 (76.703)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.8630 (0.8630) ([0.650]+[0.213])	Prec@1 75.781 (75.781)
 * Prec@1 72.140
current lr 1.00000e-01
Grad=  tensor(2.0719, device='cuda:0')
Epoch: [17][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.8755 (0.8755) ([0.663]+[0.213])	Prec@1 79.688 (79.688)
Epoch: [17][100/391]	Time 0.115 (0.113)	Data 0.000 (0.002)	Loss 0.9039 (0.8654) ([0.691]+[0.213])	Prec@1 75.781 (77.614)
Epoch: [17][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8874 (0.8590) ([0.675]+[0.213])	Prec@1 75.781 (77.837)
Epoch: [17][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.8457 (0.8633) ([0.634]+[0.212])	Prec@1 79.688 (77.627)
Test: [0/79]	Time 0.250 (0.250)	Loss 1.0437 (1.0437) ([0.832]+[0.211])	Prec@1 73.438 (73.438)
 * Prec@1 73.130
current lr 1.00000e-01
Grad=  tensor(2.1184, device='cuda:0')
Epoch: [18][0/391]	Time 0.312 (0.312)	Data 0.191 (0.191)	Loss 0.7874 (0.7874) ([0.576]+[0.211])	Prec@1 79.688 (79.688)
Epoch: [18][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7385 (0.8583) ([0.527]+[0.211])	Prec@1 78.906 (77.429)
Epoch: [18][200/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.8324 (0.8579) ([0.620]+[0.212])	Prec@1 78.125 (77.355)
Epoch: [18][300/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.8576 (0.8584) ([0.646]+[0.212])	Prec@1 72.656 (77.354)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.8793 (0.8793) ([0.669]+[0.210])	Prec@1 78.125 (78.125)
 * Prec@1 72.540
current lr 1.00000e-01
Grad=  tensor(2.3972, device='cuda:0')
Epoch: [19][0/391]	Time 0.330 (0.330)	Data 0.206 (0.206)	Loss 1.0108 (1.0108) ([0.801]+[0.210])	Prec@1 75.000 (75.000)
Epoch: [19][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.8272 (0.8252) ([0.617]+[0.210])	Prec@1 82.812 (79.216)
Epoch: [19][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.9732 (0.8303) ([0.764]+[0.209])	Prec@1 77.344 (78.832)
Epoch: [19][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8439 (0.8235) ([0.636]+[0.208])	Prec@1 77.344 (79.005)
Test: [0/79]	Time 0.245 (0.245)	Loss 1.1533 (1.1533) ([0.946]+[0.208])	Prec@1 71.875 (71.875)
 * Prec@1 69.910
current lr 1.00000e-01
Grad=  tensor(1.9878, device='cuda:0')
Epoch: [20][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.9970 (0.9970) ([0.789]+[0.208])	Prec@1 78.906 (78.906)
Epoch: [20][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8196 (0.8097) ([0.613]+[0.207])	Prec@1 81.250 (79.432)
Epoch: [20][200/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.7005 (0.8148) ([0.494]+[0.207])	Prec@1 82.031 (78.937)
Epoch: [20][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.6257 (0.8205) ([0.419]+[0.207])	Prec@1 84.375 (78.706)
Test: [0/79]	Time 0.246 (0.246)	Loss 1.1166 (1.1166) ([0.910]+[0.207])	Prec@1 71.094 (71.094)
 * Prec@1 68.160
current lr 1.00000e-01
Grad=  tensor(1.8011, device='cuda:0')
Epoch: [21][0/391]	Time 0.341 (0.341)	Data 0.220 (0.220)	Loss 0.8445 (0.8445) ([0.637]+[0.207])	Prec@1 76.562 (76.562)
Epoch: [21][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.9060 (0.8132) ([0.699]+[0.207])	Prec@1 74.219 (78.813)
Epoch: [21][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6829 (0.8124) ([0.476]+[0.207])	Prec@1 84.375 (78.941)
Epoch: [21][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7581 (0.8083) ([0.551]+[0.207])	Prec@1 81.250 (79.140)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.8587 (0.8587) ([0.652]+[0.207])	Prec@1 78.125 (78.125)
 * Prec@1 76.970
current lr 1.00000e-01
Grad=  tensor(1.8031, device='cuda:0')
Epoch: [22][0/391]	Time 0.326 (0.326)	Data 0.206 (0.206)	Loss 0.8410 (0.8410) ([0.634]+[0.207])	Prec@1 79.688 (79.688)
Epoch: [22][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8000 (0.7872) ([0.594]+[0.206])	Prec@1 80.469 (80.113)
Epoch: [22][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6403 (0.7988) ([0.435]+[0.205])	Prec@1 85.938 (79.633)
Epoch: [22][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7634 (0.7998) ([0.558]+[0.205])	Prec@1 80.469 (79.571)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.9627 (0.9627) ([0.758]+[0.205])	Prec@1 73.438 (73.438)
 * Prec@1 72.910
current lr 1.00000e-01
Grad=  tensor(1.5270, device='cuda:0')
Epoch: [23][0/391]	Time 0.307 (0.307)	Data 0.186 (0.186)	Loss 0.7366 (0.7366) ([0.532]+[0.205])	Prec@1 81.250 (81.250)
Epoch: [23][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.8805 (0.7844) ([0.676]+[0.204])	Prec@1 76.562 (80.252)
Epoch: [23][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8899 (0.7806) ([0.685]+[0.205])	Prec@1 78.906 (80.271)
Epoch: [23][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8397 (0.7791) ([0.635]+[0.204])	Prec@1 78.906 (80.323)
Test: [0/79]	Time 0.239 (0.239)	Loss 1.1775 (1.1775) ([0.973]+[0.204])	Prec@1 67.969 (67.969)
 * Prec@1 70.860
current lr 1.00000e-01
Grad=  tensor(2.4267, device='cuda:0')
Epoch: [24][0/391]	Time 0.325 (0.325)	Data 0.203 (0.203)	Loss 0.8193 (0.8193) ([0.615]+[0.204])	Prec@1 78.125 (78.125)
Epoch: [24][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.9114 (0.7797) ([0.707]+[0.204])	Prec@1 75.781 (80.500)
Epoch: [24][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7666 (0.7807) ([0.563]+[0.204])	Prec@1 79.688 (80.306)
Epoch: [24][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9579 (0.7841) ([0.756]+[0.202])	Prec@1 77.344 (80.199)
Test: [0/79]	Time 0.254 (0.254)	Loss 1.2737 (1.2737) ([1.072]+[0.202])	Prec@1 67.969 (67.969)
 * Prec@1 66.470
current lr 1.00000e-01
Grad=  tensor(1.6628, device='cuda:0')
Epoch: [25][0/391]	Time 0.325 (0.325)	Data 0.204 (0.204)	Loss 0.8030 (0.8030) ([0.601]+[0.202])	Prec@1 80.469 (80.469)
Epoch: [25][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.7687 (0.7831) ([0.567]+[0.202])	Prec@1 83.594 (79.904)
Epoch: [25][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.7326 (0.7812) ([0.530]+[0.203])	Prec@1 81.250 (80.022)
Epoch: [25][300/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.7476 (0.7778) ([0.545]+[0.203])	Prec@1 82.031 (80.220)
Test: [0/79]	Time 0.252 (0.252)	Loss 1.0972 (1.0972) ([0.895]+[0.202])	Prec@1 67.188 (67.188)
 * Prec@1 71.070
current lr 1.00000e-01
Grad=  tensor(3.0283, device='cuda:0')
Epoch: [26][0/391]	Time 0.338 (0.338)	Data 0.214 (0.214)	Loss 0.8835 (0.8835) ([0.681]+[0.202])	Prec@1 71.875 (71.875)
Epoch: [26][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.7155 (0.7908) ([0.513]+[0.202])	Prec@1 82.031 (79.718)
Epoch: [26][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7318 (0.7871) ([0.531]+[0.201])	Prec@1 82.031 (79.963)
Epoch: [26][300/391]	Time 0.109 (0.114)	Data 0.000 (0.001)	Loss 0.6668 (0.7802) ([0.466]+[0.200])	Prec@1 82.812 (80.196)
Test: [0/79]	Time 0.253 (0.253)	Loss 1.3606 (1.3606) ([1.160]+[0.200])	Prec@1 57.812 (57.812)
 * Prec@1 65.550
current lr 1.00000e-01
Grad=  tensor(1.4749, device='cuda:0')
Epoch: [27][0/391]	Time 0.333 (0.333)	Data 0.213 (0.213)	Loss 0.6465 (0.6465) ([0.446]+[0.200])	Prec@1 85.156 (85.156)
Epoch: [27][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.7477 (0.7654) ([0.547]+[0.201])	Prec@1 78.906 (80.848)
Epoch: [27][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.5923 (0.7565) ([0.392]+[0.200])	Prec@1 84.375 (81.126)
Epoch: [27][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7993 (0.7597) ([0.599]+[0.200])	Prec@1 77.344 (80.975)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.7965 (0.7965) ([0.597]+[0.199])	Prec@1 77.344 (77.344)
 * Prec@1 77.660
current lr 1.00000e-01
Grad=  tensor(1.8420, device='cuda:0')
Epoch: [28][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.7952 (0.7952) ([0.596]+[0.199])	Prec@1 77.344 (77.344)
Epoch: [28][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.8665 (0.7419) ([0.667]+[0.199])	Prec@1 76.562 (81.242)
Epoch: [28][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.8199 (0.7453) ([0.621]+[0.199])	Prec@1 81.250 (81.071)
Epoch: [28][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.7047 (0.7515) ([0.505]+[0.200])	Prec@1 81.250 (80.741)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.9366 (0.9366) ([0.737]+[0.200])	Prec@1 75.781 (75.781)
 * Prec@1 76.120
current lr 1.00000e-01
Grad=  tensor(1.9888, device='cuda:0')
Epoch: [29][0/391]	Time 0.331 (0.331)	Data 0.211 (0.211)	Loss 0.8028 (0.8028) ([0.603]+[0.200])	Prec@1 78.125 (78.125)
Epoch: [29][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.7017 (0.7494) ([0.503]+[0.199])	Prec@1 78.906 (81.258)
Epoch: [29][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.9102 (0.7524) ([0.711]+[0.199])	Prec@1 76.562 (81.102)
Epoch: [29][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.6784 (0.7530) ([0.481]+[0.198])	Prec@1 84.375 (81.066)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.7777 (0.7777) ([0.580]+[0.198])	Prec@1 78.125 (78.125)
 * Prec@1 78.720
current lr 1.00000e-01
Grad=  tensor(1.6021, device='cuda:0')
Epoch: [30][0/391]	Time 0.311 (0.311)	Data 0.192 (0.192)	Loss 0.6424 (0.6424) ([0.445]+[0.198])	Prec@1 85.156 (85.156)
Epoch: [30][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.8060 (0.7316) ([0.609]+[0.197])	Prec@1 81.250 (82.155)
Epoch: [30][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.6300 (0.7404) ([0.433]+[0.197])	Prec@1 85.938 (81.510)
Epoch: [30][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.6942 (0.7413) ([0.498]+[0.196])	Prec@1 82.031 (81.408)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.9264 (0.9264) ([0.731]+[0.196])	Prec@1 79.688 (79.688)
 * Prec@1 74.940
current lr 1.00000e-01
Grad=  tensor(2.0110, device='cuda:0')
Epoch: [31][0/391]	Time 0.324 (0.324)	Data 0.203 (0.203)	Loss 0.7295 (0.7295) ([0.534]+[0.196])	Prec@1 83.594 (83.594)
Epoch: [31][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6446 (0.7430) ([0.448]+[0.196])	Prec@1 86.719 (81.049)
Epoch: [31][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.8804 (0.7488) ([0.684]+[0.196])	Prec@1 77.344 (80.846)
Epoch: [31][300/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.7261 (0.7447) ([0.530]+[0.196])	Prec@1 82.812 (80.944)
Test: [0/79]	Time 0.244 (0.244)	Loss 1.2449 (1.2449) ([1.048]+[0.197])	Prec@1 67.188 (67.188)
 * Prec@1 68.850
current lr 1.00000e-01
Grad=  tensor(1.5488, device='cuda:0')
Epoch: [32][0/391]	Time 0.320 (0.320)	Data 0.195 (0.195)	Loss 0.6759 (0.6759) ([0.479]+[0.197])	Prec@1 83.594 (83.594)
Epoch: [32][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.8865 (0.7159) ([0.690]+[0.196])	Prec@1 75.781 (82.325)
Epoch: [32][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7249 (0.7336) ([0.529]+[0.196])	Prec@1 80.469 (81.790)
Epoch: [32][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.7111 (0.7382) ([0.515]+[0.196])	Prec@1 82.812 (81.502)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.9817 (0.9817) ([0.786]+[0.196])	Prec@1 75.000 (75.000)
 * Prec@1 73.600
current lr 1.00000e-01
Grad=  tensor(1.7925, device='cuda:0')
Epoch: [33][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.7621 (0.7621) ([0.566]+[0.196])	Prec@1 82.812 (82.812)
Epoch: [33][100/391]	Time 0.108 (0.112)	Data 0.000 (0.002)	Loss 0.6157 (0.7341) ([0.420]+[0.196])	Prec@1 86.719 (81.374)
Epoch: [33][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.7538 (0.7390) ([0.558]+[0.196])	Prec@1 79.688 (81.351)
Epoch: [33][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7070 (0.7354) ([0.511]+[0.196])	Prec@1 85.938 (81.502)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.9683 (0.9683) ([0.772]+[0.196])	Prec@1 75.000 (75.000)
 * Prec@1 72.980
current lr 1.00000e-01
Grad=  tensor(1.9031, device='cuda:0')
Epoch: [34][0/391]	Time 0.334 (0.334)	Data 0.215 (0.215)	Loss 0.7805 (0.7805) ([0.584]+[0.196])	Prec@1 78.906 (78.906)
Epoch: [34][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.6849 (0.7126) ([0.489]+[0.196])	Prec@1 88.281 (82.372)
Epoch: [34][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6974 (0.7231) ([0.502]+[0.196])	Prec@1 82.812 (81.992)
Epoch: [34][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.9244 (0.7283) ([0.729]+[0.195])	Prec@1 75.000 (81.743)
Test: [0/79]	Time 0.238 (0.238)	Loss 1.0718 (1.0718) ([0.877]+[0.195])	Prec@1 75.000 (75.000)
 * Prec@1 70.820
current lr 1.00000e-01
Grad=  tensor(1.3547, device='cuda:0')
Epoch: [35][0/391]	Time 0.349 (0.349)	Data 0.225 (0.225)	Loss 0.6538 (0.6538) ([0.459]+[0.195])	Prec@1 84.375 (84.375)
Epoch: [35][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6661 (0.7308) ([0.471]+[0.195])	Prec@1 80.469 (81.614)
Epoch: [35][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.8159 (0.7215) ([0.622]+[0.194])	Prec@1 77.344 (81.922)
Epoch: [35][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7105 (0.7204) ([0.516]+[0.194])	Prec@1 84.375 (81.977)
Test: [0/79]	Time 0.245 (0.245)	Loss 1.0615 (1.0615) ([0.867]+[0.195])	Prec@1 71.875 (71.875)
 * Prec@1 71.810
current lr 1.00000e-01
Grad=  tensor(1.7334, device='cuda:0')
Epoch: [36][0/391]	Time 0.331 (0.331)	Data 0.212 (0.212)	Loss 0.7284 (0.7284) ([0.534]+[0.195])	Prec@1 79.688 (79.688)
Epoch: [36][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.7090 (0.7239) ([0.514]+[0.195])	Prec@1 80.469 (81.722)
Epoch: [36][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7836 (0.7274) ([0.588]+[0.196])	Prec@1 78.125 (81.782)
Epoch: [36][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.8012 (0.7247) ([0.606]+[0.195])	Prec@1 78.125 (81.702)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.8068 (0.8068) ([0.612]+[0.195])	Prec@1 78.906 (78.906)
 * Prec@1 77.210
current lr 1.00000e-01
Grad=  tensor(1.2250, device='cuda:0')
Epoch: [37][0/391]	Time 0.333 (0.333)	Data 0.214 (0.214)	Loss 0.6068 (0.6068) ([0.412]+[0.195])	Prec@1 83.594 (83.594)
Epoch: [37][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.7782 (0.7123) ([0.584]+[0.194])	Prec@1 78.906 (82.194)
Epoch: [37][200/391]	Time 0.115 (0.111)	Data 0.000 (0.001)	Loss 0.8447 (0.7095) ([0.651]+[0.194])	Prec@1 76.562 (82.338)
Epoch: [37][300/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.6179 (0.7154) ([0.424]+[0.194])	Prec@1 83.594 (82.187)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.9099 (0.9099) ([0.716]+[0.194])	Prec@1 78.906 (78.906)
 * Prec@1 75.950
current lr 1.00000e-01
Grad=  tensor(2.2046, device='cuda:0')
Epoch: [38][0/391]	Time 0.356 (0.356)	Data 0.231 (0.231)	Loss 0.7791 (0.7791) ([0.585]+[0.194])	Prec@1 79.688 (79.688)
Epoch: [38][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.7532 (0.6931) ([0.560]+[0.193])	Prec@1 80.469 (82.666)
Epoch: [38][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.6631 (0.7018) ([0.469]+[0.194])	Prec@1 83.594 (82.533)
Epoch: [38][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.6102 (0.7052) ([0.416]+[0.194])	Prec@1 87.500 (82.350)
Test: [0/79]	Time 0.247 (0.247)	Loss 1.1193 (1.1193) ([0.925]+[0.194])	Prec@1 69.531 (69.531)
 * Prec@1 70.320
current lr 1.00000e-01
Grad=  tensor(1.2173, device='cuda:0')
Epoch: [39][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.5321 (0.5321) ([0.338]+[0.194])	Prec@1 89.844 (89.844)
Epoch: [39][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.6582 (0.6952) ([0.464]+[0.194])	Prec@1 83.594 (82.596)
Epoch: [39][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.7092 (0.7050) ([0.516]+[0.193])	Prec@1 82.031 (82.432)
Epoch: [39][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.7224 (0.7077) ([0.529]+[0.193])	Prec@1 85.938 (82.405)
Test: [0/79]	Time 0.251 (0.251)	Loss 1.4913 (1.4913) ([1.299]+[0.192])	Prec@1 58.594 (58.594)
 * Prec@1 61.880
current lr 1.00000e-01
Grad=  tensor(2.3158, device='cuda:0')
Epoch: [40][0/391]	Time 0.366 (0.366)	Data 0.239 (0.239)	Loss 0.7269 (0.7269) ([0.535]+[0.192])	Prec@1 84.375 (84.375)
Epoch: [40][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.5256 (0.7085) ([0.333]+[0.193])	Prec@1 88.281 (82.348)
Epoch: [40][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.7818 (0.7092) ([0.589]+[0.193])	Prec@1 82.812 (82.451)
Epoch: [40][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.6168 (0.7133) ([0.424]+[0.193])	Prec@1 83.594 (82.223)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.8373 (0.8373) ([0.644]+[0.193])	Prec@1 82.031 (82.031)
 * Prec@1 78.140
current lr 1.00000e-01
Grad=  tensor(2.0116, device='cuda:0')
Epoch: [41][0/391]	Time 0.355 (0.355)	Data 0.231 (0.231)	Loss 0.6821 (0.6821) ([0.489]+[0.193])	Prec@1 83.594 (83.594)
Epoch: [41][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.7899 (0.6965) ([0.597]+[0.193])	Prec@1 80.469 (82.952)
Epoch: [41][200/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.7695 (0.7002) ([0.577]+[0.193])	Prec@1 75.781 (82.785)
Epoch: [41][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7289 (0.7022) ([0.536]+[0.193])	Prec@1 79.688 (82.750)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.8551 (0.8551) ([0.662]+[0.193])	Prec@1 75.781 (75.781)
 * Prec@1 78.140
current lr 1.00000e-01
Grad=  tensor(1.9319, device='cuda:0')
Epoch: [42][0/391]	Time 0.334 (0.334)	Data 0.213 (0.213)	Loss 0.7573 (0.7573) ([0.564]+[0.193])	Prec@1 78.906 (78.906)
Epoch: [42][100/391]	Time 0.115 (0.114)	Data 0.000 (0.002)	Loss 0.7059 (0.7244) ([0.512]+[0.193])	Prec@1 84.375 (81.753)
Epoch: [42][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.6307 (0.7131) ([0.437]+[0.194])	Prec@1 85.938 (82.117)
Epoch: [42][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.8457 (0.7132) ([0.653]+[0.193])	Prec@1 78.125 (82.161)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.9072 (0.9072) ([0.715]+[0.193])	Prec@1 72.656 (72.656)
 * Prec@1 74.570
current lr 1.00000e-01
Grad=  tensor(1.9606, device='cuda:0')
Epoch: [43][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.7305 (0.7305) ([0.538]+[0.193])	Prec@1 82.812 (82.812)
Epoch: [43][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.6628 (0.7007) ([0.470]+[0.193])	Prec@1 82.031 (82.519)
Epoch: [43][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7107 (0.7062) ([0.518]+[0.193])	Prec@1 81.250 (82.268)
Epoch: [43][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.5267 (0.7075) ([0.334]+[0.192])	Prec@1 89.062 (82.306)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.9395 (0.9395) ([0.748]+[0.191])	Prec@1 72.656 (72.656)
 * Prec@1 75.480
current lr 1.00000e-01
Grad=  tensor(1.5041, device='cuda:0')
Epoch: [44][0/391]	Time 0.336 (0.336)	Data 0.215 (0.215)	Loss 0.6128 (0.6128) ([0.421]+[0.191])	Prec@1 84.375 (84.375)
Epoch: [44][100/391]	Time 0.110 (0.115)	Data 0.000 (0.002)	Loss 0.8504 (0.7086) ([0.659]+[0.192])	Prec@1 79.688 (82.588)
Epoch: [44][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.8299 (0.6990) ([0.639]+[0.191])	Prec@1 75.781 (82.816)
Epoch: [44][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.6512 (0.6966) ([0.460]+[0.191])	Prec@1 82.812 (82.846)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.8582 (0.8582) ([0.667]+[0.191])	Prec@1 78.125 (78.125)
 * Prec@1 75.400
current lr 1.00000e-01
Grad=  tensor(1.7972, device='cuda:0')
Epoch: [45][0/391]	Time 0.352 (0.352)	Data 0.227 (0.227)	Loss 0.7546 (0.7546) ([0.564]+[0.191])	Prec@1 81.250 (81.250)
Epoch: [45][100/391]	Time 0.111 (0.114)	Data 0.000 (0.002)	Loss 0.5808 (0.6918) ([0.389]+[0.191])	Prec@1 89.844 (82.658)
Epoch: [45][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.5267 (0.7008) ([0.335]+[0.191])	Prec@1 89.844 (82.381)
Epoch: [45][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.6003 (0.7054) ([0.409]+[0.191])	Prec@1 85.156 (82.192)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.7707 (0.7707) ([0.580]+[0.191])	Prec@1 79.688 (79.688)
 * Prec@1 77.350
current lr 1.00000e-01
Grad=  tensor(1.2997, device='cuda:0')
Epoch: [46][0/391]	Time 0.331 (0.331)	Data 0.210 (0.210)	Loss 0.6188 (0.6188) ([0.428]+[0.191])	Prec@1 83.594 (83.594)
Epoch: [46][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6812 (0.7038) ([0.490]+[0.191])	Prec@1 81.250 (82.317)
Epoch: [46][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6995 (0.7104) ([0.508]+[0.192])	Prec@1 81.250 (82.264)
Epoch: [46][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8637 (0.7015) ([0.673]+[0.191])	Prec@1 76.562 (82.589)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.9216 (0.9216) ([0.731]+[0.191])	Prec@1 78.906 (78.906)
 * Prec@1 77.100
current lr 1.00000e-01
Grad=  tensor(1.7603, device='cuda:0')
Epoch: [47][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.6952 (0.6952) ([0.504]+[0.191])	Prec@1 85.156 (85.156)
Epoch: [47][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.5896 (0.6830) ([0.399]+[0.191])	Prec@1 84.375 (83.168)
Epoch: [47][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7088 (0.6984) ([0.518]+[0.191])	Prec@1 80.469 (82.502)
Epoch: [47][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.6880 (0.7005) ([0.497]+[0.191])	Prec@1 84.375 (82.350)
Test: [0/79]	Time 0.246 (0.246)	Loss 1.1965 (1.1965) ([1.006]+[0.191])	Prec@1 68.750 (68.750)
 * Prec@1 71.690
current lr 1.00000e-01
Grad=  tensor(2.5506, device='cuda:0')
Epoch: [48][0/391]	Time 0.332 (0.332)	Data 0.211 (0.211)	Loss 0.8061 (0.8061) ([0.615]+[0.191])	Prec@1 85.156 (85.156)
Epoch: [48][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.7051 (0.7081) ([0.514]+[0.191])	Prec@1 84.375 (81.907)
Epoch: [48][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.6574 (0.7068) ([0.466]+[0.191])	Prec@1 83.594 (82.198)
Epoch: [48][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.6881 (0.7019) ([0.496]+[0.192])	Prec@1 84.375 (82.441)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.7903 (0.7903) ([0.599]+[0.191])	Prec@1 82.031 (82.031)
 * Prec@1 75.460
current lr 1.00000e-01
Grad=  tensor(2.1440, device='cuda:0')
Epoch: [49][0/391]	Time 0.340 (0.340)	Data 0.216 (0.216)	Loss 0.6432 (0.6432) ([0.452]+[0.191])	Prec@1 84.375 (84.375)
Epoch: [49][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.7212 (0.6979) ([0.530]+[0.191])	Prec@1 82.812 (82.433)
Epoch: [49][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7566 (0.6945) ([0.566]+[0.190])	Prec@1 80.469 (82.673)
Epoch: [49][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7292 (0.6973) ([0.539]+[0.190])	Prec@1 82.031 (82.631)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.9977 (0.9977) ([0.808]+[0.190])	Prec@1 76.562 (76.562)
 * Prec@1 75.520
current lr 1.00000e-01
Grad=  tensor(2.0654, device='cuda:0')
Epoch: [50][0/391]	Time 0.334 (0.334)	Data 0.213 (0.213)	Loss 0.7306 (0.7306) ([0.541]+[0.190])	Prec@1 81.250 (81.250)
Epoch: [50][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.7338 (0.6760) ([0.544]+[0.190])	Prec@1 83.594 (83.346)
Epoch: [50][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5363 (0.6847) ([0.347]+[0.190])	Prec@1 89.062 (83.092)
Epoch: [50][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7814 (0.6938) ([0.591]+[0.190])	Prec@1 78.906 (82.800)
Test: [0/79]	Time 0.256 (0.256)	Loss 1.0467 (1.0467) ([0.857]+[0.190])	Prec@1 74.219 (74.219)
 * Prec@1 71.080
current lr 1.00000e-01
Grad=  tensor(2.4121, device='cuda:0')
Epoch: [51][0/391]	Time 0.299 (0.299)	Data 0.178 (0.178)	Loss 0.7967 (0.7967) ([0.607]+[0.190])	Prec@1 80.469 (80.469)
Epoch: [51][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.7211 (0.6967) ([0.531]+[0.190])	Prec@1 81.250 (82.565)
Epoch: [51][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6423 (0.6889) ([0.453]+[0.190])	Prec@1 86.719 (82.863)
Epoch: [51][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.7566 (0.6820) ([0.568]+[0.189])	Prec@1 76.562 (83.129)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.9851 (0.9851) ([0.796]+[0.189])	Prec@1 75.000 (75.000)
 * Prec@1 73.170
current lr 1.00000e-01
Grad=  tensor(1.8201, device='cuda:0')
Epoch: [52][0/391]	Time 0.327 (0.327)	Data 0.207 (0.207)	Loss 0.6737 (0.6737) ([0.485]+[0.189])	Prec@1 80.469 (80.469)
Epoch: [52][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.7130 (0.6796) ([0.524]+[0.189])	Prec@1 82.031 (83.045)
Epoch: [52][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7046 (0.6807) ([0.515]+[0.189])	Prec@1 82.812 (83.081)
Epoch: [52][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7937 (0.6877) ([0.604]+[0.189])	Prec@1 78.125 (82.846)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.8086 (0.8086) ([0.620]+[0.189])	Prec@1 75.781 (75.781)
 * Prec@1 78.750
current lr 1.00000e-01
Grad=  tensor(1.7737, device='cuda:0')
Epoch: [53][0/391]	Time 0.307 (0.307)	Data 0.184 (0.184)	Loss 0.6539 (0.6539) ([0.465]+[0.189])	Prec@1 83.594 (83.594)
Epoch: [53][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.5285 (0.6837) ([0.340]+[0.189])	Prec@1 88.281 (83.037)
Epoch: [53][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.6737 (0.6840) ([0.485]+[0.188])	Prec@1 84.375 (83.318)
Epoch: [53][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7629 (0.6894) ([0.575]+[0.188])	Prec@1 78.906 (83.012)
Test: [0/79]	Time 0.247 (0.247)	Loss 1.0385 (1.0385) ([0.850]+[0.188])	Prec@1 73.438 (73.438)
 * Prec@1 75.830
current lr 1.00000e-01
Grad=  tensor(2.7893, device='cuda:0')
Epoch: [54][0/391]	Time 0.344 (0.344)	Data 0.222 (0.222)	Loss 0.7176 (0.7176) ([0.530]+[0.188])	Prec@1 81.250 (81.250)
Epoch: [54][100/391]	Time 0.115 (0.114)	Data 0.000 (0.002)	Loss 0.6559 (0.6838) ([0.468]+[0.188])	Prec@1 82.031 (82.975)
Epoch: [54][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6450 (0.6824) ([0.457]+[0.188])	Prec@1 82.031 (83.221)
Epoch: [54][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.7877 (0.6881) ([0.600]+[0.188])	Prec@1 82.812 (83.012)
Test: [0/79]	Time 0.261 (0.261)	Loss 1.2538 (1.2538) ([1.065]+[0.188])	Prec@1 68.750 (68.750)
 * Prec@1 71.280
current lr 1.00000e-01
Grad=  tensor(2.2405, device='cuda:0')
Epoch: [55][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.7353 (0.7353) ([0.547]+[0.188])	Prec@1 80.469 (80.469)
Epoch: [55][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6258 (0.6680) ([0.437]+[0.188])	Prec@1 85.938 (83.787)
Epoch: [55][200/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.7074 (0.6718) ([0.520]+[0.188])	Prec@1 87.500 (83.485)
Epoch: [55][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7485 (0.6790) ([0.560]+[0.188])	Prec@1 80.469 (83.178)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.9948 (0.9948) ([0.807]+[0.188])	Prec@1 76.562 (76.562)
 * Prec@1 73.850
current lr 1.00000e-01
Grad=  tensor(1.9180, device='cuda:0')
Epoch: [56][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.7075 (0.7075) ([0.520]+[0.188])	Prec@1 84.375 (84.375)
Epoch: [56][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.7279 (0.6812) ([0.540]+[0.188])	Prec@1 81.250 (82.843)
Epoch: [56][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7269 (0.6806) ([0.539]+[0.188])	Prec@1 81.250 (83.057)
Epoch: [56][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7005 (0.6847) ([0.512]+[0.189])	Prec@1 84.375 (83.012)
Test: [0/79]	Time 0.210 (0.210)	Loss 1.1283 (1.1283) ([0.940]+[0.189])	Prec@1 67.969 (67.969)
 * Prec@1 71.030
current lr 1.00000e-01
Grad=  tensor(2.8497, device='cuda:0')
Epoch: [57][0/391]	Time 0.339 (0.339)	Data 0.220 (0.220)	Loss 0.8548 (0.8548) ([0.666]+[0.189])	Prec@1 75.781 (75.781)
Epoch: [57][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.5391 (0.6822) ([0.350]+[0.189])	Prec@1 85.938 (83.091)
Epoch: [57][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6031 (0.6810) ([0.414]+[0.189])	Prec@1 82.812 (83.205)
Epoch: [57][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.8012 (0.6964) ([0.612]+[0.189])	Prec@1 78.125 (82.745)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.8841 (0.8841) ([0.696]+[0.188])	Prec@1 78.125 (78.125)
 * Prec@1 74.770
current lr 1.00000e-01
Grad=  tensor(1.4896, device='cuda:0')
Epoch: [58][0/391]	Time 0.323 (0.323)	Data 0.203 (0.203)	Loss 0.5812 (0.5812) ([0.393]+[0.188])	Prec@1 87.500 (87.500)
Epoch: [58][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.5068 (0.6656) ([0.319]+[0.188])	Prec@1 91.406 (83.718)
Epoch: [58][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6260 (0.6723) ([0.438]+[0.188])	Prec@1 87.500 (83.407)
Epoch: [58][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6141 (0.6721) ([0.427]+[0.187])	Prec@1 86.719 (83.498)
Test: [0/79]	Time 0.240 (0.240)	Loss 1.0812 (1.0812) ([0.894]+[0.187])	Prec@1 71.094 (71.094)
 * Prec@1 75.140
current lr 1.00000e-01
Grad=  tensor(1.6669, device='cuda:0')
Epoch: [59][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.6147 (0.6147) ([0.427]+[0.187])	Prec@1 86.719 (86.719)
Epoch: [59][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.4780 (0.6600) ([0.291]+[0.187])	Prec@1 89.062 (83.864)
Epoch: [59][200/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.8097 (0.6751) ([0.622]+[0.187])	Prec@1 78.125 (83.329)
Epoch: [59][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6701 (0.6851) ([0.483]+[0.188])	Prec@1 84.375 (82.955)
Test: [0/79]	Time 0.251 (0.251)	Loss 1.0562 (1.0562) ([0.869]+[0.187])	Prec@1 75.781 (75.781)
 * Prec@1 70.910
current lr 1.00000e-01
Grad=  tensor(2.0072, device='cuda:0')
Epoch: [60][0/391]	Time 0.339 (0.339)	Data 0.219 (0.219)	Loss 0.8073 (0.8073) ([0.620]+[0.187])	Prec@1 79.688 (79.688)
Epoch: [60][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.5529 (0.6791) ([0.366]+[0.187])	Prec@1 86.719 (83.037)
Epoch: [60][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.6323 (0.6835) ([0.445]+[0.187])	Prec@1 81.250 (83.046)
Epoch: [60][300/391]	Time 0.111 (0.115)	Data 0.000 (0.001)	Loss 0.7112 (0.6813) ([0.524]+[0.187])	Prec@1 82.031 (83.023)
Test: [0/79]	Time 0.263 (0.263)	Loss 1.2399 (1.2399) ([1.053]+[0.187])	Prec@1 65.625 (65.625)
 * Prec@1 66.280
current lr 1.00000e-01
Grad=  tensor(1.7445, device='cuda:0')
Epoch: [61][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.6654 (0.6654) ([0.478]+[0.187])	Prec@1 82.031 (82.031)
Epoch: [61][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.6628 (0.6770) ([0.476]+[0.187])	Prec@1 85.156 (83.083)
Epoch: [61][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6768 (0.6722) ([0.490]+[0.187])	Prec@1 82.031 (83.407)
Epoch: [61][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6745 (0.6779) ([0.487]+[0.187])	Prec@1 84.375 (83.095)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.7493 (0.7493) ([0.562]+[0.187])	Prec@1 81.250 (81.250)
 * Prec@1 80.860
current lr 1.00000e-01
Grad=  tensor(2.5079, device='cuda:0')
Epoch: [62][0/391]	Time 0.332 (0.332)	Data 0.211 (0.211)	Loss 0.7200 (0.7200) ([0.533]+[0.187])	Prec@1 81.250 (81.250)
Epoch: [62][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.6018 (0.6672) ([0.415]+[0.187])	Prec@1 87.500 (83.547)
Epoch: [62][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.6392 (0.6762) ([0.452]+[0.187])	Prec@1 85.156 (83.314)
Epoch: [62][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6616 (0.6809) ([0.474]+[0.187])	Prec@1 85.938 (83.090)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.8430 (0.8430) ([0.656]+[0.187])	Prec@1 78.125 (78.125)
 * Prec@1 78.520
current lr 1.00000e-01
Grad=  tensor(2.1102, device='cuda:0')
Epoch: [63][0/391]	Time 0.349 (0.349)	Data 0.225 (0.225)	Loss 0.5795 (0.5795) ([0.392]+[0.187])	Prec@1 86.719 (86.719)
Epoch: [63][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.6763 (0.6940) ([0.489]+[0.187])	Prec@1 82.812 (82.472)
Epoch: [63][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7307 (0.6772) ([0.544]+[0.187])	Prec@1 84.375 (82.987)
Epoch: [63][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7254 (0.6770) ([0.539]+[0.186])	Prec@1 81.250 (83.088)
Test: [0/79]	Time 0.249 (0.249)	Loss 1.3263 (1.3263) ([1.140]+[0.186])	Prec@1 64.844 (64.844)
 * Prec@1 66.250
current lr 1.00000e-01
Grad=  tensor(1.6066, device='cuda:0')
Epoch: [64][0/391]	Time 0.335 (0.335)	Data 0.214 (0.214)	Loss 0.5084 (0.5084) ([0.322]+[0.186])	Prec@1 88.281 (88.281)
Epoch: [64][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.6126 (0.6658) ([0.427]+[0.186])	Prec@1 86.719 (83.431)
Epoch: [64][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.6199 (0.6710) ([0.433]+[0.186])	Prec@1 83.594 (83.353)
Epoch: [64][300/391]	Time 0.115 (0.111)	Data 0.000 (0.001)	Loss 0.7752 (0.6782) ([0.589]+[0.187])	Prec@1 76.562 (83.069)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.8123 (0.8123) ([0.627]+[0.186])	Prec@1 82.812 (82.812)
 * Prec@1 75.490
current lr 1.00000e-01
Grad=  tensor(1.7688, device='cuda:0')
Epoch: [65][0/391]	Time 0.346 (0.346)	Data 0.223 (0.223)	Loss 0.5630 (0.5630) ([0.377]+[0.186])	Prec@1 82.031 (82.031)
Epoch: [65][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.5781 (0.6637) ([0.392]+[0.186])	Prec@1 88.281 (83.555)
Epoch: [65][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.5365 (0.6788) ([0.350]+[0.186])	Prec@1 85.938 (83.081)
Epoch: [65][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7587 (0.6739) ([0.573]+[0.186])	Prec@1 75.781 (83.285)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.9016 (0.9016) ([0.716]+[0.185])	Prec@1 79.688 (79.688)
 * Prec@1 78.140
current lr 1.00000e-01
Grad=  tensor(2.7045, device='cuda:0')
Epoch: [66][0/391]	Time 0.347 (0.347)	Data 0.221 (0.221)	Loss 0.8454 (0.8454) ([0.660]+[0.185])	Prec@1 74.219 (74.219)
Epoch: [66][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.6853 (0.6685) ([0.500]+[0.185])	Prec@1 81.250 (83.663)
Epoch: [66][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6177 (0.6713) ([0.432]+[0.185])	Prec@1 85.156 (83.450)
Epoch: [66][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6392 (0.6715) ([0.454]+[0.185])	Prec@1 84.375 (83.409)
Test: [0/79]	Time 0.247 (0.247)	Loss 1.2542 (1.2542) ([1.069]+[0.186])	Prec@1 72.656 (72.656)
 * Prec@1 71.290
current lr 1.00000e-01
Grad=  tensor(2.3313, device='cuda:0')
Epoch: [67][0/391]	Time 0.337 (0.337)	Data 0.215 (0.215)	Loss 0.7451 (0.7451) ([0.559]+[0.186])	Prec@1 78.906 (78.906)
Epoch: [67][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.7304 (0.6592) ([0.545]+[0.185])	Prec@1 80.469 (83.694)
Epoch: [67][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.6235 (0.6738) ([0.438]+[0.186])	Prec@1 84.375 (83.178)
Epoch: [67][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6753 (0.6723) ([0.490]+[0.185])	Prec@1 85.156 (83.145)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.8503 (0.8503) ([0.665]+[0.185])	Prec@1 73.438 (73.438)
 * Prec@1 75.100
current lr 1.00000e-01
Grad=  tensor(0.7743, device='cuda:0')
Epoch: [68][0/391]	Time 0.349 (0.349)	Data 0.223 (0.223)	Loss 0.4164 (0.4164) ([0.232]+[0.185])	Prec@1 96.094 (96.094)
Epoch: [68][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.7148 (0.6614) ([0.530]+[0.185])	Prec@1 82.031 (83.501)
Epoch: [68][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.6937 (0.6659) ([0.509]+[0.185])	Prec@1 84.375 (83.535)
Epoch: [68][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6312 (0.6684) ([0.447]+[0.184])	Prec@1 87.500 (83.511)
Test: [0/79]	Time 0.244 (0.244)	Loss 1.2793 (1.2793) ([1.095]+[0.184])	Prec@1 69.531 (69.531)
 * Prec@1 68.120
current lr 1.00000e-01
Grad=  tensor(2.4976, device='cuda:0')
Epoch: [69][0/391]	Time 0.362 (0.362)	Data 0.235 (0.235)	Loss 0.7551 (0.7551) ([0.571]+[0.184])	Prec@1 78.125 (78.125)
Epoch: [69][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.6943 (0.6593) ([0.510]+[0.184])	Prec@1 83.594 (83.679)
Epoch: [69][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6930 (0.6740) ([0.508]+[0.185])	Prec@1 82.812 (83.139)
Epoch: [69][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6094 (0.6759) ([0.425]+[0.184])	Prec@1 82.031 (83.116)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.9427 (0.9427) ([0.758]+[0.185])	Prec@1 78.125 (78.125)
 * Prec@1 77.150
current lr 1.00000e-01
Grad=  tensor(1.0478, device='cuda:0')
Epoch: [70][0/391]	Time 0.349 (0.349)	Data 0.229 (0.229)	Loss 0.4823 (0.4823) ([0.297]+[0.185])	Prec@1 89.844 (89.844)
Epoch: [70][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.6433 (0.6749) ([0.458]+[0.185])	Prec@1 87.500 (83.315)
Epoch: [70][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.5985 (0.6692) ([0.414]+[0.185])	Prec@1 87.500 (83.450)
Epoch: [70][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.7021 (0.6671) ([0.518]+[0.184])	Prec@1 79.688 (83.524)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.6929 (0.6929) ([0.508]+[0.185])	Prec@1 81.250 (81.250)
 * Prec@1 78.060
current lr 1.00000e-01
Grad=  tensor(2.9935, device='cuda:0')
Epoch: [71][0/391]	Time 0.352 (0.352)	Data 0.229 (0.229)	Loss 0.7531 (0.7531) ([0.568]+[0.185])	Prec@1 79.688 (79.688)
Epoch: [71][100/391]	Time 0.110 (0.114)	Data 0.000 (0.002)	Loss 0.9220 (0.6672) ([0.737]+[0.185])	Prec@1 75.000 (83.679)
Epoch: [71][200/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.7536 (0.6703) ([0.569]+[0.185])	Prec@1 79.688 (83.465)
Epoch: [71][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6644 (0.6676) ([0.480]+[0.185])	Prec@1 82.812 (83.435)
Test: [0/79]	Time 0.233 (0.233)	Loss 1.8465 (1.8465) ([1.662]+[0.184])	Prec@1 61.719 (61.719)
 * Prec@1 60.950
current lr 1.00000e-01
Grad=  tensor(1.6653, device='cuda:0')
Epoch: [72][0/391]	Time 0.348 (0.348)	Data 0.222 (0.222)	Loss 0.5975 (0.5975) ([0.413]+[0.184])	Prec@1 85.156 (85.156)
Epoch: [72][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.7052 (0.6731) ([0.521]+[0.185])	Prec@1 84.375 (83.284)
Epoch: [72][200/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 0.8266 (0.6659) ([0.642]+[0.184])	Prec@1 78.906 (83.617)
Epoch: [72][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7599 (0.6712) ([0.576]+[0.184])	Prec@1 80.469 (83.438)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.8717 (0.8717) ([0.688]+[0.184])	Prec@1 78.906 (78.906)
 * Prec@1 74.650
current lr 1.00000e-01
Grad=  tensor(1.8279, device='cuda:0')
Epoch: [73][0/391]	Time 0.330 (0.330)	Data 0.209 (0.209)	Loss 0.6914 (0.6914) ([0.507]+[0.184])	Prec@1 82.812 (82.812)
Epoch: [73][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.6924 (0.6418) ([0.509]+[0.184])	Prec@1 83.594 (84.514)
Epoch: [73][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6467 (0.6601) ([0.463]+[0.184])	Prec@1 85.156 (83.963)
Epoch: [73][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.6859 (0.6703) ([0.502]+[0.184])	Prec@1 84.375 (83.651)
Test: [0/79]	Time 0.264 (0.264)	Loss 1.1962 (1.1962) ([1.013]+[0.184])	Prec@1 69.531 (69.531)
 * Prec@1 71.000
current lr 1.00000e-01
Grad=  tensor(2.1781, device='cuda:0')
Epoch: [74][0/391]	Time 0.345 (0.345)	Data 0.220 (0.220)	Loss 0.7207 (0.7207) ([0.537]+[0.184])	Prec@1 78.906 (78.906)
Epoch: [74][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.8444 (0.6649) ([0.661]+[0.183])	Prec@1 78.906 (83.385)
Epoch: [74][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.7055 (0.6677) ([0.522]+[0.183])	Prec@1 81.250 (83.225)
Epoch: [74][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.5997 (0.6722) ([0.416]+[0.184])	Prec@1 86.719 (83.134)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.8053 (0.8053) ([0.622]+[0.183])	Prec@1 80.469 (80.469)
 * Prec@1 80.920
current lr 1.00000e-01
Grad=  tensor(2.0737, device='cuda:0')
Epoch: [75][0/391]	Time 0.340 (0.340)	Data 0.219 (0.219)	Loss 0.6830 (0.6830) ([0.500]+[0.183])	Prec@1 83.594 (83.594)
Epoch: [75][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.6993 (0.6420) ([0.517]+[0.182])	Prec@1 84.375 (84.576)
Epoch: [75][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.6111 (0.6529) ([0.429]+[0.183])	Prec@1 88.281 (83.994)
Epoch: [75][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7295 (0.6615) ([0.546]+[0.183])	Prec@1 78.125 (83.674)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.0408 (1.0408) ([0.857]+[0.184])	Prec@1 73.438 (73.438)
 * Prec@1 70.460
current lr 1.00000e-01
Grad=  tensor(2.7201, device='cuda:0')
Epoch: [76][0/391]	Time 0.358 (0.358)	Data 0.236 (0.236)	Loss 0.7200 (0.7200) ([0.536]+[0.184])	Prec@1 79.688 (79.688)
Epoch: [76][100/391]	Time 0.115 (0.114)	Data 0.000 (0.002)	Loss 0.7054 (0.6645) ([0.522]+[0.183])	Prec@1 85.156 (83.447)
Epoch: [76][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6666 (0.6645) ([0.483]+[0.183])	Prec@1 82.031 (83.547)
Epoch: [76][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.6900 (0.6678) ([0.507]+[0.183])	Prec@1 78.125 (83.480)
Test: [0/79]	Time 0.251 (0.251)	Loss 1.3538 (1.3538) ([1.171]+[0.183])	Prec@1 66.406 (66.406)
 * Prec@1 62.980
current lr 1.00000e-01
Grad=  tensor(1.6356, device='cuda:0')
Epoch: [77][0/391]	Time 0.305 (0.305)	Data 0.176 (0.176)	Loss 0.5750 (0.5750) ([0.392]+[0.183])	Prec@1 84.375 (84.375)
Epoch: [77][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.6209 (0.6548) ([0.438]+[0.183])	Prec@1 85.938 (83.803)
Epoch: [77][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.6679 (0.6607) ([0.485]+[0.183])	Prec@1 85.938 (83.640)
Epoch: [77][300/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.5746 (0.6646) ([0.392]+[0.183])	Prec@1 84.375 (83.435)
Test: [0/79]	Time 0.258 (0.258)	Loss 1.0294 (1.0294) ([0.847]+[0.183])	Prec@1 79.688 (79.688)
 * Prec@1 73.630
current lr 1.00000e-01
Grad=  tensor(1.5992, device='cuda:0')
Epoch: [78][0/391]	Time 0.333 (0.333)	Data 0.209 (0.209)	Loss 0.6218 (0.6218) ([0.439]+[0.183])	Prec@1 84.375 (84.375)
Epoch: [78][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.6884 (0.6812) ([0.505]+[0.183])	Prec@1 81.250 (82.929)
Epoch: [78][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7398 (0.6720) ([0.557]+[0.183])	Prec@1 85.156 (83.524)
Epoch: [78][300/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6310 (0.6708) ([0.449]+[0.182])	Prec@1 81.250 (83.495)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.9440 (0.9440) ([0.762]+[0.182])	Prec@1 78.125 (78.125)
 * Prec@1 78.330
current lr 1.00000e-01
Grad=  tensor(2.7063, device='cuda:0')
Epoch: [79][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.8013 (0.8013) ([0.619]+[0.182])	Prec@1 78.125 (78.125)
Epoch: [79][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.5884 (0.6448) ([0.407]+[0.182])	Prec@1 87.500 (84.151)
Epoch: [79][200/391]	Time 0.108 (0.113)	Data 0.000 (0.001)	Loss 0.7593 (0.6616) ([0.577]+[0.182])	Prec@1 77.344 (83.586)
Epoch: [79][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.6490 (0.6613) ([0.467]+[0.182])	Prec@1 85.156 (83.570)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.8311 (0.8311) ([0.649]+[0.182])	Prec@1 78.906 (78.906)
 * Prec@1 76.950
current lr 1.00000e-01
Grad=  tensor(1.4890, device='cuda:0')
Epoch: [80][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.6055 (0.6055) ([0.424]+[0.182])	Prec@1 87.500 (87.500)
Epoch: [80][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.6961 (0.6494) ([0.515]+[0.181])	Prec@1 82.031 (84.205)
Epoch: [80][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.8390 (0.6560) ([0.658]+[0.181])	Prec@1 78.906 (83.858)
Epoch: [80][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 1.0169 (0.6583) ([0.836]+[0.181])	Prec@1 74.219 (83.716)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.7311 (0.7311) ([0.550]+[0.181])	Prec@1 79.688 (79.688)
 * Prec@1 75.840
current lr 1.00000e-01
Grad=  tensor(1.7040, device='cuda:0')
Epoch: [81][0/391]	Time 0.348 (0.348)	Data 0.229 (0.229)	Loss 0.6041 (0.6041) ([0.423]+[0.181])	Prec@1 80.469 (80.469)
Epoch: [81][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.7256 (0.6352) ([0.546]+[0.180])	Prec@1 84.375 (84.669)
Epoch: [81][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6266 (0.6532) ([0.446]+[0.181])	Prec@1 85.938 (83.932)
Epoch: [81][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7789 (0.6580) ([0.598]+[0.181])	Prec@1 78.125 (83.737)
Test: [0/79]	Time 0.250 (0.250)	Loss 1.1406 (1.1406) ([0.959]+[0.181])	Prec@1 74.219 (74.219)
 * Prec@1 71.390
current lr 1.00000e-01
Grad=  tensor(1.3986, device='cuda:0')
Epoch: [82][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.5261 (0.5261) ([0.345]+[0.181])	Prec@1 88.281 (88.281)
Epoch: [82][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.5338 (0.6511) ([0.353]+[0.181])	Prec@1 85.156 (84.073)
Epoch: [82][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.8204 (0.6520) ([0.639]+[0.181])	Prec@1 76.562 (83.839)
Epoch: [82][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7430 (0.6611) ([0.562]+[0.181])	Prec@1 80.469 (83.576)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.8116 (0.8116) ([0.630]+[0.181])	Prec@1 77.344 (77.344)
 * Prec@1 79.430
current lr 1.00000e-01
Grad=  tensor(1.5286, device='cuda:0')
Epoch: [83][0/391]	Time 0.300 (0.300)	Data 0.179 (0.179)	Loss 0.5307 (0.5307) ([0.349]+[0.181])	Prec@1 84.375 (84.375)
Epoch: [83][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6701 (0.6554) ([0.489]+[0.182])	Prec@1 82.812 (83.710)
Epoch: [83][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7989 (0.6610) ([0.617]+[0.182])	Prec@1 78.125 (83.594)
Epoch: [83][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5380 (0.6599) ([0.356]+[0.182])	Prec@1 86.719 (83.594)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.9875 (0.9875) ([0.806]+[0.182])	Prec@1 76.562 (76.562)
 * Prec@1 75.370
current lr 1.00000e-01
Grad=  tensor(2.5922, device='cuda:0')
Epoch: [84][0/391]	Time 0.351 (0.351)	Data 0.226 (0.226)	Loss 0.6908 (0.6908) ([0.509]+[0.182])	Prec@1 82.031 (82.031)
Epoch: [84][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.5128 (0.6427) ([0.332]+[0.181])	Prec@1 88.281 (84.027)
Epoch: [84][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.7100 (0.6521) ([0.529]+[0.181])	Prec@1 81.250 (83.784)
Epoch: [84][300/391]	Time 0.111 (0.115)	Data 0.000 (0.001)	Loss 0.5243 (0.6572) ([0.343]+[0.181])	Prec@1 90.625 (83.583)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.8254 (0.8254) ([0.644]+[0.181])	Prec@1 78.125 (78.125)
 * Prec@1 80.520
current lr 1.00000e-01
Grad=  tensor(1.5362, device='cuda:0')
Epoch: [85][0/391]	Time 0.336 (0.336)	Data 0.215 (0.215)	Loss 0.6469 (0.6469) ([0.466]+[0.181])	Prec@1 82.812 (82.812)
Epoch: [85][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6583 (0.6553) ([0.478]+[0.180])	Prec@1 82.812 (83.671)
Epoch: [85][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7134 (0.6517) ([0.534]+[0.180])	Prec@1 81.250 (84.041)
Epoch: [85][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5455 (0.6525) ([0.366]+[0.179])	Prec@1 86.719 (83.905)
Test: [0/79]	Time 0.246 (0.246)	Loss 1.3877 (1.3877) ([1.207]+[0.180])	Prec@1 67.969 (67.969)
 * Prec@1 68.680
current lr 1.00000e-01
Grad=  tensor(1.9628, device='cuda:0')
Epoch: [86][0/391]	Time 0.351 (0.351)	Data 0.225 (0.225)	Loss 0.6231 (0.6231) ([0.443]+[0.180])	Prec@1 84.375 (84.375)
Epoch: [86][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.5808 (0.6459) ([0.400]+[0.180])	Prec@1 85.938 (84.104)
Epoch: [86][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.7432 (0.6658) ([0.562]+[0.181])	Prec@1 82.812 (83.302)
Epoch: [86][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.5900 (0.6597) ([0.410]+[0.180])	Prec@1 85.938 (83.557)
Test: [0/79]	Time 0.249 (0.249)	Loss 1.0663 (1.0663) ([0.887]+[0.179])	Prec@1 71.094 (71.094)
 * Prec@1 68.250
current lr 1.00000e-01
Grad=  tensor(1.4566, device='cuda:0')
Epoch: [87][0/391]	Time 0.339 (0.339)	Data 0.218 (0.218)	Loss 0.6094 (0.6094) ([0.430]+[0.179])	Prec@1 86.719 (86.719)
Epoch: [87][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6171 (0.6603) ([0.437]+[0.180])	Prec@1 86.719 (83.586)
Epoch: [87][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7701 (0.6662) ([0.591]+[0.179])	Prec@1 78.125 (83.403)
Epoch: [87][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6235 (0.6621) ([0.444]+[0.179])	Prec@1 82.031 (83.518)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.8903 (0.8903) ([0.711]+[0.180])	Prec@1 76.562 (76.562)
 * Prec@1 74.460
current lr 1.00000e-01
Grad=  tensor(1.3629, device='cuda:0')
Epoch: [88][0/391]	Time 0.330 (0.330)	Data 0.209 (0.209)	Loss 0.4988 (0.4988) ([0.319]+[0.180])	Prec@1 87.500 (87.500)
Epoch: [88][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.5606 (0.6544) ([0.381]+[0.180])	Prec@1 86.719 (83.679)
Epoch: [88][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.6742 (0.6528) ([0.494]+[0.180])	Prec@1 80.469 (83.916)
Epoch: [88][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6875 (0.6550) ([0.507]+[0.180])	Prec@1 82.812 (83.801)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.8851 (0.8851) ([0.705]+[0.180])	Prec@1 75.781 (75.781)
 * Prec@1 73.990
current lr 1.00000e-01
Grad=  tensor(2.2666, device='cuda:0')
Epoch: [89][0/391]	Time 0.339 (0.339)	Data 0.215 (0.215)	Loss 0.7143 (0.7143) ([0.534]+[0.180])	Prec@1 82.031 (82.031)
Epoch: [89][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.6423 (0.6741) ([0.462]+[0.181])	Prec@1 86.719 (83.269)
Epoch: [89][200/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.7209 (0.6642) ([0.541]+[0.180])	Prec@1 82.812 (83.574)
Epoch: [89][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.6025 (0.6613) ([0.422]+[0.180])	Prec@1 85.156 (83.617)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.6788 (0.6788) ([0.498]+[0.180])	Prec@1 82.031 (82.031)
 * Prec@1 76.530
current lr 1.00000e-01
Grad=  tensor(1.2987, device='cuda:0')
Epoch: [90][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.5672 (0.5672) ([0.387]+[0.180])	Prec@1 85.156 (85.156)
Epoch: [90][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.6056 (0.6492) ([0.425]+[0.180])	Prec@1 83.594 (83.857)
Epoch: [90][200/391]	Time 0.108 (0.113)	Data 0.000 (0.001)	Loss 0.7237 (0.6484) ([0.544]+[0.180])	Prec@1 80.469 (83.788)
Epoch: [90][300/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.7690 (0.6553) ([0.589]+[0.180])	Prec@1 80.469 (83.609)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.9262 (0.9262) ([0.746]+[0.180])	Prec@1 75.781 (75.781)
 * Prec@1 76.010
current lr 1.00000e-01
Grad=  tensor(2.5098, device='cuda:0')
Epoch: [91][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.7493 (0.7493) ([0.569]+[0.180])	Prec@1 77.344 (77.344)
Epoch: [91][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.6687 (0.6465) ([0.489]+[0.180])	Prec@1 85.938 (83.880)
Epoch: [91][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.6652 (0.6588) ([0.485]+[0.180])	Prec@1 83.594 (83.621)
Epoch: [91][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.5984 (0.6542) ([0.419]+[0.180])	Prec@1 86.719 (83.770)
Test: [0/79]	Time 0.244 (0.244)	Loss 1.2521 (1.2521) ([1.073]+[0.180])	Prec@1 65.625 (65.625)
 * Prec@1 67.400
current lr 1.00000e-01
Grad=  tensor(1.8442, device='cuda:0')
Epoch: [92][0/391]	Time 0.332 (0.332)	Data 0.214 (0.214)	Loss 0.6147 (0.6147) ([0.435]+[0.180])	Prec@1 85.938 (85.938)
Epoch: [92][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.5773 (0.6424) ([0.398]+[0.180])	Prec@1 85.156 (84.112)
Epoch: [92][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.5708 (0.6469) ([0.391]+[0.180])	Prec@1 86.719 (84.072)
Epoch: [92][300/391]	Time 0.107 (0.109)	Data 0.000 (0.001)	Loss 0.5839 (0.6544) ([0.404]+[0.180])	Prec@1 85.938 (83.729)
Test: [0/79]	Time 0.238 (0.238)	Loss 1.9490 (1.9490) ([1.770]+[0.179])	Prec@1 56.250 (56.250)
 * Prec@1 57.010
current lr 1.00000e-01
Grad=  tensor(2.0586, device='cuda:0')
Epoch: [93][0/391]	Time 0.313 (0.313)	Data 0.194 (0.194)	Loss 0.6978 (0.6978) ([0.519]+[0.179])	Prec@1 82.031 (82.031)
Epoch: [93][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.6855 (0.6536) ([0.506]+[0.179])	Prec@1 83.594 (83.857)
Epoch: [93][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6155 (0.6569) ([0.436]+[0.180])	Prec@1 82.812 (83.749)
Epoch: [93][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.8123 (0.6575) ([0.633]+[0.179])	Prec@1 80.469 (83.742)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.6254 (0.6254) ([0.446]+[0.179])	Prec@1 82.031 (82.031)
 * Prec@1 80.400
current lr 1.00000e-01
Grad=  tensor(2.4343, device='cuda:0')
Epoch: [94][0/391]	Time 0.344 (0.344)	Data 0.222 (0.222)	Loss 0.7011 (0.7011) ([0.522]+[0.179])	Prec@1 78.906 (78.906)
Epoch: [94][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.6703 (0.6526) ([0.491]+[0.179])	Prec@1 84.375 (83.903)
Epoch: [94][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.5928 (0.6560) ([0.414]+[0.179])	Prec@1 84.375 (83.675)
Epoch: [94][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.7349 (0.6543) ([0.556]+[0.179])	Prec@1 81.250 (83.788)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.7673 (0.7673) ([0.589]+[0.178])	Prec@1 78.906 (78.906)
 * Prec@1 78.050
current lr 1.00000e-01
Grad=  tensor(1.8148, device='cuda:0')
Epoch: [95][0/391]	Time 0.315 (0.315)	Data 0.196 (0.196)	Loss 0.5930 (0.5930) ([0.415]+[0.178])	Prec@1 87.500 (87.500)
Epoch: [95][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.5581 (0.6457) ([0.380]+[0.178])	Prec@1 85.938 (83.996)
Epoch: [95][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6707 (0.6438) ([0.493]+[0.178])	Prec@1 83.594 (84.200)
Epoch: [95][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.6379 (0.6474) ([0.460]+[0.178])	Prec@1 85.156 (83.942)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.7899 (0.7899) ([0.612]+[0.178])	Prec@1 78.906 (78.906)
 * Prec@1 78.900
current lr 1.00000e-01
Grad=  tensor(1.8260, device='cuda:0')
Epoch: [96][0/391]	Time 0.311 (0.311)	Data 0.192 (0.192)	Loss 0.6064 (0.6064) ([0.429]+[0.178])	Prec@1 83.594 (83.594)
Epoch: [96][100/391]	Time 0.111 (0.111)	Data 0.000 (0.002)	Loss 0.6176 (0.6495) ([0.440]+[0.178])	Prec@1 85.938 (84.213)
Epoch: [96][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.6664 (0.6522) ([0.488]+[0.178])	Prec@1 81.250 (84.033)
Epoch: [96][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.7144 (0.6512) ([0.537]+[0.177])	Prec@1 78.125 (83.939)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.9578 (0.9578) ([0.780]+[0.178])	Prec@1 75.000 (75.000)
 * Prec@1 75.240
current lr 1.00000e-01
Grad=  tensor(2.0325, device='cuda:0')
Epoch: [97][0/391]	Time 0.305 (0.305)	Data 0.184 (0.184)	Loss 0.6930 (0.6930) ([0.515]+[0.178])	Prec@1 82.812 (82.812)
Epoch: [97][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.6541 (0.6649) ([0.476]+[0.178])	Prec@1 85.156 (83.493)
Epoch: [97][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6900 (0.6525) ([0.512]+[0.178])	Prec@1 79.688 (83.936)
Epoch: [97][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5594 (0.6561) ([0.381]+[0.178])	Prec@1 84.375 (83.625)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.8069 (0.8069) ([0.629]+[0.178])	Prec@1 77.344 (77.344)
 * Prec@1 75.520
current lr 1.00000e-01
Grad=  tensor(1.9096, device='cuda:0')
Epoch: [98][0/391]	Time 0.307 (0.307)	Data 0.186 (0.186)	Loss 0.5801 (0.5801) ([0.402]+[0.178])	Prec@1 83.594 (83.594)
Epoch: [98][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.6491 (0.6544) ([0.471]+[0.178])	Prec@1 86.719 (83.710)
Epoch: [98][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.8434 (0.6554) ([0.666]+[0.178])	Prec@1 78.125 (83.594)
Epoch: [98][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.6685 (0.6494) ([0.491]+[0.177])	Prec@1 78.906 (83.801)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.9779 (0.9779) ([0.800]+[0.178])	Prec@1 70.312 (70.312)
 * Prec@1 74.990
current lr 1.00000e-01
Grad=  tensor(1.5693, device='cuda:0')
Epoch: [99][0/391]	Time 0.322 (0.322)	Data 0.203 (0.203)	Loss 0.5977 (0.5977) ([0.420]+[0.178])	Prec@1 87.500 (87.500)
Epoch: [99][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.6981 (0.6624) ([0.520]+[0.178])	Prec@1 80.469 (83.455)
Epoch: [99][200/391]	Time 0.114 (0.111)	Data 0.000 (0.001)	Loss 0.5228 (0.6547) ([0.345]+[0.178])	Prec@1 89.062 (83.617)
Epoch: [99][300/391]	Time 0.112 (0.111)	Data 0.000 (0.001)	Loss 0.7313 (0.6512) ([0.554]+[0.177])	Prec@1 81.250 (83.638)
Test: [0/79]	Time 0.250 (0.250)	Loss 1.3044 (1.3044) ([1.127]+[0.177])	Prec@1 68.750 (68.750)
 * Prec@1 68.350
current lr 1.00000e-02
Grad=  tensor(2.2504, device='cuda:0')
Epoch: [100][0/391]	Time 0.301 (0.301)	Data 0.179 (0.179)	Loss 0.7030 (0.7030) ([0.526]+[0.177])	Prec@1 85.156 (85.156)
Epoch: [100][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.4185 (0.5144) ([0.249]+[0.170])	Prec@1 91.406 (88.150)
Epoch: [100][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3941 (0.4855) ([0.226]+[0.168])	Prec@1 93.750 (89.370)
Epoch: [100][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4198 (0.4721) ([0.253]+[0.167])	Prec@1 89.062 (89.730)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4074 (0.4074) ([0.241]+[0.166])	Prec@1 92.969 (92.969)
 * Prec@1 90.400
current lr 1.00000e-02
Grad=  tensor(1.2102, device='cuda:0')
Epoch: [101][0/391]	Time 0.350 (0.350)	Data 0.227 (0.227)	Loss 0.3941 (0.3941) ([0.228]+[0.166])	Prec@1 92.188 (92.188)
Epoch: [101][100/391]	Time 0.109 (0.114)	Data 0.000 (0.002)	Loss 0.4892 (0.3969) ([0.324]+[0.165])	Prec@1 89.844 (92.466)
Epoch: [101][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.4419 (0.3968) ([0.278]+[0.164])	Prec@1 90.625 (92.222)
Epoch: [101][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.4978 (0.3985) ([0.335]+[0.163])	Prec@1 89.062 (92.138)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4167 (0.4167) ([0.255]+[0.162])	Prec@1 92.188 (92.188)
 * Prec@1 91.100
current lr 1.00000e-02
Grad=  tensor(1.4107, device='cuda:0')
Epoch: [102][0/391]	Time 0.357 (0.357)	Data 0.232 (0.232)	Loss 0.3401 (0.3401) ([0.178]+[0.162])	Prec@1 94.531 (94.531)
Epoch: [102][100/391]	Time 0.109 (0.114)	Data 0.000 (0.002)	Loss 0.3648 (0.3606) ([0.204]+[0.161])	Prec@1 92.188 (93.278)
Epoch: [102][200/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.3819 (0.3678) ([0.222]+[0.160])	Prec@1 90.625 (93.027)
Epoch: [102][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3204 (0.3720) ([0.162]+[0.159])	Prec@1 92.969 (92.852)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3502 (0.3502) ([0.192]+[0.158])	Prec@1 92.969 (92.969)
 * Prec@1 91.410
current lr 1.00000e-02
Grad=  tensor(1.0990, device='cuda:0')
Epoch: [103][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.3281 (0.3281) ([0.170]+[0.158])	Prec@1 94.531 (94.531)
Epoch: [103][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.3923 (0.3402) ([0.236]+[0.157])	Prec@1 92.188 (93.804)
Epoch: [103][200/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.3702 (0.3487) ([0.214]+[0.156])	Prec@1 92.188 (93.470)
Epoch: [103][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.3998 (0.3520) ([0.245]+[0.155])	Prec@1 91.406 (93.296)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3801 (0.3801) ([0.226]+[0.154])	Prec@1 93.750 (93.750)
 * Prec@1 91.560
current lr 1.00000e-02
Grad=  tensor(2.0471, device='cuda:0')
Epoch: [104][0/391]	Time 0.336 (0.336)	Data 0.218 (0.218)	Loss 0.4149 (0.4149) ([0.261]+[0.154])	Prec@1 89.844 (89.844)
Epoch: [104][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2750 (0.3316) ([0.122]+[0.153])	Prec@1 96.875 (93.889)
Epoch: [104][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3460 (0.3334) ([0.194]+[0.152])	Prec@1 91.406 (93.742)
Epoch: [104][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3625 (0.3373) ([0.211]+[0.151])	Prec@1 91.406 (93.636)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.3855 (0.3855) ([0.235]+[0.151])	Prec@1 93.750 (93.750)
 * Prec@1 91.630
current lr 1.00000e-02
Grad=  tensor(1.7818, device='cuda:0')
Epoch: [105][0/391]	Time 0.316 (0.316)	Data 0.196 (0.196)	Loss 0.3025 (0.3025) ([0.152]+[0.151])	Prec@1 94.531 (94.531)
Epoch: [105][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.4287 (0.3231) ([0.279]+[0.150])	Prec@1 88.281 (94.237)
Epoch: [105][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2775 (0.3244) ([0.129]+[0.149])	Prec@1 95.312 (94.065)
Epoch: [105][300/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.2770 (0.3252) ([0.129]+[0.148])	Prec@1 96.094 (93.939)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.3394 (0.3394) ([0.192]+[0.147])	Prec@1 95.312 (95.312)
 * Prec@1 91.620
current lr 1.00000e-02
Grad=  tensor(1.1354, device='cuda:0')
Epoch: [106][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.2491 (0.2491) ([0.102]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [106][100/391]	Time 0.109 (0.114)	Data 0.000 (0.002)	Loss 0.2879 (0.3056) ([0.141]+[0.147])	Prec@1 94.531 (94.392)
Epoch: [106][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2912 (0.3094) ([0.145]+[0.146])	Prec@1 95.312 (94.263)
Epoch: [106][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3288 (0.3118) ([0.184]+[0.145])	Prec@1 92.188 (94.256)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3505 (0.3505) ([0.206]+[0.144])	Prec@1 94.531 (94.531)
 * Prec@1 91.880
current lr 1.00000e-02
Grad=  tensor(1.2759, device='cuda:0')
Epoch: [107][0/391]	Time 0.335 (0.335)	Data 0.216 (0.216)	Loss 0.2388 (0.2388) ([0.095]+[0.144])	Prec@1 96.875 (96.875)
Epoch: [107][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2576 (0.2951) ([0.114]+[0.144])	Prec@1 95.312 (94.609)
Epoch: [107][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3175 (0.2975) ([0.175]+[0.143])	Prec@1 92.969 (94.535)
Epoch: [107][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3340 (0.2981) ([0.192]+[0.142])	Prec@1 92.969 (94.536)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4129 (0.4129) ([0.271]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 91.010
current lr 1.00000e-02
Grad=  tensor(2.4119, device='cuda:0')
Epoch: [108][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.2757 (0.2757) ([0.134]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [108][100/391]	Time 0.108 (0.112)	Data 0.000 (0.002)	Loss 0.2585 (0.2934) ([0.118]+[0.141])	Prec@1 95.312 (94.895)
Epoch: [108][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.3174 (0.2905) ([0.177]+[0.140])	Prec@1 93.750 (94.877)
Epoch: [108][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3247 (0.2939) ([0.185]+[0.139])	Prec@1 93.750 (94.705)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4773 (0.4773) ([0.338]+[0.139])	Prec@1 89.844 (89.844)
 * Prec@1 90.870
current lr 1.00000e-02
Grad=  tensor(3.1464, device='cuda:0')
Epoch: [109][0/391]	Time 0.332 (0.332)	Data 0.213 (0.213)	Loss 0.2634 (0.2634) ([0.125]+[0.139])	Prec@1 94.531 (94.531)
Epoch: [109][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.3120 (0.2862) ([0.174]+[0.138])	Prec@1 94.531 (94.856)
Epoch: [109][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3376 (0.2882) ([0.200]+[0.138])	Prec@1 92.969 (94.823)
Epoch: [109][300/391]	Time 0.113 (0.111)	Data 0.000 (0.001)	Loss 0.2877 (0.2874) ([0.151]+[0.137])	Prec@1 93.750 (94.869)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3228 (0.3228) ([0.186]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 91.610
current lr 1.00000e-02
Grad=  tensor(2.8218, device='cuda:0')
Epoch: [110][0/391]	Time 0.330 (0.330)	Data 0.206 (0.206)	Loss 0.2953 (0.2953) ([0.159]+[0.136])	Prec@1 92.969 (92.969)
Epoch: [110][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2606 (0.2806) ([0.125]+[0.136])	Prec@1 94.531 (94.957)
Epoch: [110][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2481 (0.2761) ([0.113]+[0.135])	Prec@1 96.094 (95.072)
Epoch: [110][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2570 (0.2780) ([0.122]+[0.135])	Prec@1 95.312 (95.022)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3758 (0.3758) ([0.242]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 91.340
current lr 1.00000e-02
Grad=  tensor(3.2618, device='cuda:0')
Epoch: [111][0/391]	Time 0.341 (0.341)	Data 0.218 (0.218)	Loss 0.2913 (0.2913) ([0.157]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [111][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2723 (0.2642) ([0.139]+[0.134])	Prec@1 94.531 (95.606)
Epoch: [111][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.3425 (0.2704) ([0.210]+[0.133])	Prec@1 91.406 (95.336)
Epoch: [111][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2852 (0.2741) ([0.153]+[0.133])	Prec@1 93.750 (95.152)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.3449 (0.3449) ([0.213]+[0.132])	Prec@1 93.750 (93.750)
 * Prec@1 91.370
current lr 1.00000e-02
Grad=  tensor(1.7470, device='cuda:0')
Epoch: [112][0/391]	Time 0.320 (0.320)	Data 0.200 (0.200)	Loss 0.2461 (0.2461) ([0.114]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [112][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.2456 (0.2628) ([0.114]+[0.132])	Prec@1 96.875 (95.568)
Epoch: [112][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2642 (0.2651) ([0.133]+[0.131])	Prec@1 95.312 (95.460)
Epoch: [112][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.3470 (0.2706) ([0.216]+[0.131])	Prec@1 95.312 (95.245)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.3119 (0.3119) ([0.182]+[0.130])	Prec@1 93.750 (93.750)
 * Prec@1 91.470
current lr 1.00000e-02
Grad=  tensor(2.9847, device='cuda:0')
Epoch: [113][0/391]	Time 0.329 (0.329)	Data 0.208 (0.208)	Loss 0.2636 (0.2636) ([0.133]+[0.130])	Prec@1 96.094 (96.094)
Epoch: [113][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2642 (0.2574) ([0.135]+[0.130])	Prec@1 96.094 (95.699)
Epoch: [113][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2810 (0.2614) ([0.152]+[0.129])	Prec@1 95.312 (95.658)
Epoch: [113][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.3100 (0.2633) ([0.181]+[0.129])	Prec@1 94.531 (95.450)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.3699 (0.3699) ([0.241]+[0.128])	Prec@1 92.188 (92.188)
 * Prec@1 91.430
current lr 1.00000e-02
Grad=  tensor(2.6031, device='cuda:0')
Epoch: [114][0/391]	Time 0.335 (0.335)	Data 0.210 (0.210)	Loss 0.2292 (0.2292) ([0.101]+[0.128])	Prec@1 96.094 (96.094)
Epoch: [114][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2477 (0.2561) ([0.120]+[0.128])	Prec@1 94.531 (95.560)
Epoch: [114][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2958 (0.2583) ([0.168]+[0.128])	Prec@1 92.188 (95.468)
Epoch: [114][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2297 (0.2626) ([0.102]+[0.127])	Prec@1 96.875 (95.305)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.3576 (0.3576) ([0.231]+[0.127])	Prec@1 94.531 (94.531)
 * Prec@1 91.110
current lr 1.00000e-02
Grad=  tensor(4.8626, device='cuda:0')
Epoch: [115][0/391]	Time 0.334 (0.334)	Data 0.209 (0.209)	Loss 0.2575 (0.2575) ([0.131]+[0.127])	Prec@1 95.312 (95.312)
Epoch: [115][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.3004 (0.2459) ([0.174]+[0.126])	Prec@1 94.531 (95.862)
Epoch: [115][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2939 (0.2531) ([0.168]+[0.126])	Prec@1 93.750 (95.588)
Epoch: [115][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2995 (0.2558) ([0.174]+[0.126])	Prec@1 92.969 (95.481)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.4845 (0.4845) ([0.359]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 90.800
current lr 1.00000e-02
Grad=  tensor(3.6724, device='cuda:0')
Epoch: [116][0/391]	Time 0.308 (0.308)	Data 0.188 (0.188)	Loss 0.2416 (0.2416) ([0.116]+[0.125])	Prec@1 95.312 (95.312)
Epoch: [116][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3243 (0.2502) ([0.199]+[0.125])	Prec@1 95.312 (95.823)
Epoch: [116][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3147 (0.2531) ([0.190]+[0.125])	Prec@1 90.625 (95.612)
Epoch: [116][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2074 (0.2563) ([0.083]+[0.124])	Prec@1 97.656 (95.450)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.3538 (0.3538) ([0.230]+[0.124])	Prec@1 92.969 (92.969)
 * Prec@1 90.700
current lr 1.00000e-02
Grad=  tensor(4.2514, device='cuda:0')
Epoch: [117][0/391]	Time 0.349 (0.349)	Data 0.228 (0.228)	Loss 0.2426 (0.2426) ([0.119]+[0.124])	Prec@1 96.875 (96.875)
Epoch: [117][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2153 (0.2507) ([0.092]+[0.124])	Prec@1 96.875 (95.606)
Epoch: [117][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3147 (0.2541) ([0.191]+[0.123])	Prec@1 92.188 (95.553)
Epoch: [117][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2221 (0.2544) ([0.099]+[0.123])	Prec@1 96.094 (95.533)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.3076 (0.3076) ([0.185]+[0.123])	Prec@1 94.531 (94.531)
 * Prec@1 91.230
current lr 1.00000e-02
Grad=  tensor(2.9751, device='cuda:0')
Epoch: [118][0/391]	Time 0.304 (0.304)	Data 0.186 (0.186)	Loss 0.2211 (0.2211) ([0.098]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [118][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2816 (0.2466) ([0.159]+[0.123])	Prec@1 95.312 (95.715)
Epoch: [118][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2723 (0.2500) ([0.150]+[0.122])	Prec@1 94.531 (95.534)
Epoch: [118][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2390 (0.2533) ([0.117]+[0.122])	Prec@1 94.531 (95.440)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.3061 (0.3061) ([0.184]+[0.122])	Prec@1 95.312 (95.312)
 * Prec@1 91.180
current lr 1.00000e-02
Grad=  tensor(4.8449, device='cuda:0')
Epoch: [119][0/391]	Time 0.325 (0.325)	Data 0.206 (0.206)	Loss 0.2336 (0.2336) ([0.112]+[0.122])	Prec@1 96.094 (96.094)
Epoch: [119][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2873 (0.2424) ([0.166]+[0.121])	Prec@1 94.531 (95.877)
Epoch: [119][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2694 (0.2528) ([0.148]+[0.121])	Prec@1 96.094 (95.573)
Epoch: [119][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2649 (0.2541) ([0.144]+[0.121])	Prec@1 95.312 (95.494)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3181 (0.3181) ([0.197]+[0.121])	Prec@1 95.312 (95.312)
 * Prec@1 90.490
current lr 1.00000e-02
Grad=  tensor(2.7035, device='cuda:0')
Epoch: [120][0/391]	Time 0.334 (0.334)	Data 0.214 (0.214)	Loss 0.2066 (0.2066) ([0.086]+[0.121])	Prec@1 97.656 (97.656)
Epoch: [120][100/391]	Time 0.111 (0.111)	Data 0.000 (0.002)	Loss 0.2236 (0.2581) ([0.103]+[0.121])	Prec@1 96.094 (95.436)
Epoch: [120][200/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.2504 (0.2563) ([0.130]+[0.120])	Prec@1 96.094 (95.449)
Epoch: [120][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.2490 (0.2556) ([0.129]+[0.120])	Prec@1 96.094 (95.362)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3466 (0.3466) ([0.227]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 91.130
current lr 1.00000e-02
Grad=  tensor(3.8613, device='cuda:0')
Epoch: [121][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.2103 (0.2103) ([0.090]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [121][100/391]	Time 0.110 (0.114)	Data 0.000 (0.002)	Loss 0.2811 (0.2424) ([0.161]+[0.120])	Prec@1 95.312 (95.722)
Epoch: [121][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2135 (0.2448) ([0.094]+[0.120])	Prec@1 96.094 (95.573)
Epoch: [121][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2587 (0.2485) ([0.139]+[0.119])	Prec@1 96.875 (95.481)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3675 (0.3675) ([0.248]+[0.119])	Prec@1 90.625 (90.625)
 * Prec@1 91.140
current lr 1.00000e-02
Grad=  tensor(5.3586, device='cuda:0')
Epoch: [122][0/391]	Time 0.335 (0.335)	Data 0.216 (0.216)	Loss 0.2191 (0.2191) ([0.100]+[0.119])	Prec@1 96.094 (96.094)
Epoch: [122][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.3422 (0.2446) ([0.223]+[0.119])	Prec@1 90.625 (95.730)
Epoch: [122][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2090 (0.2475) ([0.090]+[0.119])	Prec@1 97.656 (95.581)
Epoch: [122][300/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.2855 (0.2474) ([0.167]+[0.119])	Prec@1 92.969 (95.632)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.4022 (0.4022) ([0.284]+[0.119])	Prec@1 91.406 (91.406)
 * Prec@1 90.550
current lr 1.00000e-02
Grad=  tensor(2.9734, device='cuda:0')
Epoch: [123][0/391]	Time 0.320 (0.320)	Data 0.199 (0.199)	Loss 0.2051 (0.2051) ([0.087]+[0.119])	Prec@1 97.656 (97.656)
Epoch: [123][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.1870 (0.2369) ([0.069]+[0.118])	Prec@1 97.656 (95.831)
Epoch: [123][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2576 (0.2431) ([0.139]+[0.118])	Prec@1 94.531 (95.588)
Epoch: [123][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2868 (0.2476) ([0.169]+[0.118])	Prec@1 96.094 (95.460)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3705 (0.3705) ([0.253]+[0.118])	Prec@1 91.406 (91.406)
 * Prec@1 90.740
current lr 1.00000e-02
Grad=  tensor(3.6150, device='cuda:0')
Epoch: [124][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.2005 (0.2005) ([0.082]+[0.118])	Prec@1 96.875 (96.875)
Epoch: [124][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.2458 (0.2454) ([0.128]+[0.118])	Prec@1 94.531 (95.312)
Epoch: [124][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2232 (0.2453) ([0.105]+[0.118])	Prec@1 95.312 (95.487)
Epoch: [124][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2917 (0.2519) ([0.174]+[0.118])	Prec@1 93.750 (95.287)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3506 (0.3506) ([0.233]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(4.6740, device='cuda:0')
Epoch: [125][0/391]	Time 0.340 (0.340)	Data 0.218 (0.218)	Loss 0.2037 (0.2037) ([0.086]+[0.118])	Prec@1 96.875 (96.875)
Epoch: [125][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2839 (0.2423) ([0.166]+[0.117])	Prec@1 94.531 (95.630)
Epoch: [125][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2514 (0.2488) ([0.134]+[0.117])	Prec@1 96.875 (95.305)
Epoch: [125][300/391]	Time 0.115 (0.111)	Data 0.001 (0.001)	Loss 0.2306 (0.2492) ([0.113]+[0.117])	Prec@1 94.531 (95.336)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4346 (0.4346) ([0.317]+[0.117])	Prec@1 88.281 (88.281)
 * Prec@1 89.610
current lr 1.00000e-02
Grad=  tensor(3.7513, device='cuda:0')
Epoch: [126][0/391]	Time 0.350 (0.350)	Data 0.226 (0.226)	Loss 0.2143 (0.2143) ([0.097]+[0.117])	Prec@1 96.875 (96.875)
Epoch: [126][100/391]	Time 0.109 (0.115)	Data 0.000 (0.002)	Loss 0.2775 (0.2452) ([0.160]+[0.117])	Prec@1 93.750 (95.560)
Epoch: [126][200/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.2157 (0.2450) ([0.099]+[0.117])	Prec@1 96.094 (95.573)
Epoch: [126][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2707 (0.2456) ([0.154]+[0.117])	Prec@1 92.969 (95.575)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3721 (0.3721) ([0.255]+[0.117])	Prec@1 92.188 (92.188)
 * Prec@1 90.730
current lr 1.00000e-02
Grad=  tensor(12.4770, device='cuda:0')
Epoch: [127][0/391]	Time 0.339 (0.339)	Data 0.218 (0.218)	Loss 0.2873 (0.2873) ([0.171]+[0.117])	Prec@1 92.969 (92.969)
Epoch: [127][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.1934 (0.2509) ([0.077]+[0.117])	Prec@1 98.438 (95.459)
Epoch: [127][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2037 (0.2519) ([0.087]+[0.117])	Prec@1 98.438 (95.379)
Epoch: [127][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2759 (0.2542) ([0.159]+[0.117])	Prec@1 95.312 (95.255)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.4177 (0.4177) ([0.301]+[0.117])	Prec@1 91.406 (91.406)
 * Prec@1 90.630
current lr 1.00000e-02
Grad=  tensor(6.7779, device='cuda:0')
Epoch: [128][0/391]	Time 0.352 (0.352)	Data 0.227 (0.227)	Loss 0.2367 (0.2367) ([0.120]+[0.117])	Prec@1 96.094 (96.094)
Epoch: [128][100/391]	Time 0.109 (0.115)	Data 0.000 (0.002)	Loss 0.2626 (0.2416) ([0.146]+[0.117])	Prec@1 95.312 (95.769)
Epoch: [128][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.2657 (0.2459) ([0.149]+[0.116])	Prec@1 92.969 (95.623)
Epoch: [128][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1848 (0.2469) ([0.068]+[0.116])	Prec@1 97.656 (95.554)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3900 (0.3900) ([0.274]+[0.116])	Prec@1 90.625 (90.625)
 * Prec@1 90.500
current lr 1.00000e-02
Grad=  tensor(1.7351, device='cuda:0')
Epoch: [129][0/391]	Time 0.333 (0.333)	Data 0.214 (0.214)	Loss 0.1629 (0.1629) ([0.047]+[0.116])	Prec@1 98.438 (98.438)
Epoch: [129][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2333 (0.2300) ([0.117]+[0.116])	Prec@1 96.094 (96.202)
Epoch: [129][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2961 (0.2355) ([0.180]+[0.116])	Prec@1 95.312 (95.911)
Epoch: [129][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2263 (0.2429) ([0.110]+[0.116])	Prec@1 96.094 (95.608)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.3255 (0.3255) ([0.209]+[0.116])	Prec@1 89.844 (89.844)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(6.0330, device='cuda:0')
Epoch: [130][0/391]	Time 0.326 (0.326)	Data 0.206 (0.206)	Loss 0.2282 (0.2282) ([0.112]+[0.116])	Prec@1 95.312 (95.312)
Epoch: [130][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2157 (0.2440) ([0.100]+[0.116])	Prec@1 96.875 (95.575)
Epoch: [130][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2917 (0.2401) ([0.176]+[0.116])	Prec@1 94.531 (95.686)
Epoch: [130][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3432 (0.2424) ([0.227]+[0.116])	Prec@1 90.625 (95.572)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.3205 (0.3205) ([0.205]+[0.116])	Prec@1 95.312 (95.312)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(8.4725, device='cuda:0')
Epoch: [131][0/391]	Time 0.318 (0.318)	Data 0.198 (0.198)	Loss 0.2799 (0.2799) ([0.164]+[0.116])	Prec@1 92.188 (92.188)
Epoch: [131][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2478 (0.2341) ([0.132]+[0.116])	Prec@1 92.969 (95.962)
Epoch: [131][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2244 (0.2353) ([0.109]+[0.116])	Prec@1 96.094 (95.993)
Epoch: [131][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3176 (0.2419) ([0.202]+[0.116])	Prec@1 92.969 (95.728)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.3888 (0.3888) ([0.273]+[0.116])	Prec@1 92.188 (92.188)
 * Prec@1 90.390
current lr 1.00000e-02
Grad=  tensor(5.4545, device='cuda:0')
Epoch: [132][0/391]	Time 0.325 (0.325)	Data 0.203 (0.203)	Loss 0.2340 (0.2340) ([0.118]+[0.116])	Prec@1 96.875 (96.875)
Epoch: [132][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2544 (0.2281) ([0.139]+[0.115])	Prec@1 94.531 (96.233)
Epoch: [132][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2853 (0.2382) ([0.170]+[0.115])	Prec@1 93.750 (95.857)
Epoch: [132][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3846 (0.2438) ([0.269]+[0.116])	Prec@1 91.406 (95.671)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.4522 (0.4522) ([0.337]+[0.115])	Prec@1 89.062 (89.062)
 * Prec@1 91.070
current lr 1.00000e-02
Grad=  tensor(4.1415, device='cuda:0')
Epoch: [133][0/391]	Time 0.307 (0.307)	Data 0.182 (0.182)	Loss 0.1955 (0.1955) ([0.080]+[0.115])	Prec@1 96.094 (96.094)
Epoch: [133][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.2408 (0.2374) ([0.125]+[0.115])	Prec@1 95.312 (95.939)
Epoch: [133][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1869 (0.2390) ([0.071]+[0.115])	Prec@1 98.438 (95.752)
Epoch: [133][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2602 (0.2454) ([0.145]+[0.115])	Prec@1 93.750 (95.476)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.5069 (0.5069) ([0.391]+[0.115])	Prec@1 90.625 (90.625)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(8.7898, device='cuda:0')
Epoch: [134][0/391]	Time 0.349 (0.349)	Data 0.223 (0.223)	Loss 0.2672 (0.2672) ([0.152]+[0.115])	Prec@1 93.750 (93.750)
Epoch: [134][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1492 (0.2350) ([0.034]+[0.115])	Prec@1 100.000 (95.955)
Epoch: [134][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2431 (0.2395) ([0.128]+[0.115])	Prec@1 96.094 (95.794)
Epoch: [134][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1954 (0.2425) ([0.080]+[0.115])	Prec@1 98.438 (95.720)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4269 (0.4269) ([0.312]+[0.115])	Prec@1 91.406 (91.406)
 * Prec@1 90.790
current lr 1.00000e-02
Grad=  tensor(8.0330, device='cuda:0')
Epoch: [135][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.2477 (0.2477) ([0.132]+[0.115])	Prec@1 95.312 (95.312)
Epoch: [135][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.2535 (0.2384) ([0.138]+[0.115])	Prec@1 95.312 (95.815)
Epoch: [135][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.2135 (0.2408) ([0.098]+[0.115])	Prec@1 95.312 (95.701)
Epoch: [135][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1901 (0.2458) ([0.075]+[0.115])	Prec@1 97.656 (95.460)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3594 (0.3594) ([0.244]+[0.116])	Prec@1 92.188 (92.188)
 * Prec@1 89.610
current lr 1.00000e-02
Grad=  tensor(7.5533, device='cuda:0')
Epoch: [136][0/391]	Time 0.366 (0.366)	Data 0.241 (0.241)	Loss 0.2398 (0.2398) ([0.124]+[0.116])	Prec@1 96.094 (96.094)
Epoch: [136][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2365 (0.2400) ([0.121]+[0.116])	Prec@1 94.531 (95.645)
Epoch: [136][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.3749 (0.2393) ([0.259]+[0.115])	Prec@1 89.062 (95.670)
Epoch: [136][300/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 0.1797 (0.2445) ([0.064]+[0.116])	Prec@1 97.656 (95.510)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.4475 (0.4475) ([0.332]+[0.116])	Prec@1 92.188 (92.188)
 * Prec@1 90.600
current lr 1.00000e-02
Grad=  tensor(5.8022, device='cuda:0')
Epoch: [137][0/391]	Time 0.359 (0.359)	Data 0.233 (0.233)	Loss 0.2419 (0.2419) ([0.126]+[0.116])	Prec@1 95.312 (95.312)
Epoch: [137][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2835 (0.2432) ([0.168]+[0.115])	Prec@1 92.188 (95.707)
Epoch: [137][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2347 (0.2416) ([0.119]+[0.115])	Prec@1 95.312 (95.732)
Epoch: [137][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2104 (0.2439) ([0.095]+[0.115])	Prec@1 97.656 (95.681)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.4685 (0.4685) ([0.353]+[0.115])	Prec@1 89.062 (89.062)
 * Prec@1 87.650
current lr 1.00000e-02
Grad=  tensor(5.3950, device='cuda:0')
Epoch: [138][0/391]	Time 0.336 (0.336)	Data 0.212 (0.212)	Loss 0.2201 (0.2201) ([0.105]+[0.115])	Prec@1 96.875 (96.875)
Epoch: [138][100/391]	Time 0.111 (0.114)	Data 0.000 (0.002)	Loss 0.3186 (0.2433) ([0.203]+[0.116])	Prec@1 91.406 (95.490)
Epoch: [138][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2731 (0.2439) ([0.157]+[0.116])	Prec@1 94.531 (95.503)
Epoch: [138][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3765 (0.2478) ([0.261]+[0.116])	Prec@1 92.188 (95.375)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.4255 (0.4255) ([0.310]+[0.116])	Prec@1 92.188 (92.188)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(3.0598, device='cuda:0')
Epoch: [139][0/391]	Time 0.356 (0.356)	Data 0.231 (0.231)	Loss 0.1951 (0.1951) ([0.079]+[0.116])	Prec@1 97.656 (97.656)
Epoch: [139][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2248 (0.2422) ([0.109]+[0.116])	Prec@1 96.094 (95.668)
Epoch: [139][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.3475 (0.2440) ([0.232]+[0.116])	Prec@1 92.969 (95.511)
Epoch: [139][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2302 (0.2441) ([0.114]+[0.116])	Prec@1 96.875 (95.531)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.3153 (0.3153) ([0.199]+[0.116])	Prec@1 93.750 (93.750)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(6.4789, device='cuda:0')
Epoch: [140][0/391]	Time 0.335 (0.335)	Data 0.213 (0.213)	Loss 0.3106 (0.3106) ([0.195]+[0.116])	Prec@1 93.750 (93.750)
Epoch: [140][100/391]	Time 0.115 (0.114)	Data 0.000 (0.002)	Loss 0.2400 (0.2367) ([0.124]+[0.116])	Prec@1 96.094 (95.777)
Epoch: [140][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2582 (0.2390) ([0.142]+[0.116])	Prec@1 95.312 (95.732)
Epoch: [140][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2882 (0.2442) ([0.172]+[0.116])	Prec@1 95.312 (95.564)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4419 (0.4419) ([0.326]+[0.116])	Prec@1 90.625 (90.625)
 * Prec@1 90.560
current lr 1.00000e-02
Grad=  tensor(8.9315, device='cuda:0')
Epoch: [141][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.2361 (0.2361) ([0.120]+[0.116])	Prec@1 94.531 (94.531)
Epoch: [141][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1925 (0.2373) ([0.077]+[0.116])	Prec@1 97.656 (95.854)
Epoch: [141][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2665 (0.2436) ([0.151]+[0.116])	Prec@1 93.750 (95.643)
Epoch: [141][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2815 (0.2443) ([0.166]+[0.116])	Prec@1 92.969 (95.616)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4203 (0.4203) ([0.304]+[0.116])	Prec@1 90.625 (90.625)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(11.2237, device='cuda:0')
Epoch: [142][0/391]	Time 0.336 (0.336)	Data 0.214 (0.214)	Loss 0.3561 (0.3561) ([0.240]+[0.116])	Prec@1 92.969 (92.969)
Epoch: [142][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.2317 (0.2323) ([0.116]+[0.116])	Prec@1 93.750 (96.009)
Epoch: [142][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2235 (0.2375) ([0.108]+[0.116])	Prec@1 96.094 (95.880)
Epoch: [142][300/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.2509 (0.2419) ([0.135]+[0.116])	Prec@1 93.750 (95.691)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.5491 (0.5491) ([0.433]+[0.116])	Prec@1 91.406 (91.406)
 * Prec@1 89.000
current lr 1.00000e-02
Grad=  tensor(5.9582, device='cuda:0')
Epoch: [143][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.2026 (0.2026) ([0.087]+[0.116])	Prec@1 95.312 (95.312)
Epoch: [143][100/391]	Time 0.111 (0.114)	Data 0.000 (0.002)	Loss 0.2143 (0.2352) ([0.098]+[0.116])	Prec@1 96.875 (95.777)
Epoch: [143][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2893 (0.2394) ([0.173]+[0.116])	Prec@1 95.312 (95.709)
Epoch: [143][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2372 (0.2375) ([0.121]+[0.116])	Prec@1 95.312 (95.787)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3508 (0.3508) ([0.235]+[0.116])	Prec@1 90.625 (90.625)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(8.1031, device='cuda:0')
Epoch: [144][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.2592 (0.2592) ([0.143]+[0.116])	Prec@1 96.094 (96.094)
Epoch: [144][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2802 (0.2495) ([0.164]+[0.116])	Prec@1 95.312 (95.336)
Epoch: [144][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2756 (0.2428) ([0.160]+[0.116])	Prec@1 95.312 (95.639)
Epoch: [144][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2698 (0.2442) ([0.154]+[0.116])	Prec@1 93.750 (95.541)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.5326 (0.5326) ([0.417]+[0.116])	Prec@1 89.844 (89.844)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(6.8786, device='cuda:0')
Epoch: [145][0/391]	Time 0.323 (0.323)	Data 0.202 (0.202)	Loss 0.2386 (0.2386) ([0.123]+[0.116])	Prec@1 96.875 (96.875)
Epoch: [145][100/391]	Time 0.108 (0.112)	Data 0.000 (0.002)	Loss 0.3316 (0.2416) ([0.216]+[0.116])	Prec@1 92.969 (95.769)
Epoch: [145][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2312 (0.2385) ([0.115]+[0.116])	Prec@1 96.094 (95.678)
Epoch: [145][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2200 (0.2420) ([0.104]+[0.116])	Prec@1 96.094 (95.502)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.3209 (0.3209) ([0.205]+[0.116])	Prec@1 92.969 (92.969)
 * Prec@1 90.630
current lr 1.00000e-02
Grad=  tensor(6.0957, device='cuda:0')
Epoch: [146][0/391]	Time 0.317 (0.317)	Data 0.198 (0.198)	Loss 0.2026 (0.2026) ([0.086]+[0.116])	Prec@1 96.875 (96.875)
Epoch: [146][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2241 (0.2338) ([0.108]+[0.116])	Prec@1 96.875 (96.086)
Epoch: [146][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2699 (0.2382) ([0.154]+[0.116])	Prec@1 92.969 (95.713)
Epoch: [146][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2055 (0.2402) ([0.089]+[0.116])	Prec@1 96.094 (95.601)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.4648 (0.4648) ([0.349]+[0.116])	Prec@1 92.188 (92.188)
 * Prec@1 88.690
current lr 1.00000e-02
Grad=  tensor(8.8492, device='cuda:0')
Epoch: [147][0/391]	Time 0.294 (0.294)	Data 0.176 (0.176)	Loss 0.2257 (0.2257) ([0.109]+[0.116])	Prec@1 93.750 (93.750)
Epoch: [147][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2410 (0.2442) ([0.125]+[0.116])	Prec@1 94.531 (95.560)
Epoch: [147][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2413 (0.2455) ([0.125]+[0.116])	Prec@1 94.531 (95.464)
Epoch: [147][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3152 (0.2487) ([0.199]+[0.116])	Prec@1 93.750 (95.396)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.5119 (0.5119) ([0.395]+[0.116])	Prec@1 88.281 (88.281)
 * Prec@1 88.230
current lr 1.00000e-02
Grad=  tensor(8.7033, device='cuda:0')
Epoch: [148][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 0.2535 (0.2535) ([0.137]+[0.116])	Prec@1 93.750 (93.750)
Epoch: [148][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.2542 (0.2311) ([0.138]+[0.116])	Prec@1 94.531 (96.132)
Epoch: [148][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1753 (0.2343) ([0.059]+[0.116])	Prec@1 98.438 (96.067)
Epoch: [148][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2589 (0.2439) ([0.142]+[0.117])	Prec@1 95.312 (95.699)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4503 (0.4503) ([0.334]+[0.117])	Prec@1 90.625 (90.625)
 * Prec@1 89.720
current lr 1.00000e-02
Grad=  tensor(4.4710, device='cuda:0')
Epoch: [149][0/391]	Time 0.335 (0.335)	Data 0.215 (0.215)	Loss 0.1951 (0.1951) ([0.079]+[0.117])	Prec@1 97.656 (97.656)
Epoch: [149][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.2093 (0.2434) ([0.093]+[0.117])	Prec@1 97.656 (95.560)
Epoch: [149][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2254 (0.2395) ([0.109]+[0.116])	Prec@1 96.094 (95.678)
Epoch: [149][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2388 (0.2406) ([0.122]+[0.116])	Prec@1 95.312 (95.660)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.4491 (0.4491) ([0.333]+[0.116])	Prec@1 90.625 (90.625)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(9.7983, device='cuda:0')
Epoch: [150][0/391]	Time 0.313 (0.313)	Data 0.194 (0.194)	Loss 0.2390 (0.2390) ([0.123]+[0.116])	Prec@1 95.312 (95.312)
Epoch: [150][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1757 (0.2367) ([0.059]+[0.117])	Prec@1 96.875 (95.823)
Epoch: [150][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2789 (0.2363) ([0.162]+[0.116])	Prec@1 94.531 (95.841)
Epoch: [150][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2719 (0.2367) ([0.155]+[0.116])	Prec@1 94.531 (95.837)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3893 (0.3893) ([0.273]+[0.117])	Prec@1 91.406 (91.406)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(6.4730, device='cuda:0')
Epoch: [151][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.2554 (0.2554) ([0.139]+[0.117])	Prec@1 96.094 (96.094)
Epoch: [151][100/391]	Time 0.108 (0.112)	Data 0.000 (0.002)	Loss 0.2284 (0.2403) ([0.112]+[0.117])	Prec@1 96.094 (95.869)
Epoch: [151][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2744 (0.2403) ([0.158]+[0.117])	Prec@1 93.750 (95.826)
Epoch: [151][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.3043 (0.2435) ([0.187]+[0.117])	Prec@1 92.969 (95.665)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3676 (0.3676) ([0.251]+[0.117])	Prec@1 91.406 (91.406)
 * Prec@1 90.790
current lr 1.00000e-02
Grad=  tensor(3.6140, device='cuda:0')
Epoch: [152][0/391]	Time 0.356 (0.356)	Data 0.237 (0.237)	Loss 0.1952 (0.1952) ([0.078]+[0.117])	Prec@1 97.656 (97.656)
Epoch: [152][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2172 (0.2265) ([0.101]+[0.117])	Prec@1 98.438 (96.295)
Epoch: [152][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2320 (0.2289) ([0.115]+[0.117])	Prec@1 96.094 (96.148)
Epoch: [152][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2747 (0.2362) ([0.158]+[0.117])	Prec@1 94.531 (95.795)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.4597 (0.4597) ([0.343]+[0.117])	Prec@1 87.500 (87.500)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(17.0159, device='cuda:0')
Epoch: [153][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.2807 (0.2807) ([0.164]+[0.117])	Prec@1 92.969 (92.969)
Epoch: [153][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2474 (0.2302) ([0.131]+[0.117])	Prec@1 94.531 (96.194)
Epoch: [153][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2739 (0.2314) ([0.157]+[0.117])	Prec@1 95.312 (96.175)
Epoch: [153][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2573 (0.2380) ([0.140]+[0.117])	Prec@1 92.969 (95.850)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4411 (0.4411) ([0.324]+[0.117])	Prec@1 91.406 (91.406)
 * Prec@1 89.790
current lr 1.00000e-02
Grad=  tensor(2.9013, device='cuda:0')
Epoch: [154][0/391]	Time 0.338 (0.338)	Data 0.218 (0.218)	Loss 0.1721 (0.1721) ([0.055]+[0.117])	Prec@1 98.438 (98.438)
Epoch: [154][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2398 (0.2270) ([0.123]+[0.117])	Prec@1 95.312 (96.287)
Epoch: [154][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3810 (0.2306) ([0.264]+[0.117])	Prec@1 89.844 (96.168)
Epoch: [154][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2412 (0.2314) ([0.124]+[0.117])	Prec@1 95.312 (96.078)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4811 (0.4811) ([0.364]+[0.117])	Prec@1 89.844 (89.844)
 * Prec@1 90.070
current lr 1.00000e-02
Grad=  tensor(10.0610, device='cuda:0')
Epoch: [155][0/391]	Time 0.316 (0.316)	Data 0.198 (0.198)	Loss 0.2476 (0.2476) ([0.131]+[0.117])	Prec@1 96.094 (96.094)
Epoch: [155][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2660 (0.2346) ([0.149]+[0.117])	Prec@1 96.875 (96.071)
Epoch: [155][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1778 (0.2334) ([0.061]+[0.117])	Prec@1 98.438 (96.039)
Epoch: [155][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2170 (0.2396) ([0.100]+[0.117])	Prec@1 96.094 (95.780)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.3669 (0.3669) ([0.250]+[0.117])	Prec@1 92.188 (92.188)
 * Prec@1 91.110
current lr 1.00000e-02
Grad=  tensor(2.3376, device='cuda:0')
Epoch: [156][0/391]	Time 0.345 (0.345)	Data 0.222 (0.222)	Loss 0.1644 (0.1644) ([0.047]+[0.117])	Prec@1 98.438 (98.438)
Epoch: [156][100/391]	Time 0.110 (0.114)	Data 0.000 (0.002)	Loss 0.2177 (0.2253) ([0.101]+[0.117])	Prec@1 97.656 (96.426)
Epoch: [156][200/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.2302 (0.2334) ([0.113]+[0.117])	Prec@1 95.312 (96.129)
Epoch: [156][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.3137 (0.2341) ([0.197]+[0.117])	Prec@1 92.188 (96.034)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.5984 (0.5984) ([0.481]+[0.117])	Prec@1 89.062 (89.062)
 * Prec@1 87.020
current lr 1.00000e-02
Grad=  tensor(8.9457, device='cuda:0')
Epoch: [157][0/391]	Time 0.334 (0.334)	Data 0.214 (0.214)	Loss 0.2744 (0.2744) ([0.157]+[0.117])	Prec@1 93.750 (93.750)
Epoch: [157][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2327 (0.2300) ([0.115]+[0.117])	Prec@1 96.094 (96.001)
Epoch: [157][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2332 (0.2378) ([0.116]+[0.117])	Prec@1 94.531 (95.818)
Epoch: [157][300/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.2048 (0.2419) ([0.087]+[0.118])	Prec@1 97.656 (95.660)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.4590 (0.4590) ([0.341]+[0.118])	Prec@1 89.844 (89.844)
 * Prec@1 90.380
current lr 1.00000e-02
Grad=  tensor(9.3923, device='cuda:0')
Epoch: [158][0/391]	Time 0.325 (0.325)	Data 0.206 (0.206)	Loss 0.2779 (0.2779) ([0.160]+[0.118])	Prec@1 96.094 (96.094)
Epoch: [158][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2187 (0.2355) ([0.101]+[0.118])	Prec@1 96.094 (96.009)
Epoch: [158][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2547 (0.2371) ([0.137]+[0.118])	Prec@1 95.312 (95.888)
Epoch: [158][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2580 (0.2374) ([0.140]+[0.118])	Prec@1 94.531 (95.852)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.4446 (0.4446) ([0.327]+[0.118])	Prec@1 92.188 (92.188)
 * Prec@1 90.840
current lr 1.00000e-02
Grad=  tensor(11.0257, device='cuda:0')
Epoch: [159][0/391]	Time 0.326 (0.326)	Data 0.207 (0.207)	Loss 0.2794 (0.2794) ([0.162]+[0.118])	Prec@1 94.531 (94.531)
Epoch: [159][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1918 (0.2356) ([0.074]+[0.118])	Prec@1 96.875 (95.862)
Epoch: [159][200/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.1941 (0.2432) ([0.076]+[0.118])	Prec@1 97.656 (95.546)
Epoch: [159][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1919 (0.2420) ([0.074]+[0.118])	Prec@1 97.656 (95.676)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4370 (0.4370) ([0.319]+[0.118])	Prec@1 92.188 (92.188)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(5.7751, device='cuda:0')
Epoch: [160][0/391]	Time 0.345 (0.345)	Data 0.223 (0.223)	Loss 0.2223 (0.2223) ([0.104]+[0.118])	Prec@1 96.875 (96.875)
Epoch: [160][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2151 (0.2280) ([0.097]+[0.118])	Prec@1 96.094 (96.163)
Epoch: [160][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.3903 (0.2336) ([0.272]+[0.118])	Prec@1 91.406 (96.055)
Epoch: [160][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2022 (0.2370) ([0.084]+[0.118])	Prec@1 95.312 (95.907)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.5045 (0.5045) ([0.387]+[0.118])	Prec@1 89.844 (89.844)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(7.5201, device='cuda:0')
Epoch: [161][0/391]	Time 0.311 (0.311)	Data 0.191 (0.191)	Loss 0.2359 (0.2359) ([0.118]+[0.118])	Prec@1 96.094 (96.094)
Epoch: [161][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2118 (0.2353) ([0.094]+[0.118])	Prec@1 96.094 (95.939)
Epoch: [161][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2383 (0.2390) ([0.120]+[0.118])	Prec@1 94.531 (95.779)
Epoch: [161][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2488 (0.2377) ([0.131]+[0.118])	Prec@1 96.094 (95.806)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.3948 (0.3948) ([0.277]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 89.960
current lr 1.00000e-02
Grad=  tensor(8.2227, device='cuda:0')
Epoch: [162][0/391]	Time 0.343 (0.343)	Data 0.222 (0.222)	Loss 0.2731 (0.2731) ([0.155]+[0.118])	Prec@1 94.531 (94.531)
Epoch: [162][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2043 (0.2359) ([0.086]+[0.118])	Prec@1 96.875 (95.869)
Epoch: [162][200/391]	Time 0.115 (0.111)	Data 0.000 (0.001)	Loss 0.2637 (0.2317) ([0.146]+[0.118])	Prec@1 95.312 (96.086)
Epoch: [162][300/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.2514 (0.2350) ([0.133]+[0.118])	Prec@1 95.312 (95.967)
Test: [0/79]	Time 0.268 (0.268)	Loss 0.4142 (0.4142) ([0.296]+[0.118])	Prec@1 92.969 (92.969)
 * Prec@1 91.070
current lr 1.00000e-02
Grad=  tensor(7.0929, device='cuda:0')
Epoch: [163][0/391]	Time 0.345 (0.345)	Data 0.221 (0.221)	Loss 0.2360 (0.2360) ([0.118]+[0.118])	Prec@1 95.312 (95.312)
Epoch: [163][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2194 (0.2274) ([0.101]+[0.118])	Prec@1 96.875 (96.194)
Epoch: [163][200/391]	Time 0.109 (0.114)	Data 0.000 (0.001)	Loss 0.2258 (0.2323) ([0.108]+[0.118])	Prec@1 95.312 (95.965)
Epoch: [163][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.2363 (0.2334) ([0.118]+[0.118])	Prec@1 96.875 (96.016)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.5150 (0.5150) ([0.397]+[0.118])	Prec@1 91.406 (91.406)
 * Prec@1 90.670
current lr 1.00000e-02
Grad=  tensor(3.7980, device='cuda:0')
Epoch: [164][0/391]	Time 0.335 (0.335)	Data 0.215 (0.215)	Loss 0.1712 (0.1712) ([0.053]+[0.118])	Prec@1 97.656 (97.656)
Epoch: [164][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2608 (0.2172) ([0.143]+[0.118])	Prec@1 93.750 (96.635)
Epoch: [164][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1698 (0.2263) ([0.052]+[0.118])	Prec@1 99.219 (96.241)
Epoch: [164][300/391]	Time 0.114 (0.111)	Data 0.000 (0.001)	Loss 0.2833 (0.2346) ([0.165]+[0.118])	Prec@1 92.969 (95.951)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4036 (0.4036) ([0.285]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 90.580
current lr 1.00000e-02
Grad=  tensor(12.1122, device='cuda:0')
Epoch: [165][0/391]	Time 0.335 (0.335)	Data 0.211 (0.211)	Loss 0.2510 (0.2510) ([0.132]+[0.118])	Prec@1 95.312 (95.312)
Epoch: [165][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2796 (0.2153) ([0.161]+[0.118])	Prec@1 91.406 (96.612)
Epoch: [165][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1900 (0.2231) ([0.072]+[0.118])	Prec@1 96.875 (96.424)
Epoch: [165][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2120 (0.2307) ([0.094]+[0.118])	Prec@1 96.875 (96.169)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.3572 (0.3572) ([0.239]+[0.118])	Prec@1 92.969 (92.969)
 * Prec@1 90.340
current lr 1.00000e-02
Grad=  tensor(3.0888, device='cuda:0')
Epoch: [166][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.1867 (0.1867) ([0.068]+[0.118])	Prec@1 97.656 (97.656)
Epoch: [166][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2110 (0.2257) ([0.093]+[0.118])	Prec@1 97.656 (96.218)
Epoch: [166][200/391]	Time 0.109 (0.114)	Data 0.000 (0.001)	Loss 0.1773 (0.2325) ([0.059]+[0.118])	Prec@1 97.656 (95.985)
Epoch: [166][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.3129 (0.2371) ([0.194]+[0.119])	Prec@1 91.406 (95.876)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4055 (0.4055) ([0.287]+[0.119])	Prec@1 89.062 (89.062)
 * Prec@1 90.090
current lr 1.00000e-02
Grad=  tensor(9.2197, device='cuda:0')
Epoch: [167][0/391]	Time 0.335 (0.335)	Data 0.215 (0.215)	Loss 0.2767 (0.2767) ([0.158]+[0.119])	Prec@1 94.531 (94.531)
Epoch: [167][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1899 (0.2303) ([0.072]+[0.118])	Prec@1 97.656 (96.047)
Epoch: [167][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2117 (0.2261) ([0.093]+[0.118])	Prec@1 96.875 (96.257)
Epoch: [167][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2365 (0.2302) ([0.118]+[0.118])	Prec@1 96.094 (96.065)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.3836 (0.3836) ([0.265]+[0.118])	Prec@1 89.062 (89.062)
 * Prec@1 89.200
current lr 1.00000e-02
Grad=  tensor(10.5663, device='cuda:0')
Epoch: [168][0/391]	Time 0.353 (0.353)	Data 0.229 (0.229)	Loss 0.2589 (0.2589) ([0.140]+[0.118])	Prec@1 96.094 (96.094)
Epoch: [168][100/391]	Time 0.110 (0.115)	Data 0.000 (0.002)	Loss 0.2430 (0.2322) ([0.124]+[0.119])	Prec@1 96.875 (96.055)
Epoch: [168][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.2209 (0.2356) ([0.102]+[0.119])	Prec@1 96.094 (95.965)
Epoch: [168][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2212 (0.2406) ([0.102]+[0.119])	Prec@1 97.656 (95.772)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.5001 (0.5001) ([0.381]+[0.119])	Prec@1 87.500 (87.500)
 * Prec@1 90.480
current lr 1.00000e-02
Grad=  tensor(11.8036, device='cuda:0')
Epoch: [169][0/391]	Time 0.358 (0.358)	Data 0.233 (0.233)	Loss 0.2655 (0.2655) ([0.147]+[0.119])	Prec@1 95.312 (95.312)
Epoch: [169][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1949 (0.2223) ([0.076]+[0.119])	Prec@1 97.656 (96.558)
Epoch: [169][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.2421 (0.2263) ([0.123]+[0.119])	Prec@1 96.094 (96.370)
Epoch: [169][300/391]	Time 0.108 (0.114)	Data 0.000 (0.001)	Loss 0.2317 (0.2289) ([0.113]+[0.119])	Prec@1 95.312 (96.247)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.5211 (0.5211) ([0.402]+[0.119])	Prec@1 89.844 (89.844)
 * Prec@1 89.390
current lr 1.00000e-02
Grad=  tensor(8.8811, device='cuda:0')
Epoch: [170][0/391]	Time 0.332 (0.332)	Data 0.213 (0.213)	Loss 0.2971 (0.2971) ([0.178]+[0.119])	Prec@1 93.750 (93.750)
Epoch: [170][100/391]	Time 0.115 (0.113)	Data 0.000 (0.002)	Loss 0.1848 (0.2305) ([0.066]+[0.119])	Prec@1 97.656 (96.349)
Epoch: [170][200/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.1852 (0.2328) ([0.067]+[0.119])	Prec@1 99.219 (96.137)
Epoch: [170][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.1778 (0.2355) ([0.059]+[0.119])	Prec@1 98.438 (96.006)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4916 (0.4916) ([0.373]+[0.119])	Prec@1 89.844 (89.844)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(8.2566, device='cuda:0')
Epoch: [171][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.2511 (0.2511) ([0.132]+[0.119])	Prec@1 96.094 (96.094)
Epoch: [171][100/391]	Time 0.109 (0.114)	Data 0.000 (0.002)	Loss 0.2856 (0.2293) ([0.167]+[0.119])	Prec@1 93.750 (96.132)
Epoch: [171][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2132 (0.2290) ([0.094]+[0.119])	Prec@1 97.656 (96.144)
Epoch: [171][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1658 (0.2325) ([0.047]+[0.119])	Prec@1 98.438 (96.003)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3988 (0.3988) ([0.280]+[0.119])	Prec@1 85.938 (85.938)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(5.0061, device='cuda:0')
Epoch: [172][0/391]	Time 0.355 (0.355)	Data 0.230 (0.230)	Loss 0.1935 (0.1935) ([0.075]+[0.119])	Prec@1 97.656 (97.656)
Epoch: [172][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.2426 (0.2186) ([0.124]+[0.119])	Prec@1 95.312 (96.566)
Epoch: [172][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2567 (0.2273) ([0.138]+[0.119])	Prec@1 92.969 (96.370)
Epoch: [172][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2548 (0.2283) ([0.136]+[0.119])	Prec@1 93.750 (96.249)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3607 (0.3607) ([0.242]+[0.119])	Prec@1 92.969 (92.969)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(7.7588, device='cuda:0')
Epoch: [173][0/391]	Time 0.357 (0.357)	Data 0.233 (0.233)	Loss 0.2000 (0.2000) ([0.081]+[0.119])	Prec@1 97.656 (97.656)
Epoch: [173][100/391]	Time 0.109 (0.116)	Data 0.000 (0.002)	Loss 0.2651 (0.2293) ([0.146]+[0.119])	Prec@1 92.969 (96.187)
Epoch: [173][200/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.1982 (0.2308) ([0.079]+[0.119])	Prec@1 96.094 (96.074)
Epoch: [173][300/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.1894 (0.2300) ([0.071]+[0.119])	Prec@1 98.438 (96.112)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.4924 (0.4924) ([0.373]+[0.119])	Prec@1 91.406 (91.406)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(12.8617, device='cuda:0')
Epoch: [174][0/391]	Time 0.332 (0.332)	Data 0.213 (0.213)	Loss 0.2737 (0.2737) ([0.155]+[0.119])	Prec@1 92.188 (92.188)
Epoch: [174][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.2631 (0.2267) ([0.144]+[0.119])	Prec@1 94.531 (96.163)
Epoch: [174][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.2826 (0.2307) ([0.164]+[0.119])	Prec@1 93.750 (96.043)
Epoch: [174][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.3364 (0.2337) ([0.217]+[0.119])	Prec@1 92.969 (95.967)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.3947 (0.3947) ([0.276]+[0.119])	Prec@1 92.188 (92.188)
 * Prec@1 90.040
current lr 1.00000e-02
Grad=  tensor(6.5857, device='cuda:0')
Epoch: [175][0/391]	Time 0.346 (0.346)	Data 0.222 (0.222)	Loss 0.2113 (0.2113) ([0.092]+[0.119])	Prec@1 96.875 (96.875)
Epoch: [175][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2007 (0.2266) ([0.082]+[0.119])	Prec@1 97.656 (96.380)
Epoch: [175][200/391]	Time 0.116 (0.116)	Data 0.000 (0.001)	Loss 0.2003 (0.2353) ([0.081]+[0.119])	Prec@1 96.875 (95.989)
Epoch: [175][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2757 (0.2377) ([0.156]+[0.119])	Prec@1 94.531 (95.909)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.5634 (0.5634) ([0.444]+[0.119])	Prec@1 87.500 (87.500)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(7.0680, device='cuda:0')
Epoch: [176][0/391]	Time 0.336 (0.336)	Data 0.212 (0.212)	Loss 0.2275 (0.2275) ([0.108]+[0.119])	Prec@1 95.312 (95.312)
Epoch: [176][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2463 (0.2245) ([0.127]+[0.119])	Prec@1 96.094 (96.450)
Epoch: [176][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2262 (0.2301) ([0.107]+[0.119])	Prec@1 96.094 (96.241)
Epoch: [176][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2288 (0.2340) ([0.109]+[0.119])	Prec@1 96.875 (96.102)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.4007 (0.4007) ([0.281]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(5.0406, device='cuda:0')
Epoch: [177][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.2162 (0.2162) ([0.097]+[0.120])	Prec@1 97.656 (97.656)
Epoch: [177][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1800 (0.2254) ([0.061]+[0.119])	Prec@1 97.656 (96.620)
Epoch: [177][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2281 (0.2202) ([0.109]+[0.119])	Prec@1 96.875 (96.723)
Epoch: [177][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2432 (0.2274) ([0.124]+[0.119])	Prec@1 95.312 (96.397)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.5386 (0.5386) ([0.419]+[0.119])	Prec@1 88.281 (88.281)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(5.1450, device='cuda:0')
Epoch: [178][0/391]	Time 0.330 (0.330)	Data 0.211 (0.211)	Loss 0.2064 (0.2064) ([0.087]+[0.119])	Prec@1 96.875 (96.875)
Epoch: [178][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2094 (0.2258) ([0.090]+[0.119])	Prec@1 97.656 (96.411)
Epoch: [178][200/391]	Time 0.109 (0.109)	Data 0.000 (0.001)	Loss 0.2324 (0.2228) ([0.113]+[0.119])	Prec@1 96.094 (96.486)
Epoch: [178][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2987 (0.2274) ([0.179]+[0.119])	Prec@1 92.188 (96.327)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.4784 (0.4784) ([0.359]+[0.119])	Prec@1 87.500 (87.500)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(7.1840, device='cuda:0')
Epoch: [179][0/391]	Time 0.320 (0.320)	Data 0.201 (0.201)	Loss 0.2367 (0.2367) ([0.117]+[0.119])	Prec@1 96.875 (96.875)
Epoch: [179][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1809 (0.2190) ([0.062]+[0.119])	Prec@1 98.438 (96.566)
Epoch: [179][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1703 (0.2170) ([0.051]+[0.119])	Prec@1 100.000 (96.688)
Epoch: [179][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2641 (0.2191) ([0.145]+[0.119])	Prec@1 96.094 (96.600)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4887 (0.4887) ([0.370]+[0.119])	Prec@1 89.844 (89.844)
 * Prec@1 89.690
current lr 1.00000e-02
Grad=  tensor(3.1749, device='cuda:0')
Epoch: [180][0/391]	Time 0.303 (0.303)	Data 0.183 (0.183)	Loss 0.1830 (0.1830) ([0.064]+[0.119])	Prec@1 97.656 (97.656)
Epoch: [180][100/391]	Time 0.110 (0.110)	Data 0.000 (0.002)	Loss 0.2209 (0.2282) ([0.102]+[0.119])	Prec@1 94.531 (96.241)
Epoch: [180][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2917 (0.2224) ([0.173]+[0.119])	Prec@1 94.531 (96.436)
Epoch: [180][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3272 (0.2248) ([0.208]+[0.119])	Prec@1 94.531 (96.374)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.4855 (0.4855) ([0.366]+[0.119])	Prec@1 85.938 (85.938)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(6.7985, device='cuda:0')
Epoch: [181][0/391]	Time 0.307 (0.307)	Data 0.187 (0.187)	Loss 0.2141 (0.2141) ([0.095]+[0.119])	Prec@1 96.875 (96.875)
Epoch: [181][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2184 (0.2174) ([0.099]+[0.119])	Prec@1 96.875 (96.658)
Epoch: [181][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1688 (0.2269) ([0.050]+[0.119])	Prec@1 99.219 (96.261)
Epoch: [181][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2390 (0.2276) ([0.120]+[0.119])	Prec@1 96.875 (96.268)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.4975 (0.4975) ([0.378]+[0.119])	Prec@1 89.062 (89.062)
 * Prec@1 90.860
current lr 1.00000e-02
Grad=  tensor(16.0758, device='cuda:0')
Epoch: [182][0/391]	Time 0.356 (0.356)	Data 0.230 (0.230)	Loss 0.2838 (0.2838) ([0.165]+[0.119])	Prec@1 92.969 (92.969)
Epoch: [182][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.2365 (0.2250) ([0.117]+[0.119])	Prec@1 96.875 (96.303)
Epoch: [182][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.1737 (0.2303) ([0.054]+[0.119])	Prec@1 97.656 (96.175)
Epoch: [182][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2259 (0.2321) ([0.107]+[0.119])	Prec@1 96.875 (96.057)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.4853 (0.4853) ([0.366]+[0.119])	Prec@1 91.406 (91.406)
 * Prec@1 90.310
current lr 1.00000e-02
Grad=  tensor(10.5866, device='cuda:0')
Epoch: [183][0/391]	Time 0.336 (0.336)	Data 0.212 (0.212)	Loss 0.2305 (0.2305) ([0.111]+[0.119])	Prec@1 96.094 (96.094)
Epoch: [183][100/391]	Time 0.111 (0.114)	Data 0.000 (0.002)	Loss 0.1903 (0.2302) ([0.071]+[0.119])	Prec@1 97.656 (96.187)
Epoch: [183][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2275 (0.2328) ([0.108]+[0.120])	Prec@1 96.875 (96.179)
Epoch: [183][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1711 (0.2309) ([0.052]+[0.119])	Prec@1 98.438 (96.237)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.5791 (0.5791) ([0.459]+[0.120])	Prec@1 89.062 (89.062)
 * Prec@1 89.180
current lr 1.00000e-02
Grad=  tensor(2.2293, device='cuda:0')
Epoch: [184][0/391]	Time 0.357 (0.357)	Data 0.233 (0.233)	Loss 0.1634 (0.1634) ([0.044]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [184][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1690 (0.2222) ([0.049]+[0.120])	Prec@1 99.219 (96.380)
Epoch: [184][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2709 (0.2244) ([0.151]+[0.120])	Prec@1 94.531 (96.331)
Epoch: [184][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2442 (0.2271) ([0.125]+[0.120])	Prec@1 96.094 (96.242)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3941 (0.3941) ([0.274]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 89.590
current lr 1.00000e-02
Grad=  tensor(18.3684, device='cuda:0')
Epoch: [185][0/391]	Time 0.356 (0.356)	Data 0.231 (0.231)	Loss 0.3418 (0.3418) ([0.222]+[0.120])	Prec@1 92.188 (92.188)
Epoch: [185][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1778 (0.2308) ([0.058]+[0.120])	Prec@1 99.219 (96.125)
Epoch: [185][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2686 (0.2288) ([0.149]+[0.120])	Prec@1 93.750 (96.222)
Epoch: [185][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2283 (0.2275) ([0.109]+[0.120])	Prec@1 96.094 (96.270)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.4260 (0.4260) ([0.306]+[0.120])	Prec@1 89.844 (89.844)
 * Prec@1 88.850
current lr 1.00000e-02
Grad=  tensor(5.7390, device='cuda:0')
Epoch: [186][0/391]	Time 0.339 (0.339)	Data 0.219 (0.219)	Loss 0.1905 (0.1905) ([0.071]+[0.120])	Prec@1 96.094 (96.094)
Epoch: [186][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.2318 (0.2173) ([0.112]+[0.120])	Prec@1 96.094 (96.666)
Epoch: [186][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1700 (0.2160) ([0.050]+[0.120])	Prec@1 98.438 (96.665)
Epoch: [186][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2596 (0.2246) ([0.140]+[0.120])	Prec@1 93.750 (96.366)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.4962 (0.4962) ([0.376]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(6.8623, device='cuda:0')
Epoch: [187][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.2037 (0.2037) ([0.084]+[0.120])	Prec@1 96.094 (96.094)
Epoch: [187][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2841 (0.2142) ([0.164]+[0.120])	Prec@1 92.188 (96.620)
Epoch: [187][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2614 (0.2212) ([0.142]+[0.120])	Prec@1 96.094 (96.560)
Epoch: [187][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.3351 (0.2239) ([0.215]+[0.120])	Prec@1 92.969 (96.413)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.3693 (0.3693) ([0.249]+[0.120])	Prec@1 90.625 (90.625)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(5.7921, device='cuda:0')
Epoch: [188][0/391]	Time 0.343 (0.343)	Data 0.219 (0.219)	Loss 0.2026 (0.2026) ([0.083]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [188][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2188 (0.2223) ([0.099]+[0.120])	Prec@1 95.312 (96.511)
Epoch: [188][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.2293 (0.2228) ([0.110]+[0.120])	Prec@1 97.656 (96.541)
Epoch: [188][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2597 (0.2272) ([0.140]+[0.120])	Prec@1 96.094 (96.356)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.3223 (0.3223) ([0.202]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 90.920
current lr 1.00000e-02
Grad=  tensor(5.7487, device='cuda:0')
Epoch: [189][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.2043 (0.2043) ([0.084]+[0.120])	Prec@1 96.094 (96.094)
Epoch: [189][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1863 (0.2306) ([0.066]+[0.120])	Prec@1 97.656 (96.403)
Epoch: [189][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2417 (0.2252) ([0.122]+[0.120])	Prec@1 96.094 (96.463)
Epoch: [189][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.3267 (0.2268) ([0.207]+[0.120])	Prec@1 92.969 (96.382)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4037 (0.4037) ([0.284]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 90.360
current lr 1.00000e-02
Grad=  tensor(7.3284, device='cuda:0')
Epoch: [190][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.2361 (0.2361) ([0.116]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [190][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1593 (0.2158) ([0.039]+[0.120])	Prec@1 98.438 (96.883)
Epoch: [190][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2210 (0.2195) ([0.101]+[0.120])	Prec@1 96.094 (96.723)
Epoch: [190][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2112 (0.2252) ([0.091]+[0.120])	Prec@1 96.094 (96.462)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.4016 (0.4016) ([0.282]+[0.120])	Prec@1 90.625 (90.625)
 * Prec@1 90.620
current lr 1.00000e-02
Grad=  tensor(13.8123, device='cuda:0')
Epoch: [191][0/391]	Time 0.334 (0.334)	Data 0.214 (0.214)	Loss 0.2850 (0.2850) ([0.165]+[0.120])	Prec@1 92.188 (92.188)
Epoch: [191][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2378 (0.2146) ([0.118]+[0.120])	Prec@1 96.094 (96.782)
Epoch: [191][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2101 (0.2167) ([0.090]+[0.120])	Prec@1 96.875 (96.685)
Epoch: [191][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2803 (0.2188) ([0.161]+[0.120])	Prec@1 94.531 (96.587)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.3703 (0.3703) ([0.251]+[0.120])	Prec@1 89.844 (89.844)
 * Prec@1 89.730
current lr 1.00000e-02
Grad=  tensor(5.8930, device='cuda:0')
Epoch: [192][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.1913 (0.1913) ([0.072]+[0.120])	Prec@1 96.094 (96.094)
Epoch: [192][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.3078 (0.2218) ([0.188]+[0.120])	Prec@1 93.750 (96.612)
Epoch: [192][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2330 (0.2297) ([0.113]+[0.120])	Prec@1 94.531 (96.273)
Epoch: [192][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2151 (0.2312) ([0.095]+[0.120])	Prec@1 96.875 (96.174)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.5236 (0.5236) ([0.404]+[0.120])	Prec@1 88.281 (88.281)
 * Prec@1 89.880
current lr 1.00000e-02
Grad=  tensor(10.3540, device='cuda:0')
Epoch: [193][0/391]	Time 0.364 (0.364)	Data 0.228 (0.228)	Loss 0.2219 (0.2219) ([0.102]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [193][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2182 (0.2210) ([0.098]+[0.120])	Prec@1 97.656 (96.627)
Epoch: [193][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.1964 (0.2249) ([0.076]+[0.120])	Prec@1 97.656 (96.514)
Epoch: [193][300/391]	Time 0.109 (0.114)	Data 0.000 (0.001)	Loss 0.2278 (0.2308) ([0.108]+[0.120])	Prec@1 96.875 (96.268)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.3464 (0.3464) ([0.226]+[0.120])	Prec@1 93.750 (93.750)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(6.2087, device='cuda:0')
Epoch: [194][0/391]	Time 0.352 (0.352)	Data 0.227 (0.227)	Loss 0.2060 (0.2060) ([0.086]+[0.120])	Prec@1 98.438 (98.438)
Epoch: [194][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1617 (0.2269) ([0.041]+[0.120])	Prec@1 99.219 (96.411)
Epoch: [194][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2287 (0.2252) ([0.109]+[0.120])	Prec@1 95.312 (96.447)
Epoch: [194][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2524 (0.2241) ([0.132]+[0.120])	Prec@1 93.750 (96.410)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.4143 (0.4143) ([0.294]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(3.7177, device='cuda:0')
Epoch: [195][0/391]	Time 0.338 (0.338)	Data 0.219 (0.219)	Loss 0.1697 (0.1697) ([0.050]+[0.120])	Prec@1 98.438 (98.438)
Epoch: [195][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.2153 (0.2269) ([0.095]+[0.120])	Prec@1 96.875 (96.380)
Epoch: [195][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2559 (0.2224) ([0.136]+[0.120])	Prec@1 95.312 (96.525)
Epoch: [195][300/391]	Time 0.113 (0.109)	Data 0.000 (0.001)	Loss 0.2061 (0.2252) ([0.086]+[0.120])	Prec@1 96.094 (96.384)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4204 (0.4204) ([0.300]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 90.360
current lr 1.00000e-02
Grad=  tensor(14.4623, device='cuda:0')
Epoch: [196][0/391]	Time 0.351 (0.351)	Data 0.226 (0.226)	Loss 0.2694 (0.2694) ([0.149]+[0.120])	Prec@1 93.750 (93.750)
Epoch: [196][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.1918 (0.2208) ([0.072]+[0.120])	Prec@1 96.875 (96.272)
Epoch: [196][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2009 (0.2231) ([0.081]+[0.120])	Prec@1 96.875 (96.265)
Epoch: [196][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1846 (0.2251) ([0.064]+[0.120])	Prec@1 98.438 (96.260)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4349 (0.4349) ([0.315]+[0.120])	Prec@1 89.844 (89.844)
 * Prec@1 90.200
current lr 1.00000e-02
Grad=  tensor(8.7785, device='cuda:0')
Epoch: [197][0/391]	Time 0.333 (0.333)	Data 0.214 (0.214)	Loss 0.2282 (0.2282) ([0.108]+[0.120])	Prec@1 96.094 (96.094)
Epoch: [197][100/391]	Time 0.108 (0.111)	Data 0.000 (0.002)	Loss 0.2424 (0.2104) ([0.122]+[0.120])	Prec@1 95.312 (97.045)
Epoch: [197][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2285 (0.2206) ([0.108]+[0.120])	Prec@1 96.875 (96.642)
Epoch: [197][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.2705 (0.2210) ([0.150]+[0.120])	Prec@1 95.312 (96.657)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.4339 (0.4339) ([0.314]+[0.120])	Prec@1 89.844 (89.844)
 * Prec@1 90.390
current lr 1.00000e-02
Grad=  tensor(8.4550, device='cuda:0')
Epoch: [198][0/391]	Time 0.306 (0.306)	Data 0.186 (0.186)	Loss 0.2380 (0.2380) ([0.118]+[0.120])	Prec@1 94.531 (94.531)
Epoch: [198][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1946 (0.2306) ([0.074]+[0.120])	Prec@1 97.656 (96.303)
Epoch: [198][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.1851 (0.2322) ([0.065]+[0.120])	Prec@1 97.656 (96.222)
Epoch: [198][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2052 (0.2297) ([0.085]+[0.120])	Prec@1 96.875 (96.304)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.5137 (0.5137) ([0.393]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(7.3128, device='cuda:0')
Epoch: [199][0/391]	Time 0.323 (0.323)	Data 0.203 (0.203)	Loss 0.2029 (0.2029) ([0.083]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [199][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2748 (0.2226) ([0.155]+[0.120])	Prec@1 95.312 (96.488)
Epoch: [199][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2878 (0.2228) ([0.167]+[0.120])	Prec@1 94.531 (96.510)
Epoch: [199][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2137 (0.2257) ([0.093]+[0.120])	Prec@1 96.875 (96.382)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4467 (0.4467) ([0.326]+[0.120])	Prec@1 87.500 (87.500)
 * Prec@1 90.550
current lr 1.00000e-02
Grad=  tensor(7.6859, device='cuda:0')
Epoch: [200][0/391]	Time 0.346 (0.346)	Data 0.221 (0.221)	Loss 0.1986 (0.1986) ([0.078]+[0.120])	Prec@1 97.656 (97.656)
Epoch: [200][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1822 (0.2162) ([0.062]+[0.120])	Prec@1 97.656 (96.829)
Epoch: [200][200/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 0.2310 (0.2208) ([0.111]+[0.120])	Prec@1 97.656 (96.626)
Epoch: [200][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2378 (0.2224) ([0.118]+[0.120])	Prec@1 95.312 (96.602)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.4096 (0.4096) ([0.289]+[0.120])	Prec@1 94.531 (94.531)
 * Prec@1 89.930
current lr 1.00000e-02
Grad=  tensor(6.5707, device='cuda:0')
Epoch: [201][0/391]	Time 0.335 (0.335)	Data 0.215 (0.215)	Loss 0.2467 (0.2467) ([0.126]+[0.120])	Prec@1 98.438 (98.438)
Epoch: [201][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1718 (0.2311) ([0.051]+[0.120])	Prec@1 97.656 (96.248)
Epoch: [201][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2039 (0.2288) ([0.083]+[0.120])	Prec@1 97.656 (96.354)
Epoch: [201][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2335 (0.2258) ([0.113]+[0.120])	Prec@1 96.094 (96.486)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.3711 (0.3711) ([0.251]+[0.120])	Prec@1 92.188 (92.188)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(5.1069, device='cuda:0')
Epoch: [202][0/391]	Time 0.344 (0.344)	Data 0.219 (0.219)	Loss 0.2106 (0.2106) ([0.090]+[0.120])	Prec@1 96.875 (96.875)
Epoch: [202][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.2496 (0.2203) ([0.129]+[0.120])	Prec@1 96.094 (96.558)
Epoch: [202][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2214 (0.2214) ([0.101]+[0.120])	Prec@1 95.312 (96.455)
Epoch: [202][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2589 (0.2214) ([0.139]+[0.120])	Prec@1 94.531 (96.444)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3190 (0.3190) ([0.199]+[0.120])	Prec@1 90.625 (90.625)
 * Prec@1 89.840
current lr 1.00000e-02
Grad=  tensor(5.8426, device='cuda:0')
Epoch: [203][0/391]	Time 0.360 (0.360)	Data 0.234 (0.234)	Loss 0.2092 (0.2092) ([0.089]+[0.120])	Prec@1 97.656 (97.656)
Epoch: [203][100/391]	Time 0.113 (0.117)	Data 0.000 (0.002)	Loss 0.2268 (0.2201) ([0.106]+[0.120])	Prec@1 93.750 (96.504)
Epoch: [203][200/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.2384 (0.2298) ([0.118]+[0.121])	Prec@1 95.312 (96.164)
Epoch: [203][300/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2010 (0.2298) ([0.081]+[0.120])	Prec@1 97.656 (96.218)
Test: [0/79]	Time 0.258 (0.258)	Loss 0.3782 (0.3782) ([0.258]+[0.120])	Prec@1 90.625 (90.625)
 * Prec@1 89.410
current lr 1.00000e-02
Grad=  tensor(13.6192, device='cuda:0')
Epoch: [204][0/391]	Time 0.335 (0.335)	Data 0.214 (0.214)	Loss 0.3150 (0.3150) ([0.194]+[0.120])	Prec@1 93.750 (93.750)
Epoch: [204][100/391]	Time 0.114 (0.113)	Data 0.000 (0.002)	Loss 0.2098 (0.2263) ([0.089]+[0.121])	Prec@1 96.875 (96.295)
Epoch: [204][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2171 (0.2259) ([0.097]+[0.121])	Prec@1 96.094 (96.288)
Epoch: [204][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2075 (0.2248) ([0.087]+[0.120])	Prec@1 96.875 (96.358)
Test: [0/79]	Time 0.265 (0.265)	Loss 0.4665 (0.4665) ([0.346]+[0.121])	Prec@1 89.844 (89.844)
 * Prec@1 90.300
current lr 1.00000e-02
Grad=  tensor(6.5324, device='cuda:0')
Epoch: [205][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.2056 (0.2056) ([0.085]+[0.121])	Prec@1 95.312 (95.312)
Epoch: [205][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.2286 (0.2259) ([0.108]+[0.121])	Prec@1 96.875 (96.272)
Epoch: [205][200/391]	Time 0.109 (0.112)	Data 0.000 (0.001)	Loss 0.1893 (0.2249) ([0.069]+[0.121])	Prec@1 96.875 (96.343)
Epoch: [205][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2754 (0.2272) ([0.155]+[0.121])	Prec@1 94.531 (96.286)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3810 (0.3810) ([0.260]+[0.121])	Prec@1 92.188 (92.188)
 * Prec@1 90.200
current lr 1.00000e-02
Grad=  tensor(7.1434, device='cuda:0')
Epoch: [206][0/391]	Time 0.333 (0.333)	Data 0.213 (0.213)	Loss 0.2133 (0.2133) ([0.093]+[0.121])	Prec@1 96.094 (96.094)
Epoch: [206][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2122 (0.2213) ([0.091]+[0.121])	Prec@1 96.094 (96.434)
Epoch: [206][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2757 (0.2212) ([0.155]+[0.121])	Prec@1 96.094 (96.451)
Epoch: [206][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2173 (0.2263) ([0.096]+[0.121])	Prec@1 95.312 (96.275)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.4503 (0.4503) ([0.329]+[0.121])	Prec@1 89.844 (89.844)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(11.6069, device='cuda:0')
Epoch: [207][0/391]	Time 0.330 (0.330)	Data 0.205 (0.205)	Loss 0.2514 (0.2514) ([0.130]+[0.121])	Prec@1 96.094 (96.094)
Epoch: [207][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1766 (0.2295) ([0.056]+[0.121])	Prec@1 98.438 (96.140)
Epoch: [207][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2570 (0.2310) ([0.136]+[0.121])	Prec@1 95.312 (96.094)
Epoch: [207][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2122 (0.2320) ([0.091]+[0.121])	Prec@1 96.875 (96.089)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3577 (0.3577) ([0.236]+[0.121])	Prec@1 95.312 (95.312)
 * Prec@1 90.630
current lr 1.00000e-02
Grad=  tensor(5.4977, device='cuda:0')
Epoch: [208][0/391]	Time 0.304 (0.304)	Data 0.180 (0.180)	Loss 0.2108 (0.2108) ([0.090]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [208][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.2379 (0.2151) ([0.117]+[0.121])	Prec@1 96.094 (96.952)
Epoch: [208][200/391]	Time 0.116 (0.115)	Data 0.000 (0.001)	Loss 0.2873 (0.2194) ([0.166]+[0.121])	Prec@1 96.875 (96.681)
Epoch: [208][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2352 (0.2221) ([0.114]+[0.121])	Prec@1 96.094 (96.553)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.3390 (0.3390) ([0.218]+[0.121])	Prec@1 91.406 (91.406)
 * Prec@1 90.960
current lr 1.00000e-02
Grad=  tensor(3.1849, device='cuda:0')
Epoch: [209][0/391]	Time 0.330 (0.330)	Data 0.206 (0.206)	Loss 0.1694 (0.1694) ([0.048]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [209][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1919 (0.2225) ([0.071]+[0.121])	Prec@1 96.875 (96.504)
Epoch: [209][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2258 (0.2265) ([0.105]+[0.121])	Prec@1 96.094 (96.273)
Epoch: [209][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1875 (0.2251) ([0.066]+[0.121])	Prec@1 97.656 (96.327)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.3443 (0.3443) ([0.223]+[0.121])	Prec@1 89.062 (89.062)
 * Prec@1 90.060
current lr 1.00000e-02
Grad=  tensor(7.5077, device='cuda:0')
Epoch: [210][0/391]	Time 0.354 (0.354)	Data 0.229 (0.229)	Loss 0.2032 (0.2032) ([0.082]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [210][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.2031 (0.2153) ([0.082]+[0.121])	Prec@1 96.875 (96.705)
Epoch: [210][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.2790 (0.2183) ([0.158]+[0.121])	Prec@1 95.312 (96.599)
Epoch: [210][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2217 (0.2201) ([0.101]+[0.121])	Prec@1 97.656 (96.535)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4696 (0.4696) ([0.349]+[0.121])	Prec@1 92.969 (92.969)
 * Prec@1 90.080
current lr 1.00000e-02
Grad=  tensor(4.9742, device='cuda:0')
Epoch: [211][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.1741 (0.1741) ([0.053]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [211][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1891 (0.2240) ([0.068]+[0.121])	Prec@1 97.656 (96.395)
Epoch: [211][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2270 (0.2274) ([0.106]+[0.121])	Prec@1 96.094 (96.339)
Epoch: [211][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2213 (0.2286) ([0.100]+[0.121])	Prec@1 96.875 (96.291)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.4130 (0.4130) ([0.292]+[0.121])	Prec@1 88.281 (88.281)
 * Prec@1 87.850
current lr 1.00000e-02
Grad=  tensor(10.7816, device='cuda:0')
Epoch: [212][0/391]	Time 0.351 (0.351)	Data 0.227 (0.227)	Loss 0.2613 (0.2613) ([0.140]+[0.121])	Prec@1 96.094 (96.094)
Epoch: [212][100/391]	Time 0.114 (0.114)	Data 0.000 (0.002)	Loss 0.1653 (0.2171) ([0.044]+[0.121])	Prec@1 98.438 (96.798)
Epoch: [212][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1853 (0.2193) ([0.064]+[0.121])	Prec@1 96.875 (96.692)
Epoch: [212][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2854 (0.2187) ([0.164]+[0.121])	Prec@1 95.312 (96.686)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3831 (0.3831) ([0.262]+[0.121])	Prec@1 92.969 (92.969)
 * Prec@1 90.450
current lr 1.00000e-02
Grad=  tensor(10.2871, device='cuda:0')
Epoch: [213][0/391]	Time 0.356 (0.356)	Data 0.232 (0.232)	Loss 0.2474 (0.2474) ([0.126]+[0.121])	Prec@1 94.531 (94.531)
Epoch: [213][100/391]	Time 0.108 (0.114)	Data 0.000 (0.002)	Loss 0.1844 (0.2138) ([0.063]+[0.121])	Prec@1 97.656 (96.744)
Epoch: [213][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2515 (0.2134) ([0.131]+[0.121])	Prec@1 96.094 (96.821)
Epoch: [213][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2375 (0.2187) ([0.116]+[0.121])	Prec@1 95.312 (96.641)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.3571 (0.3571) ([0.236]+[0.121])	Prec@1 92.188 (92.188)
 * Prec@1 89.980
current lr 1.00000e-02
Grad=  tensor(4.6745, device='cuda:0')
Epoch: [214][0/391]	Time 0.358 (0.358)	Data 0.234 (0.234)	Loss 0.2004 (0.2004) ([0.079]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [214][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.2622 (0.2214) ([0.141]+[0.121])	Prec@1 96.875 (96.713)
Epoch: [214][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1995 (0.2191) ([0.078]+[0.121])	Prec@1 96.875 (96.770)
Epoch: [214][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2372 (0.2203) ([0.116]+[0.121])	Prec@1 95.312 (96.693)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3882 (0.3882) ([0.267]+[0.121])	Prec@1 91.406 (91.406)
 * Prec@1 90.580
current lr 1.00000e-02
Grad=  tensor(5.9635, device='cuda:0')
Epoch: [215][0/391]	Time 0.334 (0.334)	Data 0.215 (0.215)	Loss 0.2026 (0.2026) ([0.082]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [215][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.2257 (0.2083) ([0.105]+[0.121])	Prec@1 95.312 (97.223)
Epoch: [215][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2068 (0.2166) ([0.086]+[0.121])	Prec@1 98.438 (96.832)
Epoch: [215][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2949 (0.2210) ([0.174]+[0.121])	Prec@1 93.750 (96.595)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.5672 (0.5672) ([0.446]+[0.121])	Prec@1 88.281 (88.281)
 * Prec@1 90.080
current lr 1.00000e-02
Grad=  tensor(8.9511, device='cuda:0')
Epoch: [216][0/391]	Time 0.315 (0.315)	Data 0.194 (0.194)	Loss 0.2431 (0.2431) ([0.122]+[0.121])	Prec@1 96.094 (96.094)
Epoch: [216][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.1994 (0.2107) ([0.078]+[0.121])	Prec@1 98.438 (96.867)
Epoch: [216][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2321 (0.2186) ([0.111]+[0.121])	Prec@1 96.094 (96.611)
Epoch: [216][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.3453 (0.2237) ([0.224]+[0.121])	Prec@1 87.500 (96.416)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4487 (0.4487) ([0.328]+[0.121])	Prec@1 89.844 (89.844)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(12.7220, device='cuda:0')
Epoch: [217][0/391]	Time 0.327 (0.327)	Data 0.206 (0.206)	Loss 0.3218 (0.3218) ([0.201]+[0.121])	Prec@1 94.531 (94.531)
Epoch: [217][100/391]	Time 0.115 (0.113)	Data 0.000 (0.002)	Loss 0.1927 (0.2154) ([0.072]+[0.121])	Prec@1 97.656 (96.945)
Epoch: [217][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2337 (0.2177) ([0.113]+[0.121])	Prec@1 95.312 (96.723)
Epoch: [217][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.3725 (0.2240) ([0.251]+[0.121])	Prec@1 93.750 (96.460)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.4550 (0.4550) ([0.334]+[0.121])	Prec@1 89.844 (89.844)
 * Prec@1 90.450
current lr 1.00000e-02
Grad=  tensor(8.7508, device='cuda:0')
Epoch: [218][0/391]	Time 0.348 (0.348)	Data 0.223 (0.223)	Loss 0.2259 (0.2259) ([0.105]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [218][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.1587 (0.2163) ([0.038]+[0.121])	Prec@1 100.000 (96.643)
Epoch: [218][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2106 (0.2160) ([0.090]+[0.121])	Prec@1 96.094 (96.673)
Epoch: [218][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2548 (0.2191) ([0.134]+[0.121])	Prec@1 92.969 (96.532)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.4246 (0.4246) ([0.303]+[0.121])	Prec@1 87.500 (87.500)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(11.1579, device='cuda:0')
Epoch: [219][0/391]	Time 0.337 (0.337)	Data 0.216 (0.216)	Loss 0.2173 (0.2173) ([0.096]+[0.121])	Prec@1 95.312 (95.312)
Epoch: [219][100/391]	Time 0.109 (0.113)	Data 0.000 (0.002)	Loss 0.2286 (0.2266) ([0.107]+[0.121])	Prec@1 93.750 (96.187)
Epoch: [219][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2663 (0.2249) ([0.145]+[0.121])	Prec@1 95.312 (96.319)
Epoch: [219][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2587 (0.2230) ([0.138]+[0.121])	Prec@1 94.531 (96.405)
Test: [0/79]	Time 0.264 (0.264)	Loss 0.5285 (0.5285) ([0.407]+[0.121])	Prec@1 91.406 (91.406)
 * Prec@1 88.260
current lr 1.00000e-02
Grad=  tensor(4.4848, device='cuda:0')
Epoch: [220][0/391]	Time 0.352 (0.352)	Data 0.226 (0.226)	Loss 0.1747 (0.1747) ([0.053]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [220][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2056 (0.2140) ([0.085]+[0.121])	Prec@1 97.656 (96.875)
Epoch: [220][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1944 (0.2183) ([0.073]+[0.121])	Prec@1 96.875 (96.688)
Epoch: [220][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2703 (0.2201) ([0.149]+[0.121])	Prec@1 93.750 (96.618)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.3658 (0.3658) ([0.245]+[0.121])	Prec@1 92.969 (92.969)
 * Prec@1 90.310
current lr 1.00000e-02
Grad=  tensor(2.8730, device='cuda:0')
Epoch: [221][0/391]	Time 0.338 (0.338)	Data 0.217 (0.217)	Loss 0.1647 (0.1647) ([0.044]+[0.121])	Prec@1 99.219 (99.219)
Epoch: [221][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2078 (0.2158) ([0.087]+[0.121])	Prec@1 96.875 (96.751)
Epoch: [221][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1958 (0.2170) ([0.075]+[0.121])	Prec@1 97.656 (96.646)
Epoch: [221][300/391]	Time 0.115 (0.111)	Data 0.000 (0.001)	Loss 0.2019 (0.2209) ([0.081]+[0.121])	Prec@1 98.438 (96.509)
Test: [0/79]	Time 0.263 (0.263)	Loss 0.4298 (0.4298) ([0.309]+[0.121])	Prec@1 93.750 (93.750)
 * Prec@1 90.690
current lr 1.00000e-02
Grad=  tensor(8.6544, device='cuda:0')
Epoch: [222][0/391]	Time 0.346 (0.346)	Data 0.224 (0.224)	Loss 0.1865 (0.1865) ([0.065]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [222][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.2526 (0.2104) ([0.132]+[0.121])	Prec@1 95.312 (96.921)
Epoch: [222][200/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2108 (0.2119) ([0.090]+[0.121])	Prec@1 96.875 (96.879)
Epoch: [222][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2551 (0.2188) ([0.134]+[0.121])	Prec@1 92.969 (96.647)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.4143 (0.4143) ([0.293]+[0.121])	Prec@1 93.750 (93.750)
 * Prec@1 90.350
current lr 1.00000e-02
Grad=  tensor(6.6566, device='cuda:0')
Epoch: [223][0/391]	Time 0.326 (0.326)	Data 0.208 (0.208)	Loss 0.2310 (0.2310) ([0.110]+[0.121])	Prec@1 95.312 (95.312)
Epoch: [223][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1991 (0.2192) ([0.078]+[0.121])	Prec@1 97.656 (96.573)
Epoch: [223][200/391]	Time 0.114 (0.110)	Data 0.000 (0.001)	Loss 0.1815 (0.2217) ([0.060]+[0.121])	Prec@1 97.656 (96.510)
Epoch: [223][300/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.1944 (0.2233) ([0.073]+[0.121])	Prec@1 96.875 (96.462)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.3026 (0.3026) ([0.181]+[0.121])	Prec@1 92.969 (92.969)
 * Prec@1 90.980
current lr 1.00000e-02
Grad=  tensor(8.3666, device='cuda:0')
Epoch: [224][0/391]	Time 0.310 (0.310)	Data 0.185 (0.185)	Loss 0.2232 (0.2232) ([0.102]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [224][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2772 (0.2221) ([0.156]+[0.121])	Prec@1 93.750 (96.643)
Epoch: [224][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.2160 (0.2173) ([0.095]+[0.121])	Prec@1 96.094 (96.813)
Epoch: [224][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2114 (0.2165) ([0.090]+[0.121])	Prec@1 97.656 (96.844)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.4076 (0.4076) ([0.286]+[0.121])	Prec@1 90.625 (90.625)
 * Prec@1 91.010
current lr 1.00000e-02
Grad=  tensor(7.1733, device='cuda:0')
Epoch: [225][0/391]	Time 0.349 (0.349)	Data 0.224 (0.224)	Loss 0.1989 (0.1989) ([0.078]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [225][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.2475 (0.2078) ([0.126]+[0.121])	Prec@1 95.312 (97.037)
Epoch: [225][200/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.2047 (0.2163) ([0.083]+[0.121])	Prec@1 97.656 (96.622)
Epoch: [225][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2644 (0.2218) ([0.143]+[0.121])	Prec@1 94.531 (96.483)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.4772 (0.4772) ([0.356]+[0.121])	Prec@1 89.844 (89.844)
 * Prec@1 89.830
current lr 1.00000e-02
Grad=  tensor(7.3593, device='cuda:0')
Epoch: [226][0/391]	Time 0.355 (0.355)	Data 0.231 (0.231)	Loss 0.2196 (0.2196) ([0.098]+[0.121])	Prec@1 97.656 (97.656)
Epoch: [226][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2733 (0.2200) ([0.152]+[0.121])	Prec@1 94.531 (96.651)
Epoch: [226][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2116 (0.2246) ([0.090]+[0.121])	Prec@1 96.875 (96.537)
Epoch: [226][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1823 (0.2217) ([0.061]+[0.121])	Prec@1 98.438 (96.597)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.4320 (0.4320) ([0.311]+[0.121])	Prec@1 90.625 (90.625)
 * Prec@1 89.660
current lr 1.00000e-02
Grad=  tensor(4.5273, device='cuda:0')
Epoch: [227][0/391]	Time 0.358 (0.358)	Data 0.234 (0.234)	Loss 0.1885 (0.1885) ([0.067]+[0.121])	Prec@1 98.438 (98.438)
Epoch: [227][100/391]	Time 0.108 (0.113)	Data 0.000 (0.002)	Loss 0.2040 (0.2179) ([0.083]+[0.121])	Prec@1 98.438 (96.713)
Epoch: [227][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1922 (0.2174) ([0.071]+[0.121])	Prec@1 97.656 (96.696)
Epoch: [227][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2630 (0.2244) ([0.142]+[0.121])	Prec@1 94.531 (96.436)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4079 (0.4079) ([0.286]+[0.122])	Prec@1 92.969 (92.969)
 * Prec@1 91.430
current lr 1.00000e-02
Grad=  tensor(3.6065, device='cuda:0')
Epoch: [228][0/391]	Time 0.333 (0.333)	Data 0.214 (0.214)	Loss 0.1802 (0.1802) ([0.059]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [228][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3786 (0.2103) ([0.257]+[0.121])	Prec@1 91.406 (97.014)
Epoch: [228][200/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.1961 (0.2144) ([0.075]+[0.121])	Prec@1 99.219 (96.859)
Epoch: [228][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.1991 (0.2156) ([0.078]+[0.121])	Prec@1 96.875 (96.795)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.5035 (0.5035) ([0.382]+[0.121])	Prec@1 90.625 (90.625)
 * Prec@1 88.890
current lr 1.00000e-02
Grad=  tensor(8.3296, device='cuda:0')
Epoch: [229][0/391]	Time 0.341 (0.341)	Data 0.217 (0.217)	Loss 0.2219 (0.2219) ([0.100]+[0.121])	Prec@1 95.312 (95.312)
Epoch: [229][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.2486 (0.2148) ([0.127]+[0.121])	Prec@1 96.094 (96.774)
Epoch: [229][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2450 (0.2147) ([0.124]+[0.121])	Prec@1 95.312 (96.856)
Epoch: [229][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2135 (0.2178) ([0.092]+[0.121])	Prec@1 96.094 (96.699)
Test: [0/79]	Time 0.252 (0.252)	Loss 0.4738 (0.4738) ([0.352]+[0.122])	Prec@1 91.406 (91.406)
 * Prec@1 90.800
current lr 1.00000e-02
Grad=  tensor(8.2568, device='cuda:0')
Epoch: [230][0/391]	Time 0.358 (0.358)	Data 0.234 (0.234)	Loss 0.2111 (0.2111) ([0.089]+[0.122])	Prec@1 95.312 (95.312)
Epoch: [230][100/391]	Time 0.109 (0.114)	Data 0.000 (0.002)	Loss 0.2287 (0.2194) ([0.107]+[0.122])	Prec@1 96.094 (96.736)
Epoch: [230][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2168 (0.2164) ([0.095]+[0.121])	Prec@1 97.656 (96.863)
Epoch: [230][300/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.2659 (0.2184) ([0.144]+[0.121])	Prec@1 96.094 (96.758)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.4361 (0.4361) ([0.315]+[0.121])	Prec@1 90.625 (90.625)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(5.8663, device='cuda:0')
Epoch: [231][0/391]	Time 0.332 (0.332)	Data 0.212 (0.212)	Loss 0.1802 (0.1802) ([0.059]+[0.121])	Prec@1 96.875 (96.875)
Epoch: [231][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1589 (0.2212) ([0.037]+[0.121])	Prec@1 100.000 (96.627)
Epoch: [231][200/391]	Time 0.110 (0.109)	Data 0.000 (0.001)	Loss 0.3307 (0.2166) ([0.209]+[0.121])	Prec@1 92.969 (96.735)
Epoch: [231][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.2091 (0.2197) ([0.088]+[0.121])	Prec@1 96.094 (96.610)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4460 (0.4460) ([0.324]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 89.160
current lr 1.00000e-02
Grad=  tensor(4.3126, device='cuda:0')
Epoch: [232][0/391]	Time 0.303 (0.303)	Data 0.182 (0.182)	Loss 0.1774 (0.1774) ([0.056]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [232][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.2109 (0.2165) ([0.089]+[0.121])	Prec@1 96.875 (96.604)
Epoch: [232][200/391]	Time 0.115 (0.112)	Data 0.000 (0.001)	Loss 0.1900 (0.2176) ([0.069]+[0.121])	Prec@1 98.438 (96.576)
Epoch: [232][300/391]	Time 0.115 (0.113)	Data 0.000 (0.001)	Loss 0.2431 (0.2226) ([0.121]+[0.122])	Prec@1 96.094 (96.434)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.4656 (0.4656) ([0.344]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 90.470
current lr 1.00000e-02
Grad=  tensor(7.5931, device='cuda:0')
Epoch: [233][0/391]	Time 0.341 (0.341)	Data 0.216 (0.216)	Loss 0.2396 (0.2396) ([0.118]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [233][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.2076 (0.2205) ([0.086]+[0.122])	Prec@1 96.875 (96.658)
Epoch: [233][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.2200 (0.2213) ([0.098]+[0.122])	Prec@1 97.656 (96.514)
Epoch: [233][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1958 (0.2205) ([0.074]+[0.122])	Prec@1 97.656 (96.535)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.5386 (0.5386) ([0.417]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 89.470
current lr 1.00000e-02
Grad=  tensor(4.5871, device='cuda:0')
Epoch: [234][0/391]	Time 0.356 (0.356)	Data 0.231 (0.231)	Loss 0.1955 (0.1955) ([0.074]+[0.122])	Prec@1 97.656 (97.656)
Epoch: [234][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1893 (0.2187) ([0.068]+[0.122])	Prec@1 97.656 (96.867)
Epoch: [234][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1966 (0.2170) ([0.075]+[0.122])	Prec@1 97.656 (96.797)
Epoch: [234][300/391]	Time 0.113 (0.115)	Data 0.000 (0.001)	Loss 0.1855 (0.2193) ([0.064]+[0.122])	Prec@1 98.438 (96.693)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4422 (0.4422) ([0.320]+[0.122])	Prec@1 91.406 (91.406)
 * Prec@1 89.440
current lr 1.00000e-02
Grad=  tensor(15.1450, device='cuda:0')
Epoch: [235][0/391]	Time 0.352 (0.352)	Data 0.226 (0.226)	Loss 0.2460 (0.2460) ([0.124]+[0.122])	Prec@1 92.969 (92.969)
Epoch: [235][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.2838 (0.2288) ([0.162]+[0.122])	Prec@1 95.312 (96.442)
Epoch: [235][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2362 (0.2225) ([0.114]+[0.122])	Prec@1 93.750 (96.657)
Epoch: [235][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2305 (0.2258) ([0.109]+[0.122])	Prec@1 97.656 (96.519)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.3728 (0.3728) ([0.251]+[0.122])	Prec@1 95.312 (95.312)
 * Prec@1 90.600
current lr 1.00000e-02
Grad=  tensor(9.3692, device='cuda:0')
Epoch: [236][0/391]	Time 0.361 (0.361)	Data 0.237 (0.237)	Loss 0.2086 (0.2086) ([0.087]+[0.122])	Prec@1 97.656 (97.656)
Epoch: [236][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1729 (0.2091) ([0.051]+[0.122])	Prec@1 98.438 (97.053)
Epoch: [236][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1966 (0.2161) ([0.075]+[0.122])	Prec@1 98.438 (96.821)
Epoch: [236][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.1954 (0.2180) ([0.074]+[0.122])	Prec@1 97.656 (96.745)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.3172 (0.3172) ([0.195]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 90.940
current lr 1.00000e-02
Grad=  tensor(4.9339, device='cuda:0')
Epoch: [237][0/391]	Time 0.361 (0.361)	Data 0.236 (0.236)	Loss 0.1896 (0.1896) ([0.068]+[0.122])	Prec@1 97.656 (97.656)
Epoch: [237][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1849 (0.2115) ([0.063]+[0.122])	Prec@1 97.656 (97.107)
Epoch: [237][200/391]	Time 0.113 (0.116)	Data 0.000 (0.001)	Loss 0.2822 (0.2151) ([0.161]+[0.122])	Prec@1 92.969 (96.902)
Epoch: [237][300/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.1945 (0.2197) ([0.073]+[0.122])	Prec@1 96.875 (96.706)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.4489 (0.4489) ([0.327]+[0.122])	Prec@1 92.969 (92.969)
 * Prec@1 89.640
current lr 1.00000e-02
Grad=  tensor(6.9123, device='cuda:0')
Epoch: [238][0/391]	Time 0.334 (0.334)	Data 0.213 (0.213)	Loss 0.2189 (0.2189) ([0.097]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [238][100/391]	Time 0.111 (0.113)	Data 0.000 (0.002)	Loss 0.2319 (0.2159) ([0.110]+[0.122])	Prec@1 96.875 (96.883)
Epoch: [238][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2028 (0.2136) ([0.081]+[0.121])	Prec@1 96.094 (96.883)
Epoch: [238][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2323 (0.2164) ([0.111]+[0.122])	Prec@1 96.094 (96.771)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4071 (0.4071) ([0.285]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 89.290
current lr 1.00000e-02
Grad=  tensor(8.2286, device='cuda:0')
Epoch: [239][0/391]	Time 0.328 (0.328)	Data 0.207 (0.207)	Loss 0.2165 (0.2165) ([0.095]+[0.122])	Prec@1 94.531 (94.531)
Epoch: [239][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.2338 (0.2189) ([0.112]+[0.122])	Prec@1 95.312 (96.651)
Epoch: [239][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1807 (0.2178) ([0.059]+[0.122])	Prec@1 97.656 (96.762)
Epoch: [239][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.2345 (0.2230) ([0.113]+[0.122])	Prec@1 96.094 (96.574)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.3691 (0.3691) ([0.247]+[0.122])	Prec@1 91.406 (91.406)
 * Prec@1 90.700
current lr 1.00000e-02
Grad=  tensor(7.2884, device='cuda:0')
Epoch: [240][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.1867 (0.1867) ([0.065]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [240][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2407 (0.2205) ([0.119]+[0.122])	Prec@1 94.531 (96.635)
Epoch: [240][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1944 (0.2223) ([0.073]+[0.122])	Prec@1 99.219 (96.475)
Epoch: [240][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.2447 (0.2227) ([0.123]+[0.122])	Prec@1 95.312 (96.488)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.4871 (0.4871) ([0.365]+[0.122])	Prec@1 91.406 (91.406)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(16.6774, device='cuda:0')
Epoch: [241][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.2916 (0.2916) ([0.170]+[0.122])	Prec@1 95.312 (95.312)
Epoch: [241][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.1541 (0.2146) ([0.032]+[0.122])	Prec@1 100.000 (96.844)
Epoch: [241][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.1995 (0.2131) ([0.078]+[0.122])	Prec@1 97.656 (96.883)
Epoch: [241][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2461 (0.2161) ([0.124]+[0.122])	Prec@1 95.312 (96.761)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3758 (0.3758) ([0.254]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(6.7302, device='cuda:0')
Epoch: [242][0/391]	Time 0.333 (0.333)	Data 0.212 (0.212)	Loss 0.2056 (0.2056) ([0.084]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [242][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2130 (0.2095) ([0.091]+[0.122])	Prec@1 95.312 (97.022)
Epoch: [242][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1595 (0.2139) ([0.038]+[0.122])	Prec@1 100.000 (96.879)
Epoch: [242][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2373 (0.2185) ([0.116]+[0.122])	Prec@1 96.094 (96.649)
Test: [0/79]	Time 0.256 (0.256)	Loss 0.4744 (0.4744) ([0.353]+[0.122])	Prec@1 88.281 (88.281)
 * Prec@1 90.710
current lr 1.00000e-02
Grad=  tensor(8.8295, device='cuda:0')
Epoch: [243][0/391]	Time 0.351 (0.351)	Data 0.225 (0.225)	Loss 0.2217 (0.2217) ([0.100]+[0.122])	Prec@1 99.219 (99.219)
Epoch: [243][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.1845 (0.2026) ([0.063]+[0.122])	Prec@1 99.219 (97.285)
Epoch: [243][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2470 (0.2111) ([0.125]+[0.122])	Prec@1 94.531 (96.953)
Epoch: [243][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2563 (0.2145) ([0.135]+[0.122])	Prec@1 96.094 (96.758)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.5717 (0.5717) ([0.450]+[0.122])	Prec@1 88.281 (88.281)
 * Prec@1 86.410
current lr 1.00000e-02
Grad=  tensor(7.5766, device='cuda:0')
Epoch: [244][0/391]	Time 0.331 (0.331)	Data 0.210 (0.210)	Loss 0.2075 (0.2075) ([0.086]+[0.122])	Prec@1 95.312 (95.312)
Epoch: [244][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.2234 (0.2160) ([0.102]+[0.122])	Prec@1 96.094 (96.852)
Epoch: [244][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.2517 (0.2154) ([0.130]+[0.122])	Prec@1 94.531 (96.805)
Epoch: [244][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.2094 (0.2182) ([0.088]+[0.122])	Prec@1 96.094 (96.680)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.4754 (0.4754) ([0.354]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 89.430
current lr 1.00000e-02
Grad=  tensor(8.1950, device='cuda:0')
Epoch: [245][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.1990 (0.1990) ([0.077]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [245][100/391]	Time 0.112 (0.115)	Data 0.000 (0.002)	Loss 0.2117 (0.2092) ([0.090]+[0.122])	Prec@1 97.656 (96.983)
Epoch: [245][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2503 (0.2121) ([0.129]+[0.122])	Prec@1 96.094 (96.891)
Epoch: [245][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1975 (0.2184) ([0.076]+[0.122])	Prec@1 97.656 (96.680)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.4528 (0.4528) ([0.331]+[0.122])	Prec@1 88.281 (88.281)
 * Prec@1 90.730
current lr 1.00000e-02
Grad=  tensor(3.0409, device='cuda:0')
Epoch: [246][0/391]	Time 0.334 (0.334)	Data 0.213 (0.213)	Loss 0.1631 (0.1631) ([0.041]+[0.122])	Prec@1 99.219 (99.219)
Epoch: [246][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1968 (0.2045) ([0.075]+[0.121])	Prec@1 97.656 (97.184)
Epoch: [246][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2370 (0.2096) ([0.116]+[0.121])	Prec@1 94.531 (96.972)
Epoch: [246][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1828 (0.2138) ([0.061]+[0.121])	Prec@1 97.656 (96.862)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.3076 (0.3076) ([0.186]+[0.122])	Prec@1 93.750 (93.750)
 * Prec@1 90.410
current lr 1.00000e-02
Grad=  tensor(6.9224, device='cuda:0')
Epoch: [247][0/391]	Time 0.302 (0.302)	Data 0.181 (0.181)	Loss 0.2484 (0.2484) ([0.127]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [247][100/391]	Time 0.114 (0.113)	Data 0.000 (0.002)	Loss 0.1928 (0.2167) ([0.071]+[0.122])	Prec@1 98.438 (96.875)
Epoch: [247][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.2973 (0.2156) ([0.176]+[0.121])	Prec@1 93.750 (96.856)
Epoch: [247][300/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.3186 (0.2187) ([0.197]+[0.122])	Prec@1 93.750 (96.771)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.4431 (0.4431) ([0.322]+[0.122])	Prec@1 89.062 (89.062)
 * Prec@1 90.990
current lr 1.00000e-02
Grad=  tensor(2.3248, device='cuda:0')
Epoch: [248][0/391]	Time 0.343 (0.343)	Data 0.218 (0.218)	Loss 0.1606 (0.1606) ([0.039]+[0.122])	Prec@1 99.219 (99.219)
Epoch: [248][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.1906 (0.2106) ([0.069]+[0.122])	Prec@1 98.438 (96.968)
Epoch: [248][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.2043 (0.2137) ([0.083]+[0.122])	Prec@1 96.875 (96.852)
Epoch: [248][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1996 (0.2170) ([0.078]+[0.122])	Prec@1 97.656 (96.709)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.3512 (0.3512) ([0.230]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 90.650
current lr 1.00000e-02
Grad=  tensor(4.2984, device='cuda:0')
Epoch: [249][0/391]	Time 0.336 (0.336)	Data 0.215 (0.215)	Loss 0.1873 (0.1873) ([0.066]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [249][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1789 (0.2105) ([0.057]+[0.122])	Prec@1 96.875 (97.130)
Epoch: [249][200/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1614 (0.2137) ([0.040]+[0.121])	Prec@1 99.219 (97.019)
Epoch: [249][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.2342 (0.2170) ([0.113]+[0.121])	Prec@1 96.875 (96.828)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.3757 (0.3757) ([0.254]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 91.050
current lr 1.00000e-03
Grad=  tensor(4.3889, device='cuda:0')
Epoch: [250][0/391]	Time 0.347 (0.347)	Data 0.221 (0.221)	Loss 0.1799 (0.1799) ([0.058]+[0.122])	Prec@1 99.219 (99.219)
Epoch: [250][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1372 (0.1840) ([0.016]+[0.121])	Prec@1 100.000 (97.981)
Epoch: [250][200/391]	Time 0.112 (0.115)	Data 0.000 (0.001)	Loss 0.2067 (0.1736) ([0.086]+[0.121])	Prec@1 96.094 (98.317)
Epoch: [250][300/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 0.1712 (0.1687) ([0.051]+[0.121])	Prec@1 97.656 (98.484)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2921 (0.2921) ([0.172]+[0.121])	Prec@1 94.531 (94.531)
 * Prec@1 93.630
current lr 1.00000e-03
Grad=  tensor(1.3593, device='cuda:0')
Epoch: [251][0/391]	Time 0.342 (0.342)	Data 0.220 (0.220)	Loss 0.1448 (0.1448) ([0.024]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [251][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1468 (0.1467) ([0.026]+[0.120])	Prec@1 99.219 (99.397)
Epoch: [251][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1440 (0.1473) ([0.024]+[0.120])	Prec@1 99.219 (99.339)
Epoch: [251][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1336 (0.1479) ([0.013]+[0.120])	Prec@1 100.000 (99.299)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2791 (0.2791) ([0.159]+[0.120])	Prec@1 94.531 (94.531)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.9808, device='cuda:0')
Epoch: [252][0/391]	Time 0.311 (0.311)	Data 0.190 (0.190)	Loss 0.1306 (0.1306) ([0.011]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [252][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1289 (0.1439) ([0.009]+[0.120])	Prec@1 100.000 (99.358)
Epoch: [252][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1471 (0.1424) ([0.027]+[0.120])	Prec@1 99.219 (99.452)
Epoch: [252][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1322 (0.1421) ([0.012]+[0.120])	Prec@1 100.000 (99.452)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2813 (0.2813) ([0.162]+[0.120])	Prec@1 95.312 (95.312)
 * Prec@1 93.650
current lr 1.00000e-03
Grad=  tensor(2.6431, device='cuda:0')
Epoch: [253][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.1516 (0.1516) ([0.032]+[0.120])	Prec@1 99.219 (99.219)
Epoch: [253][100/391]	Time 0.110 (0.115)	Data 0.000 (0.002)	Loss 0.1382 (0.1380) ([0.019]+[0.120])	Prec@1 99.219 (99.598)
Epoch: [253][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.1232 (0.1384) ([0.004]+[0.120])	Prec@1 100.000 (99.607)
Epoch: [253][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1380 (0.1380) ([0.019]+[0.119])	Prec@1 100.000 (99.611)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2694 (0.2694) ([0.150]+[0.119])	Prec@1 95.312 (95.312)
 * Prec@1 93.850
current lr 1.00000e-03
Grad=  tensor(0.5953, device='cuda:0')
Epoch: [254][0/391]	Time 0.332 (0.332)	Data 0.213 (0.213)	Loss 0.1296 (0.1296) ([0.010]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1354 (0.1348) ([0.016]+[0.119])	Prec@1 100.000 (99.667)
Epoch: [254][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1373 (0.1358) ([0.018]+[0.119])	Prec@1 99.219 (99.584)
Epoch: [254][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1450 (0.1362) ([0.026]+[0.119])	Prec@1 99.219 (99.569)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.2849 (0.2849) ([0.166]+[0.119])	Prec@1 96.094 (96.094)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.2080, device='cuda:0')
Epoch: [255][0/391]	Time 0.307 (0.307)	Data 0.188 (0.188)	Loss 0.1272 (0.1272) ([0.008]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.1271 (0.1348) ([0.008]+[0.119])	Prec@1 100.000 (99.606)
Epoch: [255][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1272 (0.1352) ([0.008]+[0.119])	Prec@1 100.000 (99.588)
Epoch: [255][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1374 (0.1348) ([0.019]+[0.119])	Prec@1 99.219 (99.631)
Test: [0/79]	Time 0.253 (0.253)	Loss 0.2818 (0.2818) ([0.163]+[0.119])	Prec@1 95.312 (95.312)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(2.0055, device='cuda:0')
Epoch: [256][0/391]	Time 0.339 (0.339)	Data 0.216 (0.216)	Loss 0.1432 (0.1432) ([0.025]+[0.119])	Prec@1 99.219 (99.219)
Epoch: [256][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1456 (0.1330) ([0.027]+[0.119])	Prec@1 99.219 (99.660)
Epoch: [256][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1234 (0.1324) ([0.005]+[0.118])	Prec@1 100.000 (99.697)
Epoch: [256][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1442 (0.1323) ([0.026]+[0.118])	Prec@1 99.219 (99.704)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.2707 (0.2707) ([0.152]+[0.118])	Prec@1 95.312 (95.312)
 * Prec@1 93.940
current lr 1.00000e-03
Grad=  tensor(2.0546, device='cuda:0')
Epoch: [257][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 0.1357 (0.1357) ([0.017]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [257][100/391]	Time 0.113 (0.116)	Data 0.000 (0.002)	Loss 0.1224 (0.1296) ([0.004]+[0.118])	Prec@1 100.000 (99.729)
Epoch: [257][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1237 (0.1300) ([0.006]+[0.118])	Prec@1 100.000 (99.771)
Epoch: [257][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1245 (0.1305) ([0.007]+[0.118])	Prec@1 100.000 (99.751)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.2835 (0.2835) ([0.166]+[0.118])	Prec@1 96.094 (96.094)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(2.9396, device='cuda:0')
Epoch: [258][0/391]	Time 0.377 (0.377)	Data 0.240 (0.240)	Loss 0.1313 (0.1313) ([0.013]+[0.118])	Prec@1 99.219 (99.219)
Epoch: [258][100/391]	Time 0.115 (0.117)	Data 0.000 (0.003)	Loss 0.1229 (0.1287) ([0.005]+[0.118])	Prec@1 100.000 (99.791)
Epoch: [258][200/391]	Time 0.113 (0.116)	Data 0.001 (0.001)	Loss 0.1251 (0.1293) ([0.007]+[0.118])	Prec@1 100.000 (99.778)
Epoch: [258][300/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.1258 (0.1293) ([0.008]+[0.118])	Prec@1 100.000 (99.774)
Test: [0/79]	Time 0.250 (0.250)	Loss 0.2535 (0.2535) ([0.136]+[0.118])	Prec@1 96.094 (96.094)
 * Prec@1 93.990
current lr 1.00000e-03
Grad=  tensor(0.7651, device='cuda:0')
Epoch: [259][0/391]	Time 0.333 (0.333)	Data 0.213 (0.213)	Loss 0.1317 (0.1317) ([0.014]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [259][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1291 (0.1277) ([0.012]+[0.117])	Prec@1 100.000 (99.830)
Epoch: [259][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.1239 (0.1286) ([0.007]+[0.117])	Prec@1 100.000 (99.786)
Epoch: [259][300/391]	Time 0.111 (0.115)	Data 0.000 (0.001)	Loss 0.1316 (0.1289) ([0.014]+[0.117])	Prec@1 100.000 (99.774)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.2936 (0.2936) ([0.176]+[0.117])	Prec@1 93.750 (93.750)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.1422, device='cuda:0')
Epoch: [260][0/391]	Time 0.332 (0.332)	Data 0.212 (0.212)	Loss 0.1211 (0.1211) ([0.004]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1246 (0.1280) ([0.008]+[0.117])	Prec@1 100.000 (99.745)
Epoch: [260][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1274 (0.1279) ([0.010]+[0.117])	Prec@1 100.000 (99.782)
Epoch: [260][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1380 (0.1277) ([0.021]+[0.117])	Prec@1 99.219 (99.798)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2759 (0.2759) ([0.159]+[0.117])	Prec@1 96.094 (96.094)
 * Prec@1 94.120
current lr 1.00000e-03
Grad=  tensor(1.0151, device='cuda:0')
Epoch: [261][0/391]	Time 0.314 (0.314)	Data 0.193 (0.193)	Loss 0.1244 (0.1244) ([0.008]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1200 (0.1281) ([0.003]+[0.117])	Prec@1 100.000 (99.776)
Epoch: [261][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1340 (0.1277) ([0.017]+[0.117])	Prec@1 99.219 (99.798)
Epoch: [261][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1381 (0.1273) ([0.022]+[0.117])	Prec@1 99.219 (99.805)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.2624 (0.2624) ([0.146]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 94.130
current lr 1.00000e-03
Grad=  tensor(0.0914, device='cuda:0')
Epoch: [262][0/391]	Time 0.315 (0.315)	Data 0.195 (0.195)	Loss 0.1204 (0.1204) ([0.004]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1260 (0.1269) ([0.010]+[0.116])	Prec@1 100.000 (99.814)
Epoch: [262][200/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.1296 (0.1273) ([0.013]+[0.116])	Prec@1 99.219 (99.775)
Epoch: [262][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1244 (0.1269) ([0.008]+[0.116])	Prec@1 100.000 (99.787)
Test: [0/79]	Time 0.255 (0.255)	Loss 0.2611 (0.2611) ([0.145]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 94.090
current lr 1.00000e-03
Grad=  tensor(4.8607, device='cuda:0')
Epoch: [263][0/391]	Time 0.303 (0.303)	Data 0.182 (0.182)	Loss 0.1311 (0.1311) ([0.015]+[0.116])	Prec@1 99.219 (99.219)
Epoch: [263][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1202 (0.1254) ([0.004]+[0.116])	Prec@1 100.000 (99.869)
Epoch: [263][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1319 (0.1256) ([0.016]+[0.116])	Prec@1 100.000 (99.852)
Epoch: [263][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1186 (0.1252) ([0.003]+[0.116])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.259 (0.259)	Loss 0.2628 (0.2628) ([0.147]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 94.110
current lr 1.00000e-03
Grad=  tensor(0.0934, device='cuda:0')
Epoch: [264][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.1186 (0.1186) ([0.003]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1204 (0.1252) ([0.005]+[0.116])	Prec@1 100.000 (99.807)
Epoch: [264][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.1284 (0.1249) ([0.013]+[0.116])	Prec@1 99.219 (99.837)
Epoch: [264][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1308 (0.1248) ([0.015]+[0.116])	Prec@1 99.219 (99.834)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.2747 (0.2747) ([0.159]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 94.060
current lr 1.00000e-03
Grad=  tensor(0.7010, device='cuda:0')
Epoch: [265][0/391]	Time 0.347 (0.347)	Data 0.223 (0.223)	Loss 0.1233 (0.1233) ([0.008]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.109 (0.116)	Data 0.000 (0.002)	Loss 0.1229 (0.1234) ([0.008]+[0.115])	Prec@1 100.000 (99.869)
Epoch: [265][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.1282 (0.1239) ([0.013]+[0.115])	Prec@1 99.219 (99.829)
Epoch: [265][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1196 (0.1236) ([0.004]+[0.115])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2779 (0.2779) ([0.163]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.3634, device='cuda:0')
Epoch: [266][0/391]	Time 0.357 (0.357)	Data 0.233 (0.233)	Loss 0.1222 (0.1222) ([0.007]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1280 (0.1232) ([0.013]+[0.115])	Prec@1 100.000 (99.869)
Epoch: [266][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1405 (0.1230) ([0.026]+[0.115])	Prec@1 99.219 (99.868)
Epoch: [266][300/391]	Time 0.108 (0.113)	Data 0.000 (0.001)	Loss 0.1299 (0.1231) ([0.015]+[0.115])	Prec@1 99.219 (99.862)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3013 (0.3013) ([0.186]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 93.960
current lr 1.00000e-03
Grad=  tensor(0.3139, device='cuda:0')
Epoch: [267][0/391]	Time 0.356 (0.356)	Data 0.233 (0.233)	Loss 0.1196 (0.1196) ([0.005]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.109 (0.115)	Data 0.000 (0.002)	Loss 0.1186 (0.1225) ([0.004]+[0.115])	Prec@1 100.000 (99.907)
Epoch: [267][200/391]	Time 0.108 (0.112)	Data 0.000 (0.001)	Loss 0.1211 (0.1223) ([0.007]+[0.115])	Prec@1 100.000 (99.895)
Epoch: [267][300/391]	Time 0.108 (0.111)	Data 0.000 (0.001)	Loss 0.1189 (0.1225) ([0.004]+[0.115])	Prec@1 100.000 (99.878)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.2870 (0.2870) ([0.173]+[0.114])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-03
Grad=  tensor(3.9845, device='cuda:0')
Epoch: [268][0/391]	Time 0.332 (0.332)	Data 0.213 (0.213)	Loss 0.1355 (0.1355) ([0.021]+[0.114])	Prec@1 99.219 (99.219)
Epoch: [268][100/391]	Time 0.108 (0.110)	Data 0.000 (0.002)	Loss 0.1201 (0.1224) ([0.006]+[0.114])	Prec@1 100.000 (99.853)
Epoch: [268][200/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1198 (0.1223) ([0.005]+[0.114])	Prec@1 100.000 (99.860)
Epoch: [268][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1216 (0.1224) ([0.007]+[0.114])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.2730 (0.2730) ([0.159]+[0.114])	Prec@1 95.312 (95.312)
 * Prec@1 93.990
current lr 1.00000e-03
Grad=  tensor(0.1139, device='cuda:0')
Epoch: [269][0/391]	Time 0.306 (0.306)	Data 0.187 (0.187)	Loss 0.1184 (0.1184) ([0.004]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.110 (0.111)	Data 0.000 (0.002)	Loss 0.1229 (0.1209) ([0.009]+[0.114])	Prec@1 100.000 (99.938)
Epoch: [269][200/391]	Time 0.112 (0.111)	Data 0.000 (0.001)	Loss 0.1217 (0.1217) ([0.008]+[0.114])	Prec@1 100.000 (99.872)
Epoch: [269][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1168 (0.1216) ([0.003]+[0.114])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.2896 (0.2896) ([0.176]+[0.114])	Prec@1 95.312 (95.312)
 * Prec@1 93.980
current lr 1.00000e-03
Grad=  tensor(1.7578, device='cuda:0')
Epoch: [270][0/391]	Time 0.337 (0.337)	Data 0.216 (0.216)	Loss 0.1252 (0.1252) ([0.011]+[0.114])	Prec@1 99.219 (99.219)
Epoch: [270][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1177 (0.1207) ([0.004]+[0.114])	Prec@1 100.000 (99.892)
Epoch: [270][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1156 (0.1211) ([0.002]+[0.114])	Prec@1 100.000 (99.872)
Epoch: [270][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1192 (0.1207) ([0.006]+[0.114])	Prec@1 100.000 (99.878)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.2538 (0.2538) ([0.140]+[0.113])	Prec@1 96.875 (96.875)
 * Prec@1 93.940
current lr 1.00000e-03
Grad=  tensor(3.6856, device='cuda:0')
Epoch: [271][0/391]	Time 0.318 (0.318)	Data 0.198 (0.198)	Loss 0.1334 (0.1334) ([0.020]+[0.113])	Prec@1 99.219 (99.219)
Epoch: [271][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1170 (0.1213) ([0.004]+[0.113])	Prec@1 100.000 (99.869)
Epoch: [271][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1159 (0.1211) ([0.003]+[0.113])	Prec@1 100.000 (99.864)
Epoch: [271][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1196 (0.1210) ([0.006]+[0.113])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.288 (0.288)	Loss 0.2635 (0.2635) ([0.150]+[0.113])	Prec@1 96.094 (96.094)
 * Prec@1 94.070
current lr 1.00000e-03
Grad=  tensor(0.0755, device='cuda:0')
Epoch: [272][0/391]	Time 0.409 (0.409)	Data 0.250 (0.250)	Loss 0.1152 (0.1152) ([0.002]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.109 (0.113)	Data 0.000 (0.003)	Loss 0.1170 (0.1199) ([0.004]+[0.113])	Prec@1 100.000 (99.876)
Epoch: [272][200/391]	Time 0.114 (0.112)	Data 0.000 (0.001)	Loss 0.1274 (0.1203) ([0.014]+[0.113])	Prec@1 100.000 (99.880)
Epoch: [272][300/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.1149 (0.1200) ([0.002]+[0.113])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.266 (0.266)	Loss 0.2569 (0.2569) ([0.144]+[0.113])	Prec@1 95.312 (95.312)
 * Prec@1 94.010
current lr 1.00000e-03
Grad=  tensor(0.0962, device='cuda:0')
Epoch: [273][0/391]	Time 0.344 (0.344)	Data 0.220 (0.220)	Loss 0.1161 (0.1161) ([0.003]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.1157 (0.1195) ([0.003]+[0.113])	Prec@1 100.000 (99.876)
Epoch: [273][200/391]	Time 0.109 (0.113)	Data 0.000 (0.001)	Loss 0.1232 (0.1197) ([0.011]+[0.113])	Prec@1 99.219 (99.864)
Epoch: [273][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1181 (0.1197) ([0.006]+[0.113])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2732 (0.2732) ([0.161]+[0.113])	Prec@1 96.875 (96.875)
 * Prec@1 94.060
current lr 1.00000e-03
Grad=  tensor(0.1012, device='cuda:0')
Epoch: [274][0/391]	Time 0.333 (0.333)	Data 0.213 (0.213)	Loss 0.1158 (0.1158) ([0.003]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1186 (0.1191) ([0.006]+[0.112])	Prec@1 100.000 (99.861)
Epoch: [274][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1378 (0.1190) ([0.025]+[0.112])	Prec@1 98.438 (99.895)
Epoch: [274][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1180 (0.1191) ([0.006]+[0.112])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2608 (0.2608) ([0.149]+[0.112])	Prec@1 96.875 (96.875)
 * Prec@1 94.170
current lr 1.00000e-03
Grad=  tensor(0.0859, device='cuda:0')
Epoch: [275][0/391]	Time 0.358 (0.358)	Data 0.237 (0.237)	Loss 0.1149 (0.1149) ([0.003]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.114 (0.115)	Data 0.000 (0.002)	Loss 0.1162 (0.1178) ([0.004]+[0.112])	Prec@1 100.000 (99.930)
Epoch: [275][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1216 (0.1181) ([0.010]+[0.112])	Prec@1 100.000 (99.926)
Epoch: [275][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1145 (0.1182) ([0.003]+[0.112])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.273 (0.273)	Loss 0.2873 (0.2873) ([0.175]+[0.112])	Prec@1 95.312 (95.312)
 * Prec@1 94.090
current lr 1.00000e-03
Grad=  tensor(1.6035, device='cuda:0')
Epoch: [276][0/391]	Time 0.353 (0.353)	Data 0.227 (0.227)	Loss 0.1246 (0.1246) ([0.013]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.113 (0.115)	Data 0.000 (0.002)	Loss 0.1154 (0.1178) ([0.004]+[0.112])	Prec@1 100.000 (99.907)
Epoch: [276][200/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.1139 (0.1178) ([0.002]+[0.112])	Prec@1 100.000 (99.907)
Epoch: [276][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1187 (0.1180) ([0.007]+[0.112])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.245 (0.245)	Loss 0.2705 (0.2705) ([0.159]+[0.112])	Prec@1 95.312 (95.312)
 * Prec@1 94.060
current lr 1.00000e-03
Grad=  tensor(0.0555, device='cuda:0')
Epoch: [277][0/391]	Time 0.353 (0.353)	Data 0.229 (0.229)	Loss 0.1132 (0.1132) ([0.002]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1143 (0.1171) ([0.003]+[0.111])	Prec@1 100.000 (99.930)
Epoch: [277][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1163 (0.1172) ([0.005]+[0.111])	Prec@1 100.000 (99.938)
Epoch: [277][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1185 (0.1175) ([0.007]+[0.111])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2791 (0.2791) ([0.168]+[0.111])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-03
Grad=  tensor(0.0685, device='cuda:0')
Epoch: [278][0/391]	Time 0.334 (0.334)	Data 0.213 (0.213)	Loss 0.1126 (0.1126) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1140 (0.1173) ([0.003]+[0.111])	Prec@1 100.000 (99.892)
Epoch: [278][200/391]	Time 0.109 (0.111)	Data 0.000 (0.001)	Loss 0.1140 (0.1171) ([0.003]+[0.111])	Prec@1 100.000 (99.907)
Epoch: [278][300/391]	Time 0.109 (0.110)	Data 0.000 (0.001)	Loss 0.1145 (0.1169) ([0.003]+[0.111])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2916 (0.2916) ([0.181]+[0.111])	Prec@1 93.750 (93.750)
 * Prec@1 93.970
current lr 1.00000e-03
Grad=  tensor(0.2600, device='cuda:0')
Epoch: [279][0/391]	Time 0.320 (0.320)	Data 0.201 (0.201)	Loss 0.1176 (0.1176) ([0.007]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1130 (0.1167) ([0.002]+[0.111])	Prec@1 100.000 (99.930)
Epoch: [279][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1221 (0.1168) ([0.011]+[0.111])	Prec@1 100.000 (99.934)
Epoch: [279][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1174 (0.1167) ([0.007]+[0.111])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.2694 (0.2694) ([0.159]+[0.111])	Prec@1 95.312 (95.312)
 * Prec@1 94.210
current lr 1.00000e-03
Grad=  tensor(1.0608, device='cuda:0')
Epoch: [280][0/391]	Time 0.352 (0.352)	Data 0.226 (0.226)	Loss 0.1176 (0.1176) ([0.007]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1124 (0.1167) ([0.002]+[0.111])	Prec@1 100.000 (99.884)
Epoch: [280][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1225 (0.1166) ([0.012]+[0.110])	Prec@1 100.000 (99.887)
Epoch: [280][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1121 (0.1165) ([0.002]+[0.110])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.3070 (0.3070) ([0.197]+[0.110])	Prec@1 96.875 (96.875)
 * Prec@1 94.220
current lr 1.00000e-03
Grad=  tensor(0.3746, device='cuda:0')
Epoch: [281][0/391]	Time 0.357 (0.357)	Data 0.232 (0.232)	Loss 0.1147 (0.1147) ([0.004]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1185 (0.1154) ([0.008]+[0.110])	Prec@1 100.000 (99.961)
Epoch: [281][200/391]	Time 0.110 (0.114)	Data 0.000 (0.001)	Loss 0.1171 (0.1155) ([0.007]+[0.110])	Prec@1 100.000 (99.942)
Epoch: [281][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.1268 (0.1156) ([0.017]+[0.110])	Prec@1 99.219 (99.940)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2881 (0.2881) ([0.178]+[0.110])	Prec@1 95.312 (95.312)
 * Prec@1 94.090
current lr 1.00000e-03
Grad=  tensor(0.2552, device='cuda:0')
Epoch: [282][0/391]	Time 0.330 (0.330)	Data 0.209 (0.209)	Loss 0.1151 (0.1151) ([0.005]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1206 (0.1151) ([0.011]+[0.110])	Prec@1 100.000 (99.946)
Epoch: [282][200/391]	Time 0.115 (0.116)	Data 0.000 (0.001)	Loss 0.1154 (0.1152) ([0.006]+[0.110])	Prec@1 100.000 (99.934)
Epoch: [282][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1126 (0.1152) ([0.003]+[0.110])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.260 (0.260)	Loss 0.2729 (0.2729) ([0.163]+[0.110])	Prec@1 96.094 (96.094)
 * Prec@1 94.160
current lr 1.00000e-03
Grad=  tensor(0.4417, device='cuda:0')
Epoch: [283][0/391]	Time 0.334 (0.334)	Data 0.208 (0.208)	Loss 0.1148 (0.1148) ([0.005]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.115 (0.117)	Data 0.000 (0.002)	Loss 0.1107 (0.1155) ([0.001]+[0.110])	Prec@1 100.000 (99.923)
Epoch: [283][200/391]	Time 0.114 (0.116)	Data 0.000 (0.001)	Loss 0.1169 (0.1154) ([0.007]+[0.110])	Prec@1 100.000 (99.918)
Epoch: [283][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1110 (0.1151) ([0.002]+[0.109])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.2801 (0.2801) ([0.171]+[0.109])	Prec@1 96.094 (96.094)
 * Prec@1 94.100
current lr 1.00000e-03
Grad=  tensor(1.1720, device='cuda:0')
Epoch: [284][0/391]	Time 0.297 (0.297)	Data 0.177 (0.177)	Loss 0.1171 (0.1171) ([0.008]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [284][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1216 (0.1145) ([0.012]+[0.109])	Prec@1 100.000 (99.946)
Epoch: [284][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1112 (0.1144) ([0.002]+[0.109])	Prec@1 100.000 (99.946)
Epoch: [284][300/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1113 (0.1147) ([0.002]+[0.109])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.2927 (0.2927) ([0.184]+[0.109])	Prec@1 96.094 (96.094)
 * Prec@1 94.300
current lr 1.00000e-03
Grad=  tensor(0.3080, device='cuda:0')
Epoch: [285][0/391]	Time 0.330 (0.330)	Data 0.205 (0.205)	Loss 0.1126 (0.1126) ([0.004]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1114 (0.1146) ([0.002]+[0.109])	Prec@1 100.000 (99.923)
Epoch: [285][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1180 (0.1148) ([0.009]+[0.109])	Prec@1 100.000 (99.907)
Epoch: [285][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1114 (0.1145) ([0.003]+[0.109])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.2888 (0.2888) ([0.180]+[0.109])	Prec@1 95.312 (95.312)
 * Prec@1 94.180
current lr 1.00000e-03
Grad=  tensor(0.0771, device='cuda:0')
Epoch: [286][0/391]	Time 0.346 (0.346)	Data 0.219 (0.219)	Loss 0.1109 (0.1109) ([0.002]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.1369 (0.1143) ([0.028]+[0.109])	Prec@1 98.438 (99.923)
Epoch: [286][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1108 (0.1141) ([0.002]+[0.109])	Prec@1 100.000 (99.930)
Epoch: [286][300/391]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.1151 (0.1141) ([0.007]+[0.109])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.246 (0.246)	Loss 0.2831 (0.2831) ([0.175]+[0.108])	Prec@1 96.875 (96.875)
 * Prec@1 94.250
current lr 1.00000e-03
Grad=  tensor(0.1706, device='cuda:0')
Epoch: [287][0/391]	Time 0.332 (0.332)	Data 0.212 (0.212)	Loss 0.1116 (0.1116) ([0.003]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1113 (0.1143) ([0.003]+[0.108])	Prec@1 100.000 (99.899)
Epoch: [287][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1131 (0.1138) ([0.005]+[0.108])	Prec@1 100.000 (99.914)
Epoch: [287][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1116 (0.1138) ([0.003]+[0.108])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.2966 (0.2966) ([0.189]+[0.108])	Prec@1 94.531 (94.531)
 * Prec@1 94.080
current lr 1.00000e-03
Grad=  tensor(0.7622, device='cuda:0')
Epoch: [288][0/391]	Time 0.348 (0.348)	Data 0.224 (0.224)	Loss 0.1130 (0.1130) ([0.005]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1291 (0.1128) ([0.021]+[0.108])	Prec@1 99.219 (99.938)
Epoch: [288][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1094 (0.1129) ([0.001]+[0.108])	Prec@1 100.000 (99.938)
Epoch: [288][300/391]	Time 0.111 (0.114)	Data 0.000 (0.001)	Loss 0.1099 (0.1131) ([0.002]+[0.108])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.251 (0.251)	Loss 0.2767 (0.2767) ([0.169]+[0.108])	Prec@1 96.094 (96.094)
 * Prec@1 94.220
current lr 1.00000e-03
Grad=  tensor(0.3516, device='cuda:0')
Epoch: [289][0/391]	Time 0.338 (0.338)	Data 0.217 (0.217)	Loss 0.1133 (0.1133) ([0.005]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1115 (0.1128) ([0.004]+[0.108])	Prec@1 100.000 (99.938)
Epoch: [289][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1121 (0.1130) ([0.004]+[0.108])	Prec@1 100.000 (99.930)
Epoch: [289][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1176 (0.1130) ([0.010]+[0.108])	Prec@1 99.219 (99.935)
Test: [0/79]	Time 0.257 (0.257)	Loss 0.2744 (0.2744) ([0.167]+[0.108])	Prec@1 96.094 (96.094)
 * Prec@1 94.280
current lr 1.00000e-03
Grad=  tensor(0.3017, device='cuda:0')
Epoch: [290][0/391]	Time 0.350 (0.350)	Data 0.225 (0.225)	Loss 0.1133 (0.1133) ([0.006]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.111 (0.115)	Data 0.000 (0.002)	Loss 0.1133 (0.1127) ([0.006]+[0.107])	Prec@1 100.000 (99.954)
Epoch: [290][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1121 (0.1129) ([0.005]+[0.107])	Prec@1 100.000 (99.938)
Epoch: [290][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1294 (0.1127) ([0.022]+[0.107])	Prec@1 98.438 (99.927)
Test: [0/79]	Time 0.248 (0.248)	Loss 0.2953 (0.2953) ([0.188]+[0.107])	Prec@1 95.312 (95.312)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(0.0640, device='cuda:0')
Epoch: [291][0/391]	Time 0.335 (0.335)	Data 0.214 (0.214)	Loss 0.1089 (0.1089) ([0.002]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.115 (0.113)	Data 0.000 (0.002)	Loss 0.1096 (0.1120) ([0.002]+[0.107])	Prec@1 100.000 (99.938)
Epoch: [291][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.1108 (0.1123) ([0.004]+[0.107])	Prec@1 100.000 (99.918)
Epoch: [291][300/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.1354 (0.1121) ([0.028]+[0.107])	Prec@1 99.219 (99.938)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.3028 (0.3028) ([0.196]+[0.107])	Prec@1 94.531 (94.531)
 * Prec@1 94.010
current lr 1.00000e-03
Grad=  tensor(0.1681, device='cuda:0')
Epoch: [292][0/391]	Time 0.343 (0.343)	Data 0.218 (0.218)	Loss 0.1097 (0.1097) ([0.003]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.114 (0.116)	Data 0.000 (0.002)	Loss 0.1235 (0.1113) ([0.017]+[0.107])	Prec@1 99.219 (99.961)
Epoch: [292][200/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1323 (0.1114) ([0.026]+[0.107])	Prec@1 99.219 (99.953)
Epoch: [292][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1133 (0.1113) ([0.007]+[0.107])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.254 (0.254)	Loss 0.2777 (0.2777) ([0.171]+[0.107])	Prec@1 96.094 (96.094)
 * Prec@1 94.190
current lr 1.00000e-03
Grad=  tensor(0.0530, device='cuda:0')
Epoch: [293][0/391]	Time 0.359 (0.359)	Data 0.234 (0.234)	Loss 0.1078 (0.1078) ([0.001]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.114 (0.117)	Data 0.000 (0.002)	Loss 0.1136 (0.1117) ([0.007]+[0.107])	Prec@1 100.000 (99.923)
Epoch: [293][200/391]	Time 0.111 (0.115)	Data 0.000 (0.001)	Loss 0.1130 (0.1117) ([0.007]+[0.106])	Prec@1 100.000 (99.926)
Epoch: [293][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1135 (0.1116) ([0.007]+[0.106])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2796 (0.2796) ([0.173]+[0.106])	Prec@1 96.094 (96.094)
 * Prec@1 94.160
current lr 1.00000e-03
Grad=  tensor(0.0647, device='cuda:0')
Epoch: [294][0/391]	Time 0.335 (0.335)	Data 0.214 (0.214)	Loss 0.1081 (0.1081) ([0.002]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1269 (0.1112) ([0.021]+[0.106])	Prec@1 99.219 (99.930)
Epoch: [294][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1095 (0.1115) ([0.003]+[0.106])	Prec@1 100.000 (99.911)
Epoch: [294][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1090 (0.1115) ([0.003]+[0.106])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.2792 (0.2792) ([0.173]+[0.106])	Prec@1 96.094 (96.094)
 * Prec@1 94.360
current lr 1.00000e-03
Grad=  tensor(0.0593, device='cuda:0')
Epoch: [295][0/391]	Time 0.353 (0.353)	Data 0.226 (0.226)	Loss 0.1077 (0.1077) ([0.002]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.111 (0.114)	Data 0.000 (0.002)	Loss 0.1084 (0.1103) ([0.002]+[0.106])	Prec@1 100.000 (99.969)
Epoch: [295][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1208 (0.1105) ([0.015]+[0.106])	Prec@1 99.219 (99.953)
Epoch: [295][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1106 (0.1104) ([0.005]+[0.106])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.247 (0.247)	Loss 0.2760 (0.2760) ([0.170]+[0.106])	Prec@1 95.312 (95.312)
 * Prec@1 94.020
current lr 1.00000e-03
Grad=  tensor(0.1455, device='cuda:0')
Epoch: [296][0/391]	Time 0.355 (0.355)	Data 0.231 (0.231)	Loss 0.1082 (0.1082) ([0.002]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.112 (0.116)	Data 0.000 (0.002)	Loss 0.1388 (0.1110) ([0.033]+[0.106])	Prec@1 98.438 (99.915)
Epoch: [296][200/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1072 (0.1104) ([0.002]+[0.106])	Prec@1 100.000 (99.946)
Epoch: [296][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1107 (0.1102) ([0.005]+[0.106])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.249 (0.249)	Loss 0.2387 (0.2387) ([0.133]+[0.105])	Prec@1 96.094 (96.094)
 * Prec@1 93.960
current lr 1.00000e-03
Grad=  tensor(0.0679, device='cuda:0')
Epoch: [297][0/391]	Time 0.331 (0.331)	Data 0.210 (0.210)	Loss 0.1072 (0.1072) ([0.002]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.115 (0.116)	Data 0.000 (0.002)	Loss 0.1089 (0.1100) ([0.004]+[0.105])	Prec@1 100.000 (99.946)
Epoch: [297][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1072 (0.1100) ([0.002]+[0.105])	Prec@1 100.000 (99.938)
Epoch: [297][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1068 (0.1099) ([0.002]+[0.105])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.262 (0.262)	Loss 0.2600 (0.2600) ([0.155]+[0.105])	Prec@1 95.312 (95.312)
 * Prec@1 94.190
current lr 1.00000e-03
Grad=  tensor(0.0570, device='cuda:0')
Epoch: [298][0/391]	Time 0.354 (0.354)	Data 0.232 (0.232)	Loss 0.1066 (0.1066) ([0.001]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.115 (0.115)	Data 0.000 (0.002)	Loss 0.1083 (0.1107) ([0.003]+[0.105])	Prec@1 100.000 (99.892)
Epoch: [298][200/391]	Time 0.115 (0.115)	Data 0.000 (0.001)	Loss 0.1063 (0.1103) ([0.001]+[0.105])	Prec@1 100.000 (99.911)
Epoch: [298][300/391]	Time 0.114 (0.115)	Data 0.000 (0.001)	Loss 0.1070 (0.1101) ([0.002]+[0.105])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.243 (0.243)	Loss 0.2776 (0.2776) ([0.173]+[0.105])	Prec@1 94.531 (94.531)
 * Prec@1 94.260
current lr 1.00000e-03
Grad=  tensor(0.2924, device='cuda:0')
Epoch: [299][0/391]	Time 0.347 (0.347)	Data 0.222 (0.222)	Loss 0.1093 (0.1093) ([0.004]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.109 (0.112)	Data 0.000 (0.002)	Loss 0.1070 (0.1086) ([0.002]+[0.105])	Prec@1 100.000 (99.977)
Epoch: [299][200/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1201 (0.1090) ([0.015]+[0.105])	Prec@1 100.000 (99.946)
Epoch: [299][300/391]	Time 0.108 (0.110)	Data 0.000 (0.001)	Loss 0.1066 (0.1091) ([0.002]+[0.105])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.2753 (0.2753) ([0.171]+[0.105])	Prec@1 95.312 (95.312)
 * Prec@1 94.210

 Elapsed time for training  3:55:25.924032

 sparsity of   [0.0, 0.9629629850387573, 0.9259259104728699, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9259259104728699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9259259104728699, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9259259104728699, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.0, 0.5555555820465088, 0.9629629850387573, 0.4444444477558136]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.953125, 0.953125, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.953125, 0.046875, 0.984375, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.3125, 0.96875, 0.0, 0.421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484375, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.0, 0.921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.34375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.0, 0.984375, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.046875, 0.96875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.953125, 0.96875, 0.96875, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.78125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.96875, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1796875, 0.0, 0.0, 0.1796875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.9965277910232544, 0.7586805820465088, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.40625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3125, 0.96875, 0.3125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.98828125, 0.0, 0.0703125, 0.0, 0.9921875, 0.0, 0.2265625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.546875, 0.0, 0.01953125, 0.0, 0.9921875, 0.0, 0.0625, 0.9921875, 0.02734375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23046875, 0.03515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.1979166716337204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9947916865348816, 0.0243055559694767, 0.0399305559694767, 0.0, 0.0, 0.010416666977107525, 0.0399305559694767, 0.0, 0.0590277798473835, 0.0, 0.0, 0.9965277910232544, 0.0329861119389534, 0.0, 0.0, 0.0, 0.0, 0.1440972238779068, 0.0, 0.0, 0.0, 0.02604166604578495, 0.0, 0.0, 0.9982638955116272, 0.4166666567325592, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.220486119389534, 0.0, 0.0, 0.02604166604578495, 0.1232638880610466, 0.0503472238779068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9947916865348816]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.28125, 0.0, 0.0]

 sparsity of   [0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.9921875, 0.9921875, 0.0, 0.00390625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.21484375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51953125, 0.99609375, 0.0, 0.40625, 0.0, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.453125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.9921875, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.9973958134651184, 0.0503472238779068, 0.01909722201526165, 0.0, 0.0, 0.0, 0.0234375, 0.9982638955116272, 0.197048619389534, 0.0338541679084301, 0.756944477558136, 0.0, 0.4505208432674408, 0.0, 0.3333333432674408, 0.8628472089767456, 0.0, 0.0, 0.9973958134651184, 0.1631944477558136, 0.0, 0.1015625, 0.0477430559694767, 0.0416666679084301, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0234375, 0.0364583320915699, 0.8715277910232544, 0.5512152910232544, 0.850694477558136, 0.0347222238779068, 0.9982638955116272, 0.0225694440305233, 0.1380208283662796, 0.9982638955116272, 0.0, 0.1701388955116272, 0.3897569477558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0338541679084301, 0.01909722201526165, 0.8967013955116272, 0.1076388880610466, 0.0, 0.220486119389534, 0.0, 0.0, 0.0, 0.0364583320915699, 0.9982638955116272, 0.0225694440305233, 0.0564236119389534, 0.0347222238779068, 0.2291666716337204, 0.02604166604578495, 0.1640625, 0.0, 0.8029513955116272, 0.0, 0.0, 0.0494791679084301, 0.0, 0.02864583395421505, 0.0, 0.0, 0.0694444477558136, 0.0, 0.0460069440305233, 0.0, 0.8142361044883728, 0.0694444477558136, 0.1588541716337204, 0.1883680522441864, 0.9982638955116272, 0.0, 0.0737847238779068, 0.9982638955116272, 0.9982638955116272, 0.0373263880610466, 0.0, 0.0, 0.01128472201526165, 0.0, 0.0, 0.0998263880610466, 0.0529513880610466, 0.02083333395421505, 0.0, 0.1067708358168602, 0.2387152761220932, 0.0, 0.1536458283662796, 0.140625, 0.9973958134651184, 0.0, 0.0520833320915699, 0.0, 0.0234375, 0.9982638955116272, 0.0859375, 0.0, 0.0, 0.0, 0.9982638955116272, 0.8550347089767456, 0.0, 0.0, 0.2829861044883728, 0.0, 0.134548619389534, 0.0234375, 0.0, 0.1831597238779068, 0.0, 0.013020833022892475, 0.013888888992369175, 0.0, 0.02951388992369175, 0.01128472201526165, 0.046875, 0.9982638955116272]

 sparsity of   [0.0, 0.0, 0.9921875, 0.984375, 0.4921875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.703125, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9765625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9921875, 0.9765625, 0.9921875, 0.9921875, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.2734375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.4921875, 0.984375, 0.0, 0.46875, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.984375, 0.9921875, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.4921875, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.9765625, 0.984375, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.984375, 0.984375, 0.9765625, 0.984375, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.9921875, 0.4921875, 0.0, 0.0, 0.984375, 0.0, 0.4921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.4140625, 0.9765625, 0.4921875, 0.984375, 0.0, 0.9921875, 0.015625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.4921875, 0.9921875, 0.984375, 0.984375, 0.0, 0.9921875, 0.0, 0.9765625, 0.984375, 0.984375, 0.984375, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.4140625, 0.0, 0.9921875, 0.984375, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.3828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.4921875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.4921875, 0.4921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.4921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.984375, 0.0, 0.9921875, 0.4140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.00390625, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.00390625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.00390625, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.00390625, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.00390625, 0.0, 0.0, 0.9921875, 0.0, 0.00390625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.00390625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.00390625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.00390625, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.00390625, 0.0, 0.00390625, 0.9921875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.00390625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.00390625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.9921875, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.28125, 0.0, 0.99609375, 0.99609375, 0.28125, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.28125, 0.99609375, 0.0, 0.0, 0.28125, 0.0, 0.99609375, 0.28125, 0.99609375, 0.998046875, 0.28125, 0.28125, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.28125, 0.28125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.28125, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.28125, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.28125, 0.28125, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.28125, 0.28125, 0.28125, 0.28125, 0.28125, 0.99609375, 0.28125, 0.99609375, 0.28125, 0.28125, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.28125, 0.998046875, 0.99609375, 0.28125, 0.28125, 0.28125, 0.28125, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.267578125, 0.99609375, 0.0, 0.28125, 0.998046875, 0.0, 0.28125, 0.99609375, 0.99609375, 0.99609375, 0.28125, 0.0, 0.998046875, 0.28125, 0.99609375, 0.99609375, 0.28125, 0.28125, 0.99609375, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272]

 sparsity of   [0.6875, 0.0, 0.0, 0.984375, 0.6796875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6875, 0.0, 0.984375, 0.0, 0.9921875, 0.6875, 0.6875, 0.0, 0.0, 0.984375, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.4765625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.6796875, 0.0, 0.0, 0.6875, 0.984375, 0.0, 0.578125, 0.6796875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.6796875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.6875, 0.984375, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6796875, 0.0, 0.984375, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.6875, 0.984375, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.671875, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.6796875, 0.984375, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6875, 0.9765625, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.6796875, 0.984375, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.6640625, 0.6796875, 0.0, 0.0, 0.984375, 0.0, 0.5078125, 0.0, 0.0, 0.0, 0.671875, 0.6328125, 0.0, 0.9921875, 0.0, 0.984375, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.671875, 0.984375, 0.6875, 0.6875, 0.0, 0.984375, 0.6796875, 0.6796875, 0.6875, 0.0, 0.0, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6875, 0.6875, 0.0, 0.984375, 0.9921875, 0.6875, 0.3515625, 0.9765625, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.6875, 0.984375, 0.0, 0.984375, 0.9921875, 0.984375, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.6796875, 0.0, 0.5703125, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.09375, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.6796875, 0.9765625, 0.0, 0.0, 0.6875, 0.6796875, 0.0, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.484375, 0.0, 0.6875, 0.0, 0.0, 0.4921875, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.6796875, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6796875, 0.0, 0.671875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.6796875, 0.6875, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.6640625, 0.015625, 0.6875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.6796875, 0.9921875, 0.0, 0.6875, 0.0, 0.6875, 0.0, 0.0, 0.9921875, 0.6875, 0.0, 0.6875, 0.984375, 0.9921875, 0.0, 0.6875, 0.0, 0.6875, 0.984375, 0.0, 0.6875, 0.6796875, 0.6875, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.6875, 0.6796875, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6875, 0.0, 0.0, 0.984375, 0.6796875, 0.0, 0.0, 0.6875, 0.984375, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.6796875, 0.0, 0.6796875, 0.0, 0.6875, 0.9921875, 0.0, 0.6875, 0.984375, 0.0, 0.0, 0.6796875, 0.0, 0.3046875, 0.984375, 0.0, 0.984375, 0.6875, 0.984375, 0.0, 0.6875, 0.6875, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.984375, 0.0, 0.3515625, 0.0, 0.0, 0.0, 0.6796875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.013671875, 0.17578125, 0.0, 0.0, 0.037109375, 0.0, 0.0, 0.0, 0.0, 0.083984375, 0.99609375, 0.0, 0.1640625, 0.169921875, 0.138671875, 0.0, 0.0, 0.17578125, 0.17578125, 0.0, 0.0, 0.005859375, 0.267578125, 0.0, 0.0, 0.025390625, 0.17578125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.017578125, 0.99609375, 0.0, 0.0, 0.0, 0.17578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16015625, 0.0, 0.115234375, 0.0, 0.853515625, 0.138671875, 0.0, 0.029296875, 0.0, 0.02734375, 0.62109375, 0.083984375, 0.1640625, 0.0, 0.17578125, 0.158203125, 0.0, 0.029296875, 0.16796875, 0.146484375, 0.0, 0.16015625, 0.0, 0.03515625, 0.0, 0.09765625, 0.0, 0.994140625, 0.0, 0.0, 0.17578125, 0.0, 0.10546875, 0.025390625, 0.078125, 0.005859375, 0.0, 0.0, 0.03515625, 0.111328125, 0.013671875, 0.017578125, 0.0, 0.0, 0.02734375, 0.05078125, 0.013671875, 0.115234375, 0.17578125, 0.01953125, 0.0, 0.0234375, 0.013671875, 0.0, 0.0, 0.056640625, 0.99609375, 0.0078125, 0.0, 0.0234375, 0.17578125, 0.0390625, 0.0, 0.16015625, 0.146484375, 0.248046875, 0.017578125, 0.0, 0.13671875, 0.0, 0.0, 0.0, 0.0, 0.17578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.1284722238779068, 0.0, 0.0069444444961845875, 0.013020833022892475, 0.0, 0.0, 0.0, 0.02690972201526165, 0.015625, 0.0, 0.0, 0.9973958134651184, 0.0052083334885537624, 0.01128472201526165, 0.9114583134651184, 0.1015625, 0.1840277761220932, 0.0, 0.9982638955116272, 0.0, 0.859375, 0.9982638955116272, 0.0, 0.0164930559694767, 0.0859375, 0.009548611007630825, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0243055559694767, 0.999131977558136, 0.0, 0.0, 0.0381944440305233, 0.0, 0.9973958134651184, 0.0, 0.01996527798473835, 0.1883680522441864, 0.0225694440305233, 0.1449652761220932, 0.9982638955116272, 0.009548611007630825, 0.0251736119389534, 0.0, 0.118055559694767, 0.9722222089767456, 0.0, 0.014756944961845875, 0.0052083334885537624, 0.8333333134651184, 0.0251736119389534, 0.01996527798473835, 0.1779513955116272, 0.0, 0.015625, 0.1927083283662796, 0.0520833320915699, 0.0, 0.0, 0.1284722238779068, 0.02604166604578495, 0.0625, 0.0355902798473835, 0.010416666977107525, 0.8671875, 0.0607638880610466, 0.0460069440305233, 0.0460069440305233, 0.013020833022892475, 0.02083333395421505, 0.0, 0.078125, 0.0, 0.00434027798473835, 0.0373263880610466, 0.02690972201526165, 0.0, 0.0538194440305233, 0.173611119389534, 0.0, 0.0303819440305233, 0.0390625, 0.0, 0.0, 0.0572916679084301, 0.0, 0.7951388955116272, 0.0, 0.015625, 0.0694444477558136, 0.0520833320915699, 0.8177083134651184, 0.0842013880610466, 0.9982638955116272, 0.0, 0.0555555559694767, 0.0416666679084301, 0.9357638955116272, 0.0850694477558136, 0.0, 0.0347222238779068, 0.0, 0.1197916641831398, 0.0, 0.0, 0.0, 0.0, 0.02690972201526165, 0.0, 0.02864583395421505, 0.9583333134651184, 0.0998263880610466, 0.0590277798473835, 0.0, 0.9262152910232544, 0.0355902798473835, 0.0590277798473835, 0.0407986119389534, 0.0, 0.0963541641831398, 0.9973958134651184, 0.741319477558136, 0.0798611119389534, 0.110243059694767, 0.0, 0.1085069477558136]

 sparsity of   [0.640625, 0.640625, 0.640625, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.9765625, 0.0, 0.984375, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.421875, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.640625, 0.0, 0.640625, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.546875, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.453125, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.171875, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.640625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.640625, 0.984375, 0.0, 0.0, 0.0, 0.046875, 0.640625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.0, 0.984375, 0.640625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.9921875, 0.0, 0.640625, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.5703125, 0.0, 0.984375, 0.0, 0.0, 0.7734375, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.640625, 0.0, 0.984375, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.609375, 0.0, 0.0, 0.984375, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.8046875, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.4375, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.59375, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.84375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.0, 0.0, 0.640625, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.640625, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.640625, 0.625, 0.640625, 0.640625, 0.640625, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.640625, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.9765625, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.2109375, 0.640625, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.640625, 0.0, 0.984375, 0.0, 0.0, 0.640625, 0.0, 0.640625, 0.1171875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.9921875, 0.640625, 0.0, 0.640625, 0.640625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0]

 sparsity of   [0.99609375, 0.9921875, 0.0, 0.994140625, 0.068359375, 0.0, 0.99609375, 0.052734375, 0.005859375, 0.056640625, 0.01953125, 0.0, 0.08203125, 0.033203125, 0.0, 0.001953125, 0.0, 0.0859375, 0.998046875, 0.998046875, 0.0, 0.01171875, 0.01171875, 0.00390625, 0.0, 0.078125, 0.0, 0.0703125, 0.013671875, 0.0703125, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.123046875, 0.0, 0.919921875, 0.998046875, 0.037109375, 0.99609375, 0.10546875, 0.00390625, 0.013671875, 0.080078125, 0.0, 0.08203125, 0.99609375, 0.90234375, 0.0, 0.005859375, 0.0078125, 0.005859375, 0.0, 0.99609375, 0.00390625, 0.01953125, 0.998046875, 0.35546875, 0.11328125, 0.99609375, 0.08203125, 0.0, 0.005859375, 0.046875, 0.01171875, 0.125, 0.99609375, 0.0, 0.0, 0.068359375, 0.0, 0.99609375, 0.0, 0.0, 0.044921875, 0.0, 0.07421875, 0.02734375, 0.119140625, 0.0, 0.009765625, 0.0078125, 0.0, 0.021484375, 0.03515625, 0.068359375, 0.09375, 0.123046875, 0.0, 0.052734375, 0.068359375, 0.0078125, 0.0, 0.9453125, 0.99609375, 0.0, 0.998046875, 0.0, 0.009765625, 0.068359375, 0.07421875, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.076171875, 0.0, 0.99609375, 0.0, 0.0, 0.0546875, 0.005859375, 0.0859375, 0.0, 0.115234375, 0.017578125, 0.0, 0.013671875, 0.0, 0.076171875, 0.0, 0.99609375, 0.060546875, 0.99609375]

 sparsity of   [0.9982638955116272, 0.928819477558136, 0.0572916679084301, 0.9982638955116272, 0.1336805522441864, 0.0486111119389534, 0.6180555820465088, 0.999131977558136, 0.0451388880610466, 0.0078125, 0.9973958134651184, 0.8602430820465088, 0.01215277798473835, 0.0807291641831398, 0.0, 0.0555555559694767, 0.9973958134651184, 0.9973958134651184, 0.0, 0.8064236044883728, 0.02690972201526165, 0.0442708320915699, 0.3177083432674408, 0.0173611119389534, 0.1458333283662796, 0.9982638955116272, 0.046875, 0.0, 0.1883680522441864, 0.9982638955116272, 0.0399305559694767, 0.02083333395421505, 0.1241319477558136, 0.9982638955116272, 0.0381944440305233, 0.0251736119389534, 0.0, 0.0407986119389534, 0.109375, 0.9045138955116272, 0.02951388992369175, 0.00434027798473835, 0.2473958283662796, 0.9461805820465088, 0.0763888880610466, 0.0, 0.0347222238779068, 0.8862847089767456, 0.0, 0.0859375, 0.01822916604578495, 0.9982638955116272, 0.02690972201526165, 0.015625, 0.999131977558136, 0.9982638955116272, 0.02170138992369175, 0.7291666865348816, 0.0538194440305233, 0.1189236119389534, 0.134548619389534, 0.0772569477558136, 0.0451388880610466, 0.1302083283662796, 0.0, 0.0529513880610466, 0.0251736119389534, 0.0, 0.0, 0.9583333134651184, 0.1197916641831398, 0.1901041716337204, 0.0451388880610466, 0.9149305820465088, 0.8628472089767456, 0.0, 0.0381944440305233, 0.0, 0.0, 0.0954861119389534, 0.9982638955116272, 0.0, 0.944444477558136, 0.1440972238779068, 0.0538194440305233, 0.0, 0.9982638955116272, 0.0, 0.01822916604578495, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0078125, 0.9982638955116272, 0.999131977558136, 0.0225694440305233, 0.02690972201526165, 0.0642361119389534, 0.0, 0.9184027910232544, 0.0425347238779068, 0.0, 0.013888888992369175, 0.102430559694767, 0.0555555559694767, 0.890625, 0.0555555559694767, 0.9010416865348816, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.0989583358168602, 0.8567708134651184, 0.0, 0.2838541567325592, 0.9982638955116272, 0.5729166865348816, 0.7378472089767456, 0.01822916604578495, 0.9982638955116272, 0.0668402761220932, 0.0685763880610466, 0.7994791865348816, 0.0]

 sparsity of   [0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.7578125, 0.0, 0.0, 0.765625, 0.984375, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.75, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.390625, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.7578125, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.6796875, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.1015625, 0.0, 0.9921875, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.0546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.7578125, 0.765625, 0.765625, 0.7265625, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.53125, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.9921875, 0.765625, 0.765625, 0.9921875, 0.0, 0.765625, 0.0, 0.0, 0.0703125, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.7265625, 0.984375, 0.7578125, 0.0, 0.0, 0.765625, 0.7578125, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.7578125, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.0, 0.140625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.7578125, 0.765625, 0.078125, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.7421875, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0390625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.7578125, 0.765625, 0.7578125, 0.7578125, 0.9921875, 0.0, 0.7578125, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.75, 0.765625, 0.765625, 0.734375, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.125, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.7109375, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.7265625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.3828125, 0.75, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.7578125, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.7578125, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.7578125, 0.0, 0.0, 0.0, 0.7265625, 0.765625, 0.765625, 0.0, 0.7265625, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.734375, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.0, 0.7109375, 0.765625, 0.765625, 0.015625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.7578125, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.765625, 0.765625, 0.0, 0.0, 0.125, 0.765625, 0.765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.765625, 0.765625, 0.0, 0.765625, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.1953125, 0.0, 0.765625, 0.765625, 0.7265625, 0.0, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.6875, 0.765625, 0.7578125, 0.0, 0.765625, 0.0, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.0, 0.984375, 0.765625, 0.0, 0.765625, 0.0, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625, 0.765625]

 sparsity of   [0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.998046875, 0.017578125, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.017578125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.017578125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.017578125, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.017578125, 0.017578125, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.994140625, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.017578125, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.017578125, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.017578125, 0.99609375, 0.017578125, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.017578125]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.4453125, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.4348958432674408, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.4392361044883728, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.4453125, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.4383680522441864, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.4453125, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.4453125, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4453125, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.999131977558136]

 sparsity of   [0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.98828125, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.6484375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.6484375, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.6484375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.78515625, 0.0, 0.9921875, 0.9921875, 0.0, 0.6484375, 0.9921875, 0.6484375, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6484375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.6484375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.6484375, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.6484375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.6484375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.859375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.0, 0.0, 0.6484375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.33984375, 0.0, 0.0, 0.6484375, 0.6484375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.6484375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.88671875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6484375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.6484375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.98828125, 0.0, 0.0, 0.0, 0.6484375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.56640625, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6484375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.83203125, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.6484375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.98828125, 0.0, 0.6484375, 0.9921875, 0.6484375, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.98828125, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98828125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.6484375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.98828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.6484375, 0.6484375, 0.6484375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875]

 sparsity of   [0.99609375, 0.052734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.095703125, 0.998046875, 0.99609375, 0.994140625, 0.0, 0.0, 0.087890625, 0.998046875, 0.994140625, 0.0, 0.0, 0.05078125, 0.0, 0.064453125, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.013671875, 0.068359375, 0.0078125, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.017578125, 0.0, 0.99609375, 0.017578125, 0.0, 0.99609375, 0.0, 0.0, 0.017578125, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.404296875, 0.916015625, 0.99609375, 0.0, 0.0, 0.169921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.001953125, 0.1171875, 0.994140625, 0.99609375, 0.0, 0.3828125, 0.0, 0.0, 0.0, 0.462890625, 0.0, 0.0, 0.869140625, 0.00390625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.439453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.017578125, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.998046875, 0.99609375, 0.048828125, 0.310546875, 0.083984375, 0.0, 0.994140625, 0.0625, 0.0, 0.0, 0.0, 0.017578125, 0.99609375, 0.0, 0.263671875, 0.0, 0.064453125, 0.048828125, 0.0, 0.00390625, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.009765625, 0.41015625, 0.0, 0.0, 0.0, 0.1796875, 0.0, 0.015625, 0.279296875, 0.0, 0.99609375, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.009765625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.001953125, 0.0, 0.00390625, 0.0, 0.0, 0.99609375, 0.486328125, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.052734375, 0.998046875, 0.0, 0.0, 0.0, 0.072265625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.041015625, 0.998046875, 0.0, 0.99609375, 0.0, 0.423828125, 0.0, 0.99609375, 0.0, 0.22265625, 0.017578125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.994140625, 0.99609375, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.99609375, 0.0, 0.060546875, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.017578125, 0.015625, 0.9921875, 0.814453125, 0.017578125, 0.0, 0.85546875, 0.0, 0.0, 0.0, 0.009765625, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.93359375, 0.0, 0.99609375, 0.0, 0.134765625, 0.0, 0.99609375, 0.001953125, 0.45703125, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.994140625, 0.0, 0.056640625, 0.0, 0.076171875, 0.0, 0.998046875, 0.0, 0.99609375, 0.994140625, 0.99609375, 0.0, 0.0, 0.99609375, 0.994140625, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.2890625, 0.0, 0.0, 0.046875, 0.24609375, 0.0625, 0.99609375, 0.0, 0.04296875, 0.00390625, 0.408203125, 0.0, 0.431640625, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.048828125, 0.990234375, 0.0, 0.998046875, 0.158203125, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.009765625, 0.017578125, 0.775390625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.009765625, 0.0, 0.228515625, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.017578125, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.8203125, 0.99609375, 0.0, 0.99609375, 0.0, 0.78125, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.05859375, 0.0, 0.017578125, 0.99609375, 0.146484375, 0.369140625, 0.998046875, 0.0, 0.005859375, 0.001953125, 0.087890625, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.060546875, 0.0, 0.06640625, 0.0, 0.0, 0.037109375, 0.017578125, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.115234375, 0.14453125, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.00390625, 0.3046875, 0.99609375, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.056640625, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.830078125, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.078125, 0.263671875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.126953125, 0.0, 0.0703125, 0.0, 0.0, 0.9921875, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.490234375, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.00390625, 0.0, 0.078125, 0.0, 0.0, 0.326171875, 0.017578125, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.220703125, 0.0, 0.998046875, 0.01171875, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.0, 0.294921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.994140625, 0.998046875, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.087890625, 0.0, 0.47265625, 0.0078125, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.99609375, 0.994140625, 0.99609375, 0.0, 0.0, 0.017578125, 0.998046875, 0.994140625, 0.0, 0.0, 0.0078125, 0.998046875, 0.28125, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.404296875, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.998046875, 0.1328125, 0.0, 0.994140625, 0.001953125, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.125, 0.31640625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.171875, 0.017578125, 0.99609375, 0.0, 0.052734375, 0.037109375, 0.197265625, 0.0, 0.037109375, 0.0, 0.998046875, 0.4609375, 0.99609375, 0.99609375, 0.025390625, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.24609375, 0.998046875, 0.0, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.001953125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0078125, 0.998046875, 0.99609375, 0.0, 0.8515625, 0.0, 0.99609375, 0.998046875, 0.0078125, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.130859375, 0.0, 0.0, 0.13671875, 0.99609375, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14453125, 0.072265625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.072265625, 0.0, 0.0, 0.4453125, 0.13671875, 0.0, 0.060546875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.98828125, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.419921875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.017578125, 0.013671875, 0.017578125, 0.0, 0.080078125, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.50390625, 0.998046875, 0.349609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05078125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.005859375, 0.0, 0.0, 0.853515625, 0.0, 0.0, 0.0, 0.173828125, 0.87109375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0546875, 0.0, 0.998046875, 0.998046875, 0.994140625, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.001953125, 0.224609375, 0.517578125, 0.99609375, 0.048828125, 0.0, 0.0, 0.11328125, 0.0, 0.99609375, 0.0, 0.0, 0.017578125, 0.99609375, 0.0, 0.00390625, 0.859375, 0.009765625, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.99609375, 0.001953125, 0.017578125, 0.625, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.0, 0.37890625, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.177734375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.125, 0.0, 0.998046875, 0.318359375, 0.0, 0.130859375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.14453125, 0.017578125, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.99609375, 0.466796875, 0.0, 0.0, 0.994140625, 0.0, 0.99609375, 0.99609375, 0.017578125, 0.0, 0.0, 0.0, 0.119140625, 0.0, 0.017578125, 0.2421875, 0.0, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.06640625, 0.099609375, 0.0, 0.0, 0.0, 0.998046875, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.26953125, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.05859375, 0.0, 0.0, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.015625, 0.017578125, 0.00390625, 0.0, 0.0, 0.998046875, 0.017578125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.8359375, 0.240234375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.4375, 0.005859375, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.998046875, 0.99609375, 0.0, 0.234375, 0.42578125, 0.99609375, 0.990234375, 0.0, 0.416015625]

 sparsity of   [0.1044921875, 0.3759765625, 0.0, 0.0, 0.0, 0.3984375, 0.0, 0.0, 0.1181640625, 0.8046875, 0.0, 0.0, 0.998046875, 0.0, 0.3984375, 0.0, 0.0, 0.3984375, 0.3984375, 0.0, 0.9990234375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.1923828125, 0.998046875, 0.0, 0.1513671875, 0.029296875, 0.0, 0.998046875, 0.0, 0.9970703125, 0.0, 0.0, 0.34765625, 0.9970703125, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.361328125, 0.1416015625, 0.0, 0.3984375, 0.3984375, 0.2880859375, 0.13671875, 0.0, 0.0, 0.029296875, 0.3984375, 0.0, 0.3671875, 0.0, 0.08203125, 0.0, 0.0, 0.9990234375, 0.0, 0.330078125, 0.0, 0.0, 0.0, 0.3759765625, 0.080078125, 0.275390625, 0.0, 0.0, 0.0, 0.3759765625, 0.38671875, 0.12890625, 0.0, 0.3984375, 0.0, 0.0, 0.064453125, 0.0, 0.185546875, 0.998046875, 0.0, 0.134765625, 0.998046875, 0.998046875, 0.3984375, 0.0, 0.384765625, 0.5029296875, 0.998046875, 0.998046875, 0.0, 0.0751953125, 0.0, 0.0, 0.091796875, 0.353515625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1826171875, 0.0, 0.9990234375, 0.0, 0.0, 0.998046875, 0.2724609375, 0.3984375, 0.048828125, 0.0, 0.0, 0.3955078125, 0.3984375, 0.0, 0.0, 0.0498046875, 0.8681640625, 0.0, 0.3984375, 0.0, 0.0, 0.0478515625, 0.998046875, 0.0, 0.0, 0.9990234375, 0.2802734375, 0.3818359375, 0.998046875, 0.0, 0.0, 0.0185546875, 0.9970703125, 0.01953125, 0.0, 0.0, 0.0, 0.3984375, 0.0, 0.3984375, 0.998046875, 0.0, 0.0, 0.0390625, 0.0166015625, 0.052734375, 0.0, 0.0, 0.998046875, 0.21875, 0.3984375, 0.0, 0.3984375, 0.0, 0.38671875, 0.3984375, 0.2734375, 0.0830078125, 0.998046875, 0.3984375, 0.998046875, 0.0, 0.9970703125, 0.2177734375, 0.0, 0.3984375, 0.0, 0.3984375, 0.3984375, 0.3056640625, 0.0, 0.0, 0.861328125, 0.0, 0.1259765625, 0.0, 0.0703125, 0.0, 0.1533203125, 0.0830078125, 0.3359375, 0.0, 0.041015625, 0.2958984375, 0.03125, 0.42578125, 0.541015625, 0.0, 0.998046875, 0.998046875, 0.0, 0.091796875, 0.998046875, 0.3984375, 0.0, 0.048828125, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0498046875, 0.3984375, 0.0, 0.3984375, 0.1318359375, 0.998046875, 0.9990234375, 0.8720703125, 0.0, 0.1435546875, 0.998046875, 0.18359375, 0.0, 0.0, 0.0, 0.2216796875, 0.0, 0.3740234375, 0.0, 0.0888671875, 0.9990234375, 0.29296875, 0.9990234375, 0.3984375, 0.0, 0.9990234375, 0.0, 0.171875, 0.0, 0.9970703125, 0.0, 0.263671875, 0.998046875, 0.044921875, 0.0, 0.3984375, 0.0, 0.0, 0.0, 0.998046875, 0.0712890625, 0.115234375, 0.998046875, 0.8671875, 0.0, 0.31640625, 0.9990234375, 0.0]

 sparsity of   [0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.6545138955116272, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0894097238779068, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136]

 sparsity of   [0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.6171875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.6171875, 0.6171875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.9921875, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.6171875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6171875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.6171875, 0.88671875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.6171875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.61328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.6171875, 0.6171875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.6171875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.6171875, 0.6171875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.6171875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.6171875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.6171875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.6171875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.9921875, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.6171875, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.6171875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.6171875, 0.0, 0.0, 0.9921875, 0.9921875, 0.6171875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.6171875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.6171875, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.6171875, 0.0, 0.9921875, 0.6171875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.6171875, 0.6171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0]

 sparsity of   [0.2236328125, 0.0, 0.9970703125, 0.0, 0.9990234375, 0.0, 0.0, 0.998046875, 0.23828125, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.9990234375, 0.0, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.072265625, 0.998046875, 0.0, 0.0, 0.998046875, 0.3818359375, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.0, 0.2392578125, 0.9990234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0732421875, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.4072265625, 0.0, 0.0, 0.0, 0.998046875, 0.9990234375, 0.388671875, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.259765625, 0.0, 0.42578125, 0.998046875, 0.0, 0.0, 0.0, 0.17578125, 0.9990234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.0576171875, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.287109375, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.3056640625, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.203125, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.759765625, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.037109375, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.9990234375, 0.1103515625, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.0, 0.9970703125, 0.9990234375, 0.9970703125, 0.0, 0.9970703125, 0.9931640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.9970703125, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.427734375, 0.0, 0.0322265625, 0.0, 0.0, 0.9990234375, 0.9990234375, 0.0, 0.9990234375, 0.0, 0.0, 0.0, 0.0, 0.9990234375, 0.0, 0.4462890625, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.064453125, 0.0, 0.099609375, 0.0, 0.9990234375, 0.0, 0.0, 0.0, 0.9990234375, 0.0927734375, 0.9970703125, 0.197265625]

 sparsity of   [0.9986979365348816, 0.6618923544883728, 0.0729166641831398, 0.9995659589767456, 0.0, 0.0052083334885537624, 0.0243055559694767, 0.0442708320915699, 0.0, 0.6440972089767456, 0.0, 0.0, 0.02387152798473835, 0.078125, 0.0421006940305233, 0.9995659589767456, 0.09375, 0.0, 0.0, 0.0447048619389534, 0.0, 0.0, 0.01996527798473835, 0.0, 0.013454861007630825, 0.9240451455116272, 0.02864583395421505, 0.0, 0.0620659738779068, 0.999131977558136, 0.0186631940305233, 0.9995659589767456, 0.0373263880610466, 0.0, 0.00824652798473835, 0.0, 0.0, 0.0438368059694767, 0.01779513992369175, 0.01822916604578495, 0.0, 0.1453993022441864, 0.01779513992369175, 0.02864583395421505, 0.01909722201526165, 0.1597222238779068, 0.02560763992369175, 0.0, 0.0, 0.1336805522441864, 0.0, 0.999131977558136, 0.01996527798473835, 0.02690972201526165, 0.01996527798473835, 0.2578125, 0.9986979365348816, 0.0434027798473835, 0.0, 0.0, 0.0, 0.0, 0.788194477558136, 0.0707465261220932, 0.0, 0.013888888992369175, 0.009114583022892475, 0.0, 0.02864583395421505, 0.0520833320915699, 0.999131977558136, 0.0, 0.02951388992369175, 0.999131977558136, 0.378472238779068, 0.015625, 0.0, 0.0, 0.0, 0.0477430559694767, 0.0, 0.999131977558136, 0.01779513992369175, 0.0, 0.2374131977558136, 0.0594618059694767, 0.02604166604578495, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0399305559694767, 0.0290798619389534, 0.0, 0.0, 0.013020833022892475, 0.0403645820915699, 0.8671875, 0.0, 0.013888888992369175, 0.0, 0.0481770820915699, 0.1089409738779068, 0.8676215410232544, 0.0377604179084301, 0.0303819440305233, 0.01822916604578495, 0.897569477558136, 0.1032986119389534, 0.0559895820915699, 0.0264756940305233, 0.0, 0.0386284738779068, 0.7578125, 0.0, 0.0, 0.0, 0.0225694440305233, 0.0460069440305233, 0.7886284589767456, 0.581163227558136, 0.7526041865348816, 0.0, 0.9574652910232544, 0.16015625, 0.0, 0.067274309694767, 0.01996527798473835, 0.0186631940305233, 0.0164930559694767, 0.02300347201526165, 0.0, 0.1440972238779068, 0.00390625, 0.0, 0.02560763992369175, 0.0303819440305233, 0.009548611007630825, 0.0620659738779068, 0.0, 0.0876736119389534, 0.1623263955116272, 0.5620659589767456, 0.7942708134651184, 0.0, 0.0, 0.0125868059694767, 0.999131977558136, 0.0, 0.0967881977558136, 0.9709201455116272, 0.0499131940305233, 0.0321180559694767, 0.02690972201526165, 0.0, 0.0412326380610466, 0.0, 0.0655381977558136, 0.0355902798473835, 0.0503472238779068, 0.9986979365348816, 0.0078125, 0.15625, 0.0164930559694767, 0.02213541604578495, 0.0885416641831398, 0.0, 0.0, 0.02560763992369175, 0.015625, 0.0, 0.2907986044883728, 0.0, 0.1901041716337204, 0.1115451380610466, 0.014756944961845875, 0.0390625, 0.0, 0.9986979365348816, 0.0, 0.0407986119389534, 0.009114583022892475, 0.02560763992369175, 0.0846354141831398, 0.9986979365348816, 0.0347222238779068, 0.0, 0.02734375, 0.0, 0.01692708395421505, 0.200954869389534, 0.9583333134651184, 0.0234375, 0.0, 0.009114583022892475, 0.0, 0.76171875, 0.0, 0.0, 0.0303819440305233, 0.9205729365348816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02083333395421505, 0.0, 0.1401909738779068, 0.0, 0.02170138992369175, 0.0525173619389534, 0.02777777798473835, 0.0, 0.0334201380610466, 0.00824652798473835, 0.0577256940305233, 0.0681423619389534, 0.0, 0.0, 0.0703125, 0.02083333395421505, 0.0355902798473835, 0.0, 0.02170138992369175, 0.0, 0.0, 0.01605902798473835, 0.0, 0.0, 0.007378472480922937, 0.999131977558136, 0.0373263880610466, 0.0, 0.02994791604578495, 0.009982638992369175, 0.8493923544883728, 0.8797743320465088, 0.02473958395421505, 0.7591145634651184, 0.02604166604578495, 0.7986111044883728, 0.9986979365348816, 0.0464409738779068, 0.0, 0.01692708395421505, 0.0212673619389534, 0.0, 0.01171875, 0.7161458134651184, 0.0, 0.0373263880610466, 0.02994791604578495, 0.300347238779068]

 sparsity of   [0.0703125, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.67578125, 0.92578125, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.9921875, 0.0, 0.0, 0.98828125, 0.67578125, 0.98828125, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.71484375, 0.05859375, 0.0, 0.41796875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.65234375, 0.01953125, 0.9921875, 0.05078125, 0.0, 0.62890625, 0.0, 0.0, 0.61328125, 0.72265625, 0.99609375, 0.0, 0.66796875, 0.0, 0.0, 0.67578125, 0.9921875, 0.9921875, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.01171875, 0.0, 0.0, 0.01953125, 0.0, 0.59375, 0.9921875, 0.67578125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.16015625, 0.67578125, 0.0, 0.01953125, 0.67578125, 0.67578125, 0.67578125, 0.69140625, 0.67578125, 0.0, 0.0, 0.6640625, 0.0, 0.84765625, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0234375, 0.0, 0.99609375, 0.0, 0.0, 0.1015625, 0.9921875, 0.60546875, 0.9921875, 0.67578125, 0.0, 0.0, 0.67578125, 0.64453125, 0.65234375, 0.0, 0.6640625, 0.67578125, 0.19140625, 0.0, 0.0, 0.9921875, 0.0, 0.65625, 0.0, 0.9921875, 0.0, 0.67578125, 0.9921875, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.671875, 0.67578125, 0.9921875, 0.0, 0.0, 0.08984375, 0.0, 0.0, 0.67578125, 0.5546875, 0.66796875, 0.0, 0.67578125, 0.99609375, 0.0, 0.67578125, 0.0, 0.9921875, 0.13671875, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.9921875, 0.0, 0.0, 0.67578125, 0.015625, 0.0, 0.67578125, 0.17578125, 0.67578125, 0.67578125, 0.0, 0.0, 0.9921875, 0.67578125, 0.0, 0.99609375, 0.67578125, 0.83984375, 0.67578125, 0.67578125, 0.9921875, 0.67578125, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.99609375, 0.0, 0.67578125, 0.9921875, 0.0, 0.0, 0.67578125, 0.0, 0.07421875, 0.0, 0.99609375, 0.0, 0.67578125, 0.69921875, 0.05078125, 0.0, 0.0, 0.0, 0.0, 0.05859375, 0.9921875, 0.0, 0.0, 0.67578125, 0.0078125, 0.03125, 0.078125, 0.67578125, 0.67578125, 0.0, 0.0, 0.0, 0.99609375, 0.6640625, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.9921875, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.37109375, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.67578125, 0.66015625, 0.67578125, 0.0, 0.0, 0.13671875, 0.67578125, 0.0, 0.61328125, 0.99609375, 0.0, 0.60546875, 0.0, 0.078125, 0.0, 0.67578125, 0.578125, 0.99609375, 0.8046875, 0.87109375, 0.0, 0.0, 0.66796875, 0.67578125, 0.67578125, 0.0, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 0.0, 0.9921875, 0.04296875, 0.70703125, 0.0, 0.0, 0.0, 0.0859375, 0.0, 0.50390625, 0.0, 0.67578125, 0.63671875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.11328125, 0.0, 0.07421875, 0.0, 0.0, 0.9921875, 0.67578125, 0.0, 0.796875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.0390625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.10546875, 0.99609375, 0.67578125, 0.0, 0.9921875, 0.9921875, 0.0, 0.67578125, 0.953125, 0.67578125, 0.0, 0.0, 0.05078125, 0.0, 0.62109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.61328125, 0.0, 0.0, 0.05078125, 0.67578125, 0.99609375, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.9921875, 0.0, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.30078125, 0.67578125, 0.0, 0.0, 0.0, 0.67578125, 0.1171875, 0.0, 0.67578125, 0.0, 0.9921875, 0.67578125, 0.0, 0.71484375, 0.0, 0.67578125, 0.0, 0.67578125, 0.0, 0.9921875, 0.0, 0.67578125, 0.29296875, 0.0, 0.0625, 0.0, 0.9921875, 0.0, 0.0625, 0.0, 0.0, 0.69140625, 0.625, 0.0, 0.1328125, 0.0, 0.67578125, 0.0234375, 0.9921875, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.9921875, 0.99609375, 0.67578125, 0.65625, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.62109375, 0.0, 0.9921875, 0.03125, 0.0859375, 0.0, 0.0, 0.67578125, 0.66796875, 0.0, 0.2890625, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.67578125, 0.67578125, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.67578125, 0.67578125, 0.6640625, 0.16796875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.73828125, 0.9921875, 0.67578125, 0.0, 0.671875, 0.0, 0.0, 0.0, 0.0, 0.70703125, 0.0, 0.0, 0.0, 0.0, 0.64453125, 0.60546875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.6171875, 0.0, 0.0, 0.0703125, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.984375, 0.67578125, 0.67578125, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.0390625, 0.9921875, 0.0, 0.1953125, 0.67578125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.421875, 0.0, 0.0, 0.0, 0.67578125, 0.66015625, 0.9921875, 0.99609375, 0.6953125, 0.67578125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.6484375, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.72265625, 0.9921875, 0.0, 0.9921875, 0.01171875, 0.67578125, 0.04296875, 0.0, 0.67578125, 0.0, 0.6640625, 0.0, 0.67578125, 0.9921875, 0.9921875, 0.9921875, 0.67578125, 0.0, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.9921875, 0.35546875, 0.99609375, 0.0, 0.9921875, 0.765625, 0.9921875, 0.0, 0.67578125, 0.0, 0.9921875, 0.0, 0.0, 0.67578125, 0.67578125, 0.9921875, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.98828125, 0.98828125, 0.0, 0.94140625, 0.0, 0.0, 0.67578125, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.6484375, 0.0, 0.640625, 0.70703125, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 0.28515625, 0.0, 0.6171875, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.67578125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.9921875, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.34375, 0.9921875, 0.0, 0.92578125, 0.67578125, 0.99609375, 0.0, 0.67578125, 0.66796875, 0.0, 0.0, 0.67578125, 0.0, 0.67578125, 0.04296875, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.1796875, 0.9921875, 0.67578125, 0.0, 0.67578125, 0.21875, 0.0, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.0, 0.7734375, 0.2265625, 0.0, 0.67578125, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.49609375, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.671875, 0.0, 0.9921875, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.01953125, 0.6484375, 0.0, 0.0, 0.0, 0.9921875, 0.6328125, 0.0, 0.9921875, 0.671875, 0.67578125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.671875, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.99609375, 0.67578125, 0.67578125, 0.0, 0.0, 0.02734375, 0.0, 0.5859375, 0.046875, 0.0, 0.9921875, 0.0, 0.67578125, 0.67578125, 0.9921875, 0.67578125, 0.66796875, 0.88671875, 0.9921875, 0.05078125, 0.0, 0.0, 0.0, 0.99609375, 0.67578125, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.3359375, 0.0, 0.20703125, 0.67578125, 0.0, 0.0, 0.9921875, 0.81640625, 0.67578125, 0.05078125, 0.9921875, 0.0, 0.0, 0.33984375, 0.67578125, 0.0, 0.0, 0.0, 0.67578125, 0.9921875, 0.0, 0.9921875, 0.67578125, 0.9921875, 0.29296875, 0.9921875, 0.0, 0.0, 0.67578125, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.6484375, 0.046875, 0.0, 0.015625, 0.0, 0.54296875, 0.0, 0.67578125, 0.66796875, 0.67578125, 0.99609375, 0.0, 0.67578125, 0.7265625, 0.9921875, 0.67578125, 0.0, 0.9921875, 0.67578125, 0.0234375, 0.0, 0.02734375, 0.75390625, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.67578125, 0.0, 0.67578125, 0.0, 0.67578125, 0.0, 0.9921875, 0.109375, 0.67578125, 0.0, 0.0, 0.67578125, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.04296875, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.04296875, 0.67578125, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.67578125, 0.8359375, 0.0, 0.0, 0.67578125, 0.9921875, 0.0, 0.67578125, 0.6328125, 0.67578125, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.67578125, 0.0, 0.67578125, 0.9921875, 0.0, 0.0, 0.67578125, 0.0625, 0.0, 0.6171875, 0.0, 0.32421875, 0.0, 0.0, 0.0, 0.875, 0.34765625, 0.67578125, 0.67578125, 0.0, 0.64453125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.05859375, 0.0, 0.0, 0.0, 0.0, 0.67578125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.66796875, 0.9921875, 0.0, 0.0, 0.046875, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.64453125, 0.66796875, 0.0, 0.61328125, 0.67578125, 0.046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.2265625, 0.67578125, 0.66796875, 0.0, 0.0, 0.67578125, 0.67578125, 0.0, 0.0, 0.9921875, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.67578125, 0.171875, 0.125, 0.3359375, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0146484375, 0.9990234375, 0.03515625, 0.0771484375, 0.0048828125, 0.0283203125, 0.0283203125, 0.0, 0.01171875, 0.0556640625, 0.0, 0.10546875, 0.0166015625, 0.0224609375, 0.134765625, 0.0, 0.0078125, 0.91796875, 0.1181640625, 0.0166015625, 0.017578125, 0.0, 0.0751953125, 0.0, 0.013671875, 0.0224609375, 0.0556640625, 0.9990234375, 0.8798828125, 0.0283203125, 0.9970703125, 0.86328125, 0.0625, 0.044921875, 0.0205078125, 0.0185546875, 0.0, 0.1357421875, 0.02734375, 0.8798828125, 0.0048828125, 0.228515625, 0.1572265625, 0.7822265625, 0.0888671875, 0.1015625, 0.2060546875, 0.01953125, 0.861328125, 0.009765625, 0.013671875, 0.212890625, 0.0498046875, 0.0, 0.0458984375, 0.0302734375, 0.1728515625, 0.0, 0.92578125, 0.00390625, 0.01953125, 0.0537109375, 0.998046875, 0.0576171875, 0.0, 0.9990234375, 0.0439453125, 0.130859375, 0.1220703125, 0.0029296875, 0.0, 0.0, 0.009765625, 0.181640625, 0.9453125, 0.4091796875, 0.01171875, 0.8984375, 0.958984375, 0.8935546875, 0.998046875, 0.0, 0.9248046875, 0.9990234375, 0.025390625, 0.1328125, 0.0830078125, 0.1416015625, 0.0, 0.0927734375, 0.017578125, 0.009765625, 0.015625, 0.0, 0.0322265625, 0.033203125, 0.0556640625, 0.9990234375, 0.009765625, 0.0, 0.044921875, 0.1201171875, 0.0107421875, 0.7802734375, 0.0146484375, 0.0, 0.2021484375, 0.0361328125, 0.0, 0.0302734375, 0.12109375, 0.0, 0.943359375, 0.0244140625, 0.0, 0.1982421875, 0.0, 0.029296875, 0.861328125, 0.931640625, 0.9990234375, 0.0, 0.029296875, 0.0419921875, 0.021484375, 0.119140625, 0.009765625, 0.0625, 0.998046875, 0.078125, 0.0361328125, 0.9970703125, 0.94921875, 0.0673828125, 0.0263671875, 0.013671875, 0.01953125, 0.9638671875, 0.1728515625, 0.05078125, 0.0, 0.0, 0.009765625, 0.0, 0.1494140625, 0.0986328125, 0.107421875, 0.0400390625, 0.943359375, 0.783203125, 0.2646484375, 0.9990234375, 0.998046875, 0.01953125, 0.0205078125, 0.0166015625, 0.1591796875, 0.0, 0.0107421875, 0.09765625, 0.947265625, 0.0087890625, 0.1787109375, 0.0, 0.0927734375, 0.97265625, 0.0234375, 0.9970703125, 0.0185546875, 0.021484375, 0.046875, 0.1826171875, 0.91015625, 0.0068359375, 0.06640625, 0.056640625, 0.029296875, 0.01171875, 0.0, 0.029296875, 0.0234375, 0.0322265625, 0.857421875, 0.1865234375, 0.9599609375, 0.0, 0.0068359375, 0.0146484375, 0.015625, 0.2529296875, 0.9404296875, 0.0, 0.0, 0.005859375, 0.01953125, 0.373046875, 0.0419921875, 0.1611328125, 0.0, 0.0947265625, 0.998046875, 0.0, 0.0791015625, 0.0166015625, 0.0, 0.498046875, 0.0, 0.0263671875, 0.810546875, 0.9970703125, 0.0, 0.0166015625, 0.025390625, 0.9169921875, 0.0, 0.76953125, 0.009765625, 0.0166015625, 0.0, 0.9990234375, 0.6982421875, 0.0, 0.1044921875, 0.0, 0.0029296875, 0.01171875, 0.0458984375, 0.009765625, 0.0, 0.041015625, 0.9970703125, 0.810546875, 0.025390625, 0.0068359375, 0.0302734375, 0.0, 0.923828125, 0.0, 0.01953125, 0.017578125, 0.1318359375, 0.2763671875, 0.9970703125, 0.115234375, 0.0, 0.0439453125, 0.09375, 0.0, 0.0107421875, 0.0, 0.12109375, 0.12109375, 0.099609375, 0.998046875, 0.078125, 0.513671875]

 sparsity of   [0.05078125, 0.0685763880610466, 0.0625, 0.9088541865348816, 0.1006944477558136, 0.0950520858168602, 0.0677083358168602, 0.0690104141831398, 0.375, 0.1584201455116272, 0.772569477558136, 0.0030381944961845875, 0.1614583283662796, 0.3485243022441864, 0.2191840261220932, 0.0364583320915699, 0.02604166604578495, 0.075086809694767, 0.009114583022892475, 0.703125, 0.0386284738779068, 0.03515625, 0.067274309694767, 0.2660590410232544, 0.0776909738779068, 0.0, 0.01171875, 0.0, 0.6792534589767456, 0.0538194440305233, 0.0, 0.01215277798473835, 0.0008680555620230734, 0.0, 0.075086809694767, 0.0464409738779068, 0.0477430559694767, 0.5251736044883728, 0.0394965298473835, 0.0386284738779068, 0.0772569477558136, 0.0698784738779068, 0.0173611119389534, 0.02604166604578495, 0.0980902761220932, 0.1215277761220932, 0.0360243059694767, 0.0334201380610466, 0.015625, 0.0, 0.014756944961845875, 0.01215277798473835, 0.1427951455116272, 0.0538194440305233, 0.0477430559694767, 0.9631076455116272, 0.1957465261220932, 0.0125868059694767, 0.01909722201526165, 0.0, 0.1072048619389534, 0.01128472201526165, 0.0707465261220932, 0.625, 0.0, 0.0503472238779068, 0.0360243059694767, 0.1098090261220932, 0.02473958395421505, 0.4769965410232544, 0.010416666977107525, 0.0516493059694767, 0.05078125, 0.1896701455116272, 0.02387152798473835, 0.0008680555620230734, 0.0212673619389534, 0.03081597201526165, 0.0412326380610466, 0.0572916679084301, 0.4592013955116272, 0.0802951380610466, 0.0412326380610466, 0.0, 0.0347222238779068, 0.234375, 0.0282118059694767, 0.6410590410232544, 0.0, 0.01692708395421505, 0.0954861119389534, 0.014756944961845875, 0.014756944961845875, 0.0460069440305233, 0.0677083358168602, 0.01605902798473835, 0.0564236119389534, 0.5633680820465088, 0.0, 0.0, 0.06640625, 0.0607638880610466, 0.0490451380610466, 0.0924479141831398, 0.0438368059694767, 0.0, 0.2326388955116272, 0.1427951455116272, 0.013454861007630825, 0.0004340277810115367, 0.015625, 0.121961809694767, 0.0533854179084301, 0.0785590261220932, 0.0225694440305233, 0.0321180559694767, 0.57421875, 0.014756944961845875, 0.10546875, 0.015625, 0.1041666641831398, 0.0959201380610466, 0.0538194440305233, 0.0460069440305233, 0.0594618059694767, 0.05859375, 0.0386284738779068, 0.0703125, 0.2330729216337204, 0.0499131940305233, 0.0, 0.4544270932674408, 0.1588541716337204, 0.0776909738779068, 0.9986979365348816, 0.84765625, 0.009548611007630825, 0.0338541679084301, 0.0733506977558136, 0.27734375, 0.7191840410232544, 0.02951388992369175, 0.921006977558136, 0.7395833134651184, 0.0, 0.0598958320915699, 0.015625, 0.0086805559694767, 0.098524309694767, 0.0347222238779068, 0.014322916977107525, 0.0243055559694767, 0.0086805559694767, 0.1067708358168602, 0.1015625, 0.0460069440305233, 0.0447048619389534, 0.0460069440305233, 0.5520833134651184, 0.006076388992369175, 0.1883680522441864, 0.0455729179084301, 0.2317708283662796, 0.0551215298473835, 0.1098090261220932, 0.009548611007630825, 0.0399305559694767, 0.06640625, 0.1393229216337204, 0.6388888955116272, 0.2157118022441864, 0.1514756977558136, 0.1115451380610466, 0.0581597238779068, 0.6675347089767456, 0.0967881977558136, 0.0607638880610466, 0.0, 0.0616319440305233, 0.0577256940305233, 0.18359375, 0.0399305559694767, 0.1827256977558136, 0.0373263880610466, 0.9986979365348816, 0.0186631940305233, 0.02300347201526165, 0.0008680555620230734, 0.0, 0.014756944961845875, 0.005642361007630825, 0.0759548619389534, 0.0607638880610466, 0.090711809694767, 0.5872395634651184, 0.2708333432674408, 0.0, 0.0347222238779068, 0.0755208358168602, 0.01128472201526165, 0.0, 0.6549479365348816, 0.9986979365348816, 0.2274305522441864, 0.1002604141831398, 0.0698784738779068, 0.0325520820915699, 0.0004340277810115367, 0.0078125, 0.0303819440305233, 0.3559027910232544, 0.0, 0.0251736119389534, 0.9995659589767456, 0.0438368059694767, 0.0421006940305233, 0.1032986119389534, 0.0, 0.08203125, 0.10546875, 0.01953125, 0.0342881940305233, 0.0677083358168602, 0.0403645820915699, 0.0503472238779068, 0.608506977558136, 0.0321180559694767, 0.0282118059694767, 0.0620659738779068, 0.02734375, 0.0069444444961845875, 0.0065104165114462376, 0.0329861119389534, 0.0, 0.090711809694767, 0.6796875, 0.1671006977558136, 0.1684027761220932, 0.4279513955116272, 0.0234375, 0.0399305559694767, 0.007378472480922937, 0.0034722222480922937, 0.8098958134651184, 0.2521701455116272, 0.0911458358168602, 0.03125, 0.0477430559694767, 0.0499131940305233, 0.0, 0.1193576380610466, 0.0703125, 0.9123263955116272, 0.6354166865348816, 0.01128472201526165, 0.429253488779068]

 sparsity of   [0.0, 0.78515625, 0.76953125, 0.24609375, 0.71875, 0.796875, 0.85546875, 0.0390625, 0.74609375, 0.79296875, 0.16015625, 0.78515625, 0.75390625, 0.85546875, 0.82421875, 0.74609375, 0.75, 0.73046875, 0.421875, 0.75390625, 0.828125, 0.84765625, 0.83984375, 0.0, 0.08203125, 0.83203125, 0.79296875, 0.84765625, 0.2109375, 0.6796875, 0.8515625, 0.73046875, 0.15234375, 0.7265625, 0.05078125, 0.0, 0.85546875, 0.1953125, 0.1015625, 0.8203125, 0.796875, 0.12890625, 0.78515625, 0.77734375, 0.0078125, 0.7734375, 0.796875, 0.7578125, 0.37109375, 0.65625, 0.12109375, 0.71875, 0.0, 0.765625, 0.78125, 0.046875, 0.80078125, 0.78515625, 0.83203125, 0.0234375, 0.609375, 0.0, 0.79296875, 0.80078125, 0.78125, 0.85546875, 0.74609375, 0.85546875, 0.0, 0.53125, 0.671875, 0.0, 0.85546875, 0.62109375, 0.85546875, 0.85546875, 0.85546875, 0.85546875, 0.88671875, 0.75390625, 0.85546875, 0.7890625, 0.85546875, 0.875, 0.07421875, 0.11328125, 0.8046875, 0.7109375, 0.7890625, 0.0, 0.68359375, 0.85546875, 0.85546875, 0.8515625, 0.9921875, 0.8203125, 0.0, 0.0, 0.28515625, 0.0390625, 0.73046875, 0.23046875, 0.0, 0.85546875, 0.85546875, 0.7734375, 0.66796875, 0.7421875, 0.71484375, 0.0, 0.890625, 0.7734375, 0.7890625, 0.03125, 0.76953125, 0.80078125, 0.19140625, 0.00390625, 0.71875, 0.98828125, 0.74609375, 0.81640625, 0.9921875, 0.72265625, 0.0, 0.0, 0.75, 0.81640625, 0.8515625, 0.734375, 0.85546875, 0.76171875, 0.6484375, 0.0, 0.82421875, 0.9921875, 0.03515625, 0.8203125, 0.0, 0.765625, 0.7265625, 0.67578125, 0.7109375, 0.77734375, 0.7890625, 0.0, 0.8046875, 0.7578125, 0.15234375, 0.83203125, 0.8359375, 0.0, 0.85546875, 0.71875, 0.07421875, 0.76171875, 0.84375, 0.83203125, 0.85546875, 0.05859375, 0.78125, 0.7421875, 0.0, 0.7890625, 0.1015625, 0.90625, 0.73828125, 0.7109375, 0.71875, 0.79296875, 0.84375, 0.85546875, 0.8046875, 0.08984375, 0.85546875, 0.80859375, 0.015625, 0.7265625, 0.78515625, 0.71484375, 0.75390625, 0.8125, 0.01171875, 0.0, 0.64453125, 0.80078125, 0.78125, 0.08203125, 0.76953125, 0.99609375, 0.8515625, 0.7109375, 0.24609375, 0.85546875, 0.24609375, 0.78515625, 0.8515625, 0.0546875, 0.85546875, 0.85546875, 0.7578125, 0.7578125, 0.85546875, 0.79296875, 0.046875, 0.09765625, 0.85546875, 0.09765625, 0.0, 0.06640625, 0.71484375, 0.66015625, 0.75, 0.14453125, 0.9921875, 0.8515625, 0.73046875, 0.71484375, 0.828125, 0.70703125, 0.71875, 0.765625, 0.76171875, 0.72265625, 0.0390625, 0.8046875, 0.76953125, 0.78125, 0.8046875, 0.1015625, 0.00390625, 0.8515625, 0.75390625, 0.85546875, 0.0, 0.85546875, 0.80859375, 0.2265625, 0.81640625, 0.21875, 0.00390625, 0.15625, 0.0, 0.29296875, 0.7265625, 0.79296875, 0.67578125, 0.85546875, 0.0, 0.0546875, 0.00390625, 0.0078125, 0.80078125, 0.74609375, 0.72265625, 0.05859375, 0.79296875, 0.796875, 0.6953125, 0.7890625, 0.80078125, 0.85546875, 0.8125, 0.8515625, 0.8359375, 0.9921875, 0.72265625, 0.15625, 0.9921875, 0.83984375, 0.78515625, 0.82421875, 0.7578125, 0.85546875, 0.69921875, 0.84765625, 0.9921875, 0.70703125, 0.77734375, 0.03125, 0.90234375, 0.73828125, 0.0703125, 0.0, 0.62109375, 0.05078125, 0.84765625, 0.85546875, 0.76171875, 0.76171875, 0.765625, 0.85546875, 0.8203125, 0.83984375, 0.73828125, 0.74609375, 0.04296875, 0.66015625, 0.76953125, 0.69140625, 0.80859375, 0.125, 0.8359375, 0.85546875, 0.84765625, 0.15625, 0.04296875, 0.83984375, 0.0390625, 0.828125, 0.99609375, 0.02734375, 0.75390625, 0.90625, 0.79296875, 0.76171875, 0.85546875, 0.109375, 0.16796875, 0.85546875, 0.06640625, 0.0390625, 0.75, 0.12109375, 0.671875, 0.89453125, 0.83203125, 0.05078125, 0.85546875, 0.79296875, 0.69140625, 0.83984375, 0.85546875, 0.1875, 0.71875, 0.15625, 0.765625, 0.12890625, 0.85546875, 0.05859375, 0.08984375, 0.75390625, 0.0, 0.85546875, 0.703125, 0.0, 0.8046875, 0.76171875, 0.85546875, 0.703125, 0.703125, 0.85546875, 0.8515625, 0.85546875, 0.03515625, 0.203125, 0.828125, 0.03125, 0.0, 0.74609375, 0.78515625, 0.85546875, 0.83984375, 0.77734375, 0.0, 0.0, 0.00390625, 0.76953125, 0.9921875, 0.73828125, 0.85546875, 0.77734375, 0.6796875, 0.73046875, 0.70703125, 0.05859375, 0.70703125, 0.828125, 0.80078125, 0.0, 0.85546875, 0.0, 0.06640625, 0.0, 0.80078125, 0.58984375, 0.0546875, 0.609375, 0.7890625, 0.81640625, 0.83203125, 0.2734375, 0.7890625, 0.79296875, 0.81640625, 0.796875, 0.03125, 0.79296875, 0.76171875, 0.81640625, 0.77734375, 0.85546875, 0.8046875, 0.71875, 0.75, 0.15625, 0.7578125, 0.75390625, 0.85546875, 0.9921875, 0.75, 0.8125, 0.80078125, 0.80078125, 0.36328125, 0.85546875, 0.0, 0.77734375, 0.9921875, 0.76953125, 0.76953125, 0.85546875, 0.71875, 0.015625, 0.0, 0.82421875, 0.0, 0.75, 0.9921875, 0.76953125, 0.65625, 0.6640625, 0.17578125, 0.0, 0.02734375, 0.0625, 0.8203125, 0.69140625, 0.0, 0.8203125, 0.84765625, 0.85546875, 0.73828125, 0.0703125, 0.81640625, 0.68359375, 0.7265625, 0.70703125, 0.8125, 0.80078125, 0.8046875, 0.85546875, 0.0, 0.8125, 0.85546875, 0.85546875, 0.76171875, 0.80859375, 0.6875, 0.85546875, 0.0546875, 0.80078125, 0.75, 0.80859375, 0.75, 0.85546875, 0.7421875, 0.85546875, 0.85546875, 0.765625, 0.03515625, 0.68359375, 0.02734375, 0.69921875, 0.5703125, 0.71875, 0.71484375, 0.85546875, 0.85546875, 0.0, 0.9921875, 0.76953125, 0.0, 0.80078125, 0.02734375, 0.796875, 0.7890625, 0.8203125, 0.85546875, 0.73828125, 0.01953125, 0.09765625, 0.78125, 0.0, 0.2734375, 0.75, 0.77734375, 0.85546875, 0.13671875, 0.76171875, 0.0546875, 0.0078125, 0.140625, 0.5625, 0.9921875, 0.85546875, 0.8046875, 0.9921875, 0.85546875, 0.77734375, 0.85546875, 0.85546875, 0.0, 0.85546875, 0.76171875, 0.8515625, 0.85546875, 0.796875, 0.85546875, 0.75390625, 0.79296875, 0.70703125, 0.85546875, 0.07421875, 0.8515625, 0.82421875, 0.0, 0.7734375, 0.0, 0.05078125, 0.03515625, 0.76953125, 0.78515625, 0.8125, 0.7890625, 0.78125, 0.83203125, 0.85546875, 0.75, 0.7265625, 0.0, 0.734375, 0.85546875, 0.76171875, 0.76953125, 0.74609375, 0.03515625, 0.77734375, 0.75390625, 0.70703125, 0.7421875, 0.765625, 0.08203125, 0.50390625, 0.8203125, 0.81640625, 0.73828125, 0.66015625, 0.79296875, 0.5859375, 0.12109375, 0.7734375, 0.03515625, 0.0625, 0.765625, 0.078125, 0.0, 0.765625, 0.0, 0.78515625, 0.7734375, 0.88671875, 0.75390625, 0.81640625, 0.9921875, 0.9921875, 0.12890625, 0.78515625, 0.85546875, 0.85546875, 0.01953125, 0.04296875, 0.75, 0.2109375, 0.0625, 0.85546875, 0.7890625, 0.74609375, 0.8046875, 0.85546875, 0.02734375, 0.0, 0.8203125, 0.05859375, 0.8515625, 0.68359375, 0.828125, 0.703125, 0.78515625, 0.16015625, 0.8515625, 0.85546875, 0.1875, 0.81640625, 0.8515625, 0.74609375, 0.85546875, 0.72265625, 0.75, 0.1328125, 0.85546875, 0.8125, 0.85546875, 0.9921875, 0.78125, 0.75, 0.7890625, 0.078125, 0.85546875, 0.71484375, 0.99609375, 0.0, 0.0, 0.15234375, 0.7578125, 0.71484375, 0.73828125, 0.80078125, 0.85546875, 0.8515625, 0.98828125, 0.7890625, 0.73828125, 0.03125, 0.85546875, 0.78515625, 0.7578125, 0.8515625, 0.72265625, 0.79296875, 0.75390625, 0.0, 0.14453125, 0.6953125, 0.85546875, 0.7890625, 0.7109375, 0.75390625, 0.78515625, 0.25390625, 0.7421875, 0.48828125, 0.76953125, 0.0, 0.8046875, 0.85546875, 0.85546875, 0.7890625, 0.85546875, 0.17578125, 0.0, 0.7109375, 0.76171875, 0.85546875, 0.8125, 0.00390625, 0.6015625, 0.25, 0.85546875, 0.7265625, 0.79296875, 0.82421875, 0.7890625, 0.85546875, 0.71875, 0.75390625, 0.765625, 0.03515625, 0.05078125, 0.76171875, 0.0, 0.78515625, 0.85546875, 0.9921875, 0.78515625, 0.9921875, 0.78515625, 0.7265625, 0.75390625, 0.85546875, 0.74609375, 0.26171875, 0.06640625, 0.77734375, 0.85546875, 0.125, 0.9921875, 0.7734375, 0.37109375, 0.71484375, 0.0, 0.83984375, 0.85546875, 0.0, 0.80859375, 0.6484375, 0.85546875, 0.08203125, 0.703125, 0.8046875, 0.7890625, 0.203125, 0.7421875, 0.7265625, 0.78515625, 0.77734375, 0.67578125, 0.74609375, 0.0234375, 0.80078125, 0.81640625, 0.85546875, 0.7421875, 0.02734375, 0.796875, 0.859375, 0.03125, 0.84375, 0.76171875, 0.0, 0.75390625, 0.73046875, 0.04296875, 0.8359375, 0.09765625, 0.59765625, 0.78125, 0.85546875, 0.80859375, 0.0, 0.046875, 0.73828125, 0.0625, 0.8046875, 0.79296875, 0.85546875, 0.85546875, 0.0, 0.47265625, 0.76953125, 0.79296875, 0.84765625, 0.85546875, 0.7421875, 0.79296875, 0.84375, 0.82421875, 0.75390625, 0.8046875, 0.0, 0.99609375, 0.85546875, 0.00390625, 0.9921875, 0.7421875, 0.77734375, 0.76171875, 0.03125, 0.7734375, 0.0390625, 0.74609375, 0.3515625, 0.0234375, 0.85546875, 0.07421875, 0.796875, 0.0625, 0.703125, 0.66015625, 0.7265625, 0.84765625, 0.765625, 0.71484375, 0.0, 0.9921875, 0.76953125, 0.8046875, 0.85546875, 0.0, 0.8515625, 0.73828125, 0.85546875, 0.765625, 0.9921875, 0.765625, 0.7578125, 0.77734375, 0.85546875, 0.84375, 0.7578125, 0.8359375, 0.14453125, 0.65625, 0.9921875, 0.71875, 0.8125, 0.0078125, 0.73046875, 0.85546875, 0.09765625, 0.7578125, 0.80078125, 0.85546875, 0.703125, 0.84375, 0.0, 0.6875, 0.80859375, 0.85546875, 0.7109375, 0.06640625, 0.76953125, 0.85546875, 0.71484375, 0.6796875, 0.8515625, 0.03125, 0.9921875, 0.765625, 0.85546875, 0.05078125, 0.0703125, 0.82421875, 0.0, 0.0, 0.8046875, 0.8125, 0.9921875, 0.0, 0.7578125, 0.17578125, 0.7421875, 0.79296875, 0.85546875, 0.85546875, 0.8359375, 0.83984375, 0.6171875, 0.7421875, 0.8515625, 0.734375, 0.85546875, 0.9921875, 0.85546875, 0.80078125, 0.8515625, 0.72265625, 0.85546875, 0.0, 0.0625, 0.85546875, 0.74609375, 0.99609375, 0.828125, 0.80078125, 0.81640625, 0.99609375, 0.8515625, 0.78125, 0.02734375, 0.078125, 0.85546875, 0.796875, 0.9921875, 0.9921875, 0.08984375, 0.7578125, 0.8125, 0.85546875, 0.85546875, 0.03515625, 0.84765625, 0.0, 0.625, 0.0, 0.0234375, 0.85546875, 0.703125, 0.80078125, 0.74609375, 0.828125, 0.234375, 0.80078125, 0.8046875, 0.8125, 0.75390625, 0.4375, 0.83984375, 0.85546875, 0.76953125, 0.0390625, 0.8515625, 0.67578125, 0.62109375, 0.8125, 0.71875, 0.0, 0.78515625, 0.0703125, 0.85546875, 0.0546875, 0.78515625, 0.81640625, 0.69140625, 0.8125, 0.66015625, 0.78125, 0.08984375, 0.0234375, 0.74609375, 0.85546875, 0.75, 0.77734375, 0.75, 0.75, 0.85546875, 0.796875, 0.75, 0.046875, 0.76953125, 0.7265625, 0.85546875, 0.00390625, 0.7890625, 0.78125, 0.78125, 0.9921875, 0.63671875, 0.0, 0.76953125, 0.85546875, 0.0, 0.796875, 0.125, 0.8203125, 0.0, 0.71484375, 0.76953125, 0.0, 0.8515625, 0.79296875, 0.85546875, 0.796875, 0.078125, 0.0, 0.09375, 0.76953125, 0.03125, 0.0234375, 0.75390625, 0.734375, 0.7109375, 0.85546875, 0.796875, 0.80859375, 0.85546875, 0.85546875, 0.109375, 0.07421875, 0.85546875, 0.80859375, 0.73828125, 0.85546875, 0.05078125, 0.80078125, 0.79296875, 0.72265625, 0.8046875, 0.84765625, 0.703125, 0.79296875, 0.05859375, 0.6953125, 0.2265625, 0.15234375, 0.05859375, 0.92578125, 0.98828125, 0.765625, 0.2421875, 0.8515625, 0.7734375, 0.85546875, 0.82421875, 0.0703125, 0.07421875, 0.85546875, 0.0, 0.7265625, 0.98828125, 0.1171875, 0.75, 0.8125, 0.66796875, 0.80078125, 0.85546875, 0.55078125, 0.0, 0.73828125, 0.0, 0.79296875, 0.15625, 0.7578125, 0.69921875, 0.796875, 0.03515625, 0.73828125, 0.03515625, 0.78515625, 0.99609375, 0.85546875, 0.203125, 0.53125, 0.0]

 sparsity of   [0.0458984375, 0.0390625, 0.17578125, 0.970703125, 0.0390625, 0.037109375, 0.02734375, 0.0205078125, 0.087890625, 0.0, 0.033203125, 0.0, 0.7763671875, 0.0302734375, 0.033203125, 0.9248046875, 0.0, 0.9970703125, 0.0654296875, 0.8369140625, 0.013671875, 0.0478515625, 0.0, 0.021484375, 0.0908203125, 0.7900390625, 0.048828125, 0.83984375, 0.029296875, 0.0517578125, 0.021484375, 0.021484375, 0.0205078125, 0.07421875, 0.0244140625, 0.115234375, 0.0244140625, 0.0625, 0.0, 0.08984375, 0.0439453125, 0.0, 0.0322265625, 0.0625, 0.05859375, 0.0390625, 0.029296875, 0.9970703125, 0.0390625, 0.087890625, 0.0478515625, 0.0, 0.015625, 0.017578125, 0.0283203125, 0.029296875, 0.099609375, 0.052734375, 0.1083984375, 0.91796875, 0.0, 0.09765625, 0.998046875, 0.998046875, 0.07421875, 0.099609375, 0.10546875, 0.0185546875, 0.0, 0.021484375, 0.302734375, 0.0458984375, 0.0791015625, 0.998046875, 0.0927734375, 0.841796875, 0.0517578125, 0.056640625, 0.0458984375, 0.09375, 0.2265625, 0.998046875, 0.0185546875, 0.01171875, 0.0625, 0.998046875, 0.9970703125, 0.935546875, 0.0166015625, 0.0537109375, 0.0068359375, 0.125, 0.224609375, 0.0234375, 0.0927734375, 0.0224609375, 0.0537109375, 0.998046875, 0.046875, 0.125, 0.05859375, 0.8662109375, 0.0908203125, 0.0107421875, 0.037109375, 0.083984375, 0.03125, 0.9287109375, 0.0, 0.046875, 0.5537109375, 0.087890625, 0.953125, 0.1181640625, 0.0, 0.1318359375, 0.998046875, 0.7998046875, 0.0, 0.8037109375, 0.033203125, 0.015625, 0.962890625, 0.0, 0.0, 0.0576171875, 0.056640625, 0.0029296875, 0.998046875, 0.0849609375, 0.1318359375, 0.0712890625, 0.041015625, 0.7392578125, 0.998046875, 0.001953125, 0.0810546875, 0.0302734375, 0.048828125, 0.0, 0.064453125, 0.998046875, 0.9404296875, 0.1123046875, 0.0458984375, 0.025390625, 0.998046875, 0.998046875, 0.9375, 0.0146484375, 0.0, 0.9150390625, 0.048828125, 0.0771484375, 0.056640625, 0.927734375, 0.0458984375, 0.8486328125, 0.998046875, 0.09765625, 0.0625, 0.998046875, 0.0, 0.04296875, 0.1298828125, 0.0546875, 0.0380859375, 0.9189453125, 0.0380859375, 0.037109375, 0.974609375, 0.0634765625, 0.998046875, 0.0693359375, 0.025390625, 0.998046875, 0.048828125, 0.083984375, 0.0, 0.0009765625, 0.037109375, 0.0185546875, 0.0, 0.0, 0.0322265625, 0.2294921875, 0.0, 0.0234375, 0.03515625, 0.91015625, 0.8994140625, 0.0625, 0.013671875, 0.193359375, 0.021484375, 0.998046875, 0.009765625, 0.033203125, 0.998046875, 0.0419921875, 0.0, 0.068359375, 0.048828125, 0.03125, 0.998046875, 0.0, 0.80078125, 0.0380859375, 0.0634765625, 0.087890625, 0.0166015625, 0.0, 0.025390625, 0.0732421875, 0.0439453125, 0.998046875, 0.9970703125, 0.1171875, 0.0595703125, 0.9609375, 0.9970703125, 0.01953125, 0.2470703125, 0.0, 0.0380859375, 0.076171875, 0.1484375, 0.8994140625, 0.998046875, 0.09375, 0.0634765625, 0.9970703125, 0.9306640625, 0.0, 0.0634765625, 0.0078125, 0.0361328125, 0.060546875, 0.998046875, 0.111328125, 0.095703125, 0.1533203125, 0.0322265625, 0.998046875, 0.9296875, 0.8154296875, 0.2021484375, 0.849609375, 0.017578125, 0.9970703125, 0.896484375, 0.0, 0.056640625, 0.0205078125, 0.931640625, 0.998046875]

 sparsity of   [0.0403645820915699, 0.8550347089767456, 0.1158854141831398, 0.01605902798473835, 0.1705729216337204, 0.0894097238779068, 0.3815104067325592, 0.02604166604578495, 0.0572916679084301, 0.02170138992369175, 0.9995659589767456, 0.02951388992369175, 0.710069477558136, 0.0455729179084301, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.02560763992369175, 0.9986979365348816, 0.01171875, 0.05078125, 0.940538227558136, 0.0625, 0.0, 0.0447048619389534, 0.01953125, 0.0125868059694767, 0.1987847238779068, 0.01996527798473835, 0.0451388880610466, 0.114149309694767, 0.2395833283662796, 0.0334201380610466, 0.1905381977558136, 0.01953125, 0.0685763880610466, 0.0421006940305233, 0.8502604365348816, 0.0833333358168602, 0.5594618320465088, 0.0173611119389534, 0.8641493320465088, 0.9995659589767456, 0.0520833320915699, 0.0086805559694767, 0.0724826380610466, 0.0303819440305233, 0.142361119389534, 0.9275173544883728, 0.0186631940305233, 0.0, 0.013888888992369175, 0.999131977558136, 0.7018229365348816, 0.9986979365348816, 0.0529513880610466, 0.0234375, 0.0447048619389534, 0.9995659589767456, 0.0399305559694767, 0.02213541604578495, 0.02864583395421505, 0.0533854179084301, 0.0377604179084301, 0.4110243022441864, 0.106336809694767, 0.0737847238779068, 0.0802951380610466, 0.2591145932674408, 0.006076388992369175, 0.0225694440305233, 0.9986979365348816, 0.9995659589767456, 0.9275173544883728, 0.0225694440305233, 0.1154513880610466, 0.01909722201526165, 0.9986979365348816, 0.9995659589767456, 0.0638020858168602, 0.013454861007630825, 0.0455729179084301, 0.1254340261220932, 0.8142361044883728, 0.01953125, 0.639756977558136, 0.0321180559694767, 0.0, 0.1605902761220932, 0.0924479141831398, 0.7439236044883728, 0.9079861044883728, 0.0360243059694767, 0.0716145858168602, 0.0212673619389534, 0.04296875, 0.0052083334885537624, 0.796006977558136, 0.2578125, 0.03515625, 0.18359375, 0.0620659738779068, 0.0321180559694767, 0.0729166641831398, 0.098524309694767, 0.0833333358168602, 0.03125, 0.1215277761220932, 0.0477430559694767, 0.1028645858168602, 0.0859375, 0.0329861119389534, 0.999131977558136, 0.0125868059694767, 0.9626736044883728, 0.963975727558136, 0.0603298619389534, 0.9565972089767456, 0.0503472238779068, 0.0407986119389534, 0.6328125, 0.1484375, 0.7317708134651184, 0.0815972238779068, 0.0594618059694767, 0.0564236119389534, 0.1961805522441864, 0.9986979365348816, 0.8216145634651184, 0.0, 0.03515625, 0.010850694961845875, 0.0464409738779068, 0.7513020634651184, 0.9435763955116272, 0.02213541604578495, 0.0173611119389534, 0.0, 0.063368059694767, 0.090711809694767, 0.0920138880610466, 0.009982638992369175, 0.8815104365348816, 0.0625, 0.2317708283662796, 0.1440972238779068, 0.7790798544883728, 0.0516493059694767, 0.9322916865348816, 0.1032986119389534, 0.9986979365348816, 0.9045138955116272, 0.999131977558136, 0.0290798619389534, 0.0, 0.0568576380610466, 0.1592881977558136, 0.0694444477558136, 0.6844618320465088, 0.0659722238779068, 0.014756944961845875, 0.0334201380610466, 0.8754340410232544, 0.0164930559694767, 0.9175347089767456, 0.8237847089767456, 0.846788227558136, 0.1905381977558136, 0.01605902798473835, 0.109375, 0.0186631940305233, 0.8997395634651184, 0.0347222238779068, 0.9986979365348816, 0.0525173619389534, 0.999131977558136, 0.0052083334885537624, 0.0, 0.0837673619389534, 0.0069444444961845875, 0.0, 0.1779513955116272, 0.9353298544883728, 0.0451388880610466, 0.009982638992369175, 0.01779513992369175, 0.0125868059694767, 0.9986979365348816, 0.0, 0.1228298619389534, 0.2296006977558136, 0.197048619389534, 0.02083333395421505, 0.02300347201526165, 0.0, 0.02083333395421505, 0.1028645858168602, 0.0303819440305233, 0.014756944961845875, 0.0026041667442768812, 0.00390625, 0.0993923619389534, 0.01128472201526165, 0.02604166604578495, 0.0533854179084301, 0.0954861119389534, 0.0125868059694767, 0.0473090298473835, 0.0, 0.2252604216337204, 0.0225694440305233, 0.0394965298473835, 0.7990451455116272, 0.1280381977558136, 0.9995659589767456, 0.0052083334885537624, 0.0381944440305233, 0.2421875, 0.3307291567325592, 0.3229166567325592, 0.03081597201526165, 0.0, 0.9986979365348816, 0.0360243059694767, 0.014756944961845875, 0.02300347201526165, 0.067274309694767, 0.1948784738779068, 0.83984375, 0.8307291865348816, 0.0447048619389534, 0.086805559694767, 0.0881076380610466, 0.01996527798473835, 0.11328125, 0.0824652761220932, 0.0, 0.0846354141831398, 0.8650173544883728, 0.098524309694767, 0.5915798544883728, 0.0555555559694767, 0.5125868320465088, 0.013020833022892475, 0.9223090410232544, 0.776475727558136, 0.0555555559694767, 0.013888888992369175, 0.999131977558136, 0.9986979365348816, 0.015625, 0.013888888992369175, 0.0598958320915699, 0.0846354141831398, 0.1905381977558136, 0.014322916977107525]

 sparsity of   [0.93359375, 0.8984375, 0.93359375, 0.93359375, 0.81640625, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.8203125, 0.89453125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.93359375, 0.75, 0.875, 0.89453125, 0.765625, 0.0, 0.00390625, 0.9921875, 0.87109375, 0.78125, 0.93359375, 0.91796875, 0.79296875, 0.93359375, 0.890625, 0.93359375, 0.828125, 0.93359375, 0.875, 0.93359375, 0.0, 0.99609375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.89453125, 0.859375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.74609375, 0.88671875, 0.93359375, 0.93359375, 0.7890625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.7734375, 0.93359375, 0.8515625, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.875, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.0, 0.890625, 0.859375, 0.0, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.9921875, 0.86328125, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.81640625, 0.93359375, 0.93359375, 0.75, 0.90625, 0.93359375, 0.859375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.93359375, 0.8828125, 0.93359375, 0.93359375, 0.93359375, 0.99609375, 0.93359375, 0.74609375, 0.83203125, 0.9921875, 0.88671875, 0.9921875, 0.93359375, 0.93359375, 0.0, 0.890625, 0.8828125, 0.93359375, 0.93359375, 0.9921875, 0.88671875, 0.93359375, 0.93359375, 0.93359375, 0.85546875, 0.71875, 0.87890625, 0.93359375, 0.9296875, 0.0, 0.87890625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.0, 0.93359375, 0.8125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.7265625, 0.85546875, 0.93359375, 0.93359375, 0.0, 0.9921875, 0.86328125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.93359375, 0.875, 0.93359375, 0.7421875, 0.00390625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8671875, 0.8203125, 0.93359375, 0.87109375, 0.78125, 0.93359375, 0.0, 0.93359375, 0.9296875, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.93359375, 0.82421875, 0.890625, 0.93359375, 0.0, 0.0, 0.99609375, 0.93359375, 0.890625, 0.9296875, 0.78515625, 0.93359375, 0.8203125, 0.93359375, 0.93359375, 0.90234375, 0.0, 0.9296875, 0.93359375, 0.8515625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.7265625, 0.08203125, 0.88671875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.9296875, 0.8828125, 0.93359375, 0.9140625, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.93359375, 0.93359375, 0.93359375, 0.89453125, 0.93359375, 0.48828125, 0.99609375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.859375, 0.93359375, 0.9296875, 0.87890625, 0.82421875, 0.86328125, 0.86328125, 0.86328125, 0.890625, 0.93359375, 0.01171875, 0.93359375, 0.0, 0.9921875, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.89453125, 0.93359375, 0.875, 0.89453125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.99609375, 0.93359375, 0.90625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.87109375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.87109375, 0.93359375, 0.78125, 0.93359375, 0.0, 0.99609375, 0.93359375, 0.88671875, 0.8671875, 0.93359375, 0.71875, 0.86328125, 0.93359375, 0.93359375, 0.93359375, 0.8828125, 0.8828125, 0.71484375, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.23046875, 0.99609375, 0.734375, 0.93359375, 0.93359375, 0.8515625, 0.93359375, 0.0, 0.859375, 0.828125, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.890625, 0.88671875, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.8359375, 0.875, 0.80859375, 0.93359375, 0.93359375, 0.93359375, 0.90625, 0.93359375, 0.93359375, 0.93359375, 0.8671875, 0.7265625, 0.0, 0.828125, 0.90234375, 0.93359375, 0.93359375, 0.93359375, 0.81640625, 0.87109375, 0.93359375, 0.88671875, 0.93359375, 0.87109375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.87890625, 0.93359375, 0.99609375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.87890625, 0.93359375, 0.93359375, 0.93359375, 0.95703125, 0.93359375, 0.93359375, 0.9921875, 0.84765625, 0.93359375, 0.8828125, 0.93359375, 0.93359375, 0.93359375, 0.99609375, 0.83984375, 0.93359375, 0.859375, 0.93359375, 0.0, 0.875, 0.7265625, 0.93359375, 0.99609375, 0.875, 0.7890625, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.13671875, 0.93359375, 0.9921875, 0.19921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8828125, 0.93359375, 0.99609375, 0.93359375, 0.84765625, 0.93359375, 0.87890625, 0.93359375, 0.93359375, 0.8984375, 0.9296875, 0.9296875, 0.0, 0.99609375, 0.93359375, 0.93359375, 0.93359375, 0.890625, 0.93359375, 0.77734375, 0.84375, 0.88671875, 0.93359375, 0.0, 0.99609375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.99609375, 0.93359375, 0.0, 0.0, 0.90234375, 0.93359375, 0.93359375, 0.8515625, 0.0, 0.93359375, 0.93359375, 0.8828125, 0.93359375, 0.90234375, 0.93359375, 0.859375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.93359375, 0.74609375, 0.8984375, 0.8828125, 0.93359375, 0.93359375, 0.890625, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.8515625, 0.88671875, 0.90234375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.17578125, 0.93359375, 0.88671875, 0.93359375, 0.93359375, 0.75, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.93359375, 0.93359375, 0.0, 0.7265625, 0.8828125, 0.74609375, 0.93359375, 0.8359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.87109375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.0, 0.859375, 0.93359375, 0.0, 0.875, 0.875, 0.93359375, 0.90625, 0.96484375, 0.99609375, 0.93359375, 0.88671875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8515625, 0.93359375, 0.93359375, 0.93359375, 0.84765625, 0.9296875, 0.93359375, 0.87109375, 0.93359375, 0.9609375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.8671875, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.99609375, 0.0, 0.875, 0.87890625, 0.8828125, 0.93359375, 0.93359375, 0.93359375, 0.890625, 0.93359375, 0.74609375, 0.93359375, 0.9921875, 0.0, 0.93359375, 0.89453125, 0.93359375, 0.93359375, 0.84765625, 0.9921875, 0.88671875, 0.93359375, 0.8671875, 0.93359375, 0.875, 0.88671875, 0.0, 0.93359375, 0.9921875, 0.87890625, 0.8671875, 0.93359375, 0.93359375, 0.93359375, 0.91796875, 0.7265625, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.86328125, 0.93359375, 0.93359375, 0.84375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8671875, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.90234375, 0.83203125, 0.93359375, 0.828125, 0.93359375, 0.93359375, 0.88671875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8828125, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.8828125, 0.88671875, 0.93359375, 0.9921875, 0.91015625, 0.99609375, 0.8515625, 0.93359375, 0.83203125, 0.0, 0.0, 0.88671875, 0.93359375, 0.9921875, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.8984375, 0.79296875, 0.93359375, 0.93359375, 0.8671875, 0.93359375, 0.87890625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.9921875, 0.8828125, 0.93359375, 0.93359375, 0.93359375, 0.859375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.84765625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.92578125, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.90234375, 0.8359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.765625, 0.9921875, 0.93359375, 0.7890625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8515625, 0.89453125, 0.5625, 0.90625, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.86328125, 0.99609375, 0.9296875, 0.93359375, 0.8828125, 0.0, 0.87890625, 0.90234375, 0.93359375, 0.875, 0.93359375, 0.90234375, 0.765625, 0.87890625, 0.9921875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.87109375, 0.74609375, 0.93359375, 0.93359375, 0.93359375, 0.92578125, 0.7265625, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.88671875, 0.89453125, 0.87109375, 0.0, 0.88671875, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.87890625, 0.93359375, 0.9296875, 0.93359375, 0.99609375, 0.0, 0.93359375, 0.8828125, 0.87890625, 0.88671875, 0.0, 0.9296875, 0.0, 0.93359375, 0.9921875, 0.828125, 0.0, 0.9921875, 0.93359375, 0.85546875, 0.90625, 0.99609375, 0.921875, 0.86328125, 0.9921875, 0.9921875, 0.93359375, 0.76171875, 0.91015625, 0.90234375, 0.93359375, 0.7265625, 0.93359375, 0.87890625, 0.72265625, 0.73046875, 0.9921875, 0.8671875, 0.99609375, 0.7265625, 0.8828125, 0.09375, 0.93359375, 0.93359375, 0.7890625, 0.93359375, 0.7890625, 0.93359375, 0.93359375, 0.83984375, 0.93359375, 0.93359375, 0.93359375, 0.99609375, 0.93359375, 0.93359375, 0.87890625, 0.93359375, 0.93359375, 0.7109375, 0.8515625, 0.9296875, 0.93359375, 0.87890625, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.0, 0.9921875, 0.18359375, 0.93359375, 0.0, 0.93359375, 0.88671875, 0.85546875, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.765625, 0.78125, 0.84765625, 0.93359375, 0.890625, 0.93359375, 0.875, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.7578125, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.92578125, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.9296875, 0.93359375, 0.99609375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.93359375, 0.875, 0.9921875, 0.82421875, 0.93359375, 0.93359375, 0.8828125, 0.93359375, 0.89453125, 0.89453125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.8671875, 0.93359375, 0.93359375, 0.8828125, 0.8984375, 0.765625, 0.8203125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.72265625, 0.7421875, 0.93359375, 0.0, 0.93359375, 0.8671875, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.8671875, 0.93359375, 0.0, 0.8671875, 0.86328125, 0.93359375, 0.93359375, 0.93359375, 0.93359375, 0.890625, 0.890625, 0.9921875, 0.6796875, 0.734375, 0.93359375, 0.83984375, 0.93359375, 0.9921875, 0.93359375, 0.93359375, 0.80078125, 0.88671875, 0.87890625, 0.93359375, 0.93359375, 0.93359375, 0.0, 0.99609375, 0.93359375, 0.83203125, 0.99609375, 0.9921875, 0.99609375, 0.93359375, 0.9921875, 0.859375, 0.89453125, 0.93359375, 0.93359375, 0.0, 0.93359375, 0.93359375, 0.86328125, 0.93359375, 0.83984375, 0.93359375, 0.93359375, 0.93359375, 0.8984375, 0.93359375, 0.93359375, 0.93359375, 0.78125, 0.93359375, 0.93359375, 0.89453125, 0.93359375, 0.93359375, 0.0, 0.734375, 0.93359375, 0.93359375, 0.828125, 0.99609375, 0.93359375, 0.77734375, 0.0, 0.93359375]

 sparsity of   [0.078125, 0.939453125, 0.998046875, 0.3720703125, 0.0390625, 0.2138671875, 0.087890625, 0.5126953125, 0.0458984375, 0.998046875, 0.998046875, 0.1708984375, 0.9072265625, 0.1591796875, 0.9990234375, 0.998046875, 0.9970703125, 0.0390625, 0.080078125, 0.9990234375, 0.0439453125, 0.037109375, 0.9990234375, 0.0712890625, 0.0771484375, 0.921875, 0.05859375, 0.1357421875, 0.0439453125, 0.10546875, 0.1484375, 0.998046875, 0.1240234375, 0.0322265625, 0.08203125, 0.9970703125, 0.0439453125, 0.9990234375, 0.0400390625, 0.01953125, 0.0654296875, 0.0595703125, 0.056640625, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.8544921875, 0.998046875, 0.1689453125, 0.1708984375, 0.9970703125, 0.05859375, 0.1171875, 0.466796875, 0.998046875, 0.033203125, 0.109375, 0.2451171875, 0.9970703125, 0.2109375, 0.189453125, 0.0341796875, 0.017578125, 0.0888671875, 0.03125, 0.2109375, 0.0498046875, 0.3984375, 0.181640625, 0.14453125, 0.01953125, 0.029296875, 0.9970703125, 0.1220703125, 0.083984375, 0.41796875, 0.0380859375, 0.0751953125, 0.9970703125, 0.998046875, 0.01953125, 0.998046875, 0.0546875, 0.046875, 0.0615234375, 0.998046875, 0.998046875, 0.1123046875, 0.0478515625, 0.3095703125, 0.033203125, 0.998046875, 0.03515625, 0.998046875, 0.083984375, 0.10546875, 0.1181640625, 0.998046875, 0.876953125, 0.048828125, 0.056640625, 0.037109375, 0.0419921875, 0.998046875, 0.9990234375, 0.109375, 0.998046875, 0.3984375, 0.998046875, 0.06640625, 0.998046875, 0.0546875, 0.033203125, 0.0537109375, 0.0556640625, 0.03515625, 0.0791015625, 0.068359375, 0.03515625, 0.998046875, 0.0361328125, 0.09375, 0.08203125, 0.9169921875, 0.9970703125, 0.998046875, 0.9990234375, 0.078125, 0.0888671875, 0.998046875, 0.998046875, 0.0234375, 0.998046875, 0.9970703125, 0.9970703125, 0.9990234375, 0.1064453125, 0.08203125, 0.1181640625, 0.1064453125, 0.1025390625, 0.9970703125, 0.998046875, 0.0390625, 0.998046875, 0.9990234375, 0.02734375, 0.0986328125, 0.029296875, 0.140625, 0.998046875, 0.0673828125, 0.05859375, 0.998046875, 0.998046875, 0.052734375, 0.998046875, 0.2001953125, 0.802734375, 0.0400390625, 0.4873046875, 0.8681640625, 0.9970703125, 0.1044921875, 0.0654296875, 0.125, 0.1552734375, 0.0322265625, 0.12890625, 0.04296875, 0.998046875, 0.9990234375, 0.02734375, 0.86328125, 0.998046875, 0.1513671875, 0.080078125, 0.998046875, 0.91796875, 0.2294921875, 0.1181640625, 0.03515625, 0.1103515625, 0.064453125, 0.0068359375, 0.1220703125, 0.08203125, 0.037109375, 0.01953125, 0.0634765625, 0.1552734375, 0.041015625, 0.0341796875, 0.9208984375, 0.04296875, 0.9775390625, 0.998046875, 0.998046875, 0.9990234375, 0.5419921875, 0.0673828125, 0.9990234375, 0.029296875, 0.998046875, 0.1181640625, 0.9970703125, 0.9970703125, 0.8388671875, 0.998046875, 0.998046875, 0.0419921875, 0.9970703125, 0.998046875, 0.998046875, 0.8359375, 0.056640625, 0.0234375, 0.19140625, 0.041015625, 0.998046875, 0.2822265625, 0.0302734375, 0.8916015625, 0.0546875, 0.05078125, 0.8466796875, 0.0849609375, 0.033203125, 0.08203125, 0.052734375, 0.998046875, 0.091796875, 0.0556640625, 0.119140625, 0.1630859375, 0.19140625, 0.130859375, 0.998046875, 0.1494140625, 0.0556640625, 0.0546875, 0.03515625, 0.9404296875, 0.015625, 0.7978515625, 0.9423828125, 0.9970703125, 0.7978515625, 0.9326171875, 0.177734375, 0.0146484375, 0.06640625, 0.0419921875, 0.0234375, 0.03125]

 sparsity of   [0.02300347201526165, 0.9986979365348816, 0.1354166716337204, 0.0355902798473835, 0.02170138992369175, 0.7556423544883728, 0.9127604365348816, 0.02083333395421505, 0.0425347238779068, 0.02777777798473835, 0.1553819477558136, 0.010416666977107525, 0.2078993022441864, 0.0212673619389534, 0.0542534738779068, 0.01692708395421505, 0.3871527910232544, 0.9986979365348816, 0.0681423619389534, 0.0525173619389534, 0.0659722238779068, 0.1393229216337204, 0.0164930559694767, 0.0069444444961845875, 0.8459201455116272, 0.0234375, 0.0282118059694767, 0.5855034589767456, 0.0833333358168602, 0.3020833432674408, 0.01909722201526165, 0.007378472480922937, 0.0716145858168602, 0.0486111119389534, 0.02473958395421505, 0.1037326380610466, 0.0173611119389534, 0.0473090298473835, 0.1354166716337204, 0.8498263955116272, 0.0377604179084301, 0.0325520820915699, 0.063368059694767, 0.0303819440305233, 0.03125, 0.1818576455116272, 0.0438368059694767, 0.0173611119389534, 0.161892369389534, 0.7660590410232544, 0.999131977558136, 0.9409722089767456, 0.1532118022441864, 0.0373263880610466, 0.3012152910232544, 0.0173611119389534, 0.0477430559694767, 0.1875, 0.01822916604578495, 0.0234375, 0.0008680555620230734, 0.02387152798473835, 0.7799479365348816, 0.0659722238779068, 0.0442708320915699, 0.01996527798473835, 0.0360243059694767, 0.130642369389534, 0.5590277910232544, 0.0494791679084301, 0.0355902798473835, 0.9110243320465088, 0.0264756940305233, 0.9470486044883728, 0.1549479216337204, 0.0407986119389534, 0.01779513992369175, 0.600694477558136, 0.013454861007630825, 0.0234375, 0.05078125, 0.006076388992369175, 0.0902777761220932, 0.0399305559694767, 0.0603298619389534, 0.0381944440305233, 0.0164930559694767, 0.0447048619389534, 0.086805559694767, 0.9283854365348816, 0.02690972201526165, 0.0364583320915699, 0.0447048619389534, 0.0681423619389534, 0.02777777798473835, 0.1150173619389534, 0.9986979365348816, 0.0186631940305233, 0.009982638992369175, 0.0694444477558136, 0.1753472238779068, 0.8962673544883728, 0.8042534589767456, 0.0264756940305233, 0.0212673619389534, 0.0538194440305233, 0.0364583320915699, 0.0698784738779068, 0.296875, 0.0368923619389534, 0.02560763992369175, 0.1640625, 0.7786458134651184, 0.846788227558136, 0.0559895820915699, 0.0373263880610466, 0.03125, 0.9986979365348816, 0.0451388880610466, 0.6961805820465088, 0.0668402761220932, 0.013888888992369175, 0.0486111119389534, 0.0360243059694767, 0.009114583022892475, 0.01128472201526165, 0.03081597201526165, 0.01822916604578495, 0.0698784738779068, 0.0342881940305233, 0.0885416641831398, 0.015625, 0.05859375, 0.02560763992369175, 0.0334201380610466, 0.1510416716337204, 0.01519097201526165, 0.0290798619389534, 0.0698784738779068, 0.009982638992369175, 0.0559895820915699, 0.078125, 0.0855034738779068, 0.0360243059694767, 0.1223958358168602, 0.02734375, 0.6072048544883728, 0.01605902798473835, 0.0590277798473835, 0.046875, 0.999131977558136, 0.7838541865348816, 0.0334201380610466, 0.013454861007630825, 0.02083333395421505, 0.0499131940305233, 0.0651041641831398, 0.03515625, 0.00390625, 0.0282118059694767, 0.0503472238779068, 0.0225694440305233, 0.0425347238779068, 0.2647569477558136, 0.0894097238779068, 0.0212673619389534, 0.8836805820465088, 0.02994791604578495, 0.0646701380610466, 0.7239583134651184, 0.710069477558136, 0.999131977558136, 0.0186631940305233, 0.6605902910232544, 0.8472222089767456, 0.8337673544883728, 0.03081597201526165, 0.02560763992369175, 0.674913227558136, 0.4991319477558136, 0.6284722089767456, 0.0399305559694767, 0.9995659589767456, 0.0486111119389534, 0.9509548544883728, 0.4366319477558136, 0.02300347201526165, 0.0381944440305233, 0.999131977558136, 0.009548611007630825, 0.1684027761220932, 0.0086805559694767, 0.02560763992369175, 0.7782118320465088, 0.0577256940305233, 0.8480902910232544, 0.7955729365348816, 0.02734375, 0.0533854179084301, 0.157986119389534, 0.999131977558136, 0.9214409589767456, 0.8767361044883728, 0.0481770820915699, 0.0251736119389534, 0.0629340261220932, 0.010850694961845875, 0.1106770858168602, 0.0234375, 0.0625, 0.0243055559694767, 0.874131977558136, 0.0225694440305233, 0.0234375, 0.0568576380610466, 0.1870659738779068, 0.02300347201526165, 0.9986979365348816, 0.1354166716337204, 0.01128472201526165, 0.0243055559694767, 0.005642361007630825, 0.09765625, 0.02777777798473835, 0.01519097201526165, 0.0347222238779068, 0.5434027910232544, 0.02951388992369175, 0.0503472238779068, 0.7560763955116272, 0.0785590261220932, 0.0381944440305233, 0.9995659589767456, 0.9761284589767456, 0.0512152798473835, 0.01128472201526165, 0.0303819440305233, 0.014322916977107525, 0.0052083334885537624, 0.0568576380610466, 0.0368923619389534, 0.0403645820915699, 0.078993059694767, 0.02777777798473835, 0.02170138992369175, 0.9995659589767456, 0.999131977558136, 0.0325520820915699, 0.0234375, 0.5698784589767456, 0.01822916604578495, 0.0464409738779068, 0.8059895634651184, 0.897569477558136, 0.0225694440305233, 0.32421875]

 sparsity of   [0.9921875, 0.98828125, 0.9921875, 0.1796875, 0.98828125, 0.984375, 0.296875, 0.9921875, 0.9921875, 0.98828125, 0.29296875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.1875, 0.015625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.0, 0.98828125, 0.98828125, 0.16796875, 0.05078125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.109375, 0.26953125, 0.9921875, 0.1328125, 0.9921875, 0.21875, 0.9921875, 0.984375, 0.98828125, 0.01953125, 0.98828125, 0.20703125, 0.1953125, 0.9921875, 0.03515625, 0.9921875, 0.16015625, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.984375, 0.203125, 0.296875, 0.01171875, 0.98828125, 0.29296875, 0.03125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.06640625, 0.98828125, 0.984375, 0.9921875, 0.98046875, 0.98828125, 0.03125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.203125, 0.1328125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.03125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.1484375, 0.06640625, 0.23828125, 0.9921875, 0.06640625, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.23828125, 0.9921875, 0.98828125, 0.98828125, 0.2890625, 0.19921875, 0.98828125, 0.9921875, 0.9921875, 0.24609375, 0.1171875, 0.0625, 0.07421875, 0.98828125, 0.984375, 0.98828125, 0.18359375, 0.08203125, 0.98828125, 0.83203125, 0.9921875, 0.08984375, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.31640625, 0.984375, 0.98828125, 0.9921875, 0.0234375, 0.02734375, 0.05078125, 0.15234375, 0.98828125, 0.98828125, 0.9921875, 0.07421875, 0.98828125, 0.9921875, 0.16796875, 0.98828125, 0.22265625, 0.98828125, 0.4140625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.8671875, 0.98828125, 0.0625, 0.98828125, 0.9921875, 0.28125, 0.9921875, 0.984375, 0.2890625, 0.15234375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.01953125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.27734375, 0.98828125, 0.03515625, 0.03515625, 0.15234375, 0.98828125, 0.30078125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.03125, 0.9921875, 0.98828125, 0.0546875, 0.09765625, 0.125, 0.0703125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.31640625, 0.98828125, 0.9921875, 0.234375, 0.9921875, 0.98828125, 0.02734375, 0.9921875, 0.98828125, 0.9921875, 0.03125, 0.9921875, 0.9921875, 0.08203125, 0.33203125, 0.98828125, 0.9921875, 0.19921875, 0.33984375, 0.984375, 0.0703125, 0.98828125, 0.03515625, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.0234375, 0.03515625, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.04296875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.03125, 0.0, 0.9921875, 0.984375, 0.9921875, 0.05078125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.02734375, 0.9921875, 0.98828125, 0.046875, 0.98828125, 0.9921875, 0.98828125, 0.05859375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.3515625, 0.26171875, 0.98046875, 0.140625, 0.98828125, 0.02734375, 0.01953125, 0.25390625, 0.9921875, 0.98828125, 0.9921875, 0.0703125, 0.98828125, 0.1953125, 0.98046875, 0.98828125, 0.2578125, 0.984375, 0.98828125, 0.25, 0.98046875, 0.98828125, 0.171875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.30859375, 0.9921875, 0.02734375, 0.98828125, 0.26953125, 0.9921875, 0.9921875, 0.03515625, 0.796875, 0.984375, 0.01953125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.27734375, 0.109375, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.11328125, 0.9921875, 0.98828125, 0.9921875, 0.21875, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.00390625, 0.98828125, 0.19921875, 0.12109375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.2578125, 0.98828125, 0.9921875, 0.046875, 0.9921875, 0.98828125, 0.98828125, 0.0390625, 0.98828125, 0.03125, 0.05078125, 0.13671875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.12890625, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.15625, 0.9921875, 0.98828125, 0.27734375, 0.25390625, 0.08203125, 0.25, 0.9921875, 0.98828125, 0.25390625, 0.98828125, 0.265625, 0.984375, 0.9921875, 0.98828125, 0.09375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.1875, 0.9921875, 0.1484375, 0.98828125, 0.98828125, 0.1484375, 0.984375, 0.98828125, 0.9921875, 0.15625, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.28125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.2109375, 0.98828125, 0.9921875, 0.98828125, 0.10546875, 0.9921875, 0.9921875, 0.1640625, 0.984375, 0.234375, 0.98828125, 0.3125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.0546875, 0.07421875, 0.0703125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.1484375, 0.0703125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98046875, 0.21875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.1015625, 0.98828125, 0.30078125, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.03515625, 0.984375, 0.06640625, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.4296875, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.2734375, 0.98828125, 0.98828125, 0.98828125, 0.17578125, 0.98828125, 0.98828125, 0.04296875, 0.0703125, 0.2109375, 0.98828125, 0.0625, 0.1484375, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.0859375, 0.98828125, 0.328125, 0.98828125, 0.98828125, 0.24609375, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.04296875, 0.04296875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.234375, 0.98828125, 0.98828125, 0.015625, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.984375, 0.796875, 0.984375, 0.98828125, 0.98828125, 0.078125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.265625, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.05859375, 0.21875, 0.98828125, 0.98828125, 0.98828125, 0.30078125, 0.98828125, 0.05078125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.9921875, 0.04296875, 0.9921875, 0.984375, 0.01953125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.27734375, 0.9921875, 0.2578125, 0.9921875, 0.05859375, 0.984375, 0.0234375, 0.984375, 0.98828125, 0.1796875, 0.98828125, 0.98828125, 0.02734375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.0859375, 0.03515625, 0.33203125, 0.21875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.24609375, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.09375, 0.18359375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.0546875, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.03515625, 0.06640625, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.984375, 0.078125, 0.98828125, 0.984375, 0.06640625, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.3828125, 0.98828125, 0.04296875, 0.8828125, 0.03125, 0.9921875, 0.375, 0.98828125, 0.1171875, 0.9921875, 0.98828125, 0.24609375, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.17578125, 0.98828125, 0.98828125, 0.98828125, 0.19140625, 0.984375, 0.98828125, 0.34765625, 0.98828125, 0.03125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.37109375, 0.1640625, 0.98828125, 0.1796875, 0.98828125, 0.9921875, 0.98828125, 0.0390625, 0.9921875, 0.03515625, 0.98828125, 0.1328125, 0.98828125, 0.98046875, 0.19140625, 0.0390625, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.01953125, 0.984375, 0.99609375, 0.046875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.109375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.046875, 0.9921875, 0.98828125, 0.03125, 0.984375, 0.0234375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.22265625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.0078125, 0.98828125, 0.08984375, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.08984375, 0.984375, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.0390625, 0.9921875, 0.3046875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.0625, 0.04296875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.99609375, 0.11328125, 0.1640625, 0.98828125, 0.16015625, 0.9921875, 0.17578125, 0.15234375, 0.9921875, 0.85546875, 0.984375, 0.9921875, 0.00390625, 0.98828125, 0.0625, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.8125, 0.9921875, 0.046875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.1171875, 0.9921875, 0.9921875, 0.421875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.04296875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.0859375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.28515625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.0546875, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.03125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.26171875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.07421875, 0.046875, 0.9921875, 0.09765625, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.11328125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.28125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.19140625, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.01953125, 0.98828125, 0.98828125, 0.98828125, 0.07421875, 0.9921875, 0.1328125, 0.98828125, 0.03515625, 0.984375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.37890625, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.07421875, 0.2890625, 0.9921875, 0.14453125, 0.9921875, 0.98828125, 0.98828125, 0.07421875, 0.98828125, 0.9921875, 0.9921875, 0.16015625, 0.04296875, 0.9921875, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.05859375, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.06640625, 0.01953125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.0859375, 0.98828125, 0.09375, 0.9921875, 0.9921875, 0.98828125, 0.0390625, 0.98828125, 0.984375, 0.98828125, 0.08984375, 0.9921875, 0.9921875, 0.984375, 0.0234375, 0.1328125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.1875, 0.3359375, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98046875, 0.98828125, 0.9921875, 0.98828125, 0.02734375, 0.98828125, 0.1640625]

 sparsity of   [0.384765625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.1875, 0.9970703125, 0.4130859375, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.1533203125, 0.998046875, 0.171875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.1484375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.1591796875, 0.998046875, 0.998046875, 0.1650390625, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.150390625, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.21875, 0.998046875, 0.9990234375, 0.4052734375, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.1650390625, 0.998046875, 0.998046875, 0.099609375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.1982421875, 0.9970703125, 0.2099609375, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.14453125, 0.15625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.37890625, 0.998046875, 0.998046875, 0.4375, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.41015625, 0.998046875, 0.9990234375, 0.2001953125, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.400390625, 0.4189453125, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.39453125, 0.998046875, 0.1875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.15625, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.197265625, 0.998046875, 0.9970703125, 0.4580078125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.0341796875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.0810546875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.138671875, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.103515625, 0.998046875, 0.4248046875, 0.998046875, 0.470703125, 0.998046875, 0.998046875, 0.4072265625, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.1630859375, 0.998046875, 0.134765625, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.416015625, 0.2548828125, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.2783203125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.3583984375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.22265625, 0.3369140625, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.1591796875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.17578125, 0.998046875, 0.9990234375, 0.99609375, 0.9970703125, 0.998046875, 0.9970703125, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.18359375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.40625, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.4453125, 0.9990234375, 0.998046875, 0.9990234375, 0.078125, 0.9970703125, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.169921875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.150390625, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0546875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.240234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375]

 sparsity of   [0.9086371660232544, 0.9995659589767456, 0.2100694477558136, 0.9995659589767456, 0.1158854141831398, 0.9995659589767456, 0.9995659589767456, 0.0379774309694767, 0.9446614384651184, 0.9995659589767456, 0.0993923619389534, 0.0568576380610466, 0.9995659589767456, 0.05078125, 0.9997829794883728, 0.1009114608168602, 0.1076388880610466, 0.16015625, 0.9993489384651184, 0.0779079869389534, 0.0523003488779068, 0.069227434694767, 0.1143663227558136, 0.0390625, 0.1740451455116272, 0.9995659589767456, 0.0944010391831398, 0.0972222238779068, 0.1095920130610466, 0.9114583134651184, 0.259765625, 0.1573350727558136, 0.9995659589767456, 0.1076388880610466, 0.08984375, 0.9995659589767456, 0.0462239570915699, 0.0802951380610466, 0.0345052070915699, 0.1391059011220932, 0.0455729179084301, 0.0505642369389534, 0.076171875, 0.0490451380610466, 0.0865885391831398, 0.9995659589767456, 0.0709635391831398, 0.0364583320915699, 0.092664934694767, 0.9997829794883728, 0.1076388880610466, 0.0744357630610466, 0.8003472089767456, 0.0924479141831398, 0.02864583395421505, 0.9995659589767456, 0.17578125, 0.0544704869389534, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9086371660232544, 0.1525607705116272, 0.9255642294883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.4216579794883728, 0.6851128339767456, 0.0360243059694767, 0.0700954869389534, 0.0301649309694767, 0.9997829794883728, 0.0679253488779068, 0.0609809048473835, 0.0900607630610466, 0.0648871511220932, 0.0991753488779068, 0.9995659589767456, 0.0694444477558136, 0.0355902798473835, 0.0700954869389534, 0.1451822966337204, 0.0674913227558136, 0.9995659589767456, 0.0316840298473835, 0.9997829794883728, 0.9993489384651184, 0.114149309694767, 0.1293402761220932, 0.9995659589767456, 0.037109375, 0.0423177070915699, 0.0434027798473835, 0.9993489384651184, 0.095703125, 0.9995659589767456, 0.3736979067325592, 0.806640625, 0.1634114533662796, 0.150173619389534, 0.0696614608168602, 0.2673611044883728, 0.1590711772441864, 0.9995659589767456, 0.0583767369389534, 0.0755208358168602, 0.4557291567325592, 0.9995659589767456, 0.0657552108168602, 0.9995659589767456, 0.9995659589767456, 0.1825086772441864, 0.8049045205116272, 0.9461805820465088, 0.2649739682674408, 0.0509982630610466, 0.052734375, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0486111119389534, 0.1375868022441864, 0.02777777798473835, 0.1779513955116272, 0.0520833320915699, 0.09375, 0.0681423619389534, 0.0998263880610466, 0.148220494389534, 0.0381944440305233, 0.1742621511220932, 0.0496961809694767, 0.9995659589767456, 0.0431857630610466, 0.161892369389534, 0.9995659589767456, 0.9995659589767456, 0.0909288227558136, 0.1512586772441864, 0.0822482630610466, 0.932725727558136, 0.9995659589767456, 0.0798611119389534, 0.9995659589767456, 0.1165364608168602, 0.9995659589767456, 0.9995659589767456, 0.12109375, 0.9995659589767456, 0.0321180559694767, 0.07421875, 0.9993489384651184, 0.0844184011220932, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.8927951455116272, 0.3274739682674408, 0.9995659589767456, 0.0245225690305233, 0.0483940988779068, 0.0842013880610466, 0.0703125, 0.0677083358168602, 0.0347222238779068, 0.1701388955116272, 0.9993489384651184, 0.0726996511220932, 0.9997829794883728, 0.0846354141831398, 0.5744357705116272, 0.9995659589767456, 0.0737847238779068, 0.9995659589767456, 0.063368059694767, 0.9995659589767456, 0.1048177108168602, 0.1391059011220932, 0.0453559048473835, 0.9993489384651184, 0.0746527761220932, 0.5145399570465088, 0.9390190839767456, 0.810546875, 0.1163194477558136, 0.9993489384651184, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.0902777761220932, 0.9993489384651184, 0.0397135429084301, 0.1527777761220932, 0.9993489384651184, 0.9995659589767456, 0.0486111119389534, 0.1106770858168602, 0.114149309694767, 0.9995659589767456, 0.073133684694767, 0.9995659589767456, 0.1907552033662796, 0.2213541716337204, 0.0826822891831398, 0.9995659589767456, 0.1773003488779068, 0.1807725727558136, 0.1671006977558136, 0.9995659589767456, 0.9995659589767456, 0.0466579869389534, 0.9997829794883728, 0.9997829794883728, 0.1532118022441864, 0.03515625, 0.0952690988779068, 0.0388454869389534, 0.1087239608168602, 0.0364583320915699, 0.177517369389534, 0.2606336772441864, 0.0355902798473835, 0.0655381977558136, 0.063368059694767, 0.0453559048473835, 0.0588107630610466, 0.1694878488779068, 0.792100727558136, 0.9993489384651184, 0.9993489384651184, 0.0340711809694767, 0.9993489384651184, 0.1532118022441864, 0.181640625, 0.0611979179084301, 0.0536024309694767, 0.021484375, 0.9995659589767456, 0.094618059694767, 0.9995659589767456, 0.9993489384651184, 0.8884548544883728, 0.0740017369389534, 0.0577256940305233, 0.8556857705116272, 0.0538194440305233, 0.0416666679084301, 0.0705295130610466, 0.9995659589767456, 0.0792100727558136, 0.1786024272441864, 0.7825520634651184, 0.1000434011220932, 0.9995659589767456, 0.0564236119389534, 0.078125, 0.0833333358168602, 0.0744357630610466, 0.041015625, 0.0164930559694767, 0.099609375, 0.1032986119389534, 0.1436631977558136, 0.0319010429084301, 0.9995659589767456, 0.9997829794883728, 0.9993489384651184, 0.0776909738779068, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.1100260391831398, 0.9995659589767456, 0.1343315988779068, 0.0282118059694767, 0.0264756940305233, 0.0475260429084301, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0959201380610466, 0.9995659589767456, 0.09765625, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.1877170205116272, 0.0620659738779068, 0.9995659589767456, 0.0933159738779068, 0.9995659589767456, 0.0381944440305233, 0.067274309694767, 0.9995659589767456, 0.9993489384651184, 0.0494791679084301, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.0883246511220932, 0.1174045130610466, 0.9997829794883728, 0.037109375, 0.1252170205116272, 0.9997829794883728, 0.0473090298473835, 0.9995659589767456, 0.0657552108168602, 0.9995659589767456, 0.1006944477558136, 0.0594618059694767, 0.1499565988779068, 0.0575086809694767, 0.123046875, 0.1820746511220932, 0.080078125, 0.9995659589767456, 0.9995659589767456, 0.0505642369389534, 0.0271267369389534, 0.4496527910232544, 0.0251736119389534, 0.03081597201526165, 0.9997829794883728, 0.05078125, 0.9114583134651184, 0.9995659589767456, 0.0713975727558136, 0.0815972238779068, 0.06640625, 0.4103732705116272, 0.9995659589767456, 0.9995659589767456, 0.7858073115348816, 0.8967013955116272, 0.9995659589767456, 0.9869791865348816, 0.425347238779068, 0.0776909738779068, 0.9993489384651184, 0.3522135317325592, 0.0601128488779068, 0.921875, 0.0796440988779068, 0.9995659589767456, 0.063368059694767, 0.078993059694767, 0.1440972238779068, 0.9995659589767456, 0.0718315988779068, 0.0375434048473835, 0.9995659589767456, 0.15625, 0.0559895820915699, 0.082899309694767, 0.0588107630610466, 0.1145833358168602, 0.0568576380610466, 0.0533854179084301, 0.0659722238779068, 0.9995659589767456, 0.0651041641831398, 0.9997829794883728, 0.9264323115348816, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.0833333358168602, 0.9995659589767456, 0.923828125, 0.0753038227558136, 0.9995659589767456, 0.9060329794883728, 0.9995659589767456, 0.14453125, 0.0726996511220932, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.0492621548473835, 0.1918402761220932, 0.0681423619389534, 0.0980902761220932, 0.943359375, 0.0412326380610466, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0902777761220932, 0.9997829794883728, 0.9995659589767456, 0.321831613779068, 0.115234375, 0.1690538227558136, 0.915147602558136, 0.942491352558136, 0.0368923619389534, 0.9995659589767456, 0.1499565988779068, 0.9025607705116272, 0.073133684694767, 0.8224826455116272, 0.9027777910232544, 0.1184895858168602, 0.0700954869389534, 0.8268229365348816, 0.1983506977558136, 0.9995659589767456, 0.0614149309694767, 0.1857638955116272, 0.0364583320915699, 0.9995659589767456, 0.1206597238779068, 0.9995659589767456, 0.8899739384651184, 0.0421006940305233, 0.9995659589767456, 0.7743055820465088, 0.0425347238779068, 0.068359375, 0.9995659589767456, 0.069227434694767, 0.069227434694767, 0.9995659589767456, 0.123914934694767, 0.02886284701526165, 0.1681857705116272, 0.189453125, 0.9997829794883728, 0.9995659589767456, 0.0920138880610466, 0.1115451380610466, 0.9995659589767456, 0.1733940988779068, 0.9379340410232544, 0.2454427033662796, 0.01714409701526165, 0.1184895858168602, 0.0852864608168602, 0.9997829794883728, 0.1976996511220932, 0.0939670130610466, 0.1532118022441864, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0546875, 0.9995659589767456, 0.0588107630610466, 0.0412326380610466, 0.0698784738779068, 0.0457899309694767, 0.9995659589767456, 0.1106770858168602, 0.1707899272441864, 0.9995659589767456, 0.0713975727558136, 0.9212239384651184, 0.0416666679084301, 0.9993489384651184, 0.9995659589767456, 0.9223090410232544, 0.0698784738779068, 0.132595494389534, 0.167751744389534, 0.1419270783662796, 0.025390625, 0.489800363779068, 0.9995659589767456, 0.1427951455116272, 0.9995659589767456, 0.0516493059694767, 0.0959201380610466, 0.0666232630610466, 0.1234809011220932, 0.5015190839767456, 0.9995659589767456, 0.9997829794883728, 0.0818142369389534, 0.9029948115348816, 0.9995659589767456, 0.0362413190305233, 0.1273871511220932, 0.033203125, 0.1130642369389534, 0.02734375, 0.064453125, 0.104383684694767, 0.2252604216337204, 0.1694878488779068, 0.0559895820915699, 0.1846788227558136, 0.8513454794883728, 0.9995659589767456, 0.0883246511220932, 0.9993489384651184, 0.171875, 0.5134548544883728, 0.9997829794883728, 0.1243489608168602, 0.7855902910232544, 0.0876736119389534, 0.236111119389534, 0.014756944961845875]

 sparsity of   [0.01953125, 0.994140625, 0.044921875, 0.103515625, 0.015625, 0.109375, 0.001953125, 0.017578125, 0.017578125, 0.18359375, 0.103515625, 0.001953125, 0.072265625, 0.111328125, 0.076171875, 0.017578125, 0.00390625, 0.029296875, 0.89453125, 0.03125, 0.865234375, 0.107421875, 0.1640625, 0.994140625, 0.02734375, 0.9375, 0.00390625, 0.126953125, 0.0234375, 0.046875, 0.08203125, 0.99609375, 0.01171875, 0.75390625, 0.03125, 0.02734375, 0.99609375, 0.00390625, 0.01953125, 0.060546875, 0.015625, 0.009765625, 0.015625, 0.00390625, 0.072265625, 0.064453125, 0.93359375, 0.087890625, 0.01953125, 0.0234375, 0.091796875, 0.044921875, 0.017578125, 0.0703125, 0.126953125, 0.021484375, 0.048828125, 0.9296875, 0.0234375, 0.072265625, 0.11328125, 0.994140625, 0.99609375, 0.046875, 0.052734375, 0.99609375, 0.13671875, 0.017578125, 0.00390625, 0.494140625, 0.02734375, 0.99609375, 0.11328125, 0.033203125, 0.0078125, 0.99609375, 0.994140625, 0.08984375, 0.021484375, 0.025390625, 0.171875, 0.0234375, 0.033203125, 0.072265625, 0.01953125, 0.994140625, 0.0234375, 0.01953125, 0.04296875, 0.013671875, 0.044921875, 0.671875, 0.0234375, 0.025390625, 0.01171875, 0.080078125, 0.0234375, 0.05078125, 0.134765625, 0.021484375, 0.05078125, 0.955078125, 0.025390625, 0.1015625, 0.998046875, 0.1171875, 0.037109375, 0.05078125, 0.095703125, 0.04296875, 0.009765625, 0.017578125, 0.15234375, 0.1484375, 0.173828125, 0.048828125, 0.015625, 0.01953125, 0.99609375, 0.06640625, 0.046875, 0.03125, 0.01171875, 0.017578125, 0.99609375, 0.005859375, 0.9453125, 0.99609375, 0.06640625, 0.013671875, 0.017578125, 0.85546875, 0.998046875, 0.0, 0.017578125, 0.041015625, 0.814453125, 0.005859375, 0.05078125, 0.134765625, 0.072265625, 0.01953125, 0.013671875, 0.021484375, 0.994140625, 0.0078125, 0.15625, 0.955078125, 0.056640625, 0.025390625, 0.025390625, 0.03515625, 0.01171875, 0.0390625, 0.07421875, 0.994140625, 0.71484375, 0.048828125, 0.052734375, 0.080078125, 0.0625, 0.138671875, 0.02734375, 0.126953125, 0.03125, 0.99609375, 0.62890625, 0.044921875, 0.916015625, 0.060546875, 0.015625, 0.025390625, 0.26171875, 0.05078125, 0.021484375, 0.091796875, 0.05078125, 0.291015625, 0.755859375, 0.03125, 0.015625, 0.03125, 0.060546875, 0.015625, 0.998046875, 0.017578125, 0.056640625, 0.05859375, 0.037109375, 0.056640625, 0.728515625, 0.080078125, 0.04296875, 0.044921875, 0.03515625, 0.125, 0.04296875, 0.1953125, 0.921875, 0.90234375, 0.994140625, 0.064453125, 0.1171875, 0.009765625, 0.09375, 0.037109375, 0.99609375, 0.046875, 0.037109375, 0.83984375, 0.013671875, 0.99609375, 0.01171875, 0.994140625, 0.99609375, 0.0390625, 0.28515625, 0.0390625, 0.0546875, 0.107421875, 0.86328125, 0.0234375, 0.01171875, 0.083984375, 0.04296875, 0.994140625, 0.0078125, 0.7890625, 0.0, 0.0703125, 0.021484375, 0.01171875, 0.076171875, 0.060546875, 0.013671875, 0.873046875, 0.03125, 0.998046875, 0.046875, 0.03125, 0.056640625, 0.0, 0.01171875, 0.05078125, 0.0625, 0.99609375, 0.052734375, 0.0, 0.673828125, 0.013671875, 0.05859375, 0.05078125, 0.033203125, 0.046875, 0.041015625, 0.037109375, 0.107421875, 0.0390625, 0.013671875, 0.083984375, 0.2890625, 0.576171875, 0.802734375, 0.005859375, 0.048828125, 0.025390625, 0.03515625, 0.99609375, 0.00390625, 0.1484375, 0.80859375, 0.791015625, 0.01953125, 0.033203125, 0.9453125, 0.03125, 0.07421875, 0.083984375, 0.041015625, 0.056640625, 0.193359375, 0.0390625, 0.021484375, 0.0234375, 0.05859375, 0.994140625, 0.970703125, 0.052734375, 0.880859375, 0.615234375, 0.044921875, 0.8671875, 0.0078125, 0.03125, 0.087890625, 0.908203125, 0.99609375, 0.078125, 0.03125, 0.009765625, 0.8984375, 0.029296875, 0.017578125, 0.044921875, 0.01171875, 0.056640625, 0.0078125, 0.044921875, 0.994140625, 0.037109375, 0.025390625, 0.103515625, 0.99609375, 0.033203125, 0.99609375, 0.103515625, 0.01953125, 0.99609375, 0.05078125, 0.046875, 0.994140625, 0.044921875, 0.943359375, 0.0703125, 0.048828125, 0.994140625, 0.0546875, 0.064453125, 0.9296875, 0.017578125, 0.951171875, 0.068359375, 0.021484375, 0.013671875, 0.009765625, 0.021484375, 0.009765625, 0.099609375, 0.0859375, 0.03125, 0.015625, 0.015625, 0.908203125, 0.013671875, 0.0859375, 0.041015625, 0.99609375, 0.96875, 0.1015625, 0.0625, 0.03515625, 0.884765625, 0.044921875, 0.998046875, 0.021484375, 0.037109375, 0.375, 0.017578125, 0.146484375, 0.029296875, 0.041015625, 0.05859375, 0.015625, 0.0546875, 0.92578125, 0.025390625, 0.0546875, 0.765625, 0.01171875, 0.025390625, 0.05859375, 0.046875, 0.03125, 0.025390625, 0.94921875, 0.015625, 0.11328125, 0.837890625, 0.900390625, 0.857421875, 0.89453125, 0.037109375, 0.9765625, 0.021484375, 0.109375, 0.033203125, 0.015625, 0.99609375, 0.0390625, 0.1953125, 0.021484375, 0.015625, 0.912109375, 0.03125, 0.103515625, 0.0703125, 0.7421875, 0.244140625, 0.1484375, 0.078125, 0.013671875, 0.01953125, 0.984375, 0.78515625, 0.05859375, 0.99609375, 0.109375, 0.03515625, 0.3203125, 0.08984375, 0.01171875, 0.041015625, 0.123046875, 0.775390625, 0.109375, 0.017578125, 0.068359375, 0.013671875, 0.033203125, 0.0859375, 0.111328125, 0.0, 0.041015625, 0.037109375, 0.037109375, 0.04296875, 0.7734375, 0.021484375, 0.125, 0.080078125, 0.99609375, 0.046875, 0.99609375, 0.0, 0.04296875, 0.01171875, 0.0, 0.01953125, 0.99609375, 0.07421875, 0.017578125, 0.9375, 0.99609375, 0.04296875, 0.0078125, 0.044921875, 0.0546875, 0.052734375, 0.99609375, 0.056640625, 0.072265625, 0.060546875, 0.0234375, 0.30859375, 0.0078125, 0.853515625, 0.119140625, 0.947265625, 0.99609375, 0.25390625, 0.033203125, 0.0390625, 0.0390625, 0.064453125, 0.115234375, 0.0234375, 0.08203125, 0.08203125, 0.12890625, 0.021484375, 0.033203125, 0.02734375, 0.96875, 0.10546875, 0.037109375, 0.029296875, 0.01171875, 0.017578125, 0.0625, 0.083984375, 0.041015625, 0.11328125, 0.01953125, 0.046875, 0.078125, 0.791015625, 0.908203125, 0.06640625, 0.0234375, 0.056640625, 0.880859375, 0.796875, 0.01171875, 0.0234375, 0.06640625, 0.00390625, 0.033203125, 0.998046875, 0.99609375, 0.013671875, 0.041015625, 0.046875, 0.0546875, 0.125, 0.13671875, 0.064453125, 0.0546875, 0.09765625, 0.125, 0.6640625, 0.1953125, 0.40234375, 0.7578125, 0.037109375, 0.060546875, 0.15234375, 0.025390625, 0.03515625, 0.0234375, 0.072265625, 0.025390625, 0.037109375, 0.068359375, 0.0234375, 0.021484375, 0.013671875, 0.001953125, 0.115234375, 0.044921875, 0.751953125, 0.99609375, 0.05078125, 0.0625, 0.072265625, 0.08203125, 0.91015625, 0.1640625, 0.99609375, 0.03515625, 0.06640625, 0.36328125, 0.765625, 0.037109375, 0.767578125, 0.03125, 0.05078125, 0.052734375, 0.03125, 0.080078125, 0.01953125, 0.880859375, 0.04296875, 0.03125, 0.080078125, 0.865234375, 0.92578125, 0.015625, 0.99609375, 0.0078125, 0.705078125, 0.04296875, 0.115234375, 0.99609375, 0.001953125, 0.103515625, 0.017578125, 0.01171875, 0.0546875, 0.01953125, 0.8046875, 0.919921875, 0.19921875, 0.041015625, 0.0234375, 0.998046875, 0.041015625, 0.041015625, 0.046875, 0.076171875, 0.080078125, 0.99609375, 0.037109375, 0.091796875, 0.205078125, 0.005859375, 0.119140625, 0.99609375, 0.9375, 0.99609375, 0.0859375, 0.048828125, 0.1015625, 0.03125, 0.953125, 0.99609375, 0.99609375, 0.99609375, 0.056640625, 0.900390625, 0.029296875, 0.99609375, 0.025390625, 0.189453125, 0.994140625, 0.080078125, 0.025390625, 0.02734375, 0.744140625, 0.876953125, 0.013671875, 0.03125, 0.03515625, 0.0703125, 0.001953125, 0.0234375, 0.0390625, 0.005859375, 0.083984375, 0.05859375, 0.05078125, 0.685546875, 0.041015625, 0.853515625, 0.0390625, 0.05078125, 0.076171875, 0.154296875, 0.0078125, 0.9921875, 0.02734375, 0.03125, 0.08203125, 0.90234375, 0.05859375, 0.05078125, 0.017578125, 0.02734375, 0.029296875, 0.068359375, 0.0, 0.126953125, 0.115234375, 0.119140625, 0.015625, 0.8125, 0.8828125, 0.08984375, 0.02734375, 0.041015625, 0.99609375, 0.01171875, 0.810546875, 0.994140625, 0.01953125, 0.03125, 0.05078125, 0.376953125, 0.810546875, 0.08984375, 0.99609375, 0.0625, 0.056640625, 0.99609375, 0.033203125, 0.056640625, 0.138671875, 0.15625, 0.087890625, 0.8671875, 0.03515625, 0.037109375, 0.830078125, 0.064453125, 0.064453125, 0.24609375, 0.109375, 0.013671875, 0.013671875, 0.91796875, 0.8125, 0.01953125, 0.994140625, 0.056640625, 0.123046875, 0.0078125, 0.849609375, 0.994140625, 0.052734375, 0.083984375, 0.99609375, 0.998046875, 0.044921875, 0.048828125, 0.015625, 0.248046875, 0.03515625, 0.0859375, 0.06640625, 0.048828125, 0.99609375, 0.048828125, 0.13671875, 0.048828125, 0.021484375, 0.99609375, 0.79296875, 0.99609375, 0.029296875, 0.814453125, 0.0859375, 0.166015625, 0.08203125, 0.046875, 0.0234375, 0.044921875, 0.6640625, 0.025390625, 0.025390625, 0.951171875, 0.01171875, 0.03125, 0.03125, 0.0078125, 0.037109375, 0.017578125, 0.99609375, 0.044921875, 0.033203125, 0.044921875, 0.0234375, 0.939453125, 0.046875, 0.091796875, 0.01953125, 0.130859375, 0.0234375, 0.03125, 0.017578125, 0.224609375, 0.0, 0.015625, 0.02734375, 0.2265625, 0.0703125, 0.029296875, 0.029296875, 0.029296875, 0.060546875, 0.05859375, 0.76171875, 0.046875, 0.068359375, 0.03125, 0.017578125, 0.060546875, 0.109375, 0.103515625, 0.017578125, 0.0234375, 0.015625, 0.9453125, 0.064453125, 0.98046875, 0.0, 0.99609375, 0.7578125, 0.046875, 0.7421875, 0.033203125, 0.935546875, 0.0703125, 0.017578125, 0.791015625, 0.017578125, 0.1015625, 0.015625, 0.078125, 0.81640625, 0.0234375, 0.0390625, 0.1015625, 0.091796875, 0.994140625, 0.041015625, 0.947265625, 0.04296875, 0.11328125, 0.017578125, 0.00390625, 0.01171875, 0.248046875, 0.01171875, 0.029296875, 0.99609375, 0.015625, 0.04296875, 0.130859375, 0.009765625, 0.1171875, 0.78515625, 0.0078125, 0.01953125, 0.10546875, 0.109375, 0.021484375, 0.013671875, 0.017578125, 0.76953125, 0.806640625, 0.048828125, 0.99609375, 0.916015625, 0.04296875, 0.076171875, 0.99609375, 0.5703125, 0.025390625, 0.044921875, 0.01171875, 0.99609375, 0.076171875, 0.02734375, 0.994140625, 0.015625, 0.962890625, 0.04296875, 0.037109375, 0.2578125, 0.01171875, 0.80078125, 0.03125, 0.08984375, 0.013671875, 0.234375, 0.880859375, 0.091796875, 0.037109375, 0.015625, 0.03515625, 0.044921875, 0.755859375, 0.01171875, 0.05078125, 0.044921875, 0.015625, 0.025390625, 0.029296875, 0.056640625, 0.01953125, 0.0234375, 0.9453125, 0.0546875, 0.025390625, 0.01171875, 0.044921875, 0.009765625, 0.05078125, 0.099609375, 0.01171875, 0.02734375, 0.0234375, 0.05859375, 0.1015625, 0.0, 0.99609375, 0.126953125, 0.166015625, 0.0234375, 0.0234375, 0.04296875, 0.00390625, 0.755859375, 0.046875, 0.056640625, 0.037109375, 0.994140625, 0.052734375, 0.2109375, 0.185546875, 0.037109375, 0.93359375, 0.83984375, 0.04296875, 0.013671875, 0.078125, 0.853515625, 0.01171875, 0.037109375, 0.068359375, 0.380859375, 0.044921875, 0.99609375, 0.99609375, 0.998046875, 0.072265625, 0.033203125, 0.015625, 0.076171875, 0.029296875, 0.99609375, 0.0234375, 0.00390625, 0.171875, 0.0078125, 0.99609375, 0.919921875, 0.05859375, 0.189453125, 0.99609375, 0.017578125, 0.07421875, 0.0, 0.77734375, 0.99609375, 0.99609375, 0.783203125, 0.03125, 0.99609375, 0.78515625, 0.021484375, 0.99609375, 0.99609375, 0.955078125, 0.189453125, 0.923828125, 0.951171875, 0.03125, 0.0390625, 0.962890625, 0.99609375, 0.9375, 0.06640625, 0.970703125, 0.078125, 0.0546875, 0.029296875, 0.662109375, 0.06640625, 0.171875, 0.263671875, 0.03125, 0.01953125, 0.025390625, 0.05859375, 0.994140625, 0.0078125, 0.939453125, 0.00390625, 0.025390625, 0.048828125, 0.03515625, 0.439453125, 0.009765625, 0.109375, 0.037109375, 0.994140625, 0.84375, 0.8359375, 0.0546875, 0.04296875, 0.052734375, 0.8984375, 0.076171875, 0.08203125, 0.078125, 0.021484375, 0.033203125, 0.072265625, 0.935546875, 0.904296875, 0.009765625, 0.04296875, 0.005859375, 0.041015625, 0.052734375, 0.810546875, 0.044921875, 0.029296875, 0.03125, 0.06640625, 0.99609375, 0.025390625, 0.890625, 0.3125, 0.078125, 0.271484375, 0.724609375, 0.798828125, 0.09375, 0.015625, 0.009765625, 0.99609375, 0.02734375, 0.99609375, 0.025390625, 0.923828125, 0.017578125, 0.076171875, 0.033203125, 0.037109375, 0.962890625, 0.0390625, 0.16796875, 0.951171875, 0.33203125, 0.05078125, 0.017578125, 0.060546875, 0.79296875, 0.111328125, 0.068359375, 0.01171875, 0.03515625, 0.03125, 0.92578125, 0.08203125, 0.03125, 0.05859375, 0.8359375, 0.033203125, 0.134765625, 0.107421875, 0.013671875, 0.1171875, 0.02734375, 0.041015625, 0.021484375, 0.951171875, 0.068359375, 0.03125, 0.01953125, 0.03515625, 0.017578125, 0.037109375, 0.994140625, 0.998046875, 0.046875, 0.994140625, 0.107421875, 0.025390625, 0.04296875, 0.109375, 0.03125, 0.873046875, 0.03515625, 0.015625, 0.994140625, 0.00390625, 0.05078125, 0.99609375, 0.021484375, 0.0, 0.107421875, 0.017578125, 0.3671875, 0.001953125, 0.08203125, 0.025390625, 0.033203125, 0.01953125, 0.912109375, 0.046875, 0.451171875, 0.1171875, 0.99609375, 0.9453125, 0.751953125, 0.017578125, 0.083984375, 0.08203125, 0.037109375, 0.0390625, 0.0625, 0.0390625, 0.99609375, 0.029296875, 0.998046875, 0.865234375, 0.916015625, 0.064453125, 0.015625, 0.01171875, 0.08203125, 0.017578125, 0.02734375, 0.015625, 0.037109375, 0.02734375, 0.05078125, 0.05859375, 0.0703125, 0.05078125, 0.068359375, 0.0078125, 0.048828125, 0.08984375, 0.994140625, 0.03515625, 0.84765625, 0.015625, 0.029296875, 0.013671875, 0.01953125, 0.00390625, 0.013671875, 0.00390625, 0.0, 0.865234375, 0.046875, 0.0859375, 0.08984375, 0.001953125, 0.033203125, 0.0234375, 0.03515625, 0.025390625, 0.638671875, 0.07421875, 0.00390625, 0.01171875, 0.060546875, 0.091796875, 0.046875, 0.017578125, 0.015625, 0.998046875, 0.015625, 0.05078125, 0.05859375, 0.017578125, 0.0078125, 0.99609375, 0.880859375, 0.02734375, 0.048828125, 0.03515625, 0.025390625, 0.99609375, 0.04296875, 0.02734375, 0.015625, 0.021484375, 0.857421875, 0.08203125, 0.015625, 0.046875, 0.00390625, 0.025390625, 0.05078125, 0.078125, 0.0390625, 0.146484375, 0.017578125, 0.044921875, 0.095703125, 0.078125, 0.0234375, 0.0234375, 0.208984375, 0.04296875, 0.591796875, 0.072265625, 0.015625, 0.017578125, 0.99609375, 0.037109375, 0.037109375, 0.0625, 0.06640625, 0.08203125, 0.99609375, 0.12109375, 0.99609375, 0.033203125, 0.033203125, 0.08984375, 0.033203125, 0.0, 0.017578125, 0.056640625, 0.8984375, 0.771484375, 0.033203125, 0.001953125, 0.017578125, 0.0546875, 0.1171875, 0.08203125, 0.876953125, 0.06640625, 0.04296875, 0.046875, 0.99609375, 0.1484375, 0.015625, 0.880859375, 0.033203125, 0.060546875, 0.03125, 0.041015625, 0.994140625, 0.00390625, 0.0390625, 0.0625, 0.341796875, 0.015625, 0.04296875, 0.982421875, 0.09765625, 0.125, 0.99609375, 0.06640625, 0.07421875, 0.033203125, 0.111328125, 0.859375, 0.87109375, 0.013671875, 0.68359375, 0.8828125, 0.998046875, 0.99609375, 0.05078125, 0.041015625, 0.78515625, 0.0078125, 0.037109375, 0.02734375, 0.048828125, 0.212890625, 0.994140625, 0.99609375, 0.044921875, 0.064453125, 0.072265625, 0.025390625, 0.99609375, 0.83984375, 0.017578125, 0.025390625, 0.05078125, 0.044921875, 0.048828125, 0.072265625, 0.021484375, 0.900390625, 0.05078125, 0.994140625, 0.056640625, 0.01953125, 0.03125, 0.771484375, 0.99609375, 0.126953125, 0.037109375, 0.013671875, 0.021484375, 0.02734375, 0.052734375, 0.046875, 0.875, 0.103515625, 0.99609375, 0.93359375, 0.091796875, 0.08984375, 0.201171875, 0.8671875, 0.0, 0.181640625, 0.056640625, 0.015625, 0.22265625, 0.021484375, 0.125, 0.998046875, 0.958984375, 0.078125, 0.017578125, 0.041015625, 0.0390625, 0.09375, 0.994140625, 0.056640625, 0.001953125, 0.0390625, 0.10546875, 0.94921875, 0.046875, 0.044921875, 0.03515625, 0.884765625, 0.052734375, 0.02734375, 0.0703125, 0.029296875, 0.064453125, 0.04296875, 0.01953125, 0.013671875, 0.140625, 0.005859375, 0.935546875, 0.888671875, 0.21484375, 0.025390625, 0.994140625, 0.99609375, 0.041015625, 0.091796875, 0.111328125, 0.0234375, 0.939453125, 0.916015625, 0.033203125, 0.076171875, 0.0078125, 0.134765625, 0.09375, 0.052734375, 0.0625, 0.03125, 0.111328125, 0.095703125, 0.99609375, 0.0078125, 0.060546875, 0.02734375, 0.78125, 0.017578125, 0.013671875, 0.05859375, 0.005859375, 0.013671875, 0.017578125, 0.048828125, 0.841796875, 0.09765625, 0.048828125, 0.056640625, 0.08984375, 0.01171875, 0.1171875, 0.064453125, 0.84375, 0.0234375, 0.05859375, 0.025390625, 0.787109375, 0.025390625, 0.16796875, 0.99609375, 0.037109375, 0.078125, 0.994140625, 0.013671875, 0.99609375, 0.99609375, 0.28515625, 0.994140625, 0.15234375, 0.107421875, 0.7109375, 0.001953125, 0.03125, 0.060546875, 0.91015625, 0.0546875, 0.056640625, 0.994140625, 0.029296875, 0.087890625, 0.03515625, 0.857421875, 0.06640625, 0.0, 0.0078125, 0.0625, 0.033203125, 0.041015625, 0.9296875, 0.05078125, 0.033203125, 0.01953125, 0.03515625, 0.041015625, 0.06640625, 0.99609375, 0.06640625, 0.087890625, 0.994140625, 0.75390625, 0.01953125, 0.740234375, 0.015625, 0.01953125, 0.068359375, 0.06640625, 0.025390625, 0.06640625, 0.1875, 0.0546875, 0.107421875, 0.029296875, 0.083984375, 0.00390625, 0.119140625, 0.041015625, 0.05859375, 0.07421875, 0.044921875, 0.177734375, 0.0390625, 0.994140625, 0.900390625, 0.0390625, 0.12109375, 0.00390625, 0.048828125, 0.015625, 0.154296875, 0.859375, 0.0234375, 0.0, 0.1015625, 0.9453125, 0.697265625, 0.0859375, 0.99609375, 0.64453125, 0.90234375, 0.876953125, 0.02734375, 0.9375, 0.048828125, 0.919921875, 0.99609375, 0.015625, 0.0078125, 0.951171875, 0.12890625, 0.0078125, 0.12890625, 0.05859375, 0.01171875, 0.03125, 0.001953125, 0.0234375, 0.10546875, 0.951171875, 0.037109375, 0.076171875, 0.03515625, 0.041015625, 0.0, 0.09375, 0.732421875, 0.068359375, 0.041015625, 0.01953125, 0.001953125, 0.037109375, 0.017578125, 0.818359375, 0.99609375, 0.357421875, 0.060546875, 0.8125, 0.880859375, 0.09765625, 0.99609375, 0.015625, 0.056640625, 0.041015625, 0.064453125, 0.96875, 0.931640625, 0.9296875, 0.1796875, 0.025390625, 0.982421875, 0.017578125, 0.015625, 0.994140625, 0.884765625, 0.9921875, 0.037109375, 0.09765625, 0.794921875, 0.0078125, 0.955078125, 0.033203125, 0.994140625, 0.04296875, 0.126953125, 0.060546875, 0.001953125, 0.05859375, 0.03125, 0.994140625, 0.01171875, 0.025390625, 0.056640625, 0.82421875, 0.04296875, 0.271484375, 0.033203125, 0.068359375, 0.943359375, 0.91796875, 0.05859375, 0.93359375, 0.00390625, 0.01953125, 0.00390625, 0.1328125, 0.029296875, 0.1953125, 0.029296875, 0.02734375, 0.08984375, 0.046875, 0.01953125, 0.021484375, 0.0859375, 0.029296875, 0.998046875, 0.087890625, 0.99609375, 0.017578125, 0.921875, 0.751953125, 0.0234375, 0.0703125, 0.037109375, 0.01953125, 0.01171875, 0.009765625, 0.869140625, 0.044921875, 0.994140625, 0.056640625, 0.994140625, 0.01953125, 0.064453125, 0.1015625, 0.103515625, 0.080078125, 0.037109375, 0.025390625, 0.142578125, 0.189453125, 0.998046875, 0.029296875, 0.125, 0.15234375, 0.017578125, 0.048828125, 0.013671875, 0.056640625, 0.015625, 0.0078125, 0.154296875, 0.04296875, 0.01171875, 0.99609375, 0.642578125, 0.810546875, 0.09375, 0.029296875, 0.185546875, 0.041015625, 0.91796875, 0.041015625, 0.0234375, 0.033203125, 0.021484375, 0.76953125, 0.916015625, 0.048828125, 0.0546875, 0.017578125, 0.994140625, 0.798828125, 0.04296875, 0.828125, 0.021484375, 0.029296875, 0.205078125, 0.04296875, 0.62890625, 0.037109375, 0.09765625, 0.00390625, 0.046875, 0.66015625, 0.025390625, 0.046875, 0.015625, 0.015625, 0.009765625, 0.0234375, 0.044921875, 0.994140625, 0.0234375, 0.033203125, 0.04296875, 0.017578125, 0.021484375, 0.013671875, 0.05078125, 0.99609375, 0.095703125, 0.94921875, 0.107421875, 0.130859375, 0.99609375, 0.955078125, 0.072265625, 0.015625, 0.0625, 0.92578125, 0.044921875, 0.111328125, 0.056640625, 0.03125, 0.99609375, 0.865234375, 0.02734375, 0.408203125, 0.99609375, 0.0234375, 0.02734375, 0.05078125, 0.111328125, 0.029296875, 0.08984375, 0.017578125, 0.0078125, 0.017578125, 0.716796875, 0.037109375, 0.05078125, 0.0, 0.00390625, 0.09375, 0.798828125, 0.017578125, 0.759765625, 0.00390625, 0.015625, 0.99609375, 0.05078125, 0.03125, 0.76953125, 0.03125, 0.1015625, 0.90625, 0.05859375, 0.99609375, 0.083984375, 0.01171875, 0.029296875, 0.056640625, 0.11328125, 0.166015625, 0.994140625, 0.99609375, 0.0234375, 0.080078125, 0.021484375, 0.009765625, 0.0546875, 0.0234375, 0.998046875, 0.0078125, 0.01953125, 0.033203125, 0.205078125, 0.1953125, 0.052734375, 0.00390625, 0.080078125, 0.07421875, 0.994140625, 0.833984375, 0.16796875, 0.02734375, 0.03125, 0.76953125, 0.064453125, 0.00390625, 0.021484375, 0.048828125, 0.99609375, 0.138671875, 0.0078125, 0.056640625, 0.001953125, 0.017578125, 0.892578125, 0.994140625, 0.99609375, 0.005859375, 0.02734375, 0.033203125, 0.05078125, 0.0, 0.18359375, 0.033203125, 0.99609375, 0.041015625, 0.09765625, 0.0625, 0.9296875, 0.7109375, 0.02734375, 0.060546875, 0.994140625, 0.037109375, 0.03125, 0.013671875, 0.158203125, 0.072265625, 0.775390625, 0.021484375, 0.810546875, 0.013671875, 0.994140625, 0.021484375, 0.015625, 0.05078125, 0.009765625, 0.037109375, 0.123046875, 0.12109375, 0.96875, 0.0, 0.025390625, 0.078125, 0.0703125, 0.142578125, 0.935546875, 0.037109375, 0.861328125, 0.03515625, 0.935546875, 0.03125, 0.03125, 0.080078125, 0.021484375, 0.783203125, 0.052734375, 0.724609375, 0.037109375, 0.03515625, 0.19921875, 0.03515625, 0.044921875, 0.142578125, 0.908203125, 0.005859375, 0.021484375, 0.037109375, 0.12109375, 0.0703125, 0.029296875, 0.095703125, 0.0234375, 0.029296875, 0.048828125, 0.076171875, 0.017578125, 0.041015625, 0.001953125, 0.169921875, 0.05078125, 0.037109375, 0.09375, 0.25390625, 0.03515625, 0.029296875, 0.03125, 0.994140625, 0.119140625, 0.94140625, 0.99609375, 0.015625, 0.208984375, 0.8828125, 0.810546875, 0.015625, 0.994140625, 0.107421875, 0.009765625, 0.091796875, 0.025390625, 0.1328125, 0.158203125, 0.048828125, 0.03515625, 0.03125, 0.783203125, 0.029296875, 0.037109375, 0.99609375, 0.021484375, 0.052734375, 0.97265625, 0.154296875, 0.232421875, 0.02734375, 0.994140625, 0.083984375, 0.830078125, 0.88671875, 0.0, 0.03125, 0.0859375, 0.037109375, 0.052734375, 0.02734375, 0.0546875, 0.99609375, 0.037109375, 0.0546875, 0.99609375, 0.0390625, 0.029296875, 0.947265625, 0.021484375, 0.109375, 0.0390625, 0.0234375, 0.041015625, 0.998046875, 0.212890625, 0.041015625, 0.09375, 0.080078125, 0.01171875, 0.099609375, 0.802734375, 0.06640625, 0.044921875, 0.955078125, 0.0390625, 0.064453125, 0.626953125, 0.0, 0.12109375, 0.02734375, 0.095703125, 0.044921875, 0.03515625, 0.083984375, 0.015625, 0.99609375, 0.03125, 0.94921875, 0.998046875, 0.99609375, 0.029296875, 0.93359375, 0.068359375, 0.876953125, 0.060546875, 0.015625, 0.01953125, 0.00390625, 0.033203125, 0.06640625, 0.046875, 0.013671875, 0.08203125, 0.041015625, 0.23046875, 0.9375, 0.001953125, 0.037109375, 0.03125, 0.994140625, 0.001953125, 0.013671875, 0.025390625, 0.0703125, 0.65234375, 0.1171875, 0.0, 0.99609375, 0.33203125, 0.962890625, 0.904296875, 0.041015625, 0.08203125, 0.775390625, 0.0546875, 0.994140625, 0.998046875, 0.994140625, 0.0, 0.99609375, 0.056640625, 0.033203125, 0.09375, 0.0078125, 0.994140625, 0.068359375, 0.037109375, 0.03125, 0.017578125, 0.017578125, 0.998046875, 0.068359375, 0.025390625, 0.078125, 0.091796875, 0.5390625, 0.033203125, 0.99609375, 0.27734375, 0.013671875, 0.2734375, 0.99609375, 0.1875, 0.0234375, 0.21484375, 0.79296875, 0.029296875, 0.029296875, 0.107421875, 0.0625, 0.01171875, 0.076171875, 0.091796875, 0.119140625, 0.06640625, 0.072265625, 0.99609375, 0.787109375, 0.998046875, 0.064453125, 0.01953125, 0.03515625, 0.021484375, 0.029296875, 0.013671875, 0.107421875, 0.0859375, 0.041015625, 0.708984375, 0.013671875, 0.994140625, 0.041015625, 0.994140625, 0.99609375, 0.013671875, 0.021484375, 0.103515625, 0.05078125, 0.08984375, 0.0625, 0.01953125, 0.03125, 0.0390625, 0.939453125, 0.06640625, 0.0234375, 0.037109375, 0.03515625, 0.033203125, 0.017578125, 0.1015625, 0.02734375, 0.05859375, 0.109375, 0.0078125, 0.03515625, 0.140625, 0.017578125, 0.9453125, 0.02734375, 0.013671875, 0.05859375, 0.01171875, 0.775390625, 0.033203125, 0.0078125, 0.015625, 0.033203125, 0.064453125, 0.02734375, 0.04296875, 0.994140625, 0.775390625, 0.88671875, 0.07421875, 0.03515625, 0.048828125, 0.923828125, 0.033203125, 0.037109375, 0.92578125, 0.037109375, 0.037109375, 0.083984375, 0.837890625, 0.99609375, 0.029296875, 0.0, 0.95703125, 0.951171875, 0.021484375, 0.048828125, 0.001953125, 0.01171875, 0.01171875, 0.080078125, 0.03515625, 0.015625, 0.208984375, 0.10546875, 0.03515625, 0.994140625, 0.625, 0.0546875, 0.0390625, 0.017578125, 0.994140625, 0.037109375, 0.994140625, 0.029296875, 0.14453125, 0.04296875, 0.0078125, 0.138671875, 0.029296875, 0.04296875, 0.05078125, 0.029296875, 0.0546875, 0.01171875, 0.935546875, 0.138671875, 0.11328125, 0.068359375, 0.037109375, 0.05859375]

 sparsity of   [0.0224609375, 0.0, 0.0283203125, 0.03125, 0.001953125, 0.015625, 0.0419921875, 0.0126953125, 0.0146484375, 0.0009765625, 0.0, 0.0244140625, 0.1376953125, 0.005859375, 0.1416015625, 0.0361328125, 0.001953125, 0.01171875, 0.0205078125, 0.0126953125, 0.15625, 0.0009765625, 0.6904296875, 0.1083984375, 0.3544921875, 0.0029296875, 0.078125, 0.044921875, 0.03515625, 0.0009765625, 0.0, 0.0029296875, 0.0029296875, 0.0068359375, 0.0107421875, 0.9677734375, 0.0068359375, 0.001953125, 0.1748046875, 0.0205078125, 0.61328125, 0.087890625, 0.0126953125, 0.01953125, 0.0205078125, 0.01953125, 0.123046875, 0.0263671875, 0.0, 0.013671875, 0.0, 0.03515625, 0.19921875, 0.013671875, 0.0029296875, 0.0146484375, 0.0302734375, 0.0517578125, 0.04296875, 0.0341796875, 0.0146484375, 0.0, 0.0029296875, 0.9443359375, 0.9990234375, 0.046875, 0.0, 0.8984375, 0.123046875, 0.00390625, 0.00390625, 0.234375, 0.025390625, 0.291015625, 0.0166015625, 0.001953125, 0.0009765625, 0.0166015625, 0.0224609375, 0.0302734375, 0.05859375, 0.009765625, 0.0703125, 0.083984375, 0.0361328125, 0.0205078125, 0.0234375, 0.8896484375, 0.3544921875, 0.72265625, 0.0244140625, 0.001953125, 0.00390625, 0.998046875, 0.01171875, 0.126953125, 0.111328125, 0.12109375, 0.00390625, 0.0341796875, 0.0322265625, 0.0048828125, 0.0146484375, 0.0, 0.001953125, 0.0029296875, 0.6591796875, 0.38671875, 0.373046875, 0.01171875, 0.009765625, 0.8388671875, 0.001953125, 0.001953125, 0.77734375, 0.1806640625, 0.0126953125, 0.0029296875, 0.1064453125, 0.0361328125, 0.009765625, 0.06640625, 0.9970703125, 0.0361328125, 0.02734375, 0.0068359375, 0.0009765625, 0.0029296875, 0.037109375, 0.0390625, 0.0341796875, 0.0771484375, 0.03515625, 0.1181640625, 0.0009765625, 0.8291015625, 0.0009765625, 0.0810546875, 0.021484375, 0.0107421875, 0.00390625, 0.0712890625, 0.0634765625, 0.0009765625, 0.0302734375, 0.953125, 0.9267578125, 0.001953125, 0.9423828125, 0.0546875, 0.01171875, 0.0, 0.021484375, 0.0029296875, 0.8994140625, 0.0283203125, 0.068359375, 0.0322265625, 0.310546875, 0.0029296875, 0.24609375, 0.0, 0.1513671875, 0.025390625, 0.9990234375, 0.0419921875, 0.9970703125, 0.00390625, 0.0380859375, 0.8955078125, 0.0263671875, 0.126953125, 0.0341796875, 0.04296875, 0.03515625, 0.0, 0.0029296875, 0.8896484375, 0.115234375, 0.0595703125, 0.0244140625, 0.060546875, 0.0859375, 0.0234375, 0.0087890625, 0.81640625, 0.03125, 0.005859375, 0.04296875, 0.052734375, 0.2763671875, 0.01171875, 0.0, 0.0400390625, 0.310546875, 0.078125, 0.017578125, 0.9013671875, 0.0107421875, 0.0, 0.0185546875, 0.0439453125, 0.859375, 0.494140625, 0.025390625, 0.009765625, 0.015625, 0.162109375, 0.1103515625, 0.203125, 0.1728515625, 0.0322265625, 0.00390625, 0.046875, 0.0498046875, 0.123046875, 0.0439453125, 0.0341796875, 0.015625, 0.02734375, 0.0166015625, 0.1806640625, 0.0234375, 0.0341796875, 0.0146484375, 0.0146484375, 0.0712890625, 0.017578125, 0.0751953125, 0.0341796875, 0.0185546875, 0.0234375, 0.650390625, 0.0390625, 0.0126953125, 0.0361328125, 0.8779296875, 0.0146484375, 0.0380859375, 0.01953125, 0.2919921875, 0.1162109375, 0.0537109375, 0.0283203125, 0.0341796875, 0.0234375, 0.0087890625, 0.0078125, 0.001953125, 0.013671875, 0.0478515625, 0.09375, 0.009765625, 0.0234375, 0.8701171875, 0.90234375, 0.00390625, 0.11328125, 0.046875, 0.0283203125, 0.08984375, 0.037109375, 0.1220703125, 0.041015625, 0.001953125, 0.9130859375, 0.197265625, 0.021484375, 0.01953125, 0.0439453125, 0.04296875, 0.0400390625, 0.123046875, 0.0419921875, 0.0126953125, 0.0341796875, 0.04296875, 0.1630859375, 0.900390625, 0.2587890625, 0.1796875, 0.8857421875, 0.0224609375, 0.1015625, 0.9990234375, 0.0, 0.0361328125, 0.0126953125, 0.1396484375, 0.0625, 0.0029296875, 0.0, 0.166015625, 0.1611328125, 0.1708984375, 0.0, 0.0478515625, 0.048828125, 0.734375, 0.919921875, 0.1416015625, 0.017578125, 0.0, 0.017578125, 0.0146484375, 0.0048828125, 0.158203125, 0.046875, 0.1162109375, 0.0400390625, 0.015625, 0.0498046875, 0.005859375, 0.7080078125, 0.0302734375, 0.00390625, 0.001953125, 0.0, 0.220703125, 0.1083984375, 0.001953125, 0.001953125, 0.015625, 0.8740234375, 0.1259765625, 0.0205078125, 0.837890625, 0.35546875, 0.998046875, 0.02734375, 0.009765625, 0.0400390625, 0.224609375, 0.0, 0.021484375, 0.9609375, 0.005859375, 0.0224609375, 0.0205078125, 0.0908203125, 0.0244140625, 0.0615234375, 0.046875, 0.0087890625, 0.037109375, 0.15625, 0.0, 0.17578125, 0.0029296875, 0.1552734375, 0.0087890625, 0.0390625, 0.017578125, 0.8359375, 0.11328125, 0.00390625, 0.013671875, 0.998046875, 0.01171875, 0.107421875, 0.0087890625, 0.119140625, 0.0859375, 0.0, 0.9296875, 0.8935546875, 0.9365234375, 0.0419921875, 0.0283203125, 0.001953125, 0.00390625, 0.0302734375, 0.0029296875, 0.017578125, 0.001953125, 0.001953125, 0.998046875, 0.9990234375, 0.0224609375, 0.4658203125, 0.1083984375, 0.0322265625, 0.0, 0.1513671875, 0.2158203125, 0.0517578125, 0.470703125, 0.0205078125, 0.001953125, 0.0791015625, 0.0537109375, 0.07421875, 0.001953125, 0.0244140625, 0.15625, 0.0029296875, 0.013671875, 0.998046875, 0.0234375, 0.041015625, 0.0400390625, 0.0, 0.45703125, 0.005859375, 0.025390625, 0.0048828125, 0.02734375, 0.025390625, 0.015625, 0.0087890625, 0.00390625, 0.876953125, 0.0029296875, 0.0126953125, 0.0888671875, 0.00390625, 0.0361328125, 0.0361328125, 0.0322265625, 0.0234375, 0.021484375, 0.015625, 0.10546875, 0.0048828125, 0.0078125, 0.0478515625, 0.0068359375, 0.107421875, 0.126953125, 0.0146484375, 0.107421875, 0.0146484375, 0.0166015625, 0.005859375, 0.046875, 0.0771484375, 0.0, 0.0419921875, 0.001953125, 0.9970703125, 0.0029296875, 0.041015625, 0.1181640625, 0.1865234375, 0.1103515625, 0.044921875, 0.0185546875, 0.052734375, 0.904296875, 0.046875, 0.001953125, 0.029296875, 0.0302734375, 0.0234375, 0.025390625, 0.0224609375, 0.021484375, 0.9345703125, 0.0712890625, 0.0009765625, 0.0205078125, 0.0029296875, 0.916015625, 0.0107421875, 0.0185546875, 0.001953125, 0.0234375, 0.7607421875, 0.021484375, 0.0966796875, 0.0322265625, 0.0458984375, 0.0009765625, 0.0078125, 0.01953125, 0.0947265625, 0.03515625, 0.01171875, 0.0029296875, 0.9013671875, 0.1298828125, 0.001953125, 0.0029296875, 0.0, 0.029296875, 0.1376953125, 0.0283203125, 0.04296875, 0.12890625, 0.02734375, 0.013671875, 0.0498046875, 0.0537109375, 0.0361328125, 0.0673828125, 0.150390625, 0.9306640625, 0.0419921875, 0.0390625, 0.9970703125, 0.03125, 0.001953125, 0.921875, 0.1064453125, 0.1357421875, 0.2373046875, 0.2763671875, 0.052734375, 0.0029296875, 0.029296875, 0.390625, 0.1181640625, 0.1171875, 0.017578125, 0.6689453125, 0.09765625, 0.0, 0.068359375, 0.0234375, 0.0, 0.158203125, 0.0009765625, 0.0244140625, 0.01171875, 0.00390625, 0.2041015625, 0.8623046875, 0.0234375, 0.001953125, 0.0380859375, 0.9150390625, 0.0146484375, 0.1083984375, 0.03515625, 0.001953125, 0.0048828125, 0.0380859375, 0.072265625, 0.0009765625, 0.046875, 0.0234375, 0.03515625, 0.005859375, 0.01953125, 0.080078125, 0.0361328125, 0.056640625, 0.0126953125, 0.876953125, 0.0, 0.0078125, 0.0380859375, 0.283203125, 0.00390625, 0.345703125, 0.890625, 0.2578125, 0.7724609375, 0.02734375, 0.001953125, 0.046875, 0.111328125, 0.0048828125, 0.0712890625, 0.095703125, 0.0234375, 0.8583984375, 0.0224609375, 0.08984375, 0.0009765625, 0.08203125, 0.0048828125, 0.0009765625, 0.0029296875, 0.1494140625, 0.681640625, 0.04296875, 0.1943359375, 0.15625, 0.0439453125, 0.8876953125, 0.0029296875, 0.26171875, 0.0224609375, 0.0341796875, 0.013671875, 0.0, 0.9033203125, 0.009765625, 0.02734375, 0.1904296875, 0.0283203125, 0.0673828125, 0.03125, 0.0, 0.0634765625, 0.0, 0.0, 0.03515625, 0.0576171875, 0.0009765625, 0.046875, 0.8896484375, 0.9189453125, 0.015625, 0.015625, 0.037109375, 0.01171875, 0.041015625, 0.009765625, 0.0166015625, 0.025390625, 0.998046875, 0.0, 0.015625, 0.0146484375, 0.0, 0.01953125, 0.00390625, 0.0322265625, 0.8662109375, 0.8681640625, 0.134765625, 0.0224609375, 0.015625, 0.1767578125, 0.046875, 0.1162109375, 0.09375, 0.0341796875, 0.0, 0.025390625, 0.015625, 0.01953125, 0.0380859375, 0.0, 0.2353515625, 0.0146484375, 0.0185546875, 0.0, 0.0390625, 0.0, 0.00390625, 0.0419921875, 0.017578125, 0.609375, 0.3037109375, 0.033203125, 0.19140625, 0.0029296875, 0.0390625, 0.052734375, 0.0283203125, 0.015625, 0.0634765625, 0.0009765625, 0.064453125, 0.0048828125, 0.0087890625, 0.0185546875, 0.025390625, 0.046875, 0.0625, 0.1201171875, 0.0380859375, 0.9990234375, 0.0400390625, 0.146484375, 0.01953125, 0.00390625, 0.1396484375, 0.001953125, 0.1904296875, 0.8564453125, 0.02734375, 0.0107421875, 0.9990234375, 0.015625, 0.0048828125, 0.0439453125, 0.015625, 0.02734375, 0.021484375, 0.0, 0.0390625, 0.931640625, 0.021484375, 0.0048828125, 0.064453125, 0.0400390625, 0.0029296875, 0.0517578125, 0.0849609375, 0.0439453125, 0.0126953125, 0.07421875, 0.0185546875, 0.0029296875, 0.9970703125, 0.03125, 0.076171875, 0.794921875, 0.005859375, 0.7314453125, 0.0146484375, 0.0341796875, 0.0029296875, 0.013671875, 0.0927734375, 0.0, 0.0263671875, 0.0234375, 0.3291015625, 0.0224609375, 0.0126953125, 0.025390625, 0.0029296875, 0.0, 0.044921875, 0.0419921875, 0.0283203125, 0.0185546875, 0.3427734375, 0.0224609375, 0.0146484375, 0.998046875, 0.0068359375, 0.0537109375, 0.0595703125, 0.0126953125, 0.0224609375, 0.017578125, 0.0654296875, 0.0068359375, 0.13671875, 0.0341796875, 0.029296875, 0.0078125, 0.00390625, 0.1748046875, 0.998046875, 0.0400390625, 0.001953125, 0.029296875, 0.0009765625, 0.0322265625, 0.0029296875, 0.00390625, 0.0419921875, 0.0361328125, 0.041015625, 0.0009765625, 0.046875, 0.0009765625, 0.0302734375, 0.0185546875, 0.025390625, 0.001953125, 0.0703125, 0.015625, 0.9638671875, 0.0263671875, 0.025390625, 0.001953125, 0.029296875, 0.0185546875, 0.8916015625, 0.0263671875, 0.185546875, 0.04296875, 0.044921875, 0.0, 0.0, 0.1455078125, 0.033203125, 0.0732421875, 0.0234375, 0.0322265625, 0.1865234375, 0.9111328125, 0.0419921875, 0.0888671875, 0.0732421875, 0.9033203125, 0.748046875, 0.0283203125, 0.2099609375, 0.88671875, 0.73828125, 0.1259765625, 0.8046875, 0.0, 0.0341796875, 0.0146484375, 0.009765625, 0.0087890625, 0.0751953125, 0.0078125, 0.0009765625, 0.0517578125, 0.0107421875, 0.0380859375, 0.013671875, 0.1787109375, 0.9990234375, 0.0009765625, 0.869140625, 0.0244140625, 0.087890625, 0.017578125, 0.0322265625, 0.005859375, 0.0029296875, 0.0, 0.041015625, 0.1044921875, 0.013671875, 0.83984375, 0.021484375, 0.033203125, 0.025390625, 0.0029296875, 0.0029296875, 0.00390625, 0.009765625, 0.029296875, 0.017578125, 0.033203125, 0.0166015625, 0.150390625, 0.02734375, 0.1259765625, 0.0146484375, 0.0, 0.0185546875, 0.0, 0.0048828125, 0.048828125, 0.232421875, 0.001953125, 0.03515625, 0.01171875, 0.0126953125, 0.96875, 0.1669921875, 0.056640625, 0.00390625, 0.123046875, 0.9169921875, 0.0, 0.013671875, 0.0009765625, 0.1513671875, 0.0439453125, 0.0224609375, 0.14453125, 0.025390625, 0.0048828125, 0.0029296875, 0.046875, 0.0029296875, 0.771484375, 0.0322265625, 0.91015625, 0.8662109375, 0.23828125, 0.03515625, 0.1142578125, 0.888671875, 0.140625, 0.0, 0.05859375, 0.0439453125, 0.0087890625, 0.001953125, 0.0107421875, 0.0302734375, 0.9287109375, 0.0146484375, 0.015625, 0.1220703125, 0.0693359375, 0.0087890625, 0.0576171875, 0.037109375, 0.0302734375, 0.046875, 0.0185546875, 0.0234375, 0.021484375, 0.458984375, 0.47265625, 0.001953125, 0.0009765625, 0.0302734375, 0.107421875, 0.00390625, 0.9267578125, 0.033203125, 0.076171875, 0.072265625, 0.001953125, 0.1015625, 0.0, 0.001953125, 0.0302734375, 0.044921875, 0.1806640625, 0.046875, 0.017578125, 0.0, 0.1103515625, 0.0009765625, 0.015625, 0.005859375, 0.0439453125, 0.0166015625, 0.001953125, 0.0126953125, 0.9384765625, 0.1015625, 0.76953125, 0.8037109375, 0.123046875, 0.0810546875, 0.0205078125, 0.0146484375, 0.0224609375, 0.0712890625, 0.0078125, 0.6435546875, 0.0166015625, 0.0234375, 0.1318359375, 0.0068359375, 0.1279296875, 0.025390625, 0.001953125, 0.1669921875, 0.939453125, 0.9326171875, 0.0126953125, 0.046875, 0.03515625, 0.09375, 0.0205078125, 0.005859375, 0.0009765625, 0.025390625, 0.619140625, 0.1337890625, 0.001953125, 0.0087890625, 0.9150390625, 0.0, 0.9111328125, 0.0146484375, 0.3125, 0.017578125, 0.0869140625, 0.486328125, 0.0224609375, 0.0634765625, 0.154296875, 0.1796875, 0.080078125, 0.25, 0.0166015625, 0.861328125, 0.0244140625, 0.0732421875, 0.0224609375, 0.0341796875, 0.955078125, 0.0244140625, 0.998046875, 0.07421875, 0.01171875, 0.0341796875, 0.00390625, 0.037109375, 0.0146484375, 0.099609375, 0.04296875, 0.0693359375, 0.0205078125, 0.1904296875, 0.0986328125, 0.0810546875, 0.046875, 0.015625, 0.03515625, 0.9326171875, 0.16796875, 0.001953125, 0.0546875, 0.05859375, 0.0068359375, 0.0029296875, 0.00390625, 0.001953125, 0.8837890625, 0.04296875, 0.0126953125, 0.154296875, 0.02734375, 0.2294921875, 0.03125, 0.013671875, 0.0234375, 0.00390625, 0.0263671875, 0.0966796875, 0.044921875, 0.0029296875, 0.015625, 0.8623046875, 0.0, 0.998046875, 0.0, 0.046875, 0.1455078125, 0.0576171875, 0.0078125, 0.0810546875, 0.005859375, 0.0107421875, 0.4482421875, 0.0029296875, 0.0361328125, 0.06640625, 0.1826171875, 0.0234375, 0.017578125, 0.169921875, 0.044921875, 0.01171875, 0.0205078125, 0.00390625, 0.2900390625, 0.6572265625, 0.009765625, 0.4169921875, 0.185546875, 0.998046875, 0.001953125, 0.0126953125, 0.8837890625, 0.01171875, 0.0283203125, 0.28125, 0.001953125, 0.84765625, 0.9052734375, 0.04296875, 0.0322265625, 0.025390625, 0.013671875, 0.0107421875, 0.0068359375, 0.1494140625, 0.1220703125, 0.017578125, 0.884765625, 0.046875, 0.8896484375, 0.01171875, 0.998046875, 0.0517578125, 0.9375, 0.0029296875, 0.00390625, 0.0849609375, 0.724609375, 0.0146484375, 0.0166015625, 0.021484375, 0.08984375, 0.037109375, 0.0068359375, 0.017578125, 0.0390625, 0.0380859375, 0.001953125, 0.0263671875, 0.041015625, 0.052734375, 0.0224609375, 0.013671875, 0.033203125, 0.0048828125, 0.0, 0.001953125, 0.03515625, 0.03515625, 0.3466796875, 0.1416015625, 0.015625, 0.8759765625, 0.0, 0.0556640625, 0.025390625, 0.0205078125, 0.0, 0.05859375, 0.046875, 0.7470703125, 0.1455078125, 0.0, 0.0322265625, 0.001953125, 0.0009765625, 0.06640625, 0.724609375, 0.0361328125, 0.3115234375, 0.2587890625, 0.1181640625, 0.0419921875, 0.7568359375, 0.0771484375, 0.1396484375, 0.0166015625, 0.044921875, 0.0302734375, 0.064453125, 0.001953125, 0.0, 0.00390625, 0.1923828125, 0.9345703125, 0.0126953125, 0.041015625, 0.0029296875, 0.052734375, 0.03515625, 0.1328125, 0.001953125, 0.015625, 0.0068359375, 0.947265625, 0.0, 0.0283203125, 0.19921875, 0.0908203125, 0.0, 0.0068359375, 0.015625, 0.0009765625, 0.146484375, 0.9189453125, 0.0400390625, 0.8857421875, 0.9970703125, 0.021484375, 0.759765625, 0.013671875, 0.0361328125, 0.1572265625, 0.0078125, 0.033203125, 0.01171875, 0.00390625, 0.0712890625, 0.041015625, 0.03125, 0.0419921875, 0.998046875, 0.0, 0.44140625, 0.037109375, 0.0791015625, 0.021484375, 0.125, 0.0009765625, 0.0224609375, 0.01953125, 0.0654296875, 0.013671875, 0.169921875, 0.220703125, 0.1123046875, 0.9248046875, 0.0029296875, 0.0146484375, 0.9013671875, 0.859375, 0.001953125, 0.03125, 0.0576171875, 0.2890625, 0.00390625, 0.9501953125, 0.001953125, 0.9970703125, 0.2431640625, 0.068359375, 0.015625, 0.001953125, 0.0068359375, 0.001953125, 0.0908203125, 0.3720703125, 0.0009765625, 0.9267578125, 0.021484375, 0.0234375, 0.2373046875, 0.2275390625, 0.01953125, 0.0341796875, 0.9970703125, 0.8056640625, 0.12890625, 0.0439453125, 0.0390625, 0.041015625, 0.703125, 0.0224609375, 0.056640625, 0.208984375, 0.0, 0.017578125, 0.0, 0.1328125, 0.044921875, 0.9345703125, 0.017578125, 0.0, 0.0859375, 0.00390625, 0.560546875, 0.0234375, 0.0283203125, 0.0, 0.0068359375, 0.7685546875, 0.0048828125, 0.8994140625, 0.0009765625, 0.111328125, 0.0087890625, 0.2099609375, 0.1025390625, 0.0185546875, 0.0185546875, 0.0751953125, 0.0791015625, 0.013671875, 0.0234375, 0.0263671875, 0.455078125, 0.01953125, 0.1416015625, 0.8720703125, 0.2080078125, 0.0341796875, 0.029296875, 0.0166015625, 0.6259765625, 0.998046875, 0.0146484375, 0.009765625, 0.8857421875, 0.0185546875, 0.0, 0.87890625, 0.017578125, 0.0390625, 0.0, 0.029296875, 0.046875, 0.0068359375, 0.0244140625, 0.0283203125, 0.0283203125, 0.0849609375, 0.08984375, 0.19140625, 0.0009765625, 0.044921875, 0.0, 0.033203125, 0.0068359375, 0.009765625, 0.1142578125, 0.072265625, 0.056640625, 0.1201171875, 0.037109375, 0.078125, 0.1845703125, 0.0087890625, 0.126953125, 0.9033203125, 0.0400390625, 0.083984375, 0.037109375, 0.0439453125, 0.052734375, 0.0068359375, 0.00390625, 0.0732421875, 0.6455078125, 0.005859375, 0.0078125, 0.0009765625, 0.0166015625, 0.0380859375, 0.0, 0.017578125, 0.0537109375, 0.033203125, 0.0009765625, 0.0, 0.0419921875, 0.0400390625, 0.0087890625, 0.0, 0.0185546875, 0.0009765625, 0.998046875, 0.0634765625, 0.0107421875, 0.197265625, 0.001953125, 0.0634765625, 0.0224609375, 0.0185546875, 0.03515625, 0.8857421875, 0.03125, 0.02734375, 0.01171875, 0.02734375, 0.001953125, 0.0693359375, 0.7392578125, 0.3544921875, 0.015625, 0.0234375, 0.9970703125, 0.005859375, 0.021484375, 0.0205078125, 0.0009765625, 0.046875, 0.1513671875, 0.4208984375, 0.0263671875, 0.00390625, 0.0341796875, 0.087890625, 0.7998046875, 0.0, 0.013671875, 0.0029296875, 0.0, 0.0400390625, 0.0009765625, 0.3857421875, 0.3525390625, 0.0166015625, 0.01171875, 0.0029296875, 0.0068359375, 0.126953125, 0.029296875, 0.001953125, 0.916015625, 0.0, 0.02734375, 0.0244140625, 0.0234375, 0.0087890625, 0.0400390625, 0.0556640625, 0.12890625, 0.8818359375, 0.0, 0.998046875, 0.037109375, 0.0205078125, 0.9990234375, 0.0361328125, 0.998046875, 0.0478515625, 0.005859375, 0.8291015625, 0.8818359375, 0.01953125, 0.00390625, 0.001953125, 0.6572265625, 0.0283203125, 0.7724609375, 0.0517578125, 0.0263671875, 0.0029296875, 0.0, 0.21875, 0.0048828125, 0.8662109375, 0.130859375, 0.0224609375, 0.9521484375, 0.541015625, 0.7958984375, 0.001953125, 0.8896484375, 0.01171875, 0.0146484375, 0.041015625, 0.796875, 0.0478515625, 0.51953125, 0.1435546875, 0.001953125, 0.0458984375, 0.0556640625, 0.6494140625, 0.025390625, 0.0517578125, 0.2939453125, 0.0244140625, 0.04296875, 0.001953125, 0.0244140625, 0.013671875, 0.0, 0.7509765625, 0.037109375, 0.046875, 0.0263671875, 0.0009765625, 0.9990234375, 0.8427734375, 0.0205078125, 0.0, 0.0078125, 0.0166015625, 0.171875, 0.046875, 0.0244140625, 0.021484375, 0.2275390625, 0.0654296875, 0.0029296875, 0.00390625, 0.0390625, 0.802734375, 0.0361328125, 0.013671875, 0.05078125, 0.41015625, 0.8828125, 0.0234375, 0.0419921875, 0.0361328125, 0.0205078125, 0.0029296875, 0.0009765625, 0.1474609375, 0.041015625, 0.0048828125, 0.10546875, 0.0078125, 0.046875, 0.265625, 0.0458984375, 0.041015625, 0.033203125, 0.0107421875, 0.064453125, 0.2109375, 0.0224609375, 0.01171875, 0.0166015625, 0.041015625, 0.009765625, 0.0302734375, 0.037109375, 0.0107421875, 0.0966796875, 0.0146484375, 0.841796875, 0.833984375, 0.03515625, 0.0029296875, 0.666015625, 0.0810546875, 0.0048828125, 0.0107421875, 0.1650390625, 0.001953125, 0.1552734375, 0.0380859375, 0.138671875, 0.1513671875, 0.0283203125, 0.1806640625, 0.0068359375, 0.328125, 0.0390625, 0.04296875, 0.0029296875, 0.8203125, 0.009765625, 0.0478515625, 0.7294921875, 0.0146484375, 0.998046875, 0.029296875, 0.037109375, 0.857421875, 0.0556640625, 0.17578125, 0.001953125, 0.0234375, 0.0029296875, 0.125, 0.1396484375, 0.048828125, 0.3583984375, 0.0166015625, 0.021484375, 0.0224609375, 0.0859375, 0.0234375, 0.021484375, 0.951171875, 0.0, 0.04296875, 0.048828125, 0.998046875, 0.0380859375, 0.0234375, 0.0185546875, 0.00390625, 0.140625, 0.1318359375, 0.009765625, 0.2666015625, 0.0234375, 0.0673828125, 0.0146484375, 0.005859375, 0.01953125, 0.1552734375, 0.9267578125, 0.998046875, 0.001953125, 0.958984375, 0.0302734375, 0.0, 0.0068359375, 0.013671875, 0.037109375, 0.595703125, 0.6025390625, 0.138671875, 0.0029296875, 0.0302734375, 0.8857421875, 0.0439453125, 0.109375, 0.015625, 0.0078125, 0.017578125, 0.0185546875, 0.0009765625, 0.2998046875, 0.1328125, 0.0009765625, 0.05078125, 0.06640625, 0.0166015625, 0.013671875, 0.001953125, 0.8046875, 0.9990234375, 0.015625, 0.0400390625, 0.044921875, 0.0, 0.1171875, 0.009765625, 0.005859375, 0.0283203125, 0.0205078125, 0.00390625, 0.009765625, 0.0068359375, 0.904296875, 0.037109375, 0.046875, 0.025390625, 0.0009765625, 0.998046875, 0.1201171875, 0.001953125, 0.111328125, 0.0029296875, 0.015625, 0.0029296875, 0.109375, 0.666015625, 0.0224609375, 0.322265625, 0.009765625, 0.1181640625, 0.046875, 0.1201171875, 0.0048828125, 0.0048828125, 0.0009765625, 0.01953125, 0.001953125, 0.0029296875, 0.0009765625, 0.0078125, 0.0009765625, 0.1083984375, 0.912109375, 0.197265625, 0.009765625, 0.005859375, 0.0166015625, 0.0, 0.0078125, 0.04296875, 0.0244140625, 0.0087890625, 0.0048828125, 0.8525390625, 0.03515625, 0.0361328125, 0.0400390625, 0.0, 0.154296875, 0.0087890625, 0.00390625, 0.021484375, 0.1064453125, 0.626953125, 0.0869140625, 0.4384765625, 0.0263671875, 0.06640625, 0.0, 0.0302734375, 0.0234375, 0.009765625, 0.033203125, 0.0087890625, 0.0087890625, 0.2001953125, 0.041015625, 0.8798828125, 0.927734375, 0.029296875, 0.046875, 0.0244140625, 0.0029296875, 0.0, 0.0341796875, 0.0, 0.1416015625, 0.1884765625, 0.0, 0.0263671875, 0.0126953125, 0.00390625, 0.00390625, 0.0126953125, 0.0244140625, 0.009765625, 0.0078125, 0.1005859375, 0.224609375, 0.021484375, 0.1767578125, 0.0234375, 0.0693359375, 0.1181640625, 0.015625, 0.0322265625, 0.037109375, 0.28515625, 0.0283203125, 0.009765625, 0.0, 0.939453125, 0.0, 0.0166015625, 0.00390625, 0.0458984375, 0.021484375, 0.001953125, 0.025390625, 0.0302734375, 0.0068359375, 0.0078125, 0.314453125, 0.927734375, 0.0, 0.146484375, 0.02734375, 0.904296875, 0.1796875, 0.03515625, 0.0947265625, 0.1572265625, 0.0234375, 0.998046875, 0.103515625, 0.0263671875, 0.0107421875, 0.04296875, 0.1865234375, 0.001953125, 0.2216796875, 0.890625, 0.1103515625, 0.046875, 0.0263671875, 0.013671875, 0.0048828125, 0.2412109375, 0.0341796875, 0.0595703125, 0.044921875, 0.0048828125, 0.02734375, 0.021484375, 0.001953125, 0.1953125, 0.025390625, 0.7646484375, 0.0244140625, 0.015625, 0.0419921875, 0.6904296875, 0.5302734375, 0.0078125, 0.0107421875, 0.2939453125, 0.017578125, 0.046875, 0.0, 0.046875, 0.0419921875, 0.01953125, 0.0439453125, 0.0400390625, 0.005859375, 0.9208984375, 0.015625, 0.0029296875, 0.0029296875, 0.0341796875, 0.1376953125, 0.8740234375, 0.0009765625, 0.029296875, 0.0, 0.9130859375, 0.0048828125, 0.0419921875, 0.04296875, 0.0380859375, 0.0009765625, 0.3017578125, 0.029296875, 0.025390625, 0.8935546875, 0.001953125, 0.48046875, 0.046875, 0.9404296875, 0.00390625, 0.07421875, 0.0244140625, 0.2236328125, 0.0, 0.0283203125, 0.05859375, 0.0322265625, 0.0, 0.0029296875, 0.2548828125, 0.04296875, 0.001953125, 0.048828125, 0.0205078125, 0.001953125, 0.8740234375, 0.029296875, 0.0224609375, 0.01953125, 0.13671875, 0.00390625, 0.1904296875, 0.001953125, 0.083984375, 0.05078125, 0.0869140625, 0.9990234375, 0.015625, 0.0, 0.0224609375, 0.0029296875, 0.0263671875, 0.3720703125, 0.046875, 0.796875, 0.01953125, 0.859375, 0.7197265625, 0.1396484375, 0.208984375, 0.0126953125, 0.0400390625, 0.0771484375, 0.056640625, 0.015625, 0.01171875, 0.0498046875, 0.0, 0.9404296875, 0.037109375, 0.03125, 0.8720703125, 0.00390625, 0.013671875, 0.0009765625, 0.1513671875, 0.001953125, 0.0, 0.9091796875, 0.001953125, 0.0390625, 0.072265625, 0.001953125, 0.0185546875, 0.1298828125, 0.330078125, 0.8212890625, 0.03125, 0.8193359375, 0.0009765625, 0.0224609375, 0.0146484375, 0.08984375, 0.0, 0.017578125, 0.11328125, 0.0576171875, 0.3994140625, 0.0302734375, 0.0361328125, 0.01953125, 0.005859375, 0.0625, 0.029296875, 0.0009765625, 0.0478515625, 0.0048828125, 0.1357421875, 0.8779296875, 0.025390625, 0.015625, 0.025390625, 0.0, 0.046875, 0.0009765625, 0.0244140625, 0.294921875, 0.029296875, 0.0185546875, 0.03515625, 0.0029296875, 0.0126953125, 0.0341796875, 0.0078125, 0.330078125, 0.0009765625, 0.0, 0.0078125, 0.9658203125, 0.2626953125, 0.1708984375, 0.03515625, 0.193359375, 0.4013671875, 0.07421875, 0.0302734375, 0.01953125, 0.0263671875, 0.9169921875, 0.0078125, 0.0146484375, 0.005859375, 0.1015625, 0.017578125, 0.998046875, 0.025390625, 0.8359375, 0.6533203125, 0.998046875, 0.052734375, 0.0419921875, 0.9990234375, 0.5263671875, 0.001953125, 0.0146484375, 0.033203125, 0.0205078125, 0.0009765625, 0.1083984375, 0.0009765625, 0.8955078125, 0.0146484375, 0.01953125, 0.669921875, 0.005859375, 0.0009765625, 0.0048828125, 0.2138671875, 0.998046875, 0.005859375, 0.021484375, 0.0, 0.0380859375, 0.01171875, 0.046875, 0.0263671875, 0.0009765625, 0.0146484375, 0.09375, 0.1494140625, 0.8857421875, 0.0146484375, 0.1015625, 0.08984375, 0.001953125, 0.01953125, 0.029296875, 0.0302734375, 0.6171875, 0.0556640625, 0.0029296875, 0.306640625, 0.0, 0.0029296875, 0.01171875, 0.6513671875, 0.29296875, 0.0205078125, 0.7666015625, 0.0009765625, 0.0302734375, 0.03515625, 0.6611328125, 0.0, 0.0439453125, 0.046875, 0.806640625, 0.0283203125, 0.0009765625, 0.017578125, 0.0185546875, 0.0068359375, 0.01953125, 0.015625, 0.5517578125, 0.1611328125, 0.6142578125, 0.044921875, 0.0, 0.009765625, 0.0234375, 0.177734375, 0.314453125, 0.03515625, 0.0185546875, 0.005859375, 0.021484375, 0.0, 0.0703125, 0.17578125, 0.0146484375, 0.0341796875, 0.0146484375, 0.0048828125, 0.046875, 0.021484375, 0.0302734375, 0.025390625, 0.0263671875, 0.05859375, 0.0048828125, 0.3046875, 0.1884765625, 0.033203125, 0.0419921875, 0.00390625, 0.005859375, 0.03125, 0.0458984375, 0.1259765625, 0.0078125, 0.9072265625, 0.0224609375, 0.6025390625, 0.029296875, 0.0205078125, 0.0029296875, 0.0, 0.1865234375, 0.224609375, 0.1005859375, 0.0234375, 0.041015625, 0.0322265625, 0.0341796875, 0.9619140625, 0.3662109375, 0.16796875, 0.0458984375, 0.0, 0.03125, 0.0029296875, 0.009765625, 0.0, 0.0, 0.9990234375, 0.0166015625, 0.025390625, 0.0, 0.953125, 0.998046875, 0.0244140625, 0.0, 0.0, 0.0, 0.015625, 0.0009765625]

 sparsity of   [0.16015625, 0.04736328125, 0.9990234375, 0.08251953125, 0.08837890625, 0.79736328125, 0.03662109375, 0.60302734375, 0.08447265625, 0.01416015625, 0.0234375, 0.10009765625, 0.09619140625, 0.68701171875, 0.07373046875, 0.017578125, 0.447265625, 0.0166015625, 0.0595703125, 0.814453125, 0.06884765625, 0.0244140625, 0.9990234375, 0.90478515625, 0.7001953125, 0.04052734375, 0.2109375, 0.04833984375, 0.0751953125, 0.95166015625, 0.68701171875, 0.61572265625, 0.70068359375, 0.99853515625, 0.044921875, 0.58349609375, 0.02490234375, 0.1630859375, 0.16650390625, 0.4150390625, 0.0, 0.01611328125, 0.037109375, 0.61279296875, 0.1416015625, 0.10009765625, 0.017578125, 0.03759765625, 0.02734375, 0.6455078125, 0.9326171875, 0.013671875, 0.0205078125, 0.0205078125, 0.17626953125, 0.955078125, 0.203125, 0.578125, 0.47998046875, 0.93896484375, 0.72314453125, 0.01806640625, 0.078125, 0.07275390625, 0.029296875, 0.01904296875, 0.3134765625, 0.1552734375, 0.02685546875, 0.03076171875, 0.56005859375, 0.7666015625, 0.01416015625, 0.06396484375, 0.99853515625, 0.0576171875, 0.0556640625, 0.56884765625, 0.96728515625, 0.05712890625, 0.021484375, 0.5908203125, 0.05224609375, 0.06787109375, 0.83154296875, 0.02587890625, 0.046875, 0.47998046875, 0.0390625, 0.07177734375, 0.0185546875, 0.3486328125, 0.01513671875, 0.0625, 0.59716796875, 0.66552734375, 0.0, 0.037109375, 0.12158203125, 0.5751953125, 0.9228515625, 0.01416015625, 0.01708984375, 0.5537109375, 0.048828125, 0.62939453125, 0.93603515625, 0.6025390625, 0.9638671875, 0.0693359375, 0.9990234375, 0.1015625, 0.01025390625, 0.5205078125, 0.55712890625, 0.0244140625, 0.13818359375, 0.19189453125, 0.033203125, 0.0185546875, 0.0341796875, 0.03466796875, 0.5283203125, 0.18798828125, 0.03662109375, 0.03466796875, 0.06982421875, 0.015625, 0.68701171875, 0.91357421875, 0.0341796875, 0.5791015625, 0.9990234375, 0.36279296875, 0.02978515625, 0.10107421875, 0.09228515625, 0.06396484375, 0.04296875, 0.62548828125, 0.1337890625, 0.0263671875, 0.5703125, 0.99853515625, 0.01318359375, 0.07666015625, 0.68701171875, 0.59716796875, 0.6083984375, 0.02734375, 0.67041015625, 0.00048828125, 0.0166015625, 0.134765625, 0.68408203125, 0.07470703125, 0.015625, 0.08349609375, 0.03857421875, 0.04296875, 0.0625, 0.62744140625, 0.24267578125, 0.0234375, 0.09716796875, 0.1474609375, 0.08837890625, 0.041015625, 0.58251953125, 0.44091796875, 0.10205078125, 0.9326171875, 0.751953125, 0.61376953125, 0.052734375, 0.68701171875, 0.9990234375, 0.54345703125, 0.47607421875, 0.9990234375, 0.052734375, 0.369140625, 0.2177734375, 0.03466796875, 0.0751953125, 0.72802734375, 0.00927734375, 0.11767578125, 0.42822265625, 0.900390625, 0.0126953125, 0.05322265625, 0.59130859375, 0.04248046875, 0.9990234375, 0.0185546875, 0.00341796875, 0.10546875, 0.2021484375, 0.6015625, 0.2041015625, 0.15625, 0.08544921875, 0.12060546875, 0.21435546875, 0.04443359375, 0.05908203125, 0.06396484375, 0.10107421875, 0.03564453125, 0.0498046875, 0.03515625, 0.0732421875, 0.03369140625, 0.01904296875, 0.04931640625, 0.91748046875, 0.072265625, 0.16796875, 0.0478515625, 0.0224609375, 0.0, 0.03857421875, 0.50732421875, 0.01025390625, 0.0537109375, 0.45849609375, 0.8466796875, 0.1328125, 0.02880859375, 0.17236328125, 0.009765625, 0.02587890625, 0.6279296875, 0.0087890625, 0.5537109375, 0.0400390625, 0.06494140625, 0.1826171875, 0.04052734375, 0.0703125, 0.14599609375, 0.28857421875, 0.2294921875, 0.0146484375, 0.03271484375, 0.048828125, 0.0146484375, 0.61572265625, 0.58935546875, 0.05126953125, 0.94287109375, 0.06494140625, 0.09814453125, 0.67041015625, 0.13427734375, 0.0390625, 0.04833984375, 0.837890625, 0.02197265625, 0.0263671875, 0.8916015625, 0.560546875, 0.0302734375, 0.0771484375, 0.0341796875, 0.052734375, 0.99951171875, 0.05908203125, 0.01953125, 0.01953125, 0.03369140625, 0.0341796875, 0.15234375, 0.12939453125, 0.5517578125, 0.7041015625, 0.63427734375, 0.02099609375, 0.90771484375, 0.01806640625, 0.130859375, 0.5654296875, 0.03076171875, 0.01806640625, 0.12744140625, 0.0185546875, 0.5634765625, 0.72998046875, 0.0498046875, 0.1240234375, 0.05322265625, 0.0712890625, 0.552734375, 0.654296875, 0.02197265625, 0.19091796875, 0.037109375, 0.8896484375, 0.17919921875, 0.052734375, 0.015625, 0.107421875, 0.07666015625, 0.55078125, 0.0390625, 0.0712890625, 0.99853515625, 0.06640625, 0.0498046875, 0.03564453125, 0.01171875, 0.13720703125, 0.0185546875, 0.94970703125, 0.60302734375, 0.0244140625, 0.0439453125, 0.0087890625, 0.62890625, 0.0322265625, 0.57763671875, 0.05615234375, 0.88330078125, 0.03076171875, 0.078125, 0.59814453125, 0.06884765625, 0.12744140625, 0.0, 0.05078125, 0.72998046875, 0.06689453125, 0.0166015625, 0.015625, 0.0380859375, 0.0673828125, 0.99951171875, 0.02392578125, 0.54296875, 0.0263671875, 0.01513671875, 0.0595703125, 0.02392578125, 0.08544921875, 0.0703125, 0.9189453125, 0.046875, 0.0322265625, 0.05029296875, 0.99853515625, 0.0234375, 0.93017578125, 0.99853515625, 0.138671875, 0.01416015625, 0.02197265625, 0.55517578125, 0.0390625, 0.55810546875, 0.86865234375, 0.0224609375, 0.919921875, 0.9228515625, 0.03369140625, 0.68603515625, 0.2001953125, 0.005859375, 0.58447265625, 0.02978515625, 0.6259765625, 0.07421875, 0.072265625, 0.0205078125, 0.05224609375, 0.0478515625, 0.9990234375, 0.1689453125, 0.05126953125, 0.0, 0.150390625, 0.03564453125, 0.697265625, 0.14306640625, 0.0185546875, 0.07275390625, 0.939453125, 0.14697265625, 0.10546875, 0.060546875, 0.9755859375, 0.04296875, 0.03076171875, 0.03271484375, 0.99951171875, 0.0654296875, 0.01025390625, 0.0703125, 0.029296875, 0.75927734375, 0.0390625, 0.81884765625, 0.15625, 0.03857421875, 0.02490234375, 0.45263671875, 0.0, 0.029296875, 0.6484375, 0.0361328125, 0.9384765625, 0.5400390625, 0.08056640625, 0.0244140625, 0.05224609375, 0.0283203125, 0.56103515625, 0.9990234375, 0.83056640625, 0.5859375, 0.5517578125, 0.0869140625, 0.0546875, 0.68505859375, 0.087890625, 0.09521484375, 0.00732421875, 0.54541015625, 0.607421875, 0.86767578125, 0.630859375, 0.1044921875, 0.0390625, 0.68603515625, 0.34619140625, 0.1181640625, 0.0283203125, 0.08740234375, 0.0556640625, 0.01220703125, 0.078125, 0.02392578125, 0.0400390625, 0.046875, 0.0908203125, 0.01904296875, 0.59521484375, 0.13330078125, 0.02099609375, 0.0244140625, 0.08984375, 0.04150390625, 0.01123046875, 0.587890625, 0.0, 0.021484375, 0.01904296875, 0.0673828125, 0.0068359375, 0.5888671875, 0.01416015625, 0.09912109375, 0.02490234375, 0.0234375, 0.9365234375, 0.0791015625, 0.01708984375, 0.0537109375, 0.021484375, 0.17578125, 0.033203125, 0.61083984375, 0.36669921875, 0.05224609375, 0.0615234375, 0.5458984375, 0.9990234375, 0.10009765625, 0.62353515625, 0.0869140625, 0.943359375, 0.0146484375, 0.00927734375, 0.1025390625, 0.55908203125, 0.02001953125, 0.0498046875, 0.5498046875, 0.0263671875, 0.078125, 0.0, 0.75048828125, 0.56103515625, 0.61669921875, 0.02685546875, 0.09716796875, 0.01904296875, 0.07275390625, 0.58544921875, 0.103515625, 0.15966796875, 0.06201171875, 0.6162109375, 0.55322265625, 0.0380859375, 0.0322265625, 0.6201171875, 0.02685546875, 0.33056640625, 0.0302734375, 0.01904296875, 0.04931640625]

 sparsity of   [0.009982638992369175, 0.009765625, 0.0, 0.001953125, 0.9986979365348816, 0.0, 0.011501736007630825, 0.00021701389050576836, 0.0164930559694767, 0.0, 0.0657552108168602, 0.0, 0.2495659738779068, 0.1451822966337204, 0.0, 0.0004340277810115367, 0.8658854365348816, 0.0, 0.0013020833721384406, 0.005425347480922937, 0.01996527798473835, 0.0, 0.0, 0.0282118059694767, 0.00021701389050576836, 0.0405815988779068, 0.0, 0.1549479216337204, 0.3229166567325592, 0.011935763992369175, 0.0436197929084301, 0.008463541977107525, 0.667100727558136, 0.02105034701526165, 0.0451388880610466, 0.0184461809694767, 0.005859375, 0.0555555559694767, 0.0004340277810115367, 0.0891927108168602, 0.0438368059694767, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.014756944961845875, 0.0212673619389534, 0.008897569961845875, 0.0377604179084301, 0.0, 0.999131977558136, 0.0, 0.0284288190305233, 0.0440538190305233, 0.0234375, 0.0015190972480922937, 0.0531684048473835, 0.0045572915114462376, 0.009982638992369175, 0.0, 0.014322916977107525, 0.0225694440305233, 0.01215277798473835, 0.1013454869389534, 0.0, 0.02951388992369175, 0.0, 0.0, 0.0, 0.0, 0.029296875, 0.006076388992369175, 0.0316840298473835, 0.0015190972480922937, 0.0, 0.009765625, 0.9997829794883728, 0.0243055559694767, 0.0620659738779068, 0.0006510416860692203, 0.0, 0.00629340298473835, 0.0360243059694767, 0.0321180559694767, 0.0, 0.0271267369389534, 0.0, 0.011501736007630825, 0.0, 0.0, 0.0, 0.0026041667442768812, 0.04296875, 0.0336371548473835, 0.0, 0.0245225690305233, 0.0, 0.01019965298473835, 0.0453559048473835, 0.0, 0.02495659701526165, 0.00021701389050576836, 0.0, 0.01909722201526165, 0.00021701389050576836, 0.00021701389050576836, 0.02105034701526165, 0.0, 0.0, 0.00390625, 0.0, 0.0418836809694767, 0.0494791679084301, 0.0, 0.02951388992369175, 0.0193142369389534, 0.0651041641831398, 0.02799479104578495, 0.0, 0.02669270895421505, 0.0, 0.0006510416860692203, 0.1499565988779068, 0.0, 0.096571184694767, 0.0, 0.1050347238779068, 0.0685763880610466, 0.0325520820915699, 0.0327690988779068, 0.02300347201526165, 0.8854166865348816, 0.4717881977558136, 0.00824652798473835, 0.9997829794883728, 0.0, 0.8344184160232544, 0.02278645895421505, 0.0, 0.00021701389050576836, 0.0, 0.0405815988779068, 0.0431857630610466, 0.0, 0.0885416641831398, 0.0336371548473835, 0.044921875, 0.011935763992369175, 0.0, 0.0423177070915699, 0.01323784701526165, 0.00021701389050576836, 0.009765625, 0.012803819961845875, 0.01779513992369175, 0.009548611007630825, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.015407986007630825, 0.0822482630610466, 0.0, 0.00021701389050576836, 0.0004340277810115367, 0.0431857630610466, 0.001953125, 0.008897569961845875, 0.1488715261220932, 0.004123263992369175, 0.1456163227558136, 0.0065104165114462376, 0.0232204869389534, 0.2649739682674408, 0.0, 0.398003488779068, 0.2172309011220932, 0.0206163190305233, 0.0, 0.0453559048473835, 0.0514322929084301, 0.0032552082557231188, 0.005642361007630825, 0.0, 0.0, 0.046875, 0.0, 0.0004340277810115367, 0.0, 0.0065104165114462376, 0.0325520820915699, 0.0, 0.01171875, 0.0, 0.337456613779068, 0.1890190988779068, 0.0234375, 0.00021701389050576836, 0.0, 0.0, 0.01996527798473835, 0.0, 0.0004340277810115367, 0.0212673619389534, 0.0, 0.00021701389050576836, 0.0635850727558136, 0.0030381944961845875, 0.03515625, 0.01779513992369175, 0.0394965298473835, 0.0516493059694767, 0.0, 0.5434027910232544, 0.0, 0.0, 0.0034722222480922937, 0.014756944961845875, 0.0342881940305233, 0.0, 0.0243055559694767, 0.012803819961845875, 0.0264756940305233, 0.0245225690305233, 0.00021701389050576836, 0.0251736119389534, 0.0015190972480922937, 0.0525173619389534, 0.0716145858168602, 0.013888888992369175, 0.999131977558136, 0.7204861044883728, 0.9997829794883728, 0.462456613779068, 0.01128472201526165, 0.02864583395421505, 0.0034722222480922937, 0.0694444477558136, 0.037109375, 0.9993489384651184, 0.010850694961845875, 0.0325520820915699, 0.00021701389050576836, 0.0364583320915699, 0.0071614584885537624, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0045572915114462376, 0.0381944440305233, 0.01410590298473835, 0.011935763992369175, 0.0, 0.01822916604578495, 0.0212673619389534, 0.0345052070915699, 0.0023871527519077063, 0.00021701389050576836, 0.0575086809694767, 0.0436197929084301, 0.0481770820915699, 0.0, 0.0, 0.0403645820915699, 0.0004340277810115367, 0.01822916604578495, 0.0, 0.0, 0.0900607630610466, 0.0, 0.0394965298473835, 0.0251736119389534, 0.0, 0.007378472480922937, 0.0, 0.2463107705116272, 0.0, 0.0499131940305233, 0.010416666977107525, 0.0, 0.0, 0.0668402761220932, 0.012369791977107525, 0.014322916977107525, 0.0243055559694767, 0.93359375, 0.0, 0.013020833022892475, 0.0, 0.0, 0.037109375, 0.0, 0.00021701389050576836, 0.0167100690305233, 0.00021701389050576836, 0.0, 0.02973090298473835, 0.0, 0.0, 0.0, 0.02105034701526165, 0.011935763992369175, 0.0067274305038154125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0412326380610466, 0.025390625, 0.0173611119389534, 0.0, 0.0, 0.0008680555620230734, 0.02799479104578495, 0.0844184011220932, 0.0282118059694767, 0.0596788190305233, 0.2248263955116272, 0.086805559694767, 0.0264756940305233, 0.0, 0.00021701389050576836, 0.0, 0.0462239570915699, 0.0, 0.0234375, 0.314019113779068, 0.0303819440305233, 0.0, 0.025390625, 0.0, 0.010850694961845875, 0.1681857705116272, 0.0358072929084301, 0.0, 0.0642361119389534, 0.0648871511220932, 0.0, 0.004123263992369175, 0.0017361111240461469, 0.0, 0.02756076492369175, 0.0047743055038154125, 0.0, 0.02278645895421505, 0.0, 0.0, 0.0017361111240461469, 0.1002604141831398, 0.0783420130610466, 0.00021701389050576836, 0.0, 0.0609809048473835, 0.0023871527519077063, 0.02018229104578495, 0.00021701389050576836, 0.02756076492369175, 0.03059895895421505, 0.013454861007630825, 0.360894113779068, 0.02886284701526165, 0.0345052070915699, 0.0, 0.0186631940305233, 0.0568576380610466, 0.0, 0.00629340298473835, 0.0, 0.0, 0.017578125, 0.0362413190305233, 0.02387152798473835, 0.0, 0.0590277798473835, 0.0, 0.1130642369389534, 0.0, 0.2582465410232544, 0.0412326380610466, 0.02213541604578495, 0.0282118059694767, 0.02690972201526165, 0.0455729179084301, 0.0, 0.00021701389050576836, 0.0078125, 0.0, 0.0, 0.0355902798473835, 0.0355902798473835, 0.0557725690305233, 0.8333333134651184, 0.0, 0.0, 0.014756944961845875, 0.0, 0.0282118059694767, 0.0262586809694767, 0.5095486044883728, 0.0379774309694767, 0.00021701389050576836, 0.0004340277810115367, 0.0329861119389534, 0.0470920130610466, 0.0, 0.0, 0.064453125, 0.0, 0.0, 0.0, 0.0529513880610466, 0.048828125, 0.9993489384651184, 0.004123263992369175, 0.0013020833721384406, 0.0, 0.02973090298473835, 0.0436197929084301, 0.0798611119389534, 0.0173611119389534, 0.0015190972480922937, 0.0590277798473835, 0.00021701389050576836, 0.0635850727558136, 0.00021701389050576836, 0.0440538190305233, 0.005642361007630825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02799479104578495, 0.0, 0.0, 0.01909722201526165, 0.0, 0.0, 0.5115017294883728, 0.0032552082557231188, 0.0045572915114462376, 0.009982638992369175, 0.02690972201526165, 0.2306857705116272, 0.162109375, 0.00021701389050576836, 0.8943142294883728, 0.01909722201526165, 0.0225694440305233, 0.009114583022892475, 0.0013020833721384406, 0.00390625, 0.0173611119389534, 0.0377604179084301, 0.0, 0.00021701389050576836, 0.0, 0.0125868059694767, 0.0, 0.0023871527519077063, 0.0579427070915699, 0.0, 0.0, 0.03059895895421505, 0.072265625, 0.009548611007630825, 0.0, 0.0, 0.0323350690305233, 0.00021701389050576836, 0.001953125, 0.0, 0.0, 0.001953125, 0.5531684160232544, 0.9997829794883728, 0.0, 0.014973958022892475, 0.0319010429084301, 0.02604166604578495, 0.9997829794883728, 0.01779513992369175, 0.0, 0.0, 0.0, 0.0036892362404614687, 0.0514322929084301, 0.999131977558136, 0.0490451380610466, 0.00021701389050576836, 0.0034722222480922937, 0.0, 0.0744357630610466, 0.01974826492369175, 0.0377604179084301, 0.005859375, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.005859375, 0.0052083334885537624, 0.0, 0.1851128488779068, 0.0, 0.00434027798473835, 0.1571180522441864, 0.00824652798473835, 0.0]

 sparsity of   [0.0, 0.01171875, 0.005859375, 0.013671875, 0.0, 0.037109375, 0.001953125, 0.0234375, 0.01171875, 0.015625, 0.0, 0.001953125, 0.998046875, 0.03515625, 0.0, 0.0078125, 0.005859375, 0.037109375, 0.0, 0.0, 0.0, 0.00390625, 0.005859375, 0.0, 0.013671875, 0.923828125, 0.013671875, 0.029296875, 0.005859375, 0.00390625, 0.0, 0.005859375, 0.001953125, 0.0, 0.00390625, 0.001953125, 0.0, 0.01171875, 0.0, 0.177734375, 0.0, 0.009765625, 0.1015625, 0.001953125, 0.017578125, 0.001953125, 0.0, 0.021484375, 0.001953125, 0.005859375, 0.0, 0.01171875, 0.041015625, 0.009765625, 0.015625, 0.0, 0.0, 0.001953125, 0.02734375, 0.021484375, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.080078125, 0.0, 0.029296875, 0.01171875, 0.01953125, 0.048828125, 0.0, 0.0, 0.015625, 0.0234375, 0.0078125, 0.0, 0.01953125, 0.009765625, 0.03125, 0.0, 0.017578125, 0.0, 0.01953125, 0.037109375, 0.01953125, 0.0390625, 0.015625, 0.0078125, 0.0, 0.052734375, 0.00390625, 0.015625, 0.01171875, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.9609375, 0.005859375, 0.0, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.015625, 0.005859375, 0.01171875, 0.03515625, 0.005859375, 0.001953125, 0.009765625, 0.08984375, 0.0, 0.005859375, 0.009765625, 0.0, 0.001953125, 0.025390625, 0.0234375, 0.009765625, 0.013671875, 0.005859375, 0.0, 0.0, 0.0, 0.099609375, 0.0, 0.001953125, 0.0, 0.04296875, 0.001953125, 0.955078125, 0.03515625, 0.001953125, 0.001953125, 0.998046875, 0.009765625, 0.001953125, 0.0, 0.00390625, 0.076171875, 0.0, 0.001953125, 0.005859375, 0.02734375, 0.0, 0.0, 0.001953125, 0.001953125, 0.013671875, 0.0078125, 0.0, 0.0, 0.0, 0.037109375, 0.01171875, 0.0, 0.0, 0.041015625, 0.025390625, 0.0, 0.115234375, 0.0, 0.005859375, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.001953125, 0.00390625, 0.017578125, 0.01953125, 0.0390625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.013671875, 0.021484375, 0.0, 0.05859375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.013671875, 0.9921875, 0.0, 0.017578125, 0.009765625, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0234375, 0.015625, 0.005859375, 0.056640625, 0.0078125, 0.015625, 0.00390625, 0.021484375, 0.005859375, 0.005859375, 0.0, 0.0, 0.001953125, 0.00390625, 0.03125, 0.0, 0.03125, 0.01171875, 0.0, 0.0, 0.001953125, 0.064453125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.154296875, 0.009765625, 0.0, 0.04296875, 0.00390625, 0.001953125, 0.005859375, 0.009765625, 0.0, 0.052734375, 0.0234375, 0.111328125, 0.005859375, 0.0078125, 0.009765625, 0.0078125, 0.044921875, 0.00390625, 0.072265625, 0.0, 0.0, 0.03125, 0.080078125, 0.0078125, 0.0, 0.013671875, 0.015625, 0.044921875, 0.029296875, 0.0234375, 0.1171875, 0.05078125, 0.0234375, 0.0078125, 0.080078125, 0.00390625, 0.0, 0.0, 0.08984375, 0.0078125, 0.00390625, 0.0, 0.0, 0.041015625, 0.0, 0.0078125, 0.01953125, 0.0, 0.0, 0.001953125, 0.0, 0.013671875, 0.00390625, 0.021484375, 0.0, 0.998046875, 0.01953125, 0.0, 0.0, 0.005859375, 0.001953125, 0.021484375, 0.060546875, 0.0, 0.89453125, 0.00390625, 0.001953125, 0.009765625, 0.015625, 0.009765625, 0.0, 0.03125, 0.044921875, 0.0, 0.0, 0.025390625, 0.02734375, 0.001953125, 0.072265625, 0.00390625, 0.06640625, 0.005859375, 0.021484375, 0.0, 0.00390625, 0.0, 0.005859375, 0.068359375, 0.052734375, 0.0, 0.0, 0.021484375, 0.0234375, 0.001953125, 0.0, 0.0, 0.005859375, 0.037109375, 0.00390625, 0.05859375, 0.017578125, 0.05078125, 0.0, 0.025390625, 0.0, 0.01171875, 0.091796875, 0.0546875, 0.04296875, 0.005859375, 0.0, 0.00390625, 0.01171875, 0.005859375, 0.0546875, 0.01171875, 0.037109375, 0.0, 0.013671875, 0.0, 0.009765625, 0.0234375, 0.0, 0.005859375, 0.935546875, 0.01953125, 0.0, 0.037109375, 0.013671875, 0.0, 0.09765625, 0.0078125, 0.00390625, 0.001953125, 0.05859375, 0.00390625, 0.005859375, 0.0, 0.00390625, 0.001953125, 0.02734375, 0.00390625, 0.0234375, 0.0, 0.01953125, 0.001953125, 0.0, 0.04296875, 0.11328125, 0.0, 0.0, 0.0, 0.0234375, 0.033203125, 0.0, 0.0, 0.001953125, 0.013671875, 0.0, 0.001953125, 0.0, 0.0078125, 0.0, 0.009765625, 0.0, 0.005859375, 0.115234375, 0.001953125, 0.0, 0.015625, 0.001953125, 0.0, 0.04296875, 0.001953125, 0.73046875, 0.001953125, 0.01171875, 0.041015625, 0.037109375, 0.0, 0.0, 0.0078125, 0.01953125, 0.037109375, 0.005859375, 0.001953125, 0.021484375, 0.0, 0.009765625, 0.0, 0.021484375, 0.0, 0.00390625, 0.08203125, 0.0, 0.03125, 0.0, 0.033203125, 0.0, 0.0, 0.0078125, 0.00390625, 0.0, 0.62109375, 0.009765625, 0.0, 0.447265625, 0.001953125, 0.0, 0.00390625, 0.01953125, 0.021484375, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.001953125, 0.005859375, 0.03515625, 0.021484375, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.01171875, 0.962890625, 0.0, 0.072265625, 0.0, 0.080078125, 0.0, 0.005859375, 0.0, 0.0, 0.09375, 0.005859375, 0.998046875, 0.0, 0.0, 0.03515625, 0.01953125, 0.03125, 0.001953125, 0.001953125, 0.009765625, 0.03125, 0.037109375, 0.001953125, 0.9609375, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.01953125, 0.001953125, 0.71875, 0.009765625, 0.0, 0.009765625, 0.0, 0.009765625, 0.197265625, 0.0, 0.009765625, 0.0, 0.005859375, 0.041015625, 0.0078125, 0.025390625, 0.19140625, 0.005859375, 0.033203125, 0.005859375, 0.029296875, 0.70703125, 0.03515625, 0.00390625, 0.0546875, 0.00390625, 0.052734375, 0.0, 0.0, 0.0, 0.0, 0.033203125, 0.044921875, 0.0, 0.0, 0.009765625, 0.005859375, 0.001953125, 0.00390625, 0.005859375, 0.017578125, 0.001953125, 0.0, 0.0, 0.0, 0.001953125, 0.001953125, 0.013671875, 0.03515625, 0.0390625, 0.001953125, 0.017578125, 0.05859375, 0.0, 0.0, 0.0390625, 0.00390625, 0.0, 0.015625, 0.08984375, 0.0, 0.017578125, 0.009765625, 0.013671875, 0.001953125, 0.01953125, 0.001953125, 0.0078125, 0.0546875, 0.0, 0.080078125, 0.03125, 0.005859375, 0.0078125, 0.0, 0.060546875, 0.001953125, 0.021484375, 0.01171875, 0.0, 0.001953125, 0.01171875, 0.029296875, 0.01171875, 0.00390625, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.005859375, 0.0, 0.0, 0.0, 0.0234375, 0.005859375, 0.0, 0.1953125, 0.0078125, 0.009765625, 0.013671875, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.87109375, 0.021484375, 0.044921875, 0.021484375, 0.013671875, 0.025390625, 0.00390625, 0.00390625, 0.01171875, 0.779296875, 0.00390625, 0.0, 0.884765625, 0.015625, 0.0, 0.0, 0.0078125, 0.04296875, 0.01171875, 0.0, 0.0, 0.064453125, 0.001953125, 0.07421875, 0.0, 0.001953125, 0.005859375, 0.05078125, 0.025390625, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.029296875, 0.0390625, 0.044921875, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.029296875, 0.0, 0.03515625, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.03125, 0.015625, 0.001953125, 0.01953125, 0.0, 0.0, 0.041015625, 0.013671875, 0.015625, 0.05859375, 0.01953125, 0.01953125, 0.69921875, 0.005859375, 0.064453125, 0.0, 0.037109375, 0.013671875, 0.01171875, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.0, 0.03125, 0.0, 0.005859375, 0.0390625, 0.017578125, 0.009765625, 0.0, 0.1015625, 0.001953125, 0.00390625, 0.0, 0.0390625, 0.009765625, 0.001953125, 0.0, 0.0078125, 0.04296875, 0.001953125, 0.001953125, 0.0, 0.041015625, 0.0078125, 0.033203125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.001953125, 0.0, 0.00390625, 0.021484375, 0.0, 0.001953125, 0.0, 0.005859375, 0.0078125, 0.037109375, 0.001953125, 0.005859375, 0.0, 0.0546875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.00390625, 0.033203125, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.009765625, 0.00390625, 0.046875, 0.0234375, 0.046875, 0.0, 0.0, 0.052734375, 0.009765625, 0.14453125, 0.0, 0.0, 0.00390625, 0.0, 0.953125, 0.001953125, 0.05078125, 0.08203125, 0.0, 0.0, 0.0234375, 0.029296875, 0.029296875, 0.0, 0.0, 0.0, 0.01171875, 0.001953125, 0.013671875, 0.01953125, 0.0, 0.0, 0.033203125, 0.0, 0.0, 0.044921875, 0.033203125, 0.009765625, 0.01953125, 0.0078125, 0.01171875, 0.0, 0.0, 0.033203125, 0.0, 0.015625, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.00390625, 0.0390625, 0.0078125, 0.021484375, 0.0, 0.052734375, 0.00390625, 0.03125, 0.072265625, 0.0, 0.009765625, 0.001953125, 0.0, 0.001953125, 0.77734375, 0.01171875, 0.02734375, 0.013671875, 0.005859375, 0.005859375, 0.0078125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.009765625, 0.041015625, 0.0, 0.849609375, 0.0, 0.001953125, 0.013671875, 0.06640625, 0.0, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.00390625, 0.0078125, 0.0, 0.05078125, 0.0, 0.001953125, 0.0, 0.013671875, 0.93359375, 0.0, 0.04296875, 0.0, 0.03125, 0.005859375, 0.0, 0.00390625, 0.005859375, 0.0390625, 0.0078125, 0.0078125, 0.00390625, 0.0, 0.00390625, 0.0, 0.0, 0.001953125, 0.0, 0.01171875, 0.0078125, 0.001953125, 0.037109375, 0.0, 0.001953125, 0.623046875, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.01953125, 0.0, 0.041015625, 0.0, 0.00390625, 0.02734375, 0.0, 0.0, 0.005859375, 0.0, 0.029296875, 0.0, 0.0078125, 0.0, 0.080078125, 0.041015625, 0.103515625, 0.0, 0.015625, 0.0, 0.6171875, 0.01171875, 0.001953125, 0.0390625, 0.01171875, 0.0, 0.0, 0.05078125, 0.005859375, 0.0, 0.00390625, 0.001953125, 0.0, 0.001953125, 0.99609375, 0.205078125, 0.0, 0.00390625, 0.00390625, 0.01171875, 0.017578125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.001953125, 0.01171875, 0.0, 0.02734375, 0.0, 0.001953125, 0.0, 0.013671875, 0.029296875, 0.00390625, 0.013671875, 0.953125, 0.0, 0.005859375, 0.0078125, 0.001953125, 0.0, 0.017578125, 0.0, 0.00390625, 0.044921875, 0.064453125, 0.00390625, 0.009765625, 0.0, 0.0, 0.0, 0.009765625, 0.017578125, 0.0, 0.193359375, 0.0, 0.013671875, 0.005859375, 0.0546875, 0.0234375, 0.001953125, 0.0, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01171875, 0.001953125, 0.0, 0.0, 0.041015625, 0.001953125, 0.0078125, 0.001953125, 0.021484375, 0.00390625, 0.0546875, 0.05078125, 0.0, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.013671875, 0.013671875, 0.025390625, 0.001953125, 0.03125, 0.005859375, 0.001953125, 0.00390625, 0.001953125, 0.033203125, 0.03125, 0.0, 0.009765625, 0.001953125, 0.041015625, 0.0546875, 0.0, 0.001953125, 0.0, 0.005859375, 0.0, 0.05859375, 0.044921875, 0.009765625, 0.0, 0.00390625, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.013671875, 0.01953125, 0.01171875, 0.0, 0.009765625, 0.0, 0.001953125, 0.0, 0.056640625, 0.01953125, 0.01953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.005859375, 0.005859375, 0.0, 0.095703125, 0.0, 0.0, 0.00390625, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.021484375, 0.009765625, 0.0078125, 0.0, 0.0, 0.00390625, 0.001953125, 0.0, 0.0, 0.0, 0.91015625, 0.0, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.001953125, 0.00390625, 0.0, 0.03515625, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.013671875, 0.0, 0.0078125, 0.0234375, 0.01171875, 0.0, 0.025390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.001953125, 0.0859375, 0.025390625, 0.0, 0.021484375, 0.01953125, 0.0234375, 0.01953125, 0.021484375, 0.025390625, 0.005859375, 0.037109375, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.005859375, 0.0078125, 0.037109375, 0.005859375, 0.0, 0.01953125, 0.998046875, 0.0, 0.046875, 0.01171875, 0.919921875, 0.0, 0.01171875, 0.0, 0.02734375, 0.001953125, 0.0078125, 0.0, 0.00390625, 0.2578125, 0.0, 0.015625, 0.0, 0.8046875, 0.001953125, 0.0, 0.02734375, 0.009765625, 0.0, 0.00390625, 0.03125, 0.01171875, 0.0, 0.0, 0.029296875, 0.005859375, 0.00390625, 0.0546875, 0.015625, 0.0390625, 0.0, 0.029296875, 0.0, 0.0, 0.0, 0.013671875, 0.00390625, 0.01171875, 0.03515625, 0.013671875, 0.0078125, 0.001953125, 0.02734375, 0.0625, 0.0078125, 0.7578125, 0.001953125, 0.001953125, 0.064453125, 0.0078125, 0.830078125, 0.0, 0.01171875, 0.025390625, 0.00390625, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.044921875, 0.0, 0.0, 0.021484375, 0.00390625, 0.0, 0.0078125, 0.091796875, 0.0, 0.0, 0.0078125, 0.083984375, 0.005859375, 0.005859375, 0.0, 0.001953125, 0.001953125, 0.005859375, 0.060546875, 0.0, 0.072265625, 0.0, 0.041015625, 0.01171875, 0.0, 0.013671875, 0.0, 0.009765625, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.009765625, 0.01171875, 0.0, 0.00390625, 0.01171875, 0.013671875, 0.07421875, 0.0, 0.0, 0.001953125, 0.005859375, 0.015625, 0.005859375, 0.0, 0.0, 0.00390625, 0.001953125, 0.005859375, 0.0, 0.013671875, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.158203125, 0.0, 0.095703125, 0.0, 0.0, 0.994140625, 0.03125, 0.0, 0.078125, 0.025390625, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.015625, 0.0, 0.025390625, 0.005859375, 0.0, 0.005859375, 0.01171875, 0.005859375, 0.03515625, 0.001953125, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.005859375, 0.001953125, 0.005859375, 0.017578125, 0.01171875, 0.0, 0.720703125, 0.037109375, 0.025390625, 0.0, 0.0, 0.029296875, 0.302734375, 0.03515625, 0.0, 0.0, 0.0, 0.056640625, 0.001953125, 0.0390625, 0.001953125, 0.0, 0.0078125, 0.0, 0.0, 0.03515625, 0.021484375, 0.0078125, 0.0859375, 0.998046875, 0.0, 0.02734375, 0.0, 0.037109375, 0.0, 0.00390625, 0.0, 0.0, 0.001953125, 0.00390625, 0.001953125, 0.0078125, 0.0, 0.0, 0.0, 0.01171875, 0.001953125, 0.080078125, 0.025390625, 0.0, 0.001953125, 0.0, 0.00390625, 0.935546875, 0.0, 0.0390625, 0.0, 0.04296875, 0.0078125, 0.0, 0.025390625, 0.0, 0.0, 0.0078125, 0.083984375, 0.0, 0.986328125, 0.005859375, 0.02734375, 0.015625, 0.001953125, 0.0, 0.0, 0.0234375, 0.0078125, 0.00390625, 0.173828125, 0.00390625, 0.0, 0.0, 0.017578125, 0.01171875, 0.03515625, 0.009765625, 0.005859375, 0.001953125, 0.0703125, 0.0, 0.0, 0.033203125, 0.02734375, 0.0078125, 0.880859375, 0.0078125, 0.0, 0.0, 0.005859375, 0.0390625, 0.0, 0.0, 0.005859375, 0.033203125, 0.00390625, 0.06640625, 0.0, 0.0, 0.001953125, 0.0, 0.046875, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.0, 0.001953125, 0.001953125, 0.00390625, 0.0, 0.037109375, 0.044921875, 0.0, 0.0, 0.037109375, 0.009765625, 0.00390625, 0.037109375, 0.001953125, 0.0, 0.009765625, 0.041015625, 0.001953125, 0.001953125, 0.0, 0.01953125, 0.0, 0.9921875, 0.0, 0.005859375, 0.0, 0.009765625, 0.0, 0.052734375, 0.00390625, 0.0, 0.005859375, 0.04296875, 0.044921875, 0.0, 0.0, 0.005859375, 0.00390625, 0.076171875, 0.0390625, 0.16796875, 0.0078125, 0.0, 0.056640625, 0.0, 0.0, 0.0, 0.01171875, 0.0, 0.00390625, 0.001953125, 0.0, 0.00390625, 0.005859375, 0.005859375, 0.037109375, 0.01171875, 0.109375, 0.0, 0.009765625, 0.0, 0.00390625, 0.001953125, 0.013671875, 0.0, 0.0, 0.00390625, 0.037109375, 0.017578125, 0.009765625, 0.009765625, 0.00390625, 0.9609375, 0.05078125, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.03125, 0.0, 0.0, 0.005859375, 0.00390625, 0.0, 0.0, 0.04296875, 0.001953125, 0.0, 0.0, 0.052734375, 0.0078125, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.021484375, 0.0078125, 0.0, 0.0, 0.01953125, 0.00390625, 0.005859375, 0.0, 0.0078125, 0.013671875, 0.0078125, 0.0, 0.00390625, 0.02734375, 0.822265625, 0.0, 0.0234375, 0.0, 0.0, 0.02734375, 0.0, 0.03125, 0.005859375, 0.0, 0.02734375, 0.005859375, 0.005859375, 0.0, 0.001953125, 0.0, 0.0, 0.0078125, 0.998046875, 0.017578125, 0.025390625, 0.037109375, 0.0, 0.01953125, 0.0, 0.005859375, 0.0, 0.033203125, 0.01953125, 0.0, 0.0234375, 0.001953125, 0.98046875, 0.265625, 0.025390625, 0.10546875, 0.015625, 0.025390625, 0.005859375, 0.0, 0.05078125, 0.0, 0.0, 0.029296875, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.048828125, 0.9921875, 0.181640625, 0.005859375, 0.037109375, 0.0, 0.01171875, 0.0, 0.00390625, 0.00390625, 0.0078125, 0.0, 0.04296875, 0.009765625, 0.0, 0.009765625, 0.01171875, 0.015625, 0.017578125, 0.998046875, 0.005859375, 0.0, 0.0390625, 0.0, 0.0, 0.00390625, 0.005859375, 0.03125, 0.017578125, 0.0, 0.00390625, 0.001953125, 0.0, 0.0, 0.0078125, 0.0, 0.00390625, 0.0, 0.00390625, 0.00390625, 0.0, 0.01171875, 0.001953125, 0.001953125, 0.00390625, 0.013671875, 0.044921875, 0.025390625, 0.0078125, 0.0, 0.04296875, 0.0, 0.005859375, 0.0, 0.01171875, 0.01171875, 0.998046875, 0.001953125, 0.0, 0.001953125, 0.001953125, 0.0, 0.00390625, 0.001953125, 0.0, 0.001953125, 0.0, 0.0, 0.009765625, 0.0, 0.0, 0.00390625, 0.064453125, 0.025390625, 0.009765625, 0.076171875, 0.001953125, 0.00390625, 0.01953125, 0.025390625, 0.0, 0.017578125, 0.00390625, 0.0, 0.001953125, 0.998046875, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.15234375, 0.009765625, 0.08203125, 0.001953125, 0.001953125, 0.001953125, 0.017578125, 0.017578125, 0.0, 0.115234375, 0.0, 0.0234375, 0.0078125, 0.009765625, 0.0, 0.00390625, 0.0, 0.001953125, 0.01171875, 0.0, 0.013671875, 0.0, 0.001953125, 0.009765625, 0.0, 0.0, 0.0, 0.0078125, 0.021484375, 0.0, 0.001953125, 0.0, 0.044921875, 0.013671875, 0.0, 0.001953125, 0.01171875, 0.0, 0.00390625, 0.048828125, 0.033203125, 0.0234375, 0.0, 0.0, 0.005859375, 0.83984375, 0.005859375, 0.0, 0.005859375, 0.01171875, 0.0, 0.021484375, 0.0, 0.0078125, 0.052734375, 0.013671875, 0.041015625, 0.0, 0.00390625, 0.0, 0.759765625, 0.0, 0.0078125, 0.005859375, 0.00390625, 0.0, 0.017578125, 0.03515625, 0.025390625, 0.0, 0.013671875, 0.046875, 0.013671875, 0.0234375, 0.009765625, 0.0, 0.0, 0.001953125, 0.001953125, 0.05078125, 0.0, 0.0, 0.060546875, 0.0234375, 0.009765625, 0.0078125, 0.00390625, 0.013671875, 0.03515625, 0.048828125, 0.0, 0.017578125, 0.01171875, 0.0, 0.00390625, 0.00390625, 0.0, 0.009765625, 0.0, 0.0, 0.001953125, 0.015625, 0.01953125, 0.005859375, 0.009765625, 0.005859375, 0.00390625, 0.01953125, 0.009765625, 0.001953125, 0.0, 0.0, 0.001953125, 0.005859375, 0.03125, 0.01171875, 0.02734375, 0.017578125, 0.0, 0.92578125, 0.009765625, 0.904296875, 0.0, 0.0, 0.0, 0.0, 0.021484375, 0.001953125, 0.0, 0.0, 0.005859375, 0.0, 0.0, 0.013671875, 0.0, 0.076171875, 0.021484375, 0.001953125, 0.0, 0.015625, 0.0078125, 0.001953125, 0.0078125, 0.02734375, 0.0, 0.0078125, 0.015625, 0.001953125, 0.001953125, 0.0234375, 0.0, 0.0, 0.025390625, 0.0, 0.00390625, 0.9921875, 0.0, 0.0390625, 0.01171875, 0.0, 0.0, 0.005859375, 0.01953125, 0.0, 0.001953125, 0.0234375, 0.009765625, 0.01171875, 0.955078125, 0.025390625, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.9921875, 0.0, 0.02734375, 0.0, 0.06640625, 0.048828125, 0.005859375, 0.009765625, 0.001953125, 0.0, 0.005859375, 0.005859375, 0.0, 0.0, 0.0, 0.021484375, 0.005859375, 0.998046875, 0.0, 0.001953125, 0.015625, 0.0, 0.978515625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.01171875, 0.041015625, 0.005859375, 0.0, 0.01953125, 0.00390625, 0.0, 0.0234375, 0.0, 0.056640625, 0.005859375, 0.02734375, 0.373046875, 0.00390625, 0.0, 0.0, 0.076171875, 0.0, 0.06640625, 0.0, 0.029296875, 0.001953125, 0.044921875, 0.017578125, 0.013671875, 0.013671875, 0.001953125, 0.84765625, 0.0, 0.99609375, 0.0, 0.00390625, 0.001953125, 0.921875, 0.044921875, 0.025390625, 0.037109375, 0.015625, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.05859375, 0.001953125, 0.0, 0.029296875, 0.005859375, 0.044921875, 0.0, 0.033203125, 0.0, 0.01953125, 0.99609375, 0.0, 0.025390625, 0.0, 0.005859375, 0.001953125, 0.84375, 0.0, 0.001953125, 0.0, 0.0, 0.0078125, 0.0, 0.029296875, 0.0, 0.0078125, 0.814453125, 0.0078125, 0.0390625, 0.0, 0.0, 0.0, 0.001953125, 0.009765625, 0.134765625, 0.0078125, 0.0, 0.0, 0.0078125, 0.033203125, 0.005859375, 0.0, 0.59765625, 0.0, 0.095703125, 0.0, 0.03125, 0.0, 0.0, 0.017578125, 0.0, 0.017578125, 0.001953125, 0.0, 0.0078125, 0.005859375, 0.048828125, 0.0625, 0.005859375, 0.03515625, 0.0, 0.009765625, 0.0, 0.005859375, 0.005859375, 0.01953125, 0.08203125, 0.001953125, 0.009765625, 0.015625, 0.0234375, 0.0, 0.033203125, 0.009765625, 0.0, 0.046875, 0.5078125, 0.0, 0.005859375, 0.0, 0.0, 0.01171875, 0.142578125, 0.0, 0.01171875, 0.0, 0.046875, 0.017578125, 0.0, 0.0, 0.0, 0.0, 0.03515625, 0.021484375, 0.0, 0.015625, 0.0, 0.01171875, 0.0234375, 0.0, 0.001953125, 0.0, 0.041015625, 0.0, 0.0, 0.935546875, 0.025390625, 0.01171875, 0.0, 0.0078125, 0.001953125, 0.00390625, 0.015625, 0.0, 0.0, 0.01953125, 0.044921875, 0.099609375, 0.001953125, 0.033203125, 0.0, 0.001953125, 0.00390625, 0.0, 0.005859375, 0.0, 0.0078125, 0.0, 0.0, 0.013671875, 0.00390625, 0.02734375, 0.0, 0.001953125, 0.962890625, 0.0, 0.0, 0.015625, 0.001953125, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005859375, 0.0, 0.02734375, 0.0]

 sparsity of   [0.0654296875, 0.0478515625, 0.57080078125, 0.1064453125, 0.017578125, 0.0576171875, 0.015625, 0.20068359375, 0.021484375, 0.03857421875, 0.02392578125, 0.01708984375, 0.03466796875, 0.07568359375, 0.21630859375, 0.0390625, 0.00390625, 0.7529296875, 0.04833984375, 0.017578125, 0.0361328125, 0.01318359375, 0.0166015625, 0.54638671875, 0.03076171875, 0.05224609375, 0.310546875, 0.01220703125, 0.0078125, 0.02783203125, 0.0283203125, 0.05322265625, 0.13330078125, 0.0654296875, 0.01416015625, 0.04443359375, 0.06103515625, 0.01416015625, 0.02490234375, 0.0791015625, 0.10888671875, 0.0537109375, 0.099609375, 0.02197265625, 0.07861328125, 0.009765625, 0.09765625, 0.0458984375, 0.099609375, 0.025390625, 0.0009765625, 0.10546875, 0.564453125, 0.0244140625, 0.01025390625, 0.02685546875, 0.078125, 0.0, 0.0634765625, 0.03173828125, 0.03662109375, 0.0, 0.2353515625, 0.00048828125, 0.00732421875, 0.0576171875, 0.01171875, 0.0380859375, 0.01953125, 0.0, 0.03076171875, 0.00048828125, 0.11279296875, 0.01513671875, 0.021484375, 0.0458984375, 0.0009765625, 0.8212890625, 0.03857421875, 0.02783203125, 0.0302734375, 0.091796875, 0.052734375, 0.6015625, 0.01611328125, 0.03759765625, 0.10888671875, 0.2646484375, 0.046875, 0.033203125, 0.5048828125, 0.0537109375, 0.0625, 0.13037109375, 0.02880859375, 0.064453125, 0.1513671875, 0.0576171875, 0.0888671875, 0.0126953125, 0.5537109375, 0.02197265625, 0.0859375, 0.04248046875, 0.037109375, 0.06396484375, 0.0283203125, 0.00048828125, 0.04931640625, 0.03271484375, 0.0078125, 0.02099609375, 0.02001953125, 0.02587890625, 0.048828125, 0.0791015625, 0.0810546875, 0.0380859375, 0.00146484375, 0.0107421875, 0.1005859375, 0.037109375, 0.85595703125, 0.01220703125, 0.0732421875, 0.0224609375, 0.0068359375, 0.04345703125, 0.052734375, 0.0546875, 0.025390625, 0.09765625, 0.025390625, 0.0, 0.0, 0.03125, 0.0634765625, 0.0546875, 0.04345703125, 0.025390625, 0.02294921875, 0.0, 0.1884765625, 0.1328125, 0.0078125, 0.0244140625, 0.0, 0.02880859375, 0.17236328125, 0.0341796875, 0.0283203125, 0.01513671875, 0.01611328125, 0.0, 0.02880859375, 0.013671875, 0.041015625, 0.08642578125, 0.0322265625, 0.08056640625, 0.01953125, 0.00048828125, 0.04296875, 0.03125, 0.04248046875, 0.06298828125, 0.02099609375, 0.0615234375, 0.068359375, 0.01611328125, 0.03564453125, 0.0, 0.0947265625, 0.0166015625, 0.02734375, 0.17626953125, 0.02294921875, 0.8369140625, 0.0439453125, 0.0302734375, 0.0, 0.06396484375, 0.57763671875, 0.0634765625, 0.0078125, 0.01318359375, 0.0, 0.0322265625, 0.029296875, 0.095703125, 0.03759765625, 0.08642578125, 0.08740234375, 0.02685546875, 0.0859375, 0.0185546875, 0.009765625, 0.06982421875, 0.09033203125, 0.091796875, 0.025390625, 0.02490234375, 0.046875, 0.060546875, 0.041015625, 0.00927734375, 0.99951171875, 0.2275390625, 0.0, 0.05810546875, 0.01708984375, 0.04150390625, 0.0380859375, 0.0498046875, 0.04052734375, 0.06640625, 0.48876953125, 0.0400390625, 0.01025390625, 0.03369140625, 0.041015625, 0.0234375, 0.03125, 0.03369140625, 0.01953125, 0.0927734375, 0.04736328125, 0.5751953125, 0.03955078125, 0.02978515625, 0.3203125, 0.04345703125, 0.0537109375, 0.1640625, 0.0869140625, 0.08349609375, 0.0146484375, 0.07763671875, 0.12158203125, 0.0, 0.45361328125, 0.00146484375, 0.076171875, 0.0087890625, 0.35693359375, 0.029296875, 0.0009765625, 0.0732421875, 0.04638671875, 0.09814453125, 0.0556640625, 0.08740234375, 0.05224609375, 0.0673828125, 0.037109375, 0.03076171875, 0.00732421875, 0.056640625, 0.03173828125, 0.0341796875, 0.03857421875, 0.013671875, 0.00048828125, 0.0244140625, 0.03564453125, 0.0234375, 0.0751953125, 0.1044921875, 0.0537109375, 0.0478515625, 0.0, 0.02734375, 0.00927734375, 0.00048828125, 0.01513671875, 0.736328125, 0.435546875, 0.9990234375, 0.0166015625, 0.009765625, 0.02392578125, 0.01220703125, 0.0263671875, 0.04736328125, 0.7109375, 0.07470703125, 0.0400390625, 0.07373046875, 0.18505859375, 0.25439453125, 0.07373046875, 0.03759765625, 0.068359375, 0.0263671875, 0.15625, 0.00732421875, 0.23583984375, 0.0, 0.01708984375, 0.0478515625, 0.013671875, 0.03076171875, 0.00830078125, 0.11181640625, 0.76220703125, 0.015625, 0.02099609375, 0.75537109375, 0.0244140625, 0.435546875, 0.04541015625, 0.0283203125, 0.212890625, 0.1279296875, 0.03076171875, 0.03125, 0.0322265625, 0.01953125, 0.02880859375, 0.06640625, 0.07275390625, 0.025390625, 0.05615234375, 0.037109375, 0.3798828125, 0.16357421875, 0.04345703125, 0.04345703125, 0.02978515625, 0.02880859375, 0.03466796875, 0.08251953125, 0.0341796875, 0.04150390625, 0.861328125, 0.0302734375, 0.0263671875, 0.0302734375, 0.0703125, 0.41357421875, 0.056640625, 0.09912109375, 0.10302734375, 0.033203125, 0.02685546875, 0.02685546875, 0.0556640625, 0.029296875, 0.02783203125, 0.0693359375, 0.0615234375, 0.013671875, 0.03076171875, 0.1708984375, 0.0458984375, 0.044921875, 0.998046875, 0.12060546875, 0.041015625, 0.029296875, 0.02001953125, 0.8603515625, 0.029296875, 0.22412109375, 0.00048828125, 0.041015625, 0.02001953125, 0.0419921875, 0.04296875, 0.00927734375, 0.04638671875, 0.04150390625, 0.01416015625, 0.9052734375, 0.0849609375, 0.03564453125, 0.02587890625, 0.1005859375, 0.01171875, 0.06689453125, 0.017578125, 0.32421875, 0.04931640625, 0.06884765625, 0.08203125, 0.041015625, 0.02685546875, 0.2529296875, 0.0048828125, 0.02392578125, 0.00341796875, 0.04052734375, 0.02490234375, 0.04345703125, 0.0068359375, 0.0400390625, 0.05224609375, 0.04931640625, 0.0, 0.0478515625, 0.02490234375, 0.02685546875, 0.0693359375, 0.05029296875, 0.2333984375, 0.08056640625, 0.048828125, 0.0859375, 0.03564453125, 0.04150390625, 0.11328125, 0.04345703125, 0.0205078125, 0.01220703125, 0.0205078125, 0.0, 0.00146484375, 0.02099609375, 0.00048828125, 0.1181640625, 0.0439453125, 0.02294921875, 0.0, 0.0146484375, 0.0458984375, 0.02392578125, 0.021484375, 0.0322265625, 0.0, 0.02587890625, 0.033203125, 0.03076171875, 0.0302734375, 0.02099609375, 0.06396484375, 0.05419921875, 0.0751953125, 0.10009765625, 0.609375, 0.0400390625, 0.02880859375, 0.02734375, 0.03173828125, 0.03125, 0.01806640625, 0.033203125, 0.04443359375, 0.087890625, 0.0322265625, 0.037109375, 0.0205078125, 0.00048828125, 0.0849609375, 0.11669921875, 0.04296875, 0.046875, 0.03564453125, 0.00634765625, 0.02490234375, 0.0439453125, 0.0615234375, 0.0234375, 0.06103515625, 0.02392578125, 0.03564453125, 0.0205078125, 0.0615234375, 0.08544921875, 0.1376953125, 0.37158203125, 0.03076171875, 0.01708984375, 0.03076171875, 0.0390625, 0.07275390625, 0.0263671875, 0.0, 0.02294921875, 0.04931640625, 0.021484375, 0.03271484375, 0.07177734375, 0.04150390625, 0.0380859375, 0.0224609375, 0.0, 0.01025390625, 0.0341796875, 0.0595703125, 0.04150390625, 0.99560546875, 0.0263671875, 0.03662109375, 0.0615234375, 0.42529296875, 0.12060546875, 0.04052734375, 0.03466796875, 0.017578125, 0.044921875, 0.0283203125, 0.017578125, 0.05517578125, 0.0244140625, 0.03515625, 0.05322265625, 0.03759765625, 0.0, 0.052734375, 0.0263671875, 0.04248046875, 0.029296875]

 sparsity of   [0.9798176884651184, 0.008463541977107525, 0.0718315988779068, 0.00434027798473835, 0.02690972201526165, 0.0145399309694767, 0.087890625, 0.0434027798473835, 0.0, 0.073133684694767, 0.012369791977107525, 0.0030381944961845875, 0.02387152798473835, 0.0427517369389534, 0.0314670130610466, 0.5993923544883728, 0.0, 0.1399739533662796, 0.0004340277810115367, 0.02582465298473835, 0.02734375, 0.0, 0.0006510416860692203, 0.0013020833721384406, 0.0078125, 0.0414496548473835, 0.0707465261220932, 0.0859375, 0.02365451492369175, 0.02560763992369175, 0.009765625, 0.075086809694767, 0.02734375, 0.0592447929084301, 0.0427517369389534, 0.0316840298473835, 0.0010850694961845875, 0.0687934011220932, 0.0716145858168602, 0.0674913227558136, 0.6267361044883728, 0.0414496548473835, 0.0540364570915699, 0.01584201492369175, 0.1015625, 0.0698784738779068, 0.0553385429084301, 0.0, 0.0384114570915699, 0.3274739682674408, 0.0290798619389534, 0.0, 0.197048619389534, 0.0549045130610466, 0.056640625, 0.0067274305038154125, 0.0475260429084301, 0.8383246660232544, 0.01714409701526165, 0.9995659589767456, 0.025390625, 0.01714409701526165, 0.0635850727558136, 0.0518663190305233, 0.008897569961845875, 0.017578125, 0.0262586809694767, 0.011067708022892475, 0.0047743055038154125, 0.017578125, 0.01714409701526165, 0.0666232630610466, 0.01953125, 0.0006510416860692203, 0.0, 0.0065104165114462376, 0.0, 0.0549045130610466, 0.216796875, 0.0067274305038154125, 0.0284288190305233, 0.015625, 0.005425347480922937, 0.1625434011220932, 0.010416666977107525, 0.0379774309694767, 0.01019965298473835, 0.0303819440305233, 0.0303819440305233, 0.015625, 0.014973958022892475, 0.1553819477558136, 0.0030381944961845875, 0.00021701389050576836, 0.0381944440305233, 0.009982638992369175, 0.4090711772441864, 0.0, 0.0, 0.0603298619389534, 0.236328125, 0.0, 0.0071614584885537624, 0.01996527798473835, 0.0290798619389534, 0.02886284701526165, 0.082899309694767, 0.001953125, 0.0460069440305233, 0.001953125, 0.02278645895421505, 0.0316840298473835, 0.0, 0.02300347201526165, 0.0453559048473835, 0.01605902798473835, 0.0212673619389534, 0.01974826492369175, 0.0225694440305233, 0.0, 0.1030815988779068, 0.02300347201526165, 0.056640625, 0.01627604104578495, 0.0388454869389534, 0.02408854104578495, 0.013020833022892475, 0.0, 0.0368923619389534, 0.0, 0.00021701389050576836, 0.0737847238779068, 0.253472238779068, 0.0442708320915699, 0.01953125, 0.0015190972480922937, 0.01909722201526165, 0.0740017369389534, 0.0, 0.00021701389050576836, 0.0405815988779068, 0.962890625, 0.0, 0.0490451380610466, 0.0, 0.00390625, 0.02213541604578495, 0.00021701389050576836, 0.0822482630610466, 0.0049913194961845875, 0.091796875, 0.0167100690305233, 0.0243055559694767, 0.0, 0.0442708320915699, 0.0186631940305233, 0.02756076492369175, 0.0889756977558136, 0.0052083334885537624, 0.017578125, 0.02495659701526165, 0.02408854104578495, 0.1399739533662796, 0.014756944961845875, 0.0290798619389534, 0.004123263992369175, 0.02083333395421505, 0.0349392369389534, 0.0, 0.9997829794883728, 0.0232204869389534, 0.0329861119389534, 0.7263454794883728, 0.7797309160232544, 0.0036892362404614687, 0.0017361111240461469, 0.0588107630610466, 0.0284288190305233, 0.0915798619389534, 0.0397135429084301, 0.06640625, 0.2157118022441864, 0.0, 0.008463541977107525, 0.0, 0.0184461809694767, 0.02473958395421505, 0.002170138992369175, 0.0023871527519077063, 0.0, 0.021484375, 0.01974826492369175, 0.0703125, 0.0, 0.056640625, 0.9995659589767456, 0.009548611007630825, 0.0004340277810115367, 0.0364583320915699, 0.0807291641831398, 0.0436197929084301, 0.0334201380610466, 0.0399305559694767, 0.2586805522441864, 0.1740451455116272, 0.0006510416860692203, 0.0421006940305233, 0.0, 0.0036892362404614687, 0.0013020833721384406, 0.0763888880610466, 0.0460069440305233, 0.7259114384651184, 0.02604166604578495, 0.00629340298473835, 0.0698784738779068, 0.00021701389050576836, 0.0355902798473835, 0.069227434694767, 0.0, 0.00021701389050576836, 0.0785590261220932, 0.0047743055038154125, 0.173611119389534, 0.0499131940305233, 0.0544704869389534, 0.00021701389050576836, 0.0, 0.7200520634651184, 0.0562065988779068, 0.00629340298473835, 0.0657552108168602, 0.00021701389050576836, 0.005425347480922937, 0.5590277910232544, 0.02973090298473835, 0.0, 0.014973958022892475, 0.03125, 0.011501736007630825, 0.01909722201526165, 0.0078125, 0.8148871660232544, 0.0412326380610466, 0.0032552082557231188, 0.0431857630610466, 0.013020833022892475, 0.00021701389050576836, 0.0349392369389534, 0.173611119389534, 0.0818142369389534, 0.00629340298473835, 0.0744357630610466, 0.0008680555620230734, 0.967881977558136, 0.0679253488779068, 0.010850694961845875, 0.02191840298473835, 0.0336371548473835, 0.01953125, 0.00390625, 0.01692708395421505, 0.02105034701526165, 0.0392795130610466, 0.0017361111240461469, 0.0603298619389534, 0.0004340277810115367, 0.0616319440305233, 0.0, 0.0327690988779068, 0.002170138992369175, 0.0, 0.0698784738779068, 0.0, 0.0008680555620230734, 0.0618489570915699, 0.02864583395421505, 0.0655381977558136, 0.0, 0.0, 0.0, 0.0553385429084301, 0.0, 0.0314670130610466, 0.0375434048473835, 0.0067274305038154125, 0.0336371548473835, 0.0, 0.0, 0.01822916604578495, 0.0568576380610466, 0.0724826380610466, 0.0234375, 0.02973090298473835, 0.02278645895421505, 0.01714409701526165, 0.02365451492369175, 0.007595486007630825, 0.0145399309694767, 0.0384114570915699, 0.0203993059694767, 0.0481770820915699, 0.014756944961845875, 0.0690104141831398, 0.0030381944961845875, 0.78515625, 0.072265625, 0.0716145858168602, 0.0, 0.0483940988779068, 0.021484375, 0.01019965298473835, 0.02756076492369175, 0.0740017369389534, 0.0026041667442768812, 0.0006510416860692203, 0.0666232630610466, 0.02213541604578495, 0.0301649309694767, 0.6427951455116272, 0.02669270895421505, 0.0, 0.0421006940305233, 0.02213541604578495, 0.0690104141831398, 0.033203125, 0.0032552082557231188, 0.02170138992369175, 0.0342881940305233, 0.0203993059694767, 0.00390625, 0.0342881940305233, 0.8617621660232544, 0.02669270895421505, 0.0466579869389534, 0.0470920130610466, 0.0349392369389534, 0.0685763880610466, 0.1881510466337204, 0.00629340298473835, 0.00824652798473835, 0.0355902798473835, 0.02495659701526165, 0.0, 0.011067708022892475, 0.0611979179084301, 0.0577256940305233, 0.03081597201526165, 0.009548611007630825, 0.1100260391831398, 0.1276041716337204, 0.0347222238779068, 0.0542534738779068, 0.013671875, 0.00021701389050576836, 0.02604166604578495, 0.0392795130610466, 0.0310329869389534, 0.0145399309694767, 0.0, 0.0336371548473835, 0.01692708395421505, 0.0010850694961845875, 0.4446614682674408, 0.0013020833721384406, 0.033203125, 0.008463541977107525, 0.1095920130610466, 0.014756944961845875, 0.0006510416860692203, 0.013888888992369175, 0.0017361111240461469, 0.00021701389050576836, 0.01323784701526165, 0.005425347480922937, 0.0529513880610466, 0.046875, 0.021484375, 0.0785590261220932, 0.037109375, 0.0, 0.0655381977558136, 0.02105034701526165, 0.0065104165114462376, 0.01215277798473835, 0.02582465298473835, 0.01215277798473835, 0.0290798619389534, 0.0023871527519077063, 0.048828125, 0.0455729179084301, 0.0106336809694767, 0.0, 0.01605902798473835, 0.021484375, 0.0729166641831398, 0.02300347201526165, 0.0512152798473835, 0.1475694477558136, 0.06640625, 0.0579427070915699, 0.00021701389050576836, 0.0805121511220932, 0.0013020833721384406, 0.02560763992369175, 0.0026041667442768812, 0.0071614584885537624, 0.0, 0.0004340277810115367, 0.009548611007630825, 0.03515625, 0.0034722222480922937, 0.0013020833721384406, 0.0, 0.0319010429084301, 0.875, 0.1430121511220932, 0.02083333395421505, 0.0023871527519077063, 0.0414496548473835, 0.0366753488779068, 0.0, 0.05859375, 0.1547309011220932, 0.0, 0.015407986007630825, 0.120008684694767, 0.0015190972480922937, 0.4926215410232544, 0.7311198115348816, 0.0, 0.0, 0.0006510416860692203, 0.01888020895421505, 0.0, 0.0125868059694767, 0.1174045130610466, 0.199001744389534, 0.017578125, 0.033203125, 0.00434027798473835, 0.0581597238779068, 0.0, 0.009114583022892475, 0.0381944440305233, 0.010416666977107525, 0.02495659701526165, 0.1117621511220932, 0.0008680555620230734, 0.0, 0.0364583320915699, 0.1028645858168602, 0.0203993059694767, 0.0696614608168602, 0.0423177070915699, 0.0407986119389534, 0.0045572915114462376, 0.9982638955116272, 0.0, 0.095703125, 0.0427517369389534, 0.056640625, 0.0412326380610466, 0.1477864533662796, 0.004123263992369175, 0.00390625, 0.0008680555620230734, 0.0, 0.0008680555620230734, 0.00824652798473835, 0.0399305559694767, 0.0, 0.002170138992369175, 0.0, 0.0052083334885537624, 0.033203125, 0.0045572915114462376, 0.0017361111240461469, 0.01996527798473835, 0.00021701389050576836, 0.0008680555620230734, 0.0314670130610466, 0.7337239384651184, 0.0568576380610466, 0.159939244389534, 0.02973090298473835, 0.0805121511220932, 0.0360243059694767, 0.0232204869389534, 0.596788227558136, 0.00629340298473835, 0.0203993059694767, 0.817491352558136, 0.0405815988779068, 0.3934461772441864, 0.0303819440305233, 0.02018229104578495, 0.0, 0.9995659589767456, 0.1870659738779068, 0.0375434048473835, 0.4422743022441864, 0.0, 0.046875, 0.071180559694767, 0.0145399309694767, 0.001953125, 0.0655381977558136, 0.7747395634651184, 0.0028211805038154125, 0.0004340277810115367, 0.0460069440305233]

 sparsity of   [0.28125, 0.025390625, 0.01953125, 0.994140625, 0.068359375, 0.02734375, 0.177734375, 0.18359375, 0.0234375, 0.703125, 0.083984375, 0.02734375, 0.0546875, 0.01171875, 0.029296875, 0.02734375, 0.029296875, 0.939453125, 0.994140625, 0.048828125, 0.0703125, 0.0234375, 0.0546875, 0.05078125, 0.19921875, 0.0546875, 0.025390625, 0.19921875, 0.091796875, 0.14453125, 0.005859375, 0.748046875, 0.962890625, 0.00390625, 0.16796875, 0.064453125, 0.115234375, 0.1328125, 0.044921875, 0.025390625, 0.01953125, 0.04296875, 0.0234375, 0.162109375, 0.0234375, 0.130859375, 0.12890625, 0.126953125, 0.01953125, 0.056640625, 0.046875, 0.009765625, 0.03125, 0.09765625, 0.01953125, 0.009765625, 0.03125, 0.015625, 0.10546875, 0.01953125, 0.19140625, 0.017578125, 0.0, 0.048828125, 0.044921875, 0.6875, 0.12109375, 0.04296875, 0.85546875, 0.0390625, 0.125, 0.021484375, 0.109375, 0.02734375, 0.09375, 0.091796875, 0.158203125, 0.365234375, 0.10546875, 0.029296875, 0.017578125, 0.01953125, 0.017578125, 0.728515625, 0.28515625, 0.064453125, 0.029296875, 0.69921875, 0.619140625, 0.015625, 0.037109375, 0.01171875, 0.93359375, 0.79296875, 0.0234375, 0.037109375, 0.140625, 0.23046875, 0.0859375, 0.736328125, 0.078125, 0.025390625, 0.494140625, 0.009765625, 0.9140625, 0.591796875, 0.0859375, 0.015625, 0.99609375, 0.099609375, 0.712890625, 0.03125, 0.068359375, 0.880859375, 0.0, 0.033203125, 0.044921875, 0.232421875, 0.998046875, 0.005859375, 0.03515625, 0.033203125, 0.09765625, 0.029296875, 0.99609375, 0.072265625, 0.017578125, 0.03125, 0.017578125, 0.994140625, 0.021484375, 0.009765625, 0.966796875, 0.111328125, 0.03125, 0.0234375, 0.1875, 0.064453125, 0.052734375, 0.912109375, 0.79296875, 0.99609375, 0.90625, 0.095703125, 0.017578125, 0.0390625, 0.109375, 0.078125, 0.994140625, 0.033203125, 0.080078125, 0.203125, 0.05859375, 0.759765625, 0.01171875, 0.09765625, 0.107421875, 0.056640625, 0.0625, 0.0390625, 0.09375, 0.00390625, 0.251953125, 0.130859375, 0.1328125, 0.0078125, 0.005859375, 0.0625, 0.0234375, 0.0703125, 0.009765625, 0.03125, 0.033203125, 0.158203125, 0.0234375, 0.009765625, 0.654296875, 0.744140625, 0.02734375, 0.896484375, 0.009765625, 0.9296875, 0.076171875, 0.01171875, 0.021484375, 0.0234375, 0.013671875, 0.02734375, 0.994140625, 0.701171875, 0.017578125, 0.994140625, 0.09375, 0.90234375, 0.025390625, 0.19140625, 0.005859375, 0.060546875, 0.99609375, 0.208984375, 0.115234375, 0.005859375, 0.0234375, 0.080078125, 0.015625, 0.046875, 0.267578125, 0.732421875, 0.99609375, 0.86328125, 0.822265625, 0.61328125, 0.013671875, 0.060546875, 0.892578125, 0.857421875, 0.04296875, 0.533203125, 0.84375, 0.609375, 0.064453125, 0.013671875, 0.82421875, 0.015625, 0.048828125, 0.12109375, 0.0234375, 0.994140625, 0.025390625, 0.052734375, 0.890625, 0.044921875, 0.017578125, 0.0703125, 0.6015625, 0.05078125, 0.03125, 0.046875, 0.099609375, 0.0234375, 0.015625, 0.072265625, 0.033203125, 0.03515625, 0.115234375, 0.03125, 0.056640625, 0.998046875, 0.0390625, 0.041015625, 0.48046875, 0.73046875, 0.005859375, 0.2890625, 0.0234375, 0.828125, 0.7734375, 0.052734375, 0.18359375, 0.033203125, 0.404296875, 0.0234375, 0.03125, 0.0234375, 0.16796875, 0.251953125, 0.021484375, 0.064453125, 0.017578125, 0.025390625, 0.923828125, 0.076171875, 0.43359375, 0.00390625, 0.07421875, 0.787109375, 0.0234375, 0.67578125, 0.138671875, 0.107421875, 0.0859375, 0.015625, 0.03125, 0.994140625, 0.083984375, 0.99609375, 0.998046875, 0.126953125, 0.02734375, 0.091796875, 0.8984375, 0.0546875, 0.078125, 0.13671875, 0.072265625, 0.095703125, 0.0078125, 0.0234375, 0.0234375, 0.080078125, 0.013671875, 0.9609375, 0.8984375, 0.01171875, 0.01171875, 0.021484375, 0.021484375, 0.0234375, 0.041015625, 0.001953125, 0.0390625, 0.048828125, 0.0546875, 0.08984375, 0.0703125, 0.1015625, 0.025390625, 0.0703125, 0.05078125, 0.01171875, 0.0234375, 0.080078125, 0.03125, 0.041015625, 0.09375, 0.037109375, 0.603515625, 0.041015625, 0.890625, 0.01953125, 0.0625, 0.017578125, 0.03125, 0.009765625, 0.052734375, 0.029296875, 0.033203125, 0.021484375, 0.0703125, 0.013671875, 0.0546875, 0.02734375, 0.162109375, 0.109375, 0.017578125, 0.01171875, 0.072265625, 0.041015625, 0.013671875, 0.912109375, 0.548828125, 0.0703125, 0.001953125, 0.001953125, 0.0078125, 0.03515625, 0.03125, 0.998046875, 0.994140625, 0.029296875, 0.70703125, 0.146484375, 0.0078125, 0.048828125, 0.060546875, 0.015625, 0.013671875, 0.09765625, 0.994140625, 0.01171875, 0.001953125, 0.111328125, 0.044921875, 0.017578125, 0.01171875, 0.041015625, 0.048828125, 0.025390625, 0.02734375, 0.07421875, 0.12890625, 0.001953125, 0.998046875, 0.119140625, 0.921875, 0.0546875, 0.076171875, 0.0625, 0.0859375, 0.28125, 0.0625, 0.15234375, 0.99609375, 0.193359375, 0.09765625, 0.01171875, 0.130859375, 0.02734375, 0.0078125, 0.0234375, 0.0234375, 0.01171875, 0.134765625, 0.017578125, 0.046875, 0.03125, 0.03125, 0.078125, 0.095703125, 0.994140625, 0.0625, 0.060546875, 0.052734375, 0.03125, 0.07421875, 0.078125, 0.015625, 0.748046875, 0.99609375, 0.53125, 0.01953125, 0.04296875, 0.064453125, 0.05078125, 0.9140625, 0.005859375, 0.021484375, 0.017578125, 0.072265625, 0.923828125, 0.04296875, 0.03125, 0.716796875, 0.818359375, 0.11328125, 0.060546875, 0.015625, 0.017578125, 0.005859375, 0.037109375, 0.02734375, 0.009765625, 0.017578125, 0.03515625, 0.01953125, 0.052734375, 0.02734375, 0.607421875, 0.013671875, 0.125, 0.03515625, 0.029296875, 0.115234375, 0.025390625, 0.99609375, 0.884765625, 0.017578125, 0.0390625, 0.111328125, 0.998046875, 0.037109375, 0.029296875, 0.013671875, 0.02734375, 0.0, 0.068359375, 0.037109375, 0.05078125, 0.130859375, 0.04296875, 0.046875, 0.087890625, 0.021484375, 0.169921875, 0.015625, 0.0625, 0.72265625, 0.0, 0.017578125, 0.046875, 0.158203125, 0.025390625, 0.677734375, 0.4453125, 0.017578125, 0.8359375, 0.0390625, 0.0234375, 0.046875, 0.017578125, 0.083984375, 0.041015625, 0.01171875, 0.076171875, 0.3984375, 0.640625, 0.138671875, 0.033203125, 0.09375, 0.712890625, 0.0703125, 0.013671875, 0.021484375, 0.02734375, 0.583984375, 0.982421875, 0.01171875, 0.03515625, 0.033203125, 0.041015625, 0.078125, 0.67578125, 0.025390625, 0.91796875, 0.716796875, 0.015625, 0.033203125, 0.083984375, 0.09375, 0.046875, 0.21875, 0.0625, 0.03125, 0.02734375, 0.02734375, 0.0078125, 0.111328125, 0.126953125, 0.046875, 0.162109375, 0.908203125, 0.046875, 0.150390625, 0.0234375, 0.06640625, 0.173828125, 0.01171875, 0.0078125, 0.001953125, 0.998046875, 0.037109375, 0.779296875, 0.0, 0.076171875, 0.658203125, 0.0625, 0.05078125, 0.103515625, 0.71875, 0.060546875, 0.033203125, 0.06640625, 0.11328125, 0.01953125, 0.0078125, 0.328125, 0.99609375, 0.697265625, 0.10546875, 0.02734375, 0.01171875, 0.029296875, 0.060546875, 0.005859375, 0.0859375, 0.03515625, 0.99609375, 0.00390625, 0.1015625, 0.083984375, 0.013671875, 0.005859375, 0.052734375, 0.033203125, 0.02734375, 0.0234375, 0.0234375, 0.998046875, 0.078125, 0.23828125, 0.044921875, 0.056640625, 0.02734375, 0.0078125, 0.109375, 0.01953125, 0.033203125, 0.048828125, 0.08203125, 0.0, 0.056640625, 0.326171875, 0.041015625, 0.04296875, 0.025390625, 0.02734375, 0.02734375, 0.001953125, 0.326171875, 0.0234375, 0.029296875, 0.208984375, 0.798828125, 0.01953125, 0.60546875, 0.07421875, 0.3203125, 0.095703125, 0.111328125, 0.0078125, 0.037109375, 0.0234375, 0.046875, 0.009765625, 0.078125, 0.044921875, 0.03125, 0.59765625, 0.0078125, 0.99609375, 0.046875, 0.203125, 0.876953125, 0.19921875, 0.23046875, 0.275390625, 0.017578125, 0.87109375, 0.91015625, 0.04296875, 0.01171875, 0.01953125, 0.103515625, 0.015625, 0.05078125, 0.046875, 0.111328125, 0.048828125, 0.12890625, 0.056640625, 0.08203125, 0.1328125, 0.013671875, 0.033203125, 0.0234375, 0.013671875, 0.064453125, 0.123046875, 0.01953125, 0.00390625, 0.0234375, 0.037109375, 0.0078125, 0.046875, 0.015625, 0.12890625, 0.869140625, 0.033203125, 0.73046875, 0.99609375, 0.03125, 0.048828125, 0.43359375, 0.66015625, 0.0078125, 0.275390625, 0.005859375, 0.0234375, 0.0625, 0.048828125, 0.13671875, 0.59375, 0.005859375, 0.03515625, 0.166015625, 0.09375, 0.04296875, 0.05078125, 0.83203125, 0.015625, 0.15625, 0.10546875, 0.041015625, 0.017578125, 0.015625, 0.009765625, 0.083984375, 0.998046875, 0.04296875, 0.0625, 0.02734375, 0.013671875, 0.130859375, 0.08203125, 0.01953125, 0.025390625, 0.02734375, 0.076171875, 0.046875, 0.017578125, 0.03515625, 0.0078125, 0.015625, 0.73046875, 0.03125, 0.12109375, 0.873046875, 0.09375, 0.05078125, 0.130859375, 0.953125, 0.150390625, 0.0546875, 0.072265625, 0.015625, 0.1015625, 0.06640625, 0.82421875, 0.046875, 0.037109375, 0.0390625, 0.0390625, 0.048828125, 0.126953125, 0.017578125, 0.87890625, 0.017578125, 0.994140625, 0.84765625, 0.53515625, 0.169921875, 0.267578125, 0.083984375, 0.021484375, 0.080078125, 0.0234375, 0.0234375, 0.00390625, 0.017578125, 0.0546875, 0.017578125, 0.0546875, 0.015625, 0.1328125, 0.119140625, 0.955078125, 0.025390625, 0.1796875, 0.126953125, 0.015625, 0.09375, 0.0078125, 0.998046875, 0.140625, 0.083984375, 0.84765625, 0.17578125, 0.99609375, 0.12890625, 0.0546875, 0.05078125, 0.935546875, 0.01953125, 0.017578125, 0.041015625, 0.025390625, 0.08984375, 0.833984375, 0.076171875, 0.70703125, 0.271484375, 0.0390625, 0.041015625, 0.005859375, 0.044921875, 0.11328125, 0.01171875, 0.01953125, 0.0078125, 0.994140625, 0.646484375, 0.16015625, 0.05078125, 0.01171875, 0.03125, 0.1484375, 0.05859375, 0.08203125, 0.19921875, 0.078125, 0.99609375, 0.220703125, 0.056640625, 0.025390625, 0.01171875, 0.025390625, 0.044921875, 0.705078125, 0.03515625, 0.673828125, 0.021484375, 0.14453125, 0.046875, 0.341796875, 0.01171875, 0.0703125, 0.90234375, 0.154296875, 0.015625, 0.09375, 0.068359375, 0.0625, 0.0234375, 0.009765625, 0.998046875, 0.072265625, 0.033203125, 0.296875, 0.869140625, 0.05859375, 0.998046875, 0.025390625, 0.01171875, 0.05859375, 0.076171875, 0.13671875, 0.037109375, 0.044921875, 0.03125, 0.1328125, 0.021484375, 0.08203125, 0.033203125, 0.99609375, 0.078125, 0.015625, 0.01953125, 0.05078125, 0.0078125, 0.02734375, 0.021484375, 0.38671875, 0.056640625, 0.041015625, 0.0703125, 0.04296875, 0.03125, 0.99609375, 0.041015625, 0.041015625, 0.0234375, 0.048828125, 0.12890625, 0.83984375, 0.1171875, 0.11328125, 0.021484375, 0.671875, 0.03125, 0.046875, 0.041015625, 0.998046875, 0.015625, 0.994140625, 0.30859375, 0.994140625, 0.20703125, 0.55078125, 0.015625, 0.021484375, 0.03515625, 0.06640625, 0.06640625, 0.044921875, 0.021484375, 0.025390625, 0.05078125, 0.091796875, 0.009765625, 0.09765625, 0.025390625, 0.03125, 0.03125, 0.005859375, 0.02734375, 0.99609375, 0.0234375, 0.07421875, 0.953125, 0.025390625, 0.994140625, 0.267578125, 0.07421875, 0.87109375, 0.009765625, 0.052734375, 0.01953125, 0.060546875, 0.029296875, 0.0546875, 0.041015625, 0.04296875, 0.01953125, 0.107421875, 0.001953125, 0.02734375, 0.04296875, 0.0390625, 0.03515625, 0.998046875, 0.0546875, 0.068359375, 0.14453125, 0.361328125, 0.052734375, 0.021484375, 0.85546875, 0.591796875, 0.044921875, 0.05078125, 0.0703125, 0.1015625, 0.814453125, 0.025390625, 0.15625, 0.05859375, 0.033203125, 0.07421875, 0.091796875, 0.041015625, 0.015625, 0.025390625, 0.8984375, 0.8125, 0.732421875, 0.072265625, 0.486328125, 0.0390625, 0.021484375, 0.00390625, 0.1015625, 0.140625, 0.111328125, 0.025390625, 0.009765625, 0.0703125, 0.17578125, 0.07421875, 0.01953125, 0.3203125, 0.0390625, 0.017578125, 0.1171875, 0.103515625, 0.005859375, 0.11328125, 0.34375, 0.0390625, 0.029296875, 0.77734375, 0.00390625, 0.033203125, 0.998046875, 0.017578125, 0.771484375, 0.005859375, 0.57421875, 0.916015625, 0.912109375, 0.03515625, 0.23828125, 0.05078125, 0.0234375, 0.203125, 0.068359375, 0.3828125, 0.0078125, 0.0703125, 0.994140625, 0.001953125, 0.890625, 0.021484375, 0.0546875, 0.80859375, 0.068359375, 0.236328125, 0.02734375, 0.046875, 0.279296875, 0.01171875, 0.02734375, 0.99609375, 0.798828125, 0.01953125, 0.009765625, 0.009765625, 0.009765625, 0.015625, 0.07421875, 0.8984375, 0.03515625, 0.94921875, 0.04296875, 0.017578125, 0.0859375, 0.53125, 0.056640625, 0.029296875, 0.009765625, 0.10546875, 0.197265625, 0.10546875, 0.109375, 0.181640625, 0.0390625, 0.048828125, 0.041015625, 0.041015625, 0.015625, 0.111328125, 0.095703125, 0.044921875, 0.1015625, 0.998046875, 0.0390625, 0.037109375, 0.01953125, 0.0546875, 0.037109375, 0.15625, 0.998046875, 0.95703125, 0.064453125, 0.017578125, 0.06640625, 0.158203125, 0.0078125, 0.994140625, 0.109375, 0.099609375, 0.025390625, 0.00390625, 0.03125, 0.015625, 0.091796875, 0.0234375, 0.02734375, 0.798828125, 0.017578125, 0.07421875, 0.033203125, 0.0546875, 0.689453125, 0.10546875, 0.05078125, 0.630859375, 0.029296875, 0.90234375, 0.02734375, 0.3828125, 0.037109375, 0.994140625, 0.046875, 0.994140625, 0.033203125, 0.892578125, 0.048828125, 0.03125, 0.99609375, 0.08984375, 0.609375, 0.099609375, 0.658203125, 0.017578125, 0.060546875, 0.994140625, 0.03515625, 0.873046875, 0.08984375, 0.2421875, 0.84375, 0.048828125, 0.904296875, 0.0390625, 0.05078125, 0.06640625, 0.03125, 0.01953125, 0.017578125, 0.068359375, 0.22265625, 0.087890625, 0.01171875, 0.021484375, 0.009765625, 0.037109375, 0.0234375, 0.0703125, 0.048828125, 0.064453125, 0.0078125, 0.015625, 0.017578125, 0.71875, 0.06640625, 0.044921875, 0.271484375, 0.76171875, 0.775390625, 0.041015625, 0.685546875, 0.771484375, 0.044921875, 0.04296875, 0.99609375, 0.033203125, 0.044921875, 0.171875, 0.09765625, 0.033203125, 0.017578125, 0.091796875, 0.009765625, 0.052734375, 0.71484375, 0.220703125, 0.18359375, 0.173828125, 0.08984375, 0.375, 0.056640625, 0.095703125, 0.0546875, 0.048828125, 0.021484375, 0.99609375, 0.767578125, 0.99609375, 0.015625, 0.994140625, 0.14453125, 0.03125, 0.08984375, 0.048828125, 0.01171875, 0.015625, 0.078125, 0.0390625, 0.03515625, 0.05859375, 0.029296875, 0.81640625, 0.115234375, 0.830078125, 0.994140625, 0.8203125, 0.037109375, 0.009765625, 0.033203125, 0.37890625, 0.083984375, 0.021484375, 0.576171875, 0.134765625, 0.134765625, 0.93359375, 0.0234375, 0.400390625, 0.01171875, 0.060546875, 0.033203125, 0.015625, 0.0546875, 0.548828125, 0.009765625, 0.033203125, 0.23828125, 0.0625, 0.0390625, 0.052734375, 0.70703125, 0.021484375, 0.203125, 0.0078125, 0.06640625, 0.03125, 0.99609375, 0.01171875, 0.8359375, 0.025390625, 0.04296875, 0.962890625, 0.3515625, 0.03515625, 0.05078125, 0.73828125, 0.013671875, 0.6875, 0.283203125, 0.15234375, 0.10546875, 0.994140625, 0.078125, 0.025390625, 0.142578125, 0.01953125, 0.0234375, 0.0, 0.3828125, 0.015625, 0.01171875, 0.02734375, 0.029296875, 0.205078125, 0.037109375, 0.0546875, 0.02734375, 0.048828125, 0.171875, 0.046875, 0.173828125, 0.025390625, 0.04296875, 0.77734375, 0.04296875, 0.0234375, 0.03125, 0.931640625, 0.009765625, 0.0546875, 0.12109375, 0.0546875, 0.03125, 0.041015625, 0.869140625, 0.8828125, 0.126953125, 0.994140625, 0.9375, 0.017578125, 0.083984375, 0.0390625, 0.02734375, 0.025390625, 0.0390625, 0.208984375, 0.087890625, 0.0859375, 0.1328125, 0.060546875, 0.0703125, 0.021484375, 0.150390625, 0.0546875, 0.041015625, 0.998046875, 0.6484375, 0.056640625, 0.056640625, 0.041015625, 0.01171875, 0.080078125, 0.01953125, 0.142578125, 0.03125, 0.0078125, 0.013671875, 0.013671875, 0.12109375, 0.009765625, 0.025390625, 0.03125, 0.013671875, 0.041015625, 0.994140625, 0.87890625, 0.908203125, 0.99609375, 0.75, 0.080078125, 0.041015625, 0.025390625, 0.271484375, 0.13671875, 0.2265625, 0.001953125, 0.01953125, 0.275390625, 0.03515625, 0.41796875, 0.052734375, 0.123046875, 0.041015625, 0.146484375, 0.998046875, 0.130859375, 0.009765625, 0.97265625, 0.046875, 0.005859375, 0.033203125, 0.015625, 0.697265625, 0.0625, 0.021484375, 0.037109375, 0.048828125, 0.1015625, 0.005859375, 0.08984375, 0.021484375, 0.06640625, 0.013671875, 0.04296875, 0.013671875, 0.861328125, 0.029296875, 0.01953125, 0.00390625, 0.01171875, 0.884765625, 0.015625, 0.021484375, 0.05078125, 0.99609375, 0.615234375, 0.021484375, 0.037109375, 0.0625, 0.048828125, 0.009765625, 0.0546875, 0.05078125, 0.041015625, 0.994140625, 0.150390625, 0.09375, 0.037109375, 0.13671875, 0.056640625, 0.99609375, 0.08984375, 0.125, 0.228515625, 0.017578125, 0.125, 0.037109375, 0.951171875, 0.03515625, 0.095703125, 0.017578125, 0.01171875, 0.04296875, 0.021484375, 0.037109375, 0.0390625, 0.67578125, 0.734375, 0.07421875, 0.01953125, 0.025390625, 0.05078125, 0.99609375, 0.994140625, 0.017578125, 0.076171875, 0.806640625, 0.08984375, 0.017578125, 0.994140625, 0.044921875, 0.267578125, 0.015625, 0.072265625, 0.017578125, 0.091796875, 0.017578125, 0.083984375, 0.033203125, 0.013671875, 0.03515625, 0.0234375, 0.03125, 0.0078125, 0.03515625, 0.08203125, 0.009765625, 0.015625, 0.03515625, 0.017578125, 0.931640625, 0.017578125, 0.041015625, 0.244140625, 0.03515625, 0.03515625, 0.02734375, 0.2421875, 0.0078125, 0.017578125, 0.037109375, 0.029296875, 0.06640625, 0.20703125, 0.994140625, 0.064453125, 0.236328125, 0.068359375, 0.029296875, 0.142578125, 0.017578125, 0.365234375, 0.169921875, 0.048828125, 0.05078125, 0.09765625, 0.080078125, 0.076171875, 0.017578125, 0.013671875, 0.130859375, 0.009765625, 0.0390625, 0.01171875, 0.90625, 0.033203125, 0.0390625, 0.02734375, 0.048828125, 0.005859375, 0.994140625, 0.0859375, 0.09765625, 0.0078125, 0.021484375, 0.001953125, 0.056640625, 0.0234375, 0.017578125, 0.994140625, 0.28125, 0.009765625, 0.11328125, 0.029296875, 0.037109375, 0.044921875, 0.935546875, 0.12890625, 0.068359375, 0.033203125, 0.998046875, 0.20703125, 0.095703125, 0.056640625, 0.029296875, 0.068359375, 0.037109375, 0.173828125, 0.0546875, 0.095703125, 0.00390625, 0.009765625, 0.00390625, 0.4140625, 0.875, 0.146484375, 0.76953125, 0.083984375, 0.798828125, 0.005859375, 0.8515625, 0.04296875, 0.171875, 0.017578125, 0.041015625, 0.033203125, 0.005859375, 0.029296875, 0.0234375, 0.115234375, 0.06640625, 0.037109375, 0.001953125, 0.021484375, 0.099609375, 0.109375, 0.009765625, 0.04296875, 0.00390625, 0.947265625, 0.021484375, 0.05078125, 0.076171875, 0.03125, 0.6796875, 0.00390625, 0.033203125, 0.126953125, 0.025390625, 0.998046875, 0.994140625, 0.0703125, 0.99609375, 0.048828125, 0.0078125, 0.064453125, 0.0546875, 0.041015625, 0.068359375, 0.056640625, 0.0703125, 0.03125, 0.017578125, 0.015625, 0.09765625, 0.013671875, 0.01171875, 0.146484375, 0.0546875, 0.099609375, 0.3046875, 0.96484375, 0.236328125, 0.12890625, 0.021484375, 0.169921875, 0.029296875, 0.03515625, 0.03125, 0.029296875, 0.0390625, 0.009765625, 0.05078125, 0.01171875, 0.033203125, 0.029296875, 0.02734375, 0.072265625, 0.853515625, 0.021484375, 0.11328125, 0.015625, 0.17578125, 0.03515625, 0.072265625, 0.033203125, 0.083984375, 0.208984375, 0.880859375, 0.017578125, 0.05078125, 0.107421875, 0.076171875, 0.02734375, 0.998046875, 0.2109375, 0.029296875, 0.068359375, 0.009765625, 0.072265625, 0.013671875, 0.072265625, 0.01953125, 0.03125, 0.017578125, 0.10546875, 0.052734375, 0.068359375, 0.0625, 0.017578125, 0.19140625, 0.056640625, 0.2109375, 0.03125, 0.6484375, 0.888671875, 0.033203125, 0.064453125, 0.04296875, 0.052734375, 0.86328125, 0.0078125, 0.083984375, 0.08203125, 0.03515625, 0.03515625, 0.03515625, 0.033203125, 0.03515625, 0.177734375, 0.0859375, 0.931640625, 0.013671875, 0.037109375, 0.0703125, 0.064453125, 0.65234375, 0.048828125, 0.060546875, 0.02734375, 0.068359375, 0.29296875, 0.037109375, 0.091796875, 0.0, 0.03125, 0.060546875, 0.025390625, 0.03515625, 0.845703125, 0.13671875, 0.0, 0.0703125, 0.04296875, 0.0546875, 0.013671875, 0.0390625, 0.0390625, 0.998046875, 0.33203125, 0.041015625, 0.060546875, 0.013671875, 0.955078125, 0.994140625, 0.048828125, 0.208984375, 0.0546875, 0.046875, 0.00390625, 0.0234375, 0.091796875, 0.0390625, 0.091796875, 0.037109375, 0.0078125, 0.072265625, 0.99609375, 0.005859375, 0.189453125, 0.044921875, 0.73046875, 0.203125, 0.029296875, 0.751953125, 0.029296875, 0.0078125, 0.017578125, 0.025390625, 0.037109375, 0.00390625, 0.234375, 0.076171875, 0.021484375, 0.044921875, 0.021484375, 0.009765625, 0.0390625, 0.005859375, 0.02734375, 0.0390625, 0.615234375, 0.056640625, 0.044921875, 0.037109375, 0.900390625, 0.060546875, 0.0078125, 0.076171875, 0.001953125, 0.0546875, 0.796875, 0.251953125, 0.013671875, 0.962890625, 0.111328125, 0.015625, 0.029296875, 0.853515625, 0.095703125, 0.017578125, 0.04296875, 0.294921875, 0.099609375, 0.021484375, 0.0078125, 0.025390625, 0.044921875, 0.896484375, 0.037109375, 0.48828125, 0.802734375, 0.04296875, 0.029296875, 0.00390625, 0.1015625, 0.044921875, 0.37109375, 0.013671875, 0.015625, 0.146484375, 0.05078125, 0.0390625, 0.04296875, 0.95703125, 0.080078125, 0.169921875, 0.01953125, 0.033203125, 0.01171875, 0.013671875, 0.017578125, 0.251953125, 0.228515625, 0.99609375, 0.900390625, 0.068359375, 0.046875, 0.017578125, 0.03125, 0.05078125, 0.083984375, 0.00390625, 0.080078125, 0.83984375, 0.01171875, 0.0234375, 0.5, 0.04296875, 0.0625, 0.890625, 0.0390625, 0.830078125, 0.00390625, 0.169921875, 0.99609375, 0.076171875, 0.900390625, 0.08984375, 0.005859375, 0.017578125, 0.150390625, 0.01953125, 0.833984375, 0.10546875, 0.599609375, 0.939453125, 0.541015625, 0.994140625, 0.21484375, 0.80078125, 0.0625, 0.0625, 0.7890625, 0.025390625, 0.517578125, 0.02734375, 0.02734375, 0.037109375, 0.0, 0.0859375, 0.048828125, 0.716796875, 0.0390625, 0.015625, 0.234375, 0.0234375, 0.017578125, 0.0234375, 0.994140625, 0.0390625, 0.0234375, 0.9609375, 0.01171875, 0.02734375, 0.671875, 0.09375, 0.046875, 0.025390625, 0.11328125, 0.033203125, 0.02734375, 0.14453125, 0.015625, 0.03125, 0.103515625, 0.029296875, 0.103515625, 0.119140625, 0.126953125, 0.060546875, 0.134765625, 0.685546875, 0.0234375, 0.046875, 0.46875, 0.03515625, 0.04296875, 0.021484375, 0.005859375, 0.22265625, 0.0, 0.484375, 0.005859375, 0.033203125, 0.87890625, 0.28515625, 0.4140625, 0.01171875, 0.0078125, 0.505859375, 0.142578125, 0.041015625, 0.994140625, 0.03125, 0.11328125, 0.017578125, 0.02734375, 0.0625, 0.017578125, 0.318359375, 0.0078125, 0.041015625, 0.015625, 0.58984375, 0.03125, 0.201171875, 0.01171875, 0.998046875, 0.05078125, 0.0390625, 0.08984375, 0.474609375, 0.005859375, 0.025390625, 0.03515625, 0.169921875, 0.8828125, 0.0078125, 0.99609375, 0.033203125, 0.01953125, 0.01171875, 0.181640625, 0.091796875, 0.025390625, 0.056640625, 0.046875, 0.064453125, 0.23828125, 0.01953125, 0.015625, 0.05078125, 0.037109375, 0.04296875, 0.01171875, 0.99609375, 0.05078125, 0.025390625, 0.1015625, 0.185546875, 0.197265625, 0.095703125, 0.025390625, 0.859375, 0.951171875, 0.7265625, 0.013671875, 0.107421875, 0.998046875, 0.888671875, 0.0390625, 0.033203125, 0.201171875, 0.017578125, 0.013671875, 0.01171875, 0.08203125, 0.4921875, 0.05078125, 0.857421875, 0.01953125, 0.02734375, 0.03125, 0.078125, 0.048828125, 0.03515625, 0.0, 0.03125, 0.0546875, 0.013671875, 0.14453125, 0.013671875, 0.01953125, 0.037109375, 0.046875, 0.044921875, 0.0390625, 0.0625, 0.0390625, 0.017578125, 0.04296875, 0.99609375, 0.10546875, 0.00390625, 0.1015625, 0.033203125, 0.005859375, 0.046875, 0.109375, 0.029296875, 0.04296875, 0.705078125, 0.078125, 0.0859375, 0.04296875, 0.37109375, 0.0234375, 0.009765625, 0.044921875, 0.021484375, 0.025390625, 0.017578125, 0.046875, 0.068359375, 0.041015625, 0.009765625, 0.0, 0.0703125, 0.060546875, 0.0703125, 0.009765625, 0.025390625, 0.072265625, 0.00390625, 0.001953125, 0.0859375, 0.0625, 0.0625, 0.005859375, 0.9375, 0.0078125, 0.994140625, 0.001953125, 0.99609375, 0.876953125, 0.998046875, 0.21875, 0.298828125, 0.06640625, 0.529296875, 0.03515625, 0.05078125, 0.044921875, 0.216796875, 0.015625, 0.01171875, 0.046875, 0.0703125, 0.02734375, 0.08984375, 0.04296875, 0.044921875, 0.01171875, 0.04296875, 0.07421875, 0.11328125, 0.998046875, 0.0234375, 0.7109375, 0.013671875, 0.083984375, 0.994140625, 0.056640625, 0.994140625, 0.013671875, 0.021484375, 0.70703125, 0.064453125, 0.025390625, 0.08203125, 0.728515625, 0.015625, 0.048828125, 0.072265625, 0.115234375, 0.05078125, 0.044921875, 0.0234375, 0.998046875, 0.0234375, 0.001953125, 0.0078125, 0.99609375, 0.052734375, 0.20703125, 0.025390625, 0.015625, 0.0390625, 0.994140625, 0.05078125, 0.021484375, 0.171875, 0.03125, 0.01953125, 0.009765625, 0.021484375, 0.994140625, 0.033203125, 0.20703125, 0.1796875, 0.0078125, 0.052734375, 0.33984375, 0.03515625, 0.03515625, 0.0078125, 0.89453125, 0.044921875, 0.01171875, 0.099609375, 0.013671875, 0.998046875, 0.017578125, 0.015625, 0.873046875, 0.130859375, 0.064453125, 0.083984375, 0.03515625, 0.08984375, 0.0078125, 0.55078125, 0.998046875, 0.365234375, 0.017578125, 0.0234375, 0.904296875, 0.111328125, 0.021484375, 0.998046875, 0.015625, 0.705078125, 0.01171875, 0.00390625, 0.01171875, 0.095703125, 0.09765625]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 6272724.021584317 (unstructured) 0 (structured)

max weight is  tensor([6.1575e-01, 1.6354e-09, 4.5258e-09, 4.3854e-09, 8.0109e-09, 1.5637e-01,
        9.6758e-03, 9.1212e-05, 6.6704e-01, 1.2195e-01, 3.1452e-09, 2.4871e-01,
        2.2465e-01, 7.4318e-02, 4.5858e-09, 6.2340e-01, 4.6934e-02, 4.5217e-03,
        2.6009e-01, 3.0404e-09, 3.1022e-02, 1.7065e-01, 4.2374e-01, 1.5409e-01,
        1.4702e-01, 3.8670e-09, 1.5159e-01, 4.0322e-02, 1.5671e-01, 1.6354e-09,
        3.1452e-09, 1.1390e-01, 2.6601e-02, 3.2320e-09, 2.6618e-01, 6.4590e-02,
        6.5544e-01, 1.8302e-01, 5.2830e-01, 9.5338e-02, 2.4058e-01, 1.4882e-01,
        3.5192e-01, 1.8847e-09, 4.2072e-01, 3.5701e-01, 3.5015e-02, 2.7520e-09,
        1.8847e-09, 1.1732e-01, 9.1081e-02, 1.4753e-01, 6.4232e-09, 4.7507e-09,
        1.3733e-01, 8.5433e-02, 8.0109e-09, 3.5049e-01, 5.7363e-02, 3.1452e-09,
        1.2274e-01, 7.6243e-10, 1.8381e-09, 1.8847e-09], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([9.2177e-02, 6.3039e-02, 5.2622e-02, 4.6468e-02, 3.2498e-01, 1.1392e-01,
        1.1492e-08, 2.6531e-02, 6.0965e-02, 1.4353e-08, 1.6633e-08, 1.1492e-08,
        1.0077e-01, 9.2328e-02, 1.9249e-01, 1.9704e-01, 1.6674e-01, 4.0912e-02,
        1.1670e-01, 1.4110e-01, 1.5548e-01, 3.4386e-08, 7.9406e-09, 1.2090e-08,
        1.5442e-01, 4.5252e-08, 1.1492e-08, 1.4353e-08, 1.1492e-08, 9.5306e-02,
        3.4394e-04, 2.7248e-02, 1.6116e-01, 6.0377e-02, 7.3251e-09, 1.0919e-01,
        2.4659e-08, 1.2565e-02, 1.8920e-08, 6.6921e-02, 1.7714e-01, 1.5129e-01,
        7.1001e-02, 2.7257e-01, 9.4339e-02, 1.2714e-01, 1.6311e-01, 3.9715e-02,
        3.4386e-08, 3.8007e-01, 5.5959e-08, 2.8092e-03, 1.7180e-01, 4.5252e-08,
        1.8149e-08, 8.6452e-02, 1.3281e-01, 8.3019e-02, 5.5959e-08, 1.4353e-08,
        7.7354e-02, 2.4242e-01, 1.2777e-02, 1.2513e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.2055e-02, 4.8265e-02, 3.8369e-02, 8.8195e-02, 7.9854e-02, 7.5561e-08,
        7.2878e-02, 1.1909e-07, 2.8960e-02, 2.1546e-07, 5.8706e-07, 6.4702e-02,
        8.5818e-02, 1.0591e-07, 8.7438e-02, 4.8219e-02, 1.7294e-02, 3.6563e-02,
        3.6634e-07, 1.0564e-01, 2.5482e-07, 2.2490e-02, 1.0591e-07, 6.9167e-02,
        4.3524e-02, 6.1194e-02, 2.3096e-02, 6.5572e-02, 1.0591e-07, 9.3572e-02,
        1.8720e-02, 1.6539e-07, 7.6647e-02, 1.7913e-07, 7.6103e-02, 4.1225e-02,
        4.7654e-02, 5.7080e-07, 5.7080e-07, 1.2786e-07, 5.7418e-02, 1.0591e-07,
        8.2687e-02, 1.2786e-07, 8.3913e-02, 2.8783e-07, 6.7567e-02, 5.5968e-02,
        5.0248e-02, 1.2786e-07, 1.2786e-07, 4.5986e-02, 7.0846e-04, 3.9548e-02,
        1.0282e-02, 5.6086e-02, 2.8627e-02, 4.4040e-02, 8.1040e-02, 7.5561e-08,
        1.7913e-07, 5.9527e-02, 8.1631e-02, 6.3808e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.4106e-02, 8.3188e-02, 4.5442e-03, 1.1465e-08, 1.3798e-01, 7.9529e-02,
        6.1117e-03, 1.8363e-08, 2.0286e-02, 9.6446e-02, 1.2175e-02, 6.8914e-02,
        3.9802e-02, 8.4071e-08, 1.6397e-01, 3.6114e-02, 1.3638e-08, 1.4817e-02,
        5.2556e-02, 6.2642e-02, 4.1410e-02, 8.0513e-02, 3.2835e-08, 2.1099e-02,
        1.5199e-08, 2.1466e-02, 1.5240e-02, 3.7407e-08, 1.9846e-01, 1.9351e-02,
        4.1866e-02, 7.8005e-02, 2.7703e-01, 5.1808e-02, 1.0704e-01, 1.5760e-01,
        6.5588e-02, 2.6649e-02, 5.7715e-02, 3.7775e-02, 6.0311e-02, 4.8151e-02,
        1.0717e-02, 1.8691e-02, 2.6038e-02, 4.7835e-08, 4.8188e-02, 3.7644e-02,
        1.0300e-01, 1.2717e-08, 8.7328e-02, 2.4713e-08, 6.4333e-08, 1.8630e-02,
        1.1883e-02, 2.4622e-02, 4.8946e-04, 1.2715e-01, 5.1570e-08, 2.7515e-08,
        3.4484e-08, 6.4333e-08, 4.4919e-02, 4.6036e-02, 4.5733e-02, 5.5974e-02,
        1.7022e-02, 1.8589e-02, 2.2477e-08, 5.1575e-02, 2.5552e-02, 8.3382e-02,
        2.2419e-02, 3.6686e-02, 6.5863e-02, 2.8052e-02, 5.4947e-09, 4.1701e-02,
        2.4621e-02, 3.2772e-02, 5.5037e-02, 1.9722e-02, 3.1763e-08, 1.2077e-02,
        9.4045e-09, 3.4762e-08, 2.5608e-08, 9.9907e-02, 7.6141e-03, 7.0063e-02,
        3.4242e-02, 2.4713e-08, 1.6970e-02, 5.8762e-02, 3.4756e-08, 1.8363e-08,
        7.2635e-08, 2.4009e-08, 1.5916e-02, 2.4713e-08, 3.8672e-02, 1.6983e-02,
        2.9671e-02, 3.3263e-08, 1.6009e-08, 4.5052e-02, 2.8558e-08, 1.4796e-01,
        2.4716e-02, 7.8460e-02, 5.6896e-02, 8.0001e-02, 2.0340e-02, 1.8709e-08,
        2.6494e-02, 3.3036e-02, 6.4039e-02, 8.4060e-02, 7.0706e-02, 9.3163e-02,
        4.9518e-02, 6.2075e-02, 3.4806e-02, 1.9416e-02, 4.9981e-02, 6.4096e-03,
        1.3612e-01, 3.9261e-02, 5.1570e-08, 4.6879e-08, 4.5037e-02, 3.5289e-02,
        3.2740e-08, 6.6880e-02, 5.0776e-02, 2.7979e-02, 4.3990e-02, 2.3348e-02,
        9.7624e-02, 1.1976e-01, 9.6280e-02, 4.0763e-02, 8.1266e-02, 1.3566e-02,
        4.6826e-02, 9.1457e-02, 1.4672e-01, 8.6993e-02, 2.1517e-02, 4.7767e-02,
        1.2276e-08, 8.9758e-02, 3.0011e-02, 1.0005e-01, 9.9552e-02, 2.7697e-02,
        5.2884e-02, 1.0542e-02, 5.7567e-02, 4.2182e-02, 3.2908e-02, 1.5986e-08,
        7.3115e-02, 3.5890e-08, 9.6584e-02, 5.8659e-02, 1.2183e-02, 6.9434e-02,
        1.0960e-02, 4.5742e-02, 1.4564e-02, 2.7863e-02, 5.7927e-02, 3.7119e-02,
        1.5314e-02, 3.6579e-08, 4.9500e-08, 2.0606e-02, 2.7388e-02, 7.0509e-02,
        9.0023e-02, 8.6800e-02, 2.9138e-02, 1.2154e-02, 2.7611e-08, 3.7407e-08,
        1.5057e-02, 7.8288e-02, 3.9703e-08, 9.5459e-03, 1.6065e-01, 2.5779e-02,
        7.6929e-02, 1.1281e-02, 1.4535e-01, 1.0091e-01, 2.0523e-02, 3.3375e-02,
        3.5738e-02, 4.1193e-02, 1.3878e-02, 4.1531e-02, 1.4030e-01, 6.0993e-02,
        4.2503e-02, 9.8245e-02, 3.6932e-08, 5.2723e-02, 7.4678e-02, 2.3242e-02,
        2.8673e-02, 4.8635e-02, 7.2279e-02, 6.6096e-02, 1.5166e-02, 7.7347e-02,
        1.3605e-01, 4.6551e-02, 1.7580e-02, 3.4484e-08, 6.1225e-02, 8.4071e-08,
        1.8363e-08, 1.4103e-01, 1.3641e-01, 4.4189e-02, 6.6475e-02, 5.8578e-02,
        5.9782e-02, 2.5769e-02, 2.5138e-02, 2.0895e-02, 6.4701e-02, 1.7911e-02,
        9.3893e-03, 3.6146e-08, 3.2764e-02, 3.5487e-02, 8.7155e-02, 6.1213e-02,
        5.1570e-08, 3.3744e-02, 8.3907e-02, 3.3973e-02, 2.7658e-02, 4.2143e-08,
        1.6823e-01, 2.2844e-02, 3.7711e-02, 1.5405e-02, 1.7612e-01, 6.8116e-02,
        9.3701e-02, 4.6603e-02, 5.3818e-02, 7.8311e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.1668e-02, 3.0709e-02, 9.2289e-02, 1.8595e-08, 2.4894e-02, 1.5608e-01,
        8.8619e-02, 1.5189e-08, 1.4901e-01, 6.7665e-02, 1.3113e-01, 1.1870e-02,
        1.3397e-01, 6.4801e-09, 1.5803e-02, 2.6474e-01, 6.3829e-09, 4.4867e-02,
        5.4155e-02, 3.4905e-02, 5.4610e-02, 2.2255e-02, 3.0660e-08, 1.3530e-01,
        7.5566e-10, 2.2103e-02, 7.3609e-02, 6.5825e-09, 9.4085e-02, 1.5256e-01,
        8.1791e-02, 1.3545e-01, 3.7381e-02, 2.9098e-02, 2.3816e-02, 2.9486e-02,
        1.2426e-01, 2.1036e-01, 8.1317e-02, 7.4149e-09, 7.9516e-03, 1.4062e-01,
        1.1277e-01, 1.5801e-01, 1.6050e-01, 1.1538e-02, 1.5090e-02, 1.8515e-02,
        2.0182e-02, 5.9008e-09, 3.1142e-02, 6.4801e-09, 1.5189e-08, 2.1062e-01,
        5.9130e-02, 7.6268e-02, 1.9414e-02, 1.6184e-02, 1.8595e-08, 8.3146e-09,
        1.4639e-08, 5.6558e-09, 3.1367e-01, 6.3901e-02, 1.6132e-02, 1.0586e-02,
        2.1832e-02, 1.2108e-01, 6.4801e-09, 4.0930e-02, 3.5542e-02, 1.3267e-01,
        1.5149e-01, 1.0676e-01, 9.1486e-03, 5.5349e-02, 1.9857e-09, 1.7131e-02,
        2.7999e-02, 3.3733e-02, 8.9885e-02, 7.1718e-02, 6.3829e-09, 1.3824e-01,
        3.2692e-10, 1.5189e-08, 3.0874e-08, 7.3517e-02, 1.0179e-01, 1.7376e-02,
        1.9856e-02, 1.5831e-08, 7.1080e-02, 1.3860e-02, 5.9468e-09, 1.0816e-08,
        6.3829e-09, 1.3556e-08, 1.5789e-01, 6.3829e-09, 1.5690e-02, 1.4255e-01,
        6.3124e-02, 2.3694e-02, 6.5825e-09, 3.8383e-02, 1.5841e-08, 2.1952e-02,
        1.4104e-01, 2.4266e-02, 9.9947e-03, 1.2341e-02, 6.9924e-02, 5.9008e-09,
        6.1354e-02, 4.8293e-02, 6.7134e-02, 4.6802e-02, 1.1485e-08, 2.0667e-02,
        3.8611e-02, 1.9305e-02, 2.7779e-02, 1.7392e-01, 2.9347e-02, 9.3774e-02,
        2.0690e-02, 4.0158e-02, 6.5825e-09, 1.6440e-08, 8.0483e-02, 2.6560e-02,
        1.5841e-08, 3.3371e-02, 1.2030e-01, 1.3308e-01, 3.9921e-02, 1.6122e-01,
        1.7777e-02, 4.3949e-02, 8.9219e-02, 7.6857e-02, 2.3808e-02, 7.6294e-02,
        5.1709e-02, 2.3513e-02, 4.9719e-02, 2.4611e-02, 7.3089e-02, 5.2604e-02,
        1.5831e-08, 1.2624e-01, 3.6764e-02, 7.1521e-02, 1.0956e-01, 8.8693e-02,
        1.3534e-02, 6.0022e-09, 6.5968e-02, 2.2492e-02, 2.0681e-01, 1.8595e-08,
        2.1812e-02, 2.0825e-08, 1.6657e-02, 2.0688e-02, 2.4098e-01, 1.1673e-02,
        9.3623e-02, 1.6831e-02, 2.7893e-02, 3.6496e-02, 1.6774e-02, 5.3381e-02,
        7.5841e-02, 6.5822e-09, 1.5189e-08, 1.6101e-01, 7.8952e-02, 8.5937e-02,
        2.2704e-02, 1.2131e-02, 4.1564e-02, 1.1342e-01, 4.8208e-09, 2.0678e-08,
        1.2852e-01, 4.1046e-02, 1.0816e-08, 8.3104e-09, 1.1071e-01, 3.9042e-02,
        5.7643e-02, 1.0440e-01, 4.1267e-02, 1.1421e-01, 1.3341e-01, 2.9114e-02,
        1.4146e-01, 1.1533e-01, 1.0179e-01, 3.8565e-02, 1.6576e-02, 4.6007e-02,
        5.1092e-02, 2.3171e-02, 2.6184e-02, 1.5107e-02, 9.7055e-02, 2.4600e-01,
        1.6438e-01, 4.7185e-02, 1.6299e-02, 1.1450e-02, 8.7168e-02, 2.3567e-02,
        1.0242e-01, 5.2480e-02, 1.7050e-02, 1.0816e-08, 1.8952e-02, 6.3829e-09,
        1.0991e-08, 1.3262e-01, 7.4123e-02, 7.8641e-09, 1.7315e-02, 8.3479e-02,
        4.0858e-02, 6.6877e-02, 1.7099e-01, 2.3360e-01, 1.5443e-02, 1.4894e-01,
        4.5420e-02, 6.4800e-09, 5.4283e-02, 8.2592e-03, 1.5029e-02, 1.4456e-01,
        1.5189e-08, 1.1280e-01, 1.0313e-01, 2.5394e-01, 1.4388e-01, 2.0626e-08,
        2.4723e-02, 1.2385e-01, 5.3081e-02, 2.2122e-01, 7.1563e-02, 1.0975e-01,
        4.6821e-02, 6.5135e-02, 5.1258e-02, 1.7386e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.1609e-07, 5.7718e-02, 1.9655e-01, 3.6331e-02, 3.8757e-08, 1.0953e-07,
        1.0953e-07, 1.1358e-01, 8.9447e-08, 4.4611e-02, 2.5981e-02, 1.0361e-01,
        5.1166e-02, 1.4191e-07, 4.0338e-02, 2.2950e-07, 3.0390e-02, 5.7430e-02,
        1.1653e-07, 9.9202e-03, 1.9425e-02, 1.0527e-02, 5.6070e-02, 3.1242e-02,
        4.4491e-02, 1.4429e-02, 4.6779e-02, 4.1666e-02, 1.1653e-07, 4.3301e-02,
        9.3054e-03, 1.0420e-07, 1.1609e-07, 5.2405e-02, 8.1239e-02, 5.1141e-02,
        3.2311e-02, 5.6059e-02, 3.0226e-07, 5.0240e-02, 4.2712e-08, 3.7106e-02,
        3.3328e-02, 2.1633e-02, 3.3611e-02, 6.5038e-02, 6.0814e-02, 8.2361e-02,
        8.0195e-02, 7.6679e-03, 7.2203e-02, 1.1548e-01, 1.1609e-07, 4.4388e-02,
        6.8293e-02, 8.0292e-02, 1.0221e-07, 1.8681e-07, 7.7967e-08, 3.0366e-02,
        3.9742e-02, 1.4191e-07, 1.0917e-01, 6.1944e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.0597e-02, 1.2648e-01, 2.6626e-07, 4.7173e-07, 8.4014e-02, 2.6722e-07,
        7.8105e-02, 5.3515e-02, 9.1393e-02, 7.8199e-02, 9.0457e-02, 6.6530e-02,
        6.2196e-02, 7.3953e-02, 1.0443e-07, 1.2559e-01, 1.0800e-01, 8.4115e-02,
        2.6722e-07, 8.8293e-02, 7.1337e-02, 7.9543e-02, 6.1813e-02, 6.6643e-02,
        7.9019e-08, 1.2119e-01, 8.8996e-08, 4.9966e-02, 2.6722e-07, 8.8996e-08,
        5.8866e-02, 4.4603e-02, 9.5459e-02, 8.2359e-02, 9.6987e-02, 4.0908e-02,
        8.6407e-02, 1.0383e-01, 6.6628e-02, 7.9019e-08, 9.3138e-08, 6.0855e-02,
        9.7763e-02, 6.2807e-08, 1.7463e-07, 1.8616e-07, 7.8967e-08, 7.0404e-02,
        3.8870e-02, 1.3631e-07, 4.5270e-02, 1.0432e-01, 5.2361e-02, 6.2807e-08,
        9.0116e-02, 8.8115e-02, 6.9262e-02, 7.8203e-02, 8.8358e-02, 1.7029e-07,
        8.4267e-08, 1.0211e-01, 1.1002e-01, 7.8967e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.1216e-02, 9.4278e-02, 1.3235e-03, 5.3065e-02, 3.2917e-02, 7.0231e-02,
        6.2524e-03, 3.6422e-08, 2.2013e-02, 3.2751e-02, 1.2268e-02, 1.5566e-02,
        1.4351e-02, 6.3464e-02, 1.2197e-02, 3.9275e-02, 1.1968e-01, 3.6406e-08,
        5.0187e-02, 1.2498e-02, 4.3178e-02, 4.0989e-02, 3.1315e-08, 3.5153e-02,
        2.1972e-08, 1.5308e-02, 1.4900e-02, 1.0576e-01, 1.7949e-02, 2.3265e-02,
        6.8135e-03, 6.5338e-02, 2.5119e-02, 1.5278e-02, 5.5743e-02, 3.4393e-02,
        1.3937e-02, 2.9651e-02, 4.0226e-02, 3.0088e-02, 5.3355e-02, 1.4510e-02,
        2.6322e-02, 2.1237e-02, 2.5752e-02, 2.5917e-08, 1.1956e-01, 1.0866e-01,
        7.7840e-02, 4.5048e-02, 8.4444e-02, 2.9859e-08, 6.8441e-02, 2.4711e-02,
        1.7203e-08, 7.4336e-04, 3.2812e-02, 1.3756e-02, 2.4242e-08, 6.4501e-02,
        1.8616e-08, 7.1231e-02, 4.0803e-02, 3.6359e-02, 1.0995e-02, 4.4210e-02,
        5.3767e-03, 1.4679e-02, 6.7313e-08, 5.3867e-02, 1.5141e-02, 3.8842e-02,
        2.0696e-02, 4.6665e-02, 5.8722e-03, 7.0739e-02, 1.0398e-01, 1.3741e-01,
        5.1264e-04, 1.4973e-02, 2.0776e-02, 1.8305e-02, 6.7313e-08, 4.3107e-03,
        6.8561e-02, 9.5244e-02, 2.0559e-08, 5.8967e-02, 1.2331e-02, 4.6644e-02,
        1.9878e-02, 7.3663e-02, 4.9271e-03, 3.8548e-02, 1.5154e-01, 3.6300e-08,
        3.1564e-08, 1.0328e-01, 5.0168e-02, 2.1107e-08, 2.2732e-02, 2.0767e-02,
        3.5187e-08, 9.5242e-03, 1.1119e-01, 2.8145e-02, 5.9976e-02, 1.9768e-02,
        3.5060e-02, 6.1186e-02, 5.2472e-03, 1.8082e-02, 7.2097e-02, 1.5136e-01,
        1.6589e-02, 2.3314e-08, 6.8796e-02, 5.2274e-02, 3.3597e-02, 4.0975e-02,
        6.6305e-03, 8.0457e-03, 6.7908e-03, 2.2592e-02, 6.8577e-02, 1.0969e-02,
        2.6772e-02, 3.5548e-02, 9.5434e-02, 7.9706e-02, 8.5520e-03, 5.7560e-02,
        1.0663e-01, 4.6125e-02, 8.7556e-03, 4.0866e-02, 5.0441e-02, 1.9833e-02,
        1.5260e-02, 1.5113e-02, 5.4899e-02, 1.2048e-02, 9.5131e-02, 2.5884e-02,
        3.8658e-02, 1.0960e-02, 8.7700e-02, 4.0786e-02, 3.8036e-03, 1.9707e-02,
        7.9310e-02, 6.9289e-02, 1.6811e-02, 1.3639e-02, 2.2464e-02, 3.2784e-02,
        8.3916e-02, 2.1102e-08, 5.9959e-02, 6.8763e-03, 1.8315e-02, 3.3717e-08,
        1.1092e-01, 1.7636e-01, 8.1694e-03, 6.9868e-02, 2.8148e-02, 2.0502e-02,
        2.1851e-02, 5.0644e-02, 2.8024e-02, 1.4816e-02, 4.7396e-02, 7.3795e-02,
        2.8352e-02, 3.3717e-08, 7.1052e-02, 1.4932e-02, 4.3675e-02, 1.8098e-02,
        1.5895e-02, 2.1908e-03, 6.2642e-02, 8.4087e-03, 8.1259e-02, 2.9253e-02,
        1.2920e-02, 5.8106e-02, 1.1578e-01, 2.2148e-04, 5.6310e-02, 4.8891e-02,
        2.8975e-02, 1.2077e-02, 8.5229e-02, 5.4919e-02, 1.6219e-02, 2.0108e-02,
        3.6346e-02, 2.8902e-02, 4.8117e-02, 4.4152e-08, 5.6097e-02, 3.6344e-02,
        2.4775e-08, 9.1113e-02, 1.8297e-02, 6.7317e-02, 4.3711e-02, 3.1538e-02,
        3.1995e-02, 3.7504e-02, 5.6862e-02, 2.2986e-02, 1.2909e-02, 3.5160e-02,
        4.5554e-02, 2.1239e-02, 3.1330e-08, 1.5869e-01, 1.1127e-02, 6.3884e-02,
        9.8063e-09, 5.4449e-02, 8.1503e-02, 1.4879e-08, 3.1987e-02, 1.5863e-01,
        4.2800e-03, 2.1758e-02, 2.6912e-02, 5.5870e-02, 4.0225e-02, 2.4494e-02,
        3.1854e-08, 5.4587e-08, 2.3532e-08, 3.1850e-02, 9.2509e-02, 1.9265e-02,
        6.2011e-02, 2.1628e-02, 5.1141e-02, 3.1212e-02, 1.8484e-02, 3.6422e-08,
        2.5796e-02, 2.9818e-02, 3.6604e-02, 3.3853e-02, 2.2485e-02, 1.0285e-02,
        7.0577e-02, 1.8199e-02, 2.5869e-02, 6.0398e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.8593e-08, 5.5915e-03, 2.2547e-07, 1.2763e-01, 3.2930e-07, 3.2049e-02,
        1.4944e-07, 3.2523e-02, 5.5480e-02, 3.5109e-02, 1.1682e-02, 2.8260e-07,
        2.0483e-07, 5.8067e-02, 2.4901e-07, 9.6948e-03, 2.2087e-07, 7.0391e-03,
        8.1559e-08, 3.2930e-07, 1.6284e-07, 5.4327e-02, 9.8707e-08, 3.1282e-03,
        9.8048e-03, 9.4263e-03, 9.4915e-03, 9.0360e-03, 8.9102e-03, 3.8438e-02,
        1.2359e-01, 6.1013e-02, 1.0605e-07, 5.8453e-02, 1.0494e-02, 2.7523e-02,
        5.0238e-02, 5.3186e-02, 9.2395e-03, 7.9056e-03, 2.8076e-02, 9.5287e-02,
        3.3745e-08, 4.6735e-02, 7.9648e-03, 9.1644e-03, 3.9867e-02, 3.7241e-03,
        9.6599e-03, 3.3713e-02, 2.7681e-02, 2.6251e-07, 1.0605e-07, 2.0529e-02,
        1.2075e-01, 1.0314e-01, 5.6639e-02, 3.6018e-02, 1.7380e-01, 9.7519e-03,
        9.9560e-02, 5.7919e-02, 5.1201e-02, 1.3045e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.9406e-02, 9.2831e-02, 3.6039e-02, 4.5904e-02, 6.6663e-07, 4.4785e-02,
        5.8433e-03, 3.9441e-02, 5.9211e-03, 5.9351e-07, 4.0304e-02, 5.4993e-02,
        3.7121e-02, 2.5734e-03, 4.6362e-02, 4.9154e-02, 4.4917e-02, 3.9164e-02,
        4.6611e-02, 4.9807e-07, 5.3052e-07, 3.9639e-07, 1.2651e-01, 6.3680e-03,
        8.3756e-07, 2.1300e-07, 5.2263e-02, 3.5783e-07, 5.2456e-02, 5.1511e-02,
        3.8815e-07, 2.1300e-07, 4.6744e-02, 6.9674e-03, 7.9341e-03, 7.7712e-03,
        9.1409e-07, 8.8719e-02, 4.9884e-02, 6.4826e-03, 2.5173e-07, 6.7401e-03,
        2.4062e-02, 5.8980e-08, 6.9795e-07, 4.2174e-02, 4.7391e-02, 5.8980e-08,
        1.0341e-01, 1.3514e-01, 5.1347e-07, 3.9197e-02, 4.8110e-02, 3.3746e-07,
        5.9351e-07, 1.0515e-06, 5.5660e-02, 2.3579e-02, 8.7316e-02, 5.6042e-03,
        4.6688e-02, 2.7208e-02, 1.1194e-01, 7.8695e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.1358e-02, 1.6398e-02, 6.3680e-03, 7.7720e-03, 2.3756e-03, 9.7038e-03,
        7.1274e-03, 1.7437e-02, 1.9304e-02, 4.1084e-02, 6.8673e-03, 2.4177e-03,
        1.1397e-02, 1.2344e-01, 8.2601e-03, 2.3766e-02, 2.1799e-01, 1.6931e-02,
        3.8490e-02, 8.1451e-02, 2.8583e-03, 4.3147e-03, 9.0650e-02, 5.2280e-02,
        1.0076e-01, 1.1003e-02, 1.2089e-02, 6.2875e-02, 6.9931e-03, 1.4415e-02,
        3.3619e-02, 1.0516e-02, 1.3358e-02, 5.8054e-02, 4.0516e-02, 8.8264e-03,
        1.0048e-02, 1.7679e-02, 9.4119e-03, 1.7987e-08, 5.8842e-02, 2.2018e-02,
        6.7620e-03, 1.5759e-02, 1.6635e-02, 2.3073e-08, 6.9257e-02, 8.3688e-03,
        5.9732e-02, 1.2493e-01, 8.6958e-03, 2.3282e-02, 3.8695e-03, 7.0449e-03,
        2.4428e-08, 5.6974e-03, 1.6390e-02, 7.4806e-02, 8.7709e-08, 7.7564e-02,
        8.4752e-02, 5.1068e-03, 2.5777e-02, 1.7848e-02, 2.8532e-02, 2.4356e-08,
        8.6271e-03, 8.6486e-03, 5.4226e-02, 3.8379e-02, 1.1428e-02, 9.3710e-03,
        1.0383e-02, 1.4959e-02, 2.1145e-02, 6.1256e-03, 5.1170e-03, 1.3544e-01,
        1.2588e-02, 4.3543e-02, 1.1880e-02, 3.0579e-03, 1.4166e-01, 5.9346e-03,
        6.5567e-03, 1.0090e-02, 7.9508e-02, 1.8793e-08, 5.4656e-03, 1.1000e-02,
        1.6401e-02, 7.5493e-03, 5.8499e-08, 3.9367e-02, 7.6779e-03, 1.2663e-01,
        1.0184e-01, 1.4690e-01, 1.3086e-02, 1.3523e-02, 7.3114e-08, 9.5030e-03,
        6.0736e-03, 3.4251e-08, 5.1749e-03, 4.2075e-08, 4.5020e-03, 1.4003e-01,
        1.2722e-02, 5.5775e-02, 6.0055e-02, 5.1187e-03, 1.4166e-02, 1.2727e-02,
        4.3560e-03, 5.4397e-03, 8.0855e-03, 2.5502e-02, 3.2224e-08, 2.7948e-02,
        4.9478e-08, 5.9245e-02, 6.2770e-02, 1.8495e-02, 1.7705e-02, 7.0008e-03,
        1.0024e-02, 2.1577e-02, 1.0142e-02, 4.9614e-03, 1.0622e-02, 1.3858e-01,
        1.0389e-02, 3.2630e-03, 8.8727e-03, 9.0350e-03, 3.2076e-02, 1.6754e-02,
        1.0168e-01, 4.4992e-02, 4.3674e-02, 6.6685e-03, 6.1021e-03, 1.0427e-02,
        1.4509e-02, 6.8974e-03, 3.6207e-02, 7.0898e-03, 1.6611e-08, 4.7270e-02,
        1.4510e-01, 1.0619e-02, 1.5951e-02, 1.0887e-02, 8.2872e-03, 9.5512e-03,
        7.0825e-03, 3.8783e-08, 3.3623e-02, 4.3798e-02, 1.2876e-02, 8.3326e-02,
        9.9691e-03, 2.3038e-03, 6.1540e-02, 8.8986e-02, 1.2340e-02, 1.4841e-08,
        1.4476e-02, 2.7010e-02, 4.6054e-04, 3.9339e-08, 2.6878e-02, 7.4106e-02,
        1.1316e-02, 8.7177e-02, 1.3045e-02, 1.3763e-02, 2.9836e-03, 3.6303e-02,
        1.2748e-02, 5.7298e-08, 1.5889e-02, 7.7688e-03, 6.5293e-02, 7.6280e-02,
        1.2057e-02, 3.1270e-02, 5.8024e-03, 2.0082e-02, 3.5149e-03, 1.9110e-08,
        2.9645e-02, 1.0987e-02, 4.3462e-02, 9.5109e-03, 1.0620e-02, 2.7773e-02,
        7.3955e-03, 1.2099e-02, 6.6937e-03, 4.2430e-02, 1.1760e-02, 6.3263e-03,
        4.2338e-08, 9.5414e-03, 4.3690e-08, 5.6744e-02, 1.9624e-03, 1.5362e-02,
        1.3003e-02, 5.1218e-03, 2.9276e-08, 3.3516e-02, 1.0573e-02, 2.8675e-02,
        1.2102e-02, 6.7451e-03, 7.3094e-03, 2.1118e-02, 7.9065e-02, 8.3399e-02,
        2.0003e-01, 9.9200e-03, 7.3742e-02, 2.5412e-02, 1.3835e-01, 4.2392e-02,
        5.4438e-08, 1.2698e-02, 1.6176e-02, 1.1488e-02, 8.0279e-08, 1.3979e-02,
        6.6079e-08, 5.5378e-02, 3.8901e-03, 1.7158e-02, 3.6178e-02, 9.3369e-03,
        3.6653e-02, 1.1095e-02, 2.5963e-02, 1.9657e-02, 2.3224e-02, 1.2774e-01,
        5.2552e-02, 1.3088e-02, 1.2263e-02, 1.4594e-02, 4.0695e-08, 1.0422e-02,
        6.7640e-03, 3.2895e-08, 6.0327e-03, 7.3320e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.3429e-07, 1.4791e-01, 2.5016e-01, 4.3710e-07, 1.2522e-01, 2.4488e-02,
        6.8576e-02, 2.1395e-02, 6.0786e-02, 1.1897e-07, 1.3004e-07, 3.3388e-02,
        2.9463e-02, 3.8250e-02, 4.0711e-02, 6.7299e-02, 5.1427e-02, 3.1252e-02,
        7.7365e-03, 6.3859e-02, 1.1611e-01, 1.8637e-07, 5.0333e-02, 1.0507e-02,
        8.1502e-08, 1.1701e-07, 7.2421e-02, 8.2240e-08, 1.8396e-07, 4.7633e-02,
        5.3414e-02, 7.5626e-02, 7.5878e-03, 8.3622e-03, 2.2481e-02, 1.7929e-02,
        7.6990e-02, 1.6925e-03, 5.5872e-08, 1.9824e-02, 8.1502e-08, 2.3429e-07,
        2.8479e-02, 1.4956e-07, 1.3722e-01, 1.0455e-07, 6.0561e-02, 1.1850e-01,
        6.8707e-02, 2.4838e-07, 1.4783e-01, 7.3473e-03, 7.5080e-08, 7.7486e-02,
        3.4523e-07, 8.4597e-02, 4.0924e-02, 4.4342e-02, 3.9203e-03, 2.2528e-07,
        1.2852e-01, 4.6264e-02, 7.2474e-02, 3.2049e-08, 1.2720e-01, 2.8658e-02,
        7.8444e-02, 3.2049e-08, 7.5815e-03, 1.3245e-02, 2.6606e-02, 1.5845e-02,
        1.4545e-01, 5.7951e-02, 2.1202e-07, 1.6219e-01, 9.3169e-08, 6.5367e-08,
        2.4731e-02, 1.1194e-07, 2.2798e-03, 4.0918e-02, 2.2086e-07, 2.5382e-02,
        3.6992e-02, 1.7573e-02, 1.9085e-07, 7.2086e-03, 1.0318e-03, 4.2293e-02,
        4.6536e-03, 7.4827e-03, 3.8669e-02, 3.4383e-02, 7.4487e-08, 3.4206e-07,
        4.4226e-02, 1.5000e-02, 3.9859e-02, 7.1975e-08, 1.3956e-02, 4.8719e-02,
        1.0615e-07, 3.1530e-02, 1.4508e-02, 1.5458e-01, 2.0380e-07, 2.7113e-07,
        7.1128e-02, 2.2268e-07, 3.3574e-02, 2.2139e-02, 2.4417e-01, 4.3950e-07,
        1.1561e-02, 6.2029e-02, 2.2107e-02, 5.9224e-02, 1.1194e-07, 4.8083e-08,
        3.9289e-02, 1.2735e-01, 7.3667e-03, 2.7477e-02, 1.7361e-07, 1.6446e-07,
        2.2040e-07, 2.4332e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.1267e-02, 4.6316e-02, 3.1609e-02, 7.8749e-07, 6.4633e-07, 1.5143e-06,
        1.0727e-01, 2.2467e-02, 3.9977e-02, 8.3370e-07, 6.4750e-07, 2.0551e-06,
        6.0829e-07, 7.0856e-07, 4.0555e-02, 1.5951e-06, 9.2775e-02, 2.1041e-06,
        6.2263e-07, 4.5583e-03, 4.6557e-02, 7.6709e-07, 6.2046e-07, 2.6932e-02,
        7.1518e-07, 1.0760e-06, 5.5843e-07, 6.2263e-07, 2.8912e-02, 3.9647e-07,
        5.2734e-07, 6.8431e-07, 5.5843e-07, 1.4531e-06, 9.9543e-07, 1.6706e-06,
        3.9647e-07, 4.6278e-07, 7.6448e-07, 3.5007e-07, 5.5205e-03, 5.5843e-07,
        4.1135e-07, 5.5843e-07, 4.5152e-02, 9.4112e-08, 3.6025e-07, 1.2036e-06,
        7.4028e-07, 5.9035e-07, 7.5200e-03, 6.8431e-07, 4.1703e-02, 3.8550e-02,
        9.8534e-02, 1.1246e-06, 1.0958e-06, 8.1464e-07, 6.4967e-07, 7.3231e-07,
        8.0283e-07, 9.1883e-07, 1.2036e-06, 4.4594e-02, 5.4058e-07, 3.2991e-02,
        4.8778e-02, 5.1291e-07, 1.9670e-02, 1.3816e-06, 6.9056e-02, 1.0285e-01,
        1.3816e-06, 4.3181e-02, 1.5069e-06, 3.6494e-02, 9.9543e-07, 1.2925e-06,
        9.9543e-07, 7.6448e-07, 8.1817e-07, 4.6891e-02, 6.5914e-07, 3.9076e-07,
        5.2152e-07, 7.7052e-07, 3.6068e-02, 2.1890e-02, 4.6692e-07, 3.0148e-02,
        4.5721e-02, 7.6486e-07, 5.1291e-07, 1.3816e-06, 8.6899e-02, 7.8749e-07,
        8.0756e-07, 4.0832e-02, 1.0760e-06, 1.4159e-06, 7.9846e-07, 4.1647e-02,
        7.3533e-07, 3.9964e-03, 5.1012e-07, 7.9846e-07, 1.1578e-06, 4.4515e-02,
        4.8969e-02, 5.9714e-03, 6.2263e-07, 9.6144e-07, 1.8774e-02, 1.0193e-01,
        1.4869e-06, 1.0841e-02, 5.8561e-07, 5.8484e-07, 3.3314e-02, 4.6691e-07,
        1.1603e-01, 1.2036e-06, 2.5437e-07, 4.2365e-02, 1.6978e-06, 7.2789e-07,
        2.6711e-07, 5.8435e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7719e-06, 9.1126e-03, 1.5347e-08, 3.8966e-08, 6.4018e-08, 3.1802e-02,
        1.6638e-01, 5.3027e-02, 7.6321e-08, 7.5270e-02, 2.7239e-02, 4.6538e-02,
        6.3161e-08, 6.4728e-02, 9.1276e-08, 5.5999e-02, 4.4201e-02, 2.1902e-02,
        6.5975e-02, 9.4747e-08, 7.0934e-02, 1.1719e-02, 7.5739e-08, 4.5676e-08,
        5.8889e-02, 4.1464e-08, 2.9527e-02, 2.5918e-08, 6.2663e-02, 5.3983e-02,
        6.3322e-03, 7.3463e-03, 4.8146e-02, 7.2481e-02, 7.7322e-02, 9.4747e-08,
        4.1668e-02, 3.3254e-02, 6.6513e-02, 6.6286e-02, 1.4082e-02, 7.0009e-02,
        3.2934e-02, 9.2203e-02, 4.4620e-02, 4.4257e-08, 6.4026e-02, 7.3709e-03,
        4.1452e-02, 4.8559e-02, 1.3999e-07, 5.5234e-02, 1.3866e-02, 1.7763e-02,
        1.3644e-01, 7.8817e-02, 9.8596e-08, 8.0940e-02, 6.9076e-02, 7.5932e-02,
        4.6023e-02, 5.8583e-03, 7.4500e-02, 2.5918e-08, 3.6624e-08, 2.1161e-02,
        5.4493e-02, 6.3161e-08, 1.1535e-02, 1.8095e-02, 3.7212e-02, 6.6078e-08,
        6.5562e-08, 6.0600e-02, 6.3303e-08, 6.6078e-08, 1.3999e-07, 4.3684e-08,
        2.8028e-02, 8.6854e-08, 2.5918e-08, 1.0751e-01, 4.3697e-02, 6.6872e-03,
        7.6801e-03, 3.0194e-08, 1.2889e-02, 5.2567e-02, 5.2172e-08, 7.5739e-08,
        7.5090e-02, 2.5432e-02, 4.9422e-03, 1.5976e-07, 1.9973e-03, 5.0333e-02,
        1.0609e-02, 5.8110e-02, 5.3390e-02, 6.6255e-08, 3.6507e-02, 1.0413e-02,
        1.1320e-07, 4.0464e-02, 3.4603e-08, 5.2155e-03, 8.3643e-02, 4.7250e-02,
        2.5459e-02, 5.3726e-02, 9.5967e-02, 6.8164e-03, 9.2735e-03, 1.3158e-02,
        3.0231e-02, 4.4118e-08, 8.5293e-03, 1.3931e-02, 2.3298e-02, 1.0293e-07,
        9.4747e-08, 6.1947e-02, 7.0565e-08, 2.2753e-02, 5.8138e-08, 1.0371e-01,
        1.5976e-07, 6.6921e-02, 7.5739e-08, 2.9921e-08, 2.4435e-02, 1.3790e-07,
        1.7972e-01, 7.6321e-08, 5.2474e-02, 2.4011e-01, 4.5676e-08, 6.5562e-08,
        8.6860e-08, 4.5676e-08, 7.1033e-08, 1.4276e-01, 4.1505e-02, 7.6311e-02,
        5.9509e-02, 1.1242e-07, 9.2967e-03, 1.4078e-02, 8.0205e-03, 2.5754e-02,
        3.2517e-02, 4.7187e-03, 2.9503e-02, 3.4460e-08, 1.5976e-07, 1.5976e-07,
        4.0624e-02, 7.3178e-02, 3.9068e-02, 1.7661e-02, 7.6321e-08, 6.9548e-02,
        6.8684e-02, 4.8843e-02, 1.3999e-07, 1.1522e-07, 5.6477e-08, 6.2025e-08,
        5.0470e-02, 5.3218e-02, 5.6788e-02, 1.7860e-02, 2.1091e-01, 1.5976e-07,
        6.4190e-03, 2.5918e-08, 8.6862e-08, 6.2025e-08, 4.0225e-08, 5.4047e-02,
        4.3168e-02, 2.1152e-02, 1.8519e-07, 1.8021e-01, 1.4099e-07, 3.4461e-08,
        1.8827e-02, 4.9494e-02, 2.5918e-08, 3.8240e-02, 4.2539e-08, 2.3410e-02,
        5.2172e-08, 7.7448e-02, 5.5206e-02, 1.7156e-02, 1.4366e-07, 6.6078e-08,
        6.4464e-08, 6.3161e-08, 5.4938e-02, 1.4206e-07, 2.9658e-07, 5.6477e-08,
        7.2573e-02, 3.0315e-03, 2.5613e-02, 4.8333e-02, 1.1294e-02, 9.0527e-02,
        6.1556e-02, 4.4738e-02, 1.2369e-02, 2.1238e-02, 1.2229e-02, 2.4600e-02,
        5.4273e-08, 9.8651e-08, 5.8853e-02, 1.9308e-02, 7.1615e-02, 6.5562e-08,
        1.8148e-02, 1.0565e-06, 2.9549e-02, 4.0212e-02, 1.4366e-07, 6.4212e-02,
        1.2310e-02, 4.4338e-02, 1.5323e-02, 1.0222e-02, 6.3161e-08, 3.1100e-02,
        1.5828e-02, 5.6543e-02, 1.4258e-07, 8.6854e-08, 3.1423e-08, 3.2010e-08,
        3.5515e-08, 7.6321e-08, 6.1724e-02, 4.2539e-08, 4.8036e-02, 1.1896e-07,
        6.0408e-08, 7.6321e-08, 6.3161e-08, 3.4310e-08, 4.4118e-08, 3.8110e-02,
        4.7526e-02, 9.4894e-03, 4.2013e-03, 7.6204e-02, 2.9490e-08, 4.7177e-02,
        5.7490e-02, 1.7980e-02, 6.6078e-08, 3.1036e-02, 2.8947e-08, 3.4603e-08,
        4.4118e-08, 8.1092e-02, 2.1019e-03, 5.1141e-03, 7.6948e-02, 5.3428e-02,
        4.3704e-02, 3.2774e-02, 3.9761e-02, 4.0391e-08, 2.1071e-08, 4.0095e-03,
        4.9135e-03, 8.6868e-02, 7.1645e-02, 4.5676e-08, 6.0408e-08, 1.7466e-01,
        4.5676e-08, 9.6358e-02, 6.3265e-08, 7.6321e-08, 2.3701e-02, 7.6321e-08,
        4.6404e-02, 7.6321e-08, 4.9498e-02, 9.9881e-09, 6.9131e-02, 9.5006e-02,
        4.0403e-08, 2.0817e-02, 2.5918e-08, 3.7374e-02, 1.2916e-01, 5.5783e-03,
        2.2255e-02, 5.6187e-02, 4.1998e-02, 4.2807e-02, 4.3400e-08, 6.3553e-02,
        7.6321e-08, 2.6692e-02, 7.6321e-08, 5.4625e-03, 1.1462e-01, 6.1528e-02,
        9.6689e-08, 1.9690e-02, 7.5473e-02, 1.3677e-07, 9.5043e-08, 3.9117e-02,
        1.3677e-07, 6.7233e-02, 9.1562e-02, 1.0275e-01, 3.6624e-08, 6.9550e-02,
        9.3485e-03, 6.1361e-02, 2.5918e-08, 1.4256e-07, 8.9180e-02, 7.8350e-02,
        1.5976e-07, 4.3892e-02, 6.7514e-03, 2.3362e-02, 1.0994e-01, 4.5676e-08,
        3.6838e-02, 1.0556e-02, 3.2446e-02, 3.4421e-08, 1.0937e-02, 1.0214e-01,
        5.3900e-02, 4.3503e-02, 8.0467e-03, 5.8750e-02, 1.8018e-02, 5.3283e-03,
        7.6321e-08, 5.1024e-02, 9.4821e-02, 8.1215e-08, 1.5064e-01, 2.1767e-02,
        6.9776e-02, 8.0999e-03, 3.9286e-08, 5.8260e-02, 5.0983e-02, 6.6342e-02,
        3.6209e-03, 5.3199e-02, 6.6078e-08, 9.4747e-08, 5.7226e-02, 6.4132e-02,
        5.2794e-02, 5.7583e-02, 1.4206e-07, 1.4624e-02, 1.3172e-02, 1.4795e-02,
        3.4788e-02, 1.6787e-02, 3.8270e-02, 3.6635e-02, 3.3600e-02, 3.4603e-08,
        3.5355e-02, 8.8456e-03, 1.5976e-07, 7.8133e-02, 5.0757e-02, 6.2621e-02,
        6.6078e-08, 1.4251e-07, 1.1160e-01, 7.2342e-02, 7.6321e-08, 6.3161e-08,
        7.6383e-02, 3.4288e-02, 2.5918e-08, 4.3684e-08, 3.2010e-08, 2.8961e-08,
        3.9198e-08, 1.7676e-02, 3.1254e-02, 2.5918e-08, 1.3582e-02, 2.8961e-08,
        7.0700e-02, 1.3080e-02, 5.3940e-02, 1.4896e-02, 6.5562e-08, 7.2517e-02,
        2.9841e-02, 2.8506e-08, 1.0646e-02, 6.3262e-02, 4.1926e-02, 4.3725e-02,
        3.3922e-08, 1.9005e-02, 1.7936e-02, 1.4206e-07, 2.4869e-02, 4.6934e-02,
        2.0282e-02, 4.4626e-02, 9.7569e-02, 2.3860e-02, 1.5976e-07, 4.9542e-02,
        4.7932e-02, 5.9292e-03, 7.5739e-08, 1.1425e-08, 3.9005e-02, 3.9659e-02,
        9.0906e-08, 6.5562e-08, 1.8481e-08, 1.7295e-08, 7.8940e-03, 4.0096e-02,
        3.4995e-02, 5.1687e-02, 7.7305e-02, 2.5918e-08, 2.8072e-08, 3.3289e-02,
        3.4391e-02, 2.2318e-08, 1.2504e-02, 2.8557e-02, 9.1950e-03, 6.6000e-08,
        8.8330e-03, 7.9711e-03, 1.6303e-02, 1.4258e-07, 9.0082e-08, 2.0101e-08,
        1.7662e-02, 3.8202e-02, 6.8747e-08, 2.0965e-03, 5.0148e-02, 3.1070e-02,
        7.4114e-02, 3.0249e-08, 5.7224e-02, 2.8212e-02, 1.9305e-02, 4.4919e-02,
        2.8961e-08, 4.8211e-03, 5.6477e-08, 5.8613e-02, 6.1857e-02, 1.0497e-02,
        3.0300e-02, 9.4910e-03, 9.9880e-09, 6.2851e-02, 3.7833e-02, 2.1032e-08,
        5.3309e-02, 7.2212e-02, 9.0608e-03, 6.1273e-02, 4.8123e-02, 4.5676e-08,
        5.3869e-03, 1.6200e-08, 2.2027e-02, 3.2010e-08, 6.7415e-03, 2.4935e-08,
        1.4653e-02, 4.1310e-02, 1.0081e-02, 2.3362e-02, 5.6477e-08, 3.2276e-02,
        5.6523e-02, 6.9552e-09, 3.6333e-02, 1.8217e-08, 2.0534e-08, 4.2494e-02,
        1.4257e-07, 2.2700e-02, 5.7362e-02, 1.4574e-01, 4.6555e-02, 6.2941e-03,
        2.3452e-02, 6.1470e-03], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.7721e-08, 5.9918e-02, 7.1284e-08, 1.9736e-07, 2.5127e-02, 5.3583e-02,
        6.2916e-03, 5.5026e-02, 7.3070e-08, 6.3971e-02, 2.3328e-02, 4.2701e-02,
        6.6769e-08, 4.3823e-02, 2.0738e-07, 6.0558e-02, 2.6667e-02, 1.1025e-01,
        7.1881e-02, 1.0723e-07, 3.4325e-02, 7.8032e-02, 1.9935e-07, 1.1800e-07,
        4.7159e-03, 4.3946e-08, 2.8345e-02, 1.0357e-07, 6.9617e-02, 4.6689e-02,
        5.0222e-02, 8.2418e-02, 3.6876e-02, 7.1095e-02, 2.1337e-02, 1.1800e-07,
        1.8645e-02, 3.1270e-02, 7.1314e-02, 5.4568e-02, 5.7961e-02, 2.8921e-02,
        2.8611e-02, 6.6655e-02, 2.4123e-02, 3.8123e-08, 4.0037e-02, 7.7130e-02,
        1.4340e-02, 4.5766e-02, 1.1800e-07, 7.7393e-02, 2.1293e-01, 8.8499e-02,
        2.7485e-02, 4.9091e-02, 5.4580e-02, 3.1590e-02, 4.4730e-02, 4.0163e-02,
        1.1933e-02, 5.8959e-02, 4.7971e-02, 1.1800e-07, 9.7374e-08, 2.0240e-02,
        4.1591e-02, 1.6306e-07, 5.0622e-02, 2.4904e-02, 3.4734e-02, 1.0357e-07,
        4.1168e-08, 5.0673e-02, 3.8123e-08, 3.8123e-08, 2.7246e-08, 3.8123e-08,
        3.7086e-02, 1.1800e-07, 1.1800e-07, 3.1081e-02, 5.9711e-02, 6.9499e-02,
        6.4337e-02, 5.0303e-02, 7.7642e-02, 5.6055e-02, 4.6479e-08, 6.6769e-08,
        5.5274e-02, 8.0279e-08, 8.4381e-02, 3.8123e-08, 3.1973e-03, 4.6106e-02,
        6.7745e-02, 6.0215e-02, 2.2830e-02, 2.8076e-02, 1.0188e-01, 7.1549e-02,
        4.2345e-08, 6.3178e-02, 2.7246e-08, 4.0172e-02, 5.4575e-02, 4.0493e-02,
        6.0943e-02, 4.1561e-02, 5.0200e-02, 7.0460e-02, 7.3126e-08, 6.0285e-03,
        6.5237e-02, 1.9737e-07, 1.0340e-01, 1.0293e-01, 1.8924e-02, 2.9759e-02,
        6.2825e-08, 3.0389e-03, 8.8046e-03, 2.3496e-02, 3.8123e-08, 4.9503e-02,
        8.9597e-08, 4.7185e-02, 1.6306e-07, 1.0357e-07, 6.1508e-08, 4.3946e-08,
        2.7556e-02, 1.5444e-07, 4.1048e-02, 4.2611e-02, 5.0334e-08, 4.3946e-08,
        8.9597e-08, 3.8123e-08, 4.6479e-08, 3.8613e-02, 2.3193e-02, 6.6513e-03,
        4.8889e-03, 3.8123e-08, 9.9687e-02, 3.9162e-02, 6.3470e-02, 8.6167e-02,
        4.1787e-02, 5.4617e-02, 2.2400e-02, 3.8123e-08, 1.5444e-07, 8.3915e-08,
        4.2146e-02, 5.0422e-02, 4.7745e-02, 4.0318e-08, 1.0357e-07, 2.5264e-02,
        4.3899e-02, 5.5998e-02, 1.0357e-07, 3.8123e-08, 8.9430e-08, 8.9597e-08,
        6.9943e-02, 5.5478e-02, 5.2589e-02, 2.6250e-02, 1.3647e-02, 7.4112e-08,
        5.7802e-02, 1.1800e-07, 4.3946e-08, 4.3946e-08, 3.8123e-08, 5.0890e-02,
        4.0374e-02, 7.0189e-02, 4.0010e-02, 4.3340e-02, 1.0357e-07, 4.3946e-08,
        1.0967e-02, 4.3351e-02, 9.8259e-08, 4.5307e-02, 2.3867e-07, 5.1515e-02,
        2.7246e-08, 5.2236e-02, 5.5489e-02, 1.3716e-01, 9.7374e-08, 3.8123e-08,
        3.4204e-02, 4.2912e-08, 5.1794e-02, 1.0357e-07, 1.5317e-06, 3.2647e-08,
        4.1805e-02, 9.4264e-08, 7.5072e-02, 5.2315e-02, 2.0674e-01, 4.0469e-02,
        5.0929e-02, 4.0911e-02, 1.0656e-01, 6.9544e-02, 1.0385e-01, 7.4537e-02,
        8.9597e-08, 4.3946e-08, 4.8702e-02, 2.1825e-02, 4.3263e-02, 1.9737e-07,
        5.5181e-08, 6.6352e-04, 5.2597e-02, 3.7797e-02, 6.6769e-08, 6.5719e-03,
        5.2886e-03, 4.8192e-02, 1.1556e-01, 4.2386e-02, 1.1800e-07, 4.1270e-02,
        1.4546e-02, 1.0852e-02, 2.0748e-07, 3.8123e-08, 6.1732e-02, 3.8123e-08,
        3.8123e-08, 3.8123e-08, 5.4913e-02, 4.3946e-08, 6.7352e-02, 1.5444e-07,
        2.7246e-08, 3.8123e-08, 4.6479e-08, 2.7246e-08, 3.8123e-08, 3.6279e-02,
        3.9770e-03, 1.3738e-01, 5.3911e-02, 7.3793e-02, 6.4072e-08, 3.8230e-02,
        3.3652e-02, 2.2897e-02, 1.6306e-07, 4.3529e-02, 3.8123e-08, 4.3946e-08,
        1.0357e-07, 3.5545e-02, 6.8373e-08, 6.4760e-02, 4.8994e-02, 4.0201e-02,
        3.5321e-02, 7.2925e-02, 4.1463e-02, 2.1514e-07, 1.1800e-07, 8.3136e-02,
        5.8002e-02, 3.6474e-02, 2.7352e-02, 4.3946e-08, 1.6306e-07, 2.6998e-02,
        1.0357e-07, 2.9626e-03, 4.6479e-08, 4.3946e-08, 5.5737e-02, 3.8123e-08,
        8.7116e-02, 1.0357e-07, 4.3808e-02, 7.6982e-08, 3.6761e-02, 4.8194e-02,
        2.7246e-08, 1.2109e-01, 1.0723e-07, 2.8624e-02, 1.0481e-02, 7.7672e-02,
        7.2480e-02, 2.3238e-02, 5.7006e-02, 6.2097e-02, 7.6982e-08, 2.0274e-02,
        3.8123e-08, 4.0390e-02, 2.7246e-08, 3.9299e-02, 4.1213e-02, 3.3181e-02,
        8.0093e-02, 1.4020e-01, 5.3387e-02, 8.9430e-08, 1.1800e-07, 2.8662e-02,
        1.0357e-07, 5.2676e-02, 4.1917e-02, 3.4040e-02, 4.6479e-08, 4.4943e-02,
        9.6837e-02, 4.9018e-02, 7.6982e-08, 2.7246e-08, 7.7670e-02, 4.7208e-02,
        2.7246e-08, 5.4042e-02, 6.6574e-02, 1.8384e-02, 2.3210e-02, 1.1800e-07,
        8.3957e-02, 6.6799e-02, 4.6900e-02, 3.0781e-02, 1.1198e-01, 4.9796e-02,
        1.9294e-02, 2.8994e-02, 6.8736e-02, 8.9349e-03, 9.5548e-02, 6.7678e-02,
        1.9737e-07, 4.1479e-02, 4.7539e-02, 8.9820e-08, 2.6337e-02, 3.5940e-02,
        5.7816e-02, 8.7641e-02, 4.3946e-08, 4.3591e-02, 4.8909e-02, 7.4491e-02,
        3.3500e-02, 8.7174e-02, 7.4112e-08, 7.1284e-08, 2.2643e-03, 2.1887e-02,
        3.6306e-02, 4.6306e-02, 3.8123e-08, 1.3962e-02, 4.2506e-02, 1.0596e-01,
        1.1880e-01, 8.6625e-02, 6.8687e-02, 2.1671e-02, 7.1815e-02, 7.6982e-08,
        4.6994e-02, 8.5524e-02, 2.9322e-07, 4.2714e-02, 9.8970e-02, 5.2766e-02,
        1.0357e-07, 2.0747e-07, 3.2746e-02, 4.5208e-02, 9.7374e-08, 2.5235e-07,
        4.7280e-02, 2.9922e-02, 4.2722e-08, 8.9597e-08, 5.0334e-08, 8.1466e-08,
        4.6478e-08, 1.2585e-02, 6.2121e-02, 1.9737e-07, 1.0648e-02, 2.7246e-08,
        5.9903e-02, 5.9612e-08, 3.6316e-02, 5.7534e-08, 3.0274e-08, 3.8205e-02,
        5.6973e-02, 3.8123e-08, 1.0208e-01, 5.5306e-03, 8.0902e-03, 5.0398e-02,
        6.6842e-02, 2.2521e-02, 1.3541e-02, 1.9737e-07, 1.2715e-02, 3.4485e-02,
        1.9902e-02, 2.5872e-02, 4.5654e-02, 8.7915e-02, 1.1800e-07, 5.5776e-02,
        5.6831e-02, 4.9250e-03, 4.3946e-08, 6.4072e-08, 4.7621e-02, 9.1165e-08,
        1.0357e-07, 5.7449e-08, 2.7246e-08, 3.4731e-02, 7.1359e-02, 3.6036e-03,
        3.4356e-02, 4.5970e-02, 6.7728e-03, 9.8259e-08, 3.8123e-08, 6.8655e-02,
        6.2602e-03, 2.2801e-02, 7.0806e-08, 2.7023e-02, 6.7670e-02, 6.4072e-08,
        7.3291e-02, 9.0334e-02, 5.9531e-02, 7.4112e-08, 4.6609e-02, 5.4925e-02,
        1.1481e-01, 5.2259e-02, 3.8123e-08, 7.5720e-08, 8.1208e-02, 4.8386e-02,
        4.4997e-02, 2.0828e-07, 2.9570e-02, 9.7219e-03, 1.0449e-01, 1.4725e-02,
        4.6479e-08, 9.4632e-02, 4.3946e-08, 4.2906e-02, 6.5009e-02, 2.3892e-02,
        5.5844e-02, 5.0782e-08, 4.2722e-08, 4.5665e-03, 4.0083e-02, 7.1268e-08,
        7.0160e-02, 4.6983e-02, 4.7769e-02, 7.7705e-02, 1.7538e-02, 3.8123e-08,
        4.7021e-02, 2.7246e-08, 4.8186e-02, 1.5444e-07, 6.6350e-02, 3.8123e-08,
        7.6319e-03, 4.8990e-02, 1.3299e-01, 5.9690e-02, 1.1800e-07, 1.4896e-02,
        3.1761e-02, 3.8123e-08, 6.9014e-02, 4.3946e-08, 9.8259e-08, 7.9950e-02,
        2.7246e-08, 3.1368e-02, 5.9280e-02, 7.4215e-03, 4.6875e-02, 4.0463e-03,
        3.2887e-02, 6.3817e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.4953e-07, 6.6762e-02, 2.1021e-07, 3.2174e-07, 2.9579e-07, 1.2046e-07,
        1.2344e-05, 1.1327e-07, 7.5612e-02, 7.0253e-02, 6.7133e-08, 3.4054e-02,
        1.3391e-07, 1.2037e-07, 3.0497e-07, 8.4300e-02, 4.3075e-02, 6.7133e-08,
        4.1466e-02, 2.0845e-07, 2.3562e-07, 2.7612e-07, 4.7516e-07, 1.1790e-07,
        5.4969e-07, 6.7763e-02, 1.0078e-07, 5.5570e-02, 4.8756e-02, 2.4455e-02,
        1.0076e-07, 3.3865e-07, 1.2046e-07, 1.2419e-07, 5.4671e-02, 5.6198e-02,
        4.1124e-02, 1.0790e-07, 2.8090e-02, 5.0273e-02, 1.5994e-07, 4.8815e-07,
        3.4772e-02, 7.4920e-02, 4.4794e-02, 1.4525e-07, 5.1166e-02, 1.5877e-07,
        8.8825e-02, 3.2174e-07, 6.7133e-08, 5.1416e-02, 1.0790e-07, 1.2046e-07,
        2.5260e-02, 1.1996e-01, 1.2839e-07, 3.0699e-02, 2.6036e-02, 3.9415e-02,
        6.2018e-08, 6.0080e-02, 2.9579e-07, 1.0790e-07, 5.6991e-02, 3.6665e-02,
        2.7612e-07, 1.9518e-07, 5.3575e-02, 3.3865e-07, 2.0845e-07, 1.3392e-07,
        1.0790e-07, 2.2107e-07, 1.0076e-07, 1.5789e-07, 1.0076e-07, 1.2419e-07,
        3.2174e-07, 8.9204e-08, 3.3757e-02, 1.0790e-07, 4.5926e-02, 1.3777e-07,
        2.2107e-07, 1.3777e-07, 3.0569e-02, 1.0790e-07, 1.0606e-02, 9.9813e-03,
        5.6896e-02, 7.2307e-02, 1.1269e-05, 6.6760e-08, 6.8898e-02, 1.7583e-07,
        4.7516e-07, 1.0076e-07, 6.7133e-08, 2.9102e-07, 6.7133e-08, 4.7350e-08,
        6.4301e-07, 1.4527e-07, 1.3391e-07, 1.0790e-07, 1.5571e-01, 1.3777e-07,
        2.9579e-07, 3.4517e-02, 1.2419e-07, 9.7014e-08, 9.6386e-03, 1.0790e-07,
        2.0845e-07, 1.0790e-07, 1.0790e-07, 3.3832e-07, 5.8947e-02, 1.4599e-07,
        7.0741e-08, 1.0790e-07, 5.4969e-07, 2.9579e-07, 2.9579e-07, 1.2046e-07,
        4.6195e-02, 5.0094e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.6253e-07, 8.3868e-07, 3.1519e-07, 9.8193e-02, 1.5277e-02, 7.9459e-07,
        1.4158e-01, 5.1334e-07, 1.6955e-07, 1.6955e-07, 5.9145e-02, 4.6341e-07,
        4.6341e-07, 9.2622e-07, 1.9092e-07, 1.3086e-01, 2.6493e-07, 7.9459e-07,
        1.0100e-01, 9.2622e-07, 6.6883e-07, 7.8646e-02, 1.1482e-01, 8.9306e-02,
        3.9581e-07, 8.3376e-02, 3.9581e-07, 9.9757e-02, 1.3250e-01, 1.9091e-07,
        3.1519e-07, 1.0624e-02, 1.6253e-07, 5.0727e-07, 4.6341e-07, 6.6883e-07,
        3.9581e-07, 9.2622e-07, 1.6955e-07, 1.3962e-01, 1.0910e-01, 5.3277e-07,
        3.9581e-07, 1.9054e-07, 4.6341e-07, 5.1334e-07, 3.1519e-07, 1.2857e-01,
        2.6493e-07, 3.1519e-07, 1.0772e-07, 7.0735e-02, 1.9054e-07, 2.6493e-07,
        1.9091e-07, 8.8865e-02, 3.1519e-07, 5.9024e-07, 9.2622e-07, 3.1519e-07,
        4.3977e-07, 1.9092e-07, 3.1519e-07, 4.6341e-07, 1.9091e-07, 1.6955e-07,
        8.4372e-02, 6.6298e-02, 1.4312e-01, 7.2268e-02, 2.4800e-02, 7.9943e-02,
        1.9054e-07, 4.6341e-07, 1.0638e-01, 5.0727e-07, 2.4706e-02, 9.8456e-02,
        1.0772e-07, 1.3085e-01, 3.1519e-07, 3.1519e-07, 3.9581e-07, 3.9581e-07,
        4.6341e-07, 8.4679e-02, 5.3548e-02, 2.2323e-07, 1.9092e-07, 7.3400e-02,
        2.1367e-07, 8.0278e-02, 1.9092e-07, 5.0017e-07, 1.9092e-07, 1.1049e-01,
        1.6955e-07, 3.9581e-07, 4.6341e-07, 8.8085e-02, 4.1373e-07, 9.2622e-07,
        5.8789e-03, 1.4002e-01, 9.8133e-02, 4.8644e-02, 1.9091e-07, 1.9092e-07,
        4.3977e-07, 3.9581e-07, 3.9581e-07, 4.6341e-07, 4.6341e-07, 1.0595e-06,
        6.6883e-07, 5.4879e-03, 4.1373e-07, 1.0244e-01, 1.9092e-07, 1.6955e-07,
        3.1519e-07, 9.2622e-07, 9.2622e-07, 3.1519e-07, 7.9459e-07, 1.9092e-07,
        6.6883e-07, 3.9581e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.3771e-08, 2.5627e-02, 2.3995e-03, 3.5851e-08, 7.3574e-08, 1.7447e-02,
        1.7675e-01, 4.7417e-02, 5.9816e-08, 6.1322e-03, 7.2822e-08, 2.0323e-02,
        9.7432e-08, 2.1086e-02, 2.4725e-08, 4.0564e-08, 6.7077e-08, 4.3664e-02,
        9.3513e-03, 4.9113e-08, 5.2440e-08, 9.5114e-03, 5.2277e-02, 7.3875e-08,
        6.3751e-03, 1.0588e-07, 4.9732e-02, 7.5662e-08, 1.2799e-02, 4.6001e-02,
        6.5041e-02, 4.0477e-02, 3.2837e-02, 3.0099e-02, 1.1458e-02, 8.9341e-02,
        5.6304e-02, 9.5244e-08, 1.2537e-02, 5.5134e-08, 1.2025e-02, 5.8400e-08,
        1.5962e-01, 9.3183e-03, 6.8323e-08, 4.9113e-08, 4.5978e-02, 5.1490e-03,
        1.6718e-07, 3.3044e-02, 7.8808e-08, 3.5815e-02, 1.4434e-01, 8.3491e-03,
        1.0711e-02, 9.7045e-03, 9.0265e-08, 6.6989e-03, 1.4393e-01, 7.9198e-02,
        3.2057e-03, 1.1870e-01, 9.7600e-03, 4.6547e-02, 4.3108e-08, 6.9644e-08,
        5.2769e-02, 2.8969e-08, 8.4387e-02, 2.9210e-02, 4.0696e-02, 3.8219e-02,
        7.8808e-08, 1.0678e-02, 3.5321e-02, 1.0163e-07, 5.5742e-02, 7.8808e-08,
        3.6625e-08, 5.9816e-08, 3.9484e-08, 8.6017e-02, 2.6441e-02, 8.7384e-03,
        2.6313e-02, 8.0909e-08, 2.1238e-02, 1.6632e-02, 1.9210e-02, 2.3790e-08,
        1.1714e-02, 1.9101e-07, 7.8183e-02, 5.2037e-08, 6.7152e-08, 1.0576e-02,
        9.7709e-03, 1.7792e-02, 4.9499e-02, 4.5175e-02, 3.8513e-02, 1.0645e-02,
        5.3932e-08, 3.5571e-02, 2.8712e-08, 7.0325e-02, 1.6837e-02, 7.4698e-03,
        8.3821e-03, 5.5250e-02, 8.9461e-03, 4.7946e-02, 1.0381e-07, 1.5033e-02,
        1.2690e-02, 1.0441e-01, 1.2617e-02, 1.4045e-02, 2.5644e-02, 3.4853e-08,
        2.9705e-02, 4.9343e-08, 1.1887e-02, 1.2247e-02, 2.8967e-02, 1.5202e-01,
        4.0621e-08, 5.1279e-02, 1.4508e-01, 5.4086e-02, 3.3231e-08, 7.6042e-08,
        1.6776e-07, 2.3604e-02, 2.7573e-02, 3.6750e-02, 5.5598e-03, 1.0110e-02,
        6.1537e-08, 8.3558e-08, 9.7462e-09, 6.5092e-02, 3.6801e-02, 9.9144e-03,
        5.7780e-08, 5.2037e-08, 1.1250e-02, 3.9002e-08, 6.7870e-02, 2.2798e-02,
        4.2266e-02, 7.9042e-08, 2.4643e-02, 1.2513e-07, 8.3936e-02, 1.5954e-07,
        3.5847e-02, 1.4703e-01, 1.2096e-02, 3.1232e-08, 3.9484e-08, 6.8218e-02,
        4.4514e-02, 7.4765e-08, 1.0831e-07, 5.3377e-02, 8.6119e-03, 9.7432e-08,
        7.5581e-02, 2.0535e-02, 1.7101e-02, 2.5951e-02, 3.1620e-02, 4.8975e-02,
        1.4643e-02, 6.6826e-08, 5.9816e-08, 6.7151e-08, 3.5851e-08, 6.9349e-08,
        1.2543e-02, 3.1415e-02, 2.7079e-02, 1.7785e-02, 1.5602e-07, 2.8712e-08,
        1.0836e-07, 7.9977e-08, 7.8225e-02, 1.7847e-02, 7.6042e-08, 4.6461e-02,
        1.0163e-07, 2.2857e-02, 6.9931e-02, 1.3193e-02, 1.0831e-07, 9.9262e-08,
        4.2224e-02, 2.0213e-07, 1.3471e-01, 3.3880e-08, 9.7907e-08, 3.8265e-02,
        6.1894e-02, 2.2740e-08, 1.2686e-02, 7.3949e-03, 8.1666e-02, 1.2714e-02,
        1.1402e-02, 5.8136e-03, 1.5835e-01, 8.1295e-08, 1.7187e-02, 1.4873e-02,
        6.1749e-08, 3.1866e-08, 2.8095e-08, 1.7678e-08, 7.0880e-02, 7.6042e-08,
        1.5956e-07, 8.9979e-08, 2.3874e-08, 4.6278e-02, 1.4454e-01, 1.1093e-02,
        1.0164e-07, 1.0956e-07, 1.5960e-02, 3.9639e-02, 2.3790e-08, 7.8629e-02,
        2.2658e-02, 5.4100e-02, 3.1774e-03, 3.5851e-08, 1.6947e-02, 3.5839e-02,
        8.3711e-02, 4.9113e-08, 4.4938e-02, 7.8808e-08, 3.8832e-08, 8.3730e-02,
        3.4449e-08, 3.6247e-08, 3.4483e-08, 4.4689e-08, 3.1925e-08, 1.8298e-02,
        6.9394e-08, 1.5951e-02, 3.9394e-02, 1.7365e-02, 5.2037e-08, 6.3038e-02,
        5.1673e-02, 9.1072e-08, 7.6042e-08, 4.2759e-02, 4.4491e-08, 2.4725e-08,
        7.6042e-08, 2.0037e-02, 2.8724e-08, 8.2703e-08, 4.7044e-02, 2.4264e-02,
        4.2169e-02, 8.2461e-03, 7.7097e-02, 1.5582e-07, 4.9113e-08, 1.7545e-03,
        1.5291e-02, 6.1635e-02, 4.0442e-02, 1.5737e-07, 1.7230e-01, 2.0167e-02,
        5.2037e-08, 1.7679e-02, 7.8808e-08, 3.5851e-08, 4.4658e-02, 1.5856e-02,
        4.3036e-02, 5.5407e-02, 2.0138e-06, 7.3840e-08, 5.7658e-02, 6.4862e-02,
        9.7432e-08, 1.4332e-02, 7.2070e-08, 2.5905e-08, 2.6962e-02, 4.1289e-02,
        1.7831e-07, 5.5095e-03, 1.3912e-01, 7.3564e-08, 3.2753e-02, 3.9113e-02,
        3.3880e-08, 2.2072e-08, 1.8078e-01, 1.4107e-01, 1.2806e-02, 8.9491e-03,
        1.5384e-02, 7.8042e-02, 3.7065e-02, 5.2215e-02, 2.8885e-08, 1.8001e-02,
        5.7021e-08, 9.8793e-03, 3.1338e-02, 3.7161e-02, 7.8808e-08, 1.0388e-02,
        2.2831e-02, 3.9014e-02, 7.6042e-08, 3.3880e-08, 1.3563e-02, 2.9416e-02,
        3.5851e-08, 4.3707e-02, 3.3816e-02, 1.6546e-03, 6.1080e-08, 3.8631e-08,
        2.8703e-02, 2.5852e-02, 1.0128e-07, 1.2535e-07, 1.5642e-02, 2.8818e-02,
        6.0888e-08, 7.5955e-08, 7.4557e-02, 5.5601e-03, 2.7876e-02, 4.4559e-02,
        7.6042e-08, 3.2208e-02, 5.4807e-03, 1.0075e-07, 4.5598e-02, 5.9389e-08,
        2.1840e-02, 3.8568e-02, 5.2004e-08, 6.7081e-08, 5.5943e-02, 9.1218e-02,
        1.4681e-01, 4.3099e-02, 1.2494e-07, 4.7409e-02, 4.4032e-02, 2.2672e-02,
        1.3134e-01, 1.4297e-07, 1.0831e-07, 2.2767e-02, 1.0310e-02, 2.4183e-02,
        3.3897e-02, 1.5760e-02, 3.1477e-02, 2.7535e-08, 8.6003e-03, 1.3192e-01,
        3.4505e-02, 8.1112e-02, 3.4484e-08, 5.0542e-02, 2.5208e-02, 4.1780e-02,
        7.6042e-08, 5.7193e-02, 1.7739e-07, 9.6353e-03, 5.2004e-08, 5.3932e-08,
        3.6593e-02, 5.8990e-02, 4.0526e-02, 1.9570e-01, 7.3840e-08, 7.8808e-08,
        2.0462e-08, 1.3929e-02, 6.0526e-02, 3.4484e-08, 7.1353e-03, 1.0831e-07,
        1.3692e-02, 1.0831e-07, 5.9507e-07, 3.3869e-08, 2.0836e-02, 1.4330e-02,
        2.9770e-02, 1.5385e-07, 1.6728e-02, 3.1136e-08, 6.4306e-02, 1.2613e-07,
        3.2369e-02, 2.8975e-02, 1.3571e-07, 2.8136e-08, 5.1720e-02, 8.8314e-08,
        3.4554e-02, 6.2539e-08, 3.2878e-02, 3.1324e-02, 3.9078e-08, 7.9371e-08,
        1.1793e-02, 1.0245e-07, 5.3932e-08, 2.4725e-08, 7.1816e-02, 1.5734e-07,
        1.6342e-01, 1.5826e-07, 3.3880e-08, 3.9760e-02, 4.7724e-08, 9.7414e-08,
        3.5012e-08, 3.9627e-02, 1.1637e-02, 1.9993e-08, 5.7196e-08, 4.2380e-03,
        6.0511e-08, 5.9822e-08, 4.4702e-08, 1.3620e-03, 5.6589e-02, 9.8823e-02,
        9.1427e-03, 1.1627e-02, 2.6889e-02, 6.7151e-08, 4.2617e-02, 7.9228e-08,
        1.6384e-02, 5.4550e-02, 5.0497e-08, 5.7033e-08, 1.3011e-01, 3.2530e-02,
        4.3847e-08, 5.9816e-08, 9.2068e-03, 6.6736e-08, 5.8756e-02, 2.5556e-02,
        2.3388e-02, 4.9847e-02, 1.0831e-07, 1.8319e-07, 1.8947e-02, 8.2644e-08,
        5.5467e-03, 1.9714e-08, 2.3458e-08, 7.2632e-02, 3.8978e-08, 2.2736e-08,
        1.4868e-02, 5.5465e-02, 5.8279e-08, 7.0908e-02, 4.0592e-08, 5.2037e-08,
        4.0380e-02, 1.0588e-07, 7.1719e-08, 1.0163e-07, 6.0022e-02, 8.3558e-08,
        1.0646e-07, 4.8804e-02, 3.8048e-02, 3.1925e-02, 3.3880e-08, 3.9255e-02,
        3.6408e-03, 5.2037e-08, 1.0767e-02, 4.4689e-08, 3.7630e-02, 3.8131e-02,
        4.7463e-02, 7.4850e-08, 5.2658e-03, 1.1793e-01, 6.4753e-02, 5.9268e-07,
        1.3720e-02, 6.3747e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.2229e-07, 3.3771e-07, 2.6459e-02, 2.0836e-03, 1.9749e-07, 1.2905e-02,
        3.1070e-02, 2.9474e-03, 1.4543e-01, 1.0037e-06, 5.8806e-07, 3.2836e-02,
        5.7757e-07, 3.8336e-07, 3.1400e-07, 3.3905e-02, 1.7914e-01, 3.8336e-07,
        2.0670e-07, 8.6352e-03, 2.5396e-02, 4.1153e-07, 6.2301e-07, 8.3885e-06,
        4.2736e-03, 2.0995e-07, 2.9863e-07, 6.7275e-03, 3.7399e-02, 3.4905e-02,
        6.2388e-07, 1.2049e-02, 3.9259e-03, 5.7889e-02, 4.1797e-07, 4.6453e-07,
        8.3819e-03, 3.8379e-02, 4.9708e-02, 1.8171e-07, 4.3912e-02, 9.4761e-03,
        3.5062e-02, 2.8844e-02, 7.2131e-03, 2.2250e-03, 4.2164e-02, 4.6453e-07,
        1.0799e-02, 6.2301e-07, 1.5362e-01, 1.6422e-07, 5.0928e-07, 7.5150e-03,
        3.7743e-07, 2.0663e-03, 3.2500e-07, 2.8045e-07, 2.3479e-07, 4.6453e-07,
        3.3564e-02, 1.4705e-07, 1.8708e-07, 6.4820e-04, 6.6395e-07, 3.4312e-07,
        3.7553e-07, 4.4516e-02, 6.2388e-07, 2.3844e-02, 1.8171e-07, 4.8608e-02,
        1.8572e-07, 1.2442e-01, 1.8008e-07, 1.0761e-03, 1.1682e-01, 1.8631e-07,
        2.5121e-03, 2.9623e-07, 7.6349e-07, 1.6753e-07, 2.2752e-07, 1.8756e-02,
        1.8424e-03, 3.2500e-07, 2.9694e-07, 2.3154e-07, 1.8834e-07, 2.2882e-02,
        1.5799e-01, 2.0024e-07, 9.7349e-08, 3.1400e-07, 2.8985e-07, 4.0409e-07,
        1.0817e-07, 3.0947e-03, 2.7173e-07, 3.8722e-07, 4.0188e-02, 5.7383e-03,
        1.9742e-07, 1.8467e-07, 5.5201e-07, 4.0460e-02, 1.6768e-07, 6.1694e-08,
        8.3368e-08, 1.6572e-02, 3.8893e-07, 6.2860e-07, 7.6762e-07, 2.7173e-07,
        3.8378e-03, 4.0050e-07, 3.1046e-03, 1.4289e-02, 4.0023e-02, 3.3875e-02,
        6.1694e-08, 2.5957e-02, 2.3990e-02, 3.5472e-02, 1.2403e-01, 1.1559e-01,
        1.6629e-03, 4.5775e-03], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7174e-06, 1.1947e-03, 1.7551e-06, 9.8488e-07, 1.5121e-03, 2.8327e-02,
        8.0842e-02, 1.4226e-06, 3.3396e-07, 2.3033e-03, 1.5352e-03, 1.5830e-06,
        6.0733e-07, 3.9942e-07, 3.6248e-07, 5.0319e-07, 9.6812e-07, 1.5558e-03,
        2.3314e-06, 1.2597e-01, 2.2715e-06, 2.3314e-06, 3.1285e-02, 8.9009e-07,
        7.7263e-07, 9.5237e-07, 1.0363e-01, 1.7120e-06, 8.8870e-02, 9.7670e-02,
        2.2185e-06, 3.9101e-07, 1.1881e-01, 9.3735e-02, 4.2345e-07, 1.2611e-03,
        8.2806e-07, 1.2363e-02, 1.9766e-06, 2.5950e-06, 2.2667e-06, 8.7388e-07,
        1.0524e-06, 4.3382e-07, 5.6487e-07, 7.6862e-02, 8.9103e-07, 3.8719e-07,
        1.0415e-01, 1.9217e-06, 3.0880e-06, 1.8075e-06, 5.8698e-07, 1.3075e-06,
        1.1919e-06, 8.6908e-02, 8.9042e-07, 9.1216e-07, 1.6416e-06, 7.8126e-02,
        1.2513e-01, 1.9217e-06, 1.0474e-06, 6.5733e-07, 8.1044e-07, 1.2478e-06,
        1.4690e-06, 2.2667e-06, 6.6555e-07, 3.6785e-07, 1.6274e-06, 8.6658e-07,
        1.1009e-01, 1.2273e-06, 8.1612e-02, 1.1133e-06, 1.6416e-06, 2.8771e-06,
        8.6325e-02, 9.8490e-07, 4.8012e-07, 1.1268e-01, 7.3520e-07, 1.5343e-06,
        1.8695e-03, 8.9307e-02, 1.1079e-06, 7.2072e-02, 1.3075e-06, 1.0028e-01,
        6.5962e-07, 1.9217e-06, 1.9217e-06, 2.5724e-06, 8.9676e-07, 9.0849e-08,
        9.2156e-02, 1.2265e-06, 1.7358e-06, 5.3115e-07, 2.3453e-07, 7.9464e-02,
        5.1964e-07, 7.5393e-02, 4.8054e-07, 1.2696e-01, 7.7190e-02, 1.5650e-03,
        1.0587e-01, 9.8490e-07, 2.0005e-02, 7.9081e-07, 5.3035e-07, 1.4779e-06,
        1.5742e-06, 7.0608e-02, 4.8012e-07, 5.6975e-07, 2.1608e-06, 4.1519e-07,
        1.2403e-03, 9.7029e-07, 9.2394e-07, 1.2638e-06, 6.1964e-07, 1.6675e-06,
        8.8732e-02, 1.4197e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0739e-07, 4.9671e-08, 5.0994e-08, 2.2403e-02, 8.8066e-08, 5.1475e-08,
        7.2657e-03, 1.1992e-02, 6.3687e-02, 4.9553e-03, 3.2720e-08, 2.8719e-02,
        5.1233e-08, 2.4548e-02, 6.4715e-08, 6.5736e-02, 8.8416e-08, 3.2143e-02,
        9.1249e-03, 1.3600e-01, 8.9157e-03, 7.3126e-03, 6.3343e-02, 5.6021e-08,
        4.4546e-08, 4.8003e-02, 1.3028e-02, 5.3036e-02, 8.5985e-03, 5.5520e-02,
        7.4099e-03, 2.2765e-02, 7.4249e-02, 1.1493e-02, 4.9403e-03, 7.2380e-03,
        9.5935e-08, 2.9846e-08, 9.7767e-03, 4.6730e-02, 1.0018e-02, 6.5487e-02,
        4.9198e-02, 7.2960e-02, 8.0883e-08, 3.3171e-08, 3.2503e-02, 1.1695e-02,
        4.0788e-08, 7.0731e-02, 2.0589e-02, 6.0656e-03, 5.6552e-02, 1.1097e-02,
        2.4813e-02, 6.0613e-02, 3.6466e-08, 3.9276e-03, 4.1854e-02, 3.3408e-02,
        5.4101e-08, 5.8752e-03, 6.4824e-03, 1.0104e-01, 5.5577e-08, 3.2790e-02,
        5.9769e-02, 2.7984e-08, 4.2842e-02, 4.0093e-02, 4.0168e-02, 1.0930e-01,
        9.2932e-08, 2.3746e-02, 3.2128e-08, 1.9024e-01, 2.1158e-08, 3.5351e-02,
        1.7501e-02, 5.2806e-02, 5.8636e-02, 2.7480e-03, 4.3672e-02, 1.2692e-02,
        7.0443e-03, 2.6468e-02, 7.8338e-03, 5.7890e-02, 3.9880e-02, 2.2819e-08,
        1.0462e-07, 3.7685e-02, 1.0897e-07, 5.6021e-08, 2.8161e-02, 6.5642e-02,
        4.9919e-02, 1.1110e-02, 6.1580e-02, 3.6157e-02, 3.0942e-02, 9.3692e-03,
        1.4126e-07, 1.9180e-02, 6.7614e-02, 7.5234e-03, 4.1648e-02, 7.1028e-03,
        7.6170e-03, 6.7402e-02, 6.0718e-02, 2.3778e-02, 1.7520e-08, 2.9884e-02,
        1.3609e-02, 1.0580e-02, 8.2171e-03, 1.1843e-02, 1.1591e-07, 3.9523e-08,
        5.9236e-02, 1.3187e-02, 4.7198e-08, 4.2608e-02, 3.8117e-08, 1.1295e-02,
        3.1297e-02, 4.6761e-02, 7.0892e-03, 6.2643e-02, 1.0699e-07, 7.9161e-08,
        5.4850e-02, 4.3553e-02, 1.5408e-02, 2.7961e-02, 2.7677e-08, 1.5043e-01,
        1.6986e-07, 5.9944e-08, 6.9570e-08, 5.0109e-02, 4.8677e-02, 6.3930e-03,
        7.7705e-08, 5.1089e-08, 1.0471e-02, 1.5707e-02, 7.5037e-03, 2.4261e-02,
        5.2637e-02, 5.5802e-03, 2.9434e-02, 3.3196e-08, 4.6123e-02, 1.4126e-07,
        1.1705e-07, 9.1964e-02, 1.3958e-01, 6.4979e-08, 4.4879e-08, 5.7562e-03,
        2.0884e-02, 3.9825e-02, 4.2402e-08, 6.3734e-08, 1.6889e-02, 8.6198e-08,
        5.8686e-02, 7.6166e-02, 6.1079e-04, 4.7038e-02, 8.4637e-03, 3.3914e-03,
        9.2429e-03, 3.9291e-08, 1.7090e-08, 1.3845e-01, 1.6055e-07, 5.2673e-03,
        2.2080e-02, 8.0984e-03, 7.4653e-08, 1.2158e-02, 1.5288e-01, 1.6528e-07,
        2.1248e-02, 3.2041e-08, 1.3531e-02, 1.1895e-02, 2.9994e-02, 2.4735e-02,
        1.8768e-01, 3.4085e-03, 3.7707e-02, 1.5217e-02, 7.5564e-08, 1.0268e-07,
        4.4236e-02, 4.6276e-08, 5.6268e-02, 7.8244e-02, 6.9570e-08, 2.5539e-02,
        3.2413e-02, 4.4468e-08, 1.1144e-02, 2.6575e-02, 3.9957e-02, 2.2545e-02,
        1.0567e-02, 1.2074e-01, 1.8351e-02, 2.8121e-02, 1.1397e-02, 6.4824e-03,
        6.2618e-08, 6.9570e-08, 4.1939e-02, 8.7885e-08, 7.0499e-02, 3.3171e-08,
        1.1053e-07, 7.9015e-08, 1.8758e-02, 2.2238e-02, 1.4904e-02, 4.6891e-03,
        3.9922e-04, 7.9781e-02, 1.4903e-02, 1.8442e-03, 3.6762e-03, 2.2458e-02,
        2.9631e-02, 4.0078e-02, 1.8498e-02, 4.4879e-08, 6.7509e-02, 1.4064e-01,
        7.6988e-03, 4.1235e-08, 3.4603e-02, 1.1933e-07, 4.6990e-02, 1.4278e-02,
        6.2618e-08, 3.7130e-02, 8.1323e-08, 1.5230e-01, 1.5973e-01, 1.7307e-01,
        5.1166e-08, 1.2811e-02, 1.6645e-03, 4.2256e-03, 1.0613e-07, 6.8180e-03,
        3.2169e-02, 4.4272e-08, 2.4684e-02, 5.1866e-02, 1.5182e-02, 6.2618e-08,
        1.6006e-07, 6.6540e-02, 7.7706e-08, 1.7137e-02, 4.0797e-02, 1.9078e-08,
        9.4996e-02, 2.0216e-02, 8.6911e-03, 3.5741e-08, 7.0021e-08, 2.2612e-02,
        1.0860e-02, 2.8227e-02, 3.5177e-02, 2.2819e-08, 7.9799e-03, 2.9309e-08,
        1.8426e-01, 2.6267e-02, 2.7885e-08, 3.5890e-08, 4.4779e-02, 2.1503e-02,
        4.9659e-02, 1.6954e-02, 3.5052e-08, 1.8766e-01, 5.0954e-08, 6.9935e-02,
        8.6983e-08, 1.6016e-02, 5.1089e-08, 2.6903e-02, 2.5250e-02, 4.5422e-02,
        3.9632e-08, 8.5170e-02, 5.3967e-02, 1.5861e-03, 4.7088e-02, 2.8881e-02,
        5.8411e-02, 7.6068e-08, 9.1435e-03, 1.3228e-02, 1.5113e-02, 5.4944e-03,
        3.3507e-02, 3.3431e-02, 3.9086e-02, 5.7223e-02, 6.2618e-08, 1.6369e-02,
        7.0021e-08, 3.3973e-02, 6.0055e-02, 7.3093e-02, 9.9702e-08, 8.6542e-02,
        1.6048e-02, 1.7616e-01, 1.5950e-01, 5.9944e-08, 1.4257e-01, 5.0205e-02,
        1.6945e-07, 6.9220e-03, 1.5261e-02, 2.9992e-03, 1.9988e-02, 1.0770e-07,
        9.2244e-03, 3.8133e-03, 4.7513e-06, 5.1298e-08, 1.1971e-02, 2.0885e-02,
        2.6825e-02, 3.2845e-02, 1.4777e-01, 1.1983e-07, 9.8077e-03, 3.4199e-02,
        1.2695e-01, 6.9910e-08, 3.3812e-03, 1.0613e-07, 1.6932e-02, 1.1270e-01,
        6.7445e-02, 7.0436e-03, 1.6006e-07, 5.7849e-02, 3.6390e-02, 7.2620e-03,
        2.6825e-02, 1.7434e-02, 1.2910e-07, 5.5841e-02, 1.8442e-02, 2.0938e-02,
        4.0395e-02, 6.0849e-03, 4.4353e-02, 5.5014e-03, 1.1762e-02, 8.6396e-03,
        7.4191e-02, 1.2117e-02, 3.0157e-02, 2.4704e-02, 5.7902e-03, 2.0110e-03,
        1.7797e-02, 8.5042e-03, 1.0753e-07, 1.7291e-02, 5.1416e-02, 3.6910e-02,
        5.6347e-02, 7.0427e-03, 1.8162e-02, 7.3808e-02, 2.7886e-08, 5.7086e-08,
        3.4969e-02, 5.2656e-02, 4.8300e-08, 1.4156e-02, 1.5918e-01, 4.4879e-08,
        1.6055e-07, 4.3530e-03, 3.4586e-02, 8.0955e-02, 1.1973e-02, 1.6426e-07,
        3.9291e-02, 2.9673e-08, 4.7076e-02, 1.6508e-07, 1.0585e-07, 1.1669e-02,
        4.8839e-02, 3.3171e-08, 1.4836e-02, 1.0632e-07, 1.7456e-07, 6.4023e-08,
        5.0732e-08, 1.8849e-02, 4.0665e-08, 6.5100e-02, 1.5623e-02, 4.6878e-08,
        8.9812e-08, 1.1389e-07, 3.3606e-08, 1.6420e-02, 1.0970e-01, 2.5982e-02,
        2.1426e-02, 3.3136e-08, 1.6086e-07, 5.1233e-08, 6.9880e-03, 3.8623e-08,
        1.1280e-02, 6.3358e-08, 9.9799e-02, 2.9016e-02, 1.9114e-02, 4.0660e-08,
        1.2095e-07, 5.8604e-02, 8.4659e-03, 7.3533e-02, 5.1560e-08, 6.2339e-03,
        5.6681e-08, 3.2554e-08, 2.7389e-08, 3.1238e-02, 5.8568e-08, 1.5968e-02,
        7.4144e-03, 6.0224e-03, 1.3113e-02, 1.6959e-07, 4.8483e-02, 6.3335e-08,
        1.3592e-02, 9.8944e-02, 2.2198e-01, 2.4537e-08, 8.9288e-03, 4.9633e-08,
        5.4857e-03, 2.2573e-01, 4.9313e-03, 8.0000e-08, 2.2063e-02, 8.3576e-02,
        4.4533e-08, 3.0174e-02, 4.4800e-08, 6.9449e-03, 6.6139e-03, 1.4335e-07,
        2.3031e-02, 2.3917e-08, 3.3198e-08, 4.2189e-03, 7.2383e-02, 2.7984e-08,
        7.1060e-03, 4.5805e-02, 4.5569e-08, 3.1983e-02, 4.6189e-02, 1.5996e-07,
        3.7726e-02, 8.4254e-02, 5.1156e-02, 2.6854e-01, 3.5123e-03, 2.9976e-08,
        2.7237e-02, 5.6965e-02, 3.4957e-02, 2.9152e-02, 1.6259e-01, 2.7957e-02,
        6.0372e-02, 7.7928e-08, 9.2516e-03, 1.6106e-07, 7.7826e-08, 4.0593e-02,
        7.8323e-08, 5.1761e-08, 2.6347e-02, 3.3076e-02, 2.5582e-02, 1.0590e-07,
        4.0266e-02, 7.0238e-03], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7079e-07, 3.0517e-07, 1.3201e-01, 3.1599e-07, 1.7207e-07, 4.1035e-02,
        6.6317e-07, 3.4089e-07, 8.8375e-07, 5.3673e-07, 5.2181e-07, 2.5345e-02,
        2.6115e-07, 9.0358e-07, 1.4784e-01, 3.6294e-07, 1.0396e-01, 6.3147e-07,
        2.9066e-07, 4.7214e-07, 3.9575e-02, 2.6081e-07, 1.1840e-07, 1.9175e-07,
        3.8797e-02, 5.1255e-07, 2.8060e-02, 3.4111e-07, 3.1701e-07, 8.5434e-07,
        5.8128e-02, 2.8879e-07, 6.8597e-03, 3.2712e-02, 6.6317e-07, 1.8329e-07,
        2.2872e-07, 2.2745e-07, 1.7426e-07, 1.2299e-06, 9.2515e-07, 4.3646e-07,
        3.7180e-07, 1.5581e-07, 1.7426e-07, 2.6115e-07, 9.2902e-02, 1.9123e-07,
        4.7267e-07, 2.7767e-07, 1.0273e-01, 2.9330e-07, 5.1641e-07, 4.0294e-07,
        3.9605e-02, 2.8826e-07, 3.9655e-07, 2.2872e-07, 2.7316e-07, 3.6487e-07,
        4.7985e-07, 1.7079e-07, 1.9723e-07, 4.0063e-02, 3.6487e-07, 3.5996e-07,
        3.4089e-07, 6.3147e-07, 9.4980e-07, 2.4399e-02, 1.0062e-01, 1.8595e-07,
        4.1516e-02, 2.8879e-07, 4.0923e-02, 2.0198e-07, 3.6682e-07, 4.1115e-02,
        2.8826e-07, 1.6064e-07, 4.3646e-07, 8.4453e-02, 4.1660e-07, 4.3959e-07,
        4.7601e-02, 2.5207e-07, 2.2708e-07, 4.0337e-07, 3.4089e-07, 8.7880e-07,
        2.3535e-07, 8.8375e-07, 1.2032e-07, 3.5122e-07, 1.0040e-01, 6.0057e-07,
        4.1983e-08, 1.2025e-02, 6.5921e-07, 1.7192e-02, 6.3147e-07, 5.1255e-07,
        2.9330e-07, 2.4436e-02, 4.2026e-02, 3.2014e-03, 9.5758e-02, 9.0358e-07,
        3.4072e-02, 4.0337e-07, 1.2414e-01, 1.7079e-07, 8.9963e-02, 2.5015e-02,
        7.2172e-07, 5.3523e-07, 3.5829e-07, 1.0612e-02, 7.4670e-08, 6.6317e-07,
        4.8264e-02, 9.3914e-08, 2.7661e-02, 4.7267e-07, 3.2250e-02, 2.8879e-07,
        5.8444e-07, 4.8867e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0479e-06, 1.6762e-06, 6.1550e-07, 1.5670e-06, 4.2272e-07, 6.1550e-07,
        9.3416e-07, 3.5590e-07, 7.0519e-07, 2.6493e-07, 1.1130e-06, 9.6992e-07,
        8.3367e-07, 1.6762e-06, 1.0382e-01, 1.0163e-06, 6.8098e-07, 1.2820e-06,
        1.0002e-01, 1.1018e-06, 8.4301e-07, 1.5147e-06, 2.2609e-06, 8.7780e-07,
        7.7416e-07, 2.5872e-07, 9.1909e-07, 6.9109e-03, 5.3554e-07, 7.2014e-07,
        8.2300e-07, 4.9100e-07, 8.0135e-07, 8.6125e-07, 9.6961e-07, 4.9100e-07,
        1.0608e-01, 1.4527e-06, 4.6400e-07, 4.6078e-07, 3.4682e-07, 4.9100e-07,
        9.6992e-07, 2.4445e-07, 9.4348e-07, 2.0639e-01, 4.2044e-07, 4.6078e-07,
        1.1255e-03, 7.5142e-08, 8.2912e-07, 5.3296e-07, 1.5670e-06, 1.4918e-06,
        4.5328e-08, 6.7189e-07, 7.4781e-07, 2.5682e-06, 1.2666e-06, 2.2295e-07,
        6.6369e-07, 7.7416e-07, 8.0361e-07, 8.8773e-07, 9.3231e-02, 1.4527e-06,
        6.1550e-07, 8.8049e-02, 1.1095e-01, 1.1455e-06, 1.4918e-06, 6.9301e-07,
        8.4301e-07, 8.2300e-07, 9.6992e-07, 1.3688e-01, 6.8369e-07, 9.1281e-02,
        9.3695e-02, 6.9301e-07, 4.9740e-07, 1.1892e-01, 8.6125e-07, 7.5142e-08,
        4.9100e-07, 1.1218e-01, 4.1134e-07, 1.0546e-01, 1.4527e-06, 8.2300e-07,
        3.4528e-03, 8.8725e-02, 1.5313e-06, 7.8001e-07, 1.6762e-06, 2.9094e-06,
        8.2912e-07, 1.1329e-06, 7.2714e-07, 1.1282e-03, 1.6151e-06, 7.1048e-07,
        6.8507e-03, 8.2300e-07, 1.4918e-06, 1.2824e-06, 5.7115e-07, 2.0763e-06,
        7.7416e-07, 5.7236e-07, 5.3089e-07, 1.8111e-03, 9.4497e-02, 4.8964e-07,
        1.5518e-03, 1.4918e-06, 4.6078e-07, 1.0333e-01, 8.6125e-07, 7.2014e-07,
        2.1606e-06, 8.5648e-07, 4.2175e-07, 8.2300e-07, 7.6459e-07, 6.8123e-07,
        8.0361e-07, 5.9395e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.2041e-08, 1.2744e-07, 3.0544e-08, 1.4381e-07, 6.6280e-08, 3.2669e-02,
        5.2064e-02, 4.2810e-04, 1.6025e-07, 1.3160e-06, 1.0067e-07, 8.8177e-02,
        7.4204e-02, 3.5368e-08, 3.4673e-08, 4.5509e-02, 3.2962e-02, 1.3703e-02,
        3.5637e-08, 2.2404e-08, 1.7407e-02, 4.6129e-02, 5.9955e-02, 9.9870e-08,
        5.0045e-08, 8.8342e-08, 4.5908e-08, 4.1772e-08, 9.7634e-03, 3.8619e-08,
        5.8480e-08, 1.4512e-02, 3.1834e-03, 1.4838e-07, 5.7430e-08, 2.8934e-08,
        7.9274e-08, 6.6166e-08, 7.5348e-08, 7.3549e-08, 1.1272e-07, 5.4984e-08,
        9.8158e-08, 2.2710e-02, 5.5425e-08, 2.1955e-01, 5.4355e-08, 6.0568e-08,
        8.9378e-08, 2.7541e-08, 7.8312e-08, 6.2709e-03, 5.3482e-02, 8.4812e-03,
        3.9391e-08, 2.7634e-02, 5.2119e-08, 1.9976e-08, 1.0071e-01, 7.9785e-08,
        3.8428e-08, 4.3216e-08, 5.9348e-03, 1.2630e-07, 9.9870e-08, 1.0241e-07,
        3.2424e-02, 1.2151e-01, 2.9505e-03, 1.0223e-07, 1.0303e-07, 7.4723e-03,
        1.4999e-08, 2.8020e-02, 1.3815e-07, 1.8346e-01, 1.0154e-07, 7.3887e-08,
        7.7143e-08, 7.5940e-08, 1.9506e-01, 1.8925e-02, 6.3464e-03, 1.3761e-03,
        5.8139e-04, 1.0007e-07, 3.8767e-02, 6.1614e-03, 4.1988e-08, 1.1236e-07,
        2.8707e-08, 8.4185e-08, 4.2827e-02, 4.9583e-08, 5.1841e-02, 1.0709e-02,
        1.8257e-08, 3.7635e-02, 1.9229e-02, 2.0324e-08, 1.0143e-07, 6.1359e-02,
        1.0242e-07, 3.9838e-02, 1.0118e-07, 2.5994e-08, 8.1524e-03, 1.3785e-07,
        2.9931e-02, 3.7712e-02, 4.8525e-08, 3.8747e-02, 6.7661e-08, 6.2951e-08,
        8.8035e-08, 1.2219e-07, 5.3334e-02, 1.3646e-02, 4.0330e-02, 2.2901e-08,
        7.8882e-08, 1.6907e-07, 1.1587e-08, 1.2132e-07, 4.9956e-08, 3.0756e-03,
        4.0700e-08, 5.5107e-03, 4.4037e-08, 5.3710e-02, 4.4936e-08, 2.1743e-01,
        2.5213e-02, 9.4742e-08, 2.3010e-02, 6.7837e-03, 6.9858e-08, 4.9357e-08,
        3.4735e-08, 8.3838e-02, 8.5147e-03, 8.7797e-02, 9.1252e-08, 8.3045e-03,
        2.7785e-08, 3.1878e-02, 1.0281e-02, 6.5338e-02, 5.0327e-02, 4.6160e-08,
        9.8558e-08, 6.0040e-02, 4.4910e-02, 3.4736e-08, 4.9010e-08, 4.3425e-08,
        1.6453e-02, 1.5686e-01, 3.5689e-02, 2.6068e-08, 7.2187e-02, 6.7418e-08,
        8.0825e-08, 1.5440e-02, 2.5157e-08, 1.3100e-08, 1.4714e-07, 1.4999e-08,
        9.3835e-03, 1.0920e-07, 1.9488e-02, 4.8996e-02, 1.1240e-07, 3.2321e-08,
        5.9096e-08, 4.5395e-02, 3.6120e-08, 4.7707e-08, 1.0670e-07, 7.6061e-08,
        1.1429e-02, 1.6641e-02, 7.0219e-03, 1.2120e-02, 1.2316e-02, 1.0910e-07,
        1.0430e-07, 2.5714e-02, 6.9802e-02, 8.2484e-08, 1.0242e-07, 5.5459e-08,
        1.5219e-02, 5.9178e-08, 2.8422e-08, 1.1611e-02, 3.1227e-08, 1.5870e-07,
        3.1316e-08, 1.5592e-07, 1.0057e-07, 3.2037e-08, 2.9601e-03, 9.9604e-08,
        1.0517e-07, 1.4839e-07, 3.8657e-08, 4.1505e-02, 4.8153e-02, 5.9172e-08,
        6.1636e-08, 5.3680e-08, 3.8973e-02, 2.4294e-08, 1.0623e-02, 2.5412e-02,
        9.8224e-08, 4.5017e-08, 6.0257e-08, 9.9536e-08, 1.1165e-07, 3.6308e-03,
        2.9437e-08, 3.4950e-08, 2.0072e-07, 2.7165e-08, 3.6227e-08, 1.1409e-07,
        4.4936e-08, 8.8414e-03, 1.5495e-02, 1.2885e-01, 4.9500e-08, 6.4371e-02,
        8.0190e-08, 5.9879e-08, 1.1214e-02, 5.2425e-08, 3.6787e-08, 1.1847e-02,
        1.2526e-02, 3.9787e-08, 4.3690e-02, 4.9492e-08, 3.3002e-02, 1.0784e-02,
        1.5000e-08, 7.9573e-02, 1.6950e-08, 7.3663e-08, 9.7055e-03, 9.7140e-03,
        8.0400e-08, 1.3712e-02, 1.6324e-07, 3.9910e-02, 2.3281e-08, 5.8819e-02,
        3.1044e-02, 5.5243e-02, 7.8014e-08, 5.6621e-08, 1.1528e-07, 1.5833e-07,
        2.0837e-07, 2.9492e-02, 1.5123e-07, 1.4516e-02, 3.0819e-08, 7.5445e-08,
        2.3674e-02, 1.1458e-02, 8.0737e-08, 6.5625e-08, 5.4335e-08, 9.6004e-03,
        6.4813e-08, 1.6155e-08, 4.3129e-08, 4.5017e-08, 1.9390e-02, 4.7215e-03,
        3.0025e-08, 1.0132e-07, 1.0169e-07, 2.0557e-07, 1.0113e-07, 6.0621e-08,
        7.2342e-08, 4.9797e-02, 3.6036e-02, 2.6646e-02, 5.2704e-03, 4.5132e-08,
        9.9870e-08, 1.6786e-02, 3.9790e-08, 3.3190e-08, 1.3061e-02, 7.7882e-02,
        6.3214e-08, 8.0972e-08, 6.6017e-08, 5.2008e-03, 4.2390e-08, 3.5535e-02,
        8.9738e-08, 2.7968e-02, 3.6581e-08, 4.2358e-02, 8.3877e-03, 5.0082e-08,
        4.7088e-08, 3.2612e-02, 5.3631e-02, 6.0885e-03, 9.8224e-08, 1.7918e-02,
        6.3094e-02, 3.7813e-02, 3.1221e-02, 2.5118e-02, 3.9790e-08, 6.7746e-03,
        1.3063e-02, 3.1214e-02, 5.4075e-03, 1.3417e-01, 2.7802e-03, 3.0516e-02,
        4.9492e-08, 3.9554e-08, 8.7072e-04, 3.9445e-02, 3.5878e-02, 1.4935e-07,
        6.6994e-03, 5.9214e-03, 6.5482e-02, 7.6146e-08, 7.3538e-08, 5.8122e-02,
        4.7861e-02, 8.6571e-08, 3.4836e-02, 7.1671e-08, 9.2247e-03, 3.6864e-02,
        7.9654e-03, 9.0630e-02, 6.7984e-08, 8.4720e-08, 5.4922e-02, 4.7038e-02,
        3.0407e-02, 9.3721e-03, 5.2425e-08, 2.9511e-08, 7.1799e-08, 3.0822e-02,
        1.0140e-07, 2.0682e-03, 3.6122e-08, 3.9583e-08, 2.0444e-02, 7.0949e-08,
        2.0749e-02, 3.8566e-08, 3.4985e-08, 7.3306e-08, 7.2894e-08, 1.2763e-02,
        4.5222e-02, 9.1573e-03, 1.7172e-02, 2.0587e-08, 9.7144e-08, 7.4740e-08,
        5.8058e-02, 8.1742e-08, 3.9790e-08, 1.7761e-02, 3.6629e-03, 5.0758e-02,
        1.0879e-07, 4.0093e-08, 3.7701e-08, 5.4170e-03, 4.5017e-08, 2.0558e-07,
        4.8518e-08, 4.8441e-02, 1.0450e-07, 1.2508e-02, 4.0610e-02, 5.2425e-08,
        1.6646e-07, 2.0561e-02, 1.1875e-02, 1.1406e-07, 2.5544e-02, 5.2709e-02,
        3.5706e-02, 1.0236e-07, 1.3203e-02, 8.6782e-08, 1.2762e-02, 1.0598e-02,
        3.1671e-02, 1.0168e-07, 1.1444e-07, 4.1751e-08, 2.6286e-02, 1.0608e-07,
        3.4349e-02, 7.2432e-08, 8.0737e-08, 5.7244e-08, 1.7290e-07, 4.2595e-02,
        4.9100e-08, 5.8543e-08, 5.0852e-08, 5.5508e-02, 2.6566e-02, 5.4410e-08,
        1.0239e-07, 3.6103e-07, 9.6058e-08, 5.7959e-04, 1.7770e-03, 9.9414e-08,
        1.0877e-02, 1.0910e-07, 2.0874e-01, 1.1420e-07, 6.6890e-08, 5.0914e-08,
        3.7457e-08, 6.8771e-02, 3.5710e-08, 7.3031e-08, 1.4637e-02, 5.3687e-02,
        9.9894e-08, 4.9098e-08, 1.0132e-07, 1.0606e-02, 3.5818e-02, 8.5580e-03,
        1.1319e-02, 6.5111e-03, 8.2451e-02, 8.4170e-08, 3.4322e-08, 4.4892e-08,
        1.4224e-02, 2.8544e-08, 1.8958e-02, 5.0315e-03, 1.2090e-02, 2.7232e-08,
        6.9550e-03, 3.4115e-02, 9.1362e-08, 1.3064e-02, 3.9550e-08, 5.0829e-08,
        1.2635e-07, 3.2035e-02, 1.9932e-01, 4.5615e-08, 1.2347e-02, 7.2824e-08,
        4.3563e-04, 1.0246e-07, 7.7016e-02, 3.0439e-08, 5.2102e-08, 1.1298e-01,
        3.6972e-03, 8.6736e-02, 4.9672e-02, 5.3695e-02, 7.3506e-08, 4.7291e-02,
        1.5586e-01, 8.1267e-08, 9.4888e-08, 1.9197e-02, 1.3205e-08, 5.4335e-08,
        1.8528e-02, 3.2013e-08, 7.1293e-03, 2.8945e-02, 5.6733e-08, 7.9622e-08,
        8.1796e-08, 1.5038e-08, 1.0607e-02, 3.4680e-08, 2.8606e-08, 1.4204e-02,
        6.7399e-08, 4.6155e-02, 9.2904e-08, 4.0837e-08, 1.0005e-07, 3.6119e-08,
        1.1473e-07, 1.0523e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.7911e-08, 2.4463e-07, 1.0022e-01, 7.8574e-02, 7.8972e-02, 6.0564e-08,
        9.2981e-08, 7.6267e-08, 7.6267e-08, 6.3427e-02, 7.6656e-02, 7.6267e-08,
        8.8053e-02, 4.1236e-07, 1.0848e-01, 8.9866e-02, 2.3606e-07, 1.1820e-07,
        1.4827e-07, 1.2444e-01, 8.2630e-02, 9.6231e-02, 3.0897e-07, 7.6080e-02,
        2.4463e-07, 1.3307e-07, 1.7925e-07, 8.6553e-02, 2.0719e-07, 1.0675e-01,
        8.7047e-02, 7.6267e-08, 9.1877e-02, 9.3085e-02, 7.7693e-02, 2.1451e-07,
        7.6267e-08, 2.0719e-07, 1.2090e-01, 1.1997e-01, 9.1176e-02, 1.7925e-07,
        1.5471e-07, 7.7309e-02, 1.0532e-01, 6.7698e-02, 9.1852e-02, 8.5818e-02,
        1.0074e-01, 1.1820e-07, 5.4508e-08, 8.4720e-02, 7.9925e-02, 8.0909e-02,
        1.7925e-07, 1.0761e-01, 9.7546e-02, 3.1069e-07, 5.4508e-08, 1.5471e-07,
        8.7353e-02, 1.7925e-07, 5.4508e-08, 1.3856e-07, 1.0249e-01, 1.3856e-07,
        7.6267e-08, 5.4508e-08, 8.0407e-02, 8.8509e-02, 3.2623e-07, 9.4435e-02,
        1.0506e-01, 1.2036e-01, 5.4508e-08, 5.4508e-08, 5.4508e-08, 6.3341e-02,
        5.6411e-02, 8.9308e-02, 1.7925e-07, 8.2955e-02, 1.0854e-01, 9.4926e-02,
        7.8115e-02, 7.6267e-08, 7.1432e-02, 8.7911e-08, 2.3111e-07, 1.0111e-01,
        8.1480e-02, 9.3767e-02, 4.1237e-07, 5.9959e-02, 2.0719e-07, 1.0489e-01,
        6.0564e-08, 1.5471e-07, 8.7759e-02, 1.3856e-07, 9.2014e-02, 9.5025e-02,
        7.3172e-02, 6.2790e-02, 8.7263e-02, 7.2337e-02, 8.6317e-02, 6.4085e-02,
        7.6267e-08, 9.7590e-02, 7.6267e-08, 3.5865e-02, 2.1451e-07, 2.3606e-07,
        2.4463e-07, 5.4508e-08, 9.4201e-02, 2.1451e-07, 8.7911e-08, 7.6267e-08,
        7.8743e-02, 1.3856e-07, 1.1499e-01, 8.5028e-02, 8.5072e-02, 1.6297e-07,
        2.0719e-07, 1.5401e-07, 2.4463e-07, 1.0293e-01, 2.0719e-07, 6.6316e-02,
        8.7911e-08, 8.2146e-02, 9.3206e-02, 9.5745e-02, 1.9995e-07, 1.0604e-01,
        8.2358e-08, 9.5926e-02, 6.3870e-02, 9.4215e-02, 4.5160e-02, 8.8292e-02,
        5.4508e-08, 7.4519e-02, 7.9761e-02, 5.4508e-08, 8.7911e-08, 5.4508e-08,
        9.7982e-02, 5.4508e-08, 1.0984e-01, 8.3798e-02, 7.6267e-08, 8.2976e-02,
        7.6267e-08, 1.0070e-07, 6.0564e-08, 8.7911e-08, 7.6267e-08, 8.7709e-02,
        1.3307e-07, 1.5401e-07, 7.1448e-02, 9.2329e-02, 8.6518e-02, 3.0897e-07,
        2.4463e-07, 9.2981e-08, 9.5405e-02, 5.4508e-08, 1.7925e-07, 7.1461e-02,
        7.4451e-02, 8.6394e-02, 3.9757e-07, 2.1451e-07, 3.9757e-07, 8.5467e-08,
        9.8458e-02, 1.7925e-07, 5.4508e-08, 1.0044e-01, 5.4508e-08, 9.0983e-02,
        1.1820e-07, 7.2559e-02, 1.3307e-07, 4.0601e-07, 2.1387e-07, 2.4463e-07,
        1.3307e-07, 7.6702e-02, 8.6542e-02, 1.7925e-07, 1.1237e-01, 8.2801e-02,
        2.4463e-07, 2.1451e-07, 8.6064e-02, 1.2569e-07, 1.7925e-07, 1.2862e-01,
        8.4325e-02, 1.0550e-01, 1.0131e-01, 1.7925e-07, 7.8187e-02, 5.8720e-02,
        8.7911e-08, 3.0897e-07, 1.3307e-07, 1.0028e-01, 1.3307e-07, 7.5891e-02,
        1.0570e-01, 8.3126e-02, 8.2495e-02, 1.3856e-07, 1.3856e-07, 8.5847e-08,
        7.6267e-08, 2.2331e-07, 6.7862e-02, 7.6267e-08, 8.4653e-02, 2.1451e-07,
        7.6267e-08, 4.1802e-07, 6.4017e-02, 1.6638e-07, 8.8011e-02, 3.9757e-07,
        7.9758e-02, 2.5394e-02, 2.1451e-07, 2.0719e-07, 8.5847e-08, 1.9657e-07,
        4.1962e-07, 9.2197e-02, 1.2576e-01, 8.7911e-08, 5.4508e-08, 8.4123e-02,
        8.7346e-02, 1.1901e-01, 3.9757e-07, 1.5471e-07, 8.9937e-02, 9.3861e-02,
        7.6267e-08, 8.6418e-02, 8.5820e-02, 1.5471e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0956e-06, 4.8591e-07, 3.1045e-06, 8.0753e-02, 5.9893e-02, 4.8591e-07,
        8.3732e-02, 8.4925e-02, 8.4073e-02, 7.9809e-02, 7.1965e-02, 8.7357e-02,
        1.4115e-06, 1.6056e-06, 5.2215e-07, 5.6930e-02, 7.5098e-07, 1.0956e-06,
        1.0956e-06, 5.8365e-07, 8.2068e-02, 8.6777e-02, 2.4035e-06, 6.0856e-02,
        7.8356e-02, 2.3638e-06, 5.2215e-07, 8.0165e-02, 3.1045e-06, 5.8365e-07,
        2.4035e-06, 1.5026e-06, 1.2733e-06, 7.9009e-02, 1.2733e-06, 1.9336e-06,
        4.3280e-07, 4.8591e-07, 8.1508e-02, 8.5809e-02, 7.3385e-02, 1.6056e-06,
        4.3280e-07, 1.9336e-06, 3.0874e-07, 7.3321e-07, 3.1045e-06, 5.8894e-07,
        7.5098e-07, 1.6056e-06, 7.9521e-02, 6.1510e-02, 1.2733e-06, 3.1045e-06,
        3.6904e-07, 1.2733e-06, 3.4305e-07, 6.1069e-02, 9.8801e-02, 8.7904e-02,
        9.8891e-02, 7.1773e-02, 4.3280e-07, 3.0545e-06, 8.1396e-02, 6.7336e-07,
        3.4305e-07, 4.8591e-07, 7.5098e-07, 5.8365e-07, 2.4035e-06, 1.0956e-06,
        7.9585e-02, 1.0427e-06, 7.6328e-02, 2.4035e-06, 2.4035e-06, 4.8591e-07,
        1.5026e-06, 1.5768e-06, 5.8365e-07, 1.2733e-06, 6.7336e-07, 7.1437e-02,
        7.2473e-02, 5.8365e-07, 4.3280e-07, 1.9336e-06, 5.9041e-02, 8.6882e-02,
        7.1420e-02, 4.3280e-07, 7.6092e-02, 6.9973e-02, 2.4035e-06, 2.4035e-06,
        8.0003e-02, 3.4305e-07, 6.7558e-07, 1.7034e-06, 8.4820e-02, 3.4305e-07,
        7.6013e-02, 7.2771e-02, 5.2215e-07, 2.4035e-06, 9.0104e-02, 5.8365e-07,
        1.5026e-06, 1.1816e-06, 4.8591e-07, 3.1045e-06, 9.4489e-07, 5.8075e-07,
        2.5293e-06, 4.8591e-07, 8.0820e-02, 7.8796e-02, 1.3389e-06, 1.0956e-06,
        8.5321e-07, 7.4273e-07, 1.2733e-06, 7.8131e-02, 4.3280e-07, 2.4035e-06,
        7.7968e-02, 1.5768e-06, 5.8075e-07, 2.6707e-07, 9.0632e-07, 8.2973e-02,
        6.7558e-07, 1.2337e-06, 6.2634e-02, 8.1178e-07, 7.7438e-02, 8.3196e-02,
        7.6266e-02, 4.6019e-02, 4.8591e-07, 7.8820e-02, 4.3280e-07, 8.1249e-02,
        4.3280e-07, 1.0956e-06, 8.4955e-02, 1.0956e-06, 8.7936e-02, 3.4305e-07,
        7.0392e-07, 4.4593e-07, 1.5026e-06, 1.2733e-06, 3.1045e-06, 4.3280e-07,
        8.7396e-02, 1.5768e-06, 7.1819e-02, 4.3280e-07, 4.9207e-02, 1.6056e-06,
        7.9834e-02, 3.6904e-07, 3.1045e-06, 5.8365e-07, 6.7558e-07, 1.1816e-06,
        7.6575e-02, 9.4489e-07, 5.1875e-02, 8.0965e-02, 1.0956e-06, 3.1045e-06,
        1.3389e-06, 4.3280e-07, 1.5026e-06, 1.2733e-06, 9.0632e-07, 8.1658e-02,
        1.0956e-06, 1.5026e-06, 8.8498e-02, 8.0415e-02, 1.1325e-06, 2.3655e-06,
        8.2409e-02, 3.0874e-07, 1.2733e-06, 7.7897e-02, 1.0956e-06, 5.8075e-07,
        9.0632e-07, 8.4515e-02, 4.3280e-07, 1.3632e-06, 9.4489e-07, 6.8594e-02,
        1.2467e-06, 4.4597e-07, 2.6675e-06, 7.5517e-02, 1.0956e-06, 6.7336e-07,
        3.0874e-07, 6.7336e-07, 1.0956e-06, 3.1045e-06, 5.8075e-07, 1.3389e-06,
        4.3280e-07, 1.0956e-06, 1.0956e-06, 5.8075e-07, 7.9834e-02, 8.5743e-02,
        8.6436e-02, 4.6728e-07, 7.6418e-07, 8.1052e-02, 7.5098e-07, 2.4035e-06,
        5.8365e-07, 8.5341e-02, 7.1487e-02, 5.8894e-07, 1.2733e-06, 1.5026e-06,
        8.1178e-07, 1.0427e-06, 1.0427e-06, 7.5098e-07, 6.2727e-02, 7.4490e-02,
        6.7558e-07, 1.9336e-06, 5.8365e-07, 7.2237e-02, 9.4489e-07, 1.0956e-06,
        7.6048e-02, 9.3136e-02, 6.7558e-07, 4.3280e-07, 1.3389e-06, 1.5026e-06,
        4.3280e-07, 1.2733e-06, 2.4035e-06, 7.4377e-02, 7.4814e-02, 1.0956e-06,
        1.0427e-06, 4.8591e-07, 2.4035e-06, 1.2733e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.5955e-08, 1.5974e-07, 4.4375e-02,  ..., 3.2425e-08, 7.0329e-02,
        1.8588e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.3067e-07, 2.4099e-07, 2.6581e-02,  ..., 1.6376e-07, 4.0330e-02,
        4.9281e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.4216e-07, 4.7557e-07, 1.6871e-02, 1.8406e-02, 2.6285e-02, 9.8295e-07,
        3.0185e-02, 4.1573e-02, 7.0256e-07, 4.8395e-07, 3.6696e-02, 3.6467e-02,
        1.0432e-06, 3.5306e-02, 9.8526e-07, 4.6658e-02, 3.1420e-02, 2.9569e-07,
        4.6314e-07, 2.4890e-02, 2.3311e-07, 3.6944e-02, 2.4077e-02, 3.1253e-02,
        2.3180e-07, 3.7600e-02, 9.8999e-07, 3.3021e-07, 6.4893e-03, 5.4403e-07,
        1.0216e-06, 2.6149e-02, 5.9802e-07, 1.0923e-01, 3.3033e-07, 1.3409e-02,
        1.7040e-01, 6.1891e-07, 4.2658e-07, 3.8411e-02, 1.5179e-06, 2.2606e-07,
        8.7452e-07, 2.9569e-07, 3.4018e-02, 7.5849e-07, 4.8887e-07, 2.2842e-02,
        1.2660e-07, 9.8526e-07, 9.8999e-07, 7.3241e-07, 3.4171e-02, 1.7801e-01,
        1.2612e-06, 5.2607e-07, 3.6244e-02, 4.3508e-07, 9.5764e-03, 7.5849e-07,
        4.1093e-02, 2.9748e-02, 2.3271e-06, 4.0023e-02, 4.1343e-07, 3.5146e-02,
        3.0225e-02, 3.2448e-02, 9.8526e-07, 3.0763e-07, 6.6519e-07, 3.5964e-02,
        3.4589e-02, 3.6297e-02, 1.7466e-07, 4.2662e-07, 4.8395e-07, 2.2349e-02,
        7.0256e-07, 2.1482e-02, 1.7327e-02, 2.6125e-07, 3.8623e-02, 4.6921e-07,
        3.3033e-07, 3.6228e-02, 4.8887e-07, 1.0432e-06, 9.2461e-07, 4.2658e-07,
        2.9475e-02, 5.9802e-07, 1.3887e-06, 1.7097e-07, 1.0432e-06, 1.9288e-01,
        4.3508e-07, 2.9629e-02, 4.0558e-02, 7.5849e-07, 1.5131e-06, 3.4850e-02,
        2.8039e-02, 1.6171e-02, 3.2319e-02, 3.4176e-02, 3.0155e-02, 1.5794e-01,
        7.5849e-07, 1.7837e-01, 1.5162e-06, 3.5197e-02, 3.4419e-02, 9.2461e-07,
        9.2461e-07, 5.2607e-07, 5.8348e-07, 4.5044e-02, 2.8679e-02, 7.3240e-07,
        2.7583e-07, 2.8946e-02, 2.2427e-02, 1.3319e-06, 3.7097e-07, 3.2226e-02,
        2.0962e-07, 5.9803e-02, 3.8724e-02, 3.4216e-07, 7.0256e-07, 1.5675e-01,
        3.7776e-02, 1.7987e-07, 6.1891e-07, 2.5328e-07, 2.7583e-07, 3.7768e-02,
        3.8211e-02, 5.8348e-07, 3.3033e-07, 7.9550e-07, 3.9018e-02, 3.7795e-02,
        3.2670e-02, 1.8227e-07, 3.2850e-02, 1.8227e-07, 3.0488e-07, 3.5079e-02,
        3.5967e-02, 4.8887e-07, 2.7643e-07, 7.6931e-07, 1.7041e-01, 4.3549e-02,
        6.0625e-07, 9.6595e-07, 2.6906e-07, 2.7046e-02, 1.5644e-07, 3.4743e-02,
        1.4615e-06, 9.8526e-07, 9.8526e-07, 4.8395e-07, 3.9610e-07, 3.7406e-07,
        3.7730e-07, 3.1241e-02, 1.7120e-07, 5.4435e-07, 4.2853e-02, 9.8526e-07,
        5.7740e-03, 1.0432e-06, 7.5224e-07, 9.2926e-07, 2.9783e-02, 3.0167e-02,
        9.2926e-07, 3.0015e-02, 4.8395e-07, 3.4787e-02, 3.7021e-07, 3.1666e-02,
        4.1343e-07, 1.0216e-06, 9.2926e-07, 4.3550e-03, 1.2612e-06, 6.1891e-07,
        9.4761e-07, 6.6042e-07, 7.5193e-07, 3.7459e-02, 4.7457e-07, 5.8759e-07,
        4.0507e-02, 9.8999e-07, 3.1046e-07, 7.0256e-07, 3.3592e-02, 3.0763e-07,
        7.0256e-07, 2.9569e-07, 9.5982e-07, 6.3358e-07, 5.8895e-07, 2.7583e-07,
        1.6879e-01, 9.8526e-07, 5.4403e-07, 3.3033e-07, 1.5402e-06, 9.2926e-07,
        3.6385e-02, 6.7096e-07, 1.8286e-07, 5.4403e-07, 4.3367e-02, 1.8725e-02,
        4.1721e-02, 9.4761e-07, 2.7584e-02, 4.8037e-07, 2.5695e-02, 6.6042e-07,
        2.7285e-07, 4.6921e-07, 1.5402e-06, 9.8526e-07, 1.0923e-02, 3.4595e-07,
        2.5054e-02, 4.1343e-07, 4.2151e-02, 6.7221e-07, 3.2735e-02, 4.1343e-07,
        4.7457e-07, 6.1891e-07, 4.6311e-02, 5.0377e-07, 1.6289e-02, 3.7431e-05,
        4.0897e-02, 1.0216e-06, 5.8759e-07, 1.0780e-06, 5.4364e-07, 8.5072e-07,
        2.1073e-02, 4.6921e-07, 2.0962e-07, 1.6838e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.3148e-02, 1.2430e-06, 5.1712e-02, 6.0054e-02, 4.7890e-02, 6.9546e-02,
        6.5754e-02, 1.7332e-06, 3.7496e-02, 6.3710e-02, 1.1795e-06, 3.7027e-07,
        1.0294e-06, 5.7526e-02, 6.3038e-02, 1.6907e-06, 6.8508e-02, 6.7443e-07,
        6.5835e-07, 4.8182e-02, 5.4352e-07, 5.5271e-02, 1.7840e-06, 1.7014e-06,
        7.0665e-02, 6.5835e-07, 7.6667e-02, 6.0301e-02, 5.8523e-02, 1.6907e-06,
        1.1795e-06, 1.6593e-06, 1.1913e-06, 5.2238e-02, 6.5444e-02, 5.9608e-02,
        3.7027e-07, 7.2306e-02, 5.8936e-02, 1.7014e-06, 6.4923e-02, 3.7027e-07,
        1.6473e-06, 5.4783e-07, 6.3247e-02, 8.9083e-07, 8.3374e-02, 5.7716e-07,
        1.3484e-06, 6.6675e-02, 5.7716e-07, 8.9083e-07, 5.5929e-02, 4.9160e-07,
        5.6467e-07, 1.7840e-06, 6.2508e-02, 6.6117e-02, 1.6593e-06, 6.5835e-07,
        7.0488e-07, 5.9685e-02, 1.1795e-06, 7.8293e-07, 5.7716e-07, 8.3907e-07,
        6.7525e-02, 4.7330e-02, 1.6907e-06, 7.0434e-02, 3.5044e-07, 5.4604e-07,
        1.3432e-06, 6.4409e-02, 5.7949e-02, 5.4783e-07, 1.6593e-06, 1.6907e-06,
        3.0305e-06, 1.7332e-06, 8.1270e-07, 7.1970e-02, 4.9160e-07, 5.8301e-02,
        9.2310e-07, 8.1033e-07, 6.2427e-02, 6.3753e-02, 1.7332e-06, 1.6593e-06,
        3.0305e-06, 1.7840e-06, 3.0305e-06, 5.6467e-07, 6.6330e-02, 5.4783e-07,
        3.8938e-07, 1.3435e-06, 1.1795e-06, 7.9829e-02, 3.2425e-02, 6.3155e-02,
        3.0305e-06, 1.1795e-06, 5.6142e-02, 8.1033e-07, 5.4783e-07, 6.1415e-02,
        6.5011e-02, 1.2964e-06, 6.6565e-02, 7.8567e-02, 2.4526e-06, 5.7716e-07,
        1.2430e-06, 6.2809e-02, 1.5219e-06, 5.9232e-02, 2.6843e-06, 5.7771e-02,
        6.0154e-02, 1.7332e-06, 8.3907e-07, 1.1795e-06, 1.7332e-06, 5.1310e-02,
        5.6694e-07, 1.6473e-06, 2.4526e-06, 7.1830e-07, 8.1033e-07, 8.6638e-07,
        5.4352e-07, 7.3833e-02, 3.3905e-07, 6.7443e-07, 1.3487e-06, 6.7443e-07,
        7.5047e-02, 5.6608e-02, 1.2430e-06, 3.8938e-07, 1.1795e-06, 4.5925e-07,
        2.1012e-06, 5.5546e-02, 5.2808e-07, 6.4717e-02, 5.8996e-02, 7.1607e-02,
        6.7423e-02, 4.5551e-02, 8.4258e-02, 8.6638e-07, 5.4783e-07, 3.7027e-07,
        4.6901e-02, 5.6185e-02, 5.6694e-07, 1.7332e-06, 3.8938e-07, 5.4783e-07,
        6.6138e-02, 6.7705e-02, 6.5835e-07, 6.6440e-02, 6.6946e-02, 1.7840e-06,
        1.6473e-06, 8.1270e-07, 5.4783e-07, 5.4352e-07, 1.2964e-06, 1.1795e-06,
        6.6236e-02, 5.2154e-02, 8.1033e-07, 7.7494e-02, 1.2964e-06, 4.9160e-07,
        1.6593e-06, 3.8938e-07, 4.5422e-02, 3.2513e-02, 1.2935e-06, 8.3907e-07,
        1.1795e-06, 1.7332e-06, 1.6593e-06, 3.3905e-07, 2.6155e-06, 5.6403e-02,
        1.6907e-06, 1.4277e-06, 3.5345e-02, 8.3907e-07, 3.0305e-06, 2.3720e-06,
        7.1002e-02, 7.4523e-02, 6.2069e-02, 3.7027e-07, 4.9374e-02, 1.7332e-06,
        5.7716e-07, 8.3179e-02, 1.0294e-06, 1.1795e-06, 7.5678e-02, 3.7027e-07,
        6.2269e-02, 1.7332e-06, 1.7332e-06, 6.3128e-02, 3.3905e-07, 1.7840e-06,
        9.7471e-07, 5.2493e-02, 5.9448e-02, 5.7716e-07, 7.1631e-02, 5.4783e-07,
        5.4353e-07, 3.9098e-02, 5.4783e-07, 7.1044e-02, 1.1795e-06, 1.6593e-06,
        1.6593e-06, 5.4352e-07, 6.9430e-02, 4.9160e-07, 5.4783e-07, 8.9083e-07,
        4.7471e-08, 1.2964e-06, 7.4668e-02, 1.6907e-06, 7.0422e-02, 1.3433e-06,
        6.1946e-02, 6.4643e-02, 3.0305e-06, 7.3041e-02, 4.7109e-02, 1.6593e-06,
        8.9083e-07, 8.2924e-07, 4.7978e-02, 5.7716e-07, 6.1131e-02, 1.7332e-06,
        8.3907e-07, 1.6473e-06, 1.6907e-06, 3.5044e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.4117e-08, 9.1156e-08, 4.5343e-02,  ..., 4.3563e-02, 6.0910e-02,
        4.6822e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.5636e-07, 6.2017e-02, 3.2013e-07, 5.3319e-02, 3.0263e-07, 5.8840e-02,
        5.4676e-02, 7.4064e-07, 6.5163e-07, 7.0905e-07, 6.1356e-02, 5.5582e-02,
        6.2108e-02, 7.0905e-07, 6.2369e-02, 5.6776e-02, 5.8980e-02, 2.6126e-07,
        5.8893e-02, 9.2767e-07, 5.6506e-02, 5.3086e-02, 2.5800e-07, 3.1127e-07,
        4.4310e-07, 5.4127e-02, 3.8628e-07, 6.3735e-02, 5.5754e-02, 6.0313e-02,
        6.3416e-02, 5.6778e-02, 2.8185e-08, 5.9206e-02, 2.6126e-07, 1.9876e-07,
        5.4942e-02, 9.1916e-07, 6.4987e-02, 4.8500e-02, 5.6268e-02, 5.0950e-02,
        6.0510e-02, 3.8691e-07, 4.3496e-07, 5.6241e-02, 4.3016e-02, 4.6736e-02,
        7.8786e-07, 4.9389e-02, 4.8441e-02, 5.0983e-02, 6.6602e-02, 6.0846e-02,
        1.2963e-06, 3.0386e-07, 5.3677e-02, 5.1321e-02, 1.0733e-06, 4.8473e-07,
        6.5970e-02, 5.9454e-02, 4.4310e-07, 6.5163e-07, 5.5050e-02, 1.2444e-07,
        6.8254e-07, 7.1994e-07, 3.8691e-07, 5.4220e-02, 3.0926e-07, 1.4272e-06,
        5.8728e-02, 5.5589e-02, 6.1179e-02, 5.4037e-02, 3.6922e-02, 3.8205e-02,
        5.2465e-02, 4.8473e-07, 5.4366e-02, 3.8691e-07, 5.3449e-02, 5.3826e-07,
        5.2064e-02, 3.5636e-07, 5.2896e-02, 6.1858e-02, 5.6143e-02, 4.6406e-07,
        1.4201e-06, 1.2054e-06, 4.9259e-02, 5.3224e-02, 9.5509e-07, 5.1424e-02,
        2.6126e-07, 5.5694e-02, 5.9651e-02, 3.0926e-07, 5.6623e-02, 1.2054e-06,
        6.7897e-07, 5.6231e-02, 5.7309e-02, 5.2860e-02, 3.0926e-07, 2.8185e-08,
        6.0238e-02, 5.2973e-02, 4.4502e-02, 5.1743e-02, 5.1208e-02, 5.1485e-02,
        7.4064e-07, 4.4915e-02, 2.5800e-07, 8.9267e-07, 6.0649e-02, 5.0872e-02,
        5.5371e-02, 2.3398e-07, 3.6323e-07, 3.8691e-07, 1.0733e-06, 6.2288e-02,
        4.8017e-07, 6.1069e-02, 4.8849e-07, 5.4956e-07, 6.7318e-02, 2.5194e-07,
        2.6126e-07, 5.5357e-02, 5.6268e-02, 5.6323e-02, 1.5277e-07, 6.8663e-02,
        5.5947e-02, 5.7988e-02, 5.3345e-07, 4.8849e-07, 5.4956e-07, 1.3797e-06,
        6.7570e-02, 7.1994e-07, 5.0276e-07, 3.0926e-07, 5.6555e-02, 5.5988e-07,
        5.0276e-07, 3.0926e-07, 4.9497e-02, 5.6513e-07, 3.1127e-07, 5.8161e-02,
        5.9357e-02, 5.6994e-02, 5.8118e-02, 5.3457e-02, 2.6126e-07, 6.5163e-07,
        6.0163e-02, 5.4895e-02, 4.8341e-02, 5.2121e-02, 1.0383e-06, 6.1058e-02,
        5.6977e-02, 9.5509e-07, 6.0016e-02, 3.8691e-07, 5.1095e-07, 1.5277e-07,
        2.6126e-07, 5.9756e-02, 2.5800e-07, 7.0905e-07, 3.1139e-07, 4.8473e-07,
        5.5747e-02, 2.0421e-07, 1.1061e-06, 6.1043e-02, 5.4940e-02, 4.8873e-07,
        7.1994e-07, 1.0733e-06, 1.1061e-06, 6.0725e-02, 6.2311e-02, 1.0995e-07,
        6.0170e-02, 5.5988e-07, 5.8193e-07, 4.0102e-07, 5.5904e-02, 3.5298e-07,
        5.8799e-07, 6.2296e-02, 6.7397e-02, 5.0886e-02, 4.6020e-02, 5.1857e-02,
        5.5687e-02, 5.7164e-02, 6.8705e-02, 5.3826e-07, 5.3263e-02, 6.6086e-02,
        1.2996e-01, 5.6764e-02, 9.1916e-07, 6.0267e-02, 5.0887e-02, 5.1320e-07,
        5.1642e-02, 5.9975e-07, 6.2599e-02, 5.7720e-02, 4.4195e-02, 5.7442e-02,
        5.4005e-02, 8.2932e-07, 5.6166e-02, 3.8691e-07, 4.9646e-02, 5.2703e-02,
        3.5959e-07, 9.2230e-07, 4.6866e-02, 1.2444e-07, 6.2948e-02, 5.1714e-02,
        6.0342e-02, 6.1903e-02, 1.4272e-06, 6.1412e-02, 1.2054e-06, 3.5636e-07,
        5.8269e-02, 6.3885e-02, 6.4072e-02, 5.6529e-02, 6.1144e-07, 4.5873e-02,
        2.3508e-07, 6.3413e-02, 7.5678e-07, 5.5463e-02, 6.0648e-02, 5.1694e-02,
        2.8788e-07, 4.9569e-07, 3.2013e-07, 3.0926e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.4087e-06, 1.4301e-06, 2.7762e-06, 1.1933e-07, 3.2742e-02, 2.3626e-06,
        5.0048e-06, 3.3173e-06, 9.2250e-02, 4.0118e-07, 7.9499e-02, 3.9879e-02,
        2.1976e-06, 4.0889e-06, 4.7375e-06, 2.5963e-06, 1.2099e-06, 7.7860e-02,
        3.9297e-02, 2.5089e-06, 3.5810e-02, 8.2013e-02, 2.0687e-06, 4.1180e-02,
        1.4087e-06, 9.9512e-07, 1.4785e-06, 9.7523e-02, 3.7783e-06, 2.6143e-06,
        2.3782e-06, 9.2414e-08, 7.7948e-07, 3.7181e-02, 5.2547e-07, 7.1133e-02,
        7.5308e-02, 3.8992e-06, 1.9085e-06, 1.8205e-06, 7.8628e-02, 1.7717e-06,
        1.3094e-06, 2.0572e-06, 4.0158e-06, 2.5236e-06, 1.3920e-06, 4.2516e-02,
        4.7012e-02, 1.3467e-06, 1.0182e-01, 8.8539e-07, 3.1934e-07, 1.0312e-06,
        3.1788e-06, 2.2960e-06, 4.8744e-06, 3.8992e-06, 3.7678e-02, 9.6105e-02,
        4.3830e-02, 9.6594e-02, 1.2938e-06, 2.3124e-06, 3.6767e-02, 1.0851e-06,
        3.5211e-06, 3.6904e-02, 4.0118e-07, 5.9023e-06, 1.0374e-06, 6.8879e-02,
        6.9708e-06, 1.6932e-06, 1.7295e-06, 2.2729e-06, 7.8351e-02, 8.0614e-02,
        9.4866e-02, 3.2941e-06, 7.3985e-02, 2.5073e-06, 1.4785e-06, 3.6433e-02,
        1.6514e-06, 1.4785e-06, 1.2938e-06, 3.8480e-02, 2.3627e-06, 4.1750e-02,
        8.3541e-02, 4.0082e-02, 3.6632e-02, 3.0716e-06, 2.3835e-06, 8.3441e-02,
        2.7106e-02, 3.1788e-06, 3.5896e-06, 2.4437e-06, 8.5477e-02, 4.1045e-06,
        7.1089e-02, 2.0226e-06, 4.7962e-06, 7.7948e-07, 3.2797e-06, 3.5211e-06,
        2.7019e-06, 1.2938e-06, 1.5248e-06, 2.0608e-06, 1.7295e-06, 7.7274e-02,
        1.9391e-06, 7.7948e-07, 3.9493e-02, 4.0900e-02, 7.8036e-02, 3.3573e-06,
        2.2715e-06, 1.7295e-06, 2.4012e-06, 1.3785e-06, 6.2094e-02, 8.3919e-07,
        2.9160e-06, 8.7910e-02, 4.7375e-06, 4.0118e-07, 1.6421e-06, 2.8623e-06,
        4.7375e-06, 3.9841e-02, 1.3467e-06, 2.9630e-06, 7.2918e-02, 3.8992e-06,
        1.0662e-06, 5.0048e-06, 1.5556e-06, 3.6840e-02, 1.9754e-06, 1.3728e-06,
        1.6045e-06, 1.9391e-06, 3.5787e-02, 6.5698e-02, 1.0543e-06, 1.2573e-06,
        1.2828e-01, 4.6875e-06, 5.0048e-06, 1.7295e-06, 1.0040e-06, 1.4087e-06,
        3.7948e-02, 2.1976e-06, 6.9442e-02, 2.0608e-06, 3.0169e-06, 2.0701e-06,
        9.6113e-07, 6.4188e-07, 2.5898e-06, 2.6666e-06, 1.4785e-06, 4.0109e-06,
        3.8166e-02, 4.1785e-02, 3.4872e-06, 3.1934e-07, 7.6792e-02, 1.6514e-06,
        8.0452e-02, 1.7126e-06, 1.4429e-06, 9.9710e-07, 1.2528e-06, 4.1732e-02,
        8.3728e-07, 8.0209e-02, 2.0637e-06, 4.2910e-06, 1.6170e-06, 1.0173e-06,
        2.2324e-06, 7.3572e-07, 4.0285e-02, 2.3835e-06, 7.5243e-02, 3.0169e-06,
        1.5586e-06, 5.0048e-06, 2.5279e-06, 3.9519e-02, 9.9115e-07, 7.6882e-02,
        2.2960e-06, 4.1646e-02, 8.4075e-02, 1.8397e-06, 2.9129e-06, 3.5392e-02,
        4.0723e-02, 1.0023e-01, 3.5683e-02, 3.8811e-02, 1.1042e-06, 3.9431e-02,
        1.8413e-06, 1.1306e-01, 1.4879e-06, 2.0271e-06, 1.3592e-06, 3.5816e-02,
        3.0716e-06, 3.4551e-06, 7.4895e-07, 1.6932e-06, 7.5324e-02, 8.9757e-02,
        2.1112e-07, 3.4343e-06, 2.3835e-06, 3.4109e-02, 2.0420e-06, 1.1241e-01,
        8.1894e-02, 2.9630e-06, 4.9238e-02, 2.9374e-02, 1.0040e-06, 2.0701e-06,
        2.0855e-06, 3.9151e-02, 2.5898e-06, 1.1134e-06, 2.1871e-06, 2.1871e-06,
        8.8451e-07, 3.5923e-06, 3.0169e-06, 2.4415e-06, 3.0169e-06, 2.3051e-06,
        7.9464e-02, 6.8676e-07, 1.2938e-06, 6.8326e-02, 3.0169e-06, 1.5870e-06,
        3.9993e-02, 1.8615e-06, 3.4343e-06, 3.7783e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.6209e-08, 1.4713e-07, 2.9633e-02,  ..., 3.7052e-02, 6.9743e-02,
        1.8918e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.4790e-07, 2.8961e-07, 6.6215e-07, 1.1535e-06, 1.3573e-06, 1.7306e-06,
        1.2478e-06, 1.1560e-01, 1.4149e-06, 9.4399e-07, 1.2031e-01, 1.2887e-06,
        2.6271e-06, 7.4589e-07, 4.3170e-07, 8.9553e-02, 1.0302e-06, 8.7574e-07,
        1.7643e-06, 6.7905e-07, 1.2785e-06, 3.2666e-02, 9.0087e-07, 8.2070e-02,
        5.0882e-07, 6.4144e-07, 1.2887e-06, 2.5582e-08, 1.0432e-06, 5.3579e-07,
        1.3557e-06, 5.5810e-07, 1.0238e-06, 2.0422e-06, 7.7381e-07, 7.8745e-07,
        4.1224e-04, 8.5780e-07, 4.2963e-07, 1.3236e-06, 1.6058e-06, 4.3865e-07,
        4.2963e-07, 5.6900e-07, 1.8961e-06, 1.7306e-06, 8.2883e-08, 3.9933e-07,
        4.3290e-07, 1.2635e-06, 3.9676e-07, 1.3236e-06, 1.1748e-06, 3.4163e-02,
        8.9267e-07, 1.8897e-06, 8.0268e-07, 9.4790e-07, 1.1918e-06, 6.6596e-07,
        1.3155e-06, 9.1036e-07, 4.7440e-07, 1.1050e-06, 9.5200e-02, 5.8942e-08,
        7.1597e-07, 1.2295e-06, 6.6763e-07, 1.5361e-06, 7.9886e-02, 9.8199e-02,
        1.1240e-06, 1.7306e-06, 7.8729e-07, 6.3760e-07, 9.7750e-07, 5.9428e-07,
        1.1162e-06, 6.7590e-07, 1.2635e-06, 2.3949e-02, 1.7387e-07, 7.8830e-08,
        3.8286e-07, 9.1151e-07, 1.2325e-06, 8.7185e-07, 7.4121e-02, 1.0815e-06,
        7.6412e-07, 1.4745e-06, 7.8208e-07, 9.3127e-02, 1.6891e-06, 8.1832e-07,
        1.3493e-06, 4.3235e-08, 8.8086e-07, 3.5272e-02, 5.5337e-07, 1.2560e-06,
        1.4906e-06, 6.0829e-07, 1.2478e-06, 1.1151e-02, 1.3006e-06, 1.6286e-06,
        7.4479e-02, 1.4149e-06, 4.3290e-07, 8.8876e-02, 8.3244e-07, 1.3459e-06,
        6.8712e-03, 3.4274e-07, 4.8355e-03, 8.1832e-07, 1.1269e-06, 9.0204e-07,
        5.8247e-07, 7.5169e-03, 1.4793e-06, 1.2908e-06, 6.7905e-07, 1.0649e-06,
        8.9775e-07, 1.4304e-06, 1.5445e-06, 6.3760e-07, 1.3238e-06, 1.9828e-07,
        4.2060e-07, 3.2011e-07, 8.1832e-07, 2.0103e-06, 8.6186e-07, 7.4564e-07,
        1.6797e-06, 1.3579e-06, 1.1169e-01, 2.2479e-02, 1.4528e-06, 9.7759e-02,
        1.1786e-06, 3.0652e-06, 6.3760e-07, 2.3348e-06, 4.5475e-07, 8.7409e-07,
        4.6811e-07, 2.0165e-06, 5.0702e-07, 1.4793e-06, 2.7687e-07, 6.3760e-07,
        2.1573e-06, 2.8884e-02, 8.9268e-07, 2.0029e-06, 8.7574e-07, 6.3760e-07,
        1.1789e-06, 2.9265e-02, 8.7185e-07, 1.7387e-07, 1.1748e-06, 1.0665e-06,
        1.8630e-06, 1.4191e-06, 1.2848e-06, 8.3233e-07, 1.3236e-06, 1.9464e-06,
        4.7611e-07, 4.5608e-07, 6.5758e-07, 1.3926e-06, 4.6560e-05, 1.2294e-06,
        1.1275e-06, 1.3822e-06, 1.1240e-06, 7.9787e-07, 4.8164e-07, 2.3552e-02,
        1.1866e-06, 4.3743e-07, 1.1046e-06, 2.6806e-07, 4.3290e-07, 1.8659e-02,
        2.4202e-02, 6.9705e-07, 2.4350e-07, 1.6383e-06, 1.1120e-06, 7.7312e-07,
        2.9981e-02, 6.6054e-07, 1.6116e-06, 8.2713e-03, 6.4144e-07, 6.6994e-07,
        2.9183e-03, 7.9223e-07, 8.4050e-02, 4.2963e-07, 8.7409e-07, 8.8337e-07,
        1.8206e-02, 1.4745e-06, 4.5608e-07, 1.3236e-06, 1.0767e-01, 5.9428e-07,
        9.5970e-07, 1.0649e-06, 1.1946e-01, 4.3237e-08, 4.3290e-07, 8.2621e-02,
        1.5411e-06, 7.5730e-02, 1.2740e-06, 1.1866e-06, 1.5669e-06, 4.5608e-07,
        3.3924e-02, 1.9660e-06, 6.3760e-07, 5.9428e-07, 6.7388e-07, 8.1195e-07,
        5.8496e-07, 8.4705e-02, 1.7387e-07, 1.0153e-01, 8.5370e-07, 1.5425e-06,
        8.7185e-07, 4.7846e-07, 7.9426e-07, 7.4589e-07, 9.0196e-03, 1.8630e-06,
        1.3926e-06, 9.8542e-02, 4.5473e-07, 8.7301e-02, 6.3760e-07, 1.0665e-06,
        1.3097e-06, 3.3309e-07, 1.3955e-06, 8.3233e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.2601e-06, 2.0398e-06, 2.8362e-06, 3.8914e-06, 6.6945e-06, 6.9841e-06,
        4.2421e-06, 3.0862e-06, 2.5832e-06, 5.6680e-06, 4.5172e-06, 2.2491e-06,
        5.9637e-07, 2.5832e-06, 2.7250e-06, 4.0085e-06, 2.0193e-06, 3.0612e-06,
        4.9520e-06, 3.1823e-06, 3.2745e-06, 6.7075e-07, 2.1407e-06, 1.5935e-06,
        5.3986e-06, 2.6302e-03, 8.3788e-07, 1.4932e-03, 3.6992e-06, 3.0696e-06,
        9.1113e-02, 5.8777e-06, 4.6674e-06, 9.7699e-02, 2.7179e-06, 1.4433e-06,
        1.9237e-06, 2.5832e-06, 3.1134e-06, 1.6958e-06, 1.2859e-06, 5.9637e-07,
        2.4193e-06, 3.0986e-06, 3.3658e-07, 6.4507e-06, 2.4108e-06, 3.4882e-06,
        1.7417e-06, 9.6472e-02, 1.1130e-06, 1.1130e-06, 1.5378e-06, 9.8915e-07,
        1.7417e-06, 3.0726e-06, 4.4647e-06, 4.3438e-06, 2.6311e-06, 8.6593e-02,
        1.6220e-06, 4.3438e-06, 2.4590e-06, 8.7116e-07, 9.5114e-03, 3.5603e-06,
        5.2331e-06, 2.5632e-06, 3.1911e-06, 2.6814e-06, 5.0513e-06, 1.1183e-06,
        7.7152e-07, 1.5997e-06, 1.6992e-06, 3.8933e-06, 1.2336e-06, 2.5604e-06,
        1.8185e-06, 3.1907e-06, 1.7098e-06, 5.4598e-06, 4.2469e-06, 6.7005e-02,
        1.9659e-06, 2.3351e-06, 2.1573e-06, 1.7994e-06, 8.4794e-02, 4.3438e-06,
        2.6873e-06, 3.2135e-06, 1.6523e-06, 1.8690e-06, 1.0120e-06, 1.8473e-06,
        1.9659e-06, 3.3999e-06, 9.6191e-02, 1.2032e-03, 4.0730e-06, 7.7152e-07,
        3.4116e-06, 2.2757e-06, 2.9336e-06, 3.2206e-03, 2.9990e-06, 4.6087e-06,
        1.9771e-06, 3.7989e-06, 2.2055e-06, 1.5530e-06, 2.7064e-06, 5.2933e-06,
        2.4193e-06, 2.1857e-06, 2.4043e-06, 4.2019e-06, 2.2757e-06, 1.7707e-06,
        1.5567e-06, 1.7098e-06, 4.1094e-06, 3.6675e-06, 4.0399e-06, 1.2319e-06,
        1.1484e-06, 4.2611e-06, 8.7248e-07, 2.6699e-06, 5.6746e-05, 2.5826e-06,
        1.0817e-07, 3.6908e-06, 1.8185e-06, 1.2996e-06, 3.9387e-06, 3.6992e-06,
        5.2536e-06, 1.7831e-06, 1.1169e-06, 2.0524e-06, 2.8362e-06, 4.7167e-06,
        7.7449e-03, 6.2114e-06, 3.8147e-06, 2.2757e-06, 1.5151e-06, 3.2045e-06,
        3.7661e-06, 2.5832e-06, 1.3679e-06, 3.4549e-06, 1.4494e-06, 3.1823e-06,
        2.5638e-06, 2.4032e-06, 2.6814e-06, 2.7064e-06, 2.4525e-06, 1.3120e-06,
        1.5997e-06, 2.2919e-06, 1.4638e-06, 1.5447e-06, 2.4372e-06, 7.8237e-06,
        3.6686e-06, 4.3336e-06, 1.6958e-06, 3.4839e-06, 1.7126e-06, 8.1809e-07,
        1.8301e-06, 1.6716e-06, 4.4524e-06, 3.2836e-03, 2.5196e-06, 5.9637e-07,
        4.1285e-06, 2.0857e-06, 4.1285e-06, 3.1823e-06, 7.5521e-06, 2.0606e-06,
        2.2986e-06, 1.2401e-06, 6.1920e-02, 4.3438e-06, 1.2401e-06, 4.6906e-06,
        1.5151e-06, 3.1207e-06, 2.5826e-06, 5.3976e-06, 6.4359e-02, 8.9581e-07,
        3.3758e-06, 1.7393e-06, 6.1683e-02, 1.6279e-06, 1.0303e-06, 1.7393e-06,
        3.1363e-06, 4.7167e-06, 1.0724e-06, 4.5393e-06, 4.6660e-06, 3.5565e-06,
        2.0425e-06, 8.3436e-02, 5.2890e-06, 3.5458e-06, 8.8424e-07, 2.4939e-06,
        3.3412e-06, 9.8587e-04, 1.5504e-06, 3.3722e-06, 5.1469e-06, 1.2336e-06,
        3.8933e-06, 6.2072e-06, 1.9645e-06, 2.6814e-06, 1.3679e-06, 4.5725e-06,
        3.0353e-07, 6.3935e-06, 2.4174e-06, 4.3438e-06, 3.6716e-06, 8.7091e-03,
        1.2859e-06, 2.2757e-06, 2.6163e-06, 3.0726e-06, 6.0320e-07, 2.8363e-06,
        1.0273e-06, 3.8677e-06, 3.4582e-06, 9.4872e-07, 2.9101e-06, 3.2933e-06,
        1.6830e-06, 2.7312e-06, 5.8777e-06, 8.1243e-02, 1.3120e-06, 2.2919e-06,
        8.3788e-07, 3.3130e-06, 2.4939e-06, 2.3679e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.6923e-01, 3.4330e-07, 1.0555e-07,  ..., 2.9785e-07, 6.3100e-08,
        1.4110e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7318e-06, 9.2092e-07, 3.8152e-07, 7.6449e-07, 1.3446e-06, 6.0613e-07,
        7.6449e-07, 1.2895e-06, 3.7330e-07, 1.0352e-01, 1.5522e-06, 1.0867e-01,
        7.1497e-07, 2.3514e-07, 1.0647e-07, 6.3075e-07, 2.8252e-02, 1.8430e-06,
        6.0446e-07, 7.1335e-07, 8.4037e-07, 1.2531e-06, 9.9268e-02, 1.2649e-06,
        2.0782e-06, 5.0214e-07, 2.8876e-07, 1.0056e-06, 7.3053e-07, 7.9770e-08,
        1.3909e-06, 4.9122e-07, 4.3585e-07, 5.0804e-07, 7.1497e-07, 7.9770e-08,
        6.8718e-07, 1.3446e-06, 8.6289e-02, 1.0639e-06, 6.6250e-07, 9.3659e-02,
        1.3363e-06, 9.0373e-07, 2.3514e-07, 7.3597e-07, 2.2797e-06, 8.3427e-07,
        6.2602e-07, 7.1131e-07, 1.3446e-06, 8.5731e-02, 6.0613e-07, 4.5234e-07,
        2.5840e-07, 9.4025e-07, 6.7868e-07, 3.7660e-07, 1.1243e-06, 1.2649e-06,
        4.5156e-02, 1.0736e-06, 6.3923e-07, 4.7549e-07, 9.2092e-07, 4.7541e-07,
        4.3585e-07, 3.6357e-07, 7.1901e-02, 6.3923e-07, 7.4829e-07, 1.6259e-06,
        1.3909e-06, 6.8718e-07, 9.4534e-07, 7.1335e-07, 2.8876e-07, 9.3250e-07,
        3.6357e-07, 7.9770e-08, 3.7984e-07, 3.2652e-07, 8.4081e-07, 9.6325e-07,
        1.2895e-06, 6.3923e-07, 7.0995e-07, 1.2557e-06, 6.3075e-07, 1.3446e-06,
        6.0613e-07, 6.1514e-07, 3.6357e-07, 8.6069e-07, 6.1514e-07, 4.7430e-07,
        8.6108e-07, 7.3053e-07, 2.8876e-07, 9.7372e-07, 1.2895e-06, 4.0897e-07,
        6.1514e-07, 8.6098e-07, 6.1514e-07, 1.8430e-06, 6.4544e-07, 6.3075e-07,
        1.0478e-01, 1.2895e-06, 8.4678e-07, 7.9770e-08, 7.6449e-07, 4.7042e-07,
        9.0391e-02, 8.6098e-07, 4.9122e-07, 7.1497e-07, 8.7428e-02, 8.4081e-07,
        1.2895e-06, 1.6002e-06, 1.3453e-06, 1.0477e-01, 1.2286e-01, 6.0198e-07,
        5.5227e-07, 6.6208e-07, 6.4199e-07, 6.8718e-07, 3.2652e-07, 1.3909e-06,
        9.2092e-07, 7.1335e-07, 4.7108e-07, 7.4002e-07, 1.5665e-06, 3.7330e-07,
        6.3923e-07, 9.4364e-02, 6.8718e-07, 8.1662e-07, 7.3053e-07, 7.9770e-08,
        3.6357e-07, 1.1856e-06, 7.7917e-07, 2.7771e-07, 1.2649e-06, 9.2435e-07,
        1.0189e-01, 1.0056e-06, 1.2895e-06, 1.0130e-06, 1.2895e-06, 2.3514e-07,
        6.8718e-07, 7.5072e-07, 7.6449e-07, 6.1514e-07, 5.3687e-07, 2.7771e-07,
        1.1200e-01, 2.3514e-07, 7.7917e-07, 2.5840e-07, 6.6819e-07, 1.2557e-06,
        2.9462e-07, 1.2895e-06, 6.3075e-07, 6.2602e-07, 6.4199e-07, 1.2895e-06,
        6.5029e-07, 4.3904e-07, 1.0056e-06, 6.6208e-07, 8.9509e-02, 7.3053e-07,
        5.4635e-07, 6.5029e-07, 3.9231e-02, 1.0926e-01, 2.8876e-07, 5.0705e-07,
        1.1755e-01, 1.1235e-06, 6.8718e-07, 9.8972e-07, 1.2649e-06, 3.7660e-07,
        1.6282e-06, 7.5072e-07, 1.6925e-07, 6.4199e-07, 7.1497e-07, 1.9794e-07,
        6.3923e-07, 1.0736e-06, 1.0678e-01, 9.3017e-07, 7.6449e-07, 2.0370e-06,
        8.4679e-07, 1.2599e-01, 7.5072e-07, 1.3363e-06, 4.2310e-07, 3.7984e-07,
        1.2853e-06, 6.6849e-02, 1.5159e-06, 6.1514e-07, 1.7925e-06, 6.8189e-07,
        7.8594e-07, 1.0842e-06, 7.1497e-07, 1.0056e-06, 6.5029e-07, 2.2517e-06,
        1.3584e-06, 9.2848e-02, 1.2895e-06, 7.6449e-07, 7.5072e-07, 5.4751e-07,
        6.3923e-07, 9.7372e-07, 3.8599e-07, 2.5840e-07, 7.3053e-07, 1.2611e-01,
        1.2649e-06, 7.8875e-07, 1.2895e-06, 4.8577e-07, 4.7108e-07, 7.9770e-08,
        9.2783e-07, 1.8358e-06, 6.3923e-07, 4.7108e-07, 5.4751e-07, 8.6098e-07,
        6.1514e-07, 8.6098e-07, 4.7888e-07, 1.9772e-06, 6.3075e-07, 3.4310e-02,
        1.3981e-06, 3.7330e-07, 6.3075e-07, 4.7108e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.8506e-06, 2.6415e-06, 1.4698e-06, 9.3019e-07, 2.3872e-06, 2.5746e-06,
        1.5808e-06, 3.3697e-06, 1.3717e-06, 4.9770e-06, 7.4522e-07, 3.9879e-06,
        3.5711e-06, 2.9621e-06, 2.5991e-06, 8.7558e-07, 1.4682e-06, 8.6270e-07,
        1.7662e-06, 3.9879e-06, 6.3816e-07, 1.3390e-06, 2.5035e-06, 8.0340e-02,
        8.3255e-07, 6.4171e-06, 2.4464e-06, 1.6735e-06, 1.9771e-06, 3.8995e-06,
        1.6591e-06, 8.1819e-07, 2.8456e-06, 4.2968e-06, 3.0309e-06, 2.1009e-06,
        4.7095e-06, 4.5969e-06, 2.6415e-06, 3.3944e-07, 1.7263e-06, 2.3695e-06,
        5.3264e-07, 1.9992e-06, 3.0769e-06, 1.6249e-06, 4.3844e-06, 1.8508e-06,
        1.0788e-06, 3.4127e-06, 8.5068e-02, 1.8716e-06, 3.5338e-06, 2.2783e-06,
        1.3548e-06, 1.2707e-06, 3.1241e-06, 3.7559e-06, 3.0977e-07, 4.7391e-06,
        1.9393e-06, 1.6832e-06, 3.5711e-06, 1.6499e-06, 2.3695e-06, 4.7686e-06,
        1.9638e-06, 2.8832e-06, 1.2454e-06, 2.8958e-06, 1.0892e-06, 1.0788e-06,
        1.3068e-06, 2.0269e-07, 1.7024e-06, 2.0302e-06, 1.5654e-06, 2.9996e-06,
        1.4762e-06, 2.7197e-06, 3.9879e-06, 6.4171e-06, 2.2299e-06, 1.7263e-06,
        1.7263e-06, 2.0384e-06, 6.8878e-07, 8.8356e-02, 1.0600e-06, 1.2707e-06,
        1.6832e-06, 1.1884e-06, 1.4698e-06, 1.3361e-06, 1.7195e-06, 2.8832e-06,
        3.3623e-06, 1.4719e-06, 2.7355e-06, 1.1690e-06, 4.5730e-07, 2.5554e-06,
        3.2743e-06, 3.1241e-06, 3.4127e-06, 2.4805e-06, 2.9621e-06, 1.0600e-06,
        3.9304e-06, 3.1114e-06, 1.0917e-06, 2.1360e-06, 2.8958e-06, 4.9751e-06,
        1.1063e-06, 1.1063e-06, 2.6883e-06, 2.2632e-06, 2.5035e-06, 2.4945e-06,
        1.6499e-06, 1.9130e-06, 6.1788e-07, 2.8832e-06, 3.4280e-06, 1.9500e-06,
        3.5988e-06, 3.1255e-06, 9.8973e-07, 9.4038e-02, 3.0593e-06, 1.4698e-06,
        4.9751e-06, 3.5711e-06, 4.5969e-06, 1.6499e-06, 4.5428e-06, 1.0352e-01,
        1.3717e-06, 9.3019e-07, 1.3958e-06, 1.3137e-06, 2.1747e-06, 2.6254e-06,
        2.8958e-06, 2.8958e-06, 2.6478e-06, 2.1214e-06, 1.2094e-06, 1.1808e-06,
        1.0600e-06, 3.2583e-06, 1.2983e-06, 2.2777e-06, 9.9244e-02, 1.6774e-06,
        1.0600e-06, 2.8832e-06, 3.0248e-06, 3.2431e-06, 2.7898e-06, 1.2454e-06,
        2.1747e-06, 1.3513e-06, 1.2094e-06, 8.7665e-07, 1.3704e-06, 7.9944e-07,
        2.4659e-06, 5.9147e-06, 1.2743e-06, 1.8295e-06, 3.9304e-06, 1.9413e-06,
        1.1116e-06, 3.5338e-06, 3.0532e-06, 8.9667e-02, 1.8716e-06, 2.8958e-06,
        9.2164e-02, 8.3420e-07, 2.1938e-06, 4.7095e-06, 1.3137e-06, 2.4464e-06,
        2.9996e-06, 9.9809e-07, 2.3159e-02, 1.9771e-06, 2.4585e-06, 1.8264e-06,
        6.8290e-07, 3.5419e-06, 8.9011e-02, 2.1800e-06, 1.5073e-06, 1.3361e-06,
        1.0977e-06, 1.6499e-06, 3.2196e-06, 1.9551e-06, 4.7391e-06, 2.0302e-06,
        1.0600e-06, 2.5493e-06, 2.9745e-06, 1.6608e-06, 2.7181e-02, 1.8880e-06,
        4.1100e-06, 2.5351e-06, 4.5337e-06, 1.9771e-06, 1.2289e-07, 1.2707e-06,
        2.5045e-06, 1.8880e-06, 1.9794e-06, 3.5711e-06, 3.4581e-06, 9.5724e-02,
        1.3361e-06, 1.3704e-06, 3.6422e-06, 1.9771e-06, 2.5493e-06, 8.3420e-07,
        1.3994e-06, 1.0213e-06, 1.7195e-06, 2.6700e-06, 3.0672e-06, 1.8333e-06,
        2.5066e-06, 8.7951e-07, 8.3548e-02, 2.1160e-06, 2.8142e-06, 3.5003e-06,
        3.9304e-06, 3.0532e-06, 6.0548e-06, 3.1708e-06, 2.0090e-06, 1.0600e-06,
        9.9809e-07, 2.4659e-06, 5.6164e-07, 8.3255e-07, 1.4652e-06, 1.3022e-06,
        2.0127e-06, 2.9558e-06, 5.4445e-06, 3.0768e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.1147e-07, 2.4080e-07, 5.9005e-08,  ..., 1.8966e-07, 6.3604e-02,
        1.1254e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.9707e-07, 1.2045e-06, 3.1736e-07, 1.7321e-06, 1.0364e-06, 1.3005e-06,
        4.9707e-07, 1.2050e-06, 4.6191e-07, 5.3681e-07, 6.6449e-07, 3.1736e-07,
        1.8768e-07, 4.4060e-07, 2.2395e-06, 6.2068e-07, 1.2229e-06, 7.7485e-07,
        1.2019e-06, 3.0776e-07, 7.2515e-07, 1.4092e-06, 9.9846e-09, 6.6133e-07,
        4.4060e-07, 4.7391e-07, 4.2091e-07, 1.0959e-06, 6.2040e-07, 4.0725e-07,
        1.5812e-07, 1.3336e-07, 1.1791e-06, 7.6322e-07, 4.9607e-07, 4.9241e-07,
        1.4884e-06, 3.0776e-07, 9.7794e-07, 1.3886e-06, 6.2040e-07, 4.2526e-07,
        6.2063e-07, 5.9459e-07, 3.0494e-07, 3.0776e-07, 2.2375e-07, 8.3793e-07,
        5.0645e-07, 7.2093e-07, 2.1344e-07, 9.1500e-07, 9.0273e-08, 2.8792e-07,
        1.2050e-06, 4.0209e-07, 3.9341e-07, 8.8702e-07, 8.7348e-07, 6.6449e-07,
        1.0959e-06, 1.5319e-06, 4.4056e-07, 5.0230e-07, 9.6171e-07, 8.0831e-07,
        3.6795e-07, 4.4328e-07, 4.1726e-07, 2.1344e-07, 7.7485e-07, 2.4336e-07,
        4.6317e-07, 4.1866e-07, 2.1344e-07, 9.0273e-08, 4.1726e-07, 9.0273e-08,
        9.6171e-07, 1.2229e-06, 1.8514e-07, 7.3136e-07, 2.2375e-07, 7.4382e-07,
        4.2358e-07, 6.5012e-07, 6.6449e-07, 2.4336e-07, 1.1456e-06, 4.1940e-07,
        1.7307e-06, 9.0273e-08, 8.7348e-07, 9.0273e-08, 4.4056e-07, 1.4884e-06,
        7.2521e-07, 6.0427e-07, 1.5812e-07, 3.3754e-07, 7.8088e-07, 5.0230e-07,
        7.6322e-07, 4.1940e-07, 3.0776e-07, 2.2396e-06, 2.1344e-07, 4.7667e-07,
        4.1726e-07, 3.2185e-07, 1.1856e-06, 1.8863e-07, 2.8792e-07, 7.8088e-07,
        1.1379e-06, 5.1403e-07, 1.0562e-06, 1.1379e-06, 6.2856e-07, 7.8088e-07,
        2.2375e-07, 4.2389e-07, 4.2526e-07, 1.1238e-06, 8.9076e-07, 5.9459e-07,
        5.0645e-07, 3.0776e-07, 9.0273e-08, 1.0248e-06, 4.1726e-07, 6.6316e-07,
        4.9607e-07, 4.0209e-07, 2.8453e-07, 1.0959e-06, 2.8967e-07, 2.0779e-07,
        6.2040e-07, 8.4845e-07, 1.0179e-06, 2.1080e-06, 8.8435e-07, 4.1726e-07,
        7.7485e-07, 4.0667e-07, 3.0776e-07, 6.2077e-07, 1.0959e-06, 8.0839e-07,
        4.9707e-07, 5.4758e-07, 3.4260e-07, 3.4260e-07, 6.0985e-08, 5.8200e-07,
        2.8792e-07, 4.7667e-07, 5.9705e-07, 3.9012e-07, 4.1940e-07, 1.2050e-06,
        5.2933e-07, 9.2046e-07, 6.6133e-07, 6.5012e-07, 8.0081e-07, 1.4029e-06,
        8.8435e-07, 5.9131e-07, 5.5611e-07, 9.8533e-07, 1.1421e-06, 1.3005e-06,
        6.1261e-07, 5.9705e-07, 2.1344e-07, 4.4060e-07, 3.3876e-07, 1.5812e-07,
        8.8983e-07, 2.1080e-06, 5.5611e-07, 1.0248e-06, 1.1248e-06, 9.0463e-07,
        5.8953e-07, 2.7427e-07, 4.1940e-07, 9.6171e-07, 4.2526e-07, 1.1791e-06,
        1.1028e-06, 1.3005e-06, 4.1726e-07, 1.4884e-06, 1.0641e-06, 4.4056e-07,
        4.0209e-07, 7.5171e-07, 1.2050e-06, 3.4260e-07, 3.2566e-07, 7.8088e-07,
        4.4056e-07, 3.9012e-07, 1.7921e-06, 1.6564e-06, 1.1480e-06, 4.0667e-07,
        4.7667e-07, 1.1028e-06, 9.8533e-07, 1.8514e-07, 1.1791e-06, 1.1791e-06,
        5.9705e-07, 1.0641e-06, 1.2050e-06, 6.8658e-07, 1.3336e-07, 1.5472e-06,
        8.4484e-07, 3.7949e-07, 1.3005e-06, 2.8792e-07, 6.6449e-07, 1.6058e-06,
        4.1940e-07, 3.6359e-07, 4.4056e-07, 3.1736e-07, 1.3063e-06, 5.6704e-07,
        4.4060e-07, 8.0839e-07, 1.0959e-06, 6.6560e-07, 7.9479e-07, 2.8792e-07,
        8.0081e-07, 5.9855e-07, 9.7794e-07, 1.5812e-07, 1.3005e-06, 3.9012e-07,
        1.6586e-06, 4.9241e-07, 3.1736e-07, 6.2040e-07, 2.1344e-07, 7.9665e-07,
        2.5105e-07, 4.1961e-07, 1.0641e-06, 7.7485e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.5829e-06, 8.7189e-07, 3.9422e-06, 1.7327e-06, 1.0789e-06, 1.9630e-06,
        9.5022e-07, 3.3473e-06, 1.2559e-06, 2.1587e-06, 1.9700e-06, 2.3657e-06,
        1.2465e-06, 1.1130e-06, 1.6498e-06, 1.4380e-06, 2.0463e-06, 8.7189e-07,
        4.1749e-06, 2.6745e-06, 5.6269e-06, 2.6272e-06, 4.0684e-06, 2.6729e-06,
        1.7112e-06, 1.4804e-06, 2.1545e-06, 2.0843e-06, 2.4042e-06, 2.2031e-06,
        1.4380e-06, 1.8033e-06, 1.4184e-06, 1.9156e-06, 1.3757e-06, 3.6522e-06,
        3.7163e-06, 3.5846e-06, 5.1123e-06, 7.4795e-07, 4.1795e-06, 9.1627e-07,
        1.5416e-06, 1.0310e-06, 1.0348e-06, 1.5416e-06, 3.4169e-06, 1.0310e-06,
        1.3487e-06, 2.2781e-06, 3.0873e-07, 2.7175e-06, 1.5996e-06, 3.1073e-06,
        3.3746e-06, 3.1979e-06, 1.3487e-06, 1.5416e-06, 3.9422e-06, 3.1437e-06,
        2.6729e-06, 1.3487e-06, 2.6339e-06, 1.1543e-06, 4.7850e-06, 2.5150e-06,
        1.8033e-06, 8.1515e-07, 7.1450e-07, 3.1158e-06, 1.4657e-06, 1.8062e-06,
        3.1073e-06, 1.6599e-06, 3.4594e-06, 1.4241e-06, 1.8115e-06, 1.3849e-06,
        1.3777e-06, 1.0424e-06, 1.4456e-06, 2.3892e-06, 1.4380e-06, 6.6913e-07,
        2.1587e-06, 3.7798e-06, 2.1587e-06, 1.9413e-06, 3.9422e-06, 3.5829e-06,
        1.7431e-06, 1.9616e-06, 2.4115e-06, 4.8004e-07, 1.8033e-06, 2.8940e-06,
        1.8787e-06, 1.5408e-06, 3.5846e-06, 1.9252e-06, 1.2465e-06, 2.8297e-07,
        4.3598e-06, 1.3001e-06, 5.4320e-06, 8.8934e-07, 1.3487e-06, 2.1630e-06,
        9.2169e-07, 3.7798e-06, 3.2068e-06, 1.2465e-06, 1.9024e-06, 3.1437e-06,
        2.2020e-06, 1.8318e-06, 2.4042e-06, 1.4876e-06, 3.8216e-06, 2.0843e-06,
        1.6778e-06, 1.9630e-06, 5.8708e-06, 1.5416e-06, 1.8033e-06, 1.5416e-06,
        1.5408e-06, 3.5829e-06, 1.7691e-06, 1.5898e-06, 2.7547e-06, 3.2112e-06,
        2.4291e-06, 1.6323e-06, 1.7294e-06, 2.1805e-06, 4.9704e-06, 3.5256e-06,
        4.1183e-06, 1.8509e-06, 2.1505e-06, 1.3487e-06, 1.5538e-06, 1.7836e-06,
        1.9413e-06, 1.8049e-06, 1.0408e-06, 1.5530e-06, 1.7271e-06, 6.6425e-07,
        3.1673e-06, 1.0297e-06, 4.1795e-06, 2.3892e-06, 2.1587e-06, 1.0348e-06,
        3.0510e-06, 1.6323e-06, 3.3141e-06, 7.9196e-07, 1.0310e-06, 2.0897e-06,
        3.1979e-06, 1.2301e-06, 3.7144e-06, 9.5022e-07, 1.8455e-06, 1.3487e-06,
        3.8230e-06, 2.2989e-06, 1.2465e-06, 2.8690e-06, 1.4380e-06, 1.1684e-06,
        1.8033e-06, 6.6912e-07, 1.5587e-06, 2.6729e-06, 1.0348e-06, 2.0843e-06,
        1.3777e-06, 5.6399e-07, 1.2310e-07, 3.7798e-06, 1.0160e-06, 9.5022e-07,
        1.7431e-06, 4.0663e-06, 1.5482e-06, 4.0663e-06, 1.0789e-06, 1.2239e-06,
        2.1587e-06, 1.2476e-06, 6.2248e-07, 1.4143e-06, 1.9024e-06, 1.9918e-06,
        1.8033e-06, 1.2465e-06, 1.5482e-06, 4.7270e-06, 4.0457e-06, 5.4320e-06,
        2.2020e-06, 3.7469e-06, 3.3141e-06, 4.3171e-06, 7.9184e-07, 1.3776e-06,
        3.8964e-06, 5.5040e-06, 3.2068e-06, 2.6272e-06, 2.3789e-06, 2.1054e-06,
        1.4804e-06, 1.9156e-06, 7.7693e-07, 4.1795e-06, 1.3487e-06, 5.2216e-06,
        2.2329e-06, 1.5416e-06, 1.0995e-06, 2.6568e-06, 4.3668e-06, 2.1587e-06,
        1.7431e-06, 1.8062e-06, 2.2423e-06, 2.0962e-06, 1.2310e-07, 2.7175e-06,
        1.3074e-06, 7.6939e-07, 3.7163e-06, 1.7864e-06, 9.4223e-07, 4.8004e-07,
        3.3141e-06, 5.6414e-06, 9.2268e-07, 1.9156e-06, 2.2909e-06, 2.9680e-06,
        1.1076e-06, 4.1795e-06, 3.1979e-06, 7.5887e-07, 1.0134e-06, 1.5530e-06,
        2.2781e-06, 1.3487e-06, 2.3892e-06, 3.4515e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.6670e-08, 2.8265e-07, 6.2002e-09,  ..., 1.4595e-07, 5.0792e-08,
        1.0358e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.6343e-07, 6.1841e-07, 4.2330e-07, 2.9167e-07, 8.2288e-07, 6.5539e-07,
        2.7602e-07, 3.2140e-07, 3.5925e-07, 4.7357e-07, 5.4648e-07, 3.2140e-07,
        3.6343e-07, 7.9374e-07, 4.3874e-07, 2.5951e-07, 3.2140e-07, 1.6515e-06,
        3.2140e-07, 7.9374e-07, 3.9975e-07, 6.7953e-07, 5.4648e-07, 5.4648e-07,
        1.9522e-07, 3.9693e-07, 3.6343e-07, 2.0507e-07, 3.6899e-07, 4.9718e-07,
        5.4648e-07, 3.2140e-07, 3.2749e-07, 1.9300e-07, 7.0168e-07, 6.7953e-07,
        2.1021e-07, 4.2254e-07, 1.2702e-07, 2.0115e-07, 7.3079e-07, 6.1841e-07,
        6.1841e-07, 5.4648e-07, 1.2584e-06, 1.6643e-07, 6.0019e-07, 1.8493e-07,
        7.9374e-07, 4.6441e-07, 8.6182e-07, 1.6643e-07, 2.8041e-07, 2.7602e-07,
        2.5213e-07, 2.8051e-07, 2.9167e-07, 1.2571e-06, 1.2836e-06, 1.8348e-06,
        5.0250e-07, 6.4117e-07, 1.6515e-06, 5.4648e-07, 1.2702e-07, 4.6441e-07,
        2.3354e-07, 2.5951e-07, 5.0765e-07, 1.9522e-07, 3.9975e-07, 3.9693e-07,
        2.5914e-07, 6.1841e-07, 4.6441e-07, 2.8041e-07, 4.0439e-07, 4.5266e-07,
        2.9167e-07, 5.4738e-07, 7.2124e-07, 1.9522e-07, 7.8262e-07, 8.8078e-07,
        6.1841e-07, 4.6441e-07, 2.9167e-07, 3.5059e-07, 2.7976e-07, 9.6210e-08,
        3.6343e-07, 2.8041e-07, 3.2169e-07, 1.2702e-07, 4.2254e-07, 3.0423e-07,
        4.2254e-07, 7.9374e-07, 1.2782e-06, 4.2330e-07, 1.9522e-07, 3.9975e-07,
        5.4648e-07, 7.4224e-07, 2.7976e-07, 2.8041e-07, 2.8051e-07, 3.9693e-07,
        3.9975e-07, 4.6441e-07, 4.6675e-07, 3.5925e-07, 7.0168e-07, 4.2254e-07,
        7.3079e-07, 7.0168e-07, 2.5914e-07, 6.7953e-07, 3.2140e-07, 3.2749e-07,
        1.2784e-06, 1.2783e-06, 1.6643e-07, 5.6713e-07, 7.3079e-07, 3.3073e-07,
        3.2749e-07, 6.5539e-07, 4.6441e-07, 3.3073e-07, 2.0115e-07, 4.3316e-07,
        2.3354e-07, 2.5213e-07, 6.7953e-07, 4.9949e-07, 2.6974e-07, 7.9374e-07,
        2.8041e-07, 2.5213e-07, 1.9522e-07, 2.9167e-07, 4.9359e-07, 8.4156e-07,
        3.6265e-07, 2.8041e-07, 2.7976e-07, 3.6343e-07, 4.3874e-07, 8.6182e-07,
        5.4648e-07, 3.9975e-07, 1.2571e-06, 3.5925e-07, 8.6182e-07, 8.8078e-07,
        6.1841e-07, 1.2702e-07, 3.9693e-07, 2.7912e-07, 3.6343e-07, 2.7976e-07,
        1.9522e-07, 3.2749e-07, 6.1841e-07, 1.6722e-06, 2.7602e-07, 4.6441e-07,
        4.2330e-07, 2.3624e-07, 8.8078e-07, 3.6343e-07, 1.6643e-07, 2.7976e-07,
        2.8186e-07, 1.2784e-06, 3.3073e-07, 8.8078e-07, 8.8078e-07, 2.8221e-07,
        3.2749e-07, 8.8078e-07, 2.0115e-07, 6.1841e-07, 1.6515e-06, 2.8041e-07,
        2.5914e-07, 3.6343e-07, 6.1841e-07, 5.4648e-07, 1.9522e-07, 8.8078e-07,
        1.2781e-06, 3.7016e-07, 3.2140e-07, 8.8078e-07, 7.9374e-07, 7.9374e-07,
        6.3324e-07, 3.9975e-07, 3.9975e-07, 4.6441e-07, 7.3079e-07, 1.2866e-06,
        4.2330e-07, 3.2749e-07, 1.2584e-06, 2.7602e-07, 5.4347e-07, 4.7636e-07,
        9.1994e-07, 2.7976e-07, 7.0168e-07, 7.8262e-07, 6.3324e-07, 2.7976e-07,
        2.7602e-07, 2.9167e-07, 4.6441e-07, 1.1053e-06, 3.5059e-07, 4.6441e-07,
        6.5539e-07, 7.3079e-07, 4.2330e-07, 3.9693e-07, 3.6343e-07, 2.8221e-07,
        1.6573e-06, 3.9975e-07, 1.2702e-07, 6.1532e-07, 2.7602e-07, 1.9522e-07,
        1.7886e-07, 5.4150e-07, 5.4648e-07, 5.4648e-07, 4.2254e-07, 1.2781e-06,
        2.7602e-07, 6.4117e-07, 3.9975e-07, 5.4648e-07, 9.5182e-08, 1.9522e-07,
        8.2758e-07, 3.2169e-07, 1.2571e-06, 2.4785e-07, 7.3079e-07, 4.8667e-07,
        6.4117e-07, 6.1532e-07, 1.2782e-06, 2.9167e-07, 2.7976e-07, 5.4648e-07,
        2.5466e-07, 2.7602e-07, 3.3073e-07, 3.9975e-07, 3.6265e-07, 2.8221e-07,
        3.6343e-07, 5.4648e-07, 2.8051e-07, 3.6343e-07, 7.9374e-07, 3.2140e-07,
        1.6643e-07, 1.6515e-06, 8.6182e-07, 1.9522e-07, 2.3354e-07, 4.2330e-07,
        6.1532e-07, 6.5539e-07, 4.2330e-07, 6.7953e-07, 2.9167e-07, 3.6265e-07,
        6.4117e-07, 2.3354e-07, 7.2124e-07, 2.8041e-07, 6.4117e-07, 8.4156e-07,
        4.3572e-07, 6.1841e-07, 7.0168e-07, 1.9522e-07, 3.6343e-07, 7.3079e-07,
        4.2330e-07, 2.0115e-07, 2.8041e-07, 8.1363e-07, 1.6515e-06, 1.7886e-07,
        4.2330e-07, 8.2288e-07, 3.5925e-07, 4.2330e-07, 3.2140e-07, 4.6441e-07,
        5.4648e-07, 2.7976e-07, 2.5213e-07, 2.0115e-07, 1.6515e-06, 2.7976e-07,
        3.6265e-07, 1.9522e-07, 2.0115e-07, 3.2749e-07, 3.5925e-07, 3.9975e-07,
        6.4117e-07, 7.6750e-07, 4.8667e-07, 1.9522e-07, 5.4347e-07, 3.2766e-07,
        3.7510e-07, 3.9693e-07, 1.9522e-07, 8.8078e-07, 5.4648e-07, 2.5914e-07,
        7.9374e-07, 1.7886e-07, 1.2702e-07, 1.2571e-06, 3.2140e-07, 3.6265e-07,
        2.3354e-07, 4.6441e-07, 6.4117e-07, 1.2937e-06, 2.5914e-07, 5.4648e-07,
        6.0019e-07, 3.3073e-07, 3.2749e-07, 8.2758e-07, 4.2330e-07, 2.8041e-07,
        5.7179e-07, 2.9167e-07, 8.2288e-07, 8.8078e-07, 1.8493e-07, 3.7016e-07,
        1.2702e-07, 2.2854e-07, 3.9975e-07, 9.2269e-07, 3.9975e-07, 1.6515e-06,
        4.6441e-07, 1.8376e-06, 4.2330e-07, 6.4117e-07, 6.7953e-07, 2.7976e-07,
        5.4347e-07, 3.9975e-07, 3.2749e-07, 3.5059e-07, 1.2782e-06, 7.9374e-07,
        2.7976e-07, 2.8041e-07, 4.2330e-07, 4.9207e-07, 1.7886e-07, 8.8078e-07,
        3.5925e-07, 1.6515e-06, 4.3874e-07, 1.7886e-07, 4.2330e-07, 1.7886e-07,
        3.9693e-07, 1.6515e-06, 1.6515e-06, 1.9522e-07, 3.9693e-07, 5.4648e-07,
        7.9374e-07, 1.7958e-07, 6.3324e-07, 4.3874e-07, 9.2269e-07, 3.2749e-07,
        3.2749e-07, 7.9374e-07, 7.0168e-07, 1.6515e-06, 1.8493e-07, 1.2571e-06,
        1.9522e-07, 7.0168e-07, 4.3572e-07, 7.0168e-07, 4.9954e-07, 3.2140e-07,
        4.3874e-07, 2.3354e-07, 2.8221e-07, 1.6515e-06, 1.6515e-06, 7.9374e-07,
        2.1102e-07, 7.9374e-07, 1.9522e-07, 1.6643e-07, 8.2758e-07, 2.8041e-07,
        3.9975e-07, 2.7976e-07, 1.2702e-07, 4.2330e-07, 1.9300e-07, 6.1841e-07,
        7.9374e-07, 7.4224e-07, 3.6343e-07, 4.3316e-07, 3.2140e-07, 1.2702e-07,
        4.2254e-07, 3.2749e-07, 2.0115e-07, 5.4648e-07, 1.2526e-06, 8.8078e-07,
        6.1841e-07, 4.6441e-07, 7.9374e-07, 8.8078e-07, 2.8041e-07, 1.6643e-07,
        6.7953e-07, 4.8667e-07, 9.2269e-07, 2.5213e-07, 7.3079e-07, 2.5213e-07,
        2.5914e-07, 1.9522e-07, 3.2749e-07, 4.0123e-07, 7.8262e-07, 1.9522e-07,
        2.8041e-07, 3.2749e-07, 7.0214e-07, 3.2140e-07, 1.7886e-07, 7.3079e-07,
        2.0340e-07, 3.2749e-07, 6.1841e-07, 2.7602e-07, 3.2140e-07, 7.9374e-07,
        6.0019e-07, 2.0115e-07, 2.8041e-07, 2.8051e-07, 6.3324e-07, 7.0168e-07,
        2.8051e-07, 8.2758e-07, 2.7602e-07, 3.9975e-07, 2.5951e-07, 4.6441e-07,
        2.0115e-07, 9.2269e-07, 2.8041e-07, 7.9374e-07, 4.3396e-07, 6.1841e-07,
        7.0168e-07, 7.9374e-07, 7.9374e-07, 3.9975e-07, 1.2785e-06, 6.7953e-07,
        1.6515e-06, 1.6643e-07, 2.5951e-07, 2.7976e-07, 4.6441e-07, 2.7816e-07,
        3.9693e-07, 1.7886e-07, 7.0168e-07, 2.8041e-07, 2.8051e-07, 2.9167e-07,
        9.5909e-08, 1.9522e-07, 4.6441e-07, 3.2140e-07, 2.7976e-07, 8.2288e-07,
        5.4648e-07, 1.9522e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.9077e-06, 2.4381e-06, 4.0696e-06, 7.7791e-06, 2.0156e-06, 2.7307e-06,
        5.9429e-06, 3.6345e-06, 2.1860e-06, 2.7307e-06, 2.0156e-06, 2.6976e-06,
        2.7034e-06, 2.0179e-06, 1.8472e-06, 8.2879e-06, 8.2879e-06, 2.6976e-06,
        7.9701e-06, 2.2130e-06, 5.5860e-06, 4.2091e-06, 2.2676e-06, 3.2523e-06,
        5.5860e-06, 6.0892e-07, 2.9734e-06, 1.9077e-06, 8.2879e-06, 2.2872e-06,
        2.8395e-06, 2.6976e-06, 4.1194e-07, 8.2879e-06, 3.2211e-06, 1.1481e-06,
        3.0390e-06, 2.8395e-06, 1.3416e-06, 2.9926e-06, 4.6555e-06, 7.3470e-06,
        2.6391e-06, 3.6345e-06, 4.5325e-06, 6.8057e-06, 1.8625e-06, 3.0390e-06,
        9.6575e-06, 6.6556e-07, 2.7307e-06, 2.5726e-06, 2.5050e-06, 3.4862e-06,
        3.0390e-06, 7.3067e-06, 3.0242e-06, 3.7514e-07, 7.0053e-06, 3.9865e-06,
        1.9151e-06, 2.7307e-06, 1.8625e-06, 2.7307e-06, 7.7791e-06, 7.3067e-06,
        9.6789e-07, 1.9077e-06, 1.9077e-06, 4.0441e-06, 9.9262e-07, 3.9951e-06,
        6.8844e-06, 3.4862e-06, 7.3470e-06, 2.7174e-06, 1.8625e-06, 4.6555e-06,
        2.8313e-06, 1.3183e-06, 4.2091e-06, 2.7307e-06, 3.0242e-06, 1.2022e-06,
        8.0449e-07, 2.9385e-06, 7.0053e-06, 4.1194e-07, 8.2879e-06, 5.3936e-06,
        2.6856e-06, 1.9077e-06, 1.5678e-06, 2.0898e-06, 3.9951e-06, 3.2153e-06,
        3.1390e-06, 5.0141e-06, 2.5050e-06, 2.9926e-06, 5.9429e-06, 3.7514e-07,
        5.3936e-06, 1.9077e-06, 2.7034e-06, 2.0179e-06, 7.0053e-06, 5.5132e-06,
        1.7817e-06, 1.8625e-06, 2.7307e-06, 7.3067e-06, 2.6976e-06, 2.5050e-06,
        3.5036e-06, 2.6976e-06, 9.9262e-07, 1.6294e-06, 2.7307e-06, 2.4284e-06,
        2.1064e-06, 4.6555e-06, 1.9077e-06, 1.4002e-05, 4.0340e-06, 4.6555e-06,
        5.2060e-06, 3.7514e-07, 3.0390e-06, 2.6375e-06, 4.7379e-06, 5.0141e-06,
        9.0722e-07, 3.4297e-06, 3.4862e-06, 2.6976e-06, 7.7791e-06, 2.3155e-06,
        3.0390e-06, 2.2804e-06, 2.0156e-06, 1.7999e-06, 1.7817e-06, 2.2683e-06,
        8.0449e-07, 5.9429e-06, 2.1064e-06, 2.7307e-06, 9.9262e-07, 6.4191e-06,
        3.6345e-06, 2.6391e-06, 4.8669e-06, 1.8560e-06, 2.3155e-06, 4.2256e-06,
        7.9918e-06, 3.0956e-06, 7.9723e-06, 2.7307e-06, 3.6971e-06, 2.2676e-06,
        3.5904e-06, 2.7329e-06, 3.2016e-06, 3.9951e-06, 1.9077e-06, 9.6906e-06,
        3.0390e-06, 6.9607e-06, 1.5356e-06, 5.3936e-06, 4.5325e-06, 2.8391e-06,
        2.9926e-06, 5.5132e-06, 4.5325e-06, 3.5415e-06, 3.6819e-06, 3.5036e-06,
        7.9698e-06, 2.8395e-06, 5.0141e-06, 4.0213e-06, 2.5050e-06, 8.2879e-06,
        6.6195e-06, 1.5990e-06, 1.9034e-06, 3.0390e-06, 2.1064e-06, 5.4959e-06,
        1.7681e-06, 8.5948e-07, 1.7512e-06, 3.2969e-06, 3.0390e-06, 2.0179e-06,
        3.0242e-06, 4.0441e-06, 4.5325e-06, 1.2972e-06, 9.9262e-07, 5.3936e-06,
        2.0156e-06, 4.2091e-06, 3.0390e-06, 4.6145e-06, 5.3936e-06, 2.0156e-06,
        7.7791e-06, 5.3936e-06, 4.0696e-06, 2.8391e-06, 2.9926e-06, 2.7307e-06,
        4.6555e-06, 1.0308e-05, 6.8612e-06, 4.6145e-06, 3.8579e-06, 1.8660e-06,
        1.2972e-06, 8.2879e-06, 2.0179e-06, 5.5860e-06, 3.0242e-06, 2.7810e-06,
        2.4381e-06, 3.4006e-06, 3.4862e-06, 2.2748e-06, 2.6976e-06, 2.5050e-06,
        2.2676e-06, 1.9217e-06, 3.0242e-06, 3.8561e-06, 3.0242e-06, 5.3936e-06,
        3.0390e-06, 3.9951e-06, 1.9077e-06, 7.7791e-06, 3.9951e-06, 6.8057e-06,
        8.1295e-07, 3.5036e-06, 1.3183e-06, 2.2676e-06, 4.5325e-06, 2.0179e-06,
        4.7379e-06, 2.0156e-06, 2.7307e-06, 4.3955e-06, 5.5860e-06, 2.5050e-06,
        1.5356e-06, 2.9734e-06, 2.0179e-06, 1.8560e-06, 5.5860e-06, 2.0156e-06,
        1.2972e-06, 1.5471e-06, 1.5356e-06, 8.2879e-06, 1.9309e-06, 2.9385e-06,
        1.3804e-06, 2.5643e-06, 1.5990e-06, 3.2969e-06, 2.4284e-06, 7.7607e-06,
        3.0390e-06, 4.5409e-06, 4.7379e-06, 4.4721e-06, 2.9385e-06, 5.5860e-06,
        3.4862e-06, 4.5325e-06, 3.4297e-06, 3.9865e-06, 2.4537e-06, 1.2022e-06,
        3.0390e-06, 1.5356e-06, 5.2413e-06, 4.1194e-07, 3.9865e-06, 3.6819e-06,
        3.4006e-06, 1.9445e-06, 3.0242e-06, 3.4297e-06, 3.4862e-06, 1.3183e-06,
        1.0222e-06, 2.8391e-06, 2.0156e-06, 1.9735e-06, 1.3390e-06, 2.1551e-06,
        4.3955e-06, 1.7353e-06, 6.6556e-07, 1.4139e-06, 4.6830e-06, 1.3966e-06,
        5.4010e-06, 2.7307e-06, 5.5860e-06, 2.9926e-06, 5.3936e-06, 2.7810e-06,
        5.5860e-06, 2.7810e-06, 4.6830e-06, 2.4961e-06, 3.0390e-06, 2.6693e-06,
        3.4297e-06, 3.9841e-06, 2.0220e-06, 1.9077e-06, 2.0179e-06, 3.0390e-06,
        1.3960e-06, 9.9262e-07, 3.5036e-06, 7.3067e-06, 1.2972e-06, 2.3199e-06,
        1.3183e-06, 5.5132e-06, 2.3155e-06, 4.5325e-06, 2.5050e-06, 1.5414e-06,
        8.2165e-06, 7.9701e-06, 1.9077e-06, 2.4381e-06, 2.8395e-06, 3.0242e-06,
        2.0179e-06, 2.7307e-06, 2.8395e-06, 2.1064e-06, 2.8395e-06, 5.2928e-06,
        1.9077e-06, 1.1425e-06, 1.8625e-06, 2.0220e-06, 2.7307e-06, 4.6145e-06,
        4.6555e-06, 1.8560e-06, 2.3132e-06, 2.3199e-06, 4.6555e-06, 2.0194e-06,
        2.8395e-06, 2.7034e-06, 7.3470e-06, 6.5191e-07, 2.7307e-06, 2.7307e-06,
        1.3970e-06, 6.8057e-06, 6.3229e-06, 1.7817e-06, 1.7999e-06, 2.9734e-06,
        2.6433e-06, 3.0956e-06, 8.5948e-07, 5.3936e-06, 3.4862e-06, 1.4514e-06,
        4.0743e-06, 2.1064e-06, 2.5349e-06, 4.2644e-06, 3.0397e-06, 4.6830e-06,
        2.7307e-06, 2.1259e-06, 2.4951e-06, 6.8057e-06, 2.8391e-06, 2.4537e-06,
        5.3781e-06, 2.7028e-06, 3.0390e-06, 5.0141e-06, 1.5424e-06, 5.5860e-06,
        1.5414e-06, 4.0213e-06, 3.6971e-06, 8.0449e-07, 6.4191e-06, 3.0956e-06,
        3.9951e-06, 7.9701e-06, 1.5414e-06, 8.2879e-06, 9.9262e-07, 2.5050e-06,
        5.0141e-06, 2.1064e-06, 2.8391e-06, 5.5860e-06, 4.7379e-06, 3.9865e-06,
        4.5409e-06, 5.9812e-06, 5.9429e-06, 2.6976e-06, 2.3155e-06, 2.5050e-06,
        2.9734e-06, 5.5860e-06, 3.4297e-06, 5.4191e-06, 2.6433e-06, 8.0449e-07,
        5.2413e-06, 3.0390e-06, 5.1418e-06, 5.5860e-06, 1.3804e-06, 3.9865e-06,
        2.0824e-06, 4.5409e-06, 8.0449e-07, 3.0242e-06, 2.7307e-06, 5.0141e-06,
        9.9262e-07, 8.2879e-06, 1.9217e-06, 2.9021e-06, 5.1418e-06, 1.9735e-06,
        3.0242e-06, 3.9951e-06, 6.4191e-06, 3.0390e-06, 4.2091e-06, 8.0449e-07,
        2.6976e-06, 3.4862e-06, 4.3955e-06, 1.0238e-06, 3.0382e-06, 3.0242e-06,
        2.9734e-06, 8.0449e-07, 2.9734e-06, 2.7307e-06, 5.4191e-06, 2.6568e-06,
        2.7307e-06, 1.7999e-06, 2.0179e-06, 5.9429e-06, 2.2130e-06, 3.0242e-06,
        2.9385e-06, 2.8391e-06, 6.0892e-07, 1.9077e-06, 5.5132e-06, 3.6365e-06,
        1.8560e-06, 5.5892e-06, 5.3936e-06, 3.0242e-06, 2.7307e-06, 4.1675e-06,
        3.0390e-06, 9.6906e-06, 2.0156e-06, 2.0156e-06, 2.6976e-06, 2.8391e-06,
        3.2153e-06, 3.0390e-06, 1.9217e-06, 7.3273e-06, 5.1418e-06, 5.0141e-06,
        2.0179e-06, 2.6976e-06, 7.9701e-06, 2.5726e-06, 9.6575e-06, 8.2879e-06,
        5.3936e-06, 1.9077e-06, 1.3599e-06, 2.0106e-06, 2.5050e-06, 2.8395e-06,
        3.0242e-06, 2.8395e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.9807e-07, 3.9483e-07, 2.4074e-07,  ..., 1.9432e-07, 7.1854e-07,
        3.4313e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7529e-06, 6.7246e-07, 7.2392e-07,  ..., 2.4104e-01, 7.8803e-07,
        1.2056e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.8569e-07, 2.2160e-06, 1.3729e-06, 7.8955e-07, 5.5141e-06, 6.1401e-07,
        1.6737e-06, 9.4376e-07, 2.9340e-06, 9.2778e-07, 2.3500e-06, 2.2866e-06,
        1.8790e-06, 5.8191e-07, 2.5389e-06, 1.1778e-06, 1.8716e-06, 2.0700e-06,
        1.5746e-06, 4.2444e-07, 9.4368e-07, 7.5609e-07, 2.2123e-06, 1.4785e-06,
        9.1107e-07, 2.2566e-06, 1.2683e-06, 9.4368e-07, 1.4785e-06, 6.0136e-07,
        2.1946e-06, 1.6420e-07, 9.0142e-07, 2.5514e-06, 9.2778e-07, 1.2638e-06,
        4.2755e-06, 1.8233e-06, 2.9315e-06, 9.3895e-07, 2.3323e-01, 4.3003e-06,
        1.8790e-06, 1.7679e-06, 1.7359e-06, 1.0853e-06, 9.2778e-07, 2.3702e-06,
        1.1381e-06, 1.8071e-06, 1.7820e-06, 2.7977e-06, 1.2342e-06, 2.3285e-06,
        7.1360e-07, 6.1401e-07, 1.8205e-06, 3.2530e-06, 2.8792e-06, 3.9294e-07,
        1.6398e-06, 3.3196e-06, 6.8618e-07, 1.4785e-06, 1.7693e-06, 2.9667e-06,
        1.3728e-06, 1.6381e-06, 1.7694e-06, 2.2144e-06, 3.4663e-06, 3.2552e-06,
        5.5141e-06, 2.5389e-06, 1.6497e-06, 2.7700e-06, 9.2778e-07, 1.4785e-06,
        2.6231e-06, 2.7207e-07, 1.6737e-06, 7.5884e-07, 2.5959e-06, 1.8716e-06,
        1.6420e-07, 9.2778e-07, 1.9248e-06, 3.6921e-06, 2.9667e-06, 4.8274e-07,
        2.2160e-06, 4.1631e-06, 4.2755e-06, 1.3429e-06, 2.1899e-06, 1.8374e-06,
        2.9894e-02, 1.1778e-06, 1.8043e-06, 2.1522e-06, 6.1401e-07, 3.4309e-06,
        4.2755e-06, 1.4785e-06, 1.3126e-06, 2.5807e-06, 1.7883e-06, 6.4894e-07,
        8.3374e-07, 2.2577e-06, 2.2123e-06, 2.3285e-06, 3.4309e-06, 1.6673e-06,
        1.5501e-06, 3.1031e-06, 1.9496e-06, 1.7007e-06, 5.5141e-06, 2.7207e-07,
        2.6081e-06, 7.0439e-07, 2.9315e-06, 4.2755e-06, 2.3500e-06, 5.2078e-06,
        1.1619e-06, 2.5959e-06, 2.2084e-06, 4.1631e-06, 2.9370e-06, 2.9223e-06,
        1.6283e-06, 7.5884e-07, 2.8832e-06, 2.5913e-06, 2.4161e-06, 4.8351e-06,
        1.1872e-06, 3.2275e-06, 2.4815e-06, 2.7176e-06, 1.5445e-06, 1.6751e-06,
        3.7577e-06, 9.2778e-07, 5.7609e-07, 1.2638e-06, 1.6420e-07, 4.0530e-06,
        5.4883e-07, 1.0463e-06, 1.7694e-06, 2.1522e-06, 1.7694e-06, 6.6262e-07,
        1.7694e-06, 1.9498e-06, 7.9217e-07, 2.9340e-06, 1.7004e-06, 1.7820e-06,
        1.4788e-06, 3.0695e-06, 1.8879e-06, 2.8264e-06, 2.1760e-06, 1.5445e-06,
        1.6930e-06, 2.9315e-06, 2.0041e-06, 8.3374e-07, 6.4894e-07, 1.3729e-06,
        3.8733e-07, 7.5759e-07, 1.5829e-06, 2.9992e-06, 1.4776e-06, 1.6283e-06,
        2.9370e-06, 2.8126e-06, 3.2718e-07, 7.5609e-07, 1.6737e-06, 1.4785e-06,
        1.1872e-06, 1.6506e-06, 1.1778e-06, 1.4785e-06, 2.7176e-06, 3.0816e-06,
        1.2638e-06, 7.9176e-04, 1.7881e-06, 1.0933e-06, 1.8835e-06, 2.7176e-06,
        2.1438e-06, 1.4785e-06, 2.0867e-07, 2.3351e-06, 1.7854e-06, 8.5802e-07,
        2.6231e-06, 3.0666e-06, 1.7694e-06, 4.0475e-06, 2.7176e-06, 1.3126e-06,
        3.3453e-06, 2.6371e-06, 1.2638e-06, 1.8071e-06, 3.7577e-06, 1.2638e-06,
        2.1899e-06, 9.4747e-07, 1.0086e-06, 1.9094e-06, 1.3034e-06, 2.2730e-01,
        1.0852e-06, 1.1063e-06, 7.0439e-07, 4.2755e-06, 5.4883e-07, 1.7679e-06,
        1.1983e-06, 1.8702e-06, 1.9191e-06, 1.6287e-06, 1.0852e-06, 1.5716e-06,
        2.8920e-06, 4.0475e-06, 3.1331e-06, 2.3285e-06, 1.0463e-06, 4.0475e-06,
        1.6929e-06, 4.3003e-06, 2.8351e-06, 2.5514e-06, 3.3363e-06, 3.5297e-06,
        2.2566e-06, 4.0475e-06, 1.7820e-06, 1.7694e-06, 5.4883e-07, 6.1401e-07,
        3.2275e-06, 1.8612e-06, 1.4785e-06, 1.7693e-06, 1.6929e-06, 1.5895e-06,
        1.1647e-06, 1.1587e-06, 2.9370e-06, 6.1401e-07, 1.7870e-06, 1.3126e-06,
        8.5802e-07, 1.6929e-06, 1.4785e-06, 2.1955e-06, 4.2755e-06, 1.6213e-06,
        9.2778e-07, 4.0475e-06, 2.4565e-06, 1.3162e-06, 6.7644e-07, 2.6010e-06,
        8.5673e-07, 4.2444e-07, 3.5875e-06, 2.5807e-06, 2.2059e-06, 3.2275e-06,
        2.8126e-06, 6.7889e-07, 3.4663e-06, 3.4309e-06, 3.3363e-06, 4.3003e-06,
        1.6497e-06, 2.5389e-06, 2.4055e-06, 3.0424e-06, 4.0475e-06, 8.6940e-07,
        1.6420e-07, 1.0463e-06, 3.2275e-06, 1.7694e-06, 6.1401e-07, 1.8165e-06,
        1.6930e-06, 2.3285e-06, 1.7284e-06, 4.4805e-06, 2.2566e-06, 1.5703e-06,
        2.5389e-06, 3.0695e-06, 1.1898e-06, 1.8213e-06, 2.2460e-06, 1.9857e-06,
        2.2566e-06, 5.6136e-07, 8.3374e-07, 1.0700e-06, 1.1787e-06, 9.4368e-07,
        2.2460e-06, 3.4309e-06, 5.5668e-07, 3.0666e-06, 4.0475e-06, 4.1631e-06,
        1.6930e-06, 1.5046e-06, 1.0629e-06, 2.8126e-06, 2.4206e-06, 2.1042e-01,
        3.1698e-06, 1.5018e-06, 2.7864e-06, 3.7577e-06, 1.8205e-06, 2.6780e-06,
        3.4309e-06, 2.1955e-06, 1.8757e-06, 1.3126e-06, 8.0123e-07, 1.9248e-06,
        1.1063e-06, 1.4136e-06, 1.8716e-06, 9.4368e-07, 1.1367e-06, 2.7864e-06,
        5.0326e-07, 2.3094e-06, 2.0176e-06, 4.6443e-06, 1.7820e-06, 1.5909e-06,
        2.5514e-06, 7.1993e-07, 1.5100e-06, 6.7924e-07, 1.1778e-06, 6.7924e-07,
        2.5807e-06, 3.5875e-06, 1.2638e-06, 3.8757e-06, 1.7058e-06, 1.0600e-06,
        3.6131e-06, 1.0086e-06, 1.2341e-06, 2.7114e-06, 5.0326e-07, 1.3126e-06,
        1.2342e-06, 5.9070e-07, 6.7924e-07, 4.3003e-06, 1.6283e-06, 1.3053e-06,
        4.0475e-06, 2.7339e-01, 1.4679e-06, 1.0844e-06, 2.5807e-06, 1.0463e-06,
        1.4193e-06, 7.2932e-07, 8.6940e-07, 5.0326e-07, 2.7099e-06, 4.0475e-06,
        1.7359e-06, 4.8351e-06, 1.3126e-06, 4.0748e-06, 1.0938e-06, 9.4368e-07,
        6.2208e-07, 2.5346e-06, 1.2571e-06, 1.7007e-06, 1.9431e-06, 5.4883e-07,
        2.3285e-06, 2.6400e-06, 2.2460e-06, 8.6940e-07, 1.7496e-01, 2.7977e-06,
        8.3374e-07, 5.9070e-07, 1.7325e-06, 2.2566e-06, 7.1360e-07, 1.6646e-06,
        7.2932e-07, 1.6646e-06, 1.7284e-06, 1.7881e-06, 4.2444e-07, 2.6009e-06,
        1.1983e-06, 1.7325e-06, 2.5389e-06, 6.0300e-06, 1.4776e-06, 3.2530e-06,
        6.1926e-07, 2.6231e-06, 3.0570e-06, 3.6464e-06, 2.1599e-06, 2.4564e-06,
        2.2160e-06, 3.6941e-07, 1.2647e-07, 2.1522e-06, 2.0416e-06, 1.5703e-06,
        1.6497e-06, 1.9759e-06, 2.3521e-06, 3.4309e-06, 2.7207e-07, 2.5389e-06,
        1.9582e-06, 2.2460e-06, 1.6420e-07, 8.5136e-07, 2.4792e-06, 2.1532e-06,
        2.1139e-06, 3.7577e-06, 4.6814e-06, 2.3285e-06, 2.0837e-01, 4.2755e-06,
        6.1092e-07, 1.6875e-06, 4.0748e-06, 8.9717e-07, 1.0049e-06, 1.9780e-06,
        5.9884e-07, 7.5609e-07, 1.8879e-06, 7.0439e-07, 1.0086e-06, 1.9174e-06,
        3.5790e-06, 2.1223e-06, 2.3285e-06, 1.7820e-06, 2.4617e-06, 2.3022e-06,
        1.5501e-06, 9.4837e-07, 1.0600e-06, 1.5957e-06, 3.4309e-06, 1.7694e-06,
        6.1401e-07, 1.8702e-06, 3.7577e-06, 2.6388e-06, 1.8702e-06, 1.8043e-06,
        2.9223e-06, 9.4747e-07, 2.4439e-06, 1.3355e-06, 3.1367e-01, 3.4309e-06,
        2.0528e-06, 2.1899e-06, 1.7719e-06, 9.3202e-07, 3.1466e-06, 4.0475e-06,
        2.7890e-06, 1.8071e-06, 3.3459e-06, 8.9217e-07, 1.3729e-06, 4.8275e-07,
        1.7679e-06, 7.2106e-07, 7.5884e-07, 1.4633e-06, 1.6544e-06, 1.3256e-06,
        3.3363e-06, 4.8275e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.2649e-06, 9.1755e-06, 9.2033e-06, 4.1784e-06, 5.6640e-06, 5.6628e-06,
        5.3127e-06, 1.0184e-05, 5.0961e-06, 1.4998e-05, 4.5354e-06, 1.2035e-05,
        8.3964e-06, 6.5778e-06, 8.1588e-06, 1.2633e-05, 8.4008e-06, 1.2536e-05,
        1.6738e-06, 2.6602e-06, 9.2272e-06, 1.0951e-05, 1.1963e-05, 9.2272e-06,
        3.3709e-06, 1.5080e-05, 5.0210e-06, 6.4344e-06, 8.1629e-06, 1.0958e-05,
        6.7050e-06, 1.6247e-05, 2.1680e-06, 6.8278e-06, 5.3474e-06, 6.0326e-06,
        8.2416e-06, 3.8162e-06, 7.3581e-06, 1.3760e-05, 6.2649e-06, 6.9051e-06,
        3.6180e-06, 1.4300e-05, 1.0383e-05, 3.6180e-06, 7.2421e-06, 9.6275e-06,
        1.2386e-05, 1.5603e-05, 9.5415e-06, 3.4571e-06, 3.9576e-06, 5.9472e-06,
        9.9914e-06, 1.1606e-05, 1.4295e-05, 3.5156e-06, 9.6510e-06, 8.2307e-06,
        1.6271e-05, 1.0166e-05, 5.7534e-06, 5.5575e-06, 7.1304e-06, 8.8032e-06,
        1.0455e-05, 7.3278e-06, 3.2385e-06, 4.1093e-06, 6.4251e-06, 3.0472e-06,
        9.5421e-06, 2.3736e-06, 4.3431e-06, 1.2120e-05, 8.4111e-06, 3.5122e-06,
        7.4776e-06, 2.8973e-06, 1.2294e-05, 6.3403e-06, 6.6116e-06, 8.8422e-06,
        1.3729e-05, 7.5555e-06, 5.6362e-06, 7.7837e-07, 1.4300e-05, 5.3651e-06,
        6.0260e-06, 3.2633e-06, 5.4316e-06, 8.0628e-06, 9.6542e-06, 1.1614e-05,
        1.3650e-05, 7.5669e-06, 1.0607e-05, 6.7051e-06, 4.6416e-06, 5.7535e-06,
        1.4835e-05, 1.3942e-05, 3.6180e-06, 7.9456e-06, 1.1077e-05, 2.4633e-06,
        1.2294e-05, 6.4234e-06, 4.4675e-06, 3.2535e-06, 3.2345e-06, 3.5058e-06,
        1.3593e-05, 1.4300e-05, 1.8010e-05, 8.5389e-06, 5.4176e-06, 8.1144e-06,
        7.2312e-06, 3.9521e-06, 3.5156e-06, 4.7859e-06, 6.5778e-06, 7.4506e-06,
        3.5783e-06, 2.3710e-06, 1.1056e-05, 6.0742e-06, 3.9521e-06, 1.6458e-05,
        3.5409e-06, 6.3313e-06, 3.2349e-07, 9.7347e-02, 4.4052e-06, 7.7837e-07,
        2.2557e-06, 6.7050e-06, 1.4775e-06, 5.9212e-06, 9.8334e-06, 8.1514e-06,
        3.0472e-06, 1.0337e-05, 4.1784e-06, 2.2342e-06, 9.4088e-02, 9.0914e-06,
        6.8558e-06, 3.4135e-06, 6.0272e-06, 1.6271e-05, 9.2272e-06, 3.7301e-06,
        1.1439e-05, 1.0184e-05, 3.2345e-06, 7.9245e-06, 5.0961e-06, 3.6432e-06,
        3.3144e-06, 2.6535e-06, 5.3330e-06, 6.9337e-06, 1.1920e-05, 6.1127e-06,
        8.7818e-06, 4.5354e-06, 3.4890e-06, 1.0652e-05, 7.8128e-06, 6.5778e-06,
        1.5512e-05, 5.9204e-06, 5.2930e-06, 8.4008e-06, 9.2605e-06, 4.0570e-06,
        4.8268e-06, 7.8128e-06, 6.3086e-06, 7.7660e-02, 1.1704e-05, 1.1704e-05,
        4.7844e-06, 9.7942e-06, 3.8599e-06, 9.2272e-06, 4.5491e-06, 1.1241e-05,
        6.4309e-06, 1.5740e-05, 8.4844e-06, 4.1846e-06, 9.3051e-06, 9.0010e-06,
        1.1389e-05, 1.3475e-05, 8.0033e-06, 7.7661e-06, 1.5603e-05, 7.1107e-06,
        1.0631e-01, 1.0184e-05, 1.9962e-05, 1.4955e-05, 7.7585e-06, 5.3474e-06,
        1.1889e-05, 1.4300e-05, 6.3368e-06, 1.1648e-05, 1.3054e-05, 9.5105e-06,
        7.9821e-06, 1.2095e-05, 7.4146e-06, 2.6937e-06, 2.5197e-06, 8.8491e-06,
        5.5195e-06, 5.3078e-06, 4.1094e-06, 5.1450e-06, 7.9954e-06, 8.9848e-06,
        2.1680e-06, 1.0219e-05, 5.1393e-06, 1.9195e-05, 9.8971e-07, 6.5778e-06,
        5.1817e-06, 7.4506e-06, 1.2550e-05, 7.8572e-06, 8.8032e-06, 2.5311e-06,
        8.0990e-06, 7.3590e-06, 4.1094e-06, 6.3327e-06, 3.0534e-06, 1.2834e-05,
        7.4869e-06, 6.4309e-06, 2.1553e-05, 5.0210e-06, 8.9643e-06, 3.5193e-06,
        6.7160e-06, 6.6353e-06, 7.5873e-06, 5.7830e-06, 1.4998e-05, 3.0534e-06,
        2.9513e-06, 4.7358e-06, 1.0962e-05, 6.0654e-06, 7.9245e-06, 1.2290e-05,
        2.2557e-06, 1.0962e-05, 6.6757e-06, 3.5058e-06, 1.1186e-05, 8.7531e-06,
        6.2560e-06, 8.0628e-06, 9.7936e-06, 5.4036e-06, 9.7748e-06, 7.3278e-06,
        8.7818e-06, 8.0033e-06, 3.3469e-06, 1.1889e-05, 8.7851e-06, 1.3720e-01,
        9.0340e-06, 6.8526e-06, 9.3336e-06, 3.0629e-06, 3.8080e-06, 5.6206e-06,
        6.0273e-06, 7.7661e-06, 6.5913e-06, 7.3643e-06, 5.9815e-06, 1.0063e-05,
        1.0696e-05, 4.7844e-06, 7.5042e-02, 6.4157e-06, 2.3500e-06, 6.9051e-06,
        6.0172e-06, 9.9214e-06, 9.4510e-06, 6.8297e-06, 4.2250e-06, 9.4040e-06,
        6.5223e-06, 7.3580e-06, 9.9507e-02, 1.5603e-05, 3.1704e-06, 3.0629e-06,
        8.3072e-06, 7.3643e-06, 5.5300e-06, 5.5325e-06, 1.0278e-05, 6.0260e-06,
        5.6349e-06, 3.4890e-06, 8.3964e-06, 6.3768e-06, 6.6893e-06, 1.5541e-05,
        4.6823e-06, 7.1740e-06, 8.0429e-06, 1.2655e-01, 1.3792e-05, 8.4008e-06,
        1.0184e-05, 6.7050e-06, 4.4649e-06, 1.5386e-01, 7.7901e-06, 6.4344e-06,
        1.2821e-05, 7.7013e-06, 7.0934e-06, 7.5594e-06, 7.8439e-02, 5.4316e-06,
        7.2565e-06, 7.9954e-06, 2.8656e-06, 1.0861e-05, 1.8146e-05, 1.0184e-05,
        1.2343e-01, 9.6275e-06, 1.1738e-05, 6.0920e-07, 9.3817e-06, 4.0924e-06,
        7.1284e-06, 9.7332e-06, 2.7090e-06, 5.3474e-06, 1.1716e-06, 6.5960e-06,
        5.7946e-06, 6.0326e-06, 4.0570e-06, 1.5706e-06, 7.2565e-06, 8.5715e-06,
        8.1321e-06, 7.9984e-06, 7.7661e-06, 7.4914e-06, 7.2565e-06, 1.0280e-05,
        8.8318e-06, 1.0857e-05, 1.3523e-05, 4.0570e-06, 8.0318e-06, 1.0280e-05,
        9.7332e-06, 1.0623e-05, 6.5778e-06, 5.3474e-06, 1.5603e-05, 6.7051e-06,
        1.7189e-05, 1.3827e-05, 3.8162e-06, 1.0696e-05, 8.4844e-06, 4.4768e-06,
        2.1333e-06, 9.7936e-06, 8.1262e-06, 9.2592e-06, 6.7059e-06, 8.5991e-06,
        7.6093e-06, 8.3156e-06, 9.3990e-06, 8.6045e-06, 9.0344e-06, 5.9815e-06,
        7.0893e-06, 5.3651e-06, 4.4052e-06, 8.1781e-06, 9.1648e-06, 1.0283e-05,
        8.0415e-06, 9.9878e-06, 7.3706e-06, 7.1088e-06, 8.6981e-02, 8.4330e-06,
        6.9337e-06, 8.1199e-06, 3.8183e-06, 1.0952e-05, 4.4649e-06, 1.3760e-05,
        1.4195e-05, 4.0865e-06, 1.6368e-05, 1.3290e-05, 9.8334e-06, 7.6147e-06,
        6.4277e-06, 3.5409e-06, 1.8123e-05, 1.5176e-05, 4.1720e-06, 9.2033e-06,
        2.0733e-06, 8.8806e-02, 4.5941e-06, 6.0770e-06, 8.7925e-06, 3.2345e-06,
        6.9300e-06, 7.3278e-06, 7.4506e-06, 8.4844e-06, 6.2484e-06, 1.1183e-05,
        1.1328e-05, 1.6368e-05, 3.7711e-06, 9.3929e-06, 1.2082e-05, 3.8080e-06,
        1.1874e-05, 3.3144e-06, 6.2649e-06, 1.1477e-05, 5.5960e-06, 9.0510e-06,
        9.8334e-06, 3.7948e-06, 1.0159e-05, 1.3331e-01, 6.4585e-06, 9.5700e-06,
        4.5419e-06, 8.3823e-06, 6.7350e-02, 1.5603e-05, 6.9337e-06, 4.2508e-06,
        1.3482e-05, 6.8454e-06, 7.7179e-06, 6.2725e-06, 5.2677e-06, 9.1035e-06,
        1.6949e-05, 1.0532e-01, 3.7877e-06, 5.8486e-06, 1.3528e-05, 8.9643e-06,
        6.0326e-06, 6.3877e-06, 2.1680e-06, 3.2352e-07, 3.7420e-06, 6.7059e-06,
        1.4067e-01, 9.5105e-06, 9.6484e-06, 1.5586e-05, 5.1393e-06, 1.0312e-06,
        1.6002e-05, 7.0653e-06, 5.1817e-06, 9.0914e-06, 7.9610e-07, 9.0298e-06,
        4.8350e-06, 6.7160e-06, 7.7661e-06, 9.3475e-02, 3.2533e-06, 6.5778e-06,
        9.2272e-06, 6.0327e-06, 8.3808e-06, 9.2605e-06, 8.5166e-06, 6.5778e-06,
        8.9863e-06, 8.4667e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.1641e-07, 4.0629e-07, 3.3740e-07,  ..., 1.7411e-01, 4.0835e-07,
        6.9850e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.9803e-06, 6.3708e-06, 1.0357e-06, 2.8265e-06, 2.7429e-06, 1.8957e-06,
        5.0615e-06, 1.1733e-06, 1.0648e-06, 3.7713e-06, 2.4592e-06, 4.1127e-06,
        2.0369e-06, 1.5158e-06, 1.4589e-06, 2.9792e-06, 1.9825e-06, 1.4589e-06,
        1.5724e-06, 4.5434e-06, 3.2783e-06, 1.2253e-06, 5.0617e-06, 3.4376e-06,
        2.5410e-06, 3.0149e-06, 1.1733e-06, 2.7772e-06, 5.0280e-06, 2.9321e-06,
        3.3787e-06, 2.8430e-06, 2.9009e-06, 2.4903e-06, 2.9803e-06, 2.8797e-06,
        2.5982e-06, 1.2055e-06, 3.5587e-06, 3.0974e-06, 1.5724e-06, 3.3092e-06,
        2.5239e-06, 2.7278e-06, 3.4729e-06, 2.5231e-06, 3.0543e-06, 1.5168e-06,
        1.5998e-06, 1.3811e-06, 2.7356e-06, 2.5239e-06, 2.3036e-06, 3.1170e-06,
        7.5575e-06, 5.2293e-06, 3.4380e-06, 2.4275e-06, 3.3506e-06, 3.5676e-06,
        5.0617e-06, 2.7396e-06, 1.8957e-06, 2.0624e-06, 2.3550e-06, 4.1792e-07,
        5.4660e-06, 1.2369e-06, 1.6840e-06, 5.7875e-06, 4.1717e-06, 3.9918e-06,
        1.0690e-06, 1.6971e-06, 2.5551e-06, 3.9260e-06, 2.9009e-06, 2.4053e-06,
        2.9885e-06, 1.4543e-06, 1.2836e-06, 5.8857e-06, 1.2836e-06, 2.3866e-06,
        8.4347e-07, 1.8902e-06, 9.8128e-07, 2.5127e-06, 2.6177e-06, 6.1614e-06,
        2.3036e-06, 3.7647e-06, 1.9980e-06, 1.1536e-06, 2.5410e-06, 1.8695e-06,
        3.2783e-06, 1.9980e-06, 6.1405e-06, 1.5696e-06, 1.2307e-06, 9.6457e-07,
        2.3080e-06, 3.7464e-06, 2.2989e-06, 1.5724e-06, 3.9260e-06, 2.5598e-06,
        1.8310e-06, 3.2023e-06, 2.4053e-06, 2.9853e-06, 3.4078e-06, 2.7068e-06,
        1.7699e-06, 3.2264e-06, 2.7515e-06, 3.6876e-06, 1.2836e-06, 6.0233e-06,
        3.4066e-06, 5.8857e-06, 2.2084e-06, 3.3787e-06, 2.0429e-06, 3.1541e-06,
        3.7706e-06, 1.2575e-06, 1.0648e-06, 1.9980e-06, 3.1322e-06, 2.4923e-06,
        2.3082e-06, 2.6210e-06, 4.5775e-06, 5.7875e-06, 3.1231e-06, 1.1733e-06,
        1.8431e-06, 1.7414e-06, 2.4923e-06, 5.0272e-06, 1.8612e-06, 4.4611e-06,
        2.6076e-06, 1.5358e-06, 2.9515e-06, 1.5997e-06, 5.6783e-06, 1.1113e-06,
        3.4376e-06, 7.5318e-06, 3.4078e-06, 1.3318e-06, 2.5410e-06, 1.7108e-06,
        3.7734e-06, 1.4690e-06, 2.5410e-06, 4.7636e-06, 1.6220e-06, 2.6613e-06,
        7.8299e-07, 7.9381e-07, 2.5410e-06, 1.3596e-06, 3.8664e-06, 9.8128e-07,
        8.7400e-06, 9.1117e-07, 2.5410e-06, 4.6396e-06, 1.5723e-06, 4.9128e-06,
        1.4543e-06, 2.2668e-06, 2.1620e-06, 2.7346e-06, 2.2577e-06, 6.1405e-06,
        4.0756e-06, 3.3092e-06, 1.1733e-06, 2.7250e-06, 2.1522e-06, 2.9218e-06,
        4.7141e-06, 3.1938e-06, 3.4384e-06, 5.8857e-06, 2.3080e-06, 5.6532e-06,
        1.2369e-06, 4.5125e-06, 1.8517e-06, 3.4940e-06, 3.9918e-06, 1.9590e-06,
        1.2575e-06, 2.2296e-06, 5.8857e-06, 3.7734e-06, 4.0868e-06, 5.0617e-06,
        3.2889e-06, 4.1542e-06, 2.7098e-06, 3.4940e-06, 2.9754e-06, 1.6220e-06,
        1.1176e-06, 1.2369e-06, 3.5637e-06, 3.9035e-06, 2.7177e-06, 1.8695e-06,
        4.0726e-06, 2.9792e-06, 3.3836e-06, 1.3421e-06, 2.7183e-06, 3.3887e-06,
        1.5724e-06, 2.7204e-06, 1.4061e-06, 1.1176e-06, 1.0373e-06, 2.0000e-06,
        2.4204e-06, 1.8208e-06, 4.1647e-06, 3.5681e-06, 3.0711e-07, 3.8311e-06,
        2.4596e-06, 2.7772e-06, 2.4697e-06, 2.6804e-06, 3.2016e-06, 1.0648e-06,
        4.1647e-06, 4.0813e-06, 4.7141e-06, 1.1176e-06, 1.4589e-06, 2.8376e-06,
        3.0686e-06, 9.5754e-07, 3.0307e-06, 2.0565e-06, 2.3080e-06, 3.7096e-06,
        1.2369e-06, 6.8492e-06, 2.1416e-06, 1.9137e-06, 9.8965e-07, 1.7417e-06,
        1.3421e-06, 3.8532e-06, 1.5442e-06, 3.1809e-06, 1.2215e-06, 3.0020e-06,
        2.5410e-06, 1.7808e-06, 2.2427e-06, 6.8383e-07, 3.7679e-06, 2.7503e-06,
        5.2602e-06, 3.3641e-06, 9.0554e-07, 2.7560e-06, 6.3693e-07, 1.5960e-06,
        3.7264e-06, 9.5077e-07, 1.3228e-06, 4.3938e-06, 6.1405e-06, 5.0617e-06,
        7.8299e-07, 2.6543e-06, 2.8376e-06, 8.8329e-07, 3.3787e-06, 1.9262e-06,
        4.2690e-06, 3.5659e-06, 1.8683e-06, 3.1170e-06, 1.5724e-06, 1.1113e-06,
        1.3927e-06, 3.0117e-06, 2.8016e-06, 1.8004e-06, 2.8376e-06, 1.8305e-06,
        3.0020e-06, 4.9930e-06, 2.6672e-06, 1.1494e-06, 4.6304e-07, 1.6295e-06,
        2.7059e-06, 3.7264e-06, 1.9980e-06, 1.0555e-06, 1.8310e-06, 1.8880e-06,
        1.3045e-06, 3.3836e-06, 6.2579e-06, 2.8783e-06, 1.2836e-06, 3.6937e-06,
        1.2836e-06, 1.9391e-06, 3.2519e-06, 1.5358e-06, 1.8902e-06, 3.0307e-06,
        5.0615e-06, 1.1113e-06, 2.4204e-06, 2.3866e-06, 1.5358e-06, 3.5685e-06,
        1.7156e-06, 3.0020e-06, 6.0669e-06, 2.4275e-06, 7.3610e-06, 4.6480e-06,
        2.7197e-06, 3.7264e-06, 4.1647e-06, 2.5467e-06, 3.4465e-06, 2.8144e-06,
        3.3476e-06, 2.5410e-06, 9.6457e-07, 9.1117e-07, 2.8376e-06, 2.6672e-06,
        4.8386e-06, 8.4165e-07, 2.5030e-06, 2.6766e-06, 1.7417e-06, 1.3228e-06,
        6.6424e-07, 2.2063e-06, 1.1955e-06, 2.5127e-06, 5.4778e-06, 2.5862e-06,
        3.1549e-06, 4.8335e-06, 3.0803e-06, 3.5659e-06, 9.6856e-07, 1.4664e-06,
        3.3836e-06, 1.7569e-06, 3.5907e-06, 1.6869e-06, 9.0554e-07, 1.4188e-06,
        3.6495e-06, 4.2354e-06, 1.3228e-06, 6.1405e-06, 2.6804e-06, 1.8483e-06,
        3.9256e-06, 3.0052e-06, 1.7120e-06, 1.8329e-06, 2.2063e-06, 3.7026e-06,
        3.9726e-07, 1.5998e-06, 2.4052e-06, 3.0543e-06, 3.2863e-06, 2.7200e-06,
        3.9900e-06, 3.3787e-06, 4.8344e-06, 3.8913e-06, 3.0879e-06, 2.6187e-06,
        5.0982e-06, 3.7188e-06, 1.3421e-06, 1.6840e-06, 2.5410e-06, 2.8376e-06,
        4.7141e-06, 2.9515e-06, 3.5659e-06, 2.8513e-06, 3.4005e-06, 1.2801e-06,
        3.7713e-06, 1.4663e-06, 2.4596e-06, 3.3787e-06, 2.0000e-06, 2.3254e-06,
        1.7108e-06, 2.5655e-06, 1.1733e-06, 2.2905e-06, 2.5923e-06, 2.6303e-06,
        1.8654e-06, 3.6502e-06, 2.1525e-06, 1.6295e-06, 1.1955e-06, 3.4893e-06,
        2.6613e-06, 1.6869e-06, 4.1127e-06, 1.5358e-06, 4.0006e-06, 2.5862e-06,
        2.6543e-06, 1.7569e-06, 3.6937e-06, 4.1417e-06, 4.1127e-06, 1.2369e-06,
        3.3697e-06, 3.3787e-06, 3.2052e-06, 4.0801e-06, 2.3866e-06, 1.4683e-06,
        2.3550e-06, 3.6811e-06, 3.1809e-06, 3.4829e-06, 5.8857e-06, 3.3787e-06,
        1.5724e-06, 3.0717e-06, 3.7264e-06, 1.8695e-06, 3.2023e-06, 5.0272e-06,
        3.7734e-06, 2.4177e-06, 2.6323e-06, 1.9937e-06, 2.1416e-06, 5.2946e-06,
        3.0019e-06, 3.1231e-06, 1.7114e-06, 1.4033e-06, 4.8335e-06, 1.3596e-06,
        4.6284e-06, 1.1733e-06, 1.6351e-06, 2.1441e-06, 1.7706e-06, 6.1405e-06,
        1.8683e-06, 9.1117e-07, 1.6260e-06, 1.5052e-06, 2.5410e-06, 1.6971e-06,
        3.3787e-06, 2.4152e-06, 1.8534e-06, 1.6798e-06, 1.4606e-06, 1.8902e-06,
        1.8866e-06, 1.8307e-06, 4.0902e-06, 4.0352e-06, 4.0027e-06, 1.8957e-06,
        2.2430e-06, 4.6480e-06, 2.4763e-06, 2.1117e-06, 2.0813e-06, 2.8213e-06,
        1.6971e-06, 2.8376e-06, 2.2129e-06, 1.6971e-06, 9.5754e-07, 3.1809e-06,
        2.1951e-06, 2.6543e-06, 1.8329e-06, 3.5261e-06, 1.9850e-06, 3.8311e-06,
        2.6672e-06, 3.7518e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.4038e-06, 4.9494e-06, 6.5111e-06, 4.4234e-06, 1.2554e-05, 9.7736e-06,
        9.4176e-06, 5.7780e-06, 7.2892e-06, 3.7399e-06, 5.8483e-06, 9.7730e-06,
        3.5483e-06, 5.5709e-06, 8.0244e-06, 6.1989e-06, 2.9022e-06, 7.5266e-06,
        2.2546e-06, 1.1145e-05, 5.6592e-06, 3.9302e-06, 3.9948e-06, 4.2732e-06,
        7.8480e-06, 4.9635e-06, 8.7865e-06, 3.9950e-06, 7.9794e-06, 3.5376e-06,
        7.8990e-06, 1.0371e-05, 7.7187e-06, 1.1908e-05, 5.7675e-06, 1.4813e-05,
        7.0321e-06, 2.0294e-05, 4.4931e-06, 9.1251e-06, 8.9037e-06, 6.9533e-06,
        4.2731e-06, 4.4931e-06, 5.1360e-06, 3.7996e-06, 1.0330e-05, 3.8912e-06,
        3.1263e-06, 2.4918e-06, 2.5260e-06, 9.7428e-06, 3.6650e-06, 9.0459e-06,
        8.1617e-06, 5.5080e-06, 2.8598e-06, 7.8050e-06, 2.4982e-06, 3.3356e-06,
        6.0780e-06, 1.7499e-06, 6.6223e-06, 9.8586e-06, 1.1145e-05, 1.5004e-06,
        2.7894e-06, 5.6319e-06, 6.8740e-06, 3.7330e-06, 5.4532e-06, 3.8866e-06,
        6.6282e-06, 8.2275e-06, 9.6733e-06, 3.9673e-06, 4.7424e-06, 7.8860e-06,
        3.8699e-06, 3.8361e-06, 5.8729e-06, 3.9614e-06, 1.2079e-05, 4.0951e-06,
        1.3739e-05, 5.0010e-06, 4.7041e-06, 7.2937e-06, 6.7007e-06, 1.2358e-05,
        6.8741e-06, 5.7411e-06, 1.7589e-05, 7.0768e-06, 3.0129e-06, 7.6065e-06,
        7.5338e-06, 4.1439e-06, 4.2732e-06, 5.7411e-06, 1.0127e-05, 7.0086e-06,
        8.9248e-06, 5.1106e-06, 1.2697e-05, 6.6544e-06, 5.7498e-06, 8.0016e-06,
        7.7797e-06, 1.3150e-05, 3.5514e-06, 3.0155e-06, 1.3778e-05, 2.7188e-06,
        4.4234e-06, 7.2450e-06, 3.0360e-06, 1.6977e-05, 1.0910e-05, 8.0259e-06,
        4.1825e-06, 2.3554e-06, 1.2699e-05, 4.2731e-06, 3.2499e-06, 7.6619e-06,
        8.7761e-06, 7.9986e-06, 1.1281e-05, 6.5755e-06, 4.8189e-06, 4.1762e-06,
        9.0235e-06, 1.5491e-05, 8.4362e-06, 5.3114e-06, 9.0850e-06, 5.0943e-06,
        6.9792e-06, 6.5263e-06, 6.0780e-06, 7.8821e-06, 5.3678e-06, 7.4742e-06,
        6.5693e-06, 9.9420e-06, 1.0151e-05, 1.3150e-05, 2.7907e-06, 3.9614e-06,
        2.8598e-06, 3.8699e-06, 6.0780e-06, 4.8305e-06, 9.6339e-06, 3.0129e-06,
        9.7922e-06, 9.2949e-06, 7.6396e-06, 8.3327e-06, 1.0477e-05, 9.3413e-06,
        5.0090e-06, 6.5327e-06, 2.4803e-06, 3.0155e-06, 5.6275e-06, 5.6592e-06,
        5.5749e-06, 1.9210e-06, 2.4205e-06, 6.0780e-06, 3.4038e-06, 3.3356e-06,
        2.9299e-06, 4.2731e-06, 5.6456e-06, 7.0668e-06, 4.1825e-06, 5.5771e-06,
        3.1546e-06, 4.4617e-06, 3.7829e-06, 8.3807e-06, 9.9145e-06, 9.1251e-06,
        3.1282e-06, 8.7761e-06, 3.7829e-06, 3.0774e-06, 6.9532e-06, 2.7894e-06,
        1.8790e-06, 2.5324e-06, 4.7555e-06, 3.3356e-06, 1.0039e-05, 6.5988e-06,
        9.1251e-06, 4.7371e-06, 5.6144e-06, 6.6223e-06, 1.1549e-05, 7.8860e-06,
        4.0049e-06, 1.0587e-05, 7.3001e-06, 7.6396e-06, 4.8717e-06, 4.8799e-06,
        3.6885e-06, 8.0232e-06, 2.3373e-06, 3.0472e-06, 5.6144e-06, 7.9986e-06,
        1.5681e-06, 1.1809e-05, 1.0568e-05, 8.4600e-06, 1.4824e-06, 2.2194e-06,
        7.8437e-06, 3.9229e-06, 6.4939e-06, 5.3887e-06, 6.2518e-06, 8.2702e-06,
        7.2142e-06, 1.0604e-05, 4.7432e-06, 4.1762e-06, 9.7356e-06, 8.9199e-06,
        7.8590e-06, 5.5642e-06, 6.5693e-06, 5.6144e-06, 4.0932e-06, 8.3769e-06,
        4.6592e-06, 2.9224e-06, 2.3698e-07, 7.2311e-06, 4.6105e-06, 1.0935e-06,
        1.1694e-05, 3.9302e-06, 4.2552e-06, 6.5383e-06, 5.0021e-06, 6.7694e-06,
        5.3776e-06, 1.2132e-06, 3.2512e-06, 4.1605e-06, 9.7075e-06, 1.2595e-05,
        1.4164e-05, 4.4234e-06, 2.5399e-06, 8.3507e-06, 3.8699e-06, 1.4882e-06,
        6.5194e-06, 8.6010e-06, 8.5898e-06, 3.1677e-06, 4.2732e-06, 3.8866e-06,
        9.5917e-06, 8.3785e-06, 6.5675e-06, 1.4057e-05, 5.2585e-06, 1.0604e-05,
        5.3868e-06, 1.1908e-05, 4.8478e-06, 6.4814e-06, 5.8921e-06, 7.0321e-06,
        4.7424e-06, 3.9302e-06, 4.7424e-06, 5.5642e-06, 5.0778e-06, 4.2239e-06,
        7.4245e-06, 5.6591e-06, 1.1549e-05, 9.2971e-06, 9.3839e-06, 3.9504e-06,
        6.7644e-06, 4.1549e-06, 7.9585e-06, 3.1282e-06, 2.7408e-06, 4.2552e-06,
        6.1121e-06, 7.2142e-06, 6.2316e-07, 3.1677e-06, 6.8740e-06, 4.6424e-06,
        4.9494e-06, 5.3784e-06, 1.1533e-05, 7.7797e-06, 1.0555e-05, 2.5923e-06,
        1.4914e-05, 3.6660e-06, 4.2721e-06, 3.8912e-06, 4.8478e-06, 5.3114e-06,
        1.9799e-06, 6.6782e-06, 6.8188e-06, 4.1439e-06, 2.4244e-06, 5.4985e-06,
        3.6697e-06, 4.4111e-06, 2.5923e-06, 4.7371e-06, 9.9420e-06, 3.7829e-06,
        6.8740e-06, 1.1281e-05, 3.3356e-06, 1.0401e-05, 1.3739e-05, 6.0780e-06,
        6.2476e-07, 5.8893e-06, 9.1559e-06, 7.4727e-06, 9.1251e-06, 4.8305e-06,
        3.2470e-06, 8.7761e-06, 2.8882e-06, 5.9877e-06, 6.4907e-06, 5.7346e-06,
        2.4803e-06, 1.1281e-05, 8.2122e-06, 3.8054e-06, 1.0477e-05, 4.6878e-06,
        5.7386e-06, 7.2355e-06, 9.0696e-06, 6.8118e-06, 7.0586e-06, 7.8860e-06,
        9.9420e-06, 6.6224e-06, 3.5824e-06, 2.4918e-06, 4.5628e-06, 3.6123e-06,
        1.3657e-05, 5.4247e-06, 7.4245e-06, 1.4522e-05, 7.2318e-06, 5.7411e-06,
        3.4038e-06, 7.5301e-06, 2.5177e-06, 9.4293e-06, 5.2076e-06, 2.9348e-06,
        5.2609e-06, 5.3868e-06, 6.9067e-06, 9.2949e-06, 4.2699e-06, 3.4243e-06,
        8.9959e-06, 6.9769e-06, 1.1145e-05, 7.8821e-06, 5.7625e-06, 5.0083e-06,
        5.8893e-06, 3.8558e-06, 5.3098e-06, 5.0716e-06, 1.3739e-05, 3.5376e-06,
        5.4704e-06, 5.0612e-06, 7.8262e-06, 4.7600e-06, 7.8821e-06, 9.8079e-06,
        5.6624e-06, 4.2248e-06, 8.5629e-06, 5.3868e-06, 1.3295e-05, 7.5338e-06,
        8.1831e-06, 4.9635e-06, 3.6094e-06, 7.8735e-06, 1.0995e-05, 8.5157e-06,
        4.7600e-06, 7.2142e-06, 1.1549e-05, 4.3230e-06, 2.7156e-06, 6.3138e-06,
        8.9548e-06, 8.5898e-06, 8.0766e-06, 8.4154e-06, 6.8335e-06, 9.4063e-06,
        4.7805e-06, 5.4128e-06, 6.4294e-06, 3.1098e-06, 5.9830e-06, 8.7762e-06,
        5.9877e-06, 3.9302e-06, 3.1052e-06, 7.7196e-06, 8.0696e-06, 8.1510e-06,
        7.2449e-06, 1.9494e-06, 4.2748e-06, 2.8882e-06, 1.0793e-05, 5.6592e-06,
        7.5010e-06, 7.0668e-06, 6.9711e-06, 1.3362e-06, 4.6592e-06, 5.6484e-06,
        8.7505e-06, 1.5185e-05, 6.0780e-06, 1.3150e-05, 8.7935e-06, 2.4803e-06,
        1.1809e-05, 6.5746e-06, 3.8699e-06, 5.8893e-06, 6.1160e-06, 8.2702e-06,
        9.8094e-06, 4.2748e-06, 8.9959e-06, 3.9614e-06, 7.6065e-06, 9.4176e-06,
        1.1549e-05, 4.2993e-06, 3.1198e-07, 2.0603e-06, 5.7498e-06, 3.2086e-06,
        2.2869e-06, 5.7498e-06, 1.0829e-05, 8.3507e-06, 4.7676e-06, 1.2152e-05,
        8.3068e-06, 5.6345e-06, 7.4727e-06, 3.1198e-07, 2.7894e-06, 3.1546e-06,
        8.5054e-06, 1.1549e-05, 7.9530e-06, 1.0610e-05, 3.6697e-06, 2.8717e-06,
        8.8800e-06, 6.3515e-06, 1.3739e-05, 5.0612e-06, 1.2853e-05, 4.9554e-06,
        6.0576e-06, 7.8262e-06, 6.0784e-06, 2.4918e-06, 1.0616e-06, 7.6521e-06,
        4.2993e-06, 4.2731e-06, 8.6376e-06, 2.2869e-06, 9.9182e-06, 6.8402e-06,
        8.1493e-06, 1.4813e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.1816e-07, 3.9600e-07, 5.4262e-07,  ..., 2.7937e-07, 5.1816e-07,
        4.4903e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0938, 1.1720, 0.8304, 0.9105, 0.9985, 0.9563, 1.2612, 1.2005, 1.4260,
        1.1790, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,
        0.0182], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 0.03703703731298447, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111119389534, 0.0, 1.0, 0.0, 0.0, 0.0, 0.03703703731298447, 0.0, 1.0, 0.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.03703703731298447, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.34375, 0.359375, 0.359375, 0.328125, 0.34375, 0.34375, 1.0, 0.34375, 0.328125, 1.0, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 0.328125, 0.328125, 0.328125, 0.34375, 0.328125, 0.34375, 1.0, 1.0, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.65625, 0.375, 0.359375, 0.328125, 1.0, 0.34375, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.328125, 0.34375, 0.34375, 0.34375, 0.34375, 0.375, 0.34375, 1.0, 0.34375, 1.0, 0.40625, 0.328125, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.328125, 0.390625, 0.328125]

 sparsity of   [0.3194444477558136, 0.3211805522441864, 0.3333333432674408, 0.3194444477558136, 0.3246527910232544, 1.0, 0.3246527910232544, 1.0, 0.3263888955116272, 1.0, 1.0, 0.3194444477558136, 0.315972238779068, 1.0, 0.3194444477558136, 0.328125, 0.347222238779068, 0.3263888955116272, 1.0, 0.315972238779068, 1.0, 0.3333333432674408, 1.0, 0.3229166567325592, 0.3333333432674408, 0.3194444477558136, 0.34375, 0.3333333432674408, 1.0, 0.331597238779068, 0.3402777910232544, 1.0, 0.3177083432674408, 1.0, 0.3177083432674408, 0.3298611044883728, 0.3194444477558136, 1.0, 1.0, 1.0, 0.3211805522441864, 1.0, 0.3194444477558136, 1.0, 0.3229166567325592, 1.0, 0.3211805522441864, 0.315972238779068, 0.3194444477558136, 1.0, 1.0, 0.3177083432674408, 0.647569477558136, 0.3263888955116272, 0.375, 0.3229166567325592, 0.328125, 0.3229166567325592, 0.3229166567325592, 1.0, 1.0, 0.3211805522441864, 0.3263888955116272, 0.3211805522441864]

 sparsity of   [0.3125, 0.296875, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 1.0, 0.359375, 0.3125, 0.328125, 0.3125, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 0.328125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.3125, 1.0, 0.3125, 0.296875, 1.0, 0.296875, 0.34375, 0.296875, 0.28125, 0.296875, 0.296875, 0.296875, 0.28125, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.296875, 0.3125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.296875, 1.0, 0.296875, 1.0, 1.0, 0.328125, 0.3125, 0.3125, 0.390625, 0.296875, 1.0, 1.0, 1.0, 1.0, 0.328125, 0.296875, 0.34375, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.3125, 0.328125, 0.296875, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.328125, 1.0, 0.296875, 0.328125, 0.328125, 1.0, 1.0, 0.3125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 1.0, 0.328125, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.296875, 0.3125, 0.296875, 0.3125, 0.3125, 0.359375, 0.296875, 0.3125, 1.0, 1.0, 0.3125, 0.296875, 1.0, 0.3125, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.296875, 0.296875, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.296875, 0.296875, 0.3125, 0.296875, 1.0, 0.3125, 0.3125, 0.28125, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 1.0, 0.296875, 0.296875, 0.359375, 0.296875, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 0.328125, 0.328125, 1.0, 1.0, 0.34375, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 1.0, 0.328125, 0.3125, 1.0, 0.328125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.328125, 0.296875, 0.296875, 0.328125, 0.3125, 0.28125, 1.0, 0.296875, 0.296875, 0.328125, 0.3125, 0.3125, 0.296875, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 1.0, 1.0, 0.3125, 0.296875, 0.359375, 0.296875, 0.3125, 0.328125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.34375, 1.0, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.3125, 0.296875, 0.34375, 0.328125, 1.0, 0.296875, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 0.296875, 0.3125]

 sparsity of   [0.328125, 0.375, 0.34375, 1.0, 0.34375, 0.34375, 0.328125, 1.0, 0.328125, 0.34375, 0.34375, 0.40625, 0.34375, 1.0, 0.375, 0.328125, 1.0, 0.328125, 0.328125, 0.375, 0.328125, 0.359375, 1.0, 0.359375, 1.0, 0.328125, 0.34375, 1.0, 0.34375, 0.328125, 0.359375, 0.34375, 0.359375, 0.375, 0.375, 0.34375, 0.328125, 0.34375, 0.34375, 1.0, 0.421875, 0.328125, 0.328125, 0.328125, 0.328125, 0.359375, 0.375, 0.390625, 0.40625, 1.0, 0.40625, 1.0, 1.0, 0.359375, 0.328125, 0.34375, 0.375, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.328125, 0.34375, 0.375, 0.34375, 0.34375, 1.0, 0.34375, 0.328125, 0.34375, 0.328125, 0.34375, 0.421875, 0.328125, 1.0, 0.375, 0.359375, 0.328125, 0.34375, 0.34375, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.34375, 0.359375, 0.390625, 0.34375, 1.0, 0.328125, 0.390625, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.375, 0.34375, 0.328125, 0.34375, 1.0, 0.328125, 1.0, 0.375, 0.34375, 0.359375, 0.40625, 0.375, 0.328125, 1.0, 0.328125, 0.328125, 0.328125, 0.359375, 1.0, 0.34375, 0.359375, 0.359375, 0.359375, 0.328125, 0.34375, 0.328125, 0.359375, 0.328125, 1.0, 1.0, 0.328125, 0.359375, 1.0, 0.359375, 0.34375, 0.328125, 0.34375, 0.359375, 0.375, 0.359375, 0.34375, 0.328125, 0.390625, 0.328125, 0.328125, 0.375, 0.375, 0.375, 0.34375, 0.328125, 1.0, 0.328125, 0.328125, 0.359375, 0.375, 0.34375, 0.375, 1.0, 0.328125, 0.359375, 0.34375, 1.0, 0.34375, 1.0, 0.34375, 0.359375, 0.328125, 0.375, 0.34375, 0.375, 0.328125, 0.328125, 0.4375, 0.34375, 0.328125, 1.0, 1.0, 0.328125, 0.328125, 0.328125, 0.34375, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.34375, 1.0, 1.0, 0.328125, 0.34375, 0.34375, 0.34375, 0.359375, 0.34375, 0.328125, 0.34375, 0.34375, 0.328125, 0.328125, 0.359375, 0.40625, 0.359375, 0.328125, 0.34375, 0.34375, 0.40625, 0.328125, 0.328125, 0.328125, 0.328125, 0.34375, 0.359375, 0.328125, 0.359375, 0.359375, 0.328125, 0.328125, 1.0, 0.34375, 1.0, 1.0, 0.328125, 0.359375, 1.0, 0.390625, 0.328125, 0.34375, 0.328125, 0.328125, 0.34375, 0.421875, 0.328125, 0.328125, 1.0, 0.359375, 0.359375, 0.359375, 0.34375, 1.0, 0.328125, 0.34375, 0.34375, 0.328125, 1.0, 0.375, 0.34375, 0.328125, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 0.34375, 0.359375]

 sparsity of   [1.0, 0.1796875, 0.17578125, 0.1875, 1.0, 1.0, 1.0, 0.17578125, 1.0, 0.1875, 0.1796875, 0.171875, 0.18359375, 1.0, 0.1796875, 1.0, 0.19140625, 0.1796875, 1.0, 0.1953125, 0.19921875, 0.203125, 0.17578125, 0.1875, 0.18359375, 0.203125, 0.1875, 0.1875, 1.0, 0.1796875, 0.1953125, 1.0, 1.0, 0.16796875, 0.18359375, 0.171875, 0.1875, 0.18359375, 1.0, 0.19140625, 1.0, 0.18359375, 0.19140625, 0.19921875, 0.18359375, 0.17578125, 0.18359375, 0.1640625, 0.19140625, 0.21875, 0.18359375, 0.171875, 1.0, 0.18359375, 0.171875, 0.16796875, 1.0, 1.0, 1.0, 0.1875, 0.19140625, 1.0, 0.1640625, 0.18359375]

 sparsity of   [0.284722238779068, 0.2673611044883728, 1.0, 1.0, 0.2795138955116272, 1.0, 0.28125, 0.2760416567325592, 0.269097238779068, 0.2795138955116272, 0.2743055522441864, 0.28125, 0.284722238779068, 0.284722238779068, 1.0, 0.2743055522441864, 0.28125, 0.2725694477558136, 1.0, 0.269097238779068, 0.2743055522441864, 0.2743055522441864, 0.2934027910232544, 0.2725694477558136, 1.0, 0.2673611044883728, 1.0, 0.284722238779068, 1.0, 1.0, 0.2829861044883728, 0.2881944477558136, 0.2760416567325592, 0.2725694477558136, 0.2743055522441864, 0.2829861044883728, 0.2743055522441864, 0.265625, 0.269097238779068, 1.0, 1.0, 0.28125, 0.2743055522441864, 1.0, 1.0, 1.0, 1.0, 0.2777777910232544, 0.2864583432674408, 1.0, 0.2743055522441864, 0.2708333432674408, 0.2864583432674408, 1.0, 0.2760416567325592, 0.2725694477558136, 0.2864583432674408, 0.2760416567325592, 0.2725694477558136, 1.0, 1.0, 0.2725694477558136, 0.265625, 1.0]

 sparsity of   [0.3125, 0.28125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 1.0, 0.296875, 0.296875, 0.328125, 0.3125, 0.296875, 0.296875, 0.34375, 0.28125, 0.3125, 1.0, 0.3125, 0.328125, 0.296875, 0.296875, 1.0, 0.3125, 1.0, 0.328125, 0.3125, 0.296875, 0.3125, 0.328125, 0.34375, 0.296875, 0.3125, 0.296875, 0.296875, 0.28125, 0.296875, 0.296875, 0.296875, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 0.328125, 1.0, 0.28125, 0.296875, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 0.296875, 1.0, 0.375, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.296875, 0.296875, 0.296875, 0.3125, 0.296875, 0.328125, 0.296875, 1.0, 0.28125, 0.3125, 0.296875, 0.296875, 0.328125, 0.3125, 0.3125, 0.296875, 0.3125, 0.453125, 0.328125, 0.296875, 0.3125, 1.0, 0.328125, 0.296875, 0.296875, 1.0, 0.3125, 0.296875, 0.296875, 0.3125, 0.296875, 0.34375, 0.3125, 0.296875, 1.0, 1.0, 0.296875, 0.34375, 1.0, 0.3125, 0.296875, 1.0, 0.328125, 0.28125, 0.3125, 0.3125, 0.296875, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.28125, 0.296875, 0.34375, 0.296875, 0.328125, 0.34375, 0.328125, 0.296875, 0.3125, 0.34375, 0.3125, 0.3125, 0.28125, 0.296875, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.296875, 0.296875, 0.296875, 0.3125, 0.328125, 0.296875, 0.328125, 0.296875, 0.328125, 0.296875, 0.296875, 0.296875, 0.3125, 0.359375, 0.296875, 0.3125, 0.296875, 0.3125, 0.328125, 0.296875, 0.296875, 0.28125, 1.0, 0.296875, 0.3125, 0.328125, 1.0, 0.296875, 0.3125, 0.296875, 0.296875, 0.28125, 0.328125, 0.328125, 0.296875, 0.3125, 0.3125, 0.296875, 0.28125, 0.296875, 1.0, 0.28125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.296875, 0.296875, 0.28125, 0.3125, 0.3125, 0.3125, 0.296875, 0.515625, 0.296875, 0.328125, 0.296875, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.296875, 0.3125, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 0.3125, 0.3125, 0.3125, 0.28125, 0.296875, 0.296875, 0.328125, 0.296875, 0.328125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 0.328125, 0.328125, 1.0, 0.296875, 0.3125, 1.0, 0.3125, 0.296875, 0.359375, 0.3125, 0.296875, 0.296875, 0.296875, 0.3125, 1.0, 1.0, 1.0, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.28125, 0.296875, 0.3125, 1.0, 0.296875, 0.3125, 0.296875, 0.296875, 0.296875, 0.328125, 0.296875, 0.3125, 0.328125, 0.28125]

 sparsity of   [1.0, 0.109375, 1.0, 0.0625, 1.0, 0.078125, 1.0, 0.0703125, 0.06640625, 0.078125, 0.09765625, 1.0, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 0.08984375, 1.0, 1.0, 1.0, 0.0703125, 1.0, 0.10546875, 0.1015625, 0.08203125, 0.08984375, 0.08203125, 0.08203125, 0.078125, 0.0703125, 0.06640625, 1.0, 0.0625, 0.09375, 0.07421875, 0.07421875, 0.06640625, 0.08984375, 0.0859375, 0.07421875, 0.0703125, 1.0, 0.078125, 0.1015625, 0.1015625, 0.06640625, 0.11328125, 0.09375, 0.078125, 0.06640625, 1.0, 1.0, 0.078125, 0.0703125, 0.0703125, 0.0703125, 0.078125, 0.07421875, 0.09375, 0.06640625, 0.0625, 0.078125, 0.0625]

 sparsity of   [0.2482638955116272, 0.25, 0.2482638955116272, 0.253472238779068, 1.0, 0.2482638955116272, 0.28125, 0.2638888955116272, 0.2829861044883728, 1.0, 0.2569444477558136, 0.2517361044883728, 0.2569444477558136, 0.3229166567325592, 0.25, 0.2621527910232544, 0.25, 0.253472238779068, 0.2569444477558136, 1.0, 1.0, 1.0, 0.2465277761220932, 0.2795138955116272, 1.0, 1.0, 0.265625, 1.0, 0.2552083432674408, 0.2604166567325592, 1.0, 1.0, 0.2604166567325592, 0.2829861044883728, 0.2621527910232544, 0.2708333432674408, 1.0, 0.2673611044883728, 0.2465277761220932, 0.2829861044883728, 1.0, 0.269097238779068, 0.2673611044883728, 1.0, 1.0, 0.253472238779068, 0.2552083432674408, 1.0, 0.2447916716337204, 0.2465277761220932, 1.0, 0.2517361044883728, 0.2465277761220932, 1.0, 1.0, 1.0, 0.2447916716337204, 0.253472238779068, 0.25, 0.2743055522441864, 0.2795138955116272, 0.2760416567325592, 0.2309027761220932, 1.0]

 sparsity of   [0.3125, 0.3125, 0.3125, 0.3125, 0.375, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.328125, 0.328125, 0.3125, 0.3125, 0.34375, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 0.328125, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 1.0, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 0.3125, 0.3125, 0.3125, 0.359375, 0.34375, 0.3125, 0.34375, 0.3125, 0.328125, 0.34375, 1.0, 0.3125, 0.3125, 0.34375, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 1.0, 0.328125, 0.3125, 1.0, 0.328125, 1.0, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 0.3125, 0.34375, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 0.328125, 0.34375, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.359375, 0.359375, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.3125, 0.3125, 0.34375, 0.3125, 0.34375, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.359375, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 0.3125, 0.328125, 1.0, 0.3125, 0.3125, 0.390625, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.34375, 0.3125, 0.328125, 0.3125, 0.328125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.359375, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.34375, 0.3125, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 1.0, 0.34375, 1.0, 0.3125, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.328125, 1.0, 0.328125, 0.3125]

 sparsity of   [1.0, 0.00390625, 0.01171875, 1.0, 0.015625, 0.015625, 0.00390625, 0.015625, 0.0078125, 1.0, 1.0, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.00390625, 0.015625, 0.01953125, 0.03515625, 0.0078125, 0.00390625, 1.0, 0.0234375, 0.01953125, 1.0, 1.0, 0.00390625, 1.0, 1.0, 0.01171875, 0.00390625, 0.0078125, 0.03515625, 0.02734375, 0.01953125, 0.01171875, 0.01171875, 0.09765625, 1.0, 0.03125, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 1.0, 0.01171875, 0.0078125, 0.00390625, 1.0, 0.01953125, 0.01953125, 1.0, 0.01171875, 1.0, 0.00390625, 0.0078125, 0.00390625, 0.0625, 1.0, 0.00390625, 0.00390625, 0.015625, 1.0, 0.0078125, 0.01171875, 0.00390625, 1.0, 0.01171875, 0.0234375, 0.01171875, 0.01953125, 0.00390625, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.01953125, 1.0, 0.0546875, 0.00390625, 1.0, 0.00390625, 0.01171875, 0.01171875, 1.0, 0.01953125, 0.15234375, 0.0078125, 0.046875, 0.0234375, 0.01953125, 0.0078125, 1.0, 1.0, 0.01953125, 0.0234375, 0.00390625, 1.0, 0.01953125, 0.00390625, 1.0, 0.01953125, 0.015625, 0.01171875, 1.0, 1.0, 0.0078125, 1.0, 0.01171875, 0.00390625, 0.00390625, 1.0, 0.01171875, 0.0078125, 0.03515625, 0.00390625, 1.0, 1.0, 0.015625, 0.00390625, 0.03125, 0.0234375, 1.0, 1.0, 1.0, 0.00390625]

 sparsity of   [0.2838541567325592, 0.2795138955116272, 0.296875, 1.0, 1.0, 1.0, 0.2890625, 0.3046875, 0.2760416567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2916666567325592, 1.0, 0.2855902910232544, 1.0, 1.0, 0.3602430522441864, 0.2829861044883728, 1.0, 1.0, 0.2951388955116272, 1.0, 1.0, 1.0, 1.0, 0.3012152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3454861044883728, 1.0, 1.0, 1.0, 0.276909738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3402777910232544, 1.0, 0.2899305522441864, 0.2951388955116272, 0.2881944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2890625, 1.0, 0.3020833432674408, 0.2899305522441864, 1.0, 0.3151041567325592, 1.0, 0.2795138955116272, 0.2855902910232544, 1.0, 0.2795138955116272, 1.0, 0.2960069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2743055522441864, 1.0, 1.0, 1.0, 1.0, 0.2934027910232544, 0.3142361044883728, 1.0, 0.296875, 0.2829861044883728, 1.0, 1.0, 1.0, 0.284722238779068, 1.0, 1.0, 0.284722238779068, 1.0, 1.0, 1.0, 0.2890625, 1.0, 0.3645833432674408, 1.0, 1.0, 1.0, 0.2760416567325592, 0.2821180522441864, 0.3515625, 1.0, 1.0, 0.3229166567325592, 0.2708333432674408, 1.0, 0.3333333432674408, 1.0, 1.0, 0.2994791567325592, 1.0, 0.2664930522441864, 1.0, 1.0, 0.2873263955116272, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.65625, 1.0, 1.0, 1.0, 0.609375, 0.625, 0.609375, 1.0, 0.6015625, 0.625, 0.609375, 1.0, 0.6171875, 1.0, 0.625, 0.640625, 0.6015625, 0.6171875, 1.0, 0.625, 0.6171875, 1.0, 1.0, 0.625, 1.0, 0.640625, 1.0, 0.6171875, 0.625, 0.640625, 0.6171875, 0.6328125, 0.609375, 0.6171875, 1.0, 0.640625, 0.6171875, 0.625, 0.6171875, 0.6328125, 0.609375, 0.6015625, 0.625, 0.6171875, 1.0, 0.6171875, 0.6484375, 0.6328125, 0.625, 1.0, 0.6015625, 0.609375, 0.609375, 0.625, 0.609375, 1.0, 0.6171875, 0.609375, 0.6328125, 0.625, 0.640625, 0.6171875, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.671875, 0.6484375, 0.6328125, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 0.609375, 0.6171875, 0.640625, 0.6328125, 1.0, 0.640625, 0.6328125, 1.0, 1.0, 0.6015625, 0.6328125, 0.640625, 1.0, 0.65625, 0.6171875, 0.6328125, 0.625, 0.625, 1.0, 0.625, 0.625, 1.0, 0.640625, 1.0, 0.6328125, 0.6015625, 0.609375, 0.6171875, 0.6171875, 0.609375, 0.6484375, 0.671875, 0.6328125, 0.640625, 1.0, 0.609375, 0.625, 0.625, 1.0, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.640625, 1.0, 0.609375, 1.0, 0.609375, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.625, 0.6171875, 0.609375, 1.0, 0.625, 0.6328125, 0.625, 0.609375, 0.6171875, 0.65625, 0.6328125, 1.0, 1.0, 1.0, 0.640625, 0.6171875, 0.6171875, 0.6484375, 1.0, 0.6015625, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.609375, 0.6171875, 0.625, 0.6171875, 0.6171875, 1.0, 0.6484375, 1.0, 1.0, 1.0, 1.0, 0.609375, 0.625, 0.609375, 1.0, 0.6171875, 1.0, 1.0, 0.6328125, 0.640625, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 0.6171875, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.6015625, 0.6640625, 0.6171875, 0.625, 0.6328125, 0.609375, 0.609375, 0.6171875, 0.6171875, 0.640625, 0.6328125, 0.625, 1.0, 1.0, 0.609375, 0.6328125, 0.6328125, 1.0, 0.6328125, 1.0, 0.640625, 0.6171875, 1.0, 0.6015625, 0.6328125, 0.625, 0.6171875, 0.6328125, 1.0, 0.6484375, 0.6328125, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6328125, 0.625, 0.65625, 0.6171875, 1.0, 0.625, 0.6171875, 0.640625, 1.0, 0.625, 1.0, 1.0, 1.0, 0.6015625, 0.6796875, 0.65625, 0.640625, 0.6171875, 0.640625, 0.6171875, 0.6171875, 1.0, 1.0, 0.640625, 0.6640625, 0.609375, 0.609375, 1.0, 1.0, 0.6171875, 1.0, 0.6015625, 1.0, 1.0, 0.625, 1.0, 0.609375, 1.0, 0.6171875, 1.0, 0.6328125, 0.625, 1.0, 0.6328125, 1.0, 0.625, 0.625, 0.640625, 0.6171875, 0.6171875, 0.6171875, 0.625, 1.0, 0.625, 1.0, 0.6484375, 1.0, 0.65625, 0.609375, 0.6171875, 1.0, 0.640625, 0.6171875, 1.0, 1.0, 0.625, 1.0, 0.6171875, 0.609375, 0.6328125, 1.0, 0.609375, 0.609375, 0.6171875, 1.0, 1.0, 0.625, 0.625, 1.0, 0.640625, 0.6484375, 0.6171875, 0.625, 1.0, 0.625, 0.6328125, 0.6171875, 1.0, 0.6328125, 0.609375, 0.6171875, 0.6328125, 0.625, 0.6171875, 0.6171875, 0.6640625, 1.0, 0.6171875, 0.6171875, 1.0, 0.625, 0.6328125, 0.6171875, 0.6328125, 1.0, 0.6328125, 0.625, 0.609375, 0.6640625, 0.6015625, 1.0, 1.0, 0.625, 0.6171875, 0.609375, 0.609375, 1.0, 0.640625, 0.6328125, 0.625, 0.6171875, 0.6171875, 0.625, 0.625, 0.6171875, 1.0, 0.625, 0.6171875, 1.0, 0.6328125, 0.609375, 0.6171875, 1.0, 1.0, 0.6171875, 0.625, 1.0, 1.0, 0.6015625, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.640625, 1.0, 0.6171875, 0.6484375, 0.6171875, 0.640625, 1.0, 0.6328125, 0.6328125, 1.0, 0.625, 0.6171875, 0.625, 0.625, 1.0, 0.6328125, 0.6171875, 1.0, 0.625, 0.640625, 0.6171875, 0.625, 0.6171875, 0.625, 1.0, 0.6171875, 0.6171875, 0.6484375, 1.0, 1.0, 0.6171875, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.6328125, 0.6328125, 0.625, 0.6171875, 0.609375, 1.0, 1.0, 0.625, 0.640625, 1.0, 0.6484375, 0.6328125, 0.65625, 1.0, 0.6328125, 0.640625, 0.6171875, 1.0, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.65625, 0.6171875, 0.625, 0.6171875, 1.0, 0.625, 0.6328125, 0.6171875, 0.6171875, 1.0, 0.65625, 1.0, 0.6171875, 0.609375, 0.6328125, 0.6171875, 0.65625, 1.0, 0.609375, 0.6328125, 1.0, 0.609375, 0.609375, 0.6328125, 0.609375, 0.6328125, 1.0, 0.640625, 1.0, 0.6328125, 1.0, 0.65625, 1.0, 0.640625, 0.6171875, 0.625, 0.625, 1.0, 0.625, 0.625, 1.0, 0.609375, 1.0, 1.0, 0.6328125, 1.0, 0.640625, 0.609375, 0.625, 0.6171875, 0.65625, 0.625, 0.6640625]

 sparsity of   [1.0, 0.00390625, 1.0, 1.0, 0.015625, 0.01171875, 0.046875, 0.0078125, 1.0, 0.01953125, 0.0234375, 0.01953125, 1.0, 0.0078125, 1.0, 0.0078125, 0.03515625, 0.0078125, 0.01171875, 1.0, 0.01953125, 0.01171875, 1.0, 1.0, 0.0703125, 1.0, 0.01953125, 1.0, 0.015625, 0.0078125, 0.01171875, 0.00390625, 0.01953125, 0.015625, 0.00390625, 1.0, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.01171875, 0.02734375, 0.03125, 0.015625, 0.015625, 1.0, 0.0078125, 0.0078125, 0.03515625, 0.01171875, 1.0, 0.01171875, 0.00390625, 0.015625, 0.02734375, 0.01171875, 0.01171875, 0.0234375, 0.0078125, 0.01953125, 0.01953125, 0.015625, 0.01171875, 1.0, 1.0, 0.02734375, 0.01171875, 1.0, 0.01171875, 0.01171875, 0.0078125, 1.0, 1.0, 0.01953125, 1.0, 1.0, 1.0, 1.0, 0.015625, 1.0, 1.0, 0.03515625, 0.01171875, 0.00390625, 0.00390625, 0.01171875, 0.00390625, 0.015625, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 1.0, 0.109375, 0.0078125, 0.015625, 0.0078125, 0.01953125, 0.015625, 0.0078125, 0.015625, 1.0, 0.00390625, 1.0, 0.02734375, 0.015625, 0.015625, 0.01171875, 0.0234375, 0.01953125, 0.0078125, 1.0, 0.0546875, 0.0078125, 1.0, 0.0078125, 0.015625, 0.01171875, 0.0234375, 1.0, 0.11328125, 0.03515625, 0.0078125, 1.0, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.0234375, 1.0, 0.015625, 0.01953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0234375, 0.01171875, 0.03125, 0.08984375, 1.0, 0.01171875, 0.01171875, 0.015625, 0.0078125, 0.01171875, 0.00390625, 0.01953125, 1.0, 1.0, 1.0, 0.0078125, 0.01171875, 0.0234375, 1.0, 1.0, 0.0078125, 0.0078125, 0.00390625, 1.0, 1.0, 1.0, 1.0, 0.0078125, 0.0078125, 0.01171875, 0.00390625, 0.03125, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.01171875, 0.0078125, 0.0078125, 0.0078125, 0.01171875, 1.0, 1.0, 0.02734375, 0.00390625, 1.0, 0.01171875, 1.0, 0.01171875, 1.0, 0.0078125, 0.00390625, 0.015625, 1.0, 1.0, 0.01953125, 1.0, 0.01171875, 1.0, 1.0, 1.0, 0.015625, 1.0, 0.01171875, 0.01171875, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.015625, 0.015625, 0.01171875, 1.0, 1.0, 0.01171875, 0.02734375, 0.01953125, 1.0, 1.0, 0.51953125, 0.01171875, 0.015625, 1.0, 0.0390625, 0.05859375, 0.01171875, 0.00390625, 0.0078125, 1.0, 0.015625, 0.03125, 0.03125, 1.0, 1.0, 0.015625, 1.0, 1.0, 1.0, 0.00390625, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.01171875, 0.046875, 0.0078125, 0.00390625, 0.00390625, 1.0, 0.0078125, 0.01171875, 0.01953125, 1.0, 0.015625, 1.0, 1.0, 1.0, 0.015625, 1.0, 0.0078125, 0.0078125, 0.0234375, 0.015625, 0.0078125, 0.0078125, 1.0, 1.0, 0.0078125, 0.01953125, 0.01171875, 0.0234375, 1.0, 1.0, 0.01953125, 1.0, 0.1015625, 1.0, 1.0, 0.015625, 1.0, 0.0078125, 1.0, 0.015625, 1.0, 0.01171875, 0.0078125, 1.0, 0.0078125, 1.0, 0.01953125, 0.03125, 0.015625, 0.01171875, 0.01953125, 0.0078125, 0.015625, 1.0, 0.03125, 1.0, 0.015625, 1.0, 0.015625, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.01171875, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 0.01171875, 0.03515625, 1.0, 0.0078125, 0.00390625, 0.0078125, 1.0, 1.0, 0.01171875, 0.01953125, 1.0, 0.00390625, 0.015625, 0.0234375, 0.01171875, 1.0, 0.0078125, 0.00390625, 0.0078125, 0.015625, 0.00390625, 0.0078125, 0.01953125, 0.01953125, 0.015625, 0.0234375, 0.0078125, 0.0078125, 1.0, 0.01171875, 0.0234375, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.01953125, 0.0234375, 0.015625, 0.01171875, 0.00390625, 1.0, 1.0, 0.125, 0.015625, 0.01953125, 0.01171875, 1.0, 0.015625, 0.01171875, 0.01171875, 0.00390625, 0.0078125, 0.01171875, 0.02734375, 0.00390625, 1.0, 0.01171875, 0.01171875, 1.0, 0.01171875, 0.0078125, 0.01171875, 1.0, 1.0, 0.01171875, 0.01171875, 1.0, 1.0, 0.00390625, 0.01171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.0078125, 1.0, 0.03515625, 1.0, 0.015625, 1.0, 0.01171875, 1.0, 1.0, 0.00390625, 0.00390625, 1.0, 0.0078125, 0.0859375, 0.05078125, 0.01171875, 0.015625, 0.01171875, 0.0234375, 1.0, 0.01171875, 0.01171875, 0.02734375, 0.015625, 0.00390625, 0.01171875, 1.0, 0.015625, 0.0078125, 0.0625, 1.0, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.015625, 0.0078125, 0.10546875, 0.015625, 0.015625, 0.05859375, 1.0, 1.0, 0.0078125, 0.05859375, 0.015625, 1.0, 0.0078125, 0.00390625, 1.0, 0.00390625, 0.01171875, 0.0078125, 1.0, 0.01171875, 0.015625, 0.0078125, 0.01171875, 1.0, 1.0, 0.0078125, 0.0078125, 0.02734375, 1.0, 0.02734375, 0.0390625, 0.00390625, 0.03515625, 1.0, 0.00390625, 1.0, 0.00390625, 0.015625, 0.01171875, 0.0078125, 1.0, 1.0, 0.07421875, 0.0234375, 1.0, 0.0078125, 0.015625, 0.00390625, 0.01171875, 0.03125, 1.0, 0.0078125, 1.0, 0.00390625, 1.0, 0.01171875, 1.0, 0.03515625, 0.0078125, 0.01953125, 0.0078125, 1.0, 0.0078125, 0.0078125, 1.0, 0.01171875, 1.0, 1.0, 0.0078125, 1.0, 0.01171875, 0.00390625, 0.0703125, 0.0078125, 0.12109375, 0.0078125, 0.015625]

 sparsity of   [1.0, 0.31640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 0.298828125, 1.0, 0.314453125, 1.0, 1.0, 1.0, 0.3046875, 0.306640625, 1.0, 0.31640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.296875, 1.0, 0.30078125, 0.3046875, 0.31640625, 1.0, 1.0, 1.0, 1.0, 0.30859375, 0.3046875, 0.31640625, 1.0, 0.314453125, 0.3125, 1.0, 1.0, 0.31640625, 0.294921875, 0.30859375, 1.0, 0.318359375, 1.0, 0.306640625, 1.0, 1.0, 0.30859375, 1.0, 1.0, 0.310546875, 0.30859375, 1.0, 0.3125, 0.314453125, 0.314453125, 1.0, 0.310546875, 1.0, 1.0, 0.3046875, 0.31640625, 1.0, 1.0, 0.306640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 0.310546875, 1.0, 1.0, 1.0, 0.31640625, 1.0, 0.337890625, 0.341796875, 0.296875, 0.302734375, 1.0, 1.0, 0.302734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.302734375, 1.0, 1.0, 0.328125, 1.0, 1.0, 0.3359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.310546875, 0.314453125]

 sparsity of   [1.0, 1.0, 1.0, 0.6076388955116272, 0.6519097089767456, 1.0, 0.5859375, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.5980902910232544, 1.0, 1.0, 0.6050347089767456, 1.0, 1.0, 0.6102430820465088, 0.592881977558136, 0.6024305820465088, 1.0, 0.609375, 1.0, 0.592881977558136, 0.5963541865348816, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 1.0, 1.0, 1.0, 0.6067708134651184, 1.0, 1.0, 1.0, 0.6041666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 0.6293402910232544, 0.5868055820465088, 0.5946180820465088, 0.6354166865348816, 0.616319477558136, 1.0, 1.0, 0.5963541865348816, 1.0, 0.6267361044883728, 0.5963541865348816, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 0.59375, 1.0, 1.0, 0.609375, 1.0, 0.608506977558136, 1.0, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 1.0, 0.6102430820465088, 1.0, 1.0, 0.6805555820465088, 0.5876736044883728, 0.5989583134651184, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6640625, 0.6796875, 1.0, 0.6875, 1.0, 0.6640625, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6640625, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6640625, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6640625, 0.6796875, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6953125, 0.6796875, 0.6640625, 0.6796875, 0.671875, 0.703125, 0.671875, 1.0, 1.0, 0.671875, 1.0, 0.6640625, 0.6875, 0.6796875, 0.671875, 1.0, 0.6796875, 0.671875, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6640625, 0.6875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.6640625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.6640625, 0.6796875, 1.0, 0.6796875, 1.0, 0.6640625, 0.6640625, 0.6875, 0.6875, 0.6640625, 0.6875, 0.6796875, 1.0, 0.6875, 0.6953125, 0.671875, 0.6875, 0.6875, 0.6796875, 1.0, 0.6875, 1.0, 0.6875, 0.6796875, 0.671875, 0.671875, 1.0, 0.6640625, 0.671875, 0.671875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 0.6796875, 0.703125, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6640625, 0.671875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 0.671875, 1.0, 1.0, 0.6640625, 0.671875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6640625, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 0.6796875, 0.6875, 0.671875, 0.7109375, 0.6875, 0.6875, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6640625, 0.6640625, 0.6796875, 1.0, 1.0, 0.6796875, 0.6640625, 1.0, 0.671875, 0.6875, 0.671875, 0.6953125, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6640625, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.6875, 0.6640625, 0.6875, 0.6640625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6875, 0.6640625, 1.0, 0.671875, 0.6640625, 1.0, 1.0, 0.671875, 0.6640625, 0.6875, 0.6796875, 0.6796875, 0.6640625, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 0.671875, 0.6640625, 0.671875, 1.0, 0.6796875, 0.6875, 0.6640625, 1.0, 1.0, 0.6953125, 0.671875, 1.0, 0.6640625, 0.6796875, 0.6875, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.6640625, 0.6953125, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6796875, 1.0, 0.6640625, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.671875, 0.671875, 0.671875, 0.671875, 1.0, 0.671875, 0.671875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.6640625, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6640625, 0.671875, 1.0, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.671875, 0.6875, 0.6640625, 1.0, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6640625, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.7109375, 0.671875, 0.6640625, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6640625, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.6640625, 1.0, 0.6640625, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 0.671875, 0.6875, 0.6640625, 1.0, 0.6875, 0.6796875, 1.0, 0.6953125, 1.0, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6875, 0.671875, 0.6640625, 1.0, 0.6796875, 0.671875]

 sparsity of   [1.0, 1.0, 0.2265625, 0.3046875, 1.0, 0.24609375, 0.23046875, 0.298828125, 0.208984375, 1.0, 1.0, 0.21484375, 1.0, 1.0, 1.0, 0.220703125, 0.21484375, 1.0, 1.0, 0.240234375, 0.2265625, 1.0, 1.0, 1.0, 0.259765625, 1.0, 1.0, 0.25390625, 0.21875, 0.22265625, 1.0, 0.236328125, 0.26953125, 0.21484375, 1.0, 1.0, 0.236328125, 0.220703125, 0.21875, 1.0, 0.216796875, 0.244140625, 0.2265625, 0.22265625, 0.25, 0.28515625, 0.220703125, 1.0, 0.24609375, 1.0, 0.21875, 1.0, 1.0, 0.251953125, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.2265625, 1.0, 1.0, 0.439453125, 1.0, 1.0, 1.0, 0.216796875, 1.0, 0.236328125, 1.0, 0.212890625, 1.0, 0.22265625, 1.0, 0.349609375, 0.2109375, 1.0, 0.302734375, 1.0, 1.0, 1.0, 1.0, 0.23046875, 0.302734375, 1.0, 1.0, 1.0, 1.0, 0.224609375, 0.20703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 0.220703125, 0.25390625, 1.0, 1.0, 1.0, 0.212890625, 1.0, 1.0, 1.0, 0.23046875, 1.0, 1.0, 1.0, 1.0, 0.255859375, 1.0, 0.2890625, 0.228515625, 0.212890625, 0.21875, 1.0, 0.224609375, 0.240234375, 0.232421875, 0.2109375, 0.212890625, 0.345703125, 0.255859375]

 sparsity of   [1.0, 0.5920138955116272, 1.0, 1.0, 0.5868055820465088, 0.487847238779068, 0.4626736044883728, 1.0, 1.0, 0.5564236044883728, 0.5720486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5833333134651184, 1.0, 0.4583333432674408, 1.0, 1.0, 0.480034738779068, 1.0, 1.0, 1.0, 0.4791666567325592, 1.0, 0.4765625, 0.46875, 1.0, 1.0, 0.453125, 0.4921875, 1.0, 0.5798611044883728, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4539930522441864, 1.0, 1.0, 0.4713541567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4739583432674408, 1.0, 1.0, 1.0, 0.4817708432674408, 0.4826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4739583432674408, 1.0, 0.4765625, 1.0, 1.0, 1.0, 0.4652777910232544, 1.0, 1.0, 0.4826388955116272, 1.0, 1.0, 0.5598958134651184, 0.4670138955116272, 1.0, 0.480034738779068, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4774305522441864, 1.0, 1.0, 1.0, 1.0, 0.4626736044883728, 1.0, 0.4661458432674408, 1.0, 0.4748263955116272, 0.4678819477558136, 0.5494791865348816, 0.4835069477558136, 1.0, 0.4869791567325592, 1.0, 1.0, 1.0, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4852430522441864, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6953125, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.703125, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.703125, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6875, 0.703125, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6953125, 0.7109375, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6953125, 0.703125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 0.703125, 0.6953125, 1.0, 0.703125, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6875, 0.6875, 1.0, 0.703125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6875, 0.7109375, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 0.6953125, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.7109375, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.703125, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.703125, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6953125]

 sparsity of   [1.0, 1.0, 0.13671875, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.134765625, 1.0, 0.142578125, 1.0, 1.0, 1.0, 0.1328125, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.12890625, 1.0, 0.16015625, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.130859375, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.138671875, 1.0, 0.138671875, 1.0, 0.140625, 1.0, 1.0, 0.13671875, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.154296875, 1.0, 0.154296875, 1.0, 1.0, 1.0, 0.146484375, 0.13671875, 0.193359375, 0.126953125, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 0.138671875, 0.14453125, 1.0, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.140625, 1.0, 0.138671875, 1.0, 0.140625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7065972089767456, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7152777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7057291865348816, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 1.0, 0.7057291865348816, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 0.7083333134651184, 0.7057291865348816, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 0.7065972089767456, 1.0, 1.0, 0.7170138955116272, 0.7065972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.7152777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7074652910232544, 1.0, 0.7456597089767456, 1.0, 1.0, 0.7065972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7092013955116272]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.796875, 0.8359375, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 0.8046875, 0.859375, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.796875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.796875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8125, 0.8125, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 0.8046875, 1.0, 0.8125, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 0.796875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 0.828125, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8203125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 1.0, 0.828125, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 0.8046875, 0.796875, 0.8125, 0.8125, 0.8125, 0.8046875, 1.0, 1.0, 0.828125, 0.8046875, 0.8203125, 1.0, 0.8046875, 0.8203125, 0.8046875, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 0.796875, 0.8125, 0.8046875, 0.8203125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8203125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8046875, 0.8203125, 1.0, 0.8125, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8359375, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.796875, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8203125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.103515625, 0.10546875, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.107421875, 1.0, 0.103515625, 1.0, 0.095703125, 0.103515625, 1.0, 1.0, 1.0, 0.091796875, 0.10546875, 0.095703125, 1.0, 0.09765625, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.09765625, 0.095703125, 1.0, 0.111328125, 0.10546875, 0.1015625, 1.0, 1.0, 1.0, 0.095703125, 0.09375, 0.099609375, 1.0, 1.0, 0.09765625, 0.103515625, 0.10546875, 0.103515625, 0.10546875, 0.099609375, 1.0, 1.0, 0.107421875, 0.111328125, 0.111328125, 1.0, 0.099609375, 0.1015625, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.099609375, 0.09765625, 1.0, 0.091796875, 0.09375, 0.103515625, 1.0, 1.0, 1.0, 0.111328125, 0.1015625, 0.1015625, 1.0, 0.111328125, 0.10546875, 0.1015625, 0.1015625, 1.0, 0.099609375, 1.0, 1.0, 0.103515625, 0.099609375, 0.103515625, 1.0, 0.1015625, 1.0, 0.095703125, 1.0, 1.0, 0.095703125, 1.0, 0.09765625, 0.107421875, 0.109375, 0.1015625, 0.1015625, 0.10546875, 0.109375, 0.109375, 1.0, 0.1015625, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.109375, 1.0, 0.10546875, 0.095703125, 0.103515625, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.103515625, 1.0, 0.099609375, 0.099609375, 0.103515625, 1.0, 0.09765625, 1.0, 0.1015625, 0.10546875, 0.09765625, 0.123046875, 0.1015625, 1.0, 0.11328125, 0.1015625, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.095703125, 0.103515625, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 0.099609375, 0.103515625, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 0.109375, 0.10546875, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.09765625, 1.0, 0.099609375, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 0.099609375, 1.0, 0.09765625, 0.1015625, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.08984375, 0.1015625, 0.103515625, 0.099609375, 1.0, 0.109375, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 1.0, 0.109375, 0.091796875, 0.103515625, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.107421875, 1.0, 0.10546875, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.099609375, 1.0, 1.0, 0.099609375, 0.095703125, 0.107421875, 1.0, 1.0, 0.103515625, 0.09375, 1.0, 0.1015625, 0.095703125, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 1.0, 0.503038227558136, 0.5056423544883728, 0.5026041865348816, 0.5017361044883728, 0.5034722089767456, 0.503038227558136, 1.0, 1.0, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 0.5052083134651184, 0.5017361044883728, 1.0, 0.503038227558136, 0.5060763955116272, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5043402910232544, 1.0, 1.0, 1.0, 1.0, 0.5034722089767456, 0.5052083134651184, 0.5043402910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5021701455116272, 0.5047743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5060763955116272, 0.499565988779068, 0.5043402910232544, 0.5043402910232544, 0.503038227558136, 1.0, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5021701455116272, 1.0, 0.5034722089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 1.0, 1.0, 1.0, 0.5021701455116272, 0.5017361044883728, 0.5056423544883728, 1.0, 0.5047743320465088, 0.50390625, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 0.50390625, 0.5034722089767456, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.50390625, 0.5017361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5047743320465088, 1.0, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 1.0, 0.5047743320465088, 1.0, 0.5034722089767456, 0.503038227558136, 0.5017361044883728, 0.5065104365348816, 1.0, 0.503038227558136, 1.0, 0.5021701455116272, 1.0, 1.0, 0.5017361044883728, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 1.0, 0.5021701455116272, 1.0, 0.5052083134651184, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 0.50390625, 0.503038227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5021701455116272, 0.5043402910232544, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 1.0, 0.5052083134651184, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 0.5017361044883728, 1.0, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 0.5, 0.50390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5047743320465088, 0.5021701455116272, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5026041865348816, 0.5021701455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.50390625, 0.5017361044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.69140625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 0.6875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.75, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6875, 1.0, 1.0, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 0.765625, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.68359375, 0.68359375, 0.6796875, 0.68359375, 0.6953125, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.69140625, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 0.68359375, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.69140625, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6875, 1.0, 0.68359375, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6875, 0.6796875, 1.0, 0.6875, 0.68359375, 1.0, 1.0, 0.68359375, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6875, 0.69140625, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6953125, 0.6796875, 0.6796875, 1.0, 0.6875, 0.68359375, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.703125, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 0.68359375, 0.69140625, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.70703125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 1.0, 0.6875, 1.0, 0.6875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 0.70703125, 0.6796875, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.8515625, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.69140625, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6875, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 0.6796875, 0.68359375, 1.0, 1.0, 0.69921875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.74609375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0]

 sparsity of   [1.0, 1.0, 0.109375, 0.1015625, 0.138671875, 0.109375, 0.103515625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.115234375, 0.109375, 1.0, 1.0, 1.0, 0.103515625, 0.107421875, 1.0, 0.103515625, 1.0, 1.0, 0.12109375, 1.0, 0.09375, 1.0, 1.0, 0.111328125, 0.17578125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.115234375, 0.1015625, 0.11328125, 1.0, 0.11328125, 0.107421875, 1.0, 0.125, 0.10546875, 1.0, 0.1171875, 1.0, 1.0, 0.11328125, 1.0, 0.1015625, 0.10546875, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.119140625, 0.107421875, 1.0, 1.0, 1.0, 0.11328125, 0.1015625, 1.0, 0.10546875, 0.107421875, 0.1015625, 0.109375, 0.099609375, 0.126953125, 0.107421875, 1.0, 0.109375, 0.111328125, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 0.095703125, 0.123046875, 1.0, 0.09765625, 0.095703125, 1.0, 1.0, 0.16015625, 0.125, 1.0, 0.123046875, 0.10546875, 0.11328125, 1.0, 0.11328125, 1.0, 0.1328125, 0.109375, 0.1015625, 0.111328125, 0.10546875, 1.0, 1.0, 0.107421875, 0.107421875, 1.0, 0.12890625, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 0.107421875, 0.1171875, 0.130859375, 1.0, 1.0, 0.123046875, 1.0, 0.107421875, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.115234375, 0.109375, 1.0, 0.130859375, 0.166015625, 1.0, 1.0, 1.0, 0.1015625, 0.09765625, 0.109375, 1.0, 0.10546875, 1.0, 1.0, 0.103515625, 1.0, 0.1171875, 0.150390625, 1.0, 0.12109375, 0.16796875, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.10546875, 1.0, 0.125, 0.10546875, 0.09765625, 0.11328125, 0.123046875, 1.0, 1.0, 0.109375, 0.11328125, 0.154296875, 0.09375, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 0.103515625, 0.107421875, 1.0, 1.0, 0.111328125, 1.0, 0.115234375, 1.0, 0.115234375, 1.0, 1.0, 0.220703125, 0.109375, 0.1328125, 1.0, 0.109375, 0.203125, 0.099609375, 1.0, 0.125, 1.0, 1.0, 1.0, 0.138671875, 1.0, 0.099609375, 1.0, 0.107421875, 1.0, 0.09765625, 1.0, 1.0, 1.0, 0.12890625, 0.111328125, 0.115234375, 0.107421875, 1.0, 1.0, 0.125, 0.115234375, 0.134765625, 1.0, 1.0, 0.111328125, 0.134765625, 0.24609375, 0.10546875, 1.0, 0.099609375, 1.0, 1.0, 0.142578125, 0.099609375, 0.111328125, 0.1015625, 0.11328125, 0.111328125, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 0.109375, 0.111328125, 0.10546875, 1.0, 0.126953125, 0.1171875, 1.0, 0.1015625, 1.0, 0.11328125, 0.134765625, 1.0, 0.11328125, 0.09375, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 0.11328125, 0.1015625, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.10546875, 0.107421875, 0.1171875, 0.12890625, 1.0, 0.111328125, 1.0, 0.158203125, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.107421875, 0.1015625, 0.107421875, 0.16015625, 1.0, 1.0, 0.119140625, 1.0, 0.2890625, 1.0, 0.1171875, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.099609375, 0.115234375, 1.0, 1.0, 0.123046875, 1.0, 0.111328125, 0.115234375, 1.0, 1.0, 0.107421875, 1.0, 0.109375, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.107421875, 0.107421875, 1.0, 1.0, 0.115234375, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.1015625, 0.123046875, 1.0, 0.109375, 0.10546875, 0.109375, 1.0, 0.107421875, 0.12109375, 0.103515625, 0.103515625, 0.109375, 0.1171875, 0.10546875, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.109375, 0.115234375, 0.123046875, 0.109375, 1.0, 0.109375, 1.0, 0.1015625, 1.0, 1.0, 0.1171875, 0.123046875, 0.107421875, 0.115234375, 0.134765625, 0.1015625, 0.099609375, 1.0, 1.0, 0.10546875, 0.107421875, 0.103515625, 1.0, 1.0, 0.1015625, 0.103515625, 0.1015625, 1.0, 0.115234375, 1.0, 1.0, 0.10546875, 1.0, 0.103515625, 1.0, 0.10546875, 1.0, 0.107421875, 0.146484375, 1.0, 0.140625, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.267578125, 1.0, 0.109375, 1.0, 0.1015625, 1.0, 1.0, 0.10546875, 1.0, 0.1328125, 0.107421875, 1.0, 1.0, 0.11328125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.10546875, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.1171875, 0.111328125, 0.107421875, 0.095703125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.10546875, 0.12109375, 0.11328125, 1.0, 1.0, 0.08984375, 1.0, 0.107421875, 0.119140625, 0.119140625, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.09765625, 0.10546875, 0.125, 0.115234375, 1.0, 0.1171875, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.107421875, 1.0, 0.103515625, 0.10546875, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 0.11328125, 1.0, 0.11328125, 0.119140625, 1.0, 0.21484375, 0.11328125, 1.0, 0.115234375, 0.107421875, 1.0, 0.11328125, 1.0, 0.10546875, 0.111328125, 0.1171875, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 0.099609375, 1.0, 1.0, 0.099609375, 0.115234375, 1.0, 0.126953125, 0.12109375, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.099609375, 0.099609375, 0.107421875, 0.1015625, 0.115234375, 1.0, 1.0, 0.103515625, 1.0, 0.11328125, 0.10546875, 0.1171875, 1.0, 0.11328125, 0.1171875, 1.0, 0.111328125, 0.107421875, 0.11328125, 0.1328125, 1.0, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.109375, 0.119140625, 0.1015625, 0.1015625, 1.0, 0.115234375, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.115234375, 1.0, 1.0, 1.0, 0.109375, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 0.10546875, 1.0, 0.09375, 0.1328125, 0.1015625, 1.0, 0.115234375, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.10546875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.09765625, 1.0, 0.11328125, 0.10546875, 1.0, 1.0, 0.12109375, 1.0, 0.12890625, 0.1015625, 0.216796875, 1.0, 0.109375, 0.09375, 0.115234375, 0.115234375, 0.11328125, 0.119140625, 0.16015625, 0.109375, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.177734375, 0.099609375, 1.0, 0.1171875, 0.1015625, 0.111328125, 0.11328125, 1.0, 1.0, 0.109375, 0.16015625, 0.103515625, 1.0, 0.119140625, 0.1015625, 1.0, 1.0, 0.103515625, 0.189453125, 0.119140625, 0.1484375, 1.0, 0.111328125, 0.13671875, 0.111328125, 0.12890625, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.1171875, 0.09765625, 1.0, 0.123046875, 0.10546875, 1.0, 1.0, 0.123046875, 1.0, 0.115234375, 0.14453125, 0.099609375, 1.0, 0.1171875, 0.107421875, 0.109375, 0.109375, 1.0, 1.0, 0.16796875, 0.11328125, 1.0, 1.0, 0.109375, 0.119140625, 1.0, 0.109375, 0.109375, 0.107421875, 0.10546875, 0.107421875, 0.103515625, 1.0, 1.0, 0.119140625, 1.0, 0.111328125, 0.109375, 0.271484375, 0.111328125, 0.12109375, 1.0, 1.0, 0.111328125, 0.130859375, 1.0, 1.0, 0.111328125, 1.0, 0.103515625, 1.0, 0.109375, 1.0, 0.099609375, 0.10546875, 1.0, 0.119140625, 0.109375, 1.0, 1.0, 0.109375, 0.134765625, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 0.103515625, 0.1171875, 0.09765625, 1.0, 0.12109375, 0.10546875, 0.111328125, 1.0, 1.0, 1.0, 0.10546875, 0.109375, 0.1328125, 0.142578125, 0.158203125, 1.0, 0.103515625, 0.109375, 0.171875, 0.119140625, 1.0, 0.125, 1.0, 0.181640625, 0.099609375, 1.0, 0.1171875, 0.11328125, 0.099609375, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.119140625, 1.0, 1.0, 0.111328125, 1.0, 0.248046875, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.12109375, 1.0, 0.138671875, 1.0, 0.09765625, 0.107421875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 0.11328125, 0.115234375, 1.0, 0.109375, 1.0, 0.103515625, 0.10546875, 0.11328125, 1.0, 0.111328125, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.115234375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.099609375, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 0.18359375, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 0.1875, 0.1171875, 0.09765625, 1.0, 0.107421875, 1.0, 0.11328125, 1.0, 0.1171875, 1.0, 1.0, 0.107421875, 1.0, 0.1171875, 0.169921875, 0.11328125, 0.12890625, 1.0, 0.11328125, 0.15234375, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 0.1171875, 1.0, 1.0, 1.0, 0.130859375, 0.099609375, 1.0, 0.130859375, 0.111328125, 0.111328125, 1.0, 1.0, 0.115234375, 0.11328125, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.134765625, 0.19140625, 0.125, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.126953125, 1.0, 0.11328125, 0.10546875, 0.115234375, 1.0, 1.0, 0.11328125, 0.10546875, 0.115234375, 1.0, 1.0, 0.103515625, 0.107421875, 0.10546875, 0.11328125, 0.11328125, 0.1171875, 0.115234375, 1.0, 1.0, 1.0, 0.115234375, 0.099609375, 0.185546875, 0.109375, 1.0, 0.123046875, 0.119140625, 1.0, 0.119140625, 0.10546875, 0.12109375, 0.125, 1.0, 1.0, 0.115234375, 0.111328125, 0.111328125, 0.10546875, 0.109375, 0.115234375, 0.107421875, 0.103515625, 0.1015625, 1.0, 0.10546875, 0.119140625, 1.0, 1.0, 1.0, 0.115234375, 0.1171875, 1.0, 1.0, 0.123046875, 1.0, 0.11328125, 0.109375, 0.1015625, 0.119140625, 1.0, 1.0, 1.0, 0.12109375, 0.107421875, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.1171875, 0.111328125, 0.111328125, 0.123046875, 1.0, 1.0, 1.0, 0.177734375, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0]

 sparsity of   [1.0, 1.0, 0.4326171875, 0.431640625, 0.4296875, 1.0, 0.4189453125, 0.4150390625, 1.0, 1.0, 0.4169921875, 0.4189453125, 1.0, 0.423828125, 1.0, 0.4169921875, 0.4189453125, 1.0, 1.0, 0.4228515625, 1.0, 0.416015625, 0.4287109375, 0.4189453125, 1.0, 0.4189453125, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 0.4228515625, 1.0, 0.416015625, 1.0, 0.4345703125, 0.416015625, 1.0, 1.0, 0.4248046875, 1.0, 1.0, 1.0, 1.0, 0.4189453125, 1.0, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 1.0, 0.4228515625, 0.412109375, 1.0, 1.0, 0.419921875, 1.0, 0.4462890625, 1.0, 0.4228515625, 0.42578125, 1.0, 0.41796875, 1.0, 0.4189453125, 0.4189453125, 0.41796875, 1.0, 1.0, 1.0, 0.419921875, 0.41796875, 0.4189453125, 1.0, 1.0, 1.0, 0.4306640625, 1.0, 0.4267578125, 0.4296875, 1.0, 0.4169921875, 1.0, 1.0, 0.423828125, 1.0, 1.0, 1.0, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.41796875, 1.0, 0.421875, 0.4208984375, 1.0, 1.0, 0.419921875, 0.4228515625, 0.4326171875, 0.41796875, 0.419921875, 0.419921875, 0.416015625, 1.0, 0.416015625, 1.0, 0.4140625, 0.4248046875, 1.0, 1.0, 1.0, 1.0, 0.421875, 0.4287109375, 1.0, 1.0, 0.421875, 0.431640625, 1.0, 1.0, 0.41796875, 1.0, 0.41796875, 0.4189453125, 1.0, 1.0, 0.4130859375, 0.4169921875, 1.0, 1.0, 1.0, 1.0, 0.419921875, 0.421875, 1.0, 1.0, 1.0, 0.4150390625, 0.41796875, 0.4189453125, 1.0, 0.421875, 1.0, 1.0, 0.419921875, 0.4208984375, 1.0, 1.0, 1.0, 0.4150390625, 0.416015625, 1.0, 1.0, 1.0, 0.423828125, 1.0, 0.4189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42578125, 1.0, 1.0, 0.4208984375, 1.0, 0.453125, 1.0, 1.0, 1.0, 0.421875, 0.41796875, 1.0, 0.4287109375, 1.0, 0.4228515625, 1.0, 0.421875, 1.0, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.419921875, 1.0, 1.0, 0.4169921875, 1.0, 1.0, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 0.41796875, 0.4326171875, 0.4150390625, 1.0, 0.423828125, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.435546875, 1.0, 0.4296875, 1.0, 0.4140625, 1.0, 0.4228515625, 1.0, 1.0, 1.0, 0.4140625, 1.0, 0.4296875, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.423828125, 1.0, 1.0, 0.41796875]

 sparsity of   [0.5907118320465088, 1.0, 0.5933159589767456, 0.5885416865348816, 0.5881076455116272, 0.588975727558136, 0.5907118320465088, 1.0, 0.5933159589767456, 0.5902777910232544, 1.0, 1.0, 1.0, 0.5894097089767456, 0.592881977558136, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5911458134651184, 1.0, 0.5894097089767456, 1.0, 1.0, 0.5855034589767456, 1.0, 0.5872395634651184, 0.5907118320465088, 0.5907118320465088, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 0.5894097089767456, 0.5881076455116272, 1.0, 0.5915798544883728, 0.5885416865348816, 1.0, 0.5868055820465088, 1.0, 1.0, 1.0, 0.5872395634651184, 1.0, 0.5855034589767456, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5868055820465088, 1.0, 1.0, 1.0, 0.5907118320465088, 0.5894097089767456, 1.0, 1.0, 1.0, 0.58984375, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.5911458134651184, 1.0, 0.5872395634651184, 1.0, 1.0, 1.0, 0.588975727558136, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 1.0, 0.588975727558136, 1.0, 1.0, 0.5894097089767456, 0.5911458134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.5941840410232544, 0.5876736044883728, 1.0, 1.0, 0.5902777910232544, 1.0, 1.0, 0.588975727558136, 0.5876736044883728, 1.0, 0.5885416865348816, 0.588975727558136, 1.0, 1.0, 1.0, 0.5911458134651184, 1.0, 0.5885416865348816, 1.0, 0.5894097089767456, 0.5902777910232544, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.58984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5907118320465088, 1.0, 0.5915798544883728, 0.5915798544883728, 0.588975727558136, 0.5855034589767456, 0.5907118320465088, 0.5907118320465088, 1.0, 1.0, 1.0, 0.5920138955116272, 0.5885416865348816, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 0.5885416865348816, 1.0, 0.5868055820465088, 0.588975727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.588975727558136, 0.5902777910232544, 1.0, 0.5863715410232544, 1.0, 1.0, 1.0, 1.0, 0.5933159589767456, 0.59375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 1.0, 0.5881076455116272, 0.588975727558136, 0.5894097089767456, 1.0, 0.5894097089767456, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5885416865348816, 1.0, 0.58984375, 1.0, 1.0, 0.58984375, 1.0, 1.0, 1.0, 0.5902777910232544, 0.58984375, 1.0, 0.5920138955116272, 1.0, 1.0, 0.5924479365348816, 1.0, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 0.5907118320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.585069477558136, 1.0, 0.5902777910232544, 1.0, 0.588975727558136, 0.5907118320465088, 1.0, 0.5885416865348816, 0.5920138955116272, 1.0, 1.0, 1.0, 0.5885416865348816, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.62109375, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.62109375, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.62109375, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 0.90625, 1.0, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 1.0, 0.62109375, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.62109375, 1.0, 0.6328125, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.62109375, 0.6171875, 0.61328125, 1.0, 0.62109375, 0.6171875, 0.61328125, 0.61328125, 0.60546875, 0.62109375, 0.62890625, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.62109375, 0.62890625, 0.61328125, 1.0, 0.6171875, 0.6171875, 1.0, 1.0, 0.62109375, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.62890625, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 0.625, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 1.0, 0.62890625, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.62109375, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.62890625, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.62109375, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.62109375, 0.6171875, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.62109375, 1.0, 0.62109375, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.62890625, 0.6171875, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.64453125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.625, 0.61328125, 0.61328125, 1.0, 0.6328125, 0.61328125, 0.61328125, 1.0, 0.62109375, 1.0, 0.6171875, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.61328125, 0.63671875, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.6171875, 1.0, 0.6171875, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.62109375, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.609375, 1.0, 0.6171875, 1.0, 1.0, 0.609375, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.60546875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.609375, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 0.703125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.62109375, 0.6171875, 0.6171875, 0.609375, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.625, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.63671875, 0.61328125, 0.68359375, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.62890625, 0.6328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 0.625, 1.0, 0.6171875, 0.625, 0.6171875, 0.6171875, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.62109375, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.7890625, 1.0, 0.62109375, 1.0, 1.0, 1.0, 0.609375, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.609375, 0.61328125, 1.0, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.62109375, 0.61328125, 1.0, 1.0, 0.62109375, 1.0, 0.62109375, 0.609375, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.62109375, 1.0, 0.609375, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.62109375, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.6484375, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.62109375, 1.0, 1.0, 0.6171875, 1.0, 0.61328125, 0.62109375, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.609375, 0.61328125, 1.0, 0.6171875, 1.0, 0.625, 0.6171875, 0.61328125, 0.61328125, 0.6328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.625, 0.6171875, 0.625, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.62890625, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6328125, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.62109375, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.62109375, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.625, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.62109375, 0.64453125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.6328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.62109375, 1.0, 0.6171875, 0.6171875, 0.61328125, 0.625, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 0.625, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.625, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.62109375, 0.61328125, 0.61328125, 0.625, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 1.0, 0.62109375, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 0.625, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125]

 sparsity of   [1.0, 0.2783203125, 1.0, 0.2890625, 1.0, 0.27734375, 0.2841796875, 1.0, 1.0, 1.0, 0.283203125, 0.2822265625, 0.279296875, 1.0, 0.2861328125, 0.287109375, 0.2890625, 1.0, 0.283203125, 1.0, 0.28515625, 0.2841796875, 1.0, 1.0, 1.0, 0.2841796875, 1.0, 0.2822265625, 0.2890625, 0.2841796875, 0.275390625, 0.287109375, 1.0, 0.2880859375, 1.0, 1.0, 0.2861328125, 1.0, 0.28125, 0.287109375, 0.2890625, 0.2900390625, 0.2861328125, 1.0, 1.0, 0.283203125, 0.29296875, 0.2880859375, 1.0, 0.2841796875, 0.2939453125, 0.28515625, 0.2783203125, 0.28515625, 1.0, 1.0, 0.2841796875, 0.28515625, 1.0, 1.0, 0.2822265625, 0.27734375, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 0.28125, 0.28515625, 0.2841796875, 0.283203125, 0.2939453125, 0.291015625, 0.287109375, 1.0, 0.283203125, 1.0, 0.28515625, 1.0, 0.28515625, 1.0, 0.2841796875, 0.27734375, 0.279296875, 1.0, 1.0, 1.0, 0.2890625, 0.283203125, 1.0, 0.283203125, 1.0, 0.2861328125, 0.28515625, 1.0, 0.2822265625, 1.0, 1.0, 0.283203125, 0.283203125, 0.28125, 1.0, 1.0, 0.279296875, 0.2783203125, 0.2861328125, 0.2880859375, 0.287109375, 0.2861328125, 1.0, 0.287109375, 1.0, 1.0, 0.28125, 0.2900390625, 0.2841796875, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 0.2783203125, 1.0, 1.0, 0.279296875, 1.0, 1.0, 0.28515625, 0.2802734375, 0.2900390625, 1.0, 0.28125, 0.2802734375, 0.2880859375, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 0.287109375, 1.0, 1.0, 1.0, 0.2822265625, 1.0, 1.0, 0.287109375, 0.28125, 0.2841796875, 0.2841796875, 0.291015625, 1.0, 1.0, 0.28515625, 0.2861328125, 0.2861328125, 0.287109375, 1.0, 0.2861328125, 0.2822265625, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 1.0, 1.0, 0.28515625, 1.0, 1.0, 0.2822265625, 0.27734375, 1.0, 1.0, 1.0, 1.0, 0.283203125, 0.2841796875, 1.0, 0.283203125, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 0.2802734375, 0.279296875, 0.29296875, 0.287109375, 0.283203125, 0.2841796875, 0.2802734375, 0.275390625, 1.0, 0.275390625, 0.2841796875, 0.283203125, 0.2900390625, 1.0, 0.279296875, 0.291015625, 1.0, 0.287109375, 1.0, 0.27734375, 0.2783203125, 0.2890625, 0.28125, 0.27734375, 1.0, 0.2822265625, 1.0, 0.291015625, 0.28515625, 1.0, 1.0, 0.2919921875, 1.0, 0.2763671875, 0.2900390625, 0.283203125, 0.28125, 1.0, 0.28515625, 1.0, 1.0, 0.2900390625, 0.28515625, 0.2783203125, 0.2880859375, 1.0, 0.2880859375, 1.0, 0.2783203125, 1.0, 0.28515625, 0.279296875, 0.28515625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.44921875, 1.0, 1.0, 1.0, 0.4470486044883728, 1.0, 0.4461805522441864, 0.444878488779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444477558136, 0.4461805522441864, 1.0, 0.4435763955116272, 0.4427083432674408, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4470486044883728, 1.0, 0.444878488779068, 0.4431423544883728, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4440104067325592, 0.444878488779068, 1.0, 0.4405381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4479166567325592, 0.4422743022441864, 0.4435763955116272, 0.44140625, 1.0, 1.0, 0.4457465410232544, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4418402910232544, 1.0, 1.0, 1.0, 1.0, 0.444878488779068, 0.4418402910232544, 0.4435763955116272, 1.0, 0.4431423544883728, 1.0, 1.0, 0.4466145932674408, 1.0, 1.0, 1.0, 0.4474826455116272, 1.0, 0.444878488779068, 0.4444444477558136, 0.4431423544883728, 0.4457465410232544, 1.0, 1.0, 0.4435763955116272, 0.448784738779068, 1.0, 1.0, 1.0, 0.4405381977558136, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 0.4435763955116272, 0.4466145932674408, 0.4431423544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 1.0, 0.4457465410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444477558136, 1.0, 1.0, 0.4422743022441864, 1.0, 1.0, 1.0, 1.0, 0.4483506977558136, 1.0, 1.0, 1.0, 1.0, 0.4418402910232544, 0.4431423544883728, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4431423544883728, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4461805522441864, 0.4431423544883728, 1.0, 1.0, 0.444878488779068, 1.0, 0.4444444477558136, 1.0, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.444878488779068, 1.0, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 0.4457465410232544, 1.0, 0.4431423544883728, 1.0, 0.4444444477558136, 0.4431423544883728, 1.0, 1.0, 0.4479166567325592, 0.4427083432674408, 0.4418402910232544, 0.4431423544883728, 0.444878488779068, 1.0, 0.4440104067325592, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 0.4440104067325592, 0.4418402910232544, 1.0, 1.0, 1.0, 0.444878488779068, 1.0, 0.4405381977558136, 0.4427083432674408, 1.0, 0.44140625, 0.44921875, 1.0, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4418402910232544, 1.0, 1.0, 0.4422743022441864, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 0.6875, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.6875, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.68359375, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 0.6875, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6953125, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 0.6875, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.6953125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 0.6875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 0.6796875, 0.67578125, 1.0, 1.0, 0.6875, 1.0, 0.67578125, 1.0, 1.0, 0.6953125, 1.0, 0.67578125, 1.0, 0.6875, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.69140625, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.68359375, 1.0, 1.0, 0.68359375, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.7109375, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.68359375, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.6796875, 0.6796875, 1.0, 0.67578125, 0.6796875, 0.68359375, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.68359375, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.6796875, 0.67578125, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.68359375, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.68359375, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.68359375, 1.0, 0.68359375, 0.67578125, 0.67578125, 0.6875, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.69140625, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 0.275390625, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 0.3095703125, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 0.279296875, 1.0, 0.2783203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2861328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2880859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29296875, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 0.318359375, 1.0, 1.0, 0.341796875, 1.0, 0.2783203125, 1.0, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 1.0, 0.275390625, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 0.2705078125, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3017578125, 1.0, 1.0, 0.2763671875, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8376736044883728, 1.0, 0.8420138955116272, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 0.8142361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.811631977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8081597089767456, 1.0, 1.0, 1.0, 1.0, 0.827256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 0.8059895634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8146701455116272, 0.8441840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823350727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8133680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 1.0, 0.8103298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8493923544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8177083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8068576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.91015625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 0.2587890625, 1.0, 1.0, 1.0, 1.0, 0.2841796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2529296875, 1.0, 1.0, 1.0, 0.27734375, 0.263671875, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8958333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3212890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 0.6396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3271484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.798828125, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.330078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 0.3291015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2939453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6513671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 1.0, 1.0, 0.3330078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.287109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.298828125, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.44140625, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30859375, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 0.26171875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95947265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9619140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.973741352558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9774305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.971788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9754774570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9761284589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9763454794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9772135615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.978515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9778645634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9789496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 0.9735243320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9750434160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9778645634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9763454794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.71533203125, 0.7080078125, 0.70068359375, 0.70849609375, 0.7041015625, 0.70556640625, 0.7060546875, 0.71484375, 0.70947265625, 0.70947265625, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125]

Total parameter pruned: 22184105.006554656 (unstructured) 20737874 (structured)

Test: [0/79]	Time 0.243 (0.243)	Loss 0.2751 (0.2751) ([0.171]+[0.104])	Prec@1 95.312 (95.312)
 * Prec@1 94.210

 Total elapsed time  3:55:34.210140 
 FINETUNING


 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 0.03703703731298447, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111119389534, 0.0, 1.0, 0.0, 0.0, 0.0, 0.03703703731298447, 0.0, 1.0, 0.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.03703703731298447, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.34375, 0.359375, 0.359375, 0.328125, 0.34375, 0.34375, 1.0, 0.34375, 0.328125, 1.0, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 0.328125, 0.328125, 0.328125, 0.34375, 0.328125, 0.34375, 1.0, 1.0, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.65625, 0.375, 0.359375, 0.328125, 1.0, 0.34375, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.328125, 0.34375, 0.34375, 0.34375, 0.34375, 0.375, 0.34375, 1.0, 0.34375, 1.0, 0.40625, 0.328125, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.328125, 0.390625, 0.328125]

 sparsity of   [0.3194444477558136, 0.3211805522441864, 0.3333333432674408, 0.3194444477558136, 0.3246527910232544, 1.0, 0.3246527910232544, 1.0, 0.3263888955116272, 1.0, 1.0, 0.3194444477558136, 0.315972238779068, 1.0, 0.3194444477558136, 0.328125, 0.347222238779068, 0.3263888955116272, 1.0, 0.315972238779068, 1.0, 0.3333333432674408, 1.0, 0.3229166567325592, 0.3333333432674408, 0.3194444477558136, 0.34375, 0.3333333432674408, 1.0, 0.331597238779068, 0.3402777910232544, 1.0, 0.3177083432674408, 1.0, 0.3177083432674408, 0.3298611044883728, 0.3194444477558136, 1.0, 1.0, 1.0, 0.3211805522441864, 1.0, 0.3194444477558136, 1.0, 0.3229166567325592, 1.0, 0.3211805522441864, 0.315972238779068, 0.3194444477558136, 1.0, 1.0, 0.3177083432674408, 0.647569477558136, 0.3263888955116272, 0.375, 0.3229166567325592, 0.328125, 0.3229166567325592, 0.3229166567325592, 1.0, 1.0, 0.3211805522441864, 0.3263888955116272, 0.3211805522441864]

 sparsity of   [0.3125, 0.296875, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 1.0, 0.359375, 0.3125, 0.328125, 0.3125, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 0.328125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.3125, 1.0, 0.3125, 0.296875, 1.0, 0.296875, 0.34375, 0.296875, 0.28125, 0.296875, 0.296875, 0.296875, 0.28125, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.296875, 0.3125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.296875, 1.0, 0.296875, 1.0, 1.0, 0.328125, 0.3125, 0.3125, 0.390625, 0.296875, 1.0, 1.0, 1.0, 1.0, 0.328125, 0.296875, 0.34375, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.3125, 0.328125, 0.296875, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.328125, 1.0, 0.296875, 0.328125, 0.328125, 1.0, 1.0, 0.3125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 1.0, 0.328125, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.296875, 0.3125, 0.296875, 0.3125, 0.3125, 0.359375, 0.296875, 0.3125, 1.0, 1.0, 0.3125, 0.296875, 1.0, 0.3125, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.296875, 0.296875, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.296875, 0.296875, 0.3125, 0.296875, 1.0, 0.3125, 0.3125, 0.28125, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 1.0, 0.296875, 0.296875, 0.359375, 0.296875, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 0.328125, 0.328125, 1.0, 1.0, 0.34375, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 1.0, 0.328125, 0.3125, 1.0, 0.328125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.328125, 0.296875, 0.296875, 0.328125, 0.3125, 0.28125, 1.0, 0.296875, 0.296875, 0.328125, 0.3125, 0.3125, 0.296875, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 1.0, 1.0, 0.3125, 0.296875, 0.359375, 0.296875, 0.3125, 0.328125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.34375, 1.0, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.3125, 0.296875, 0.34375, 0.328125, 1.0, 0.296875, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 0.296875, 0.3125]

 sparsity of   [0.328125, 0.375, 0.34375, 1.0, 0.34375, 0.34375, 0.328125, 1.0, 0.328125, 0.34375, 0.34375, 0.40625, 0.34375, 1.0, 0.375, 0.328125, 1.0, 0.328125, 0.328125, 0.375, 0.328125, 0.359375, 1.0, 0.359375, 1.0, 0.328125, 0.34375, 1.0, 0.34375, 0.328125, 0.359375, 0.34375, 0.359375, 0.375, 0.375, 0.34375, 0.328125, 0.34375, 0.34375, 1.0, 0.421875, 0.328125, 0.328125, 0.328125, 0.328125, 0.359375, 0.375, 0.390625, 0.40625, 1.0, 0.40625, 1.0, 1.0, 0.359375, 0.328125, 0.34375, 0.375, 0.34375, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.328125, 0.34375, 0.375, 0.34375, 0.34375, 1.0, 0.34375, 0.328125, 0.34375, 0.328125, 0.34375, 0.421875, 0.328125, 1.0, 0.375, 0.359375, 0.328125, 0.34375, 0.34375, 1.0, 0.328125, 1.0, 1.0, 1.0, 0.34375, 0.359375, 0.390625, 0.34375, 1.0, 0.328125, 0.390625, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.375, 0.34375, 0.328125, 0.34375, 1.0, 0.328125, 1.0, 0.375, 0.34375, 0.359375, 0.40625, 0.375, 0.328125, 1.0, 0.328125, 0.328125, 0.328125, 0.359375, 1.0, 0.34375, 0.359375, 0.359375, 0.359375, 0.328125, 0.34375, 0.328125, 0.359375, 0.328125, 1.0, 1.0, 0.328125, 0.359375, 1.0, 0.359375, 0.34375, 0.328125, 0.34375, 0.359375, 0.375, 0.359375, 0.34375, 0.328125, 0.390625, 0.328125, 0.328125, 0.375, 0.375, 0.375, 0.34375, 0.328125, 1.0, 0.328125, 0.328125, 0.359375, 0.375, 0.34375, 0.375, 1.0, 0.328125, 0.359375, 0.34375, 1.0, 0.34375, 1.0, 0.34375, 0.359375, 0.328125, 0.375, 0.34375, 0.375, 0.328125, 0.328125, 0.4375, 0.34375, 0.328125, 1.0, 1.0, 0.328125, 0.328125, 0.328125, 0.34375, 0.34375, 0.34375, 0.34375, 1.0, 1.0, 0.359375, 0.34375, 1.0, 1.0, 0.328125, 0.34375, 0.34375, 0.34375, 0.359375, 0.34375, 0.328125, 0.34375, 0.34375, 0.328125, 0.328125, 0.359375, 0.40625, 0.359375, 0.328125, 0.34375, 0.34375, 0.40625, 0.328125, 0.328125, 0.328125, 0.328125, 0.34375, 0.359375, 0.328125, 0.359375, 0.359375, 0.328125, 0.328125, 1.0, 0.34375, 1.0, 1.0, 0.328125, 0.359375, 1.0, 0.390625, 0.328125, 0.34375, 0.328125, 0.328125, 0.34375, 0.421875, 0.328125, 0.328125, 1.0, 0.359375, 0.359375, 0.359375, 0.34375, 1.0, 0.328125, 0.34375, 0.34375, 0.328125, 1.0, 0.375, 0.34375, 0.328125, 0.359375, 0.34375, 0.34375, 0.359375, 0.34375, 0.34375, 0.359375]

 sparsity of   [1.0, 0.1796875, 0.17578125, 0.1875, 1.0, 1.0, 1.0, 0.17578125, 1.0, 0.1875, 0.1796875, 0.171875, 0.18359375, 1.0, 0.1796875, 1.0, 0.19140625, 0.1796875, 1.0, 0.1953125, 0.19921875, 0.203125, 0.17578125, 0.1875, 0.18359375, 0.203125, 0.1875, 0.1875, 1.0, 0.1796875, 0.1953125, 1.0, 1.0, 0.16796875, 0.18359375, 0.171875, 0.1875, 0.18359375, 1.0, 0.19140625, 1.0, 0.18359375, 0.19140625, 0.19921875, 0.18359375, 0.17578125, 0.18359375, 0.1640625, 0.19140625, 0.21875, 0.18359375, 0.171875, 1.0, 0.18359375, 0.171875, 0.16796875, 1.0, 1.0, 1.0, 0.1875, 0.19140625, 1.0, 0.1640625, 0.18359375]

 sparsity of   [0.284722238779068, 0.2673611044883728, 1.0, 1.0, 0.2795138955116272, 1.0, 0.28125, 0.2760416567325592, 0.269097238779068, 0.2795138955116272, 0.2743055522441864, 0.28125, 0.284722238779068, 0.284722238779068, 1.0, 0.2743055522441864, 0.28125, 0.2725694477558136, 1.0, 0.269097238779068, 0.2743055522441864, 0.2743055522441864, 0.2934027910232544, 0.2725694477558136, 1.0, 0.2673611044883728, 1.0, 0.284722238779068, 1.0, 1.0, 0.2829861044883728, 0.2881944477558136, 0.2760416567325592, 0.2725694477558136, 0.2743055522441864, 0.2829861044883728, 0.2743055522441864, 0.265625, 0.269097238779068, 1.0, 1.0, 0.28125, 0.2743055522441864, 1.0, 1.0, 1.0, 1.0, 0.2777777910232544, 0.2864583432674408, 1.0, 0.2743055522441864, 0.2708333432674408, 0.2864583432674408, 1.0, 0.2760416567325592, 0.2725694477558136, 0.2864583432674408, 0.2760416567325592, 0.2725694477558136, 1.0, 1.0, 0.2725694477558136, 0.265625, 1.0]

 sparsity of   [0.3125, 0.28125, 0.328125, 0.3125, 0.3125, 0.296875, 0.34375, 1.0, 0.296875, 0.296875, 0.328125, 0.3125, 0.296875, 0.296875, 0.34375, 0.28125, 0.3125, 1.0, 0.3125, 0.328125, 0.296875, 0.296875, 1.0, 0.3125, 1.0, 0.328125, 0.3125, 0.296875, 0.3125, 0.328125, 0.34375, 0.296875, 0.3125, 0.296875, 0.296875, 0.28125, 0.296875, 0.296875, 0.296875, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 0.328125, 1.0, 0.28125, 0.296875, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 0.296875, 1.0, 0.375, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.296875, 0.296875, 0.296875, 0.3125, 0.296875, 0.328125, 0.296875, 1.0, 0.28125, 0.3125, 0.296875, 0.296875, 0.328125, 0.3125, 0.3125, 0.296875, 0.3125, 0.453125, 0.328125, 0.296875, 0.3125, 1.0, 0.328125, 0.296875, 0.296875, 1.0, 0.3125, 0.296875, 0.296875, 0.3125, 0.296875, 0.34375, 0.3125, 0.296875, 1.0, 1.0, 0.296875, 0.34375, 1.0, 0.3125, 0.296875, 1.0, 0.328125, 0.28125, 0.3125, 0.3125, 0.296875, 0.3125, 0.3125, 0.3125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.28125, 0.296875, 0.34375, 0.296875, 0.328125, 0.34375, 0.328125, 0.296875, 0.3125, 0.34375, 0.3125, 0.3125, 0.28125, 0.296875, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.296875, 0.296875, 0.296875, 0.3125, 0.328125, 0.296875, 0.328125, 0.296875, 0.328125, 0.296875, 0.296875, 0.296875, 0.3125, 0.359375, 0.296875, 0.3125, 0.296875, 0.3125, 0.328125, 0.296875, 0.296875, 0.28125, 1.0, 0.296875, 0.3125, 0.328125, 1.0, 0.296875, 0.3125, 0.296875, 0.296875, 0.28125, 0.328125, 0.328125, 0.296875, 0.3125, 0.3125, 0.296875, 0.28125, 0.296875, 1.0, 0.28125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.296875, 0.296875, 0.28125, 0.3125, 0.3125, 0.3125, 0.296875, 0.515625, 0.296875, 0.328125, 0.296875, 0.296875, 0.3125, 0.296875, 0.328125, 0.3125, 0.296875, 0.3125, 0.3125, 1.0, 0.296875, 0.3125, 1.0, 0.3125, 0.3125, 0.3125, 0.28125, 0.296875, 0.296875, 0.328125, 0.296875, 0.328125, 0.3125, 0.296875, 0.296875, 0.3125, 1.0, 0.296875, 0.328125, 0.328125, 1.0, 0.296875, 0.3125, 1.0, 0.3125, 0.296875, 0.359375, 0.3125, 0.296875, 0.296875, 0.296875, 0.3125, 1.0, 1.0, 1.0, 0.3125, 0.3125, 0.296875, 0.328125, 0.296875, 0.28125, 0.296875, 0.3125, 1.0, 0.296875, 0.3125, 0.296875, 0.296875, 0.296875, 0.328125, 0.296875, 0.3125, 0.328125, 0.28125]

 sparsity of   [1.0, 0.109375, 1.0, 0.0625, 1.0, 0.078125, 1.0, 0.0703125, 0.06640625, 0.078125, 0.09765625, 1.0, 1.0, 0.07421875, 1.0, 0.08203125, 1.0, 0.08984375, 1.0, 1.0, 1.0, 0.0703125, 1.0, 0.10546875, 0.1015625, 0.08203125, 0.08984375, 0.08203125, 0.08203125, 0.078125, 0.0703125, 0.06640625, 1.0, 0.0625, 0.09375, 0.07421875, 0.07421875, 0.06640625, 0.08984375, 0.0859375, 0.07421875, 0.0703125, 1.0, 0.078125, 0.1015625, 0.1015625, 0.06640625, 0.11328125, 0.09375, 0.078125, 0.06640625, 1.0, 1.0, 0.078125, 0.0703125, 0.0703125, 0.0703125, 0.078125, 0.07421875, 0.09375, 0.06640625, 0.0625, 0.078125, 0.0625]

 sparsity of   [0.2482638955116272, 0.25, 0.2482638955116272, 0.253472238779068, 1.0, 0.2482638955116272, 0.28125, 0.2638888955116272, 0.2829861044883728, 1.0, 0.2569444477558136, 0.2517361044883728, 0.2569444477558136, 0.3229166567325592, 0.25, 0.2621527910232544, 0.25, 0.253472238779068, 0.2569444477558136, 1.0, 1.0, 1.0, 0.2465277761220932, 0.2795138955116272, 1.0, 1.0, 0.265625, 1.0, 0.2552083432674408, 0.2604166567325592, 1.0, 1.0, 0.2604166567325592, 0.2829861044883728, 0.2621527910232544, 0.2708333432674408, 1.0, 0.2673611044883728, 0.2465277761220932, 0.2829861044883728, 1.0, 0.269097238779068, 0.2673611044883728, 1.0, 1.0, 0.253472238779068, 0.2552083432674408, 1.0, 0.2447916716337204, 0.2465277761220932, 1.0, 0.2517361044883728, 0.2465277761220932, 1.0, 1.0, 1.0, 0.2447916716337204, 0.253472238779068, 0.25, 0.2743055522441864, 0.2795138955116272, 0.2760416567325592, 0.2309027761220932, 1.0]

 sparsity of   [0.3125, 0.3125, 0.3125, 0.3125, 0.375, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.328125, 0.328125, 0.3125, 0.3125, 0.34375, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.328125, 0.328125, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 1.0, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 0.3125, 0.3125, 0.3125, 0.359375, 0.34375, 0.3125, 0.34375, 0.3125, 0.328125, 0.34375, 1.0, 0.3125, 0.3125, 0.34375, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 1.0, 0.328125, 0.3125, 1.0, 0.328125, 1.0, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.328125, 0.3125, 0.34375, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 0.328125, 0.34375, 0.3125, 0.3125, 0.34375, 0.34375, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.359375, 0.359375, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.3125, 0.3125, 0.34375, 0.3125, 0.34375, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.359375, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.328125, 0.3125, 0.328125, 1.0, 0.3125, 0.3125, 0.390625, 1.0, 0.3125, 0.3125, 0.3125, 0.3125, 0.34375, 0.3125, 0.328125, 0.3125, 0.328125, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.359375, 1.0, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 1.0, 0.34375, 0.3125, 0.3125, 0.3125, 0.328125, 1.0, 0.3125, 0.3125, 0.3125, 0.328125, 0.3125, 0.328125, 0.3125, 0.3125, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 0.3125, 1.0, 0.3125, 0.3125, 0.328125, 1.0, 0.34375, 1.0, 0.3125, 0.328125, 0.34375, 0.3125, 0.3125, 0.3125, 0.34375, 0.328125, 0.3125, 0.3125, 0.3125, 0.3125, 0.328125, 0.328125, 0.3125, 1.0, 0.3125, 0.328125, 1.0, 0.328125, 0.3125]

 sparsity of   [1.0, 0.00390625, 0.01171875, 1.0, 0.015625, 0.015625, 0.00390625, 0.015625, 0.0078125, 1.0, 1.0, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.00390625, 0.015625, 0.01953125, 0.03515625, 0.0078125, 0.00390625, 1.0, 0.0234375, 0.01953125, 1.0, 1.0, 0.00390625, 1.0, 1.0, 0.01171875, 0.00390625, 0.0078125, 0.03515625, 0.02734375, 0.01953125, 0.01171875, 0.01171875, 0.09765625, 1.0, 0.03125, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 1.0, 0.01171875, 0.0078125, 0.00390625, 1.0, 0.01953125, 0.01953125, 1.0, 0.01171875, 1.0, 0.00390625, 0.0078125, 0.00390625, 0.0625, 1.0, 0.00390625, 0.00390625, 0.015625, 1.0, 0.0078125, 0.01171875, 0.00390625, 1.0, 0.01171875, 0.0234375, 0.01171875, 0.01953125, 0.00390625, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.01953125, 1.0, 0.0546875, 0.00390625, 1.0, 0.00390625, 0.01171875, 0.01171875, 1.0, 0.01953125, 0.15234375, 0.0078125, 0.046875, 0.0234375, 0.01953125, 0.0078125, 1.0, 1.0, 0.01953125, 0.0234375, 0.00390625, 1.0, 0.01953125, 0.00390625, 1.0, 0.01953125, 0.015625, 0.01171875, 1.0, 1.0, 0.0078125, 1.0, 0.01171875, 0.00390625, 0.00390625, 1.0, 0.01171875, 0.0078125, 0.03515625, 0.00390625, 1.0, 1.0, 0.015625, 0.00390625, 0.03125, 0.0234375, 1.0, 1.0, 1.0, 0.00390625]

 sparsity of   [0.2838541567325592, 0.2795138955116272, 0.296875, 1.0, 1.0, 1.0, 0.2890625, 0.3046875, 0.2760416567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2916666567325592, 1.0, 0.2855902910232544, 1.0, 1.0, 0.3602430522441864, 0.2829861044883728, 1.0, 1.0, 0.2951388955116272, 1.0, 1.0, 1.0, 1.0, 0.3012152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3454861044883728, 1.0, 1.0, 1.0, 0.276909738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3402777910232544, 1.0, 0.2899305522441864, 0.2951388955116272, 0.2881944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2890625, 1.0, 0.3020833432674408, 0.2899305522441864, 1.0, 0.3151041567325592, 1.0, 0.2795138955116272, 0.2855902910232544, 1.0, 0.2795138955116272, 1.0, 0.2960069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2743055522441864, 1.0, 1.0, 1.0, 1.0, 0.2934027910232544, 0.3142361044883728, 1.0, 0.296875, 0.2829861044883728, 1.0, 1.0, 1.0, 0.284722238779068, 1.0, 1.0, 0.284722238779068, 1.0, 1.0, 1.0, 0.2890625, 1.0, 0.3645833432674408, 1.0, 1.0, 1.0, 0.2760416567325592, 0.2821180522441864, 0.3515625, 1.0, 1.0, 0.3229166567325592, 0.2708333432674408, 1.0, 0.3333333432674408, 1.0, 1.0, 0.2994791567325592, 1.0, 0.2664930522441864, 1.0, 1.0, 0.2873263955116272, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.65625, 1.0, 1.0, 1.0, 0.609375, 0.625, 0.609375, 1.0, 0.6015625, 0.625, 0.609375, 1.0, 0.6171875, 1.0, 0.625, 0.640625, 0.6015625, 0.6171875, 1.0, 0.625, 0.6171875, 1.0, 1.0, 0.625, 1.0, 0.640625, 1.0, 0.6171875, 0.625, 0.640625, 0.6171875, 0.6328125, 0.609375, 0.6171875, 1.0, 0.640625, 0.6171875, 0.625, 0.6171875, 0.6328125, 0.609375, 0.6015625, 0.625, 0.6171875, 1.0, 0.6171875, 0.6484375, 0.6328125, 0.625, 1.0, 0.6015625, 0.609375, 0.609375, 0.625, 0.609375, 1.0, 0.6171875, 0.609375, 0.6328125, 0.625, 0.640625, 0.6171875, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.671875, 0.6484375, 0.6328125, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 0.609375, 0.6171875, 0.640625, 0.6328125, 1.0, 0.640625, 0.6328125, 1.0, 1.0, 0.6015625, 0.6328125, 0.640625, 1.0, 0.65625, 0.6171875, 0.6328125, 0.625, 0.625, 1.0, 0.625, 0.625, 1.0, 0.640625, 1.0, 0.6328125, 0.6015625, 0.609375, 0.6171875, 0.6171875, 0.609375, 0.6484375, 0.671875, 0.6328125, 0.640625, 1.0, 0.609375, 0.625, 0.625, 1.0, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 0.6015625, 1.0, 0.6015625, 1.0, 1.0, 0.640625, 1.0, 0.609375, 1.0, 0.609375, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.625, 0.6171875, 0.609375, 1.0, 0.625, 0.6328125, 0.625, 0.609375, 0.6171875, 0.65625, 0.6328125, 1.0, 1.0, 1.0, 0.640625, 0.6171875, 0.6171875, 0.6484375, 1.0, 0.6015625, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.609375, 0.6171875, 0.625, 0.6171875, 0.6171875, 1.0, 0.6484375, 1.0, 1.0, 1.0, 1.0, 0.609375, 0.625, 0.609375, 1.0, 0.6171875, 1.0, 1.0, 0.6328125, 0.640625, 1.0, 0.6171875, 1.0, 0.640625, 1.0, 0.6171875, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.6015625, 0.6640625, 0.6171875, 0.625, 0.6328125, 0.609375, 0.609375, 0.6171875, 0.6171875, 0.640625, 0.6328125, 0.625, 1.0, 1.0, 0.609375, 0.6328125, 0.6328125, 1.0, 0.6328125, 1.0, 0.640625, 0.6171875, 1.0, 0.6015625, 0.6328125, 0.625, 0.6171875, 0.6328125, 1.0, 0.6484375, 0.6328125, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6015625, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6328125, 0.625, 0.65625, 0.6171875, 1.0, 0.625, 0.6171875, 0.640625, 1.0, 0.625, 1.0, 1.0, 1.0, 0.6015625, 0.6796875, 0.65625, 0.640625, 0.6171875, 0.640625, 0.6171875, 0.6171875, 1.0, 1.0, 0.640625, 0.6640625, 0.609375, 0.609375, 1.0, 1.0, 0.6171875, 1.0, 0.6015625, 1.0, 1.0, 0.625, 1.0, 0.609375, 1.0, 0.6171875, 1.0, 0.6328125, 0.625, 1.0, 0.6328125, 1.0, 0.625, 0.625, 0.640625, 0.6171875, 0.6171875, 0.6171875, 0.625, 1.0, 0.625, 1.0, 0.6484375, 1.0, 0.65625, 0.609375, 0.6171875, 1.0, 0.640625, 0.6171875, 1.0, 1.0, 0.625, 1.0, 0.6171875, 0.609375, 0.6328125, 1.0, 0.609375, 0.609375, 0.6171875, 1.0, 1.0, 0.625, 0.625, 1.0, 0.640625, 0.6484375, 0.6171875, 0.625, 1.0, 0.625, 0.6328125, 0.6171875, 1.0, 0.6328125, 0.609375, 0.6171875, 0.6328125, 0.625, 0.6171875, 0.6171875, 0.6640625, 1.0, 0.6171875, 0.6171875, 1.0, 0.625, 0.6328125, 0.6171875, 0.6328125, 1.0, 0.6328125, 0.625, 0.609375, 0.6640625, 0.6015625, 1.0, 1.0, 0.625, 0.6171875, 0.609375, 0.609375, 1.0, 0.640625, 0.6328125, 0.625, 0.6171875, 0.6171875, 0.625, 0.625, 0.6171875, 1.0, 0.625, 0.6171875, 1.0, 0.6328125, 0.609375, 0.6171875, 1.0, 1.0, 0.6171875, 0.625, 1.0, 1.0, 0.6015625, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.640625, 1.0, 0.6171875, 0.6484375, 0.6171875, 0.640625, 1.0, 0.6328125, 0.6328125, 1.0, 0.625, 0.6171875, 0.625, 0.625, 1.0, 0.6328125, 0.6171875, 1.0, 0.625, 0.640625, 0.6171875, 0.625, 0.6171875, 0.625, 1.0, 0.6171875, 0.6171875, 0.6484375, 1.0, 1.0, 0.6171875, 0.640625, 1.0, 1.0, 1.0, 1.0, 0.6328125, 0.6328125, 0.625, 0.6171875, 0.609375, 1.0, 1.0, 0.625, 0.640625, 1.0, 0.6484375, 0.6328125, 0.65625, 1.0, 0.6328125, 0.640625, 0.6171875, 1.0, 1.0, 1.0, 0.6328125, 0.6171875, 1.0, 0.65625, 0.6171875, 0.625, 0.6171875, 1.0, 0.625, 0.6328125, 0.6171875, 0.6171875, 1.0, 0.65625, 1.0, 0.6171875, 0.609375, 0.6328125, 0.6171875, 0.65625, 1.0, 0.609375, 0.6328125, 1.0, 0.609375, 0.609375, 0.6328125, 0.609375, 0.6328125, 1.0, 0.640625, 1.0, 0.6328125, 1.0, 0.65625, 1.0, 0.640625, 0.6171875, 0.625, 0.625, 1.0, 0.625, 0.625, 1.0, 0.609375, 1.0, 1.0, 0.6328125, 1.0, 0.640625, 0.609375, 0.625, 0.6171875, 0.65625, 0.625, 0.6640625]

 sparsity of   [1.0, 0.00390625, 1.0, 1.0, 0.015625, 0.01171875, 0.046875, 0.0078125, 1.0, 0.01953125, 0.0234375, 0.01953125, 1.0, 0.0078125, 1.0, 0.0078125, 0.03515625, 0.0078125, 0.01171875, 1.0, 0.01953125, 0.01171875, 1.0, 1.0, 0.0703125, 1.0, 0.01953125, 1.0, 0.015625, 0.0078125, 0.01171875, 0.00390625, 0.01953125, 0.015625, 0.00390625, 1.0, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.01171875, 0.02734375, 0.03125, 0.015625, 0.015625, 1.0, 0.0078125, 0.0078125, 0.03515625, 0.01171875, 1.0, 0.01171875, 0.00390625, 0.015625, 0.02734375, 0.01171875, 0.01171875, 0.0234375, 0.0078125, 0.01953125, 0.01953125, 0.015625, 0.01171875, 1.0, 1.0, 0.02734375, 0.01171875, 1.0, 0.01171875, 0.01171875, 0.0078125, 1.0, 1.0, 0.01953125, 1.0, 1.0, 1.0, 1.0, 0.015625, 1.0, 1.0, 0.03515625, 0.01171875, 0.00390625, 0.00390625, 0.01171875, 0.00390625, 0.015625, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 1.0, 0.109375, 0.0078125, 0.015625, 0.0078125, 0.01953125, 0.015625, 0.0078125, 0.015625, 1.0, 0.00390625, 1.0, 0.02734375, 0.015625, 0.015625, 0.01171875, 0.0234375, 0.01953125, 0.0078125, 1.0, 0.0546875, 0.0078125, 1.0, 0.0078125, 0.015625, 0.01171875, 0.0234375, 1.0, 0.11328125, 0.03515625, 0.0078125, 1.0, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.0234375, 1.0, 0.015625, 0.01953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0234375, 0.01171875, 0.03125, 0.08984375, 1.0, 0.01171875, 0.01171875, 0.015625, 0.0078125, 0.01171875, 0.00390625, 0.01953125, 1.0, 1.0, 1.0, 0.0078125, 0.01171875, 0.0234375, 1.0, 1.0, 0.0078125, 0.0078125, 0.00390625, 1.0, 1.0, 1.0, 1.0, 0.0078125, 0.0078125, 0.01171875, 0.00390625, 0.03125, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.01171875, 0.0078125, 0.0078125, 0.0078125, 0.01171875, 1.0, 1.0, 0.02734375, 0.00390625, 1.0, 0.01171875, 1.0, 0.01171875, 1.0, 0.0078125, 0.00390625, 0.015625, 1.0, 1.0, 0.01953125, 1.0, 0.01171875, 1.0, 1.0, 1.0, 0.015625, 1.0, 0.01171875, 0.01171875, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.015625, 0.015625, 0.01171875, 1.0, 1.0, 0.01171875, 0.02734375, 0.01953125, 1.0, 1.0, 0.51953125, 0.01171875, 0.015625, 1.0, 0.0390625, 0.05859375, 0.01171875, 0.00390625, 0.0078125, 1.0, 0.015625, 0.03125, 0.03125, 1.0, 1.0, 0.015625, 1.0, 1.0, 1.0, 0.00390625, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.01171875, 0.046875, 0.0078125, 0.00390625, 0.00390625, 1.0, 0.0078125, 0.01171875, 0.01953125, 1.0, 0.015625, 1.0, 1.0, 1.0, 0.015625, 1.0, 0.0078125, 0.0078125, 0.0234375, 0.015625, 0.0078125, 0.0078125, 1.0, 1.0, 0.0078125, 0.01953125, 0.01171875, 0.0234375, 1.0, 1.0, 0.01953125, 1.0, 0.1015625, 1.0, 1.0, 0.015625, 1.0, 0.0078125, 1.0, 0.015625, 1.0, 0.01171875, 0.0078125, 1.0, 0.0078125, 1.0, 0.01953125, 0.03125, 0.015625, 0.01171875, 0.01953125, 0.0078125, 0.015625, 1.0, 0.03125, 1.0, 0.015625, 1.0, 0.015625, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.01171875, 1.0, 1.0, 0.01171875, 1.0, 0.0078125, 0.01171875, 0.03515625, 1.0, 0.0078125, 0.00390625, 0.0078125, 1.0, 1.0, 0.01171875, 0.01953125, 1.0, 0.00390625, 0.015625, 0.0234375, 0.01171875, 1.0, 0.0078125, 0.00390625, 0.0078125, 0.015625, 0.00390625, 0.0078125, 0.01953125, 0.01953125, 0.015625, 0.0234375, 0.0078125, 0.0078125, 1.0, 0.01171875, 0.0234375, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.01953125, 0.0234375, 0.015625, 0.01171875, 0.00390625, 1.0, 1.0, 0.125, 0.015625, 0.01953125, 0.01171875, 1.0, 0.015625, 0.01171875, 0.01171875, 0.00390625, 0.0078125, 0.01171875, 0.02734375, 0.00390625, 1.0, 0.01171875, 0.01171875, 1.0, 0.01171875, 0.0078125, 0.01171875, 1.0, 1.0, 0.01171875, 0.01171875, 1.0, 1.0, 0.00390625, 0.01171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03125, 0.0078125, 1.0, 0.03515625, 1.0, 0.015625, 1.0, 0.01171875, 1.0, 1.0, 0.00390625, 0.00390625, 1.0, 0.0078125, 0.0859375, 0.05078125, 0.01171875, 0.015625, 0.01171875, 0.0234375, 1.0, 0.01171875, 0.01171875, 0.02734375, 0.015625, 0.00390625, 0.01171875, 1.0, 0.015625, 0.0078125, 0.0625, 1.0, 1.0, 0.0078125, 1.0, 1.0, 1.0, 1.0, 0.015625, 0.0078125, 0.10546875, 0.015625, 0.015625, 0.05859375, 1.0, 1.0, 0.0078125, 0.05859375, 0.015625, 1.0, 0.0078125, 0.00390625, 1.0, 0.00390625, 0.01171875, 0.0078125, 1.0, 0.01171875, 0.015625, 0.0078125, 0.01171875, 1.0, 1.0, 0.0078125, 0.0078125, 0.02734375, 1.0, 0.02734375, 0.0390625, 0.00390625, 0.03515625, 1.0, 0.00390625, 1.0, 0.00390625, 0.015625, 0.01171875, 0.0078125, 1.0, 1.0, 0.07421875, 0.0234375, 1.0, 0.0078125, 0.015625, 0.00390625, 0.01171875, 0.03125, 1.0, 0.0078125, 1.0, 0.00390625, 1.0, 0.01171875, 1.0, 0.03515625, 0.0078125, 0.01953125, 0.0078125, 1.0, 0.0078125, 0.0078125, 1.0, 0.01171875, 1.0, 1.0, 0.0078125, 1.0, 0.01171875, 0.00390625, 0.0703125, 0.0078125, 0.12109375, 0.0078125, 0.015625]

 sparsity of   [1.0, 0.31640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 0.298828125, 1.0, 0.314453125, 1.0, 1.0, 1.0, 0.3046875, 0.306640625, 1.0, 0.31640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.296875, 1.0, 0.30078125, 0.3046875, 0.31640625, 1.0, 1.0, 1.0, 1.0, 0.30859375, 0.3046875, 0.31640625, 1.0, 0.314453125, 0.3125, 1.0, 1.0, 0.31640625, 0.294921875, 0.30859375, 1.0, 0.318359375, 1.0, 0.306640625, 1.0, 1.0, 0.30859375, 1.0, 1.0, 0.310546875, 0.30859375, 1.0, 0.3125, 0.314453125, 0.314453125, 1.0, 0.310546875, 1.0, 1.0, 0.3046875, 0.31640625, 1.0, 1.0, 0.306640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 0.310546875, 1.0, 1.0, 1.0, 0.31640625, 1.0, 0.337890625, 0.341796875, 0.296875, 0.302734375, 1.0, 1.0, 0.302734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.302734375, 1.0, 1.0, 0.328125, 1.0, 1.0, 0.3359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.310546875, 0.314453125]

 sparsity of   [1.0, 1.0, 1.0, 0.6076388955116272, 0.6519097089767456, 1.0, 0.5859375, 1.0, 1.0, 1.0, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.5980902910232544, 1.0, 1.0, 0.6050347089767456, 1.0, 1.0, 0.6102430820465088, 0.592881977558136, 0.6024305820465088, 1.0, 0.609375, 1.0, 0.592881977558136, 0.5963541865348816, 1.0, 1.0, 0.6649305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5876736044883728, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 1.0, 1.0, 1.0, 0.6067708134651184, 1.0, 1.0, 1.0, 0.6041666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 0.6293402910232544, 0.5868055820465088, 0.5946180820465088, 0.6354166865348816, 0.616319477558136, 1.0, 1.0, 0.5963541865348816, 1.0, 0.6267361044883728, 0.5963541865348816, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 0.59375, 1.0, 1.0, 0.609375, 1.0, 0.608506977558136, 1.0, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 1.0, 0.6102430820465088, 1.0, 1.0, 0.6805555820465088, 0.5876736044883728, 0.5989583134651184, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.5980902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6640625, 0.6796875, 1.0, 0.6875, 1.0, 0.6640625, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 0.6640625, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6640625, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6640625, 0.6796875, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6953125, 0.6796875, 0.6640625, 0.6796875, 0.671875, 0.703125, 0.671875, 1.0, 1.0, 0.671875, 1.0, 0.6640625, 0.6875, 0.6796875, 0.671875, 1.0, 0.6796875, 0.671875, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6640625, 0.6875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.6640625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.6640625, 0.6796875, 1.0, 0.6796875, 1.0, 0.6640625, 0.6640625, 0.6875, 0.6875, 0.6640625, 0.6875, 0.6796875, 1.0, 0.6875, 0.6953125, 0.671875, 0.6875, 0.6875, 0.6796875, 1.0, 0.6875, 1.0, 0.6875, 0.6796875, 0.671875, 0.671875, 1.0, 0.6640625, 0.671875, 0.671875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 0.6796875, 0.703125, 0.6796875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6640625, 0.671875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 0.671875, 1.0, 1.0, 0.6640625, 0.671875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6796875, 0.6875, 0.671875, 0.6796875, 0.671875, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6640625, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 0.6796875, 0.6875, 0.671875, 0.7109375, 0.6875, 0.6875, 0.6796875, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.6640625, 0.6640625, 0.6796875, 1.0, 1.0, 0.6796875, 0.6640625, 1.0, 0.671875, 0.6875, 0.671875, 0.6953125, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6640625, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.6875, 0.6640625, 0.6875, 0.6640625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6875, 0.6640625, 1.0, 0.671875, 0.6640625, 1.0, 1.0, 0.671875, 0.6640625, 0.6875, 0.6796875, 0.6796875, 0.6640625, 0.6640625, 0.671875, 1.0, 0.6796875, 1.0, 0.671875, 0.6640625, 0.671875, 1.0, 0.6796875, 0.6875, 0.6640625, 1.0, 1.0, 0.6953125, 0.671875, 1.0, 0.6640625, 0.6796875, 0.6875, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.6640625, 0.6953125, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6796875, 1.0, 0.6640625, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.671875, 0.671875, 0.671875, 0.671875, 1.0, 0.671875, 0.671875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.6640625, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6640625, 0.6640625, 0.671875, 1.0, 0.671875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.671875, 0.6875, 0.6640625, 1.0, 1.0, 1.0, 0.6796875, 0.671875, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.671875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.671875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 1.0, 0.6640625, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.671875, 0.6796875, 1.0, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.7109375, 0.671875, 0.6640625, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 0.671875, 0.671875, 1.0, 1.0, 0.6640625, 0.6796875, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 0.671875, 0.6796875, 0.671875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.671875, 0.6640625, 1.0, 0.6640625, 1.0, 1.0, 0.671875, 1.0, 1.0, 1.0, 0.671875, 1.0, 1.0, 0.671875, 0.6875, 0.6640625, 1.0, 0.6875, 0.6796875, 1.0, 0.6953125, 1.0, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6875, 0.671875, 0.6640625, 1.0, 0.6796875, 0.671875]

 sparsity of   [1.0, 1.0, 0.2265625, 0.3046875, 1.0, 0.24609375, 0.23046875, 0.298828125, 0.208984375, 1.0, 1.0, 0.21484375, 1.0, 1.0, 1.0, 0.220703125, 0.21484375, 1.0, 1.0, 0.240234375, 0.2265625, 1.0, 1.0, 1.0, 0.259765625, 1.0, 1.0, 0.25390625, 0.21875, 0.22265625, 1.0, 0.236328125, 0.26953125, 0.21484375, 1.0, 1.0, 0.236328125, 0.220703125, 0.21875, 1.0, 0.216796875, 0.244140625, 0.2265625, 0.22265625, 0.25, 0.28515625, 0.220703125, 1.0, 0.24609375, 1.0, 0.21875, 1.0, 1.0, 0.251953125, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 0.2265625, 1.0, 1.0, 0.439453125, 1.0, 1.0, 1.0, 0.216796875, 1.0, 0.236328125, 1.0, 0.212890625, 1.0, 0.22265625, 1.0, 0.349609375, 0.2109375, 1.0, 0.302734375, 1.0, 1.0, 1.0, 1.0, 0.23046875, 0.302734375, 1.0, 1.0, 1.0, 1.0, 0.224609375, 0.20703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 0.220703125, 0.25390625, 1.0, 1.0, 1.0, 0.212890625, 1.0, 1.0, 1.0, 0.23046875, 1.0, 1.0, 1.0, 1.0, 0.255859375, 1.0, 0.2890625, 0.228515625, 0.212890625, 0.21875, 1.0, 0.224609375, 0.240234375, 0.232421875, 0.2109375, 0.212890625, 0.345703125, 0.255859375]

 sparsity of   [1.0, 0.5920138955116272, 1.0, 1.0, 0.5868055820465088, 0.487847238779068, 0.4626736044883728, 1.0, 1.0, 0.5564236044883728, 0.5720486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5833333134651184, 1.0, 0.4583333432674408, 1.0, 1.0, 0.480034738779068, 1.0, 1.0, 1.0, 0.4791666567325592, 1.0, 0.4765625, 0.46875, 1.0, 1.0, 0.453125, 0.4921875, 1.0, 0.5798611044883728, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4539930522441864, 1.0, 1.0, 0.4713541567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4739583432674408, 1.0, 1.0, 1.0, 0.4817708432674408, 0.4826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4739583432674408, 1.0, 0.4765625, 1.0, 1.0, 1.0, 0.4652777910232544, 1.0, 1.0, 0.4826388955116272, 1.0, 1.0, 0.5598958134651184, 0.4670138955116272, 1.0, 0.480034738779068, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4774305522441864, 1.0, 1.0, 1.0, 1.0, 0.4626736044883728, 1.0, 0.4661458432674408, 1.0, 0.4748263955116272, 0.4678819477558136, 0.5494791865348816, 0.4835069477558136, 1.0, 0.4869791567325592, 1.0, 1.0, 1.0, 1.0, 0.4809027910232544, 1.0, 1.0, 1.0, 1.0, 0.6050347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4852430522441864, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6953125, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.703125, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.703125, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6875, 0.703125, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.703125, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6953125, 0.7109375, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6953125, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6953125, 0.703125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 0.703125, 0.6953125, 1.0, 0.703125, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6875, 0.6875, 1.0, 0.703125, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6875, 0.7109375, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6953125, 0.6953125, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 0.6953125, 0.6953125, 0.6953125, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.7109375, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.703125, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.703125, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 1.0, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6875, 0.6953125]

 sparsity of   [1.0, 1.0, 0.13671875, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.150390625, 1.0, 1.0, 0.134765625, 1.0, 0.142578125, 1.0, 1.0, 1.0, 0.1328125, 1.0, 1.0, 1.0, 0.134765625, 1.0, 0.14453125, 1.0, 1.0, 1.0, 0.12890625, 1.0, 0.16015625, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.130859375, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.138671875, 1.0, 0.138671875, 1.0, 0.140625, 1.0, 1.0, 0.13671875, 1.0, 1.0, 1.0, 0.138671875, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.140625, 1.0, 1.0, 0.154296875, 1.0, 0.154296875, 1.0, 1.0, 1.0, 0.146484375, 0.13671875, 0.193359375, 0.126953125, 1.0, 0.146484375, 1.0, 0.14453125, 1.0, 0.138671875, 0.14453125, 1.0, 1.0, 1.0, 0.1640625, 1.0, 1.0, 0.140625, 1.0, 0.138671875, 1.0, 0.140625, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7065972089767456, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7152777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7057291865348816, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 1.0, 0.7057291865348816, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7074652910232544, 1.0, 0.7083333134651184, 0.7057291865348816, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 0.7065972089767456, 1.0, 1.0, 0.7170138955116272, 0.7065972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.7152777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7369791865348816, 0.7074652910232544, 1.0, 0.7456597089767456, 1.0, 1.0, 0.7065972089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7092013955116272]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.796875, 0.8359375, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 1.0, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 0.8125, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 0.8046875, 0.859375, 1.0, 0.8125, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.796875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8125, 0.796875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8125, 0.8125, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 0.8203125, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 0.8046875, 1.0, 0.8125, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 0.8046875, 0.796875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8203125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 0.828125, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8203125, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 0.8125, 1.0, 0.828125, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 1.0, 0.8125, 0.8046875, 0.796875, 0.8125, 0.8125, 0.8125, 0.8046875, 1.0, 1.0, 0.828125, 0.8046875, 0.8203125, 1.0, 0.8046875, 0.8203125, 0.8046875, 1.0, 1.0, 0.8125, 0.8046875, 1.0, 0.8046875, 1.0, 0.8046875, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 0.796875, 0.8125, 0.8046875, 0.8203125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8203125, 1.0, 1.0, 0.8046875, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.8125, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8125, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8046875, 0.8203125, 1.0, 0.8125, 1.0, 0.8046875, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8359375, 0.8125, 1.0, 0.8125, 1.0, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.8046875, 0.796875, 1.0, 1.0, 1.0, 0.8125, 1.0, 0.8125, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 0.8046875, 0.8046875, 1.0, 0.8046875, 1.0, 0.8203125, 1.0, 0.8046875, 1.0, 1.0, 0.8046875, 0.8125, 0.8046875, 0.8046875, 0.8125, 1.0, 0.8046875, 0.8046875, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8046875, 1.0, 0.8125, 0.8046875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.8125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.103515625, 0.10546875, 0.099609375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.107421875, 1.0, 0.103515625, 1.0, 0.095703125, 0.103515625, 1.0, 1.0, 1.0, 0.091796875, 0.10546875, 0.095703125, 1.0, 0.09765625, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.09765625, 0.095703125, 1.0, 0.111328125, 0.10546875, 0.1015625, 1.0, 1.0, 1.0, 0.095703125, 0.09375, 0.099609375, 1.0, 1.0, 0.09765625, 0.103515625, 0.10546875, 0.103515625, 0.10546875, 0.099609375, 1.0, 1.0, 0.107421875, 0.111328125, 0.111328125, 1.0, 0.099609375, 0.1015625, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 0.1015625, 1.0, 1.0, 1.0, 0.099609375, 0.09765625, 1.0, 0.091796875, 0.09375, 0.103515625, 1.0, 1.0, 1.0, 0.111328125, 0.1015625, 0.1015625, 1.0, 0.111328125, 0.10546875, 0.1015625, 0.1015625, 1.0, 0.099609375, 1.0, 1.0, 0.103515625, 0.099609375, 0.103515625, 1.0, 0.1015625, 1.0, 0.095703125, 1.0, 1.0, 0.095703125, 1.0, 0.09765625, 0.107421875, 0.109375, 0.1015625, 0.1015625, 0.10546875, 0.109375, 0.109375, 1.0, 0.1015625, 1.0, 0.126953125, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 1.0, 0.109375, 1.0, 0.10546875, 0.095703125, 0.103515625, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.103515625, 1.0, 0.099609375, 0.099609375, 0.103515625, 1.0, 0.09765625, 1.0, 0.1015625, 0.10546875, 0.09765625, 0.123046875, 0.1015625, 1.0, 0.11328125, 0.1015625, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.095703125, 0.103515625, 1.0, 0.099609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09765625, 1.0, 1.0, 0.099609375, 0.103515625, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 1.0, 1.0, 0.109375, 0.10546875, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 0.09765625, 1.0, 0.099609375, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.103515625, 0.099609375, 1.0, 0.09765625, 0.1015625, 1.0, 1.0, 0.095703125, 1.0, 1.0, 0.08984375, 0.1015625, 0.103515625, 0.099609375, 1.0, 0.109375, 0.103515625, 1.0, 1.0, 1.0, 0.099609375, 1.0, 0.109375, 0.091796875, 0.103515625, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.1015625, 1.0, 0.107421875, 1.0, 0.10546875, 0.130859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1015625, 0.099609375, 1.0, 1.0, 0.099609375, 0.095703125, 0.107421875, 1.0, 1.0, 0.103515625, 0.09375, 1.0, 0.1015625, 0.095703125, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 1.0, 0.503038227558136, 0.5056423544883728, 0.5026041865348816, 0.5017361044883728, 0.5034722089767456, 0.503038227558136, 1.0, 1.0, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 0.5052083134651184, 0.5017361044883728, 1.0, 0.503038227558136, 0.5060763955116272, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5043402910232544, 1.0, 1.0, 1.0, 1.0, 0.5034722089767456, 0.5052083134651184, 0.5043402910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5021701455116272, 0.5047743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5060763955116272, 0.499565988779068, 0.5043402910232544, 0.5043402910232544, 0.503038227558136, 1.0, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5021701455116272, 1.0, 0.5034722089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 1.0, 1.0, 1.0, 0.5021701455116272, 0.5017361044883728, 0.5056423544883728, 1.0, 0.5047743320465088, 0.50390625, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 0.50390625, 0.5034722089767456, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.50390625, 0.5017361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5047743320465088, 1.0, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 1.0, 0.5047743320465088, 1.0, 0.5034722089767456, 0.503038227558136, 0.5017361044883728, 0.5065104365348816, 1.0, 0.503038227558136, 1.0, 0.5021701455116272, 1.0, 1.0, 0.5017361044883728, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 1.0, 0.5021701455116272, 1.0, 0.5052083134651184, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.503038227558136, 1.0, 0.50390625, 0.503038227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5021701455116272, 0.5043402910232544, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 0.5021701455116272, 1.0, 1.0, 1.0, 0.5017361044883728, 1.0, 1.0, 1.0, 0.5052083134651184, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5026041865348816, 0.5043402910232544, 0.5017361044883728, 1.0, 1.0, 0.5026041865348816, 1.0, 1.0, 1.0, 0.5, 0.50390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5047743320465088, 0.5021701455116272, 1.0, 1.0, 1.0, 0.5013020634651184, 1.0, 1.0, 0.5026041865348816, 0.5021701455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.50390625, 0.5017361044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.69140625, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 0.6875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.75, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6875, 1.0, 1.0, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 0.765625, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.68359375, 0.68359375, 0.6796875, 0.68359375, 0.6953125, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.69140625, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 0.68359375, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.69140625, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6875, 1.0, 0.68359375, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6875, 0.6796875, 1.0, 0.6875, 0.68359375, 1.0, 1.0, 0.68359375, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.68359375, 1.0, 1.0, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6875, 0.69140625, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6953125, 0.6796875, 0.6796875, 1.0, 0.6875, 0.68359375, 0.6796875, 0.6875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.703125, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 0.68359375, 0.69140625, 1.0, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.70703125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 1.0, 0.6875, 1.0, 0.6875, 0.6796875, 0.6875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 0.70703125, 0.6796875, 0.68359375, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.68359375, 0.6796875, 0.6875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.69140625, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 0.6796875, 0.6875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.8515625, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 0.6796875, 0.69140625, 0.6796875, 1.0, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 0.6875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6875, 1.0, 1.0, 0.68359375, 1.0, 0.68359375, 0.6796875, 0.68359375, 1.0, 1.0, 0.69921875, 0.68359375, 0.6796875, 1.0, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.68359375, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 0.6796875, 0.6875, 0.6796875, 0.68359375, 0.6796875, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 0.6796875, 1.0, 0.74609375, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 0.68359375, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0]

 sparsity of   [1.0, 1.0, 0.109375, 0.1015625, 0.138671875, 0.109375, 0.103515625, 1.0, 0.095703125, 1.0, 1.0, 1.0, 1.0, 0.115234375, 0.109375, 1.0, 1.0, 1.0, 0.103515625, 0.107421875, 1.0, 0.103515625, 1.0, 1.0, 0.12109375, 1.0, 0.09375, 1.0, 1.0, 0.111328125, 0.17578125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.115234375, 0.1015625, 0.11328125, 1.0, 0.11328125, 0.107421875, 1.0, 0.125, 0.10546875, 1.0, 0.1171875, 1.0, 1.0, 0.11328125, 1.0, 0.1015625, 0.10546875, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.119140625, 0.107421875, 1.0, 1.0, 1.0, 0.11328125, 0.1015625, 1.0, 0.10546875, 0.107421875, 0.1015625, 0.109375, 0.099609375, 0.126953125, 0.107421875, 1.0, 0.109375, 0.111328125, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 0.095703125, 0.123046875, 1.0, 0.09765625, 0.095703125, 1.0, 1.0, 0.16015625, 0.125, 1.0, 0.123046875, 0.10546875, 0.11328125, 1.0, 0.11328125, 1.0, 0.1328125, 0.109375, 0.1015625, 0.111328125, 0.10546875, 1.0, 1.0, 0.107421875, 0.107421875, 1.0, 0.12890625, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 0.107421875, 0.1171875, 0.130859375, 1.0, 1.0, 0.123046875, 1.0, 0.107421875, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.115234375, 0.109375, 1.0, 0.130859375, 0.166015625, 1.0, 1.0, 1.0, 0.1015625, 0.09765625, 0.109375, 1.0, 0.10546875, 1.0, 1.0, 0.103515625, 1.0, 0.1171875, 0.150390625, 1.0, 0.12109375, 0.16796875, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.10546875, 1.0, 0.125, 0.10546875, 0.09765625, 0.11328125, 0.123046875, 1.0, 1.0, 0.109375, 0.11328125, 0.154296875, 0.09375, 0.1171875, 1.0, 1.0, 0.1171875, 1.0, 0.103515625, 0.107421875, 1.0, 1.0, 0.111328125, 1.0, 0.115234375, 1.0, 0.115234375, 1.0, 1.0, 0.220703125, 0.109375, 0.1328125, 1.0, 0.109375, 0.203125, 0.099609375, 1.0, 0.125, 1.0, 1.0, 1.0, 0.138671875, 1.0, 0.099609375, 1.0, 0.107421875, 1.0, 0.09765625, 1.0, 1.0, 1.0, 0.12890625, 0.111328125, 0.115234375, 0.107421875, 1.0, 1.0, 0.125, 0.115234375, 0.134765625, 1.0, 1.0, 0.111328125, 0.134765625, 0.24609375, 0.10546875, 1.0, 0.099609375, 1.0, 1.0, 0.142578125, 0.099609375, 0.111328125, 0.1015625, 0.11328125, 0.111328125, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.09765625, 0.109375, 0.111328125, 0.10546875, 1.0, 0.126953125, 0.1171875, 1.0, 0.1015625, 1.0, 0.11328125, 0.134765625, 1.0, 0.11328125, 0.09375, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 0.11328125, 0.1015625, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.10546875, 0.107421875, 0.1171875, 0.12890625, 1.0, 0.111328125, 1.0, 0.158203125, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.1171875, 0.107421875, 0.1015625, 0.107421875, 0.16015625, 1.0, 1.0, 0.119140625, 1.0, 0.2890625, 1.0, 0.1171875, 1.0, 0.115234375, 1.0, 1.0, 1.0, 0.099609375, 0.115234375, 1.0, 1.0, 0.123046875, 1.0, 0.111328125, 0.115234375, 1.0, 1.0, 0.107421875, 1.0, 0.109375, 0.111328125, 1.0, 1.0, 1.0, 1.0, 0.103515625, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.107421875, 0.107421875, 1.0, 1.0, 0.115234375, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.1015625, 0.123046875, 1.0, 0.109375, 0.10546875, 0.109375, 1.0, 0.107421875, 0.12109375, 0.103515625, 0.103515625, 0.109375, 0.1171875, 0.10546875, 1.0, 0.107421875, 1.0, 1.0, 1.0, 0.109375, 0.115234375, 0.123046875, 0.109375, 1.0, 0.109375, 1.0, 0.1015625, 1.0, 1.0, 0.1171875, 0.123046875, 0.107421875, 0.115234375, 0.134765625, 0.1015625, 0.099609375, 1.0, 1.0, 0.10546875, 0.107421875, 0.103515625, 1.0, 1.0, 0.1015625, 0.103515625, 0.1015625, 1.0, 0.115234375, 1.0, 1.0, 0.10546875, 1.0, 0.103515625, 1.0, 0.10546875, 1.0, 0.107421875, 0.146484375, 1.0, 0.140625, 1.0, 0.10546875, 1.0, 0.109375, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.267578125, 1.0, 0.109375, 1.0, 0.1015625, 1.0, 1.0, 0.10546875, 1.0, 0.1328125, 0.107421875, 1.0, 1.0, 0.11328125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.10546875, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.1171875, 0.111328125, 0.107421875, 0.095703125, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 0.10546875, 0.12109375, 0.11328125, 1.0, 1.0, 0.08984375, 1.0, 0.107421875, 0.119140625, 0.119140625, 1.0, 1.0, 1.0, 0.10546875, 1.0, 0.09765625, 0.10546875, 0.125, 0.115234375, 1.0, 0.1171875, 0.1015625, 1.0, 1.0, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.107421875, 1.0, 0.103515625, 0.10546875, 1.0, 1.0, 0.10546875, 1.0, 1.0, 0.109375, 0.11328125, 1.0, 0.11328125, 0.119140625, 1.0, 0.21484375, 0.11328125, 1.0, 0.115234375, 0.107421875, 1.0, 0.11328125, 1.0, 0.10546875, 0.111328125, 0.1171875, 0.109375, 1.0, 1.0, 0.10546875, 1.0, 0.109375, 0.099609375, 1.0, 1.0, 0.099609375, 0.115234375, 1.0, 0.126953125, 0.12109375, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.099609375, 0.099609375, 0.107421875, 0.1015625, 0.115234375, 1.0, 1.0, 0.103515625, 1.0, 0.11328125, 0.10546875, 0.1171875, 1.0, 0.11328125, 0.1171875, 1.0, 0.111328125, 0.107421875, 0.11328125, 0.1328125, 1.0, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.109375, 0.119140625, 0.1015625, 0.1015625, 1.0, 0.115234375, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.115234375, 1.0, 1.0, 1.0, 0.109375, 0.115234375, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 0.10546875, 1.0, 0.09375, 0.1328125, 0.1015625, 1.0, 0.115234375, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.111328125, 0.10546875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 0.140625, 0.09765625, 1.0, 0.11328125, 0.10546875, 1.0, 1.0, 0.12109375, 1.0, 0.12890625, 0.1015625, 0.216796875, 1.0, 0.109375, 0.09375, 0.115234375, 0.115234375, 0.11328125, 0.119140625, 0.16015625, 0.109375, 1.0, 1.0, 1.0, 0.119140625, 1.0, 1.0, 1.0, 0.09765625, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10546875, 0.177734375, 0.099609375, 1.0, 0.1171875, 0.1015625, 0.111328125, 0.11328125, 1.0, 1.0, 0.109375, 0.16015625, 0.103515625, 1.0, 0.119140625, 0.1015625, 1.0, 1.0, 0.103515625, 0.189453125, 0.119140625, 0.1484375, 1.0, 0.111328125, 0.13671875, 0.111328125, 0.12890625, 0.111328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 1.0, 1.0, 1.0, 0.1171875, 0.09765625, 1.0, 0.123046875, 0.10546875, 1.0, 1.0, 0.123046875, 1.0, 0.115234375, 0.14453125, 0.099609375, 1.0, 0.1171875, 0.107421875, 0.109375, 0.109375, 1.0, 1.0, 0.16796875, 0.11328125, 1.0, 1.0, 0.109375, 0.119140625, 1.0, 0.109375, 0.109375, 0.107421875, 0.10546875, 0.107421875, 0.103515625, 1.0, 1.0, 0.119140625, 1.0, 0.111328125, 0.109375, 0.271484375, 0.111328125, 0.12109375, 1.0, 1.0, 0.111328125, 0.130859375, 1.0, 1.0, 0.111328125, 1.0, 0.103515625, 1.0, 0.109375, 1.0, 0.099609375, 0.10546875, 1.0, 0.119140625, 0.109375, 1.0, 1.0, 0.109375, 0.134765625, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 0.111328125, 1.0, 1.0, 0.103515625, 0.1171875, 0.09765625, 1.0, 0.12109375, 0.10546875, 0.111328125, 1.0, 1.0, 1.0, 0.10546875, 0.109375, 0.1328125, 0.142578125, 0.158203125, 1.0, 0.103515625, 0.109375, 0.171875, 0.119140625, 1.0, 0.125, 1.0, 0.181640625, 0.099609375, 1.0, 0.1171875, 0.11328125, 0.099609375, 1.0, 1.0, 0.138671875, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.119140625, 1.0, 1.0, 0.111328125, 1.0, 0.248046875, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.107421875, 0.12109375, 1.0, 0.138671875, 1.0, 0.09765625, 0.107421875, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.119140625, 0.11328125, 0.115234375, 1.0, 0.109375, 1.0, 0.103515625, 0.10546875, 0.11328125, 1.0, 0.111328125, 0.10546875, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.115234375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 0.099609375, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 1.0, 0.107421875, 1.0, 0.111328125, 0.18359375, 1.0, 0.109375, 1.0, 0.115234375, 1.0, 0.1875, 0.1171875, 0.09765625, 1.0, 0.107421875, 1.0, 0.11328125, 1.0, 0.1171875, 1.0, 1.0, 0.107421875, 1.0, 0.1171875, 0.169921875, 0.11328125, 0.12890625, 1.0, 0.11328125, 0.15234375, 1.0, 1.0, 0.10546875, 1.0, 0.1015625, 0.1171875, 1.0, 1.0, 1.0, 0.130859375, 0.099609375, 1.0, 0.130859375, 0.111328125, 0.111328125, 1.0, 1.0, 0.115234375, 0.11328125, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.134765625, 0.19140625, 0.125, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.126953125, 1.0, 0.11328125, 0.10546875, 0.115234375, 1.0, 1.0, 0.11328125, 0.10546875, 0.115234375, 1.0, 1.0, 0.103515625, 0.107421875, 0.10546875, 0.11328125, 0.11328125, 0.1171875, 0.115234375, 1.0, 1.0, 1.0, 0.115234375, 0.099609375, 0.185546875, 0.109375, 1.0, 0.123046875, 0.119140625, 1.0, 0.119140625, 0.10546875, 0.12109375, 0.125, 1.0, 1.0, 0.115234375, 0.111328125, 0.111328125, 0.10546875, 0.109375, 0.115234375, 0.107421875, 0.103515625, 0.1015625, 1.0, 0.10546875, 0.119140625, 1.0, 1.0, 1.0, 0.115234375, 0.1171875, 1.0, 1.0, 0.123046875, 1.0, 0.11328125, 0.109375, 0.1015625, 0.119140625, 1.0, 1.0, 1.0, 0.12109375, 0.107421875, 1.0, 1.0, 0.10546875, 1.0, 1.0, 1.0, 0.1171875, 0.111328125, 0.111328125, 0.123046875, 1.0, 1.0, 1.0, 0.177734375, 1.0, 1.0, 1.0, 1.0, 0.099609375, 1.0]

 sparsity of   [1.0, 1.0, 0.4326171875, 0.431640625, 0.4296875, 1.0, 0.4189453125, 0.4150390625, 1.0, 1.0, 0.4169921875, 0.4189453125, 1.0, 0.423828125, 1.0, 0.4169921875, 0.4189453125, 1.0, 1.0, 0.4228515625, 1.0, 0.416015625, 0.4287109375, 0.4189453125, 1.0, 0.4189453125, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 0.4228515625, 1.0, 0.416015625, 1.0, 0.4345703125, 0.416015625, 1.0, 1.0, 0.4248046875, 1.0, 1.0, 1.0, 1.0, 0.4189453125, 1.0, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 1.0, 0.4228515625, 0.412109375, 1.0, 1.0, 0.419921875, 1.0, 0.4462890625, 1.0, 0.4228515625, 0.42578125, 1.0, 0.41796875, 1.0, 0.4189453125, 0.4189453125, 0.41796875, 1.0, 1.0, 1.0, 0.419921875, 0.41796875, 0.4189453125, 1.0, 1.0, 1.0, 0.4306640625, 1.0, 0.4267578125, 0.4296875, 1.0, 0.4169921875, 1.0, 1.0, 0.423828125, 1.0, 1.0, 1.0, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.41796875, 1.0, 0.421875, 0.4208984375, 1.0, 1.0, 0.419921875, 0.4228515625, 0.4326171875, 0.41796875, 0.419921875, 0.419921875, 0.416015625, 1.0, 0.416015625, 1.0, 0.4140625, 0.4248046875, 1.0, 1.0, 1.0, 1.0, 0.421875, 0.4287109375, 1.0, 1.0, 0.421875, 0.431640625, 1.0, 1.0, 0.41796875, 1.0, 0.41796875, 0.4189453125, 1.0, 1.0, 0.4130859375, 0.4169921875, 1.0, 1.0, 1.0, 1.0, 0.419921875, 0.421875, 1.0, 1.0, 1.0, 0.4150390625, 0.41796875, 0.4189453125, 1.0, 0.421875, 1.0, 1.0, 0.419921875, 0.4208984375, 1.0, 1.0, 1.0, 0.4150390625, 0.416015625, 1.0, 1.0, 1.0, 0.423828125, 1.0, 0.4189453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42578125, 1.0, 1.0, 0.4208984375, 1.0, 0.453125, 1.0, 1.0, 1.0, 0.421875, 0.41796875, 1.0, 0.4287109375, 1.0, 0.4228515625, 1.0, 0.421875, 1.0, 1.0, 1.0, 0.4521484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.419921875, 1.0, 1.0, 0.4169921875, 1.0, 1.0, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4150390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 0.41796875, 0.4326171875, 0.4150390625, 1.0, 0.423828125, 1.0, 0.421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.435546875, 1.0, 0.4296875, 1.0, 0.4140625, 1.0, 0.4228515625, 1.0, 1.0, 1.0, 0.4140625, 1.0, 0.4296875, 1.0, 0.4208984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.423828125, 1.0, 1.0, 0.41796875]

 sparsity of   [0.5907118320465088, 1.0, 0.5933159589767456, 0.5885416865348816, 0.5881076455116272, 0.588975727558136, 0.5907118320465088, 1.0, 0.5933159589767456, 0.5902777910232544, 1.0, 1.0, 1.0, 0.5894097089767456, 0.592881977558136, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5911458134651184, 1.0, 0.5894097089767456, 1.0, 1.0, 0.5855034589767456, 1.0, 0.5872395634651184, 0.5907118320465088, 0.5907118320465088, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 0.5894097089767456, 0.5881076455116272, 1.0, 0.5915798544883728, 0.5885416865348816, 1.0, 0.5868055820465088, 1.0, 1.0, 1.0, 0.5872395634651184, 1.0, 0.5855034589767456, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5868055820465088, 1.0, 1.0, 1.0, 0.5907118320465088, 0.5894097089767456, 1.0, 1.0, 1.0, 0.58984375, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.5911458134651184, 1.0, 0.5872395634651184, 1.0, 1.0, 1.0, 0.588975727558136, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 1.0, 0.588975727558136, 1.0, 1.0, 0.5894097089767456, 0.5911458134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.5941840410232544, 0.5876736044883728, 1.0, 1.0, 0.5902777910232544, 1.0, 1.0, 0.588975727558136, 0.5876736044883728, 1.0, 0.5885416865348816, 0.588975727558136, 1.0, 1.0, 1.0, 0.5911458134651184, 1.0, 0.5885416865348816, 1.0, 0.5894097089767456, 0.5902777910232544, 1.0, 1.0, 1.0, 1.0, 0.5941840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 1.0, 1.0, 1.0, 1.0, 0.5881076455116272, 0.58984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5907118320465088, 1.0, 0.5915798544883728, 0.5915798544883728, 0.588975727558136, 0.5855034589767456, 0.5907118320465088, 0.5907118320465088, 1.0, 1.0, 1.0, 0.5920138955116272, 0.5885416865348816, 1.0, 1.0, 1.0, 1.0, 0.5902777910232544, 0.5885416865348816, 1.0, 0.5868055820465088, 0.588975727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.588975727558136, 0.5902777910232544, 1.0, 0.5863715410232544, 1.0, 1.0, 1.0, 1.0, 0.5933159589767456, 0.59375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 0.5920138955116272, 1.0, 1.0, 1.0, 0.5881076455116272, 0.588975727558136, 0.5894097089767456, 1.0, 0.5894097089767456, 1.0, 1.0, 0.5885416865348816, 1.0, 1.0, 0.5885416865348816, 1.0, 0.58984375, 1.0, 1.0, 0.58984375, 1.0, 1.0, 1.0, 0.5902777910232544, 0.58984375, 1.0, 0.5920138955116272, 1.0, 1.0, 0.5924479365348816, 1.0, 0.5894097089767456, 1.0, 1.0, 1.0, 1.0, 0.5907118320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.585069477558136, 1.0, 0.5902777910232544, 1.0, 0.588975727558136, 0.5907118320465088, 1.0, 0.5885416865348816, 0.5920138955116272, 1.0, 1.0, 1.0, 0.5885416865348816, 1.0, 0.5876736044883728, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.62109375, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.62109375, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.62109375, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 0.90625, 1.0, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 1.0, 0.62109375, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.6171875, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.62109375, 1.0, 0.6328125, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.62109375, 0.6171875, 0.61328125, 1.0, 0.62109375, 0.6171875, 0.61328125, 0.61328125, 0.60546875, 0.62109375, 0.62890625, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.62109375, 0.62890625, 0.61328125, 1.0, 0.6171875, 0.6171875, 1.0, 1.0, 0.62109375, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.62890625, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 0.625, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 1.0, 0.62890625, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.62109375, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.62890625, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.62109375, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.62109375, 0.6171875, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.62109375, 1.0, 0.62109375, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.62890625, 0.6171875, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.64453125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.625, 0.61328125, 0.61328125, 1.0, 0.6328125, 0.61328125, 0.61328125, 1.0, 0.62109375, 1.0, 0.6171875, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.61328125, 0.63671875, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.6171875, 1.0, 0.6171875, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.62109375, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.609375, 1.0, 0.6171875, 1.0, 1.0, 0.609375, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.60546875, 1.0, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 0.609375, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 0.703125, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 0.6171875, 0.6171875, 0.6171875, 0.62109375, 0.6171875, 0.6171875, 0.609375, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.625, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.63671875, 0.61328125, 0.68359375, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.62890625, 0.6328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 0.625, 1.0, 0.6171875, 0.625, 0.6171875, 0.6171875, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.62109375, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.7890625, 1.0, 0.62109375, 1.0, 1.0, 1.0, 0.609375, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.609375, 0.61328125, 1.0, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.62109375, 0.61328125, 1.0, 1.0, 0.62109375, 1.0, 0.62109375, 0.609375, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.62109375, 1.0, 0.609375, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.62109375, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6328125, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.6484375, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.62109375, 1.0, 1.0, 0.6171875, 1.0, 0.61328125, 0.62109375, 0.61328125, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.609375, 0.61328125, 1.0, 0.6171875, 1.0, 0.625, 0.6171875, 0.61328125, 0.61328125, 0.6328125, 1.0, 0.6171875, 1.0, 0.6171875, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.625, 0.6171875, 0.625, 0.61328125, 1.0, 0.6171875, 0.61328125, 1.0, 1.0, 0.62890625, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6328125, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.62109375, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.62109375, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.625, 0.6171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62109375, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 0.62109375, 0.61328125, 1.0, 0.62109375, 0.64453125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 0.6171875, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 0.61328125, 1.0, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.6328125, 0.6171875, 0.61328125, 1.0, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.61328125, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.6171875, 0.62109375, 1.0, 0.6171875, 0.6171875, 0.61328125, 0.625, 0.6171875, 0.61328125, 0.6171875, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.6171875, 1.0, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.6171875, 1.0, 0.61328125, 1.0, 0.62109375, 0.61328125, 0.61328125, 1.0, 0.625, 0.6171875, 0.6171875, 1.0, 0.61328125, 0.625, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 1.0, 1.0, 0.62109375, 0.61328125, 0.61328125, 0.625, 0.61328125, 0.61328125, 0.61328125, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.62109375, 0.6171875, 1.0, 0.61328125, 0.6171875, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.625, 1.0, 0.61328125, 0.6171875, 0.6171875, 0.61328125, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.61328125, 1.0, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 1.0, 0.61328125, 1.0, 1.0, 0.62109375, 1.0, 0.6171875, 0.61328125, 0.61328125, 0.6171875, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 0.61328125, 1.0, 1.0, 0.61328125, 0.625, 0.61328125, 0.61328125, 0.6171875, 0.61328125, 0.6171875, 1.0, 0.640625, 1.0, 1.0, 1.0, 0.6171875, 0.61328125, 0.61328125]

 sparsity of   [1.0, 0.2783203125, 1.0, 0.2890625, 1.0, 0.27734375, 0.2841796875, 1.0, 1.0, 1.0, 0.283203125, 0.2822265625, 0.279296875, 1.0, 0.2861328125, 0.287109375, 0.2890625, 1.0, 0.283203125, 1.0, 0.28515625, 0.2841796875, 1.0, 1.0, 1.0, 0.2841796875, 1.0, 0.2822265625, 0.2890625, 0.2841796875, 0.275390625, 0.287109375, 1.0, 0.2880859375, 1.0, 1.0, 0.2861328125, 1.0, 0.28125, 0.287109375, 0.2890625, 0.2900390625, 0.2861328125, 1.0, 1.0, 0.283203125, 0.29296875, 0.2880859375, 1.0, 0.2841796875, 0.2939453125, 0.28515625, 0.2783203125, 0.28515625, 1.0, 1.0, 0.2841796875, 0.28515625, 1.0, 1.0, 0.2822265625, 0.27734375, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 0.28125, 0.28515625, 0.2841796875, 0.283203125, 0.2939453125, 0.291015625, 0.287109375, 1.0, 0.283203125, 1.0, 0.28515625, 1.0, 0.28515625, 1.0, 0.2841796875, 0.27734375, 0.279296875, 1.0, 1.0, 1.0, 0.2890625, 0.283203125, 1.0, 0.283203125, 1.0, 0.2861328125, 0.28515625, 1.0, 0.2822265625, 1.0, 1.0, 0.283203125, 0.283203125, 0.28125, 1.0, 1.0, 0.279296875, 0.2783203125, 0.2861328125, 0.2880859375, 0.287109375, 0.2861328125, 1.0, 0.287109375, 1.0, 1.0, 0.28125, 0.2900390625, 0.2841796875, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 0.2783203125, 1.0, 1.0, 0.279296875, 1.0, 1.0, 0.28515625, 0.2802734375, 0.2900390625, 1.0, 0.28125, 0.2802734375, 0.2880859375, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 0.287109375, 1.0, 1.0, 1.0, 0.2822265625, 1.0, 1.0, 0.287109375, 0.28125, 0.2841796875, 0.2841796875, 0.291015625, 1.0, 1.0, 0.28515625, 0.2861328125, 0.2861328125, 0.287109375, 1.0, 0.2861328125, 0.2822265625, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 1.0, 1.0, 0.28515625, 1.0, 1.0, 0.2822265625, 0.27734375, 1.0, 1.0, 1.0, 1.0, 0.283203125, 0.2841796875, 1.0, 0.283203125, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 0.2802734375, 0.279296875, 0.29296875, 0.287109375, 0.283203125, 0.2841796875, 0.2802734375, 0.275390625, 1.0, 0.275390625, 0.2841796875, 0.283203125, 0.2900390625, 1.0, 0.279296875, 0.291015625, 1.0, 0.287109375, 1.0, 0.27734375, 0.2783203125, 0.2890625, 0.28125, 0.27734375, 1.0, 0.2822265625, 1.0, 0.291015625, 0.28515625, 1.0, 1.0, 0.2919921875, 1.0, 0.2763671875, 0.2900390625, 0.283203125, 0.28125, 1.0, 0.28515625, 1.0, 1.0, 0.2900390625, 0.28515625, 0.2783203125, 0.2880859375, 1.0, 0.2880859375, 1.0, 0.2783203125, 1.0, 0.28515625, 0.279296875, 0.28515625, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.44921875, 1.0, 1.0, 1.0, 0.4470486044883728, 1.0, 0.4461805522441864, 0.444878488779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444477558136, 0.4461805522441864, 1.0, 0.4435763955116272, 0.4427083432674408, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4470486044883728, 1.0, 0.444878488779068, 0.4431423544883728, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4440104067325592, 0.444878488779068, 1.0, 0.4405381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4479166567325592, 0.4422743022441864, 0.4435763955116272, 0.44140625, 1.0, 1.0, 0.4457465410232544, 1.0, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4418402910232544, 1.0, 1.0, 1.0, 1.0, 0.444878488779068, 0.4418402910232544, 0.4435763955116272, 1.0, 0.4431423544883728, 1.0, 1.0, 0.4466145932674408, 1.0, 1.0, 1.0, 0.4474826455116272, 1.0, 0.444878488779068, 0.4444444477558136, 0.4431423544883728, 0.4457465410232544, 1.0, 1.0, 0.4435763955116272, 0.448784738779068, 1.0, 1.0, 1.0, 0.4405381977558136, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 0.4435763955116272, 0.4466145932674408, 0.4431423544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 1.0, 0.4457465410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444477558136, 1.0, 1.0, 0.4422743022441864, 1.0, 1.0, 1.0, 1.0, 0.4483506977558136, 1.0, 1.0, 1.0, 1.0, 0.4418402910232544, 0.4431423544883728, 1.0, 1.0, 0.440972238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4431423544883728, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4461805522441864, 0.4431423544883728, 1.0, 1.0, 0.444878488779068, 1.0, 0.4444444477558136, 1.0, 1.0, 1.0, 1.0, 0.4435763955116272, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.444878488779068, 1.0, 0.4435763955116272, 1.0, 1.0, 1.0, 1.0, 0.4457465410232544, 1.0, 0.4431423544883728, 1.0, 0.4444444477558136, 0.4431423544883728, 1.0, 1.0, 0.4479166567325592, 0.4427083432674408, 0.4418402910232544, 0.4431423544883728, 0.444878488779068, 1.0, 0.4440104067325592, 1.0, 0.4427083432674408, 1.0, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 0.4440104067325592, 0.4418402910232544, 1.0, 1.0, 1.0, 0.444878488779068, 1.0, 0.4405381977558136, 0.4427083432674408, 1.0, 0.44140625, 0.44921875, 1.0, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4418402910232544, 1.0, 1.0, 0.4422743022441864, 1.0, 1.0, 0.4440104067325592, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 0.6875, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.6875, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 0.6875, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.68359375, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.6796875, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 0.6875, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6953125, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 0.6875, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.68359375, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.6953125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 0.6875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 0.6796875, 0.67578125, 1.0, 1.0, 0.6875, 1.0, 0.67578125, 1.0, 1.0, 0.6953125, 1.0, 0.67578125, 1.0, 0.6875, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.6875, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.69140625, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.68359375, 1.0, 1.0, 0.68359375, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.7109375, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.67578125, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.6796875, 0.68359375, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.6796875, 0.6796875, 1.0, 0.67578125, 0.6796875, 0.68359375, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.68359375, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.68359375, 1.0, 0.6796875, 0.67578125, 0.6796875, 0.6796875, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.6796875, 0.68359375, 0.6796875, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.68359375, 0.68359375, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 0.68359375, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 1.0, 0.6796875, 1.0, 1.0, 0.68359375, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.68359375, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 0.6796875, 0.67578125, 0.6796875, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 0.67578125, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.68359375, 1.0, 1.0, 1.0, 0.6796875, 0.6796875, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.68359375, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.6875, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.6796875, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.6796875, 1.0, 0.68359375, 1.0, 0.68359375, 0.67578125, 0.67578125, 0.6875, 1.0, 0.6796875, 0.67578125, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125, 1.0, 1.0, 0.67578125, 0.6796875, 1.0, 1.0, 0.68359375, 0.6796875, 0.6796875, 0.67578125, 0.67578125, 1.0, 0.67578125, 0.69140625, 1.0, 1.0, 1.0, 0.6796875, 0.68359375, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 0.6796875, 0.67578125, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 1.0, 0.67578125, 1.0, 1.0, 1.0, 1.0, 0.67578125, 0.67578125, 0.67578125, 0.67578125]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 0.275390625, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 0.3095703125, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 0.279296875, 1.0, 0.2783203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2861328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2880859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29296875, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 0.318359375, 1.0, 1.0, 0.341796875, 1.0, 0.2783203125, 1.0, 1.0, 1.0, 0.2900390625, 1.0, 1.0, 1.0, 0.275390625, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 0.2705078125, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 0.279296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3017578125, 1.0, 1.0, 0.2763671875, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8376736044883728, 1.0, 0.8420138955116272, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 0.8142361044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.811631977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8081597089767456, 1.0, 1.0, 1.0, 1.0, 0.827256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8094618320465088, 1.0, 1.0, 1.0, 1.0, 0.8059895634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8146701455116272, 0.8441840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.823350727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.835069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8133680820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 1.0, 0.8103298544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8111979365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8493923544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8177083134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8068576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 0.91015625, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91015625]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 0.2587890625, 1.0, 1.0, 1.0, 1.0, 0.2841796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2529296875, 1.0, 1.0, 1.0, 0.27734375, 0.263671875, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.89453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8958333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8949652910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 0.94921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94921875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3212890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 0.6396484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3271484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5087890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.310546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.798828125, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.283203125, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.263671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.267578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.330078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2724609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 0.3291015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2939453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7197265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6513671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 0.3203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2763671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30078125, 1.0, 1.0, 0.3330078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2802734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2685546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.287109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.298828125, 1.0, 1.0, 1.0, 1.0, 0.2666015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.44140625, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2705078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30859375, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2626953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2646484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.294921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2744140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3955078125, 1.0, 0.26171875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96044921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95947265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9599609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9619140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.973741352558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9774305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.971788227558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9754774570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9761284589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9763454794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9772135615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.978515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9778645634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9789496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 0.9735243320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9750434160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9778645634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9763454794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.96484375, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.71533203125, 0.7080078125, 0.70068359375, 0.70849609375, 0.7041015625, 0.70556640625, 0.7060546875, 0.71484375, 0.70947265625, 0.70947265625, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125]

Total parameter pruned: 22184105.006554656 (unstructured) 20737874 (structured)

Test: [0/79]	Time 0.268 (0.268)	Loss 0.2751 (0.2751) ([0.171]+[0.104])	Prec@1 95.312 (95.312)
 * Prec@1 94.210
current lr 1.00000e-03
Grad=  tensor(1.7015, device='cuda:0')
Epoch: [300][0/391]	Time 0.297 (0.297)	Data 0.224 (0.224)	Loss 0.0081 (0.0081) ([0.008]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [300][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [300][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0120 (0.0042) ([0.012]+[0.000])	Prec@1 99.219 (99.949)
Epoch: [300][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0045) ([0.001]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1564 (0.1564) ([0.156]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.230
current lr 1.00000e-03
Grad=  tensor(0.0086, device='cuda:0')
Epoch: [301][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0063 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [301][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [301][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0073 (0.0040) ([0.007]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1499 (0.1499) ([0.150]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.110
current lr 1.00000e-03
Grad=  tensor(0.0162, device='cuda:0')
Epoch: [302][0/391]	Time 0.284 (0.284)	Data 0.214 (0.214)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [302][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [302][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.1644 (0.1644) ([0.164]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(2.5625, device='cuda:0')
Epoch: [303][0/391]	Time 0.296 (0.296)	Data 0.225 (0.225)	Loss 0.0147 (0.0147) ([0.015]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [303][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0043) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [303][200/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [303][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.1607 (0.1607) ([0.161]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.070
current lr 1.00000e-03
Grad=  tensor(0.2227, device='cuda:0')
Epoch: [304][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0145 (0.0044) ([0.014]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [304][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [304][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.1544 (0.1544) ([0.154]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.310
current lr 1.00000e-03
Grad=  tensor(0.9512, device='cuda:0')
Epoch: [305][0/391]	Time 0.271 (0.271)	Data 0.201 (0.201)	Loss 0.0065 (0.0065) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [305][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0094 (0.0041) ([0.009]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [305][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.1721 (0.1721) ([0.172]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(2.7640, device='cuda:0')
Epoch: [306][0/391]	Time 0.272 (0.272)	Data 0.201 (0.201)	Loss 0.0141 (0.0141) ([0.014]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [306][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [306][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0079 (0.0041) ([0.008]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [306][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1901 (0.1901) ([0.190]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.220
current lr 1.00000e-03
Grad=  tensor(0.0131, device='cuda:0')
Epoch: [307][0/391]	Time 0.278 (0.278)	Data 0.207 (0.207)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [307][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [307][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0056 (0.0042) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1645 (0.1645) ([0.165]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-03
Grad=  tensor(0.0668, device='cuda:0')
Epoch: [308][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [308][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [308][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.1915 (0.1915) ([0.192]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.020
current lr 1.00000e-03
Grad=  tensor(8.1595, device='cuda:0')
Epoch: [309][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0232 (0.0232) ([0.023]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [309][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0018 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [309][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [309][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0042) ([0.001]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1904 (0.1904) ([0.190]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.260
current lr 1.00000e-03
Grad=  tensor(0.3707, device='cuda:0')
Epoch: [310][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.064 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [310][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0044 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [310][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1943 (0.1943) ([0.194]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.080
current lr 1.00000e-03
Grad=  tensor(2.2492, device='cuda:0')
Epoch: [311][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0036 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [311][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0101 (0.0041) ([0.010]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [311][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.2011 (0.2011) ([0.201]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-03
Grad=  tensor(0.1098, device='cuda:0')
Epoch: [312][0/391]	Time 0.306 (0.306)	Data 0.233 (0.233)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [312][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0098 (0.0038) ([0.010]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [312][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1872 (0.1872) ([0.187]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-03
Grad=  tensor(0.7375, device='cuda:0')
Epoch: [313][0/391]	Time 0.294 (0.294)	Data 0.216 (0.216)	Loss 0.0076 (0.0076) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0098 (0.0040) ([0.010]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [313][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [313][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0075 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1702 (0.1702) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.160
current lr 1.00000e-03
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [314][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0005 (0.0005) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [314][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [314][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.242 (0.242)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.280
current lr 1.00000e-03
Grad=  tensor(0.2442, device='cuda:0')
Epoch: [315][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0076 (0.0038) ([0.008]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [315][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [315][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1727 (0.1727) ([0.173]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.270
current lr 1.00000e-03
Grad=  tensor(0.1083, device='cuda:0')
Epoch: [316][0/391]	Time 0.286 (0.286)	Data 0.216 (0.216)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0029 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [316][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0050 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [316][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.277 (0.277)	Loss 0.1714 (0.1714) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.250
current lr 1.00000e-03
Grad=  tensor(2.7620, device='cuda:0')
Epoch: [317][0/391]	Time 0.308 (0.308)	Data 0.234 (0.234)	Loss 0.0087 (0.0087) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0052 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [317][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [317][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0073 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1718 (0.1718) ([0.172]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.300
current lr 1.00000e-03
Grad=  tensor(0.3288, device='cuda:0')
Epoch: [318][0/391]	Time 0.303 (0.303)	Data 0.232 (0.232)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0052 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [318][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [318][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2009 (0.2009) ([0.201]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.120
current lr 1.00000e-03
Grad=  tensor(4.6129, device='cuda:0')
Epoch: [319][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [319][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0026 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [319][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [319][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1992 (0.1992) ([0.199]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(0.2490, device='cuda:0')
Epoch: [320][0/391]	Time 0.309 (0.309)	Data 0.236 (0.236)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0028 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [320][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0085 (0.0035) ([0.008]+[0.000])	Prec@1 99.219 (99.953)
Epoch: [320][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1878 (0.1878) ([0.188]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.250
current lr 1.00000e-03
Grad=  tensor(1.5063, device='cuda:0')
Epoch: [321][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0041 (0.0029) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [321][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0031) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [321][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0152 (0.0036) ([0.015]+[0.000])	Prec@1 99.219 (99.951)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1592 (0.1592) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.210
current lr 1.00000e-03
Grad=  tensor(0.0356, device='cuda:0')
Epoch: [322][0/391]	Time 0.283 (0.283)	Data 0.213 (0.213)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0166 (0.0040) ([0.017]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [322][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [322][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0051 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1614 (0.1614) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.190
current lr 1.00000e-03
Grad=  tensor(11.5006, device='cuda:0')
Epoch: [323][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0353 (0.0353) ([0.035]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [323][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0015 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [323][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0101 (0.0042) ([0.010]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [323][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1816 (0.1816) ([0.182]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 94.140
current lr 1.00000e-03
Grad=  tensor(0.0897, device='cuda:0')
Epoch: [324][0/391]	Time 0.308 (0.308)	Data 0.237 (0.237)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [324][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [324][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1901 (0.1901) ([0.190]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.290
current lr 1.00000e-03
Grad=  tensor(0.0948, device='cuda:0')
Epoch: [325][0/391]	Time 0.265 (0.265)	Data 0.195 (0.195)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [325][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [325][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2081 (0.2081) ([0.208]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.210
current lr 1.00000e-03
Grad=  tensor(0.0025, device='cuda:0')
Epoch: [326][0/391]	Time 0.277 (0.277)	Data 0.207 (0.207)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0044 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [326][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [326][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.1731 (0.1731) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-03
Grad=  tensor(0.0225, device='cuda:0')
Epoch: [327][0/391]	Time 0.256 (0.256)	Data 0.186 (0.186)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0053 (0.0032) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [327][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [327][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1836 (0.1836) ([0.184]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.430
current lr 1.00000e-03
Grad=  tensor(0.2510, device='cuda:0')
Epoch: [328][0/391]	Time 0.266 (0.266)	Data 0.195 (0.195)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [328][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [328][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1854 (0.1854) ([0.185]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.320
current lr 1.00000e-03
Grad=  tensor(0.1736, device='cuda:0')
Epoch: [329][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [329][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [329][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-03
Grad=  tensor(1.0541, device='cuda:0')
Epoch: [330][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0064 (0.0064) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [330][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [330][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1806 (0.1806) ([0.181]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.370
current lr 1.00000e-03
Grad=  tensor(0.0924, device='cuda:0')
Epoch: [331][0/391]	Time 0.299 (0.299)	Data 0.228 (0.228)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0008 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [331][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0210 (0.0034) ([0.021]+[0.000])	Prec@1 99.219 (99.965)
Epoch: [331][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.1853 (0.1853) ([0.185]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.220
current lr 1.00000e-03
Grad=  tensor(0.2544, device='cuda:0')
Epoch: [332][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0010 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [332][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [332][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1902 (0.1902) ([0.190]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 94.360
current lr 1.00000e-03
Grad=  tensor(0.0186, device='cuda:0')
Epoch: [333][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [333][200/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [333][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0034) ([0.005]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1912 (0.1912) ([0.191]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.260
current lr 1.00000e-03
Grad=  tensor(0.5115, device='cuda:0')
Epoch: [334][0/391]	Time 0.304 (0.304)	Data 0.234 (0.234)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [334][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [334][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.1922 (0.1922) ([0.192]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.270
current lr 1.00000e-03
Grad=  tensor(0.2687, device='cuda:0')
Epoch: [335][0/391]	Time 0.263 (0.263)	Data 0.193 (0.193)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [335][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [335][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0069 (0.0036) ([0.007]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [335][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1832 (0.1832) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-03
Grad=  tensor(1.9215, device='cuda:0')
Epoch: [336][0/391]	Time 0.299 (0.299)	Data 0.229 (0.229)	Loss 0.0078 (0.0078) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [336][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [336][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0107 (0.0028) ([0.011]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.1757 (0.1757) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.210
current lr 1.00000e-03
Grad=  tensor(0.0056, device='cuda:0')
Epoch: [337][0/391]	Time 0.268 (0.268)	Data 0.198 (0.198)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [337][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [337][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.1815 (0.1815) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.170
current lr 1.00000e-03
Grad=  tensor(0.1549, device='cuda:0')
Epoch: [338][0/391]	Time 0.255 (0.255)	Data 0.184 (0.184)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [338][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [338][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1754 (0.1754) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(0.0159, device='cuda:0')
Epoch: [339][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0011 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [339][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0034) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [339][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1775 (0.1775) ([0.177]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.270
current lr 1.00000e-03
Grad=  tensor(0.0811, device='cuda:0')
Epoch: [340][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0169 (0.0036) ([0.017]+[0.000])	Prec@1 99.219 (99.946)
Epoch: [340][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0072 (0.0033) ([0.007]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [340][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0062 (0.0032) ([0.006]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.1701 (0.1701) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.130
current lr 1.00000e-03
Grad=  tensor(0.2299, device='cuda:0')
Epoch: [341][0/391]	Time 0.270 (0.270)	Data 0.200 (0.200)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [341][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [341][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0030) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.239 (0.239)	Loss 0.1472 (0.1472) ([0.147]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.440
current lr 1.00000e-03
Grad=  tensor(0.1121, device='cuda:0')
Epoch: [342][0/391]	Time 0.289 (0.289)	Data 0.218 (0.218)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [342][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0044 (0.0032) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [342][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1658 (0.1658) ([0.166]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.390
current lr 1.00000e-03
Grad=  tensor(0.1797, device='cuda:0')
Epoch: [343][0/391]	Time 0.283 (0.283)	Data 0.213 (0.213)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [343][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1840 (0.1840) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.280
current lr 1.00000e-03
Grad=  tensor(0.0244, device='cuda:0')
Epoch: [344][0/391]	Time 0.275 (0.275)	Data 0.206 (0.206)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0032) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [344][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [344][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1832 (0.1832) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-03
Grad=  tensor(0.1411, device='cuda:0')
Epoch: [345][0/391]	Time 0.280 (0.280)	Data 0.211 (0.211)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [345][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [345][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.2010 (0.2010) ([0.201]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.490
current lr 1.00000e-03
Grad=  tensor(0.0166, device='cuda:0')
Epoch: [346][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [346][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [346][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0032) ([0.001]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1623 (0.1623) ([0.162]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.220
current lr 1.00000e-03
Grad=  tensor(4.3148, device='cuda:0')
Epoch: [347][0/391]	Time 0.289 (0.289)	Data 0.216 (0.216)	Loss 0.0090 (0.0090) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [347][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0031) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [347][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0202 (0.0034) ([0.020]+[0.000])	Prec@1 99.219 (99.956)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1862 (0.1862) ([0.186]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.200
current lr 1.00000e-03
Grad=  tensor(0.0185, device='cuda:0')
Epoch: [348][0/391]	Time 0.290 (0.290)	Data 0.220 (0.220)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [348][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [348][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1885 (0.1885) ([0.188]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.330
current lr 1.00000e-03
Grad=  tensor(0.0379, device='cuda:0')
Epoch: [349][0/391]	Time 0.270 (0.270)	Data 0.201 (0.201)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [349][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0093 (0.0032) ([0.009]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [349][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0033) ([0.004]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.1967 (0.1967) ([0.197]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340
current lr 1.00000e-04
Grad=  tensor(0.0870, device='cuda:0')
Epoch: [350][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [350][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0125 (0.0030) ([0.013]+[0.000])	Prec@1 99.219 (99.973)
Epoch: [350][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0032) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.2068 (0.2068) ([0.207]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.320
current lr 1.00000e-04
Grad=  tensor(0.0456, device='cuda:0')
Epoch: [351][0/391]	Time 0.309 (0.309)	Data 0.237 (0.237)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0014 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [351][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [351][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1929 (0.1929) ([0.193]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.340
current lr 1.00000e-04
Grad=  tensor(0.0078, device='cuda:0')
Epoch: [352][0/391]	Time 0.289 (0.289)	Data 0.219 (0.219)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0027 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [352][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [352][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0028) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1787 (0.1787) ([0.179]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.240
current lr 1.00000e-04
Grad=  tensor(0.0131, device='cuda:0')
Epoch: [353][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [353][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1785 (0.1785) ([0.179]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-04
Grad=  tensor(0.0080, device='cuda:0')
Epoch: [354][0/391]	Time 0.254 (0.254)	Data 0.184 (0.184)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0010 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [354][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [354][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.1688 (0.1688) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [355][0/391]	Time 0.297 (0.297)	Data 0.227 (0.227)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0096 (0.0027) ([0.010]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [355][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0027) ([0.005]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [355][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.244 (0.244)	Loss 0.1801 (0.1801) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-04
Grad=  tensor(0.1759, device='cuda:0')
Epoch: [356][0/391]	Time 0.311 (0.311)	Data 0.239 (0.239)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0016 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [356][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [356][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.1929 (0.1929) ([0.193]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.370
current lr 1.00000e-04
Grad=  tensor(0.1996, device='cuda:0')
Epoch: [357][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [357][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0202 (0.0030) ([0.020]+[0.000])	Prec@1 99.219 (99.969)
Epoch: [357][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1677 (0.1677) ([0.168]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-04
Grad=  tensor(0.0189, device='cuda:0')
Epoch: [358][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0054 (0.0026) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [358][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [358][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1790 (0.1790) ([0.179]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.110
current lr 1.00000e-04
Grad=  tensor(0.0540, device='cuda:0')
Epoch: [359][0/391]	Time 0.306 (0.306)	Data 0.234 (0.234)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [359][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [359][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1737 (0.1737) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.300
current lr 1.00000e-04
Grad=  tensor(0.0248, device='cuda:0')
Epoch: [360][0/391]	Time 0.310 (0.310)	Data 0.239 (0.239)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [360][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [360][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1741 (0.1741) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-04
Grad=  tensor(0.0347, device='cuda:0')
Epoch: [361][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0056 (0.0028) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [361][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [361][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1851 (0.1851) ([0.185]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.280
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [362][0/391]	Time 0.280 (0.280)	Data 0.208 (0.208)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.064 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [362][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [362][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1873 (0.1873) ([0.187]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.320
current lr 1.00000e-04
Grad=  tensor(0.1606, device='cuda:0')
Epoch: [363][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1922 (0.1922) ([0.192]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.400
current lr 1.00000e-04
Grad=  tensor(0.0136, device='cuda:0')
Epoch: [364][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [364][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [364][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1838 (0.1838) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.300
current lr 1.00000e-04
Grad=  tensor(0.0184, device='cuda:0')
Epoch: [365][0/391]	Time 0.303 (0.303)	Data 0.233 (0.233)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0036 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [365][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [365][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.1972 (0.1972) ([0.197]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.280
current lr 1.00000e-04
Grad=  tensor(0.3343, device='cuda:0')
Epoch: [366][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0021 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [366][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [366][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0075 (0.0025) ([0.007]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1751 (0.1751) ([0.175]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.400
current lr 1.00000e-04
Grad=  tensor(0.0015, device='cuda:0')
Epoch: [367][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [367][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [367][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1607 (0.1607) ([0.161]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-04
Grad=  tensor(0.0062, device='cuda:0')
Epoch: [368][0/391]	Time 0.315 (0.315)	Data 0.243 (0.243)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.063 (0.066)	Data 0.000 (0.003)	Loss 0.0031 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [368][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [368][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0104 (0.0026) ([0.010]+[0.000])	Prec@1 99.219 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1799 (0.1799) ([0.180]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.410
current lr 1.00000e-04
Grad=  tensor(0.0310, device='cuda:0')
Epoch: [369][0/391]	Time 0.313 (0.313)	Data 0.242 (0.242)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [369][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1652 (0.1652) ([0.165]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.230
current lr 1.00000e-04
Grad=  tensor(0.0163, device='cuda:0')
Epoch: [370][0/391]	Time 0.291 (0.291)	Data 0.221 (0.221)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [370][200/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [370][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.1830 (0.1830) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-04
Grad=  tensor(0.0064, device='cuda:0')
Epoch: [371][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [371][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [371][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0076 (0.0026) ([0.008]+[0.000])	Prec@1 99.219 (99.979)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1811 (0.1811) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.280
current lr 1.00000e-04
Grad=  tensor(0.0483, device='cuda:0')
Epoch: [372][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0130 (0.0027) ([0.013]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [372][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [372][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1764 (0.1764) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-04
Grad=  tensor(0.0302, device='cuda:0')
Epoch: [373][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [373][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [373][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1794 (0.1794) ([0.179]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-04
Grad=  tensor(0.8694, device='cuda:0')
Epoch: [374][0/391]	Time 0.283 (0.283)	Data 0.213 (0.213)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [374][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [374][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.1862 (0.1862) ([0.186]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.250
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [375][0/391]	Time 0.304 (0.304)	Data 0.233 (0.233)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [375][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0028) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [375][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1764 (0.1764) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-04
Grad=  tensor(2.4681, device='cuda:0')
Epoch: [376][0/391]	Time 0.306 (0.306)	Data 0.234 (0.234)	Loss 0.0092 (0.0092) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0014 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [376][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [376][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1693 (0.1693) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-04
Grad=  tensor(1.5597, device='cuda:0')
Epoch: [377][0/391]	Time 0.312 (0.312)	Data 0.240 (0.240)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0025 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [377][200/391]	Time 0.063 (0.065)	Data 0.000 (0.001)	Loss 0.0024 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [377][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0049 (0.0024) ([0.005]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1673 (0.1673) ([0.167]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.470
current lr 1.00000e-04
Grad=  tensor(0.2333, device='cuda:0')
Epoch: [378][0/391]	Time 0.282 (0.282)	Data 0.212 (0.212)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [378][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [378][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1771 (0.1771) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-04
Grad=  tensor(0.0054, device='cuda:0')
Epoch: [379][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0015 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [379][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [379][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1572 (0.1572) ([0.157]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.420
current lr 1.00000e-04
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [380][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [380][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [380][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.1700 (0.1700) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-04
Grad=  tensor(0.1449, device='cuda:0')
Epoch: [381][0/391]	Time 0.287 (0.287)	Data 0.215 (0.215)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.064 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [381][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [381][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1577 (0.1577) ([0.158]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-04
Grad=  tensor(0.1188, device='cuda:0')
Epoch: [382][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [382][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [382][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.400
current lr 1.00000e-04
Grad=  tensor(0.0119, device='cuda:0')
Epoch: [383][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0043 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [383][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [383][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1697 (0.1697) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-04
Grad=  tensor(0.0678, device='cuda:0')
Epoch: [384][0/391]	Time 0.282 (0.282)	Data 0.211 (0.211)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [384][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [384][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1672 (0.1672) ([0.167]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.420
current lr 1.00000e-04
Grad=  tensor(0.1201, device='cuda:0')
Epoch: [385][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0060 (0.0024) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [385][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [385][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1686 (0.1686) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-04
Grad=  tensor(2.8530, device='cuda:0')
Epoch: [386][0/391]	Time 0.303 (0.303)	Data 0.232 (0.232)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [386][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0099 (0.0024) ([0.010]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [386][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1548 (0.1548) ([0.155]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.400
current lr 1.00000e-04
Grad=  tensor(0.1119, device='cuda:0')
Epoch: [387][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [387][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [387][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [387][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1722 (0.1722) ([0.172]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-04
Grad=  tensor(0.1760, device='cuda:0')
Epoch: [388][0/391]	Time 0.441 (0.441)	Data 0.371 (0.371)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.063 (0.067)	Data 0.000 (0.004)	Loss 0.0010 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [388][200/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [388][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.400
current lr 1.00000e-04
Grad=  tensor(0.0204, device='cuda:0')
Epoch: [389][0/391]	Time 0.301 (0.301)	Data 0.231 (0.231)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [389][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [389][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.1800 (0.1800) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.310
current lr 1.00000e-04
Grad=  tensor(0.0041, device='cuda:0')
Epoch: [390][0/391]	Time 0.273 (0.273)	Data 0.203 (0.203)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0052 (0.0026) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [390][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [390][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1689 (0.1689) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.210
current lr 1.00000e-04
Grad=  tensor(0.3966, device='cuda:0')
Epoch: [391][0/391]	Time 0.278 (0.278)	Data 0.205 (0.205)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [391][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [391][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1749 (0.1749) ([0.175]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.290
current lr 1.00000e-04
Grad=  tensor(0.0173, device='cuda:0')
Epoch: [392][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0007 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [392][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0062 (0.0023) ([0.006]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1747 (0.1747) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-04
Grad=  tensor(0.0926, device='cuda:0')
Epoch: [393][0/391]	Time 0.309 (0.309)	Data 0.237 (0.237)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [393][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [393][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1970 (0.1970) ([0.197]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.240
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [394][0/391]	Time 0.285 (0.285)	Data 0.215 (0.215)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [394][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0024) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1924 (0.1924) ([0.192]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.240
current lr 1.00000e-04
Grad=  tensor(0.0301, device='cuda:0')
Epoch: [395][0/391]	Time 0.292 (0.292)	Data 0.222 (0.222)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [395][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.1848 (0.1848) ([0.185]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.300
current lr 1.00000e-04
Grad=  tensor(0.0312, device='cuda:0')
Epoch: [396][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0091 (0.0032) ([0.009]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [396][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0028) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [396][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0027) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1824 (0.1824) ([0.182]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.260
current lr 1.00000e-04
Grad=  tensor(0.1921, device='cuda:0')
Epoch: [397][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [397][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [397][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1719 (0.1719) ([0.172]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.390
current lr 1.00000e-04
Grad=  tensor(0.0593, device='cuda:0')
Epoch: [398][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [398][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [398][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1841 (0.1841) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-04
Grad=  tensor(0.2996, device='cuda:0')
Epoch: [399][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0010 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [399][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [399][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1668 (0.1668) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.380
current lr 1.00000e-05
Grad=  tensor(0.0554, device='cuda:0')
Epoch: [400][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0020 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [400][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [400][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1800 (0.1800) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-05
Grad=  tensor(0.0294, device='cuda:0')
Epoch: [401][0/391]	Time 0.287 (0.287)	Data 0.216 (0.216)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0065 (0.0024) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [401][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [401][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1857 (0.1857) ([0.186]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.350
current lr 1.00000e-05
Grad=  tensor(0.0585, device='cuda:0')
Epoch: [402][0/391]	Time 0.269 (0.269)	Data 0.199 (0.199)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0030 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [402][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [402][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.1916 (0.1916) ([0.192]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-05
Grad=  tensor(0.0061, device='cuda:0')
Epoch: [403][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0037 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [403][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0056 (0.0026) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [403][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1766 (0.1766) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.520
current lr 1.00000e-05
Grad=  tensor(0.7877, device='cuda:0')
Epoch: [404][0/391]	Time 0.282 (0.282)	Data 0.212 (0.212)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [404][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [404][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.240 (0.240)	Loss 0.1604 (0.1604) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.360
current lr 1.00000e-05
Grad=  tensor(0.0086, device='cuda:0')
Epoch: [405][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0020 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [405][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1707 (0.1707) ([0.171]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-05
Grad=  tensor(0.0647, device='cuda:0')
Epoch: [406][0/391]	Time 0.302 (0.302)	Data 0.230 (0.230)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0035 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [406][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [406][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1644 (0.1644) ([0.164]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-05
Grad=  tensor(0.0063, device='cuda:0')
Epoch: [407][0/391]	Time 0.292 (0.292)	Data 0.220 (0.220)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [407][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1806 (0.1806) ([0.181]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.280
current lr 1.00000e-05
Grad=  tensor(0.0081, device='cuda:0')
Epoch: [408][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0087 (0.0022) ([0.009]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [408][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [408][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1780 (0.1780) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.400
current lr 1.00000e-05
Grad=  tensor(0.3008, device='cuda:0')
Epoch: [409][0/391]	Time 0.289 (0.289)	Data 0.220 (0.220)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [409][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [409][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1724 (0.1724) ([0.172]+[0.000])	Prec@1 96.094 (96.094)
 * Prec@1 94.300
current lr 1.00000e-05
Grad=  tensor(1.1897, device='cuda:0')
Epoch: [410][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0024 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [410][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [410][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.210
current lr 1.00000e-05
Grad=  tensor(0.4911, device='cuda:0')
Epoch: [411][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [411][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.1768 (0.1768) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.280
current lr 1.00000e-05
Grad=  tensor(0.1357, device='cuda:0')
Epoch: [412][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [412][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [412][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1748 (0.1748) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-05
Grad=  tensor(0.4359, device='cuda:0')
Epoch: [413][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0031 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [413][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [413][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1651 (0.1651) ([0.165]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-05
Grad=  tensor(0.0222, device='cuda:0')
Epoch: [414][0/391]	Time 0.289 (0.289)	Data 0.218 (0.218)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0037 (0.0028) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [414][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [414][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0081 (0.0024) ([0.008]+[0.000])	Prec@1 99.219 (99.990)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.1769 (0.1769) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-05
Grad=  tensor(0.0461, device='cuda:0')
Epoch: [415][0/391]	Time 0.290 (0.290)	Data 0.220 (0.220)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [415][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.1690 (0.1690) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.460
current lr 1.00000e-05
Grad=  tensor(0.5209, device='cuda:0')
Epoch: [416][0/391]	Time 0.270 (0.270)	Data 0.200 (0.200)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0038 (0.0024) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [416][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [416][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.1924 (0.1924) ([0.192]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.270
current lr 1.00000e-05
Grad=  tensor(0.0092, device='cuda:0')
Epoch: [417][0/391]	Time 0.253 (0.253)	Data 0.183 (0.183)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [417][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [417][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1774 (0.1774) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-05
Grad=  tensor(0.0083, device='cuda:0')
Epoch: [418][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0015 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [418][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [418][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1796 (0.1796) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-05
Grad=  tensor(0.0338, device='cuda:0')
Epoch: [419][0/391]	Time 0.308 (0.308)	Data 0.237 (0.237)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0030 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [419][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [419][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.1862 (0.1862) ([0.186]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.260
current lr 1.00000e-05
Grad=  tensor(1.6391, device='cuda:0')
Epoch: [420][0/391]	Time 0.308 (0.308)	Data 0.234 (0.234)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0019 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [420][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [420][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1868 (0.1868) ([0.187]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-05
Grad=  tensor(0.0728, device='cuda:0')
Epoch: [421][0/391]	Time 0.296 (0.296)	Data 0.226 (0.226)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0082 (0.0027) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [421][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [421][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.1844 (0.1844) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.290
current lr 1.00000e-05
Grad=  tensor(0.0921, device='cuda:0')
Epoch: [422][0/391]	Time 0.262 (0.262)	Data 0.191 (0.191)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0006 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [422][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [422][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1875 (0.1875) ([0.188]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.290
current lr 1.00000e-05
Grad=  tensor(0.0635, device='cuda:0')
Epoch: [423][0/391]	Time 0.269 (0.269)	Data 0.199 (0.199)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [423][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [423][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1796 (0.1796) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-05
Grad=  tensor(0.0231, device='cuda:0')
Epoch: [424][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0025 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [424][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [424][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1720 (0.1720) ([0.172]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.470
current lr 1.00000e-05
Grad=  tensor(0.0065, device='cuda:0')
Epoch: [425][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [425][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [425][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0093 (0.0025) ([0.009]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1837 (0.1837) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.450
current lr 1.00000e-05
Grad=  tensor(0.0212, device='cuda:0')
Epoch: [426][0/391]	Time 0.279 (0.279)	Data 0.209 (0.209)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [426][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [426][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0051 (0.0025) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.1690 (0.1690) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-05
Grad=  tensor(0.0029, device='cuda:0')
Epoch: [427][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [427][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [427][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1833 (0.1833) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-05
Grad=  tensor(0.0047, device='cuda:0')
Epoch: [428][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0068 (0.0021) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [428][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1761 (0.1761) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.290
current lr 1.00000e-05
Grad=  tensor(0.0420, device='cuda:0')
Epoch: [429][0/391]	Time 0.289 (0.289)	Data 0.219 (0.219)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [429][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [429][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.1826 (0.1826) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.320
current lr 1.00000e-05
Grad=  tensor(0.0723, device='cuda:0')
Epoch: [430][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [430][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [430][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1629 (0.1629) ([0.163]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-05
Grad=  tensor(0.0056, device='cuda:0')
Epoch: [431][0/391]	Time 0.286 (0.286)	Data 0.217 (0.217)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [431][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0093 (0.0023) ([0.009]+[0.000])	Prec@1 99.219 (99.984)
Epoch: [431][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1793 (0.1793) ([0.179]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.350
current lr 1.00000e-05
Grad=  tensor(0.0112, device='cuda:0')
Epoch: [432][0/391]	Time 0.294 (0.294)	Data 0.222 (0.222)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [432][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [432][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0006 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1700 (0.1700) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-05
Grad=  tensor(0.0595, device='cuda:0')
Epoch: [433][0/391]	Time 0.296 (0.296)	Data 0.224 (0.224)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [433][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [433][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1862 (0.1862) ([0.186]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.390
current lr 1.00000e-05
Grad=  tensor(0.2017, device='cuda:0')
Epoch: [434][0/391]	Time 0.310 (0.310)	Data 0.237 (0.237)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [434][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [434][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1799 (0.1799) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-05
Grad=  tensor(0.0334, device='cuda:0')
Epoch: [435][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [435][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [435][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1804 (0.1804) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.420
current lr 1.00000e-05
Grad=  tensor(0.3952, device='cuda:0')
Epoch: [436][0/391]	Time 0.304 (0.304)	Data 0.232 (0.232)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0035 (0.0028) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [436][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [436][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1836 (0.1836) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.370
current lr 1.00000e-05
Grad=  tensor(0.4052, device='cuda:0')
Epoch: [437][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [437][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.1826 (0.1826) ([0.183]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.500
current lr 1.00000e-05
Grad=  tensor(0.0145, device='cuda:0')
Epoch: [438][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [438][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [438][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1729 (0.1729) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.470
current lr 1.00000e-05
Grad=  tensor(0.3199, device='cuda:0')
Epoch: [439][0/391]	Time 0.284 (0.284)	Data 0.214 (0.214)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0040 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [439][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0285 (0.0025) ([0.029]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [439][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.1778 (0.1778) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.400
current lr 1.00000e-05
Grad=  tensor(0.0187, device='cuda:0')
Epoch: [440][0/391]	Time 0.268 (0.268)	Data 0.197 (0.197)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0050 (0.0026) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [440][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [440][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.1807 (0.1807) ([0.181]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.360
current lr 1.00000e-05
Grad=  tensor(0.2450, device='cuda:0')
Epoch: [441][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [441][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [441][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0053 (0.0026) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1821 (0.1821) ([0.182]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-05
Grad=  tensor(0.0064, device='cuda:0')
Epoch: [442][0/391]	Time 0.307 (0.307)	Data 0.236 (0.236)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [442][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [442][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1776 (0.1776) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-05
Grad=  tensor(0.1076, device='cuda:0')
Epoch: [443][0/391]	Time 0.262 (0.262)	Data 0.192 (0.192)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [443][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0056 (0.0023) ([0.006]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1727 (0.1727) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340
current lr 1.00000e-05
Grad=  tensor(0.0124, device='cuda:0')
Epoch: [444][0/391]	Time 0.292 (0.292)	Data 0.221 (0.221)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0052 (0.0024) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [444][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [444][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1748 (0.1748) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-05
Grad=  tensor(0.0662, device='cuda:0')
Epoch: [445][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [445][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [445][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1840 (0.1840) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.230
current lr 1.00000e-05
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [446][0/391]	Time 0.292 (0.292)	Data 0.220 (0.220)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.064 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [446][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [446][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1908 (0.1908) ([0.191]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.290
current lr 1.00000e-05
Grad=  tensor(0.6477, device='cuda:0')
Epoch: [447][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [447][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1769 (0.1769) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-05
Grad=  tensor(0.1740, device='cuda:0')
Epoch: [448][0/391]	Time 0.284 (0.284)	Data 0.214 (0.214)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0037 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [448][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [448][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1665 (0.1665) ([0.167]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-05
Grad=  tensor(0.0452, device='cuda:0')
Epoch: [449][0/391]	Time 0.277 (0.277)	Data 0.207 (0.207)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [449][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1815 (0.1815) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.520
current lr 1.00000e-06
Grad=  tensor(0.1038, device='cuda:0')
Epoch: [450][0/391]	Time 0.296 (0.296)	Data 0.223 (0.223)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [450][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0078 (0.0027) ([0.008]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [450][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1812 (0.1812) ([0.181]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.270
current lr 1.00000e-06
Grad=  tensor(2.6800, device='cuda:0')
Epoch: [451][0/391]	Time 0.289 (0.289)	Data 0.219 (0.219)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [451][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [451][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1752 (0.1752) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.300
current lr 1.00000e-06
Grad=  tensor(0.0070, device='cuda:0')
Epoch: [452][0/391]	Time 0.297 (0.297)	Data 0.226 (0.226)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [452][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0116 (0.0023) ([0.012]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [452][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1610 (0.1610) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.380
current lr 1.00000e-06
Grad=  tensor(0.0037, device='cuda:0')
Epoch: [453][0/391]	Time 0.303 (0.303)	Data 0.233 (0.233)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [453][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [453][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1843 (0.1843) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.310
current lr 1.00000e-06
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [454][0/391]	Time 0.253 (0.253)	Data 0.182 (0.182)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [454][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1771 (0.1771) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.460
current lr 1.00000e-06
Grad=  tensor(0.2589, device='cuda:0')
Epoch: [455][0/391]	Time 0.259 (0.259)	Data 0.189 (0.189)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [455][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [455][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.1654 (0.1654) ([0.165]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-06
Grad=  tensor(0.0841, device='cuda:0')
Epoch: [456][0/391]	Time 0.285 (0.285)	Data 0.213 (0.213)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [456][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [456][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1836 (0.1836) ([0.184]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-06
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [457][0/391]	Time 0.283 (0.283)	Data 0.212 (0.212)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0010 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [457][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1726 (0.1726) ([0.173]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.420
current lr 1.00000e-06
Grad=  tensor(6.6079, device='cuda:0')
Epoch: [458][0/391]	Time 0.299 (0.299)	Data 0.227 (0.227)	Loss 0.0202 (0.0202) ([0.020]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [458][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0044 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [458][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [458][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1741 (0.1741) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-06
Grad=  tensor(0.0154, device='cuda:0')
Epoch: [459][0/391]	Time 0.301 (0.301)	Data 0.230 (0.230)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0010 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [459][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [459][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1763 (0.1763) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.230
current lr 1.00000e-06
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [460][0/391]	Time 0.298 (0.298)	Data 0.226 (0.226)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [460][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1799 (0.1799) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-06
Grad=  tensor(0.0053, device='cuda:0')
Epoch: [461][0/391]	Time 0.290 (0.290)	Data 0.218 (0.218)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [461][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [461][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.1692 (0.1692) ([0.169]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.490
current lr 1.00000e-06
Grad=  tensor(0.0987, device='cuda:0')
Epoch: [462][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [462][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.1593 (0.1593) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.370
current lr 1.00000e-06
Grad=  tensor(0.5349, device='cuda:0')
Epoch: [463][0/391]	Time 0.284 (0.284)	Data 0.213 (0.213)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0022 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [463][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.1915 (0.1915) ([0.191]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.500
current lr 1.00000e-06
Grad=  tensor(0.8136, device='cuda:0')
Epoch: [464][0/391]	Time 0.273 (0.273)	Data 0.203 (0.203)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0168 (0.0028) ([0.017]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [464][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [464][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1947 (0.1947) ([0.195]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340
current lr 1.00000e-06
Grad=  tensor(0.1336, device='cuda:0')
Epoch: [465][0/391]	Time 0.300 (0.300)	Data 0.229 (0.229)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [465][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [465][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1714 (0.1714) ([0.171]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-06
Grad=  tensor(0.2720, device='cuda:0')
Epoch: [466][0/391]	Time 0.308 (0.308)	Data 0.236 (0.236)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0019 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [466][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [466][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1808 (0.1808) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-06
Grad=  tensor(0.8096, device='cuda:0')
Epoch: [467][0/391]	Time 0.315 (0.315)	Data 0.245 (0.245)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.063 (0.066)	Data 0.000 (0.003)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [467][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [467][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.1798 (0.1798) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.420
current lr 1.00000e-06
Grad=  tensor(0.1922, device='cuda:0')
Epoch: [468][0/391]	Time 0.301 (0.301)	Data 0.229 (0.229)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0023 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [468][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [468][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1786 (0.1786) ([0.179]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-06
Grad=  tensor(0.0012, device='cuda:0')
Epoch: [469][0/391]	Time 0.286 (0.286)	Data 0.216 (0.216)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [469][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [469][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.1839 (0.1839) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.360
current lr 1.00000e-06
Grad=  tensor(0.0206, device='cuda:0')
Epoch: [470][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0015 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [470][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [470][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.1830 (0.1830) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.320
current lr 1.00000e-06
Grad=  tensor(0.0170, device='cuda:0')
Epoch: [471][0/391]	Time 0.306 (0.306)	Data 0.234 (0.234)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [471][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [471][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1820 (0.1820) ([0.182]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-06
Grad=  tensor(0.0283, device='cuda:0')
Epoch: [472][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.064 (0.066)	Data 0.000 (0.002)	Loss 0.0020 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [472][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1673 (0.1673) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.400
current lr 1.00000e-06
Grad=  tensor(0.1876, device='cuda:0')
Epoch: [473][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [473][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [473][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0096 (0.0024) ([0.010]+[0.000])	Prec@1 99.219 (99.982)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1739 (0.1739) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-06
Grad=  tensor(0.4528, device='cuda:0')
Epoch: [474][0/391]	Time 0.291 (0.291)	Data 0.221 (0.221)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [474][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1931 (0.1931) ([0.193]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.330
current lr 1.00000e-06
Grad=  tensor(0.2908, device='cuda:0')
Epoch: [475][0/391]	Time 0.260 (0.260)	Data 0.190 (0.190)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0042 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [475][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [475][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0077 (0.0025) ([0.008]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.1802 (0.1802) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.470
current lr 1.00000e-06
Grad=  tensor(0.0126, device='cuda:0')
Epoch: [476][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0012 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [476][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [476][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1701 (0.1701) ([0.170]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340
current lr 1.00000e-06
Grad=  tensor(0.0106, device='cuda:0')
Epoch: [477][0/391]	Time 0.303 (0.303)	Data 0.231 (0.231)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0018 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [477][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1799 (0.1799) ([0.180]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-06
Grad=  tensor(0.0046, device='cuda:0')
Epoch: [478][0/391]	Time 0.305 (0.305)	Data 0.233 (0.233)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0027 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [478][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [478][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.1913 (0.1913) ([0.191]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.290
current lr 1.00000e-06
Grad=  tensor(0.1268, device='cuda:0')
Epoch: [479][0/391]	Time 0.291 (0.291)	Data 0.219 (0.219)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0008 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [479][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [479][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.1663 (0.1663) ([0.166]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.320
current lr 1.00000e-06
Grad=  tensor(0.0338, device='cuda:0')
Epoch: [480][0/391]	Time 0.293 (0.293)	Data 0.221 (0.221)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0023 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [480][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [480][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1629 (0.1629) ([0.163]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.490
current lr 1.00000e-06
Grad=  tensor(0.0062, device='cuda:0')
Epoch: [481][0/391]	Time 0.300 (0.300)	Data 0.228 (0.228)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0033 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [481][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1869 (0.1869) ([0.187]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-06
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [482][0/391]	Time 0.289 (0.289)	Data 0.218 (0.218)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [482][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [482][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1738 (0.1738) ([0.174]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-06
Grad=  tensor(0.4670, device='cuda:0')
Epoch: [483][0/391]	Time 0.303 (0.303)	Data 0.232 (0.232)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [483][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [483][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.1715 (0.1715) ([0.172]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-06
Grad=  tensor(0.0035, device='cuda:0')
Epoch: [484][0/391]	Time 0.300 (0.300)	Data 0.230 (0.230)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.1846 (0.1846) ([0.185]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.300
current lr 1.00000e-06
Grad=  tensor(0.0088, device='cuda:0')
Epoch: [485][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0035 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [485][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1762 (0.1762) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.370
current lr 1.00000e-06
Grad=  tensor(0.1860, device='cuda:0')
Epoch: [486][0/391]	Time 0.281 (0.281)	Data 0.211 (0.211)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0021 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [486][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [486][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.241 (0.241)	Loss 0.1751 (0.1751) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.310
current lr 1.00000e-06
Grad=  tensor(0.1760, device='cuda:0')
Epoch: [487][0/391]	Time 0.283 (0.283)	Data 0.212 (0.212)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [487][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [487][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1770 (0.1770) ([0.177]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-06
Grad=  tensor(0.0862, device='cuda:0')
Epoch: [488][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0009 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [488][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [488][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0023) ([0.005]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1761 (0.1761) ([0.176]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-06
Grad=  tensor(0.8425, device='cuda:0')
Epoch: [489][0/391]	Time 0.297 (0.297)	Data 0.224 (0.224)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0040 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [489][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [489][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0007 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1617 (0.1617) ([0.162]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.310
current lr 1.00000e-06
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [490][0/391]	Time 0.298 (0.298)	Data 0.227 (0.227)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0010 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [490][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [490][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1918 (0.1918) ([0.192]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.360
current lr 1.00000e-06
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [491][0/391]	Time 0.288 (0.288)	Data 0.217 (0.217)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [491][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [491][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.1808 (0.1808) ([0.181]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.410
current lr 1.00000e-06
Grad=  tensor(0.0250, device='cuda:0')
Epoch: [492][0/391]	Time 0.308 (0.308)	Data 0.237 (0.237)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0015 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [492][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [492][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.1753 (0.1753) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.450
current lr 1.00000e-06
Grad=  tensor(0.1371, device='cuda:0')
Epoch: [493][0/391]	Time 0.274 (0.274)	Data 0.204 (0.204)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [493][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [493][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.237 (0.237)	Loss 0.1673 (0.1673) ([0.167]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.430
current lr 1.00000e-06
Grad=  tensor(0.0373, device='cuda:0')
Epoch: [494][0/391]	Time 0.297 (0.297)	Data 0.225 (0.225)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0034 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [494][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [494][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.1781 (0.1781) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.550
current lr 1.00000e-06
Grad=  tensor(0.0421, device='cuda:0')
Epoch: [495][0/391]	Time 0.308 (0.308)	Data 0.235 (0.235)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [495][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0008 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [495][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0006 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.1833 (0.1833) ([0.183]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-06
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [496][0/391]	Time 0.293 (0.293)	Data 0.222 (0.222)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0034 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [496][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [496][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1783 (0.1783) ([0.178]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-06
Grad=  tensor(0.0415, device='cuda:0')
Epoch: [497][0/391]	Time 0.295 (0.295)	Data 0.223 (0.223)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0025 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [497][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0025) ([0.005]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [497][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0006 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.1701 (0.1701) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.440
current lr 1.00000e-06
Grad=  tensor(0.0223, device='cuda:0')
Epoch: [498][0/391]	Time 0.305 (0.305)	Data 0.234 (0.234)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.063 (0.066)	Data 0.000 (0.002)	Loss 0.0012 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [498][200/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [498][300/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.1718 (0.1718) ([0.172]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.380
current lr 1.00000e-06
Grad=  tensor(0.0017, device='cuda:0')
Epoch: [499][0/391]	Time 0.294 (0.294)	Data 0.223 (0.223)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [499][200/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [499][300/391]	Time 0.064 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.1745 (0.1745) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340

 Elapsed time for training  5:26:08.730593

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.578125, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.546875, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.59375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4375, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.609375, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7265625, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7265625, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7109375, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.640625, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.67578125, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.66796875, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.66796875, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7265625, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 0.72265625, 0.73046875, 1.0, 1.0, 1.0, 0.7265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72265625, 0.7421875, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04248046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.71533203125, 0.7080078125, 0.70068359375, 0.70849609375, 0.7041015625, 0.70556640625, 0.7060546875, 0.71484375, 0.70947265625, 0.70947265625, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125, 0.923828125]
Total parameter pruned: 20929973.0 (unstructured) 20737874 (structured)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.1745 (0.1745) ([0.175]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.340
Best accuracy:  94.55
