V0.0.1-_resnet18_Cifar10_lr0.1_l1.0_a0.3_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5487061738967896, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.3523094356060028, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21628862619400024, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.18689115345478058, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.14327552914619446, Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.12737567722797394, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11029236763715744, Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.2971794605255127, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09336047619581223, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11640232801437378, Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.1180206686258316, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09721729904413223, Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.1253737211227417, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11488018184900284, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08681126683950424, Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08893948793411255, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0640970766544342, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.08375772833824158, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06052006036043167, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.017058581113815308, Linear(in_features=512, out_features=100, bias=True): 0.39575180411338806}
current lr 1.00000e-01
Grad=  tensor(44.8525, device='cuda:0')
Epoch: [0][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 5.1381 (5.1381) ([4.628]+[0.510])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 2.6595 (3.0709) ([1.968]+[0.692])	Prec@1 25.781 (17.450)
Epoch: [0][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 2.4994 (2.8317) ([1.877]+[0.623])	Prec@1 30.469 (22.439)
Epoch: [0][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 2.3604 (2.6812) ([1.804]+[0.556])	Prec@1 35.938 (26.017)
Test: [0/79]	Time 0.124 (0.124)	Loss 2.2022 (2.2022) ([1.708]+[0.495])	Prec@1 37.500 (37.500)
 * Prec@1 38.900
current lr 1.00000e-01
Grad=  tensor(1.6983, device='cuda:0')
Epoch: [1][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 2.1431 (2.1431) ([1.649]+[0.495])	Prec@1 41.406 (41.406)
Epoch: [1][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 2.1068 (2.0830) ([1.672]+[0.434])	Prec@1 45.312 (40.022)
Epoch: [1][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 1.7800 (2.0058) ([1.401]+[0.379])	Prec@1 45.312 (41.923)
Epoch: [1][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.7068 (1.9334) ([1.368]+[0.339])	Prec@1 49.219 (43.685)
Test: [0/79]	Time 0.120 (0.120)	Loss 1.7436 (1.7436) ([1.430]+[0.314])	Prec@1 45.312 (45.312)
 * Prec@1 48.580
current lr 1.00000e-01
Grad=  tensor(1.1286, device='cuda:0')
Epoch: [2][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 1.5949 (1.5949) ([1.281]+[0.314])	Prec@1 49.219 (49.219)
Epoch: [2][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.6281 (1.5990) ([1.330]+[0.298])	Prec@1 48.438 (53.218)
Epoch: [2][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.3358 (1.5642) ([1.047]+[0.289])	Prec@1 65.625 (54.027)
Epoch: [2][300/391]	Time 0.043 (0.038)	Data 0.000 (0.001)	Loss 1.4072 (1.5169) ([1.127]+[0.280])	Prec@1 58.594 (55.702)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.4089 (1.4089) ([1.135]+[0.274])	Prec@1 60.938 (60.938)
 * Prec@1 62.380
current lr 1.00000e-01
Grad=  tensor(1.1331, device='cuda:0')
Epoch: [3][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 1.0806 (1.0806) ([0.807]+[0.274])	Prec@1 68.750 (68.750)
Epoch: [3][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.2655 (1.2727) ([0.998]+[0.267])	Prec@1 65.625 (64.171)
Epoch: [3][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.4084 (1.2682) ([1.143]+[0.265])	Prec@1 63.281 (64.385)
Epoch: [3][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.0251 (1.2533) ([0.760]+[0.265])	Prec@1 71.875 (64.781)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.5432 (1.5432) ([1.279]+[0.264])	Prec@1 55.469 (55.469)
 * Prec@1 54.370
current lr 1.00000e-01
Grad=  tensor(1.5657, device='cuda:0')
Epoch: [4][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 1.0096 (1.0096) ([0.745]+[0.264])	Prec@1 71.875 (71.875)
Epoch: [4][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 1.0056 (1.1239) ([0.741]+[0.265])	Prec@1 72.656 (69.508)
Epoch: [4][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 1.0912 (1.1096) ([0.824]+[0.267])	Prec@1 69.531 (70.060)
Epoch: [4][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 1.1023 (1.0961) ([0.834]+[0.268])	Prec@1 72.656 (70.697)
Test: [0/79]	Time 0.122 (0.122)	Loss 1.2035 (1.2035) ([0.933]+[0.270])	Prec@1 68.750 (68.750)
 * Prec@1 66.870
current lr 1.00000e-01
Grad=  tensor(1.4800, device='cuda:0')
Epoch: [5][0/391]	Time 0.159 (0.159)	Data 0.118 (0.118)	Loss 0.9486 (0.9486) ([0.678]+[0.270])	Prec@1 71.875 (71.875)
Epoch: [5][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 1.1956 (1.0047) ([0.923]+[0.273])	Prec@1 67.969 (74.033)
Epoch: [5][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.9037 (0.9885) ([0.632]+[0.272])	Prec@1 81.250 (74.918)
Epoch: [5][300/391]	Time 0.037 (0.036)	Data 0.000 (0.001)	Loss 0.9873 (0.9896) ([0.714]+[0.273])	Prec@1 73.438 (75.091)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.0773 (1.0773) ([0.804]+[0.274])	Prec@1 70.312 (70.312)
 * Prec@1 69.720
current lr 1.00000e-01
Grad=  tensor(1.2864, device='cuda:0')
Epoch: [6][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.9200 (0.9200) ([0.646]+[0.274])	Prec@1 81.250 (81.250)
Epoch: [6][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9249 (0.9273) ([0.650]+[0.275])	Prec@1 75.000 (77.684)
Epoch: [6][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9400 (0.9323) ([0.664]+[0.276])	Prec@1 78.125 (77.495)
Epoch: [6][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9246 (0.9285) ([0.647]+[0.277])	Prec@1 75.000 (77.544)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.3824 (1.3824) ([1.103]+[0.279])	Prec@1 61.719 (61.719)
 * Prec@1 59.890
current lr 1.00000e-01
Grad=  tensor(1.7048, device='cuda:0')
Epoch: [7][0/391]	Time 0.170 (0.170)	Data 0.126 (0.126)	Loss 0.7592 (0.7592) ([0.480]+[0.279])	Prec@1 84.375 (84.375)
Epoch: [7][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.8919 (0.9026) ([0.613]+[0.279])	Prec@1 78.906 (78.674)
Epoch: [7][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.9654 (0.8904) ([0.687]+[0.278])	Prec@1 76.562 (78.933)
Epoch: [7][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9375 (0.8910) ([0.658]+[0.280])	Prec@1 78.906 (78.906)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.1618 (1.1618) ([0.880]+[0.281])	Prec@1 71.875 (71.875)
 * Prec@1 72.380
current lr 1.00000e-01
Grad=  tensor(1.0814, device='cuda:0')
Epoch: [8][0/391]	Time 0.171 (0.171)	Data 0.127 (0.127)	Loss 0.8107 (0.8107) ([0.529]+[0.281])	Prec@1 84.375 (84.375)
Epoch: [8][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7945 (0.8530) ([0.515]+[0.279])	Prec@1 81.250 (80.446)
Epoch: [8][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9427 (0.8540) ([0.664]+[0.279])	Prec@1 76.562 (80.375)
Epoch: [8][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8886 (0.8546) ([0.609]+[0.279])	Prec@1 82.031 (80.264)
Test: [0/79]	Time 0.140 (0.140)	Loss 1.1143 (1.1143) ([0.831]+[0.283])	Prec@1 72.656 (72.656)
 * Prec@1 74.240
current lr 1.00000e-01
Grad=  tensor(1.5699, device='cuda:0')
Epoch: [9][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.7279 (0.7279) ([0.445]+[0.283])	Prec@1 82.812 (82.812)
Epoch: [9][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9150 (0.8426) ([0.634]+[0.281])	Prec@1 78.906 (81.080)
Epoch: [9][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7197 (0.8404) ([0.437]+[0.282])	Prec@1 84.375 (81.199)
Epoch: [9][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9804 (0.8398) ([0.701]+[0.279])	Prec@1 77.344 (81.141)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.2301 (1.2301) ([0.952]+[0.278])	Prec@1 67.969 (67.969)
 * Prec@1 67.400
current lr 1.00000e-01
Grad=  tensor(2.2590, device='cuda:0')
Epoch: [10][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.7401 (0.7401) ([0.462]+[0.278])	Prec@1 85.156 (85.156)
Epoch: [10][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8806 (0.8349) ([0.602]+[0.279])	Prec@1 78.906 (81.002)
Epoch: [10][200/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.7916 (0.8266) ([0.513]+[0.279])	Prec@1 79.688 (81.215)
Epoch: [10][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8094 (0.8224) ([0.532]+[0.277])	Prec@1 82.812 (81.305)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.9269 (0.9269) ([0.651]+[0.276])	Prec@1 75.781 (75.781)
 * Prec@1 77.220
current lr 1.00000e-01
Grad=  tensor(1.8472, device='cuda:0')
Epoch: [11][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.8101 (0.8101) ([0.534]+[0.276])	Prec@1 80.469 (80.469)
Epoch: [11][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7261 (0.7904) ([0.450]+[0.276])	Prec@1 85.156 (82.789)
Epoch: [11][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8410 (0.8062) ([0.564]+[0.277])	Prec@1 79.688 (82.031)
Epoch: [11][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 1.0927 (0.8079) ([0.817]+[0.276])	Prec@1 70.312 (81.940)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.0305 (1.0305) ([0.756]+[0.274])	Prec@1 70.312 (70.312)
 * Prec@1 74.920
current lr 1.00000e-01
Grad=  tensor(2.0247, device='cuda:0')
Epoch: [12][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.8176 (0.8176) ([0.543]+[0.274])	Prec@1 79.688 (79.688)
Epoch: [12][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6407 (0.7668) ([0.368]+[0.273])	Prec@1 88.281 (83.176)
Epoch: [12][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7486 (0.7872) ([0.475]+[0.274])	Prec@1 82.812 (82.486)
Epoch: [12][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7434 (0.7841) ([0.471]+[0.273])	Prec@1 85.938 (82.610)
Test: [0/79]	Time 0.134 (0.134)	Loss 1.0930 (1.0930) ([0.821]+[0.272])	Prec@1 74.219 (74.219)
 * Prec@1 74.330
current lr 1.00000e-01
Grad=  tensor(1.4119, device='cuda:0')
Epoch: [13][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.7060 (0.7060) ([0.434]+[0.272])	Prec@1 87.500 (87.500)
Epoch: [13][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7021 (0.7514) ([0.431]+[0.271])	Prec@1 85.938 (83.540)
Epoch: [13][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7593 (0.7747) ([0.487]+[0.272])	Prec@1 82.812 (82.840)
Epoch: [13][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8535 (0.7746) ([0.583]+[0.270])	Prec@1 78.906 (82.927)
Test: [0/79]	Time 0.122 (0.122)	Loss 1.0048 (1.0048) ([0.735]+[0.270])	Prec@1 80.469 (80.469)
 * Prec@1 76.640
current lr 1.00000e-01
Grad=  tensor(1.9123, device='cuda:0')
Epoch: [14][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.6992 (0.6992) ([0.429]+[0.270])	Prec@1 86.719 (86.719)
Epoch: [14][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7264 (0.7592) ([0.455]+[0.271])	Prec@1 85.156 (82.944)
Epoch: [14][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7682 (0.7651) ([0.497]+[0.271])	Prec@1 81.250 (82.921)
Epoch: [14][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.0272 (0.7674) ([0.756]+[0.272])	Prec@1 72.656 (82.958)
Test: [0/79]	Time 0.120 (0.120)	Loss 1.0138 (1.0138) ([0.744]+[0.270])	Prec@1 79.688 (79.688)
 * Prec@1 76.850
current lr 1.00000e-01
Grad=  tensor(2.2522, device='cuda:0')
Epoch: [15][0/391]	Time 0.159 (0.159)	Data 0.116 (0.116)	Loss 0.7835 (0.7835) ([0.514]+[0.270])	Prec@1 80.469 (80.469)
Epoch: [15][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7328 (0.7355) ([0.463]+[0.270])	Prec@1 85.156 (84.545)
Epoch: [15][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8015 (0.7495) ([0.530]+[0.271])	Prec@1 83.594 (83.862)
Epoch: [15][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7428 (0.7538) ([0.473]+[0.270])	Prec@1 82.031 (83.617)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.7934 (0.7934) ([0.524]+[0.269])	Prec@1 82.812 (82.812)
 * Prec@1 77.180
current lr 1.00000e-01
Grad=  tensor(1.7682, device='cuda:0')
Epoch: [16][0/391]	Time 0.161 (0.161)	Data 0.118 (0.118)	Loss 0.6669 (0.6669) ([0.398]+[0.269])	Prec@1 86.719 (86.719)
Epoch: [16][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6097 (0.7355) ([0.341]+[0.268])	Prec@1 89.062 (83.957)
Epoch: [16][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8390 (0.7464) ([0.570]+[0.269])	Prec@1 84.375 (83.671)
Epoch: [16][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8062 (0.7475) ([0.538]+[0.269])	Prec@1 80.469 (83.757)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9062 (0.9062) ([0.637]+[0.269])	Prec@1 85.156 (85.156)
 * Prec@1 78.210
current lr 1.00000e-01
Grad=  tensor(2.1805, device='cuda:0')
Epoch: [17][0/391]	Time 0.160 (0.160)	Data 0.118 (0.118)	Loss 0.8534 (0.8534) ([0.584]+[0.269])	Prec@1 79.688 (79.688)
Epoch: [17][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6780 (0.7418) ([0.409]+[0.269])	Prec@1 85.938 (83.911)
Epoch: [17][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7305 (0.7422) ([0.462]+[0.268])	Prec@1 85.938 (83.726)
Epoch: [17][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7133 (0.7409) ([0.446]+[0.268])	Prec@1 84.375 (83.747)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.9405 (0.9405) ([0.672]+[0.269])	Prec@1 78.125 (78.125)
 * Prec@1 75.970
current lr 1.00000e-01
Grad=  tensor(1.3799, device='cuda:0')
Epoch: [18][0/391]	Time 0.161 (0.161)	Data 0.118 (0.118)	Loss 0.6424 (0.6424) ([0.374]+[0.269])	Prec@1 87.500 (87.500)
Epoch: [18][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7838 (0.7314) ([0.516]+[0.268])	Prec@1 82.812 (84.127)
Epoch: [18][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7378 (0.7335) ([0.471]+[0.267])	Prec@1 85.938 (84.029)
Epoch: [18][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7137 (0.7436) ([0.446]+[0.268])	Prec@1 86.719 (83.762)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7063 (0.7063) ([0.440]+[0.266])	Prec@1 81.250 (81.250)
 * Prec@1 81.180
current lr 1.00000e-01
Grad=  tensor(2.6904, device='cuda:0')
Epoch: [19][0/391]	Time 0.161 (0.161)	Data 0.119 (0.119)	Loss 0.7718 (0.7718) ([0.505]+[0.266])	Prec@1 78.906 (78.906)
Epoch: [19][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6785 (0.7244) ([0.411]+[0.268])	Prec@1 83.594 (84.429)
Epoch: [19][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7409 (0.7236) ([0.474]+[0.267])	Prec@1 78.906 (84.344)
Epoch: [19][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8628 (0.7256) ([0.596]+[0.267])	Prec@1 81.250 (84.263)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.9956 (0.9956) ([0.730]+[0.266])	Prec@1 76.562 (76.562)
 * Prec@1 77.250
current lr 1.00000e-01
Grad=  tensor(1.7038, device='cuda:0')
Epoch: [20][0/391]	Time 0.173 (0.173)	Data 0.130 (0.130)	Loss 0.6537 (0.6537) ([0.388]+[0.266])	Prec@1 88.281 (88.281)
Epoch: [20][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7713 (0.7249) ([0.505]+[0.266])	Prec@1 78.906 (84.220)
Epoch: [20][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7189 (0.7279) ([0.452]+[0.267])	Prec@1 84.375 (84.251)
Epoch: [20][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8567 (0.7254) ([0.591]+[0.265])	Prec@1 84.375 (84.422)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.8855 (0.8855) ([0.620]+[0.265])	Prec@1 82.031 (82.031)
 * Prec@1 78.650
current lr 1.00000e-01
Grad=  tensor(1.6283, device='cuda:0')
Epoch: [21][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.6861 (0.6861) ([0.421]+[0.265])	Prec@1 85.938 (85.938)
Epoch: [21][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8220 (0.7172) ([0.556]+[0.266])	Prec@1 80.469 (84.352)
Epoch: [21][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7770 (0.7075) ([0.515]+[0.262])	Prec@1 83.594 (84.764)
Epoch: [21][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7076 (0.7173) ([0.443]+[0.265])	Prec@1 85.938 (84.541)
Test: [0/79]	Time 0.133 (0.133)	Loss 1.1051 (1.1051) ([0.842]+[0.263])	Prec@1 73.438 (73.438)
 * Prec@1 76.750
current lr 1.00000e-01
Grad=  tensor(2.4590, device='cuda:0')
Epoch: [22][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.7863 (0.7863) ([0.524]+[0.263])	Prec@1 81.250 (81.250)
Epoch: [22][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7865 (0.7124) ([0.524]+[0.263])	Prec@1 81.250 (84.793)
Epoch: [22][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7401 (0.7077) ([0.477]+[0.264])	Prec@1 88.281 (84.861)
Epoch: [22][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6055 (0.7145) ([0.341]+[0.265])	Prec@1 87.500 (84.645)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.8051 (0.8051) ([0.541]+[0.264])	Prec@1 81.250 (81.250)
 * Prec@1 81.310
current lr 1.00000e-01
Grad=  tensor(1.8568, device='cuda:0')
Epoch: [23][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6938 (0.6938) ([0.430]+[0.264])	Prec@1 85.156 (85.156)
Epoch: [23][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6259 (0.6883) ([0.362]+[0.263])	Prec@1 89.844 (85.558)
Epoch: [23][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7165 (0.6998) ([0.452]+[0.264])	Prec@1 85.156 (84.966)
Epoch: [23][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8490 (0.7074) ([0.584]+[0.265])	Prec@1 78.125 (84.816)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.0682 (1.0682) ([0.804]+[0.264])	Prec@1 76.562 (76.562)
 * Prec@1 78.460
current lr 1.00000e-01
Grad=  tensor(2.0151, device='cuda:0')
Epoch: [24][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6945 (0.6945) ([0.430]+[0.264])	Prec@1 85.938 (85.938)
Epoch: [24][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6829 (0.6963) ([0.420]+[0.263])	Prec@1 83.594 (84.994)
Epoch: [24][200/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.6324 (0.7005) ([0.369]+[0.264])	Prec@1 89.062 (84.985)
Epoch: [24][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7728 (0.7047) ([0.508]+[0.264])	Prec@1 80.469 (84.954)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8251 (0.8251) ([0.561]+[0.264])	Prec@1 75.781 (75.781)
 * Prec@1 81.360
current lr 1.00000e-01
Grad=  tensor(2.1671, device='cuda:0')
Epoch: [25][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.7594 (0.7594) ([0.496]+[0.264])	Prec@1 85.938 (85.938)
Epoch: [25][100/391]	Time 0.043 (0.039)	Data 0.000 (0.001)	Loss 0.5442 (0.7006) ([0.281]+[0.263])	Prec@1 89.844 (85.025)
Epoch: [25][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6537 (0.6966) ([0.390]+[0.264])	Prec@1 87.500 (85.347)
Epoch: [25][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7236 (0.6961) ([0.461]+[0.262])	Prec@1 85.156 (85.400)
Test: [0/79]	Time 0.134 (0.134)	Loss 1.2350 (1.2350) ([0.973]+[0.262])	Prec@1 72.656 (72.656)
 * Prec@1 74.370
current lr 1.00000e-01
Grad=  tensor(1.6202, device='cuda:0')
Epoch: [26][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.6400 (0.6400) ([0.378]+[0.262])	Prec@1 84.375 (84.375)
Epoch: [26][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7151 (0.6907) ([0.451]+[0.264])	Prec@1 85.938 (85.760)
Epoch: [26][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6108 (0.6972) ([0.347]+[0.264])	Prec@1 87.500 (85.475)
Epoch: [26][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7961 (0.6973) ([0.532]+[0.264])	Prec@1 81.250 (85.351)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.8418 (0.8418) ([0.579]+[0.263])	Prec@1 78.906 (78.906)
 * Prec@1 78.510
current lr 1.00000e-01
Grad=  tensor(2.8951, device='cuda:0')
Epoch: [27][0/391]	Time 0.173 (0.173)	Data 0.131 (0.131)	Loss 0.7806 (0.7806) ([0.517]+[0.263])	Prec@1 81.250 (81.250)
Epoch: [27][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7729 (0.6900) ([0.509]+[0.264])	Prec@1 82.812 (85.512)
Epoch: [27][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6625 (0.7011) ([0.398]+[0.264])	Prec@1 87.500 (85.160)
Epoch: [27][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5798 (0.7003) ([0.316]+[0.264])	Prec@1 86.719 (85.216)
Test: [0/79]	Time 0.125 (0.125)	Loss 1.0663 (1.0663) ([0.802]+[0.264])	Prec@1 74.219 (74.219)
 * Prec@1 77.840
current lr 1.00000e-01
Grad=  tensor(2.0362, device='cuda:0')
Epoch: [28][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.7745 (0.7745) ([0.510]+[0.264])	Prec@1 84.375 (84.375)
Epoch: [28][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8649 (0.6881) ([0.601]+[0.264])	Prec@1 81.250 (85.605)
Epoch: [28][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5925 (0.6920) ([0.328]+[0.264])	Prec@1 89.844 (85.522)
Epoch: [28][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7449 (0.6932) ([0.481]+[0.264])	Prec@1 85.156 (85.501)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.8254 (0.8254) ([0.562]+[0.263])	Prec@1 82.812 (82.812)
 * Prec@1 78.960
current lr 1.00000e-01
Grad=  tensor(1.3561, device='cuda:0')
Epoch: [29][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.5866 (0.5866) ([0.323]+[0.263])	Prec@1 88.281 (88.281)
Epoch: [29][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6429 (0.6786) ([0.380]+[0.263])	Prec@1 86.719 (85.821)
Epoch: [29][200/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss 0.7327 (0.6875) ([0.469]+[0.264])	Prec@1 82.812 (85.463)
Epoch: [29][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6787 (0.6886) ([0.416]+[0.263])	Prec@1 85.156 (85.572)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.8616 (0.8616) ([0.598]+[0.263])	Prec@1 81.250 (81.250)
 * Prec@1 78.980
current lr 1.00000e-01
Grad=  tensor(1.7374, device='cuda:0')
Epoch: [30][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.7578 (0.7578) ([0.494]+[0.263])	Prec@1 84.375 (84.375)
Epoch: [30][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6149 (0.7039) ([0.350]+[0.264])	Prec@1 88.281 (85.125)
Epoch: [30][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6225 (0.6952) ([0.359]+[0.263])	Prec@1 87.500 (85.351)
Epoch: [30][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7589 (0.6905) ([0.498]+[0.261])	Prec@1 84.375 (85.603)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.8433 (0.8433) ([0.582]+[0.262])	Prec@1 80.469 (80.469)
 * Prec@1 81.420
current lr 1.00000e-01
Grad=  tensor(1.5485, device='cuda:0')
Epoch: [31][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.6449 (0.6449) ([0.383]+[0.262])	Prec@1 87.500 (87.500)
Epoch: [31][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6129 (0.6841) ([0.351]+[0.261])	Prec@1 89.062 (85.883)
Epoch: [31][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6323 (0.6877) ([0.369]+[0.263])	Prec@1 85.156 (85.864)
Epoch: [31][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8340 (0.6870) ([0.571]+[0.263])	Prec@1 83.594 (85.745)
Test: [0/79]	Time 0.123 (0.123)	Loss 1.2505 (1.2505) ([0.988]+[0.263])	Prec@1 71.094 (71.094)
 * Prec@1 68.850
current lr 1.00000e-01
Grad=  tensor(1.8870, device='cuda:0')
Epoch: [32][0/391]	Time 0.159 (0.159)	Data 0.116 (0.116)	Loss 0.6895 (0.6895) ([0.427]+[0.263])	Prec@1 89.062 (89.062)
Epoch: [32][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5734 (0.6858) ([0.311]+[0.262])	Prec@1 91.406 (85.427)
Epoch: [32][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6733 (0.6905) ([0.409]+[0.264])	Prec@1 86.719 (85.459)
Epoch: [32][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6604 (0.6895) ([0.398]+[0.263])	Prec@1 85.156 (85.470)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.1928 (1.1928) ([0.930]+[0.263])	Prec@1 75.000 (75.000)
 * Prec@1 73.860
current lr 1.00000e-01
Grad=  tensor(1.6679, device='cuda:0')
Epoch: [33][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.5647 (0.5647) ([0.302]+[0.263])	Prec@1 89.844 (89.844)
Epoch: [33][100/391]	Time 0.036 (0.039)	Data 0.000 (0.001)	Loss 0.6204 (0.6912) ([0.357]+[0.264])	Prec@1 89.844 (85.365)
Epoch: [33][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7238 (0.6869) ([0.461]+[0.263])	Prec@1 85.156 (85.568)
Epoch: [33][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7168 (0.6800) ([0.454]+[0.263])	Prec@1 82.812 (85.706)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.8647 (0.8647) ([0.603]+[0.262])	Prec@1 82.812 (82.812)
 * Prec@1 81.370
current lr 1.00000e-01
Grad=  tensor(1.4644, device='cuda:0')
Epoch: [34][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.6122 (0.6122) ([0.350]+[0.262])	Prec@1 88.281 (88.281)
Epoch: [34][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6368 (0.6737) ([0.376]+[0.260])	Prec@1 85.938 (85.837)
Epoch: [34][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6722 (0.6807) ([0.410]+[0.263])	Prec@1 83.594 (85.630)
Epoch: [34][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7264 (0.6838) ([0.464]+[0.263])	Prec@1 84.375 (85.574)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7639 (0.7639) ([0.501]+[0.263])	Prec@1 79.688 (79.688)
 * Prec@1 81.850
current lr 1.00000e-01
Grad=  tensor(1.7029, device='cuda:0')
Epoch: [35][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.6167 (0.6167) ([0.354]+[0.263])	Prec@1 86.719 (86.719)
Epoch: [35][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7002 (0.6737) ([0.438]+[0.262])	Prec@1 85.938 (86.154)
Epoch: [35][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6816 (0.6813) ([0.420]+[0.262])	Prec@1 85.938 (85.879)
Epoch: [35][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6427 (0.6745) ([0.381]+[0.261])	Prec@1 87.500 (86.026)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.7942 (0.7942) ([0.531]+[0.263])	Prec@1 83.594 (83.594)
 * Prec@1 80.120
current lr 1.00000e-01
Grad=  tensor(1.5576, device='cuda:0')
Epoch: [36][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.5202 (0.5202) ([0.257]+[0.263])	Prec@1 91.406 (91.406)
Epoch: [36][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7102 (0.6838) ([0.447]+[0.263])	Prec@1 83.594 (85.729)
Epoch: [36][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7413 (0.6850) ([0.478]+[0.263])	Prec@1 82.812 (85.693)
Epoch: [36][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8588 (0.6826) ([0.596]+[0.263])	Prec@1 78.125 (85.849)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.9084 (0.9084) ([0.647]+[0.261])	Prec@1 75.781 (75.781)
 * Prec@1 76.540
current lr 1.00000e-01
Grad=  tensor(2.1159, device='cuda:0')
Epoch: [37][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.6944 (0.6944) ([0.433]+[0.261])	Prec@1 86.719 (86.719)
Epoch: [37][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6177 (0.6741) ([0.356]+[0.262])	Prec@1 89.062 (85.999)
Epoch: [37][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7873 (0.6791) ([0.526]+[0.261])	Prec@1 82.031 (85.945)
Epoch: [37][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8155 (0.6760) ([0.554]+[0.262])	Prec@1 80.469 (86.039)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8776 (0.8776) ([0.617]+[0.261])	Prec@1 81.250 (81.250)
 * Prec@1 80.150
current lr 1.00000e-01
Grad=  tensor(2.6385, device='cuda:0')
Epoch: [38][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7506 (0.7506) ([0.490]+[0.261])	Prec@1 85.156 (85.156)
Epoch: [38][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5233 (0.6745) ([0.263]+[0.260])	Prec@1 92.188 (85.791)
Epoch: [38][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6750 (0.6761) ([0.415]+[0.260])	Prec@1 85.156 (85.766)
Epoch: [38][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5792 (0.6834) ([0.318]+[0.261])	Prec@1 92.188 (85.587)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.9617 (0.9617) ([0.701]+[0.261])	Prec@1 78.125 (78.125)
 * Prec@1 80.250
current lr 1.00000e-01
Grad=  tensor(2.0881, device='cuda:0')
Epoch: [39][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.6995 (0.6995) ([0.438]+[0.261])	Prec@1 85.156 (85.156)
Epoch: [39][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6715 (0.6589) ([0.411]+[0.261])	Prec@1 85.938 (86.448)
Epoch: [39][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7426 (0.6673) ([0.482]+[0.261])	Prec@1 83.594 (86.112)
Epoch: [39][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5406 (0.6776) ([0.279]+[0.262])	Prec@1 93.750 (85.909)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.7129 (0.7129) ([0.452]+[0.261])	Prec@1 86.719 (86.719)
 * Prec@1 82.130
current lr 1.00000e-01
Grad=  tensor(1.5322, device='cuda:0')
Epoch: [40][0/391]	Time 0.161 (0.161)	Data 0.119 (0.119)	Loss 0.6373 (0.6373) ([0.377]+[0.261])	Prec@1 86.719 (86.719)
Epoch: [40][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5894 (0.6701) ([0.328]+[0.262])	Prec@1 86.719 (85.953)
Epoch: [40][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6468 (0.6666) ([0.386]+[0.261])	Prec@1 85.156 (86.233)
Epoch: [40][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5098 (0.6720) ([0.249]+[0.260])	Prec@1 90.625 (86.062)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.7567 (0.7567) ([0.497]+[0.259])	Prec@1 84.375 (84.375)
 * Prec@1 83.300
current lr 1.00000e-01
Grad=  tensor(2.4672, device='cuda:0')
Epoch: [41][0/391]	Time 0.168 (0.168)	Data 0.124 (0.124)	Loss 0.6365 (0.6365) ([0.377]+[0.259])	Prec@1 85.938 (85.938)
Epoch: [41][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6459 (0.6647) ([0.385]+[0.261])	Prec@1 84.375 (86.309)
Epoch: [41][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6261 (0.6683) ([0.365]+[0.261])	Prec@1 89.062 (86.225)
Epoch: [41][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7162 (0.6728) ([0.456]+[0.260])	Prec@1 82.031 (85.989)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.8492 (0.8492) ([0.589]+[0.260])	Prec@1 78.125 (78.125)
 * Prec@1 78.860
current lr 1.00000e-01
Grad=  tensor(1.8199, device='cuda:0')
Epoch: [42][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6048 (0.6048) ([0.345]+[0.260])	Prec@1 88.281 (88.281)
Epoch: [42][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6371 (0.6489) ([0.377]+[0.260])	Prec@1 85.156 (86.773)
Epoch: [42][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7255 (0.6640) ([0.465]+[0.261])	Prec@1 83.594 (86.186)
Epoch: [42][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6277 (0.6716) ([0.367]+[0.261])	Prec@1 86.719 (85.940)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.7795 (0.7795) ([0.519]+[0.261])	Prec@1 79.688 (79.688)
 * Prec@1 76.080
current lr 1.00000e-01
Grad=  tensor(2.1238, device='cuda:0')
Epoch: [43][0/391]	Time 0.160 (0.160)	Data 0.116 (0.116)	Loss 0.6396 (0.6396) ([0.379]+[0.261])	Prec@1 86.719 (86.719)
Epoch: [43][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6843 (0.6680) ([0.423]+[0.261])	Prec@1 84.375 (85.899)
Epoch: [43][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6685 (0.6722) ([0.408]+[0.261])	Prec@1 85.156 (86.039)
Epoch: [43][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6991 (0.6736) ([0.439]+[0.260])	Prec@1 85.156 (86.041)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.9510 (0.9510) ([0.691]+[0.260])	Prec@1 78.125 (78.125)
 * Prec@1 78.610
current lr 1.00000e-01
Grad=  tensor(1.4839, device='cuda:0')
Epoch: [44][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.6008 (0.6008) ([0.340]+[0.260])	Prec@1 88.281 (88.281)
Epoch: [44][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6538 (0.6675) ([0.392]+[0.261])	Prec@1 85.938 (86.340)
Epoch: [44][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6361 (0.6641) ([0.376]+[0.260])	Prec@1 85.938 (86.330)
Epoch: [44][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7295 (0.6735) ([0.468]+[0.262])	Prec@1 85.156 (86.021)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.6186 (0.6186) ([0.358]+[0.260])	Prec@1 87.500 (87.500)
 * Prec@1 83.400
current lr 1.00000e-01
Grad=  tensor(1.7262, device='cuda:0')
Epoch: [45][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.5935 (0.5935) ([0.333]+[0.260])	Prec@1 89.844 (89.844)
Epoch: [45][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5819 (0.6644) ([0.321]+[0.261])	Prec@1 90.625 (86.402)
Epoch: [45][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7183 (0.6680) ([0.456]+[0.262])	Prec@1 85.938 (86.256)
Epoch: [45][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7137 (0.6681) ([0.453]+[0.261])	Prec@1 87.500 (86.280)
Test: [0/79]	Time 0.122 (0.122)	Loss 1.0892 (1.0892) ([0.829]+[0.260])	Prec@1 78.125 (78.125)
 * Prec@1 77.770
current lr 1.00000e-01
Grad=  tensor(1.7251, device='cuda:0')
Epoch: [46][0/391]	Time 0.170 (0.170)	Data 0.120 (0.120)	Loss 0.5787 (0.5787) ([0.319]+[0.260])	Prec@1 91.406 (91.406)
Epoch: [46][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6185 (0.6522) ([0.360]+[0.259])	Prec@1 89.062 (86.858)
Epoch: [46][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5899 (0.6525) ([0.331]+[0.259])	Prec@1 89.844 (86.707)
Epoch: [46][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7586 (0.6627) ([0.498]+[0.260])	Prec@1 84.375 (86.444)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.8722 (0.8722) ([0.612]+[0.260])	Prec@1 82.031 (82.031)
 * Prec@1 78.730
current lr 1.00000e-01
Grad=  tensor(2.8029, device='cuda:0')
Epoch: [47][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.7776 (0.7776) ([0.517]+[0.260])	Prec@1 83.594 (83.594)
Epoch: [47][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6633 (0.6703) ([0.403]+[0.260])	Prec@1 85.156 (86.247)
Epoch: [47][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6392 (0.6711) ([0.379]+[0.260])	Prec@1 85.156 (86.074)
Epoch: [47][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6051 (0.6744) ([0.345]+[0.260])	Prec@1 88.281 (85.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8541 (0.8541) ([0.593]+[0.261])	Prec@1 78.125 (78.125)
 * Prec@1 78.690
current lr 1.00000e-01
Grad=  tensor(1.7098, device='cuda:0')
Epoch: [48][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6298 (0.6298) ([0.369]+[0.261])	Prec@1 89.062 (89.062)
Epoch: [48][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7190 (0.6573) ([0.458]+[0.261])	Prec@1 82.031 (86.347)
Epoch: [48][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7957 (0.6707) ([0.534]+[0.262])	Prec@1 82.031 (85.961)
Epoch: [48][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6343 (0.6750) ([0.373]+[0.261])	Prec@1 90.625 (85.865)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.6744 (0.6744) ([0.413]+[0.261])	Prec@1 81.250 (81.250)
 * Prec@1 84.280
current lr 1.00000e-01
Grad=  tensor(2.0640, device='cuda:0')
Epoch: [49][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.6990 (0.6990) ([0.438]+[0.261])	Prec@1 85.156 (85.156)
Epoch: [49][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.5883 (0.6747) ([0.327]+[0.261])	Prec@1 88.281 (85.968)
Epoch: [49][200/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.7703 (0.6706) ([0.509]+[0.261])	Prec@1 82.031 (86.062)
Epoch: [49][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6338 (0.6696) ([0.374]+[0.260])	Prec@1 86.719 (86.174)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.7458 (0.7458) ([0.486]+[0.260])	Prec@1 84.375 (84.375)
 * Prec@1 83.530
current lr 1.00000e-01
Grad=  tensor(1.5534, device='cuda:0')
Epoch: [50][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.6750 (0.6750) ([0.415]+[0.260])	Prec@1 85.938 (85.938)
Epoch: [50][100/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6604 (0.6527) ([0.400]+[0.260])	Prec@1 82.031 (86.610)
Epoch: [50][200/391]	Time 0.040 (0.037)	Data 0.000 (0.001)	Loss 0.7035 (0.6556) ([0.443]+[0.260])	Prec@1 86.719 (86.602)
Epoch: [50][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7676 (0.6661) ([0.506]+[0.261])	Prec@1 85.156 (86.202)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.7721 (0.7721) ([0.512]+[0.260])	Prec@1 80.469 (80.469)
 * Prec@1 81.990
current lr 1.00000e-01
Grad=  tensor(2.6599, device='cuda:0')
Epoch: [51][0/391]	Time 0.169 (0.169)	Data 0.121 (0.121)	Loss 0.7577 (0.7577) ([0.497]+[0.260])	Prec@1 85.156 (85.156)
Epoch: [51][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7986 (0.6681) ([0.538]+[0.261])	Prec@1 81.250 (86.355)
Epoch: [51][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5895 (0.6756) ([0.328]+[0.262])	Prec@1 86.719 (86.124)
Epoch: [51][300/391]	Time 0.039 (0.038)	Data 0.000 (0.001)	Loss 0.7372 (0.6718) ([0.475]+[0.263])	Prec@1 83.594 (86.189)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.2039 (1.2039) ([0.942]+[0.262])	Prec@1 72.656 (72.656)
 * Prec@1 74.200
current lr 1.00000e-01
Grad=  tensor(1.4256, device='cuda:0')
Epoch: [52][0/391]	Time 0.174 (0.174)	Data 0.131 (0.131)	Loss 0.5604 (0.5604) ([0.298]+[0.262])	Prec@1 89.062 (89.062)
Epoch: [52][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5880 (0.6520) ([0.327]+[0.261])	Prec@1 89.062 (86.626)
Epoch: [52][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6492 (0.6597) ([0.388]+[0.261])	Prec@1 85.156 (86.318)
Epoch: [52][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.8904 (0.6709) ([0.629]+[0.261])	Prec@1 82.031 (86.148)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.7238 (0.7238) ([0.465]+[0.259])	Prec@1 84.375 (84.375)
 * Prec@1 82.400
current lr 1.00000e-01
Grad=  tensor(1.6334, device='cuda:0')
Epoch: [53][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.5624 (0.5624) ([0.303]+[0.259])	Prec@1 92.188 (92.188)
Epoch: [53][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5572 (0.6405) ([0.299]+[0.259])	Prec@1 92.969 (87.291)
Epoch: [53][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7046 (0.6556) ([0.445]+[0.260])	Prec@1 82.031 (86.625)
Epoch: [53][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7392 (0.6648) ([0.479]+[0.260])	Prec@1 82.812 (86.381)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.9621 (0.9621) ([0.703]+[0.259])	Prec@1 79.688 (79.688)
 * Prec@1 78.230
current lr 1.00000e-01
Grad=  tensor(2.2755, device='cuda:0')
Epoch: [54][0/391]	Time 0.159 (0.159)	Data 0.118 (0.118)	Loss 0.6741 (0.6741) ([0.415]+[0.259])	Prec@1 84.375 (84.375)
Epoch: [54][100/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.7429 (0.6572) ([0.483]+[0.260])	Prec@1 82.812 (86.564)
Epoch: [54][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.6869 (0.6683) ([0.428]+[0.259])	Prec@1 82.031 (86.178)
Epoch: [54][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6514 (0.6726) ([0.393]+[0.258])	Prec@1 86.719 (86.119)
Test: [0/79]	Time 0.118 (0.118)	Loss 1.0434 (1.0434) ([0.786]+[0.257])	Prec@1 78.906 (78.906)
 * Prec@1 74.490
current lr 1.00000e-01
Grad=  tensor(2.2658, device='cuda:0')
Epoch: [55][0/391]	Time 0.161 (0.161)	Data 0.119 (0.119)	Loss 0.6794 (0.6794) ([0.422]+[0.257])	Prec@1 81.250 (81.250)
Epoch: [55][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6539 (0.6667) ([0.395]+[0.259])	Prec@1 85.938 (86.200)
Epoch: [55][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7302 (0.6708) ([0.470]+[0.260])	Prec@1 86.719 (85.922)
Epoch: [55][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6696 (0.6682) ([0.410]+[0.260])	Prec@1 85.156 (86.036)
Test: [0/79]	Time 0.122 (0.122)	Loss 1.5215 (1.5215) ([1.262]+[0.260])	Prec@1 66.406 (66.406)
 * Prec@1 67.400
current lr 1.00000e-01
Grad=  tensor(2.4016, device='cuda:0')
Epoch: [56][0/391]	Time 0.170 (0.170)	Data 0.119 (0.119)	Loss 0.6386 (0.6386) ([0.379]+[0.260])	Prec@1 85.938 (85.938)
Epoch: [56][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5668 (0.6444) ([0.310]+[0.257])	Prec@1 89.062 (86.866)
Epoch: [56][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7464 (0.6461) ([0.489]+[0.258])	Prec@1 83.594 (86.812)
Epoch: [56][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.5436 (0.6561) ([0.284]+[0.259])	Prec@1 89.844 (86.477)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.8175 (0.8175) ([0.559]+[0.258])	Prec@1 78.906 (78.906)
 * Prec@1 82.510
current lr 1.00000e-01
Grad=  tensor(2.3948, device='cuda:0')
Epoch: [57][0/391]	Time 0.159 (0.159)	Data 0.116 (0.116)	Loss 0.7283 (0.7283) ([0.470]+[0.258])	Prec@1 85.938 (85.938)
Epoch: [57][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.7803 (0.6568) ([0.523]+[0.258])	Prec@1 85.938 (86.510)
Epoch: [57][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6716 (0.6595) ([0.414]+[0.258])	Prec@1 85.156 (86.427)
Epoch: [57][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6154 (0.6698) ([0.356]+[0.259])	Prec@1 85.938 (86.080)
Test: [0/79]	Time 0.134 (0.134)	Loss 0.8260 (0.8260) ([0.566]+[0.260])	Prec@1 81.250 (81.250)
 * Prec@1 78.070
current lr 1.00000e-01
Grad=  tensor(2.5666, device='cuda:0')
Epoch: [58][0/391]	Time 0.189 (0.189)	Data 0.145 (0.145)	Loss 0.7785 (0.7785) ([0.519]+[0.260])	Prec@1 81.250 (81.250)
Epoch: [58][100/391]	Time 0.043 (0.040)	Data 0.000 (0.002)	Loss 0.6199 (0.6545) ([0.362]+[0.258])	Prec@1 85.938 (86.564)
Epoch: [58][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6140 (0.6566) ([0.357]+[0.257])	Prec@1 88.281 (86.505)
Epoch: [58][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.7936 (0.6618) ([0.537]+[0.257])	Prec@1 83.594 (86.270)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7683 (0.7683) ([0.511]+[0.257])	Prec@1 85.156 (85.156)
 * Prec@1 83.190
current lr 1.00000e-01
Grad=  tensor(2.3459, device='cuda:0')
Epoch: [59][0/391]	Time 0.162 (0.162)	Data 0.118 (0.118)	Loss 0.7512 (0.7512) ([0.494]+[0.257])	Prec@1 83.594 (83.594)
Epoch: [59][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.5782 (0.6638) ([0.321]+[0.257])	Prec@1 90.625 (86.208)
Epoch: [59][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6244 (0.6555) ([0.368]+[0.256])	Prec@1 85.938 (86.458)
Epoch: [59][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6745 (0.6658) ([0.417]+[0.257])	Prec@1 88.281 (86.093)
Test: [0/79]	Time 0.118 (0.118)	Loss 1.0454 (1.0454) ([0.788]+[0.257])	Prec@1 75.000 (75.000)
 * Prec@1 76.780
current lr 1.00000e-01
Grad=  tensor(2.1131, device='cuda:0')
Epoch: [60][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6644 (0.6644) ([0.407]+[0.257])	Prec@1 86.719 (86.719)
Epoch: [60][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.7138 (0.6628) ([0.456]+[0.257])	Prec@1 81.250 (86.131)
Epoch: [60][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7159 (0.6636) ([0.459]+[0.257])	Prec@1 86.719 (86.221)
Epoch: [60][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6139 (0.6718) ([0.356]+[0.258])	Prec@1 88.281 (86.028)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.8091 (0.8091) ([0.552]+[0.257])	Prec@1 84.375 (84.375)
 * Prec@1 82.380
current lr 1.00000e-01
Grad=  tensor(1.4988, device='cuda:0')
Epoch: [61][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.5505 (0.5505) ([0.293]+[0.257])	Prec@1 92.188 (92.188)
Epoch: [61][100/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss 0.7303 (0.6519) ([0.472]+[0.258])	Prec@1 88.281 (86.641)
Epoch: [61][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6205 (0.6598) ([0.362]+[0.259])	Prec@1 87.500 (86.291)
Epoch: [61][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7492 (0.6621) ([0.492]+[0.257])	Prec@1 82.812 (86.215)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.8622 (0.8622) ([0.606]+[0.257])	Prec@1 79.688 (79.688)
 * Prec@1 80.210
current lr 1.00000e-01
Grad=  tensor(2.3742, device='cuda:0')
Epoch: [62][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.6880 (0.6880) ([0.431]+[0.257])	Prec@1 84.375 (84.375)
Epoch: [62][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5458 (0.6523) ([0.289]+[0.256])	Prec@1 91.406 (86.804)
Epoch: [62][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6267 (0.6595) ([0.369]+[0.257])	Prec@1 86.719 (86.427)
Epoch: [62][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.7525 (0.6618) ([0.495]+[0.258])	Prec@1 85.156 (86.342)
Test: [0/79]	Time 0.133 (0.133)	Loss 1.1001 (1.1001) ([0.842]+[0.258])	Prec@1 72.656 (72.656)
 * Prec@1 74.900
current lr 1.00000e-01
Grad=  tensor(2.0494, device='cuda:0')
Epoch: [63][0/391]	Time 0.187 (0.187)	Data 0.143 (0.143)	Loss 0.7039 (0.7039) ([0.446]+[0.258])	Prec@1 83.594 (83.594)
Epoch: [63][100/391]	Time 0.037 (0.040)	Data 0.000 (0.002)	Loss 0.6456 (0.6651) ([0.387]+[0.259])	Prec@1 85.156 (86.162)
Epoch: [63][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6871 (0.6599) ([0.430]+[0.257])	Prec@1 85.156 (86.252)
Epoch: [63][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss 0.6798 (0.6630) ([0.422]+[0.258])	Prec@1 86.719 (86.246)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.8710 (0.8710) ([0.613]+[0.258])	Prec@1 82.031 (82.031)
 * Prec@1 83.560
current lr 1.00000e-01
Grad=  tensor(1.5199, device='cuda:0')
Epoch: [64][0/391]	Time 0.177 (0.177)	Data 0.135 (0.135)	Loss 0.5369 (0.5369) ([0.279]+[0.258])	Prec@1 89.062 (89.062)
Epoch: [64][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7970 (0.6669) ([0.540]+[0.257])	Prec@1 83.594 (86.139)
Epoch: [64][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7112 (0.6656) ([0.452]+[0.259])	Prec@1 85.156 (86.241)
Epoch: [64][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.7752 (0.6657) ([0.517]+[0.258])	Prec@1 82.812 (86.278)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.8579 (0.8579) ([0.600]+[0.258])	Prec@1 81.250 (81.250)
 * Prec@1 77.140
current lr 1.00000e-01
Grad=  tensor(1.7638, device='cuda:0')
Epoch: [65][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6026 (0.6026) ([0.345]+[0.258])	Prec@1 89.062 (89.062)
Epoch: [65][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5604 (0.6556) ([0.303]+[0.257])	Prec@1 90.625 (86.765)
Epoch: [65][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6608 (0.6579) ([0.403]+[0.258])	Prec@1 86.719 (86.641)
Epoch: [65][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6166 (0.6650) ([0.358]+[0.258])	Prec@1 90.625 (86.438)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.8942 (0.8942) ([0.636]+[0.258])	Prec@1 81.250 (81.250)
 * Prec@1 77.690
current lr 1.00000e-01
Grad=  tensor(1.3293, device='cuda:0')
Epoch: [66][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.5240 (0.5240) ([0.266]+[0.258])	Prec@1 91.406 (91.406)
Epoch: [66][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8597 (0.6433) ([0.602]+[0.258])	Prec@1 83.594 (87.044)
Epoch: [66][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6825 (0.6554) ([0.425]+[0.258])	Prec@1 82.031 (86.645)
Epoch: [66][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.6556 (0.6570) ([0.398]+[0.258])	Prec@1 87.500 (86.480)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.7148 (0.7148) ([0.457]+[0.258])	Prec@1 83.594 (83.594)
 * Prec@1 81.170
current lr 1.00000e-01
Grad=  tensor(1.9285, device='cuda:0')
Epoch: [67][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.5800 (0.5800) ([0.322]+[0.258])	Prec@1 86.719 (86.719)
Epoch: [67][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6880 (0.6474) ([0.430]+[0.258])	Prec@1 85.156 (86.610)
Epoch: [67][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6061 (0.6642) ([0.348]+[0.258])	Prec@1 85.938 (86.213)
Epoch: [67][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6758 (0.6628) ([0.418]+[0.258])	Prec@1 85.938 (86.233)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.9003 (0.9003) ([0.643]+[0.258])	Prec@1 76.562 (76.562)
 * Prec@1 80.710
current lr 1.00000e-01
Grad=  tensor(1.7006, device='cuda:0')
Epoch: [68][0/391]	Time 0.163 (0.163)	Data 0.119 (0.119)	Loss 0.6409 (0.6409) ([0.383]+[0.258])	Prec@1 87.500 (87.500)
Epoch: [68][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6310 (0.6391) ([0.373]+[0.258])	Prec@1 85.938 (87.075)
Epoch: [68][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6799 (0.6463) ([0.423]+[0.257])	Prec@1 85.938 (86.948)
Epoch: [68][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6971 (0.6561) ([0.439]+[0.258])	Prec@1 85.938 (86.555)
Test: [0/79]	Time 0.120 (0.120)	Loss 1.0207 (1.0207) ([0.764]+[0.257])	Prec@1 78.125 (78.125)
 * Prec@1 76.580
current lr 1.00000e-01
Grad=  tensor(1.9222, device='cuda:0')
Epoch: [69][0/391]	Time 0.163 (0.163)	Data 0.118 (0.118)	Loss 0.5718 (0.5718) ([0.315]+[0.257])	Prec@1 89.062 (89.062)
Epoch: [69][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5775 (0.6668) ([0.319]+[0.259])	Prec@1 88.281 (86.533)
Epoch: [69][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8005 (0.6597) ([0.543]+[0.257])	Prec@1 82.031 (86.618)
Epoch: [69][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7234 (0.6625) ([0.466]+[0.258])	Prec@1 79.688 (86.446)
Test: [0/79]	Time 0.133 (0.133)	Loss 0.7859 (0.7859) ([0.529]+[0.257])	Prec@1 81.250 (81.250)
 * Prec@1 83.310
current lr 1.00000e-01
Grad=  tensor(1.5361, device='cuda:0')
Epoch: [70][0/391]	Time 0.170 (0.170)	Data 0.124 (0.124)	Loss 0.6306 (0.6306) ([0.374]+[0.257])	Prec@1 87.500 (87.500)
Epoch: [70][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.8261 (0.6696) ([0.568]+[0.258])	Prec@1 82.031 (85.999)
Epoch: [70][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6895 (0.6635) ([0.432]+[0.258])	Prec@1 88.281 (86.233)
Epoch: [70][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.8322 (0.6690) ([0.574]+[0.258])	Prec@1 81.250 (86.127)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.7893 (0.7893) ([0.532]+[0.258])	Prec@1 84.375 (84.375)
 * Prec@1 83.950
current lr 1.00000e-01
Grad=  tensor(1.8990, device='cuda:0')
Epoch: [71][0/391]	Time 0.161 (0.161)	Data 0.118 (0.118)	Loss 0.6137 (0.6137) ([0.356]+[0.258])	Prec@1 86.719 (86.719)
Epoch: [71][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7447 (0.6529) ([0.488]+[0.257])	Prec@1 82.812 (86.456)
Epoch: [71][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5819 (0.6638) ([0.324]+[0.258])	Prec@1 89.844 (86.241)
Epoch: [71][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.5177 (0.6617) ([0.260]+[0.257])	Prec@1 91.406 (86.337)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.8459 (0.8459) ([0.588]+[0.258])	Prec@1 78.125 (78.125)
 * Prec@1 80.210
current lr 1.00000e-01
Grad=  tensor(1.3552, device='cuda:0')
Epoch: [72][0/391]	Time 0.167 (0.167)	Data 0.122 (0.122)	Loss 0.5650 (0.5650) ([0.308]+[0.258])	Prec@1 89.844 (89.844)
Epoch: [72][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5715 (0.6571) ([0.315]+[0.256])	Prec@1 89.844 (86.510)
Epoch: [72][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6693 (0.6606) ([0.412]+[0.257])	Prec@1 83.594 (86.396)
Epoch: [72][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6534 (0.6645) ([0.396]+[0.258])	Prec@1 89.844 (86.301)
Test: [0/79]	Time 0.132 (0.132)	Loss 1.0025 (1.0025) ([0.745]+[0.257])	Prec@1 76.562 (76.562)
 * Prec@1 77.560
current lr 1.00000e-01
Grad=  tensor(2.2619, device='cuda:0')
Epoch: [73][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7242 (0.7242) ([0.467]+[0.257])	Prec@1 85.156 (85.156)
Epoch: [73][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6310 (0.6649) ([0.374]+[0.257])	Prec@1 86.719 (85.984)
Epoch: [73][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6955 (0.6532) ([0.440]+[0.255])	Prec@1 83.594 (86.439)
Epoch: [73][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7133 (0.6598) ([0.457]+[0.257])	Prec@1 85.156 (86.267)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.9026 (0.9026) ([0.646]+[0.256])	Prec@1 80.469 (80.469)
 * Prec@1 81.040
current lr 1.00000e-01
Grad=  tensor(1.7932, device='cuda:0')
Epoch: [74][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.6340 (0.6340) ([0.378]+[0.256])	Prec@1 87.500 (87.500)
Epoch: [74][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5476 (0.6534) ([0.291]+[0.257])	Prec@1 89.844 (86.657)
Epoch: [74][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6180 (0.6598) ([0.360]+[0.258])	Prec@1 89.062 (86.431)
Epoch: [74][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8078 (0.6625) ([0.551]+[0.257])	Prec@1 77.344 (86.332)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7001 (0.7001) ([0.443]+[0.257])	Prec@1 87.500 (87.500)
 * Prec@1 83.730
current lr 1.00000e-01
Grad=  tensor(1.9233, device='cuda:0')
Epoch: [75][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.5778 (0.5778) ([0.321]+[0.257])	Prec@1 88.281 (88.281)
Epoch: [75][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7800 (0.6548) ([0.523]+[0.257])	Prec@1 80.469 (86.371)
Epoch: [75][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6669 (0.6556) ([0.409]+[0.258])	Prec@1 83.594 (86.245)
Epoch: [75][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5806 (0.6588) ([0.322]+[0.258])	Prec@1 89.844 (86.215)
Test: [0/79]	Time 0.136 (0.136)	Loss 1.2534 (1.2534) ([0.996]+[0.258])	Prec@1 75.000 (75.000)
 * Prec@1 74.110
current lr 1.00000e-01
Grad=  tensor(2.0908, device='cuda:0')
Epoch: [76][0/391]	Time 0.166 (0.166)	Data 0.122 (0.122)	Loss 0.6945 (0.6945) ([0.437]+[0.258])	Prec@1 82.812 (82.812)
Epoch: [76][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6592 (0.6461) ([0.401]+[0.258])	Prec@1 89.844 (86.425)
Epoch: [76][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5968 (0.6562) ([0.339]+[0.258])	Prec@1 90.625 (86.171)
Epoch: [76][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6321 (0.6585) ([0.374]+[0.258])	Prec@1 89.062 (86.254)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.9011 (0.9011) ([0.644]+[0.258])	Prec@1 78.125 (78.125)
 * Prec@1 80.650
current lr 1.00000e-01
Grad=  tensor(2.3981, device='cuda:0')
Epoch: [77][0/391]	Time 0.172 (0.172)	Data 0.128 (0.128)	Loss 0.7624 (0.7624) ([0.505]+[0.258])	Prec@1 82.812 (82.812)
Epoch: [77][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6877 (0.6502) ([0.431]+[0.257])	Prec@1 82.812 (86.680)
Epoch: [77][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7492 (0.6471) ([0.493]+[0.256])	Prec@1 82.031 (86.758)
Epoch: [77][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5705 (0.6534) ([0.315]+[0.256])	Prec@1 91.406 (86.493)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.9232 (0.9232) ([0.667]+[0.256])	Prec@1 72.656 (72.656)
 * Prec@1 77.830
current lr 1.00000e-01
Grad=  tensor(2.5478, device='cuda:0')
Epoch: [78][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6922 (0.6922) ([0.436]+[0.256])	Prec@1 85.156 (85.156)
Epoch: [78][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8152 (0.6712) ([0.558]+[0.257])	Prec@1 81.250 (86.131)
Epoch: [78][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6573 (0.6644) ([0.400]+[0.258])	Prec@1 85.938 (86.361)
Epoch: [78][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6364 (0.6621) ([0.379]+[0.257])	Prec@1 88.281 (86.387)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.8413 (0.8413) ([0.585]+[0.257])	Prec@1 79.688 (79.688)
 * Prec@1 81.690
current lr 1.00000e-01
Grad=  tensor(2.3029, device='cuda:0')
Epoch: [79][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.6676 (0.6676) ([0.411]+[0.257])	Prec@1 87.500 (87.500)
Epoch: [79][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6585 (0.6491) ([0.402]+[0.257])	Prec@1 85.156 (86.850)
Epoch: [79][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6223 (0.6585) ([0.365]+[0.258])	Prec@1 88.281 (86.567)
Epoch: [79][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6088 (0.6645) ([0.352]+[0.257])	Prec@1 87.500 (86.397)
Test: [0/79]	Time 0.118 (0.118)	Loss 1.0698 (1.0698) ([0.814]+[0.256])	Prec@1 76.562 (76.562)
 * Prec@1 78.330
current lr 1.00000e-01
Grad=  tensor(1.6180, device='cuda:0')
Epoch: [80][0/391]	Time 0.170 (0.170)	Data 0.122 (0.122)	Loss 0.5908 (0.5908) ([0.335]+[0.256])	Prec@1 91.406 (91.406)
Epoch: [80][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.7121 (0.6536) ([0.455]+[0.257])	Prec@1 81.250 (86.386)
Epoch: [80][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6297 (0.6573) ([0.374]+[0.256])	Prec@1 86.719 (86.486)
Epoch: [80][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5827 (0.6540) ([0.328]+[0.255])	Prec@1 87.500 (86.537)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8214 (0.8214) ([0.566]+[0.255])	Prec@1 85.156 (85.156)
 * Prec@1 80.420
current lr 1.00000e-01
Grad=  tensor(2.0972, device='cuda:0')
Epoch: [81][0/391]	Time 0.168 (0.168)	Data 0.118 (0.118)	Loss 0.5859 (0.5859) ([0.331]+[0.255])	Prec@1 86.719 (86.719)
Epoch: [81][100/391]	Time 0.036 (0.039)	Data 0.000 (0.001)	Loss 0.5706 (0.6491) ([0.315]+[0.256])	Prec@1 89.062 (86.417)
Epoch: [81][200/391]	Time 0.040 (0.039)	Data 0.000 (0.001)	Loss 0.7401 (0.6565) ([0.484]+[0.256])	Prec@1 84.375 (86.307)
Epoch: [81][300/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss 0.6014 (0.6588) ([0.345]+[0.256])	Prec@1 86.719 (86.202)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.9172 (0.9172) ([0.662]+[0.255])	Prec@1 75.000 (75.000)
 * Prec@1 76.810
current lr 1.00000e-01
Grad=  tensor(2.1756, device='cuda:0')
Epoch: [82][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.6945 (0.6945) ([0.440]+[0.255])	Prec@1 85.156 (85.156)
Epoch: [82][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6869 (0.6478) ([0.431]+[0.256])	Prec@1 85.938 (86.935)
Epoch: [82][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7869 (0.6529) ([0.531]+[0.256])	Prec@1 79.688 (86.544)
Epoch: [82][300/391]	Time 0.041 (0.038)	Data 0.000 (0.001)	Loss 0.6639 (0.6599) ([0.407]+[0.257])	Prec@1 83.594 (86.340)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.7695 (0.7695) ([0.515]+[0.255])	Prec@1 82.812 (82.812)
 * Prec@1 78.080
current lr 1.00000e-01
Grad=  tensor(2.0397, device='cuda:0')
Epoch: [83][0/391]	Time 0.168 (0.168)	Data 0.119 (0.119)	Loss 0.6910 (0.6910) ([0.436]+[0.255])	Prec@1 89.844 (89.844)
Epoch: [83][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7457 (0.6284) ([0.490]+[0.255])	Prec@1 84.375 (87.338)
Epoch: [83][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6215 (0.6410) ([0.365]+[0.256])	Prec@1 91.406 (86.940)
Epoch: [83][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7590 (0.6506) ([0.503]+[0.256])	Prec@1 84.375 (86.568)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8008 (0.8008) ([0.544]+[0.256])	Prec@1 83.594 (83.594)
 * Prec@1 81.940
current lr 1.00000e-01
Grad=  tensor(1.9256, device='cuda:0')
Epoch: [84][0/391]	Time 0.166 (0.166)	Data 0.116 (0.116)	Loss 0.6507 (0.6507) ([0.394]+[0.256])	Prec@1 85.156 (85.156)
Epoch: [84][100/391]	Time 0.038 (0.040)	Data 0.000 (0.001)	Loss 0.6164 (0.6420) ([0.360]+[0.256])	Prec@1 87.500 (87.106)
Epoch: [84][200/391]	Time 0.043 (0.039)	Data 0.000 (0.001)	Loss 0.6385 (0.6502) ([0.382]+[0.256])	Prec@1 84.375 (86.800)
Epoch: [84][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6600 (0.6532) ([0.404]+[0.256])	Prec@1 86.719 (86.706)
Test: [0/79]	Time 0.117 (0.117)	Loss 1.2101 (1.2101) ([0.954]+[0.256])	Prec@1 67.188 (67.188)
 * Prec@1 68.650
current lr 1.00000e-01
Grad=  tensor(2.1368, device='cuda:0')
Epoch: [85][0/391]	Time 0.171 (0.171)	Data 0.126 (0.126)	Loss 0.6056 (0.6056) ([0.349]+[0.256])	Prec@1 86.719 (86.719)
Epoch: [85][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6098 (0.6497) ([0.354]+[0.256])	Prec@1 89.062 (86.247)
Epoch: [85][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6505 (0.6494) ([0.395]+[0.256])	Prec@1 87.500 (86.412)
Epoch: [85][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7786 (0.6542) ([0.523]+[0.256])	Prec@1 82.031 (86.462)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.8918 (0.8918) ([0.636]+[0.255])	Prec@1 78.906 (78.906)
 * Prec@1 73.920
current lr 1.00000e-01
Grad=  tensor(1.6440, device='cuda:0')
Epoch: [86][0/391]	Time 0.173 (0.173)	Data 0.122 (0.122)	Loss 0.5775 (0.5775) ([0.322]+[0.255])	Prec@1 89.062 (89.062)
Epoch: [86][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6185 (0.6556) ([0.363]+[0.256])	Prec@1 86.719 (86.494)
Epoch: [86][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.4664 (0.6511) ([0.213]+[0.254])	Prec@1 91.406 (86.556)
Epoch: [86][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7112 (0.6504) ([0.457]+[0.254])	Prec@1 84.375 (86.550)
Test: [0/79]	Time 0.130 (0.130)	Loss 1.0411 (1.0411) ([0.788]+[0.253])	Prec@1 78.125 (78.125)
 * Prec@1 77.420
current lr 1.00000e-01
Grad=  tensor(2.4181, device='cuda:0')
Epoch: [87][0/391]	Time 0.175 (0.175)	Data 0.132 (0.132)	Loss 0.6854 (0.6854) ([0.432]+[0.253])	Prec@1 86.719 (86.719)
Epoch: [87][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6138 (0.6408) ([0.360]+[0.253])	Prec@1 89.062 (86.920)
Epoch: [87][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7034 (0.6510) ([0.450]+[0.254])	Prec@1 84.375 (86.571)
Epoch: [87][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5861 (0.6477) ([0.332]+[0.254])	Prec@1 87.500 (86.659)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.7165 (0.7165) ([0.462]+[0.254])	Prec@1 82.031 (82.031)
 * Prec@1 81.220
current lr 1.00000e-01
Grad=  tensor(1.6981, device='cuda:0')
Epoch: [88][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.6244 (0.6244) ([0.370]+[0.254])	Prec@1 86.719 (86.719)
Epoch: [88][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6036 (0.6477) ([0.349]+[0.254])	Prec@1 84.375 (86.541)
Epoch: [88][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.5854 (0.6434) ([0.331]+[0.254])	Prec@1 86.719 (86.723)
Epoch: [88][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6576 (0.6504) ([0.402]+[0.256])	Prec@1 85.938 (86.612)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.8262 (0.8262) ([0.571]+[0.255])	Prec@1 83.594 (83.594)
 * Prec@1 80.230
current lr 1.00000e-01
Grad=  tensor(1.5528, device='cuda:0')
Epoch: [89][0/391]	Time 0.208 (0.208)	Data 0.167 (0.167)	Loss 0.6114 (0.6114) ([0.357]+[0.255])	Prec@1 88.281 (88.281)
Epoch: [89][100/391]	Time 0.037 (0.040)	Data 0.000 (0.002)	Loss 0.6474 (0.6427) ([0.393]+[0.254])	Prec@1 88.281 (86.959)
Epoch: [89][200/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss 0.6675 (0.6408) ([0.414]+[0.254])	Prec@1 83.594 (86.664)
Epoch: [89][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6147 (0.6474) ([0.360]+[0.255])	Prec@1 86.719 (86.384)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9252 (0.9252) ([0.670]+[0.255])	Prec@1 78.906 (78.906)
 * Prec@1 79.520
current lr 1.00000e-01
Grad=  tensor(1.8518, device='cuda:0')
Epoch: [90][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6322 (0.6322) ([0.377]+[0.255])	Prec@1 88.281 (88.281)
Epoch: [90][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.6413 (0.6436) ([0.386]+[0.256])	Prec@1 91.406 (86.796)
Epoch: [90][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7282 (0.6562) ([0.473]+[0.255])	Prec@1 80.469 (86.361)
Epoch: [90][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7735 (0.6600) ([0.518]+[0.256])	Prec@1 82.812 (86.368)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.7826 (0.7826) ([0.529]+[0.254])	Prec@1 82.031 (82.031)
 * Prec@1 81.000
current lr 1.00000e-01
Grad=  tensor(2.6648, device='cuda:0')
Epoch: [91][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.7771 (0.7771) ([0.523]+[0.254])	Prec@1 83.594 (83.594)
Epoch: [91][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.5923 (0.6386) ([0.339]+[0.254])	Prec@1 89.062 (87.075)
Epoch: [91][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7637 (0.6458) ([0.510]+[0.254])	Prec@1 80.469 (86.796)
Epoch: [91][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6116 (0.6560) ([0.357]+[0.255])	Prec@1 89.062 (86.516)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.6835 (0.6835) ([0.429]+[0.255])	Prec@1 85.156 (85.156)
 * Prec@1 82.680
current lr 1.00000e-01
Grad=  tensor(1.5915, device='cuda:0')
Epoch: [92][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.5169 (0.5169) ([0.262]+[0.255])	Prec@1 91.406 (91.406)
Epoch: [92][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.8213 (0.6324) ([0.566]+[0.255])	Prec@1 79.688 (87.361)
Epoch: [92][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6853 (0.6508) ([0.430]+[0.255])	Prec@1 87.500 (86.633)
Epoch: [92][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.7129 (0.6610) ([0.457]+[0.256])	Prec@1 82.812 (86.272)
Test: [0/79]	Time 0.122 (0.122)	Loss 1.2909 (1.2909) ([1.037]+[0.254])	Prec@1 74.219 (74.219)
 * Prec@1 71.540
current lr 1.00000e-01
Grad=  tensor(2.2564, device='cuda:0')
Epoch: [93][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.7442 (0.7442) ([0.490]+[0.254])	Prec@1 86.719 (86.719)
Epoch: [93][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5600 (0.6596) ([0.305]+[0.255])	Prec@1 89.844 (86.278)
Epoch: [93][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6186 (0.6561) ([0.364]+[0.255])	Prec@1 85.156 (86.447)
Epoch: [93][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6445 (0.6574) ([0.392]+[0.253])	Prec@1 86.719 (86.361)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.7379 (0.7379) ([0.485]+[0.253])	Prec@1 83.594 (83.594)
 * Prec@1 82.370
current lr 1.00000e-01
Grad=  tensor(1.7538, device='cuda:0')
Epoch: [94][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.5661 (0.5661) ([0.313]+[0.253])	Prec@1 89.844 (89.844)
Epoch: [94][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.7531 (0.6288) ([0.501]+[0.252])	Prec@1 81.250 (86.897)
Epoch: [94][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5395 (0.6460) ([0.286]+[0.253])	Prec@1 91.406 (86.524)
Epoch: [94][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5673 (0.6492) ([0.315]+[0.252])	Prec@1 89.062 (86.542)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.7952 (0.7952) ([0.543]+[0.252])	Prec@1 81.250 (81.250)
 * Prec@1 76.930
current lr 1.00000e-01
Grad=  tensor(2.2281, device='cuda:0')
Epoch: [95][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.6358 (0.6358) ([0.384]+[0.252])	Prec@1 83.594 (83.594)
Epoch: [95][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.8316 (0.6474) ([0.579]+[0.252])	Prec@1 78.906 (86.757)
Epoch: [95][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7150 (0.6487) ([0.463]+[0.252])	Prec@1 86.719 (86.590)
Epoch: [95][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5424 (0.6582) ([0.288]+[0.255])	Prec@1 92.188 (86.272)
Test: [0/79]	Time 0.123 (0.123)	Loss 1.0808 (1.0808) ([0.826]+[0.255])	Prec@1 72.656 (72.656)
 * Prec@1 75.810
current lr 1.00000e-01
Grad=  tensor(2.3417, device='cuda:0')
Epoch: [96][0/391]	Time 0.166 (0.166)	Data 0.122 (0.122)	Loss 0.6856 (0.6856) ([0.431]+[0.255])	Prec@1 86.719 (86.719)
Epoch: [96][100/391]	Time 0.036 (0.039)	Data 0.000 (0.001)	Loss 0.8482 (0.6410) ([0.594]+[0.254])	Prec@1 80.469 (86.843)
Epoch: [96][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6308 (0.6495) ([0.376]+[0.254])	Prec@1 87.500 (86.478)
Epoch: [96][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6377 (0.6494) ([0.384]+[0.254])	Prec@1 89.844 (86.579)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.9185 (0.9185) ([0.663]+[0.256])	Prec@1 80.469 (80.469)
 * Prec@1 81.060
current lr 1.00000e-01
Grad=  tensor(2.0412, device='cuda:0')
Epoch: [97][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.6548 (0.6548) ([0.399]+[0.256])	Prec@1 87.500 (87.500)
Epoch: [97][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6373 (0.6448) ([0.383]+[0.255])	Prec@1 84.375 (86.966)
Epoch: [97][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8549 (0.6436) ([0.601]+[0.254])	Prec@1 80.469 (86.995)
Epoch: [97][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5957 (0.6496) ([0.340]+[0.255])	Prec@1 88.281 (86.727)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.7219 (0.7219) ([0.467]+[0.255])	Prec@1 87.500 (87.500)
 * Prec@1 83.390
current lr 1.00000e-01
Grad=  tensor(1.8829, device='cuda:0')
Epoch: [98][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.6752 (0.6752) ([0.420]+[0.255])	Prec@1 86.719 (86.719)
Epoch: [98][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7807 (0.6403) ([0.526]+[0.254])	Prec@1 82.031 (86.873)
Epoch: [98][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7090 (0.6586) ([0.453]+[0.256])	Prec@1 87.500 (86.373)
Epoch: [98][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.4978 (0.6612) ([0.243]+[0.255])	Prec@1 93.750 (86.265)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.2396 (1.2396) ([0.985]+[0.255])	Prec@1 79.688 (79.688)
 * Prec@1 73.090
current lr 1.00000e-01
Grad=  tensor(2.3397, device='cuda:0')
Epoch: [99][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.6049 (0.6049) ([0.350]+[0.255])	Prec@1 85.938 (85.938)
Epoch: [99][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6390 (0.6373) ([0.385]+[0.254])	Prec@1 87.500 (87.090)
Epoch: [99][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6799 (0.6460) ([0.426]+[0.253])	Prec@1 85.938 (86.824)
Epoch: [99][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8213 (0.6468) ([0.569]+[0.252])	Prec@1 81.250 (86.755)
Test: [0/79]	Time 0.125 (0.125)	Loss 1.0593 (1.0593) ([0.807]+[0.252])	Prec@1 76.562 (76.562)
 * Prec@1 79.030
current lr 1.00000e-02
Grad=  tensor(1.9735, device='cuda:0')
Epoch: [100][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.6424 (0.6424) ([0.390]+[0.252])	Prec@1 86.719 (86.719)
Epoch: [100][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5311 (0.4947) ([0.302]+[0.229])	Prec@1 86.719 (91.221)
Epoch: [100][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3813 (0.4751) ([0.155]+[0.227])	Prec@1 95.312 (91.865)
Epoch: [100][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3695 (0.4581) ([0.145]+[0.225])	Prec@1 96.875 (92.408)
Test: [0/79]	Time 0.134 (0.134)	Loss 0.4697 (0.4697) ([0.247]+[0.223])	Prec@1 91.406 (91.406)
 * Prec@1 92.040
current lr 1.00000e-02
Grad=  tensor(1.2431, device='cuda:0')
Epoch: [101][0/391]	Time 0.177 (0.177)	Data 0.129 (0.129)	Loss 0.4350 (0.4350) ([0.212]+[0.223])	Prec@1 95.312 (95.312)
Epoch: [101][100/391]	Time 0.038 (0.039)	Data 0.000 (0.001)	Loss 0.3224 (0.3916) ([0.102]+[0.220])	Prec@1 96.875 (94.230)
Epoch: [101][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3535 (0.3876) ([0.135]+[0.218])	Prec@1 96.875 (94.298)
Epoch: [101][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.4239 (0.3880) ([0.208]+[0.216])	Prec@1 93.750 (94.277)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4519 (0.4519) ([0.237]+[0.214])	Prec@1 92.188 (92.188)
 * Prec@1 92.610
current lr 1.00000e-02
Grad=  tensor(1.2500, device='cuda:0')
Epoch: [102][0/391]	Time 0.170 (0.170)	Data 0.125 (0.125)	Loss 0.3210 (0.3210) ([0.106]+[0.214])	Prec@1 96.094 (96.094)
Epoch: [102][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3678 (0.3590) ([0.155]+[0.213])	Prec@1 95.312 (94.972)
Epoch: [102][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2982 (0.3595) ([0.088]+[0.211])	Prec@1 97.656 (95.037)
Epoch: [102][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3890 (0.3577) ([0.180]+[0.209])	Prec@1 94.531 (95.097)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4655 (0.4655) ([0.258]+[0.207])	Prec@1 91.406 (91.406)
 * Prec@1 92.630
current lr 1.00000e-02
Grad=  tensor(1.8426, device='cuda:0')
Epoch: [103][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.3624 (0.3624) ([0.155]+[0.207])	Prec@1 94.531 (94.531)
Epoch: [103][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3388 (0.3402) ([0.133]+[0.206])	Prec@1 96.875 (95.583)
Epoch: [103][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2579 (0.3400) ([0.054]+[0.204])	Prec@1 99.219 (95.503)
Epoch: [103][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3300 (0.3401) ([0.128]+[0.202])	Prec@1 95.312 (95.468)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4326 (0.4326) ([0.232]+[0.201])	Prec@1 92.969 (92.969)
 * Prec@1 92.940
current lr 1.00000e-02
Grad=  tensor(2.3895, device='cuda:0')
Epoch: [104][0/391]	Time 0.173 (0.173)	Data 0.124 (0.124)	Loss 0.3462 (0.3462) ([0.146]+[0.201])	Prec@1 92.188 (92.188)
Epoch: [104][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2910 (0.3253) ([0.092]+[0.199])	Prec@1 96.875 (95.831)
Epoch: [104][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3324 (0.3190) ([0.135]+[0.197])	Prec@1 96.875 (96.043)
Epoch: [104][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2778 (0.3180) ([0.082]+[0.196])	Prec@1 97.656 (96.055)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.4829 (0.4829) ([0.289]+[0.194])	Prec@1 90.625 (90.625)
 * Prec@1 92.680
current lr 1.00000e-02
Grad=  tensor(1.2175, device='cuda:0')
Epoch: [105][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2653 (0.2653) ([0.071]+[0.194])	Prec@1 96.875 (96.875)
Epoch: [105][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3273 (0.3075) ([0.134]+[0.193])	Prec@1 97.656 (96.248)
Epoch: [105][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2539 (0.3022) ([0.063]+[0.191])	Prec@1 98.438 (96.389)
Epoch: [105][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2631 (0.3039) ([0.073]+[0.190])	Prec@1 96.875 (96.224)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3995 (0.3995) ([0.211]+[0.189])	Prec@1 92.188 (92.188)
 * Prec@1 92.800
current lr 1.00000e-02
Grad=  tensor(1.3844, device='cuda:0')
Epoch: [106][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.2490 (0.2490) ([0.060]+[0.189])	Prec@1 99.219 (99.219)
Epoch: [106][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2654 (0.2872) ([0.078]+[0.187])	Prec@1 96.875 (96.813)
Epoch: [106][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2372 (0.2888) ([0.051]+[0.186])	Prec@1 99.219 (96.677)
Epoch: [106][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3002 (0.2905) ([0.116]+[0.185])	Prec@1 96.094 (96.538)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4630 (0.4630) ([0.280]+[0.183])	Prec@1 92.188 (92.188)
 * Prec@1 92.980
current lr 1.00000e-02
Grad=  tensor(2.2218, device='cuda:0')
Epoch: [107][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2724 (0.2724) ([0.089]+[0.183])	Prec@1 96.875 (96.875)
Epoch: [107][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2610 (0.2754) ([0.079]+[0.182])	Prec@1 96.875 (96.945)
Epoch: [107][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2490 (0.2789) ([0.068]+[0.181])	Prec@1 98.438 (96.832)
Epoch: [107][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2552 (0.2815) ([0.075]+[0.180])	Prec@1 98.438 (96.745)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3873 (0.3873) ([0.209]+[0.179])	Prec@1 93.750 (93.750)
 * Prec@1 93.060
current lr 1.00000e-02
Grad=  tensor(2.1052, device='cuda:0')
Epoch: [108][0/391]	Time 0.169 (0.169)	Data 0.120 (0.120)	Loss 0.2566 (0.2566) ([0.078]+[0.179])	Prec@1 97.656 (97.656)
Epoch: [108][100/391]	Time 0.041 (0.040)	Data 0.000 (0.001)	Loss 0.2228 (0.2599) ([0.045]+[0.177])	Prec@1 99.219 (97.300)
Epoch: [108][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2152 (0.2642) ([0.039]+[0.176])	Prec@1 99.219 (97.112)
Epoch: [108][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2563 (0.2662) ([0.081]+[0.175])	Prec@1 97.656 (96.937)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3704 (0.3704) ([0.196]+[0.174])	Prec@1 93.750 (93.750)
 * Prec@1 92.790
current lr 1.00000e-02
Grad=  tensor(2.4135, device='cuda:0')
Epoch: [109][0/391]	Time 0.161 (0.161)	Data 0.117 (0.117)	Loss 0.2495 (0.2495) ([0.075]+[0.174])	Prec@1 96.875 (96.875)
Epoch: [109][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2650 (0.2597) ([0.092]+[0.173])	Prec@1 96.875 (97.030)
Epoch: [109][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2341 (0.2598) ([0.062]+[0.172])	Prec@1 97.656 (96.968)
Epoch: [109][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2912 (0.2604) ([0.120]+[0.171])	Prec@1 94.531 (96.930)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4265 (0.4265) ([0.256]+[0.170])	Prec@1 89.844 (89.844)
 * Prec@1 92.840
current lr 1.00000e-02
Grad=  tensor(2.9466, device='cuda:0')
Epoch: [110][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2497 (0.2497) ([0.080]+[0.170])	Prec@1 97.656 (97.656)
Epoch: [110][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2928 (0.2521) ([0.124]+[0.169])	Prec@1 94.531 (97.285)
Epoch: [110][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2463 (0.2513) ([0.078]+[0.168])	Prec@1 96.875 (97.275)
Epoch: [110][300/391]	Time 0.045 (0.038)	Data 0.000 (0.001)	Loss 0.2120 (0.2523) ([0.045]+[0.167])	Prec@1 98.438 (97.267)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.3789 (0.3789) ([0.213]+[0.166])	Prec@1 93.750 (93.750)
 * Prec@1 93.020
current lr 1.00000e-02
Grad=  tensor(1.9964, device='cuda:0')
Epoch: [111][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2230 (0.2230) ([0.057]+[0.166])	Prec@1 99.219 (99.219)
Epoch: [111][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2157 (0.2447) ([0.050]+[0.165])	Prec@1 98.438 (97.355)
Epoch: [111][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2299 (0.2422) ([0.065]+[0.164])	Prec@1 96.875 (97.314)
Epoch: [111][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2321 (0.2463) ([0.069]+[0.164])	Prec@1 98.438 (97.166)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.4536 (0.4536) ([0.291]+[0.163])	Prec@1 92.188 (92.188)
 * Prec@1 93.140
current lr 1.00000e-02
Grad=  tensor(1.5355, device='cuda:0')
Epoch: [112][0/391]	Time 0.168 (0.168)	Data 0.123 (0.123)	Loss 0.2080 (0.2080) ([0.045]+[0.163])	Prec@1 98.438 (98.438)
Epoch: [112][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2049 (0.2365) ([0.043]+[0.162])	Prec@1 98.438 (97.548)
Epoch: [112][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2145 (0.2395) ([0.053]+[0.161])	Prec@1 98.438 (97.341)
Epoch: [112][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.1964 (0.2420) ([0.036]+[0.160])	Prec@1 99.219 (97.277)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3975 (0.3975) ([0.238]+[0.160])	Prec@1 92.969 (92.969)
 * Prec@1 92.920
current lr 1.00000e-02
Grad=  tensor(7.4381, device='cuda:0')
Epoch: [113][0/391]	Time 0.164 (0.164)	Data 0.119 (0.119)	Loss 0.3043 (0.3043) ([0.145]+[0.160])	Prec@1 95.312 (95.312)
Epoch: [113][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2473 (0.2303) ([0.089]+[0.159])	Prec@1 96.875 (97.602)
Epoch: [113][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2297 (0.2319) ([0.072]+[0.158])	Prec@1 96.875 (97.520)
Epoch: [113][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2940 (0.2352) ([0.137]+[0.157])	Prec@1 94.531 (97.368)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.4279 (0.4279) ([0.271]+[0.157])	Prec@1 91.406 (91.406)
 * Prec@1 92.380
current lr 1.00000e-02
Grad=  tensor(2.6142, device='cuda:0')
Epoch: [114][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2058 (0.2058) ([0.049]+[0.157])	Prec@1 98.438 (98.438)
Epoch: [114][100/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2032 (0.2293) ([0.047]+[0.156])	Prec@1 99.219 (97.602)
Epoch: [114][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2223 (0.2301) ([0.067]+[0.155])	Prec@1 96.875 (97.575)
Epoch: [114][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2496 (0.2333) ([0.095]+[0.154])	Prec@1 96.875 (97.397)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3988 (0.3988) ([0.245]+[0.154])	Prec@1 92.969 (92.969)
 * Prec@1 93.070
current lr 1.00000e-02
Grad=  tensor(2.4326, device='cuda:0')
Epoch: [115][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.2002 (0.2002) ([0.046]+[0.154])	Prec@1 98.438 (98.438)
Epoch: [115][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2374 (0.2286) ([0.084]+[0.153])	Prec@1 98.438 (97.502)
Epoch: [115][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.1991 (0.2260) ([0.047]+[0.152])	Prec@1 98.438 (97.551)
Epoch: [115][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2602 (0.2266) ([0.109]+[0.152])	Prec@1 96.094 (97.513)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.5104 (0.5104) ([0.359]+[0.151])	Prec@1 90.625 (90.625)
 * Prec@1 92.010
current lr 1.00000e-02
Grad=  tensor(1.7717, device='cuda:0')
Epoch: [116][0/391]	Time 0.171 (0.171)	Data 0.123 (0.123)	Loss 0.1901 (0.1901) ([0.039]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [116][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2138 (0.2273) ([0.063]+[0.151])	Prec@1 97.656 (97.440)
Epoch: [116][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2070 (0.2269) ([0.057]+[0.150])	Prec@1 98.438 (97.481)
Epoch: [116][300/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2216 (0.2280) ([0.072]+[0.150])	Prec@1 97.656 (97.399)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4999 (0.4999) ([0.351]+[0.149])	Prec@1 90.625 (90.625)
 * Prec@1 92.040
current lr 1.00000e-02
Grad=  tensor(2.0575, device='cuda:0')
Epoch: [117][0/391]	Time 0.178 (0.178)	Data 0.127 (0.127)	Loss 0.1985 (0.1985) ([0.049]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [117][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2050 (0.2228) ([0.056]+[0.149])	Prec@1 98.438 (97.509)
Epoch: [117][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2199 (0.2207) ([0.072]+[0.148])	Prec@1 96.875 (97.610)
Epoch: [117][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2639 (0.2273) ([0.116]+[0.148])	Prec@1 96.094 (97.308)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4771 (0.4771) ([0.330]+[0.147])	Prec@1 89.062 (89.062)
 * Prec@1 92.190
current lr 1.00000e-02
Grad=  tensor(5.1325, device='cuda:0')
Epoch: [118][0/391]	Time 0.159 (0.159)	Data 0.116 (0.116)	Loss 0.2281 (0.2281) ([0.081]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [118][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2539 (0.2189) ([0.107]+[0.147])	Prec@1 96.875 (97.656)
Epoch: [118][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2313 (0.2206) ([0.085]+[0.146])	Prec@1 96.094 (97.637)
Epoch: [118][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1973 (0.2241) ([0.051]+[0.146])	Prec@1 98.438 (97.454)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.4673 (0.4673) ([0.322]+[0.145])	Prec@1 90.625 (90.625)
 * Prec@1 92.310
current lr 1.00000e-02
Grad=  tensor(5.5137, device='cuda:0')
Epoch: [119][0/391]	Time 0.173 (0.173)	Data 0.127 (0.127)	Loss 0.2580 (0.2580) ([0.113]+[0.145])	Prec@1 96.094 (96.094)
Epoch: [119][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.1952 (0.2183) ([0.050]+[0.145])	Prec@1 99.219 (97.540)
Epoch: [119][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1808 (0.2168) ([0.036]+[0.144])	Prec@1 99.219 (97.629)
Epoch: [119][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2197 (0.2166) ([0.076]+[0.144])	Prec@1 97.656 (97.612)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.4203 (0.4203) ([0.277]+[0.144])	Prec@1 92.188 (92.188)
 * Prec@1 90.970
current lr 1.00000e-02
Grad=  tensor(6.2561, device='cuda:0')
Epoch: [120][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.2208 (0.2208) ([0.077]+[0.144])	Prec@1 97.656 (97.656)
Epoch: [120][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2177 (0.2241) ([0.074]+[0.143])	Prec@1 96.875 (97.239)
Epoch: [120][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2283 (0.2222) ([0.085]+[0.143])	Prec@1 95.312 (97.361)
Epoch: [120][300/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.2464 (0.2218) ([0.104]+[0.143])	Prec@1 96.094 (97.373)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4411 (0.4411) ([0.299]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 92.240
current lr 1.00000e-02
Grad=  tensor(3.5415, device='cuda:0')
Epoch: [121][0/391]	Time 0.162 (0.162)	Data 0.119 (0.119)	Loss 0.2045 (0.2045) ([0.062]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [121][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2872 (0.2177) ([0.145]+[0.142])	Prec@1 95.312 (97.525)
Epoch: [121][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2381 (0.2217) ([0.096]+[0.142])	Prec@1 96.094 (97.322)
Epoch: [121][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2094 (0.2233) ([0.068]+[0.142])	Prec@1 97.656 (97.202)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.4300 (0.4300) ([0.288]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 92.140
current lr 1.00000e-02
Grad=  tensor(4.3215, device='cuda:0')
Epoch: [122][0/391]	Time 0.172 (0.172)	Data 0.120 (0.120)	Loss 0.2124 (0.2124) ([0.071]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [122][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2179 (0.2275) ([0.077]+[0.141])	Prec@1 97.656 (97.246)
Epoch: [122][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2234 (0.2249) ([0.082]+[0.141])	Prec@1 96.875 (97.221)
Epoch: [122][300/391]	Time 0.043 (0.038)	Data 0.000 (0.001)	Loss 0.2025 (0.2291) ([0.062]+[0.141])	Prec@1 98.438 (97.090)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3666 (0.3666) ([0.226]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 92.070
current lr 1.00000e-02
Grad=  tensor(5.7650, device='cuda:0')
Epoch: [123][0/391]	Time 0.161 (0.161)	Data 0.118 (0.118)	Loss 0.2243 (0.2243) ([0.083]+[0.141])	Prec@1 96.094 (96.094)
Epoch: [123][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2077 (0.2201) ([0.067]+[0.140])	Prec@1 97.656 (97.293)
Epoch: [123][200/391]	Time 0.038 (0.038)	Data 0.000 (0.001)	Loss 0.1623 (0.2235) ([0.022]+[0.140])	Prec@1 100.000 (97.159)
Epoch: [123][300/391]	Time 0.042 (0.038)	Data 0.000 (0.001)	Loss 0.1909 (0.2254) ([0.051]+[0.140])	Prec@1 97.656 (97.039)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3333 (0.3333) ([0.193]+[0.140])	Prec@1 93.750 (93.750)
 * Prec@1 92.500
current lr 1.00000e-02
Grad=  tensor(1.8258, device='cuda:0')
Epoch: [124][0/391]	Time 0.162 (0.162)	Data 0.116 (0.116)	Loss 0.1698 (0.1698) ([0.030]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [124][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.1780 (0.2113) ([0.038]+[0.140])	Prec@1 99.219 (97.649)
Epoch: [124][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2376 (0.2169) ([0.098]+[0.140])	Prec@1 95.312 (97.306)
Epoch: [124][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2747 (0.2221) ([0.135]+[0.139])	Prec@1 94.531 (97.150)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3827 (0.3827) ([0.243]+[0.139])	Prec@1 92.969 (92.969)
 * Prec@1 92.020
current lr 1.00000e-02
Grad=  tensor(1.7552, device='cuda:0')
Epoch: [125][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1715 (0.1715) ([0.032]+[0.139])	Prec@1 99.219 (99.219)
Epoch: [125][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2165 (0.2105) ([0.077]+[0.139])	Prec@1 96.094 (97.672)
Epoch: [125][200/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1920 (0.2168) ([0.053]+[0.139])	Prec@1 99.219 (97.345)
Epoch: [125][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2726 (0.2219) ([0.134]+[0.139])	Prec@1 96.875 (97.171)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4386 (0.4386) ([0.300]+[0.139])	Prec@1 92.188 (92.188)
 * Prec@1 92.070
current lr 1.00000e-02
Grad=  tensor(3.5003, device='cuda:0')
Epoch: [126][0/391]	Time 0.190 (0.190)	Data 0.139 (0.139)	Loss 0.1967 (0.1967) ([0.058]+[0.139])	Prec@1 99.219 (99.219)
Epoch: [126][100/391]	Time 0.038 (0.040)	Data 0.000 (0.002)	Loss 0.2416 (0.2148) ([0.103]+[0.139])	Prec@1 95.312 (97.455)
Epoch: [126][200/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2024 (0.2145) ([0.064]+[0.138])	Prec@1 97.656 (97.439)
Epoch: [126][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2498 (0.2193) ([0.112]+[0.138])	Prec@1 95.312 (97.259)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.3332 (0.3332) ([0.195]+[0.138])	Prec@1 93.750 (93.750)
 * Prec@1 91.410
current lr 1.00000e-02
Grad=  tensor(1.8043, device='cuda:0')
Epoch: [127][0/391]	Time 0.173 (0.173)	Data 0.130 (0.130)	Loss 0.1785 (0.1785) ([0.040]+[0.138])	Prec@1 99.219 (99.219)
Epoch: [127][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2771 (0.2236) ([0.139]+[0.138])	Prec@1 95.312 (97.123)
Epoch: [127][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1971 (0.2248) ([0.059]+[0.138])	Prec@1 98.438 (97.027)
Epoch: [127][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1999 (0.2285) ([0.062]+[0.138])	Prec@1 98.438 (96.948)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3854 (0.3854) ([0.247]+[0.138])	Prec@1 91.406 (91.406)
 * Prec@1 90.550
current lr 1.00000e-02
Grad=  tensor(8.0135, device='cuda:0')
Epoch: [128][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.2200 (0.2200) ([0.082]+[0.138])	Prec@1 98.438 (98.438)
Epoch: [128][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1888 (0.2172) ([0.051]+[0.138])	Prec@1 96.875 (97.254)
Epoch: [128][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2221 (0.2184) ([0.084]+[0.138])	Prec@1 96.094 (97.256)
Epoch: [128][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1927 (0.2226) ([0.055]+[0.138])	Prec@1 98.438 (97.101)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4223 (0.4223) ([0.285]+[0.138])	Prec@1 92.969 (92.969)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(2.0754, device='cuda:0')
Epoch: [129][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.1771 (0.1771) ([0.039]+[0.138])	Prec@1 99.219 (99.219)
Epoch: [129][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2504 (0.2202) ([0.113]+[0.137])	Prec@1 96.875 (97.169)
Epoch: [129][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2148 (0.2245) ([0.077]+[0.137])	Prec@1 96.094 (97.050)
Epoch: [129][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2047 (0.2253) ([0.067]+[0.137])	Prec@1 97.656 (97.031)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3790 (0.3790) ([0.242]+[0.137])	Prec@1 92.188 (92.188)
 * Prec@1 91.620
current lr 1.00000e-02
Grad=  tensor(6.7410, device='cuda:0')
Epoch: [130][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2279 (0.2279) ([0.091]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [130][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2001 (0.2158) ([0.063]+[0.137])	Prec@1 98.438 (97.192)
Epoch: [130][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2170 (0.2177) ([0.080]+[0.137])	Prec@1 96.875 (97.182)
Epoch: [130][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2259 (0.2198) ([0.089]+[0.137])	Prec@1 96.875 (97.090)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3394 (0.3394) ([0.203]+[0.137])	Prec@1 92.969 (92.969)
 * Prec@1 92.830
current lr 1.00000e-02
Grad=  tensor(3.8427, device='cuda:0')
Epoch: [131][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2018 (0.2018) ([0.065]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [131][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1982 (0.2212) ([0.061]+[0.137])	Prec@1 98.438 (97.208)
Epoch: [131][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2834 (0.2234) ([0.147]+[0.137])	Prec@1 95.312 (97.093)
Epoch: [131][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2684 (0.2272) ([0.132]+[0.137])	Prec@1 97.656 (96.989)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2924 (0.2924) ([0.156]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 91.940
current lr 1.00000e-02
Grad=  tensor(16.7816, device='cuda:0')
Epoch: [132][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.3019 (0.3019) ([0.165]+[0.137])	Prec@1 93.750 (93.750)
Epoch: [132][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1694 (0.2180) ([0.033]+[0.137])	Prec@1 99.219 (97.177)
Epoch: [132][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2291 (0.2262) ([0.092]+[0.137])	Prec@1 96.875 (96.937)
Epoch: [132][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2345 (0.2263) ([0.098]+[0.137])	Prec@1 95.312 (96.950)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3557 (0.3557) ([0.219]+[0.137])	Prec@1 95.312 (95.312)
 * Prec@1 91.020
current lr 1.00000e-02
Grad=  tensor(3.0033, device='cuda:0')
Epoch: [133][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.1783 (0.1783) ([0.042]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [133][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1789 (0.2136) ([0.042]+[0.137])	Prec@1 99.219 (97.478)
Epoch: [133][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2285 (0.2159) ([0.092]+[0.136])	Prec@1 97.656 (97.407)
Epoch: [133][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2657 (0.2213) ([0.129]+[0.136])	Prec@1 95.312 (97.176)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3505 (0.3505) ([0.214]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 91.860
current lr 1.00000e-02
Grad=  tensor(8.8769, device='cuda:0')
Epoch: [134][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1940 (0.1940) ([0.058]+[0.136])	Prec@1 98.438 (98.438)
Epoch: [134][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2559 (0.2187) ([0.120]+[0.136])	Prec@1 96.875 (97.362)
Epoch: [134][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2713 (0.2168) ([0.135]+[0.136])	Prec@1 95.312 (97.330)
Epoch: [134][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2182 (0.2185) ([0.082]+[0.136])	Prec@1 96.094 (97.267)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4402 (0.4402) ([0.304]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 90.730
current lr 1.00000e-02
Grad=  tensor(6.2417, device='cuda:0')
Epoch: [135][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2312 (0.2312) ([0.095]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [135][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2558 (0.2242) ([0.119]+[0.136])	Prec@1 94.531 (97.045)
Epoch: [135][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2153 (0.2246) ([0.079]+[0.136])	Prec@1 97.656 (96.933)
Epoch: [135][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2425 (0.2240) ([0.106]+[0.136])	Prec@1 96.875 (96.979)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.5004 (0.5004) ([0.364]+[0.136])	Prec@1 90.625 (90.625)
 * Prec@1 91.190
current lr 1.00000e-02
Grad=  tensor(6.6119, device='cuda:0')
Epoch: [136][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.2332 (0.2332) ([0.097]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [136][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2103 (0.2237) ([0.074]+[0.136])	Prec@1 98.438 (97.200)
Epoch: [136][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2094 (0.2253) ([0.073]+[0.136])	Prec@1 97.656 (97.097)
Epoch: [136][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2087 (0.2227) ([0.072]+[0.136])	Prec@1 96.875 (97.137)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5178 (0.5178) ([0.382]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 90.720
current lr 1.00000e-02
Grad=  tensor(8.3283, device='cuda:0')
Epoch: [137][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2493 (0.2493) ([0.113]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [137][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2256 (0.2234) ([0.089]+[0.136])	Prec@1 96.094 (96.976)
Epoch: [137][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2473 (0.2239) ([0.111]+[0.136])	Prec@1 95.312 (96.926)
Epoch: [137][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2302 (0.2294) ([0.094]+[0.136])	Prec@1 98.438 (96.813)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3886 (0.3886) ([0.252]+[0.136])	Prec@1 92.188 (92.188)
 * Prec@1 91.690
current lr 1.00000e-02
Grad=  tensor(5.5895, device='cuda:0')
Epoch: [138][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.1950 (0.1950) ([0.059]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [138][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1998 (0.2228) ([0.063]+[0.137])	Prec@1 97.656 (97.115)
Epoch: [138][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2143 (0.2282) ([0.078]+[0.137])	Prec@1 98.438 (96.906)
Epoch: [138][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2266 (0.2271) ([0.090]+[0.136])	Prec@1 95.312 (96.940)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3569 (0.3569) ([0.220]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 91.660
current lr 1.00000e-02
Grad=  tensor(5.5151, device='cuda:0')
Epoch: [139][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1951 (0.1951) ([0.059]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [139][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2577 (0.2216) ([0.121]+[0.137])	Prec@1 95.312 (97.084)
Epoch: [139][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1799 (0.2233) ([0.043]+[0.137])	Prec@1 99.219 (97.019)
Epoch: [139][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1941 (0.2227) ([0.058]+[0.137])	Prec@1 98.438 (97.090)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3113 (0.3113) ([0.175]+[0.137])	Prec@1 95.312 (95.312)
 * Prec@1 91.620
current lr 1.00000e-02
Grad=  tensor(6.2627, device='cuda:0')
Epoch: [140][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2009 (0.2009) ([0.064]+[0.137])	Prec@1 98.438 (98.438)
Epoch: [140][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2370 (0.2182) ([0.100]+[0.137])	Prec@1 96.875 (97.416)
Epoch: [140][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2344 (0.2235) ([0.098]+[0.137])	Prec@1 96.094 (97.198)
Epoch: [140][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1773 (0.2232) ([0.041]+[0.137])	Prec@1 99.219 (97.176)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5145 (0.5145) ([0.378]+[0.137])	Prec@1 89.062 (89.062)
 * Prec@1 91.410
current lr 1.00000e-02
Grad=  tensor(8.3792, device='cuda:0')
Epoch: [141][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2403 (0.2403) ([0.104]+[0.137])	Prec@1 96.094 (96.094)
Epoch: [141][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2105 (0.2121) ([0.074]+[0.136])	Prec@1 97.656 (97.478)
Epoch: [141][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1954 (0.2163) ([0.059]+[0.136])	Prec@1 97.656 (97.318)
Epoch: [141][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2341 (0.2211) ([0.098]+[0.137])	Prec@1 95.312 (97.140)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4657 (0.4657) ([0.329]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 91.300
current lr 1.00000e-02
Grad=  tensor(8.9439, device='cuda:0')
Epoch: [142][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2600 (0.2600) ([0.123]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [142][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2273 (0.2259) ([0.091]+[0.137])	Prec@1 96.094 (96.921)
Epoch: [142][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2394 (0.2195) ([0.103]+[0.136])	Prec@1 96.094 (97.236)
Epoch: [142][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2095 (0.2206) ([0.073]+[0.137])	Prec@1 97.656 (97.215)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4074 (0.4074) ([0.271]+[0.137])	Prec@1 94.531 (94.531)
 * Prec@1 90.960
current lr 1.00000e-02
Grad=  tensor(6.8993, device='cuda:0')
Epoch: [143][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2073 (0.2073) ([0.071]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [143][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2241 (0.2357) ([0.087]+[0.137])	Prec@1 96.094 (96.612)
Epoch: [143][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2029 (0.2299) ([0.066]+[0.137])	Prec@1 97.656 (96.821)
Epoch: [143][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2960 (0.2270) ([0.159]+[0.137])	Prec@1 92.969 (96.935)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4159 (0.4159) ([0.279]+[0.137])	Prec@1 92.969 (92.969)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(7.7369, device='cuda:0')
Epoch: [144][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2374 (0.2374) ([0.101]+[0.137])	Prec@1 96.094 (96.094)
Epoch: [144][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2842 (0.2279) ([0.147]+[0.137])	Prec@1 96.094 (96.898)
Epoch: [144][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2303 (0.2222) ([0.093]+[0.137])	Prec@1 97.656 (97.135)
Epoch: [144][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2549 (0.2230) ([0.118]+[0.137])	Prec@1 93.750 (97.109)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3460 (0.3460) ([0.209]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 91.240
current lr 1.00000e-02
Grad=  tensor(4.1376, device='cuda:0')
Epoch: [145][0/391]	Time 0.170 (0.170)	Data 0.125 (0.125)	Loss 0.1804 (0.1804) ([0.044]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [145][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2008 (0.2214) ([0.064]+[0.137])	Prec@1 97.656 (97.169)
Epoch: [145][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3183 (0.2209) ([0.182]+[0.137])	Prec@1 95.312 (97.170)
Epoch: [145][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2129 (0.2229) ([0.076]+[0.137])	Prec@1 96.875 (97.072)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.6380 (0.6380) ([0.501]+[0.137])	Prec@1 87.500 (87.500)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(3.6765, device='cuda:0')
Epoch: [146][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2137 (0.2137) ([0.077]+[0.137])	Prec@1 98.438 (98.438)
Epoch: [146][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2634 (0.2207) ([0.127]+[0.137])	Prec@1 96.094 (97.092)
Epoch: [146][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2155 (0.2176) ([0.079]+[0.137])	Prec@1 96.875 (97.283)
Epoch: [146][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2879 (0.2206) ([0.151]+[0.137])	Prec@1 96.094 (97.202)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4109 (0.4109) ([0.274]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 91.730
current lr 1.00000e-02
Grad=  tensor(3.8686, device='cuda:0')
Epoch: [147][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1805 (0.1805) ([0.044]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [147][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2639 (0.2168) ([0.127]+[0.137])	Prec@1 96.094 (97.447)
Epoch: [147][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2097 (0.2220) ([0.073]+[0.137])	Prec@1 96.875 (97.116)
Epoch: [147][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2943 (0.2249) ([0.157]+[0.137])	Prec@1 94.531 (96.994)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4344 (0.4344) ([0.297]+[0.137])	Prec@1 87.500 (87.500)
 * Prec@1 91.010
current lr 1.00000e-02
Grad=  tensor(5.1885, device='cuda:0')
Epoch: [148][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2191 (0.2191) ([0.082]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [148][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2203 (0.2139) ([0.083]+[0.137])	Prec@1 96.875 (97.347)
Epoch: [148][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1827 (0.2188) ([0.046]+[0.137])	Prec@1 99.219 (97.209)
Epoch: [148][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1957 (0.2205) ([0.058]+[0.137])	Prec@1 97.656 (97.127)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.2455 (0.2455) ([0.108]+[0.137])	Prec@1 96.875 (96.875)
 * Prec@1 91.240
current lr 1.00000e-02
Grad=  tensor(11.7273, device='cuda:0')
Epoch: [149][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2519 (0.2519) ([0.115]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [149][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2240 (0.2047) ([0.087]+[0.137])	Prec@1 97.656 (97.811)
Epoch: [149][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1935 (0.2112) ([0.057]+[0.137])	Prec@1 97.656 (97.536)
Epoch: [149][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2180 (0.2196) ([0.081]+[0.137])	Prec@1 97.656 (97.220)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3210 (0.3210) ([0.184]+[0.137])	Prec@1 94.531 (94.531)
 * Prec@1 91.300
current lr 1.00000e-02
Grad=  tensor(3.0304, device='cuda:0')
Epoch: [150][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.1827 (0.1827) ([0.045]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [150][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2162 (0.2230) ([0.079]+[0.137])	Prec@1 97.656 (97.215)
Epoch: [150][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2322 (0.2290) ([0.095]+[0.138])	Prec@1 96.875 (96.832)
Epoch: [150][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2098 (0.2284) ([0.072]+[0.138])	Prec@1 97.656 (96.872)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3884 (0.3884) ([0.251]+[0.138])	Prec@1 92.188 (92.188)
 * Prec@1 91.420
current lr 1.00000e-02
Grad=  tensor(5.4101, device='cuda:0')
Epoch: [151][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1989 (0.1989) ([0.061]+[0.138])	Prec@1 98.438 (98.438)
Epoch: [151][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2287 (0.2186) ([0.091]+[0.138])	Prec@1 97.656 (97.200)
Epoch: [151][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2176 (0.2208) ([0.080]+[0.138])	Prec@1 97.656 (97.194)
Epoch: [151][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1953 (0.2232) ([0.058]+[0.138])	Prec@1 98.438 (97.166)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3514 (0.3514) ([0.214]+[0.138])	Prec@1 92.188 (92.188)
 * Prec@1 91.140
current lr 1.00000e-02
Grad=  tensor(11.5080, device='cuda:0')
Epoch: [152][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2612 (0.2612) ([0.123]+[0.138])	Prec@1 96.875 (96.875)
Epoch: [152][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2723 (0.2129) ([0.135]+[0.138])	Prec@1 96.094 (97.347)
Epoch: [152][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2058 (0.2162) ([0.068]+[0.138])	Prec@1 98.438 (97.303)
Epoch: [152][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1679 (0.2212) ([0.030]+[0.138])	Prec@1 100.000 (97.129)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3637 (0.3637) ([0.226]+[0.138])	Prec@1 94.531 (94.531)
 * Prec@1 91.720
current lr 1.00000e-02
Grad=  tensor(9.1426, device='cuda:0')
Epoch: [153][0/391]	Time 0.175 (0.175)	Data 0.125 (0.125)	Loss 0.2270 (0.2270) ([0.089]+[0.138])	Prec@1 96.094 (96.094)
Epoch: [153][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2199 (0.2166) ([0.082]+[0.138])	Prec@1 96.094 (97.378)
Epoch: [153][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2201 (0.2193) ([0.082]+[0.138])	Prec@1 96.875 (97.186)
Epoch: [153][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1795 (0.2238) ([0.042]+[0.138])	Prec@1 98.438 (97.059)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3324 (0.3324) ([0.195]+[0.138])	Prec@1 92.969 (92.969)
 * Prec@1 91.220
current lr 1.00000e-02
Grad=  tensor(3.9310, device='cuda:0')
Epoch: [154][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1992 (0.1992) ([0.061]+[0.138])	Prec@1 98.438 (98.438)
Epoch: [154][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2101 (0.2102) ([0.073]+[0.138])	Prec@1 97.656 (97.664)
Epoch: [154][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2680 (0.2142) ([0.130]+[0.138])	Prec@1 94.531 (97.423)
Epoch: [154][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2734 (0.2182) ([0.136]+[0.138])	Prec@1 96.094 (97.233)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4794 (0.4794) ([0.342]+[0.138])	Prec@1 93.750 (93.750)
 * Prec@1 91.540
current lr 1.00000e-02
Grad=  tensor(4.5352, device='cuda:0')
Epoch: [155][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2068 (0.2068) ([0.069]+[0.138])	Prec@1 97.656 (97.656)
Epoch: [155][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1651 (0.2020) ([0.028]+[0.137])	Prec@1 99.219 (97.857)
Epoch: [155][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2130 (0.2104) ([0.076]+[0.137])	Prec@1 98.438 (97.575)
Epoch: [155][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1962 (0.2191) ([0.059]+[0.138])	Prec@1 96.875 (97.285)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3768 (0.3768) ([0.239]+[0.138])	Prec@1 94.531 (94.531)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(4.8693, device='cuda:0')
Epoch: [156][0/391]	Time 0.171 (0.171)	Data 0.125 (0.125)	Loss 0.2034 (0.2034) ([0.066]+[0.138])	Prec@1 97.656 (97.656)
Epoch: [156][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2169 (0.2238) ([0.079]+[0.138])	Prec@1 96.875 (97.068)
Epoch: [156][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2002 (0.2278) ([0.062]+[0.138])	Prec@1 98.438 (96.929)
Epoch: [156][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2161 (0.2281) ([0.078]+[0.138])	Prec@1 96.875 (96.955)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3834 (0.3834) ([0.245]+[0.138])	Prec@1 92.188 (92.188)
 * Prec@1 91.810
current lr 1.00000e-02
Grad=  tensor(5.5145, device='cuda:0')
Epoch: [157][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2037 (0.2037) ([0.065]+[0.138])	Prec@1 98.438 (98.438)
Epoch: [157][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1912 (0.2170) ([0.053]+[0.138])	Prec@1 97.656 (97.331)
Epoch: [157][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2290 (0.2153) ([0.091]+[0.138])	Prec@1 95.312 (97.497)
Epoch: [157][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2107 (0.2178) ([0.073]+[0.138])	Prec@1 96.875 (97.389)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.5065 (0.5065) ([0.368]+[0.138])	Prec@1 92.969 (92.969)
 * Prec@1 91.340
current lr 1.00000e-02
Grad=  tensor(6.7068, device='cuda:0')
Epoch: [158][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.1978 (0.1978) ([0.060]+[0.138])	Prec@1 97.656 (97.656)
Epoch: [158][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2216 (0.2123) ([0.084]+[0.138])	Prec@1 97.656 (97.494)
Epoch: [158][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2297 (0.2186) ([0.092]+[0.138])	Prec@1 96.875 (97.279)
Epoch: [158][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2595 (0.2203) ([0.121]+[0.138])	Prec@1 96.875 (97.306)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3114 (0.3114) ([0.173]+[0.138])	Prec@1 94.531 (94.531)
 * Prec@1 91.790
current lr 1.00000e-02
Grad=  tensor(11.3809, device='cuda:0')
Epoch: [159][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2230 (0.2230) ([0.085]+[0.138])	Prec@1 96.875 (96.875)
Epoch: [159][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1693 (0.2148) ([0.031]+[0.138])	Prec@1 100.000 (97.432)
Epoch: [159][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2388 (0.2195) ([0.101]+[0.138])	Prec@1 96.875 (97.182)
Epoch: [159][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3196 (0.2234) ([0.182]+[0.138])	Prec@1 93.750 (97.098)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4960 (0.4960) ([0.358]+[0.138])	Prec@1 92.188 (92.188)
 * Prec@1 90.940
current lr 1.00000e-02
Grad=  tensor(12.1843, device='cuda:0')
Epoch: [160][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2773 (0.2773) ([0.139]+[0.138])	Prec@1 94.531 (94.531)
Epoch: [160][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2330 (0.2245) ([0.095]+[0.138])	Prec@1 96.875 (97.138)
Epoch: [160][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1835 (0.2263) ([0.045]+[0.138])	Prec@1 100.000 (97.027)
Epoch: [160][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2218 (0.2294) ([0.083]+[0.139])	Prec@1 97.656 (96.906)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4593 (0.4593) ([0.321]+[0.139])	Prec@1 89.844 (89.844)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(7.6309, device='cuda:0')
Epoch: [161][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2170 (0.2170) ([0.078]+[0.139])	Prec@1 96.875 (96.875)
Epoch: [161][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2367 (0.2257) ([0.098]+[0.139])	Prec@1 96.094 (97.084)
Epoch: [161][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1660 (0.2233) ([0.027]+[0.139])	Prec@1 100.000 (97.159)
Epoch: [161][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2123 (0.2260) ([0.074]+[0.139])	Prec@1 96.875 (97.075)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5048 (0.5048) ([0.366]+[0.139])	Prec@1 91.406 (91.406)
 * Prec@1 91.800
current lr 1.00000e-02
Grad=  tensor(5.8985, device='cuda:0')
Epoch: [162][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.2047 (0.2047) ([0.066]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [162][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1902 (0.2174) ([0.051]+[0.139])	Prec@1 98.438 (97.401)
Epoch: [162][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2699 (0.2232) ([0.131]+[0.139])	Prec@1 96.875 (97.225)
Epoch: [162][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2418 (0.2265) ([0.103]+[0.139])	Prec@1 96.875 (97.088)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4320 (0.4320) ([0.293]+[0.139])	Prec@1 90.625 (90.625)
 * Prec@1 90.810
current lr 1.00000e-02
Grad=  tensor(7.2713, device='cuda:0')
Epoch: [163][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2234 (0.2234) ([0.084]+[0.139])	Prec@1 96.094 (96.094)
Epoch: [163][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1887 (0.2147) ([0.050]+[0.139])	Prec@1 98.438 (97.463)
Epoch: [163][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1912 (0.2169) ([0.052]+[0.139])	Prec@1 98.438 (97.334)
Epoch: [163][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2114 (0.2214) ([0.072]+[0.139])	Prec@1 98.438 (97.210)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3403 (0.3403) ([0.202]+[0.139])	Prec@1 92.969 (92.969)
 * Prec@1 91.700
current lr 1.00000e-02
Grad=  tensor(5.4042, device='cuda:0')
Epoch: [164][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1939 (0.1939) ([0.055]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [164][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2851 (0.2222) ([0.146]+[0.139])	Prec@1 93.750 (97.107)
Epoch: [164][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2205 (0.2277) ([0.081]+[0.139])	Prec@1 97.656 (96.972)
Epoch: [164][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2203 (0.2266) ([0.081]+[0.139])	Prec@1 97.656 (97.031)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4364 (0.4364) ([0.297]+[0.139])	Prec@1 90.625 (90.625)
 * Prec@1 91.430
current lr 1.00000e-02
Grad=  tensor(10.7280, device='cuda:0')
Epoch: [165][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.2392 (0.2392) ([0.100]+[0.139])	Prec@1 96.094 (96.094)
Epoch: [165][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2014 (0.2172) ([0.062]+[0.139])	Prec@1 98.438 (97.409)
Epoch: [165][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2006 (0.2178) ([0.062]+[0.139])	Prec@1 98.438 (97.357)
Epoch: [165][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2032 (0.2207) ([0.064]+[0.139])	Prec@1 99.219 (97.262)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3609 (0.3609) ([0.222]+[0.139])	Prec@1 92.969 (92.969)
 * Prec@1 92.210
current lr 1.00000e-02
Grad=  tensor(5.8000, device='cuda:0')
Epoch: [166][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.1941 (0.1941) ([0.055]+[0.139])	Prec@1 97.656 (97.656)
Epoch: [166][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2342 (0.2058) ([0.095]+[0.139])	Prec@1 96.094 (97.834)
Epoch: [166][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2408 (0.2110) ([0.102]+[0.139])	Prec@1 98.438 (97.683)
Epoch: [166][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2308 (0.2175) ([0.092]+[0.139])	Prec@1 96.875 (97.399)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5499 (0.5499) ([0.411]+[0.139])	Prec@1 89.062 (89.062)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(12.4249, device='cuda:0')
Epoch: [167][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2578 (0.2578) ([0.119]+[0.139])	Prec@1 93.750 (93.750)
Epoch: [167][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2021 (0.2120) ([0.063]+[0.139])	Prec@1 99.219 (97.502)
Epoch: [167][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1922 (0.2152) ([0.054]+[0.139])	Prec@1 97.656 (97.392)
Epoch: [167][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2450 (0.2174) ([0.106]+[0.139])	Prec@1 97.656 (97.394)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5111 (0.5111) ([0.372]+[0.139])	Prec@1 91.406 (91.406)
 * Prec@1 90.950
current lr 1.00000e-02
Grad=  tensor(14.4907, device='cuda:0')
Epoch: [168][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.3247 (0.3247) ([0.186]+[0.139])	Prec@1 93.750 (93.750)
Epoch: [168][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2075 (0.2189) ([0.068]+[0.139])	Prec@1 96.875 (97.231)
Epoch: [168][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1928 (0.2219) ([0.054]+[0.139])	Prec@1 97.656 (97.163)
Epoch: [168][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2761 (0.2260) ([0.137]+[0.139])	Prec@1 94.531 (97.124)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5519 (0.5519) ([0.413]+[0.139])	Prec@1 91.406 (91.406)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(1.9205, device='cuda:0')
Epoch: [169][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1734 (0.1734) ([0.034]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [169][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1981 (0.2143) ([0.059]+[0.139])	Prec@1 97.656 (97.486)
Epoch: [169][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2546 (0.2206) ([0.115]+[0.139])	Prec@1 96.094 (97.163)
Epoch: [169][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2828 (0.2230) ([0.144]+[0.139])	Prec@1 96.094 (97.109)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4197 (0.4197) ([0.280]+[0.139])	Prec@1 92.188 (92.188)
 * Prec@1 92.370
current lr 1.00000e-02
Grad=  tensor(9.6263, device='cuda:0')
Epoch: [170][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2207 (0.2207) ([0.081]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [170][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2181 (0.2104) ([0.079]+[0.139])	Prec@1 97.656 (97.633)
Epoch: [170][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2874 (0.2209) ([0.148]+[0.139])	Prec@1 96.875 (97.314)
Epoch: [170][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2150 (0.2214) ([0.076]+[0.139])	Prec@1 97.656 (97.262)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4777 (0.4777) ([0.339]+[0.139])	Prec@1 94.531 (94.531)
 * Prec@1 91.660
current lr 1.00000e-02
Grad=  tensor(3.0720, device='cuda:0')
Epoch: [171][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2118 (0.2118) ([0.073]+[0.139])	Prec@1 97.656 (97.656)
Epoch: [171][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2385 (0.2184) ([0.099]+[0.139])	Prec@1 97.656 (97.331)
Epoch: [171][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2944 (0.2181) ([0.155]+[0.139])	Prec@1 93.750 (97.384)
Epoch: [171][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2083 (0.2187) ([0.069]+[0.139])	Prec@1 98.438 (97.332)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5114 (0.5114) ([0.372]+[0.139])	Prec@1 90.625 (90.625)
 * Prec@1 91.660
current lr 1.00000e-02
Grad=  tensor(6.5390, device='cuda:0')
Epoch: [172][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2307 (0.2307) ([0.092]+[0.139])	Prec@1 97.656 (97.656)
Epoch: [172][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2470 (0.2068) ([0.108]+[0.139])	Prec@1 96.094 (97.641)
Epoch: [172][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1846 (0.2118) ([0.046]+[0.139])	Prec@1 99.219 (97.586)
Epoch: [172][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2096 (0.2156) ([0.071]+[0.139])	Prec@1 97.656 (97.456)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4971 (0.4971) ([0.358]+[0.139])	Prec@1 91.406 (91.406)
 * Prec@1 90.780
current lr 1.00000e-02
Grad=  tensor(10.3977, device='cuda:0')
Epoch: [173][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.2157 (0.2157) ([0.077]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [173][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2734 (0.2233) ([0.134]+[0.139])	Prec@1 91.406 (97.146)
Epoch: [173][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1719 (0.2253) ([0.033]+[0.139])	Prec@1 100.000 (97.065)
Epoch: [173][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2978 (0.2247) ([0.159]+[0.139])	Prec@1 95.312 (97.098)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3821 (0.3821) ([0.243]+[0.139])	Prec@1 92.188 (92.188)
 * Prec@1 90.600
current lr 1.00000e-02
Grad=  tensor(3.5036, device='cuda:0')
Epoch: [174][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1979 (0.1979) ([0.059]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [174][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2315 (0.2197) ([0.092]+[0.139])	Prec@1 96.094 (97.440)
Epoch: [174][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2302 (0.2202) ([0.091]+[0.139])	Prec@1 96.094 (97.345)
Epoch: [174][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2342 (0.2233) ([0.095]+[0.139])	Prec@1 98.438 (97.205)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5068 (0.5068) ([0.367]+[0.139])	Prec@1 91.406 (91.406)
 * Prec@1 90.700
current lr 1.00000e-02
Grad=  tensor(8.8189, device='cuda:0')
Epoch: [175][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.2541 (0.2541) ([0.115]+[0.139])	Prec@1 96.094 (96.094)
Epoch: [175][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2662 (0.2155) ([0.127]+[0.139])	Prec@1 92.969 (97.502)
Epoch: [175][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2155 (0.2157) ([0.076]+[0.139])	Prec@1 97.656 (97.446)
Epoch: [175][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1784 (0.2175) ([0.039]+[0.139])	Prec@1 99.219 (97.404)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5306 (0.5306) ([0.391]+[0.139])	Prec@1 90.625 (90.625)
 * Prec@1 90.490
current lr 1.00000e-02
Grad=  tensor(8.5346, device='cuda:0')
Epoch: [176][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2278 (0.2278) ([0.088]+[0.139])	Prec@1 96.875 (96.875)
Epoch: [176][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2041 (0.2136) ([0.065]+[0.139])	Prec@1 97.656 (97.579)
Epoch: [176][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1910 (0.2161) ([0.052]+[0.139])	Prec@1 98.438 (97.474)
Epoch: [176][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2163 (0.2218) ([0.077]+[0.140])	Prec@1 98.438 (97.251)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4365 (0.4365) ([0.297]+[0.140])	Prec@1 93.750 (93.750)
 * Prec@1 91.020
current lr 1.00000e-02
Grad=  tensor(3.6326, device='cuda:0')
Epoch: [177][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1773 (0.1773) ([0.038]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [177][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2068 (0.2185) ([0.067]+[0.140])	Prec@1 97.656 (97.300)
Epoch: [177][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1804 (0.2223) ([0.041]+[0.140])	Prec@1 99.219 (97.201)
Epoch: [177][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2130 (0.2213) ([0.073]+[0.140])	Prec@1 97.656 (97.233)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5329 (0.5329) ([0.393]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 90.850
current lr 1.00000e-02
Grad=  tensor(8.3516, device='cuda:0')
Epoch: [178][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2127 (0.2127) ([0.073]+[0.140])	Prec@1 96.875 (96.875)
Epoch: [178][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1776 (0.2131) ([0.038]+[0.140])	Prec@1 99.219 (97.486)
Epoch: [178][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2730 (0.2154) ([0.133]+[0.140])	Prec@1 95.312 (97.470)
Epoch: [178][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1909 (0.2192) ([0.051]+[0.140])	Prec@1 98.438 (97.324)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3970 (0.3970) ([0.257]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 91.790
current lr 1.00000e-02
Grad=  tensor(12.7824, device='cuda:0')
Epoch: [179][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.2659 (0.2659) ([0.126]+[0.140])	Prec@1 96.094 (96.094)
Epoch: [179][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2485 (0.2152) ([0.109]+[0.140])	Prec@1 97.656 (97.455)
Epoch: [179][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1660 (0.2185) ([0.026]+[0.140])	Prec@1 99.219 (97.373)
Epoch: [179][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2589 (0.2198) ([0.119]+[0.140])	Prec@1 95.312 (97.311)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4429 (0.4429) ([0.303]+[0.140])	Prec@1 94.531 (94.531)
 * Prec@1 91.920
current lr 1.00000e-02
Grad=  tensor(4.9841, device='cuda:0')
Epoch: [180][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2083 (0.2083) ([0.068]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [180][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2154 (0.2149) ([0.075]+[0.140])	Prec@1 98.438 (97.486)
Epoch: [180][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2067 (0.2186) ([0.067]+[0.140])	Prec@1 97.656 (97.376)
Epoch: [180][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1918 (0.2232) ([0.052]+[0.140])	Prec@1 99.219 (97.267)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4432 (0.4432) ([0.303]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 91.870
current lr 1.00000e-02
Grad=  tensor(10.2932, device='cuda:0')
Epoch: [181][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2435 (0.2435) ([0.103]+[0.140])	Prec@1 95.312 (95.312)
Epoch: [181][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2016 (0.2218) ([0.061]+[0.140])	Prec@1 98.438 (97.277)
Epoch: [181][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2432 (0.2237) ([0.103]+[0.140])	Prec@1 96.094 (97.201)
Epoch: [181][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2264 (0.2240) ([0.086]+[0.140])	Prec@1 97.656 (97.231)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3220 (0.3220) ([0.182]+[0.140])	Prec@1 96.094 (96.094)
 * Prec@1 91.870
current lr 1.00000e-02
Grad=  tensor(5.2548, device='cuda:0')
Epoch: [182][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2132 (0.2132) ([0.073]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [182][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2077 (0.2105) ([0.068]+[0.140])	Prec@1 97.656 (97.633)
Epoch: [182][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1882 (0.2131) ([0.048]+[0.140])	Prec@1 99.219 (97.645)
Epoch: [182][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1960 (0.2132) ([0.056]+[0.140])	Prec@1 98.438 (97.597)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4545 (0.4545) ([0.314]+[0.140])	Prec@1 93.750 (93.750)
 * Prec@1 91.290
current lr 1.00000e-02
Grad=  tensor(4.6716, device='cuda:0')
Epoch: [183][0/391]	Time 0.173 (0.173)	Data 0.123 (0.123)	Loss 0.2017 (0.2017) ([0.062]+[0.140])	Prec@1 97.656 (97.656)
Epoch: [183][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1950 (0.2254) ([0.055]+[0.140])	Prec@1 97.656 (97.177)
Epoch: [183][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2520 (0.2176) ([0.112]+[0.140])	Prec@1 97.656 (97.501)
Epoch: [183][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2773 (0.2205) ([0.137]+[0.140])	Prec@1 96.875 (97.384)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.2939 (0.2939) ([0.154]+[0.140])	Prec@1 96.094 (96.094)
 * Prec@1 92.120
current lr 1.00000e-02
Grad=  tensor(9.2636, device='cuda:0')
Epoch: [184][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2177 (0.2177) ([0.078]+[0.140])	Prec@1 96.875 (96.875)
Epoch: [184][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2482 (0.2125) ([0.109]+[0.140])	Prec@1 97.656 (97.525)
Epoch: [184][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1781 (0.2123) ([0.039]+[0.140])	Prec@1 99.219 (97.470)
Epoch: [184][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2163 (0.2139) ([0.077]+[0.140])	Prec@1 96.875 (97.438)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3302 (0.3302) ([0.190]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 91.340
current lr 1.00000e-02
Grad=  tensor(3.8342, device='cuda:0')
Epoch: [185][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1928 (0.1928) ([0.053]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [185][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1912 (0.2082) ([0.051]+[0.140])	Prec@1 97.656 (97.780)
Epoch: [185][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2367 (0.2096) ([0.097]+[0.140])	Prec@1 96.875 (97.672)
Epoch: [185][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1779 (0.2130) ([0.038]+[0.140])	Prec@1 98.438 (97.547)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5419 (0.5419) ([0.402]+[0.140])	Prec@1 90.625 (90.625)
 * Prec@1 90.850
current lr 1.00000e-02
Grad=  tensor(12.7008, device='cuda:0')
Epoch: [186][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2633 (0.2633) ([0.123]+[0.140])	Prec@1 95.312 (95.312)
Epoch: [186][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1827 (0.2209) ([0.043]+[0.140])	Prec@1 99.219 (97.285)
Epoch: [186][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1868 (0.2256) ([0.047]+[0.140])	Prec@1 97.656 (97.163)
Epoch: [186][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1906 (0.2241) ([0.051]+[0.140])	Prec@1 98.438 (97.184)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4111 (0.4111) ([0.271]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 91.500
current lr 1.00000e-02
Grad=  tensor(20.3123, device='cuda:0')
Epoch: [187][0/391]	Time 0.171 (0.171)	Data 0.122 (0.122)	Loss 0.2994 (0.2994) ([0.159]+[0.140])	Prec@1 93.750 (93.750)
Epoch: [187][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2288 (0.2188) ([0.089]+[0.140])	Prec@1 96.094 (97.293)
Epoch: [187][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2183 (0.2207) ([0.078]+[0.140])	Prec@1 97.656 (97.271)
Epoch: [187][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1895 (0.2224) ([0.049]+[0.140])	Prec@1 98.438 (97.181)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4194 (0.4194) ([0.279]+[0.140])	Prec@1 93.750 (93.750)
 * Prec@1 91.430
current lr 1.00000e-02
Grad=  tensor(2.0150, device='cuda:0')
Epoch: [188][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1611 (0.1611) ([0.021]+[0.140])	Prec@1 100.000 (100.000)
Epoch: [188][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1753 (0.2172) ([0.035]+[0.140])	Prec@1 100.000 (97.563)
Epoch: [188][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1946 (0.2157) ([0.054]+[0.140])	Prec@1 98.438 (97.563)
Epoch: [188][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2123 (0.2159) ([0.072]+[0.140])	Prec@1 97.656 (97.493)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4612 (0.4612) ([0.321]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 91.600
current lr 1.00000e-02
Grad=  tensor(3.5763, device='cuda:0')
Epoch: [189][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1927 (0.1927) ([0.053]+[0.140])	Prec@1 97.656 (97.656)
Epoch: [189][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1981 (0.2107) ([0.058]+[0.140])	Prec@1 97.656 (97.502)
Epoch: [189][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2190 (0.2094) ([0.079]+[0.140])	Prec@1 96.875 (97.602)
Epoch: [189][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1950 (0.2115) ([0.055]+[0.140])	Prec@1 97.656 (97.558)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3528 (0.3528) ([0.213]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 91.030
current lr 1.00000e-02
Grad=  tensor(6.3490, device='cuda:0')
Epoch: [190][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1913 (0.1913) ([0.051]+[0.140])	Prec@1 96.875 (96.875)
Epoch: [190][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1820 (0.2182) ([0.042]+[0.140])	Prec@1 99.219 (97.440)
Epoch: [190][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3158 (0.2162) ([0.176]+[0.140])	Prec@1 93.750 (97.493)
Epoch: [190][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2366 (0.2203) ([0.097]+[0.140])	Prec@1 94.531 (97.270)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4831 (0.4831) ([0.343]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 91.580
current lr 1.00000e-02
Grad=  tensor(9.4256, device='cuda:0')
Epoch: [191][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2080 (0.2080) ([0.068]+[0.140])	Prec@1 97.656 (97.656)
Epoch: [191][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2721 (0.2189) ([0.132]+[0.140])	Prec@1 96.875 (97.378)
Epoch: [191][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2245 (0.2176) ([0.085]+[0.140])	Prec@1 96.094 (97.373)
Epoch: [191][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2340 (0.2218) ([0.094]+[0.140])	Prec@1 96.094 (97.202)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3994 (0.3994) ([0.259]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 90.900
current lr 1.00000e-02
Grad=  tensor(12.0585, device='cuda:0')
Epoch: [192][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2366 (0.2366) ([0.096]+[0.140])	Prec@1 94.531 (94.531)
Epoch: [192][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2830 (0.2257) ([0.142]+[0.141])	Prec@1 93.750 (97.061)
Epoch: [192][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1987 (0.2238) ([0.058]+[0.140])	Prec@1 97.656 (97.143)
Epoch: [192][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2281 (0.2251) ([0.088]+[0.141])	Prec@1 96.875 (97.145)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.5371 (0.5371) ([0.397]+[0.140])	Prec@1 89.062 (89.062)
 * Prec@1 90.850
current lr 1.00000e-02
Grad=  tensor(3.9903, device='cuda:0')
Epoch: [193][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1841 (0.1841) ([0.044]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [193][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2385 (0.2173) ([0.098]+[0.140])	Prec@1 96.875 (97.494)
Epoch: [193][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1694 (0.2173) ([0.029]+[0.140])	Prec@1 99.219 (97.423)
Epoch: [193][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2094 (0.2166) ([0.069]+[0.140])	Prec@1 97.656 (97.454)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4106 (0.4106) ([0.270]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 91.390
current lr 1.00000e-02
Grad=  tensor(8.7298, device='cuda:0')
Epoch: [194][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2290 (0.2290) ([0.089]+[0.140])	Prec@1 96.875 (96.875)
Epoch: [194][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1794 (0.2075) ([0.039]+[0.140])	Prec@1 99.219 (97.842)
Epoch: [194][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1597 (0.2107) ([0.020]+[0.140])	Prec@1 100.000 (97.730)
Epoch: [194][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2768 (0.2136) ([0.137]+[0.140])	Prec@1 94.531 (97.558)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3561 (0.3561) ([0.216]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 91.080
current lr 1.00000e-02
Grad=  tensor(2.9328, device='cuda:0')
Epoch: [195][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1748 (0.1748) ([0.034]+[0.140])	Prec@1 99.219 (99.219)
Epoch: [195][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1863 (0.2086) ([0.046]+[0.140])	Prec@1 98.438 (97.649)
Epoch: [195][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1792 (0.2155) ([0.039]+[0.140])	Prec@1 98.438 (97.481)
Epoch: [195][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2444 (0.2168) ([0.104]+[0.140])	Prec@1 96.875 (97.443)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5013 (0.5013) ([0.361]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 91.360
current lr 1.00000e-02
Grad=  tensor(10.8874, device='cuda:0')
Epoch: [196][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2688 (0.2688) ([0.129]+[0.140])	Prec@1 95.312 (95.312)
Epoch: [196][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2075 (0.2094) ([0.067]+[0.140])	Prec@1 96.875 (97.625)
Epoch: [196][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2606 (0.2169) ([0.120]+[0.140])	Prec@1 96.094 (97.318)
Epoch: [196][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1725 (0.2206) ([0.032]+[0.140])	Prec@1 99.219 (97.280)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5496 (0.5496) ([0.409]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 91.270
current lr 1.00000e-02
Grad=  tensor(4.8182, device='cuda:0')
Epoch: [197][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.2027 (0.2027) ([0.062]+[0.140])	Prec@1 99.219 (99.219)
Epoch: [197][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2030 (0.2136) ([0.063]+[0.140])	Prec@1 99.219 (97.502)
Epoch: [197][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1642 (0.2144) ([0.024]+[0.140])	Prec@1 99.219 (97.520)
Epoch: [197][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2689 (0.2159) ([0.128]+[0.140])	Prec@1 96.875 (97.498)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3792 (0.3792) ([0.239]+[0.140])	Prec@1 95.312 (95.312)
 * Prec@1 91.790
current lr 1.00000e-02
Grad=  tensor(7.5691, device='cuda:0')
Epoch: [198][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.2233 (0.2233) ([0.083]+[0.140])	Prec@1 97.656 (97.656)
Epoch: [198][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1802 (0.2112) ([0.040]+[0.140])	Prec@1 99.219 (97.602)
Epoch: [198][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2030 (0.2130) ([0.063]+[0.140])	Prec@1 97.656 (97.516)
Epoch: [198][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2524 (0.2144) ([0.112]+[0.140])	Prec@1 95.312 (97.508)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4138 (0.4138) ([0.274]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 91.260
current lr 1.00000e-02
Grad=  tensor(6.7904, device='cuda:0')
Epoch: [199][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.1988 (0.1988) ([0.059]+[0.140])	Prec@1 98.438 (98.438)
Epoch: [199][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2356 (0.2229) ([0.095]+[0.140])	Prec@1 96.875 (97.161)
Epoch: [199][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1917 (0.2208) ([0.051]+[0.140])	Prec@1 99.219 (97.338)
Epoch: [199][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2132 (0.2236) ([0.073]+[0.141])	Prec@1 97.656 (97.168)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3182 (0.3182) ([0.178]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 91.360
current lr 1.00000e-02
Grad=  tensor(4.4626, device='cuda:0')
Epoch: [200][0/391]	Time 0.175 (0.175)	Data 0.127 (0.127)	Loss 0.1948 (0.1948) ([0.054]+[0.141])	Prec@1 98.438 (98.438)
Epoch: [200][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2073 (0.2175) ([0.067]+[0.141])	Prec@1 98.438 (97.587)
Epoch: [200][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1991 (0.2176) ([0.059]+[0.141])	Prec@1 98.438 (97.489)
Epoch: [200][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2095 (0.2208) ([0.069]+[0.141])	Prec@1 97.656 (97.342)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3488 (0.3488) ([0.208]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 92.220
current lr 1.00000e-02
Grad=  tensor(8.0828, device='cuda:0')
Epoch: [201][0/391]	Time 0.174 (0.174)	Data 0.125 (0.125)	Loss 0.2042 (0.2042) ([0.064]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [201][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1859 (0.2117) ([0.045]+[0.141])	Prec@1 98.438 (97.672)
Epoch: [201][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2321 (0.2129) ([0.092]+[0.141])	Prec@1 95.312 (97.617)
Epoch: [201][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1760 (0.2206) ([0.035]+[0.141])	Prec@1 97.656 (97.311)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4702 (0.4702) ([0.329]+[0.141])	Prec@1 92.969 (92.969)
 * Prec@1 91.130
current lr 1.00000e-02
Grad=  tensor(17.8415, device='cuda:0')
Epoch: [202][0/391]	Time 0.168 (0.168)	Data 0.124 (0.124)	Loss 0.3733 (0.3733) ([0.232]+[0.141])	Prec@1 93.750 (93.750)
Epoch: [202][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2133 (0.2155) ([0.072]+[0.141])	Prec@1 96.875 (97.525)
Epoch: [202][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1971 (0.2128) ([0.056]+[0.141])	Prec@1 97.656 (97.555)
Epoch: [202][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2411 (0.2152) ([0.100]+[0.141])	Prec@1 96.875 (97.488)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.5308 (0.5308) ([0.390]+[0.141])	Prec@1 92.188 (92.188)
 * Prec@1 91.200
current lr 1.00000e-02
Grad=  tensor(2.5019, device='cuda:0')
Epoch: [203][0/391]	Time 0.175 (0.175)	Data 0.126 (0.126)	Loss 0.1702 (0.1702) ([0.029]+[0.141])	Prec@1 99.219 (99.219)
Epoch: [203][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.1660 (0.2154) ([0.025]+[0.141])	Prec@1 99.219 (97.571)
Epoch: [203][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2253 (0.2173) ([0.084]+[0.141])	Prec@1 96.875 (97.493)
Epoch: [203][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1880 (0.2226) ([0.047]+[0.141])	Prec@1 98.438 (97.355)
Test: [0/79]	Time 0.133 (0.133)	Loss 0.4415 (0.4415) ([0.300]+[0.141])	Prec@1 90.625 (90.625)
 * Prec@1 91.800
current lr 1.00000e-02
Grad=  tensor(11.2964, device='cuda:0')
Epoch: [204][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2662 (0.2662) ([0.125]+[0.141])	Prec@1 96.094 (96.094)
Epoch: [204][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2209 (0.2057) ([0.080]+[0.141])	Prec@1 96.875 (97.989)
Epoch: [204][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1973 (0.2102) ([0.056]+[0.141])	Prec@1 98.438 (97.730)
Epoch: [204][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1754 (0.2164) ([0.034]+[0.141])	Prec@1 100.000 (97.529)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.5417 (0.5417) ([0.401]+[0.141])	Prec@1 90.625 (90.625)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(7.0375, device='cuda:0')
Epoch: [205][0/391]	Time 0.174 (0.174)	Data 0.125 (0.125)	Loss 0.2066 (0.2066) ([0.065]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [205][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1867 (0.2168) ([0.046]+[0.141])	Prec@1 97.656 (97.455)
Epoch: [205][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2120 (0.2198) ([0.071]+[0.141])	Prec@1 97.656 (97.361)
Epoch: [205][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2598 (0.2242) ([0.119]+[0.141])	Prec@1 96.875 (97.186)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5876 (0.5876) ([0.446]+[0.141])	Prec@1 90.625 (90.625)
 * Prec@1 90.660
current lr 1.00000e-02
Grad=  tensor(10.3549, device='cuda:0')
Epoch: [206][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.2584 (0.2584) ([0.117]+[0.141])	Prec@1 96.875 (96.875)
Epoch: [206][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1662 (0.2099) ([0.025]+[0.141])	Prec@1 99.219 (97.834)
Epoch: [206][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1840 (0.2127) ([0.043]+[0.141])	Prec@1 98.438 (97.664)
Epoch: [206][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2023 (0.2153) ([0.061]+[0.141])	Prec@1 97.656 (97.513)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4301 (0.4301) ([0.289]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 90.360
current lr 1.00000e-02
Grad=  tensor(7.2468, device='cuda:0')
Epoch: [207][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2127 (0.2127) ([0.071]+[0.141])	Prec@1 96.875 (96.875)
Epoch: [207][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2216 (0.2210) ([0.080]+[0.141])	Prec@1 97.656 (97.246)
Epoch: [207][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2272 (0.2207) ([0.086]+[0.141])	Prec@1 97.656 (97.373)
Epoch: [207][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2230 (0.2176) ([0.082]+[0.141])	Prec@1 96.875 (97.472)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4474 (0.4474) ([0.306]+[0.141])	Prec@1 89.844 (89.844)
 * Prec@1 91.060
current lr 1.00000e-02
Grad=  tensor(4.5343, device='cuda:0')
Epoch: [208][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.2006 (0.2006) ([0.060]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [208][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2352 (0.2090) ([0.094]+[0.141])	Prec@1 95.312 (97.795)
Epoch: [208][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2089 (0.2148) ([0.068]+[0.141])	Prec@1 97.656 (97.579)
Epoch: [208][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2064 (0.2168) ([0.065]+[0.141])	Prec@1 96.875 (97.519)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5588 (0.5588) ([0.418]+[0.141])	Prec@1 92.969 (92.969)
 * Prec@1 91.620
current lr 1.00000e-02
Grad=  tensor(11.5899, device='cuda:0')
Epoch: [209][0/391]	Time 0.173 (0.173)	Data 0.124 (0.124)	Loss 0.2535 (0.2535) ([0.112]+[0.141])	Prec@1 94.531 (94.531)
Epoch: [209][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.2446 (0.2136) ([0.104]+[0.141])	Prec@1 97.656 (97.509)
Epoch: [209][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2127 (0.2122) ([0.072]+[0.141])	Prec@1 97.656 (97.594)
Epoch: [209][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2011 (0.2133) ([0.060]+[0.141])	Prec@1 98.438 (97.563)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.4010 (0.4010) ([0.260]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 91.790
current lr 1.00000e-02
Grad=  tensor(10.4529, device='cuda:0')
Epoch: [210][0/391]	Time 0.177 (0.177)	Data 0.128 (0.128)	Loss 0.2022 (0.2022) ([0.062]+[0.141])	Prec@1 96.875 (96.875)
Epoch: [210][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1957 (0.2206) ([0.055]+[0.141])	Prec@1 99.219 (97.339)
Epoch: [210][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2026 (0.2192) ([0.062]+[0.141])	Prec@1 97.656 (97.357)
Epoch: [210][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1866 (0.2179) ([0.046]+[0.141])	Prec@1 98.438 (97.430)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4189 (0.4189) ([0.278]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 91.090
current lr 1.00000e-02
Grad=  tensor(9.2499, device='cuda:0')
Epoch: [211][0/391]	Time 0.178 (0.178)	Data 0.131 (0.131)	Loss 0.2491 (0.2491) ([0.108]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [211][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2724 (0.2231) ([0.132]+[0.141])	Prec@1 95.312 (97.262)
Epoch: [211][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1842 (0.2175) ([0.043]+[0.141])	Prec@1 99.219 (97.427)
Epoch: [211][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2292 (0.2189) ([0.089]+[0.141])	Prec@1 97.656 (97.394)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2757 (0.2757) ([0.135]+[0.141])	Prec@1 95.312 (95.312)
 * Prec@1 91.930
current lr 1.00000e-02
Grad=  tensor(1.4278, device='cuda:0')
Epoch: [212][0/391]	Time 0.170 (0.170)	Data 0.126 (0.126)	Loss 0.1663 (0.1663) ([0.026]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [212][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1859 (0.2003) ([0.046]+[0.140])	Prec@1 97.656 (98.020)
Epoch: [212][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1896 (0.2053) ([0.049]+[0.140])	Prec@1 98.438 (97.854)
Epoch: [212][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2546 (0.2115) ([0.114]+[0.140])	Prec@1 96.094 (97.584)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4231 (0.4231) ([0.283]+[0.140])	Prec@1 92.969 (92.969)
 * Prec@1 90.860
current lr 1.00000e-02
Grad=  tensor(5.1478, device='cuda:0')
Epoch: [213][0/391]	Time 0.169 (0.169)	Data 0.128 (0.128)	Loss 0.1993 (0.1993) ([0.059]+[0.140])	Prec@1 97.656 (97.656)
Epoch: [213][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2213 (0.2119) ([0.081]+[0.140])	Prec@1 95.312 (97.633)
Epoch: [213][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2342 (0.2154) ([0.094]+[0.140])	Prec@1 96.875 (97.497)
Epoch: [213][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2087 (0.2159) ([0.068]+[0.140])	Prec@1 97.656 (97.467)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4647 (0.4647) ([0.324]+[0.140])	Prec@1 91.406 (91.406)
 * Prec@1 90.720
current lr 1.00000e-02
Grad=  tensor(7.2458, device='cuda:0')
Epoch: [214][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2338 (0.2338) ([0.093]+[0.140])	Prec@1 96.875 (96.875)
Epoch: [214][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2192 (0.2193) ([0.079]+[0.141])	Prec@1 97.656 (97.331)
Epoch: [214][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1882 (0.2174) ([0.048]+[0.140])	Prec@1 99.219 (97.388)
Epoch: [214][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2376 (0.2178) ([0.097]+[0.141])	Prec@1 96.875 (97.399)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4519 (0.4519) ([0.311]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 92.330
current lr 1.00000e-02
Grad=  tensor(2.0549, device='cuda:0')
Epoch: [215][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1630 (0.1630) ([0.022]+[0.141])	Prec@1 99.219 (99.219)
Epoch: [215][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2302 (0.2136) ([0.090]+[0.140])	Prec@1 96.875 (97.664)
Epoch: [215][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1733 (0.2137) ([0.033]+[0.140])	Prec@1 98.438 (97.586)
Epoch: [215][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2438 (0.2175) ([0.103]+[0.141])	Prec@1 97.656 (97.404)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4546 (0.4546) ([0.314]+[0.141])	Prec@1 90.625 (90.625)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(6.0376, device='cuda:0')
Epoch: [216][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2183 (0.2183) ([0.078]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [216][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2440 (0.2175) ([0.103]+[0.141])	Prec@1 96.094 (97.525)
Epoch: [216][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1821 (0.2209) ([0.041]+[0.141])	Prec@1 99.219 (97.314)
Epoch: [216][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2254 (0.2229) ([0.085]+[0.141])	Prec@1 96.875 (97.215)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3106 (0.3106) ([0.170]+[0.141])	Prec@1 95.312 (95.312)
 * Prec@1 91.140
current lr 1.00000e-02
Grad=  tensor(5.5405, device='cuda:0')
Epoch: [217][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2218 (0.2218) ([0.081]+[0.141])	Prec@1 98.438 (98.438)
Epoch: [217][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2328 (0.2193) ([0.092]+[0.141])	Prec@1 96.875 (97.494)
Epoch: [217][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3356 (0.2169) ([0.195]+[0.141])	Prec@1 94.531 (97.555)
Epoch: [217][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2594 (0.2179) ([0.119]+[0.141])	Prec@1 93.750 (97.480)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3854 (0.3854) ([0.245]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 91.840
current lr 1.00000e-02
Grad=  tensor(1.5883, device='cuda:0')
Epoch: [218][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.1680 (0.1680) ([0.027]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [218][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1895 (0.2058) ([0.049]+[0.141])	Prec@1 97.656 (97.749)
Epoch: [218][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2149 (0.2098) ([0.074]+[0.141])	Prec@1 96.094 (97.617)
Epoch: [218][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2087 (0.2139) ([0.068]+[0.141])	Prec@1 97.656 (97.485)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3528 (0.3528) ([0.212]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 90.530
current lr 1.00000e-02
Grad=  tensor(5.9754, device='cuda:0')
Epoch: [219][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.1955 (0.1955) ([0.055]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [219][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2422 (0.2195) ([0.102]+[0.141])	Prec@1 96.875 (97.285)
Epoch: [219][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2131 (0.2186) ([0.072]+[0.141])	Prec@1 97.656 (97.361)
Epoch: [219][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2604 (0.2224) ([0.119]+[0.141])	Prec@1 95.312 (97.285)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.2898 (0.2898) ([0.149]+[0.141])	Prec@1 92.969 (92.969)
 * Prec@1 90.410
current lr 1.00000e-02
Grad=  tensor(12.1346, device='cuda:0')
Epoch: [220][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2188 (0.2188) ([0.078]+[0.141])	Prec@1 96.875 (96.875)
Epoch: [220][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2423 (0.2137) ([0.101]+[0.141])	Prec@1 96.094 (97.478)
Epoch: [220][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2245 (0.2138) ([0.084]+[0.141])	Prec@1 98.438 (97.501)
Epoch: [220][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2254 (0.2158) ([0.085]+[0.141])	Prec@1 96.094 (97.454)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4769 (0.4769) ([0.336]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(9.2807, device='cuda:0')
Epoch: [221][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2108 (0.2108) ([0.070]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [221][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1897 (0.2093) ([0.049]+[0.141])	Prec@1 98.438 (97.873)
Epoch: [221][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1647 (0.2118) ([0.024]+[0.141])	Prec@1 100.000 (97.683)
Epoch: [221][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1931 (0.2155) ([0.052]+[0.141])	Prec@1 97.656 (97.498)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4874 (0.4874) ([0.346]+[0.141])	Prec@1 90.625 (90.625)
 * Prec@1 90.710
current lr 1.00000e-02
Grad=  tensor(10.0519, device='cuda:0')
Epoch: [222][0/391]	Time 0.173 (0.173)	Data 0.123 (0.123)	Loss 0.2202 (0.2202) ([0.079]+[0.141])	Prec@1 98.438 (98.438)
Epoch: [222][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2340 (0.2142) ([0.093]+[0.141])	Prec@1 96.875 (97.625)
Epoch: [222][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1913 (0.2197) ([0.050]+[0.141])	Prec@1 98.438 (97.431)
Epoch: [222][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2755 (0.2211) ([0.134]+[0.141])	Prec@1 94.531 (97.342)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3809 (0.3809) ([0.239]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 91.630
current lr 1.00000e-02
Grad=  tensor(3.0660, device='cuda:0')
Epoch: [223][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1805 (0.1805) ([0.039]+[0.141])	Prec@1 98.438 (98.438)
Epoch: [223][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2402 (0.2166) ([0.099]+[0.141])	Prec@1 96.094 (97.525)
Epoch: [223][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2115 (0.2151) ([0.070]+[0.141])	Prec@1 96.875 (97.559)
Epoch: [223][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2129 (0.2184) ([0.071]+[0.141])	Prec@1 96.875 (97.417)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3955 (0.3955) ([0.254]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 91.790
current lr 1.00000e-02
Grad=  tensor(5.1560, device='cuda:0')
Epoch: [224][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.2215 (0.2215) ([0.080]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [224][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1703 (0.2031) ([0.029]+[0.141])	Prec@1 99.219 (97.919)
Epoch: [224][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2945 (0.2080) ([0.153]+[0.141])	Prec@1 94.531 (97.804)
Epoch: [224][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2261 (0.2141) ([0.085]+[0.141])	Prec@1 96.094 (97.602)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3626 (0.3626) ([0.221]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 91.330
current lr 1.00000e-02
Grad=  tensor(9.0418, device='cuda:0')
Epoch: [225][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2503 (0.2503) ([0.109]+[0.141])	Prec@1 96.094 (96.094)
Epoch: [225][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1651 (0.2166) ([0.024]+[0.141])	Prec@1 99.219 (97.424)
Epoch: [225][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2095 (0.2178) ([0.068]+[0.141])	Prec@1 96.875 (97.442)
Epoch: [225][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2039 (0.2203) ([0.063]+[0.141])	Prec@1 98.438 (97.340)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2950 (0.2950) ([0.154]+[0.141])	Prec@1 94.531 (94.531)
 * Prec@1 91.810
current lr 1.00000e-02
Grad=  tensor(8.5024, device='cuda:0')
Epoch: [226][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2015 (0.2015) ([0.060]+[0.141])	Prec@1 98.438 (98.438)
Epoch: [226][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2055 (0.2103) ([0.064]+[0.141])	Prec@1 97.656 (97.641)
Epoch: [226][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1846 (0.2139) ([0.043]+[0.141])	Prec@1 99.219 (97.544)
Epoch: [226][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1744 (0.2140) ([0.033]+[0.141])	Prec@1 99.219 (97.586)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4752 (0.4752) ([0.334]+[0.141])	Prec@1 91.406 (91.406)
 * Prec@1 90.310
current lr 1.00000e-02
Grad=  tensor(1.4696, device='cuda:0')
Epoch: [227][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1570 (0.1570) ([0.016]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [227][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2059 (0.2229) ([0.064]+[0.141])	Prec@1 99.219 (97.215)
Epoch: [227][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2341 (0.2209) ([0.093]+[0.141])	Prec@1 96.094 (97.303)
Epoch: [227][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2256 (0.2206) ([0.084]+[0.141])	Prec@1 96.094 (97.303)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3962 (0.3962) ([0.255]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 91.300
current lr 1.00000e-02
Grad=  tensor(10.8909, device='cuda:0')
Epoch: [228][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2445 (0.2445) ([0.103]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [228][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1809 (0.2145) ([0.039]+[0.142])	Prec@1 99.219 (97.494)
Epoch: [228][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2112 (0.2123) ([0.070]+[0.141])	Prec@1 97.656 (97.606)
Epoch: [228][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3150 (0.2175) ([0.173]+[0.142])	Prec@1 94.531 (97.441)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4206 (0.4206) ([0.279]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 91.820
current lr 1.00000e-02
Grad=  tensor(5.7103, device='cuda:0')
Epoch: [229][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1928 (0.1928) ([0.051]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [229][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1749 (0.2021) ([0.033]+[0.142])	Prec@1 97.656 (97.896)
Epoch: [229][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1772 (0.2076) ([0.036]+[0.141])	Prec@1 99.219 (97.645)
Epoch: [229][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2117 (0.2094) ([0.070]+[0.141])	Prec@1 98.438 (97.617)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4057 (0.4057) ([0.264]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 91.710
current lr 1.00000e-02
Grad=  tensor(3.6951, device='cuda:0')
Epoch: [230][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1775 (0.1775) ([0.036]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [230][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1971 (0.2148) ([0.056]+[0.141])	Prec@1 97.656 (97.649)
Epoch: [230][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1702 (0.2153) ([0.029]+[0.141])	Prec@1 99.219 (97.536)
Epoch: [230][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1954 (0.2145) ([0.054]+[0.141])	Prec@1 98.438 (97.578)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.5793 (0.5793) ([0.438]+[0.141])	Prec@1 89.844 (89.844)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(8.4666, device='cuda:0')
Epoch: [231][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2172 (0.2172) ([0.076]+[0.141])	Prec@1 97.656 (97.656)
Epoch: [231][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2008 (0.2150) ([0.059]+[0.142])	Prec@1 97.656 (97.548)
Epoch: [231][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2249 (0.2189) ([0.083]+[0.142])	Prec@1 96.094 (97.384)
Epoch: [231][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2093 (0.2220) ([0.067]+[0.142])	Prec@1 96.094 (97.306)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4163 (0.4163) ([0.274]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 92.200
current lr 1.00000e-02
Grad=  tensor(6.2139, device='cuda:0')
Epoch: [232][0/391]	Time 0.175 (0.175)	Data 0.126 (0.126)	Loss 0.1941 (0.1941) ([0.052]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [232][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1856 (0.2068) ([0.044]+[0.142])	Prec@1 99.219 (97.850)
Epoch: [232][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2463 (0.2131) ([0.105]+[0.142])	Prec@1 94.531 (97.617)
Epoch: [232][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2119 (0.2179) ([0.070]+[0.142])	Prec@1 97.656 (97.443)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3833 (0.3833) ([0.242]+[0.142])	Prec@1 92.969 (92.969)
 * Prec@1 91.810
current lr 1.00000e-02
Grad=  tensor(3.9043, device='cuda:0')
Epoch: [233][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.1718 (0.1718) ([0.030]+[0.142])	Prec@1 99.219 (99.219)
Epoch: [233][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1608 (0.1984) ([0.019]+[0.141])	Prec@1 100.000 (98.136)
Epoch: [233][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2109 (0.2016) ([0.070]+[0.141])	Prec@1 96.875 (98.045)
Epoch: [233][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2436 (0.2081) ([0.102]+[0.141])	Prec@1 96.875 (97.757)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5757 (0.5757) ([0.434]+[0.142])	Prec@1 88.281 (88.281)
 * Prec@1 91.300
current lr 1.00000e-02
Grad=  tensor(7.4969, device='cuda:0')
Epoch: [234][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2182 (0.2182) ([0.077]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [234][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2309 (0.2243) ([0.089]+[0.142])	Prec@1 97.656 (97.339)
Epoch: [234][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1963 (0.2175) ([0.055]+[0.142])	Prec@1 97.656 (97.528)
Epoch: [234][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1945 (0.2193) ([0.053]+[0.142])	Prec@1 97.656 (97.451)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3787 (0.3787) ([0.237]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 91.290
current lr 1.00000e-02
Grad=  tensor(8.7985, device='cuda:0')
Epoch: [235][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2241 (0.2241) ([0.082]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [235][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3054 (0.2166) ([0.164]+[0.142])	Prec@1 94.531 (97.424)
Epoch: [235][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1764 (0.2180) ([0.035]+[0.142])	Prec@1 97.656 (97.396)
Epoch: [235][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2806 (0.2189) ([0.139]+[0.142])	Prec@1 96.094 (97.379)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.6036 (0.6036) ([0.462]+[0.142])	Prec@1 89.844 (89.844)
 * Prec@1 90.840
current lr 1.00000e-02
Grad=  tensor(10.3567, device='cuda:0')
Epoch: [236][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2391 (0.2391) ([0.097]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [236][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2632 (0.2139) ([0.121]+[0.142])	Prec@1 96.875 (97.556)
Epoch: [236][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2124 (0.2161) ([0.071]+[0.142])	Prec@1 96.875 (97.442)
Epoch: [236][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2124 (0.2166) ([0.071]+[0.142])	Prec@1 96.875 (97.467)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4654 (0.4654) ([0.324]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 91.350
current lr 1.00000e-02
Grad=  tensor(6.6154, device='cuda:0')
Epoch: [237][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.2143 (0.2143) ([0.073]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [237][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2095 (0.2183) ([0.068]+[0.142])	Prec@1 97.656 (97.393)
Epoch: [237][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2216 (0.2169) ([0.080]+[0.142])	Prec@1 96.875 (97.384)
Epoch: [237][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1752 (0.2203) ([0.033]+[0.142])	Prec@1 99.219 (97.295)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3306 (0.3306) ([0.189]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 92.440
current lr 1.00000e-02
Grad=  tensor(9.5675, device='cuda:0')
Epoch: [238][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2130 (0.2130) ([0.071]+[0.142])	Prec@1 97.656 (97.656)
Epoch: [238][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2018 (0.2076) ([0.060]+[0.142])	Prec@1 97.656 (97.734)
Epoch: [238][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1808 (0.2119) ([0.039]+[0.142])	Prec@1 97.656 (97.621)
Epoch: [238][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1869 (0.2173) ([0.045]+[0.142])	Prec@1 98.438 (97.477)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.5047 (0.5047) ([0.363]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 91.490
current lr 1.00000e-02
Grad=  tensor(4.7246, device='cuda:0')
Epoch: [239][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.2238 (0.2238) ([0.082]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [239][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2334 (0.2028) ([0.092]+[0.141])	Prec@1 97.656 (98.151)
Epoch: [239][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1949 (0.2092) ([0.054]+[0.141])	Prec@1 98.438 (97.874)
Epoch: [239][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2051 (0.2091) ([0.064]+[0.141])	Prec@1 97.656 (97.825)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4289 (0.4289) ([0.287]+[0.141])	Prec@1 89.844 (89.844)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(15.2312, device='cuda:0')
Epoch: [240][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.2536 (0.2536) ([0.112]+[0.141])	Prec@1 95.312 (95.312)
Epoch: [240][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2840 (0.2197) ([0.142]+[0.142])	Prec@1 94.531 (97.355)
Epoch: [240][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1761 (0.2224) ([0.034]+[0.142])	Prec@1 99.219 (97.349)
Epoch: [240][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2352 (0.2217) ([0.094]+[0.142])	Prec@1 97.656 (97.360)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4387 (0.4387) ([0.297]+[0.142])	Prec@1 92.188 (92.188)
 * Prec@1 91.070
current lr 1.00000e-02
Grad=  tensor(4.8522, device='cuda:0')
Epoch: [241][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2099 (0.2099) ([0.068]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [241][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1587 (0.2093) ([0.017]+[0.141])	Prec@1 99.219 (97.703)
Epoch: [241][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2510 (0.2100) ([0.109]+[0.142])	Prec@1 97.656 (97.648)
Epoch: [241][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2047 (0.2151) ([0.063]+[0.142])	Prec@1 97.656 (97.485)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3388 (0.3388) ([0.197]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 92.190
current lr 1.00000e-02
Grad=  tensor(7.8857, device='cuda:0')
Epoch: [242][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.1924 (0.1924) ([0.051]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [242][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2128 (0.2117) ([0.071]+[0.142])	Prec@1 97.656 (97.548)
Epoch: [242][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2542 (0.2181) ([0.113]+[0.142])	Prec@1 96.875 (97.427)
Epoch: [242][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1985 (0.2208) ([0.057]+[0.142])	Prec@1 98.438 (97.290)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3516 (0.3516) ([0.210]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 91.610
current lr 1.00000e-02
Grad=  tensor(4.2722, device='cuda:0')
Epoch: [243][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1921 (0.1921) ([0.050]+[0.142])	Prec@1 98.438 (98.438)
Epoch: [243][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2077 (0.2110) ([0.066]+[0.142])	Prec@1 96.875 (97.765)
Epoch: [243][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3328 (0.2124) ([0.191]+[0.142])	Prec@1 93.750 (97.680)
Epoch: [243][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2230 (0.2174) ([0.081]+[0.142])	Prec@1 96.875 (97.539)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3965 (0.3965) ([0.255]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 91.620
current lr 1.00000e-02
Grad=  tensor(9.4108, device='cuda:0')
Epoch: [244][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2467 (0.2467) ([0.105]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [244][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1751 (0.2081) ([0.033]+[0.142])	Prec@1 99.219 (97.942)
Epoch: [244][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1824 (0.2109) ([0.041]+[0.142])	Prec@1 98.438 (97.827)
Epoch: [244][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1941 (0.2130) ([0.052]+[0.142])	Prec@1 97.656 (97.734)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4298 (0.4298) ([0.288]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 90.860
current lr 1.00000e-02
Grad=  tensor(8.2610, device='cuda:0')
Epoch: [245][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2163 (0.2163) ([0.075]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [245][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1849 (0.2017) ([0.044]+[0.141])	Prec@1 99.219 (98.089)
Epoch: [245][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2213 (0.2075) ([0.080]+[0.141])	Prec@1 97.656 (97.851)
Epoch: [245][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2117 (0.2134) ([0.070]+[0.142])	Prec@1 96.875 (97.623)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4186 (0.4186) ([0.277]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 91.110
current lr 1.00000e-02
Grad=  tensor(7.0209, device='cuda:0')
Epoch: [246][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2135 (0.2135) ([0.072]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [246][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1867 (0.2115) ([0.045]+[0.142])	Prec@1 98.438 (97.765)
Epoch: [246][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1983 (0.2093) ([0.057]+[0.141])	Prec@1 97.656 (97.773)
Epoch: [246][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2271 (0.2149) ([0.086]+[0.141])	Prec@1 96.875 (97.599)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4638 (0.4638) ([0.322]+[0.141])	Prec@1 92.969 (92.969)
 * Prec@1 91.390
current lr 1.00000e-02
Grad=  tensor(13.6591, device='cuda:0')
Epoch: [247][0/391]	Time 0.168 (0.168)	Data 0.127 (0.127)	Loss 0.2563 (0.2563) ([0.115]+[0.141])	Prec@1 94.531 (94.531)
Epoch: [247][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2026 (0.2078) ([0.061]+[0.141])	Prec@1 98.438 (97.703)
Epoch: [247][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2027 (0.2076) ([0.061]+[0.141])	Prec@1 97.656 (97.827)
Epoch: [247][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2150 (0.2138) ([0.074]+[0.141])	Prec@1 96.875 (97.654)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.4444 (0.4444) ([0.303]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(10.3010, device='cuda:0')
Epoch: [248][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.2233 (0.2233) ([0.082]+[0.142])	Prec@1 96.094 (96.094)
Epoch: [248][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2009 (0.2205) ([0.059]+[0.142])	Prec@1 98.438 (97.277)
Epoch: [248][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2243 (0.2184) ([0.083]+[0.142])	Prec@1 98.438 (97.411)
Epoch: [248][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2055 (0.2188) ([0.064]+[0.142])	Prec@1 97.656 (97.420)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3304 (0.3304) ([0.189]+[0.142])	Prec@1 94.531 (94.531)
 * Prec@1 91.560
current lr 1.00000e-02
Grad=  tensor(0.9353, device='cuda:0')
Epoch: [249][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1560 (0.1560) ([0.014]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [249][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2275 (0.2095) ([0.086]+[0.142])	Prec@1 96.875 (97.795)
Epoch: [249][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1803 (0.2103) ([0.039]+[0.142])	Prec@1 99.219 (97.734)
Epoch: [249][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2216 (0.2146) ([0.080]+[0.142])	Prec@1 97.656 (97.571)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.6404 (0.6404) ([0.499]+[0.142])	Prec@1 86.719 (86.719)
 * Prec@1 88.120
current lr 1.00000e-03
Grad=  tensor(8.9922, device='cuda:0')
Epoch: [250][0/391]	Time 0.168 (0.168)	Data 0.127 (0.127)	Loss 0.2691 (0.2691) ([0.127]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [250][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1938 (0.1966) ([0.054]+[0.140])	Prec@1 97.656 (98.082)
Epoch: [250][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1803 (0.1871) ([0.041]+[0.139])	Prec@1 98.438 (98.480)
Epoch: [250][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1536 (0.1815) ([0.014]+[0.139])	Prec@1 100.000 (98.687)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3843 (0.3843) ([0.245]+[0.139])	Prec@1 94.531 (94.531)
 * Prec@1 94.140
current lr 1.00000e-03
Grad=  tensor(0.7432, device='cuda:0')
Epoch: [251][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.1489 (0.1489) ([0.010]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [251][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1437 (0.1599) ([0.005]+[0.139])	Prec@1 100.000 (99.474)
Epoch: [251][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1518 (0.1599) ([0.013]+[0.139])	Prec@1 100.000 (99.475)
Epoch: [251][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1451 (0.1588) ([0.006]+[0.139])	Prec@1 100.000 (99.507)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4064 (0.4064) ([0.268]+[0.139])	Prec@1 94.531 (94.531)
 * Prec@1 94.330
current lr 1.00000e-03
Grad=  tensor(0.9736, device='cuda:0')
Epoch: [252][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1518 (0.1518) ([0.013]+[0.139])	Prec@1 99.219 (99.219)
Epoch: [252][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1492 (0.1544) ([0.011]+[0.139])	Prec@1 100.000 (99.629)
Epoch: [252][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1703 (0.1550) ([0.032]+[0.138])	Prec@1 98.438 (99.588)
Epoch: [252][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1461 (0.1544) ([0.008]+[0.138])	Prec@1 100.000 (99.605)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.3978 (0.3978) ([0.260]+[0.138])	Prec@1 95.312 (95.312)
 * Prec@1 94.350
current lr 1.00000e-03
Grad=  tensor(0.2717, device='cuda:0')
Epoch: [253][0/391]	Time 0.172 (0.172)	Data 0.131 (0.131)	Loss 0.1437 (0.1437) ([0.006]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [253][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1686 (0.1523) ([0.031]+[0.138])	Prec@1 98.438 (99.644)
Epoch: [253][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1408 (0.1512) ([0.003]+[0.138])	Prec@1 100.000 (99.681)
Epoch: [253][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1859 (0.1519) ([0.048]+[0.138])	Prec@1 98.438 (99.650)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4234 (0.4234) ([0.286]+[0.138])	Prec@1 94.531 (94.531)
 * Prec@1 94.450
current lr 1.00000e-03
Grad=  tensor(0.2958, device='cuda:0')
Epoch: [254][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.1437 (0.1437) ([0.006]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1556 (0.1486) ([0.018]+[0.137])	Prec@1 99.219 (99.783)
Epoch: [254][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1407 (0.1497) ([0.003]+[0.137])	Prec@1 100.000 (99.728)
Epoch: [254][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1411 (0.1493) ([0.004]+[0.137])	Prec@1 100.000 (99.740)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.4209 (0.4209) ([0.284]+[0.137])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-03
Grad=  tensor(3.3163, device='cuda:0')
Epoch: [255][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1528 (0.1528) ([0.016]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [255][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1605 (0.1480) ([0.024]+[0.137])	Prec@1 99.219 (99.783)
Epoch: [255][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1399 (0.1475) ([0.003]+[0.137])	Prec@1 100.000 (99.802)
Epoch: [255][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1468 (0.1475) ([0.010]+[0.137])	Prec@1 100.000 (99.803)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4324 (0.4324) ([0.296]+[0.137])	Prec@1 95.312 (95.312)
 * Prec@1 94.460
current lr 1.00000e-03
Grad=  tensor(0.6268, device='cuda:0')
Epoch: [256][0/391]	Time 0.169 (0.169)	Data 0.128 (0.128)	Loss 0.1446 (0.1446) ([0.008]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1432 (0.1468) ([0.007]+[0.136])	Prec@1 100.000 (99.776)
Epoch: [256][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1522 (0.1460) ([0.016]+[0.136])	Prec@1 99.219 (99.806)
Epoch: [256][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1406 (0.1456) ([0.004]+[0.136])	Prec@1 100.000 (99.818)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4387 (0.4387) ([0.303]+[0.136])	Prec@1 94.531 (94.531)
 * Prec@1 94.440
current lr 1.00000e-03
Grad=  tensor(2.6947, device='cuda:0')
Epoch: [257][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.1587 (0.1587) ([0.023]+[0.136])	Prec@1 99.219 (99.219)
Epoch: [257][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1417 (0.1454) ([0.006]+[0.136])	Prec@1 100.000 (99.830)
Epoch: [257][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1467 (0.1451) ([0.011]+[0.136])	Prec@1 100.000 (99.833)
Epoch: [257][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1435 (0.1446) ([0.008]+[0.136])	Prec@1 100.000 (99.844)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4476 (0.4476) ([0.312]+[0.135])	Prec@1 95.312 (95.312)
 * Prec@1 94.500
current lr 1.00000e-03
Grad=  tensor(0.5008, device='cuda:0')
Epoch: [258][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.1422 (0.1422) ([0.007]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1387 (0.1432) ([0.003]+[0.135])	Prec@1 100.000 (99.892)
Epoch: [258][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1378 (0.1433) ([0.003]+[0.135])	Prec@1 100.000 (99.876)
Epoch: [258][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1473 (0.1434) ([0.012]+[0.135])	Prec@1 99.219 (99.862)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4381 (0.4381) ([0.303]+[0.135])	Prec@1 94.531 (94.531)
 * Prec@1 94.380
current lr 1.00000e-03
Grad=  tensor(0.1936, device='cuda:0')
Epoch: [259][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1374 (0.1374) ([0.002]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [259][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1385 (0.1424) ([0.004]+[0.135])	Prec@1 100.000 (99.884)
Epoch: [259][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1447 (0.1424) ([0.010]+[0.135])	Prec@1 100.000 (99.883)
Epoch: [259][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1459 (0.1424) ([0.011]+[0.135])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4175 (0.4175) ([0.283]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 94.460
current lr 1.00000e-03
Grad=  tensor(0.5180, device='cuda:0')
Epoch: [260][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1412 (0.1412) ([0.007]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1431 (0.1428) ([0.009]+[0.134])	Prec@1 100.000 (99.876)
Epoch: [260][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1359 (0.1420) ([0.002]+[0.134])	Prec@1 100.000 (99.891)
Epoch: [260][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1408 (0.1417) ([0.007]+[0.134])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4082 (0.4082) ([0.274]+[0.134])	Prec@1 95.312 (95.312)
 * Prec@1 94.470
current lr 1.00000e-03
Grad=  tensor(0.2265, device='cuda:0')
Epoch: [261][0/391]	Time 0.162 (0.162)	Data 0.121 (0.121)	Loss 0.1368 (0.1368) ([0.003]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1414 (0.1411) ([0.008]+[0.134])	Prec@1 100.000 (99.853)
Epoch: [261][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1383 (0.1411) ([0.005]+[0.134])	Prec@1 100.000 (99.848)
Epoch: [261][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1369 (0.1406) ([0.003]+[0.134])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4383 (0.4383) ([0.305]+[0.133])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.1980, device='cuda:0')
Epoch: [262][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.1360 (0.1360) ([0.003]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1439 (0.1394) ([0.011]+[0.133])	Prec@1 99.219 (99.907)
Epoch: [262][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1400 (0.1394) ([0.007]+[0.133])	Prec@1 100.000 (99.907)
Epoch: [262][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1361 (0.1396) ([0.003]+[0.133])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4265 (0.4265) ([0.294]+[0.133])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-03
Grad=  tensor(0.2792, device='cuda:0')
Epoch: [263][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.1377 (0.1377) ([0.005]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1490 (0.1401) ([0.016]+[0.133])	Prec@1 99.219 (99.869)
Epoch: [263][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1353 (0.1393) ([0.003]+[0.133])	Prec@1 100.000 (99.880)
Epoch: [263][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1465 (0.1392) ([0.014]+[0.133])	Prec@1 99.219 (99.886)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4363 (0.4363) ([0.304]+[0.132])	Prec@1 94.531 (94.531)
 * Prec@1 94.530
current lr 1.00000e-03
Grad=  tensor(0.6782, device='cuda:0')
Epoch: [264][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.1417 (0.1417) ([0.009]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1576 (0.1384) ([0.025]+[0.132])	Prec@1 98.438 (99.892)
Epoch: [264][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1374 (0.1379) ([0.005]+[0.132])	Prec@1 100.000 (99.914)
Epoch: [264][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1433 (0.1379) ([0.011]+[0.132])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4435 (0.4435) ([0.312]+[0.132])	Prec@1 94.531 (94.531)
 * Prec@1 94.610
current lr 1.00000e-03
Grad=  tensor(0.2008, device='cuda:0')
Epoch: [265][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1345 (0.1345) ([0.003]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1335 (0.1370) ([0.002]+[0.132])	Prec@1 100.000 (99.954)
Epoch: [265][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1437 (0.1370) ([0.012]+[0.132])	Prec@1 100.000 (99.957)
Epoch: [265][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1358 (0.1373) ([0.004]+[0.132])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4371 (0.4371) ([0.306]+[0.131])	Prec@1 94.531 (94.531)
 * Prec@1 94.610
current lr 1.00000e-03
Grad=  tensor(0.7824, device='cuda:0')
Epoch: [266][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1380 (0.1380) ([0.007]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1388 (0.1370) ([0.007]+[0.131])	Prec@1 100.000 (99.923)
Epoch: [266][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1341 (0.1368) ([0.003]+[0.131])	Prec@1 100.000 (99.930)
Epoch: [266][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1347 (0.1365) ([0.004]+[0.131])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4309 (0.4309) ([0.300]+[0.131])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.2141, device='cuda:0')
Epoch: [267][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.1337 (0.1337) ([0.003]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1321 (0.1367) ([0.001]+[0.131])	Prec@1 100.000 (99.915)
Epoch: [267][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1371 (0.1364) ([0.006]+[0.131])	Prec@1 100.000 (99.930)
Epoch: [267][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1347 (0.1363) ([0.004]+[0.131])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4070 (0.4070) ([0.277]+[0.130])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-03
Grad=  tensor(0.3675, device='cuda:0')
Epoch: [268][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1346 (0.1346) ([0.004]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1336 (0.1353) ([0.003]+[0.130])	Prec@1 100.000 (99.923)
Epoch: [268][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1314 (0.1356) ([0.001]+[0.130])	Prec@1 100.000 (99.918)
Epoch: [268][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1345 (0.1355) ([0.004]+[0.130])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4374 (0.4374) ([0.307]+[0.130])	Prec@1 94.531 (94.531)
 * Prec@1 94.540
current lr 1.00000e-03
Grad=  tensor(2.1941, device='cuda:0')
Epoch: [269][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1394 (0.1394) ([0.009]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1372 (0.1344) ([0.007]+[0.130])	Prec@1 100.000 (99.938)
Epoch: [269][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1338 (0.1342) ([0.004]+[0.130])	Prec@1 100.000 (99.946)
Epoch: [269][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1318 (0.1345) ([0.002]+[0.130])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4171 (0.4171) ([0.288]+[0.129])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.1851, device='cuda:0')
Epoch: [270][0/391]	Time 0.170 (0.170)	Data 0.128 (0.128)	Loss 0.1315 (0.1315) ([0.002]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1344 (0.1347) ([0.005]+[0.129])	Prec@1 100.000 (99.923)
Epoch: [270][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1385 (0.1344) ([0.009]+[0.129])	Prec@1 100.000 (99.942)
Epoch: [270][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1345 (0.1341) ([0.005]+[0.129])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4256 (0.4256) ([0.297]+[0.129])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.1782, device='cuda:0')
Epoch: [271][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1301 (0.1301) ([0.001]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1346 (0.1339) ([0.006]+[0.129])	Prec@1 100.000 (99.907)
Epoch: [271][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1311 (0.1335) ([0.002]+[0.129])	Prec@1 100.000 (99.926)
Epoch: [271][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1325 (0.1333) ([0.004]+[0.129])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4140 (0.4140) ([0.286]+[0.128])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.2495, device='cuda:0')
Epoch: [272][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1310 (0.1310) ([0.003]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1295 (0.1326) ([0.001]+[0.128])	Prec@1 100.000 (99.938)
Epoch: [272][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1304 (0.1328) ([0.002]+[0.128])	Prec@1 100.000 (99.942)
Epoch: [272][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1297 (0.1326) ([0.002]+[0.128])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4224 (0.4224) ([0.294]+[0.128])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(1.9725, device='cuda:0')
Epoch: [273][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1400 (0.1400) ([0.012]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1308 (0.1330) ([0.003]+[0.128])	Prec@1 100.000 (99.961)
Epoch: [273][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1306 (0.1324) ([0.003]+[0.128])	Prec@1 100.000 (99.953)
Epoch: [273][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1374 (0.1323) ([0.010]+[0.128])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4351 (0.4351) ([0.308]+[0.127])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-03
Grad=  tensor(0.2727, device='cuda:0')
Epoch: [274][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1306 (0.1306) ([0.003]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1305 (0.1316) ([0.003]+[0.127])	Prec@1 100.000 (99.954)
Epoch: [274][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1301 (0.1316) ([0.003]+[0.127])	Prec@1 100.000 (99.934)
Epoch: [274][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1346 (0.1315) ([0.008]+[0.127])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4336 (0.4336) ([0.307]+[0.127])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.2382, device='cuda:0')
Epoch: [275][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1307 (0.1307) ([0.004]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1303 (0.1312) ([0.003]+[0.127])	Prec@1 100.000 (99.961)
Epoch: [275][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1277 (0.1311) ([0.001]+[0.127])	Prec@1 100.000 (99.949)
Epoch: [275][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1361 (0.1311) ([0.010]+[0.127])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4597 (0.4597) ([0.333]+[0.126])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.1878, device='cuda:0')
Epoch: [276][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1285 (0.1285) ([0.002]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1282 (0.1300) ([0.002]+[0.126])	Prec@1 100.000 (99.985)
Epoch: [276][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1315 (0.1301) ([0.005]+[0.126])	Prec@1 100.000 (99.965)
Epoch: [276][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1281 (0.1301) ([0.002]+[0.126])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4422 (0.4422) ([0.316]+[0.126])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.2150, device='cuda:0')
Epoch: [277][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1281 (0.1281) ([0.002]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1275 (0.1297) ([0.002]+[0.126])	Prec@1 100.000 (99.969)
Epoch: [277][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1284 (0.1295) ([0.003]+[0.126])	Prec@1 100.000 (99.969)
Epoch: [277][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1291 (0.1296) ([0.003]+[0.126])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4503 (0.4503) ([0.325]+[0.125])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.2833, device='cuda:0')
Epoch: [278][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1284 (0.1284) ([0.003]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1331 (0.1297) ([0.008]+[0.125])	Prec@1 100.000 (99.938)
Epoch: [278][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1297 (0.1294) ([0.004]+[0.125])	Prec@1 100.000 (99.949)
Epoch: [278][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1283 (0.1294) ([0.003]+[0.125])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4318 (0.4318) ([0.307]+[0.125])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.1905, device='cuda:0')
Epoch: [279][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1269 (0.1269) ([0.002]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1265 (0.1291) ([0.002]+[0.125])	Prec@1 100.000 (99.946)
Epoch: [279][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1268 (0.1286) ([0.002]+[0.125])	Prec@1 100.000 (99.961)
Epoch: [279][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1294 (0.1286) ([0.005]+[0.125])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4212 (0.4212) ([0.297]+[0.124])	Prec@1 95.312 (95.312)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.2172, device='cuda:0')
Epoch: [280][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1273 (0.1273) ([0.003]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1325 (0.1278) ([0.008]+[0.124])	Prec@1 100.000 (99.954)
Epoch: [280][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1272 (0.1280) ([0.003]+[0.124])	Prec@1 100.000 (99.949)
Epoch: [280][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1277 (0.1282) ([0.004]+[0.124])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4460 (0.4460) ([0.322]+[0.124])	Prec@1 94.531 (94.531)
 * Prec@1 94.610
current lr 1.00000e-03
Grad=  tensor(0.2405, device='cuda:0')
Epoch: [281][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.1273 (0.1273) ([0.003]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1261 (0.1273) ([0.002]+[0.124])	Prec@1 100.000 (99.992)
Epoch: [281][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1261 (0.1273) ([0.002]+[0.124])	Prec@1 100.000 (99.981)
Epoch: [281][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1306 (0.1272) ([0.007]+[0.124])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4437 (0.4437) ([0.320]+[0.123])	Prec@1 94.531 (94.531)
 * Prec@1 94.620
current lr 1.00000e-03
Grad=  tensor(2.8675, device='cuda:0')
Epoch: [282][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1503 (0.1503) ([0.027]+[0.123])	Prec@1 99.219 (99.219)
Epoch: [282][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1251 (0.1277) ([0.002]+[0.123])	Prec@1 100.000 (99.915)
Epoch: [282][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1281 (0.1274) ([0.005]+[0.123])	Prec@1 100.000 (99.934)
Epoch: [282][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1265 (0.1273) ([0.003]+[0.123])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4320 (0.4320) ([0.309]+[0.123])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.2504, device='cuda:0')
Epoch: [283][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1262 (0.1262) ([0.003]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1265 (0.1263) ([0.004]+[0.123])	Prec@1 100.000 (99.961)
Epoch: [283][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1243 (0.1264) ([0.002]+[0.123])	Prec@1 100.000 (99.961)
Epoch: [283][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1264 (0.1266) ([0.004]+[0.123])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4112 (0.4112) ([0.289]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-03
Grad=  tensor(1.3957, device='cuda:0')
Epoch: [284][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1321 (0.1321) ([0.010]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [284][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1244 (0.1255) ([0.002]+[0.122])	Prec@1 100.000 (99.992)
Epoch: [284][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1244 (0.1256) ([0.002]+[0.122])	Prec@1 100.000 (99.973)
Epoch: [284][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1238 (0.1255) ([0.002]+[0.122])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4199 (0.4199) ([0.298]+[0.122])	Prec@1 95.312 (95.312)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.8189, device='cuda:0')
Epoch: [285][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1269 (0.1269) ([0.005]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1240 (0.1255) ([0.002]+[0.122])	Prec@1 100.000 (99.954)
Epoch: [285][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1262 (0.1253) ([0.004]+[0.122])	Prec@1 100.000 (99.965)
Epoch: [285][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1258 (0.1252) ([0.004]+[0.122])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4292 (0.4292) ([0.308]+[0.122])	Prec@1 95.312 (95.312)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.2953, device='cuda:0')
Epoch: [286][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1242 (0.1242) ([0.003]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1232 (0.1246) ([0.002]+[0.121])	Prec@1 100.000 (99.977)
Epoch: [286][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1242 (0.1247) ([0.003]+[0.121])	Prec@1 100.000 (99.969)
Epoch: [286][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1235 (0.1246) ([0.002]+[0.121])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4336 (0.4336) ([0.312]+[0.121])	Prec@1 94.531 (94.531)
 * Prec@1 94.560
current lr 1.00000e-03
Grad=  tensor(0.2162, device='cuda:0')
Epoch: [287][0/391]	Time 0.170 (0.170)	Data 0.121 (0.121)	Loss 0.1235 (0.1235) ([0.002]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1231 (0.1243) ([0.002]+[0.121])	Prec@1 100.000 (99.985)
Epoch: [287][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1277 (0.1240) ([0.007]+[0.121])	Prec@1 100.000 (99.977)
Epoch: [287][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1238 (0.1239) ([0.003]+[0.121])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4200 (0.4200) ([0.299]+[0.121])	Prec@1 94.531 (94.531)
 * Prec@1 94.630
current lr 1.00000e-03
Grad=  tensor(0.2178, device='cuda:0')
Epoch: [288][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.1233 (0.1233) ([0.003]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1237 (0.1237) ([0.003]+[0.120])	Prec@1 100.000 (99.969)
Epoch: [288][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1223 (0.1235) ([0.002]+[0.120])	Prec@1 100.000 (99.969)
Epoch: [288][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1222 (0.1234) ([0.002]+[0.120])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4249 (0.4249) ([0.305]+[0.120])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.2157, device='cuda:0')
Epoch: [289][0/391]	Time 0.170 (0.170)	Data 0.122 (0.122)	Loss 0.1228 (0.1228) ([0.003]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1210 (0.1229) ([0.001]+[0.120])	Prec@1 100.000 (99.985)
Epoch: [289][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1269 (0.1229) ([0.007]+[0.120])	Prec@1 100.000 (99.981)
Epoch: [289][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1341 (0.1230) ([0.014]+[0.120])	Prec@1 99.219 (99.974)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4155 (0.4155) ([0.296]+[0.120])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.1852, device='cuda:0')
Epoch: [290][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1214 (0.1214) ([0.002]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1275 (0.1231) ([0.008]+[0.119])	Prec@1 100.000 (99.969)
Epoch: [290][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1215 (0.1229) ([0.002]+[0.119])	Prec@1 100.000 (99.965)
Epoch: [290][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1213 (0.1227) ([0.002]+[0.119])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4390 (0.4390) ([0.320]+[0.119])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.1874, device='cuda:0')
Epoch: [291][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1213 (0.1213) ([0.002]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1210 (0.1218) ([0.002]+[0.119])	Prec@1 100.000 (99.977)
Epoch: [291][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1221 (0.1218) ([0.003]+[0.119])	Prec@1 100.000 (99.984)
Epoch: [291][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1236 (0.1219) ([0.005]+[0.119])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.4178 (0.4178) ([0.299]+[0.119])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.2204, device='cuda:0')
Epoch: [292][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.1217 (0.1217) ([0.003]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1248 (0.1211) ([0.006]+[0.119])	Prec@1 100.000 (99.992)
Epoch: [292][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1197 (0.1212) ([0.001]+[0.118])	Prec@1 100.000 (99.988)
Epoch: [292][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1198 (0.1212) ([0.002]+[0.118])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4466 (0.4466) ([0.328]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.3626, device='cuda:0')
Epoch: [293][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1210 (0.1210) ([0.003]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1202 (0.1212) ([0.002]+[0.118])	Prec@1 100.000 (99.969)
Epoch: [293][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1187 (0.1211) ([0.001]+[0.118])	Prec@1 100.000 (99.973)
Epoch: [293][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1202 (0.1209) ([0.002]+[0.118])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4217 (0.4217) ([0.304]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.3148, device='cuda:0')
Epoch: [294][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1213 (0.1213) ([0.004]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1203 (0.1203) ([0.003]+[0.118])	Prec@1 100.000 (99.985)
Epoch: [294][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1196 (0.1202) ([0.002]+[0.117])	Prec@1 100.000 (99.988)
Epoch: [294][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1238 (0.1203) ([0.006]+[0.117])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4222 (0.4222) ([0.305]+[0.117])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.2176, device='cuda:0')
Epoch: [295][0/391]	Time 0.173 (0.173)	Data 0.126 (0.126)	Loss 0.1195 (0.1195) ([0.002]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1193 (0.1197) ([0.002]+[0.117])	Prec@1 100.000 (99.985)
Epoch: [295][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1199 (0.1198) ([0.003]+[0.117])	Prec@1 100.000 (99.977)
Epoch: [295][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1216 (0.1199) ([0.005]+[0.117])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3905 (0.3905) ([0.274]+[0.117])	Prec@1 95.312 (95.312)
 * Prec@1 94.780
current lr 1.00000e-03
Grad=  tensor(1.4953, device='cuda:0')
Epoch: [296][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.1213 (0.1213) ([0.005]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1190 (0.1191) ([0.002]+[0.117])	Prec@1 100.000 (99.992)
Epoch: [296][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1188 (0.1195) ([0.002]+[0.116])	Prec@1 100.000 (99.984)
Epoch: [296][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1210 (0.1194) ([0.005]+[0.116])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4087 (0.4087) ([0.292]+[0.116])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.4053, device='cuda:0')
Epoch: [297][0/391]	Time 0.166 (0.166)	Data 0.122 (0.122)	Loss 0.1202 (0.1202) ([0.004]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1186 (0.1193) ([0.003]+[0.116])	Prec@1 100.000 (99.946)
Epoch: [297][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1172 (0.1191) ([0.001]+[0.116])	Prec@1 100.000 (99.953)
Epoch: [297][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1175 (0.1189) ([0.002]+[0.116])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4025 (0.4025) ([0.287]+[0.116])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.1817, device='cuda:0')
Epoch: [298][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1171 (0.1171) ([0.001]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1171 (0.1183) ([0.001]+[0.116])	Prec@1 100.000 (99.985)
Epoch: [298][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1193 (0.1182) ([0.004]+[0.116])	Prec@1 100.000 (99.981)
Epoch: [298][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1166 (0.1183) ([0.001]+[0.115])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4253 (0.4253) ([0.310]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.2331, device='cuda:0')
Epoch: [299][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.1178 (0.1178) ([0.003]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1167 (0.1182) ([0.001]+[0.115])	Prec@1 100.000 (99.977)
Epoch: [299][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1167 (0.1180) ([0.002]+[0.115])	Prec@1 100.000 (99.984)
Epoch: [299][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1207 (0.1179) ([0.006]+[0.115])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4069 (0.4069) ([0.292]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 94.870

 Elapsed time for training  1:18:43.257938

 sparsity of   [0.9629629850387573, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.7037037014961243, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.8148148059844971, 0.0, 0.0, 0.0, 0.0, 0.8148148059844971, 0.0, 0.48148149251937866, 0.8148148059844971, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.8148148059844971, 0.9629629850387573, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7777777910232544, 0.0, 0.0, 0.7037037014961243, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.5925925970077515, 0.0, 0.0, 0.0, 0.9629629850387573]

 sparsity of   [0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0]

 sparsity of   [0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.296875, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0538194440305233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.292534738779068, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.9982638955116272, 0.2482638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.3098958432674408, 0.0, 0.3263888955116272, 0.9982638955116272, 0.4869791567325592, 0.0, 0.0, 0.0, 0.0, 0.9973958134651184, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3177083432674408, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.6796875, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.5078125, 0.0, 0.0, 0.0399305559694767, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.3680555522441864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9973958134651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.1857638955116272, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0551215298473835, 0.0, 0.1015625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0920138880610466, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0998263880610466, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.999131977558136, 0.2699652910232544, 0.1015625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.4900173544883728, 0.1015625, 0.0, 0.0, 0.1015625, 0.999131977558136, 0.0533854179084301, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.1015625, 0.0, 0.0, 0.1015625, 0.1015625, 0.09765625, 0.0, 0.0, 0.3133680522441864, 0.0568576380610466, 0.1015625, 0.0, 0.1015625, 0.0, 0.0, 0.0963541641831398, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.1015625, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0998263880610466, 0.5907118320465088, 0.0, 0.0, 0.0, 0.2990451455116272, 0.1015625, 0.0, 0.1011284738779068, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1006944477558136, 0.1015625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.1002604141831398, 0.1015625, 0.0503472238779068, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0833333358168602, 0.3155381977558136, 0.1015625, 0.1015625, 0.0837673619389534, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0928819477558136, 0.1015625, 0.0, 0.0, 0.999131977558136, 0.1015625, 0.999131977558136, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.1015625, 0.0, 0.1015625, 0.0, 0.0993923619389534, 0.0, 0.1015625, 0.0, 0.09765625, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.1015625, 0.1401909738779068, 0.0, 0.0, 0.9995659589767456, 0.0, 0.1015625, 0.0980902761220932, 0.0989583358168602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.1015625, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0998263880610466, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.1015625, 0.1015625, 0.0, 0.0, 0.0, 0.0, 0.1015625, 0.0, 0.0, 0.999131977558136, 0.0, 0.0993923619389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.9995659589767456, 0.0490451380610466, 0.0, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.4084201455116272, 0.999131977558136, 0.999131977558136, 0.0086805559694767, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0412326380610466, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.0, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0355902798473835, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.2578125, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.3289930522441864, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.41796875, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.421440988779068, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.327690988779068, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.304253488779068, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136]

 sparsity of   [0.0052083334885537624, 0.0, 0.73828125, 0.0, 0.0290798619389534, 0.7326388955116272, 0.0, 0.0486111119389534, 0.002170138992369175, 0.7309027910232544, 0.0034722222480922937, 0.0034722222480922937, 0.0, 0.7252604365348816, 0.0, 0.006076388992369175, 0.73046875, 0.0, 0.7044270634651184, 0.0421006940305233, 0.1558159738779068, 0.01519097201526165, 0.01953125, 0.0, 0.7330729365348816, 0.7044270634651184, 0.6575520634651184, 0.02300347201526165, 0.0, 0.734375, 0.005642361007630825, 0.0078125, 0.73828125, 0.0, 0.6675347089767456, 0.73828125, 0.73828125, 0.7300347089767456, 0.640625, 0.0481770820915699, 0.73828125, 0.73828125, 0.73828125, 0.8697916865348816, 0.7352430820465088, 0.0, 0.0, 0.73828125, 0.0, 0.0386284738779068, 0.00824652798473835, 0.7000868320465088, 0.0303819440305233, 0.0, 0.01171875, 0.3754340410232544, 0.013454861007630825, 0.002170138992369175, 0.0486111119389534, 0.006076388992369175, 0.7317708134651184, 0.0, 0.7322048544883728, 0.73828125, 0.01605902798473835, 0.717881977558136, 0.6783854365348816, 0.69921875, 0.0, 0.0, 0.999131977558136, 0.00434027798473835, 0.6883680820465088, 0.73828125, 0.73828125, 0.73828125, 0.0017361111240461469, 0.671875, 0.73828125, 0.2834201455116272, 0.00824652798473835, 0.73828125, 0.73828125, 0.0386284738779068, 0.7326388955116272, 0.0, 0.0, 0.0, 0.01909722201526165, 0.733506977558136, 0.8489583134651184, 0.73828125, 0.01692708395421505, 0.0047743055038154125, 0.999131977558136, 0.73828125, 0.03081597201526165, 0.71875, 0.0, 0.737413227558136, 0.674913227558136, 0.73828125, 0.0017361111240461469, 0.73828125, 0.002170138992369175, 0.0173611119389534, 0.7339409589767456, 0.73828125, 0.007378472480922937, 0.0520833320915699, 0.7165798544883728, 0.468315988779068, 0.702256977558136, 0.73828125, 0.6896701455116272, 0.4539930522441864, 0.0, 0.0316840298473835, 0.57421875, 0.0, 0.734375, 0.73828125, 0.0243055559694767, 0.4778645932674408, 0.0065104165114462376, 0.0, 0.6011284589767456, 0.73828125, 0.7313368320465088, 0.5412326455116272, 0.6779513955116272, 0.596788227558136, 0.73828125, 0.6427951455116272, 0.7278645634651184, 0.7278645634651184, 0.73046875, 0.71484375, 0.0069444444961845875, 0.015625, 0.0, 0.7339409589767456, 0.7105034589767456, 0.006076388992369175, 0.73828125, 0.013888888992369175, 0.0, 0.6414930820465088, 0.706163227558136, 0.7313368320465088, 0.73828125, 0.734375, 0.5095486044883728, 0.7109375, 0.0, 0.005642361007630825, 0.0086805559694767, 0.0, 0.73828125, 0.0, 0.73828125, 0.0078125, 0.73828125, 0.73828125, 0.734375, 0.73828125, 0.733506977558136, 0.7369791865348816, 0.7313368320465088, 0.73828125, 0.090711809694767, 0.0377604179084301, 0.73828125, 0.014322916977107525, 0.73828125, 0.73828125, 0.73828125, 0.73828125, 0.002170138992369175, 0.0017361111240461469, 0.7170138955116272, 0.0, 0.009114583022892475, 0.72265625, 0.0, 0.72265625, 0.6840277910232544, 0.006076388992369175, 0.7057291865348816, 0.0, 0.73828125, 0.73828125, 0.0, 0.0, 0.7282986044883728, 0.698350727558136, 0.0034722222480922937, 0.0125868059694767, 0.0360243059694767, 0.999131977558136, 0.1011284738779068, 0.7326388955116272, 0.0, 0.013020833022892475, 0.0555555559694767, 0.73828125, 0.6545138955116272, 0.6328125, 0.73828125, 0.1427951455116272, 0.7317708134651184, 0.0078125, 0.009982638992369175, 0.73828125, 0.73828125, 0.73828125, 0.01953125, 0.6666666865348816, 0.7026909589767456, 0.73828125, 0.010850694961845875, 0.0, 0.01822916604578495, 0.6510416865348816, 0.7222222089767456, 0.01215277798473835, 0.733506977558136, 0.0850694477558136, 0.73828125, 0.009114583022892475, 0.73828125, 0.0125868059694767, 0.624131977558136, 0.73828125, 0.01128472201526165, 0.733506977558136, 0.7313368320465088, 0.010850694961845875, 0.620225727558136, 0.0, 0.0607638880610466, 0.698350727558136, 0.73828125, 0.0, 0.8246527910232544, 0.73828125, 0.2855902910232544, 0.0, 0.005642361007630825, 0.73828125, 0.73828125, 0.0065104165114462376, 0.4010416567325592, 0.73828125, 0.73828125, 0.7131076455116272]

 sparsity of   [0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.16015625, 0.4848090410232544, 0.0915798619389534, 0.2894965410232544, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.2326388955116272, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.8511284589767456, 0.9995659589767456, 0.999131977558136, 0.02213541604578495, 0.999131977558136, 0.0, 0.999131977558136, 0.0729166641831398, 0.9995659589767456, 0.2795138955116272, 0.999131977558136, 0.999131977558136, 0.0924479141831398, 0.9019097089767456, 0.8993055820465088, 0.0525173619389534, 0.2178819477558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.0425347238779068, 0.18359375, 0.1315104216337204, 0.9995659589767456, 0.0590277798473835, 0.0, 0.999131977558136, 0.7777777910232544, 0.1597222238779068, 0.1011284738779068, 0.2078993022441864, 0.999131977558136, 0.999131977558136, 0.0590277798473835, 0.999131977558136, 0.134548619389534, 0.0, 0.9995659589767456, 0.999131977558136, 0.02690972201526165, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0698784738779068, 0.0338541679084301, 0.9986979365348816, 0.0572916679084301, 0.6063368320465088, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0933159738779068, 0.999131977558136, 0.999131977558136, 0.0668402761220932, 0.83984375, 0.0, 0.08203125, 0.0, 0.999131977558136, 0.0, 0.0581597238779068, 0.9227430820465088, 0.780381977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0707465261220932, 0.10546875, 0.999131977558136, 0.999131977558136, 0.0, 0.0551215298473835, 0.1675347238779068, 0.0629340261220932, 0.0, 0.0, 0.0, 0.08203125, 0.0, 0.1935763955116272, 0.9995659589767456, 0.999131977558136, 0.0347222238779068, 0.999131977558136, 0.5720486044883728, 0.999131977558136, 0.0, 0.0525173619389534, 0.0, 0.1236979141831398, 0.999131977558136, 0.874131977558136, 0.0555555559694767, 0.1805555522441864, 0.1592881977558136, 0.02994791604578495, 0.999131977558136, 0.010416666977107525, 0.999131977558136, 0.02951388992369175, 0.999131977558136, 0.0933159738779068, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.8949652910232544, 0.9995659589767456, 0.9995659589767456, 0.0607638880610466, 0.0377604179084301, 0.999131977558136, 0.999131977558136, 0.03515625, 0.9995659589767456, 0.8181423544883728, 0.0655381977558136, 0.0282118059694767, 0.999131977558136, 0.071180559694767, 0.9982638955116272, 0.999131977558136, 0.17578125, 0.999131977558136, 0.999131977558136, 0.1927083283662796, 0.0802951380610466, 0.999131977558136, 0.046875, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.2391493022441864, 0.0, 0.999131977558136, 0.999131977558136, 0.456597238779068, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1245659738779068, 0.1401909738779068, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8211805820465088, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.6184895634651184, 0.0572916679084301, 0.0963541641831398, 0.1115451380610466, 0.999131977558136, 0.366753488779068, 0.9144965410232544, 0.9995659589767456, 0.8550347089767456, 0.9995659589767456, 0.8841145634651184, 0.1206597238779068, 0.1263020783662796, 0.02734375, 0.1744791716337204, 0.09375, 0.02300347201526165, 0.0, 0.0850694477558136, 0.3715277910232544, 0.9986979365348816, 0.0559895820915699, 0.7890625, 0.0390625, 0.1827256977558136, 0.1822916716337204, 0.09375, 0.2217881977558136, 0.9995659589767456, 0.0881076380610466, 0.999131977558136, 0.0, 0.8567708134651184, 0.999131977558136, 0.0698784738779068, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.3012152910232544, 0.8359375, 0.0, 0.0651041641831398, 0.999131977558136, 0.0325520820915699, 0.999131977558136, 0.9995659589767456, 0.1671006977558136, 0.0594618059694767, 0.9045138955116272, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1137152761220932, 0.8324652910232544, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9982638955116272, 0.0, 0.9986979365348816, 0.9986979365348816, 0.1436631977558136, 0.23046875, 0.0642361119389534, 0.999131977558136, 0.9995659589767456, 0.0846354141831398, 0.1966145783662796, 0.0850694477558136, 0.0737847238779068, 0.1723090261220932, 0.273003488779068, 0.999131977558136, 0.9995659589767456, 0.0533854179084301, 0.999131977558136, 0.1753472238779068, 0.1566840261220932, 0.0889756977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.2213541716337204, 0.1271701455116272, 0.9986979365348816, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0629340261220932, 0.999131977558136, 0.4748263955116272, 0.1749131977558136, 0.02734375, 0.3949652910232544, 0.9986979365348816, 0.9982638955116272, 0.999131977558136, 0.1254340261220932, 0.999131977558136, 0.0, 0.2569444477558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0629340261220932, 0.0282118059694767, 0.0703125, 0.9986979365348816, 0.3294270932674408, 0.0, 0.0668402761220932, 0.09765625, 0.999131977558136, 0.8962673544883728, 0.8125, 0.9995659589767456, 0.1328125, 0.1644965261220932, 0.999131977558136, 0.17578125, 0.999131977558136, 0.999131977558136, 0.2170138955116272, 0.8177083134651184, 0.999131977558136, 0.0368923619389534, 0.0, 0.1566840261220932, 0.1419270783662796, 0.999131977558136, 0.999131977558136, 0.0963541641831398, 0.999131977558136, 0.1714409738779068, 0.999131977558136, 0.0855034738779068, 0.2764756977558136, 0.2348090261220932, 0.9986979365348816, 0.999131977558136, 0.2552083432674408, 0.999131977558136, 0.2348090261220932, 0.999131977558136, 0.02951388992369175, 0.0, 0.0542534738779068, 0.9995659589767456, 0.0, 0.999131977558136, 0.9084201455116272, 0.4752604067325592, 0.999131977558136, 0.2178819477558136, 0.1254340261220932, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0603298619389534, 0.9995659589767456, 0.0516493059694767, 0.999131977558136, 0.9995659589767456, 0.0577256940305233, 0.999131977558136, 0.999131977558136, 0.0, 0.0794270858168602, 0.0603298619389534, 0.8246527910232544, 0.3116319477558136, 0.2065972238779068, 0.138454869389534, 0.9995659589767456, 0.2291666716337204, 0.999131977558136, 0.0, 0.16796875, 0.1310763955116272, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.106336809694767, 0.9995659589767456, 0.0499131940305233, 0.999131977558136, 0.1119791641831398, 0.1297743022441864, 0.4131944477558136, 0.999131977558136, 0.9986979365348816, 0.02951388992369175, 0.999131977558136, 0.0, 0.0377604179084301, 0.1783854216337204, 0.0451388880610466, 0.999131977558136, 0.999131977558136, 0.0815972238779068, 0.9995659589767456, 0.9995659589767456, 0.2426215261220932, 0.02994791604578495, 0.999131977558136, 0.999131977558136, 0.2274305522441864, 0.9995659589767456, 0.80078125, 0.999131977558136, 0.90234375, 0.9986979365348816, 0.999131977558136, 0.3181423544883728, 0.0603298619389534, 0.1640625, 0.9995659589767456, 0.792100727558136, 0.999131977558136, 0.9986979365348816, 0.1974826455116272, 0.0572916679084301, 0.999131977558136, 0.9986979365348816, 0.1193576380610466, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.8832465410232544, 0.9995659589767456, 0.1818576455116272, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.542100727558136, 0.078125, 0.1276041716337204, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0720486119389534, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0004340277810115367, 0.8211805820465088, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9986979365348816, 0.0577256940305233, 0.9995659589767456, 0.999131977558136, 0.1575520783662796, 0.1822916716337204, 0.0529513880610466, 0.999131977558136, 0.1866319477558136, 0.999131977558136, 0.0, 0.1011284738779068, 0.999131977558136, 0.1315104216337204, 0.0872395858168602, 0.999131977558136, 0.0772569477558136, 0.1796875, 0.999131977558136, 0.999131977558136, 0.0, 0.1597222238779068, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.827256977558136, 0.0, 0.9995659589767456, 0.03125, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.2013888955116272, 0.0598958320915699, 0.0, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.8493923544883728, 0.999131977558136, 0.0477430559694767, 0.1302083283662796, 0.0668402761220932, 0.999131977558136, 0.7595486044883728, 0.0, 0.3407118022441864, 0.9986979365348816, 0.999131977558136, 0.0525173619389534, 0.999131977558136, 0.086805559694767, 0.146267369389534, 0.0529513880610466, 0.999131977558136, 0.0577256940305233]

 sparsity of   [0.02734375, 0.3064236044883728, 0.4309895932674408, 0.7701823115348816, 0.7213541865348816, 0.7593315839767456, 0.098524309694767, 0.916015625, 0.916015625, 0.8172743320465088, 0.0, 0.0648871511220932, 0.0399305559694767, 0.6538628339767456, 0.5911458134651184, 0.8888888955116272, 0.7654079794883728, 0.7000868320465088, 0.0414496548473835, 0.6859809160232544, 0.0353732630610466, 0.0303819440305233, 0.7977430820465088, 0.0594618059694767, 0.846788227558136, 0.0668402761220932, 0.0342881940305233, 0.8315972089767456, 0.01410590298473835, 0.1137152761220932, 0.0531684048473835, 0.655381977558136, 0.8092448115348816, 0.1050347238779068, 0.837022602558136, 0.0360243059694767, 0.0579427070915699, 0.7953559160232544, 0.0555555559694767, 0.0, 0.0, 0.0334201380610466, 0.0720486119389534, 0.7662760615348816, 0.6759982705116272, 0.9103732705116272, 0.0, 0.0329861119389534, 0.8485243320465088, 0.8943142294883728, 0.02886284701526165, 0.817491352558136, 0.0475260429084301, 0.7447916865348816, 0.0, 0.753038227558136, 0.0379774309694767, 0.0384114570915699, 0.0505642369389534, 0.02191840298473835, 0.8413628339767456, 0.6783854365348816, 0.9157986044883728, 0.1534288227558136, 0.7899305820465088, 0.754991352558136, 0.0368923619389534, 0.6829426884651184, 0.6820746660232544, 0.7300347089767456, 0.0397135429084301, 0.02669270895421505, 0.0431857630610466, 0.8990885615348816, 0.6653645634651184, 0.2634548544883728, 0.0, 0.0366753488779068, 0.02951388992369175, 0.0360243059694767, 0.0366753488779068, 0.0366753488779068, 0.7699652910232544, 0.6987847089767456, 0.1126302108168602, 0.7465277910232544, 0.6955295205116272, 0.865234375, 0.8394097089767456, 0.6857638955116272, 0.7358940839767456, 0.0414496548473835, 0.5308159589767456, 0.8888888955116272, 0.916015625, 0.02951388992369175, 0.0935329869389534, 0.5438368320465088, 0.1662326455116272, 0.0753038227558136, 0.8912760615348816, 0.1104600727558136, 0.0397135429084301, 0.0234375, 0.7751736044883728, 0.0572916679084301, 0.0366753488779068, 0.7836371660232544, 0.03059895895421505, 0.6668837070465088, 0.0, 0.7326388955116272, 0.0625, 0.175564244389534, 0.7502170205116272, 0.6647135615348816, 0.033203125, 0.3274739682674408, 0.1115451380610466, 0.6564670205116272, 0.9157986044883728, 0.751085102558136, 0.916015625, 0.0407986119389534, 0.0627170130610466, 0.6668837070465088, 0.916015625, 0.6625434160232544, 0.732421875, 0.099609375, 0.7682291865348816, 0.4715711772441864, 0.8828125, 0.0509982630610466, 0.134548619389534, 0.768663227558136, 0.6536458134651184, 0.7150607705116272, 0.8578559160232544, 0.468315988779068, 0.916015625, 0.0251736119389534, 0.0358072929084301, 0.087890625, 0.1265190988779068, 0.6595051884651184, 0.0748697891831398, 0.0518663190305233, 0.659288227558136, 0.0499131940305233, 0.716796875, 0.048828125, 0.03081597201526165, 0.0618489570915699, 0.8036024570465088, 0.02278645895421505, 0.1117621511220932, 0.080078125, 0.0323350690305233, 0.8263888955116272, 0.0768229141831398, 0.9157986044883728, 0.8888888955116272, 0.2795138955116272, 0.5779079794883728, 0.0245225690305233, 0.7630208134651184, 0.0329861119389534, 0.0952690988779068, 0.7860243320465088, 0.5082465410232544, 0.8005642294883728, 0.7378472089767456, 0.0, 0.7018229365348816, 0.681640625, 0.0709635391831398, 0.7361111044883728, 0.8036024570465088, 0.033203125, 0.0989583358168602, 0.7877604365348816, 0.669921875, 0.6720920205116272, 0.0698784738779068, 0.0282118059694767, 0.138671875, 0.056640625, 0.046875, 0.0431857630610466, 0.8359375, 0.6245659589767456, 0.759765625, 0.916015625, 0.6809895634651184, 0.0290798619389534, 0.5436198115348816, 0.7395833134651184, 0.2495659738779068, 0.4691840410232544, 0.8090277910232544, 0.4249131977558136, 0.708116352558136, 0.1017795130610466, 0.6608073115348816, 0.7732204794883728, 0.6295573115348816, 0.8003472089767456, 0.7690972089767456, 0.7966579794883728, 0.0, 0.353081613779068, 0.041015625, 0.731553852558136, 0.1098090261220932, 0.0, 0.5101996660232544, 0.5757378339767456, 0.8381076455116272, 0.8198784589767456, 0.791015625, 0.766710102558136, 0.7065972089767456, 0.8832465410232544, 0.1187065988779068, 0.0, 0.1015625, 0.0, 0.6890190839767456, 0.833984375, 0.6920573115348816, 0.7265625, 0.8706597089767456, 0.0794270858168602, 0.1378038227558136, 0.9157986044883728, 0.733506977558136, 0.1232638880610466, 0.671006977558136, 0.0774739608168602, 0.02170138992369175, 0.02213541604578495, 0.02777777798473835, 0.7508680820465088, 0.7024739384651184, 0.0193142369389534, 0.9006076455116272, 0.13671875, 0.0457899309694767, 0.2586805522441864, 0.7580295205116272, 0.612413227558136, 0.0, 0.0243055559694767, 0.0861545130610466, 0.8776041865348816, 0.893663227558136, 0.110243059694767, 0.6818576455116272, 0.7951388955116272, 0.0661892369389534, 0.6898871660232544, 0.1360677033662796, 0.7337239384651184, 0.0416666679084301, 0.6586371660232544, 0.0338541679084301, 0.0737847238779068, 0.9157986044883728, 0.8231337070465088, 0.9047309160232544, 0.7484809160232544, 0.0347222238779068, 0.7701823115348816, 0.072265625, 0.2473958283662796, 0.02799479104578495, 0.1371527761220932, 0.356987863779068, 0.1184895858168602, 0.69140625, 0.9060329794883728, 0.916015625, 0.8079426884651184, 0.6521267294883728, 0.7250434160232544, 0.1017795130610466, 0.0998263880610466, 0.01605902798473835, 0.1158854141831398, 0.7328559160232544, 0.0434027798473835, 0.5950520634651184, 0.6968315839767456, 0.0414496548473835, 0.15625, 0.021484375, 0.0366753488779068, 0.0358072929084301, 0.5078125, 0.2096354216337204, 0.0, 0.0, 0.7834201455116272, 0.0394965298473835, 0.0668402761220932, 0.0551215298473835, 0.909288227558136, 0.6069878339767456, 0.103515625, 0.8960503339767456, 0.6584201455116272, 0.7756076455116272, 0.1391059011220932, 0.0232204869389534, 0.0, 0.2183159738779068, 0.1002604141831398, 0.6814236044883728, 0.373046875, 0.8148871660232544, 0.052734375, 0.6662326455116272, 0.6048176884651184, 0.7606337070465088, 0.0310329869389534, 0.7404513955116272, 0.2042100727558136, 0.03059895895421505, 0.0381944440305233, 0.916015625, 0.096571184694767, 0.6892361044883728, 0.6404079794883728, 0.1349826455116272, 0.02170138992369175, 0.7504340410232544, 0.2384982705116272, 0.0353732630610466, 0.195095494389534, 0.0407986119389534, 0.1330295205116272, 0.1846788227558136, 0.8901909589767456, 0.829210102558136, 0.0440538190305233, 0.0316840298473835, 0.0707465261220932, 0.02213541604578495, 0.0, 0.916015625, 0.0611979179084301, 0.1694878488779068, 0.167751744389534, 0.0, 0.265190988779068, 0.7469618320465088, 0.8287760615348816, 0.0, 0.0865885391831398, 0.0375434048473835, 0.7122395634651184, 0.1929253488779068, 0.0434027798473835, 0.821397602558136, 0.02690972201526165, 0.0301649309694767, 0.1421440988779068, 0.8157551884651184, 0.666015625, 0.9016926884651184, 0.2868923544883728, 0.830078125, 0.13671875, 0.0486111119389534, 0.0405815988779068, 0.0316840298473835, 0.0503472238779068, 0.01953125, 0.7545573115348816, 0.8049045205116272, 0.03081597201526165, 0.080946184694767, 0.0423177070915699, 0.0243055559694767, 0.6922743320465088, 0.03081597201526165, 0.0590277798473835, 0.8177083134651184, 0.8752170205116272, 0.0, 0.2026909738779068, 0.0466579869389534, 0.7567274570465088, 0.7033420205116272, 0.1397569477558136, 0.8170573115348816, 0.8854166865348816, 0.0607638880610466, 0.1590711772441864, 0.7322048544883728, 0.651475727558136, 0.7760416865348816, 0.4778645932674408, 0.0694444477558136, 0.754991352558136, 0.0661892369389534, 0.7795138955116272, 0.0451388880610466, 0.1247829869389534, 0.754991352558136, 0.0481770820915699, 0.0575086809694767, 0.0368923619389534, 0.161892369389534, 0.6178385615348816, 0.6399739384651184, 0.0355902798473835, 0.7154948115348816, 0.6883680820465088, 0.8363715410232544, 0.106336809694767, 0.0193142369389534, 0.878038227558136, 0.0444878488779068, 0.4911024272441864, 0.0418836809694767, 0.0438368059694767, 0.8033854365348816, 0.916015625, 0.0486111119389534, 0.0592447929084301, 0.7174479365348816, 0.5405815839767456, 0.0314670130610466, 0.8335503339767456, 0.02365451492369175, 0.02604166604578495, 0.6716579794883728, 0.0, 0.916015625, 0.5707465410232544, 0.0516493059694767, 0.0, 0.7154948115348816, 0.8891059160232544, 0.1098090261220932, 0.0342881940305233, 0.1714409738779068, 0.6488715410232544, 0.8075087070465088, 0.0340711809694767, 0.01584201492369175, 0.8328993320465088, 0.7473958134651184, 0.0, 0.029296875, 0.0501302070915699, 0.0414496548473835, 0.0, 0.0364583320915699, 0.0611979179084301, 0.0646701380610466, 0.0785590261220932, 0.7543402910232544, 0.1547309011220932, 0.01822916604578495, 0.7868923544883728, 0.2217881977558136, 0.7252604365348816, 0.0384114570915699, 0.7892795205116272, 0.6946614384651184, 0.7380642294883728, 0.767578125, 0.71484375, 0.0690104141831398, 0.7072482705116272, 0.0184461809694767, 0.1143663227558136, 0.6694878339767456, 0.0642361119389534, 0.835069477558136, 0.6538628339767456, 0.3014322817325592, 0.0052083334885537624, 0.1108940988779068, 0.0251736119389534, 0.8266059160232544, 0.0544704869389534, 0.2643229067325592, 0.0716145858168602, 0.1434461772441864, 0.0588107630610466, 0.8572048544883728, 0.0416666679084301, 0.0475260429084301, 0.01779513992369175, 0.8576388955116272, 0.7489149570465088, 0.0418836809694767, 0.3532986044883728, 0.7515190839767456, 0.1085069477558136, 0.8092448115348816, 0.3502604067325592, 0.916015625, 0.0451388880610466, 0.0618489570915699, 0.1156684011220932, 0.794053852558136, 0.02387152798473835]

 sparsity of   [0.0234375, 0.01953125, 0.02734375, 0.01953125, 0.01953125, 0.015625, 0.015625, 0.0078125, 0.02734375, 0.02734375, 0.0, 0.01171875, 0.015625, 0.02734375, 0.015625, 0.02734375, 0.015625, 0.01171875, 0.015625, 0.01171875, 0.9375, 0.02734375, 0.015625, 0.0078125, 0.015625, 0.01953125, 0.015625, 0.0078125, 0.02734375, 0.02734375, 0.02734375, 0.01171875, 0.015625, 0.02734375, 0.015625, 0.015625, 0.0078125, 0.01953125, 0.01953125, 0.0, 0.0, 0.01953125, 0.02734375, 0.01171875, 0.0078125, 0.078125, 0.0, 0.0859375, 0.1640625, 0.02734375, 0.01171875, 0.0, 0.02734375, 0.01171875, 0.0, 0.01953125, 0.0234375, 0.02734375, 0.015625, 0.0234375, 0.0, 0.015625, 0.015625, 0.01171875, 0.0, 0.0, 0.02734375, 0.02734375, 0.01171875, 0.01171875, 0.02734375, 0.015625, 0.01953125, 0.01953125, 0.01953125, 0.015625, 0.0, 0.02734375, 0.01171875, 0.01953125, 0.01171875, 0.01171875, 0.0234375, 0.015625, 0.01171875, 0.02734375, 0.02734375, 0.02734375, 0.02734375, 0.01171875, 0.02734375, 0.01171875, 0.0078125, 0.015625, 0.02734375, 0.01953125, 0.29296875, 0.01171875, 0.0234375, 0.02734375, 0.01171875, 0.01953125, 0.0234375, 0.015625, 0.02734375, 0.12109375, 0.01171875, 0.0234375, 0.02734375, 0.0234375, 0.0, 0.015625, 0.01953125, 0.015625, 0.0078125, 0.015625, 0.01953125, 0.02734375, 0.01953125, 0.0234375, 0.01171875, 0.01171875, 0.01171875, 0.015625, 0.015625, 0.015625, 0.01171875, 0.0234375, 0.015625, 0.01953125, 0.02734375, 0.015625, 0.02734375, 0.02734375, 0.015625, 0.01171875, 0.01171875, 0.015625, 0.0, 0.0234375, 0.01171875, 0.0078125, 0.015625, 0.01953125, 0.01953125, 0.01171875, 0.01953125, 0.01171875, 0.0234375, 0.01171875, 0.0, 0.00390625, 0.015625, 0.01953125, 0.02734375, 0.02734375, 0.02734375, 0.02734375, 0.02734375, 0.01953125, 0.10546875, 0.0234375, 0.02734375, 0.01171875, 0.015625, 0.01171875, 0.01953125, 0.015625, 0.01171875, 0.01171875, 0.01171875, 0.00390625, 0.0234375, 0.0, 0.02734375, 0.0234375, 0.02734375, 0.015625, 0.2578125, 0.02734375, 0.02734375, 0.01953125, 0.015625, 0.02734375, 0.01171875, 0.01953125, 0.015625, 0.0234375, 0.01953125, 0.00390625, 0.01171875, 0.01953125, 0.015625, 0.0078125, 0.02734375, 0.02734375, 0.015625, 0.02734375, 0.01171875, 0.01953125, 0.0234375, 0.0234375, 0.0234375, 0.01953125, 0.0078125, 0.0078125, 0.01953125, 0.0234375, 0.02734375, 0.01953125, 0.0, 0.0234375, 0.02734375, 0.0078125, 0.015625, 0.0, 0.015625, 0.01953125, 0.01171875, 0.0, 0.00390625, 0.015625, 0.01953125, 0.015625, 0.015625, 0.0, 0.01171875, 0.0, 0.0234375, 0.02734375, 0.02734375, 0.01953125, 0.01171875, 0.01953125, 0.02734375, 0.01171875, 0.01953125, 0.02734375, 0.01171875, 0.02734375, 0.02734375, 0.01171875, 0.01171875, 0.015625, 0.0234375, 0.02734375, 0.01171875, 0.90625, 0.02734375, 0.00390625, 0.0234375, 0.01953125, 0.0, 0.01953125, 0.02734375, 0.01171875, 0.015625, 0.015625, 0.01171875, 0.00390625, 0.02734375, 0.02734375, 0.01171875, 0.015625, 0.01953125, 0.02734375, 0.015625, 0.02734375, 0.015625, 0.00390625, 0.01953125, 0.0234375, 0.015625, 0.01171875, 0.01953125, 0.01171875, 0.01953125, 0.02734375, 0.01953125, 0.01171875, 0.0234375, 0.02734375, 0.01953125, 0.015625, 0.01953125, 0.00390625, 0.01953125, 0.02734375, 0.00390625, 0.01171875, 0.02734375, 0.01171875, 0.015625, 0.01171875, 0.01171875, 0.015625, 0.015625, 0.015625, 0.0078125, 0.02734375, 0.015625, 0.0, 0.0, 0.015625, 0.02734375, 0.0234375, 0.01171875, 0.01171875, 0.01953125, 0.01953125, 0.02734375, 0.0234375, 0.01953125, 0.01953125, 0.0234375, 0.0, 0.01953125, 0.01953125, 0.0, 0.015625, 0.015625, 0.0234375, 0.02734375, 0.02734375, 0.0234375, 0.01953125, 0.02734375, 0.02734375, 0.0, 0.0078125, 0.0078125, 0.02734375, 0.00390625, 0.015625, 0.02734375, 0.01953125, 0.02734375, 0.02734375, 0.01171875, 0.0234375, 0.02734375, 0.01953125, 0.01171875, 0.0078125, 0.00390625, 0.00390625, 0.015625, 0.0, 0.02734375, 0.0, 0.01171875, 0.015625, 0.0234375, 0.01953125, 0.0, 0.0078125, 0.02734375, 0.00390625, 0.0, 0.015625, 0.02734375, 0.015625, 0.01953125, 0.02734375, 0.015625, 0.0, 0.0078125, 0.01953125, 0.015625, 0.02734375, 0.0234375, 0.015625, 0.0, 0.0234375, 0.01171875, 0.02734375, 0.01171875, 0.01953125, 0.01171875, 0.0, 0.0234375, 0.02734375, 0.01953125, 0.015625, 0.015625, 0.0078125, 0.0, 0.02734375, 0.02734375, 0.01171875, 0.0, 0.01953125, 0.02734375, 0.01171875, 0.015625, 0.02734375, 0.0234375, 0.01953125, 0.015625, 0.015625, 0.0078125, 0.02734375, 0.01953125, 0.015625, 0.02734375, 0.00390625, 0.0, 0.0234375, 0.01171875, 0.01953125, 0.02734375, 0.0234375, 0.015625, 0.0078125, 0.01953125, 0.02734375, 0.01953125, 0.01953125, 0.0234375, 0.01953125, 0.01171875, 0.015625, 0.01171875, 0.015625, 0.02734375, 0.0078125, 0.02734375, 0.0234375, 0.0, 0.02734375, 0.0, 0.02734375, 0.01953125, 0.02734375, 0.01171875, 0.015625, 0.015625, 0.015625, 0.01171875, 0.0, 0.0234375, 0.01953125, 0.01953125, 0.0, 0.01171875, 0.01953125, 0.0078125, 0.02734375, 0.015625, 0.01953125, 0.015625, 0.02734375, 0.01953125, 0.015625, 0.02734375, 0.0, 0.02734375, 0.015625, 0.01953125, 0.0, 0.015625, 0.0, 0.0234375, 0.01171875, 0.02734375, 0.0078125, 0.015625, 0.02734375, 0.02734375, 0.01953125, 0.01171875, 0.0078125, 0.01953125, 0.0, 0.02734375, 0.015625, 0.01953125, 0.00390625, 0.015625, 0.01171875, 0.01953125, 0.0234375, 0.02734375, 0.01171875, 0.02734375, 0.0, 0.01171875, 0.01953125, 0.01953125, 0.02734375, 0.796875, 0.01953125, 0.01171875, 0.00390625, 0.0, 0.01171875, 0.015625, 0.0078125, 0.02734375, 0.01171875, 0.01953125, 0.00390625, 0.0234375, 0.01171875, 0.765625, 0.01953125, 0.0078125, 0.0078125, 0.015625, 0.0234375, 0.02734375, 0.01171875]

 sparsity of   [0.1041666641831398, 0.1202256977558136, 0.4505208432674408, 0.1321614533662796, 0.4694010317325592, 0.1111111119389534, 0.0709635391831398, 0.274956613779068, 0.0316840298473835, 0.1920572966337204, 0.224392369389534, 0.0842013880610466, 0.0891927108168602, 0.2022569477558136, 0.6011284589767456, 0.0978732630610466, 0.265625, 0.169704869389534, 0.501953125, 0.1223958358168602, 0.0768229141831398, 0.166015625, 0.2476128488779068, 0.0963541641831398, 0.1842447966337204, 0.0568576380610466, 0.1796875, 0.0954861119389534, 0.1128472238779068, 0.1647135466337204, 0.128689244389534, 0.2493489533662796, 0.2065972238779068, 0.096571184694767, 0.3151041567325592, 0.0989583358168602, 0.214626744389534, 0.661241352558136, 0.447265625, 0.234375, 0.3474392294883728, 0.9995659589767456, 0.0861545130610466, 0.0813802108168602, 0.108289934694767, 0.1236979141831398, 0.0842013880610466, 0.09765625, 0.0709635391831398, 0.4084201455116272, 0.5544704794883728, 0.2018229216337204, 0.1000434011220932, 0.0846354141831398, 0.0852864608168602, 0.1497395783662796, 0.6705729365348816, 0.2335069477558136, 0.1640625, 0.1419270783662796, 0.173828125, 0.1362847238779068, 0.2790798544883728, 0.3524305522441864, 0.1529947966337204, 0.2039930522441864, 0.3220486044883728, 0.0972222238779068, 0.104383684694767, 0.0855034738779068, 0.10546875, 0.302300363779068, 0.2669270932674408, 0.423828125, 0.4654947817325592, 0.2037760466337204, 0.3962673544883728, 0.0783420130610466, 0.2734375, 0.2348090261220932, 0.1089409738779068, 0.152126744389534, 0.125, 0.2137586772441864, 0.126736119389534, 0.0815972238779068, 0.1532118022441864, 0.2003038227558136, 0.2712673544883728, 0.2834201455116272, 0.2658420205116272, 0.0703125, 0.1968315988779068, 0.175564244389534, 0.103515625, 0.1879340261220932, 0.1354166716337204, 0.1069878488779068, 0.2254774272441864, 0.3936631977558136, 0.4012586772441864, 0.2122395783662796, 0.1399739533662796, 0.1321614533662796, 0.1918402761220932, 0.091796875, 0.0523003488779068, 0.104383684694767, 0.1434461772441864, 0.0928819477558136, 0.2686631977558136, 0.4919704794883728, 0.5290798544883728, 0.2358940988779068, 0.0696614608168602, 0.1506076455116272, 0.0857204869389534, 0.0685763880610466, 0.9995659589767456, 0.4264322817325592, 0.1733940988779068, 0.1770833283662796, 0.076171875, 0.15625, 0.1935763955116272, 0.150390625, 0.35546875, 0.1163194477558136, 0.1343315988779068, 0.084852434694767, 0.1204427108168602, 0.0915798619389534, 0.1634114533662796, 0.5230034589767456, 0.2005208283662796, 0.2994791567325592, 0.1399739533662796, 0.08984375, 0.4915364682674408, 0.1490885466337204, 0.1714409738779068, 0.516710102558136, 0.398003488779068, 0.12890625, 0.2777777910232544, 0.0944010391831398, 0.1171875, 0.2855902910232544, 0.3181423544883728, 0.154079869389534, 0.9997829794883728, 0.3318142294883728, 0.1167534738779068, 0.6848958134651184, 0.2493489533662796, 0.1749131977558136, 0.1213107630610466, 0.513671875, 0.077039934694767, 0.1649305522441864, 0.218532994389534, 0.286675363779068, 0.6688368320465088, 0.1028645858168602, 0.0822482630610466, 0.0594618059694767, 0.1723090261220932, 0.0961371511220932, 0.1671006977558136, 0.28125, 0.15625, 0.0844184011220932, 0.548828125, 0.109375, 0.1130642369389534, 0.3524305522441864, 0.1056857630610466, 0.4299045205116272, 0.1182725727558136, 0.1046006977558136, 0.0950520858168602, 0.2452256977558136, 0.1217447891831398, 0.1651475727558136, 0.9997829794883728, 0.3728298544883728, 0.278862863779068, 0.0998263880610466, 0.098524309694767, 0.2306857705116272, 0.4828559160232544, 0.02495659701526165, 0.1354166716337204, 0.1532118022441864, 0.3196614682674408, 0.2962239682674408, 0.2775607705116272, 0.6286892294883728, 0.3780381977558136, 0.3741319477558136, 0.4474826455116272, 0.2220052033662796, 0.4084201455116272, 0.22265625, 0.082899309694767, 0.4587673544883728, 0.142361119389534, 0.1799045205116272, 0.181423619389534, 0.0716145858168602, 0.0889756977558136, 0.106336809694767, 0.0642361119389534, 0.069227434694767, 0.1208767369389534, 0.158203125, 0.1061197891831398, 0.0894097238779068, 0.1525607705116272, 0.1773003488779068, 0.1193576380610466, 0.3255208432674408, 0.4205729067325592, 0.3532986044883728, 0.0659722238779068, 0.094618059694767, 0.840928852558136, 0.1768663227558136, 0.0833333358168602, 0.5551215410232544, 0.2233072966337204, 0.078125, 0.2923177182674408, 0.1861979216337204, 0.157986119389534, 0.1382378488779068, 0.0772569477558136, 0.1265190988779068, 0.1848958283662796, 0.1688368022441864, 0.1419270783662796, 0.125, 0.1447482705116272, 0.0891927108168602, 0.3125, 0.0894097238779068, 0.2764756977558136, 0.1037326380610466, 0.2727864682674408, 0.234375, 0.1378038227558136, 0.1536458283662796, 0.1356336772441864, 0.1573350727558136, 0.470269113779068, 0.0846354141831398, 0.173611119389534, 0.3684895932674408, 0.2897135317325592, 0.1401909738779068, 0.1019965261220932, 0.1050347238779068, 0.10546875, 0.1165364608168602, 0.1143663227558136, 0.1013454869389534, 0.2879774272441864, 0.1909722238779068, 0.1447482705116272, 0.120008684694767, 0.3146701455116272, 0.0748697891831398, 0.1354166716337204, 0.1707899272441864, 0.1243489608168602, 0.39453125, 0.1469184011220932, 0.1011284738779068, 0.1671006977558136, 0.1469184011220932, 0.300347238779068, 0.1729600727558136, 0.396484375, 0.1529947966337204, 0.1436631977558136, 0.0748697891831398, 0.504991352558136, 0.1412760466337204, 0.205078125, 0.1013454869389534, 0.2389322966337204, 0.2022569477558136, 0.1061197891831398, 0.244140625, 0.1019965261220932, 0.0815972238779068, 0.7111545205116272, 0.3133680522441864, 0.2890625, 0.384765625, 0.056640625, 0.169921875, 0.5805121660232544, 0.2697482705116272, 0.0811631977558136, 0.150173619389534, 0.337890625, 0.1098090261220932, 0.2241753488779068, 0.5119357705116272, 0.152126744389534, 0.4609375, 0.3088107705116272, 0.0857204869389534, 0.1842447966337204, 0.329644113779068, 0.1783854216337204, 0.0924479141831398, 0.171657994389534, 0.1013454869389534, 0.236111119389534, 0.0792100727558136, 0.0759548619389534, 0.0861545130610466, 0.2352430522441864, 0.4500868022441864, 0.0935329869389534, 0.1490885466337204, 0.2981770932674408, 0.609375, 0.1319444477558136, 0.9997829794883728, 0.1940104216337204, 0.286675363779068, 0.1883680522441864, 0.0716145858168602, 0.3865017294883728, 0.125, 0.6803385615348816, 0.150390625, 0.1284722238779068, 0.1026475727558136, 0.4704861044883728, 0.1506076455116272, 0.251519113779068, 0.1469184011220932, 0.2063802033662796, 0.0954861119389534, 0.9995659589767456, 0.154296875, 0.0989583358168602, 0.1642795205116272, 0.248046875, 0.4147135317325592, 0.1759982705116272, 0.5876736044883728, 0.4203559160232544, 0.3361545205116272, 0.3526475727558136, 0.1506076455116272, 0.1714409738779068, 0.1206597238779068, 0.3116319477558136, 0.2200520783662796, 0.0785590261220932, 0.3736979067325592, 0.5052083134651184, 0.1098090261220932, 0.0651041641831398, 0.2078993022441864, 0.2237413227558136, 0.283203125, 0.1139322891831398, 0.8090277910232544, 0.3411458432674408, 0.0941840261220932, 0.1842447966337204, 0.1512586772441864, 0.1538628488779068, 0.462890625, 0.41015625, 0.1291232705116272, 0.1427951455116272, 0.2552083432674408, 0.2996961772441864, 0.435112863779068, 0.2504340410232544, 0.0920138880610466, 0.1351996511220932, 0.1343315988779068, 0.6575520634651184, 0.0753038227558136, 0.2415364533662796, 0.1213107630610466, 0.0544704869389534, 0.2888454794883728, 0.9997829794883728, 0.5618489384651184, 0.0729166641831398, 0.6276041865348816, 0.1525607705116272, 0.1297743022441864, 0.3255208432674408, 0.2782118022441864, 0.08203125, 0.9997829794883728, 0.2790798544883728, 0.1056857630610466, 0.1966145783662796, 0.526475727558136, 0.29296875, 0.0837673619389534, 0.4568142294883728, 0.34375, 0.5672743320465088, 0.1809895783662796, 0.1610243022441864, 0.2463107705116272, 0.0666232630610466, 0.247829869389534, 0.5900607705116272, 0.1078559011220932, 0.1985677033662796, 0.6254340410232544, 0.1030815988779068, 0.1171875, 0.0755208358168602, 0.2628038227558136, 0.4296875, 0.109375, 0.0575086809694767, 0.2862413227558136, 0.2445746511220932, 0.3532986044883728, 0.3600260317325592, 0.1349826455116272, 0.372612863779068, 0.5177951455116272, 0.3643663227558136, 0.3988715410232544, 0.2387152761220932, 0.1410590261220932, 0.15234375, 0.2437065988779068, 0.3630642294883728, 0.2217881977558136, 0.106336809694767, 0.5789930820465088, 0.1399739533662796, 0.232204869389534, 0.9995659589767456, 0.3157552182674408, 0.2280815988779068, 0.1158854141831398, 0.703125, 0.1566840261220932, 0.2957899272441864, 0.1612413227558136, 0.1438802033662796, 0.1484375, 0.468315988779068, 0.286675363779068, 0.0581597238779068, 0.1206597238779068, 0.1028645858168602, 0.1705729216337204, 0.1176215261220932, 0.1614583283662796, 0.1050347238779068, 0.40234375, 0.2111545205116272, 0.567491352558136, 0.474609375, 0.278862863779068, 0.3185763955116272, 0.370659738779068, 0.247829869389534, 0.3190104067325592, 0.351128488779068, 0.3617621660232544, 0.1516927033662796, 0.2775607705116272, 0.121961809694767, 0.1493055522441864, 0.2486979216337204, 0.1263020783662796, 0.1640625, 0.3641493022441864, 0.4212239682674408, 0.123914934694767, 0.1447482705116272, 0.0609809048473835, 0.1215277761220932, 0.3111979067325592, 0.1061197891831398, 0.2411024272441864, 0.4407552182674408, 0.4088541567325592, 0.0913628488779068, 0.1395399272441864, 0.1338975727558136, 0.4561631977558136, 0.2469618022441864, 0.1510416716337204, 0.1252170205116272, 0.1447482705116272, 0.132595494389534, 0.2196180522441864, 0.2996961772441864, 0.2044270783662796, 0.4092881977558136, 0.2335069477558136]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009982638992369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0052083334885537624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9665798544883728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7565104365348816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 2997028.038916327 (unstructured) 0 (structured)

max weight is  tensor([2.3348e-09, 1.1824e-08, 4.3922e-11, 3.3327e-09, 1.7418e-09, 6.6238e-01,
        4.5129e-01, 3.7118e-01, 1.5246e-01, 4.6703e-01, 2.4457e-09, 1.4655e-01,
        5.1542e-01, 2.5656e-10, 3.3327e-09, 2.3297e-09, 4.0301e-09, 2.3348e-09,
        3.4201e-09, 1.9956e-09, 4.4530e-10, 2.3348e-09, 3.3327e-09, 1.2938e-11,
        2.2944e-09, 2.3218e-01, 5.5979e-01, 1.9702e-01, 2.8464e-01, 2.9377e-09,
        3.3590e-10, 1.0714e-10, 1.9143e-09, 3.4418e-01, 1.6906e-08, 3.3206e-01,
        2.8581e-01, 1.3163e-09, 4.1495e-10, 3.1109e-09, 9.6082e-10, 3.5415e-09,
        3.8130e-09, 1.2636e-08, 3.2934e-10, 1.0669e-09, 4.3654e-11, 9.8716e-02,
        4.2858e-01, 3.4203e-01, 2.5801e-01, 9.8538e-10, 2.4838e-01, 1.6908e-09,
        3.3878e-09, 3.3374e-10, 3.8130e-09, 2.3348e-09, 1.3032e-01, 4.3143e-09,
        2.9279e-09, 4.5570e-10, 1.4963e-01, 6.7710e-09], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.7522e-01, 2.2380e-01, 3.4740e-07, 1.0322e-07, 8.5906e-08, 7.5391e-02,
        2.5669e-01, 2.5364e-01, 2.3631e-01, 1.2730e-01, 9.6565e-08, 5.5959e-02,
        8.4486e-02, 9.9847e-02, 3.9757e-02, 2.2462e-01, 7.3300e-02, 1.1671e-01,
        6.5046e-02, 1.2860e-01, 1.9920e-01, 6.6742e-02, 6.3973e-02, 1.0003e-01,
        9.1855e-02, 6.3462e-02, 2.6565e-01, 1.7104e-02, 2.1655e-07, 8.4829e-02,
        7.0274e-08, 9.5294e-02, 1.6545e-01, 4.5420e-03, 6.5602e-08, 1.0322e-07,
        9.7363e-02, 9.5763e-02, 1.6275e-01, 7.5533e-08, 1.6580e-01, 1.1277e-07,
        1.1442e-01, 9.8241e-02, 6.2973e-02, 1.2689e-01, 2.2202e-01, 2.7638e-01,
        1.0322e-07, 2.6317e-01, 1.0846e-01, 1.7186e-01, 2.0762e-01, 1.7215e-01,
        2.0784e-07, 1.4835e-01, 3.4750e-07, 4.5058e-07, 1.8540e-01, 3.3745e-07,
        3.1630e-02, 6.7012e-02, 2.0784e-07, 2.5616e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.4319e-07, 1.2108e-01, 1.4118e-01, 2.7173e-07, 2.4196e-08, 1.6452e-01,
        1.8988e-01, 1.6496e-01, 2.5772e-02, 9.3180e-02, 1.0355e-01, 1.9574e-02,
        1.6788e-01, 1.9581e-07, 5.3332e-02, 5.9664e-02, 8.1762e-02, 1.4684e-01,
        2.5984e-07, 2.4081e-07, 9.9629e-08, 1.4675e-01, 1.5731e-01, 8.0330e-02,
        2.3627e-07, 7.8848e-02, 1.5408e-01, 5.4661e-02, 1.0051e-01, 7.3742e-02,
        2.0592e-07, 1.3365e-01, 1.4997e-01, 1.1513e-01, 1.6429e-01, 1.4544e-01,
        8.2302e-02, 1.4606e-01, 1.8235e-01, 2.5097e-07, 1.1343e-01, 5.1532e-02,
        1.2171e-07, 1.7168e-07, 1.2288e-01, 5.0942e-07, 9.2274e-08, 1.1271e-01,
        1.6784e-01, 1.1871e-01, 2.3784e-02, 1.1733e-01, 1.0425e-01, 2.0592e-07,
        1.7168e-07, 1.2967e-01, 9.2272e-08, 1.3780e-01, 1.2021e-01, 1.7168e-07,
        1.7168e-07, 1.5081e-01, 7.6563e-02, 7.3540e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.2585e-07, 1.3054e-01, 1.0649e-07, 1.0650e-01, 8.2913e-02, 1.3112e-01,
        2.8505e-07, 1.2516e-01, 1.7094e-07, 1.0613e-01, 1.3102e-01, 1.2476e-01,
        1.7218e-02, 1.2273e-01, 2.7650e-07, 1.1884e-01, 1.0012e-01, 1.2514e-01,
        1.5034e-07, 1.3100e-01, 9.5232e-02, 2.9148e-07, 1.2314e-01, 5.8798e-07,
        1.4407e-01, 7.9127e-02, 1.7094e-07, 1.0649e-07, 1.1827e-01, 1.0289e-01,
        8.1148e-02, 1.1656e-01, 1.9097e-01, 7.1271e-02, 6.4673e-02, 4.9807e-03,
        1.0005e-01, 1.0649e-07, 1.0519e-01, 1.0649e-07, 3.6072e-07, 1.1094e-01,
        1.0146e-01, 1.3156e-01, 9.5106e-02, 1.8160e-01, 1.0875e-01, 2.8505e-07,
        1.0649e-07, 1.7094e-07, 3.6072e-07, 1.2044e-01, 9.3431e-02, 2.1608e-02,
        4.7856e-02, 3.2075e-07, 2.4938e-02, 2.8505e-07, 6.5858e-02, 8.8652e-02,
        7.4978e-07, 8.7026e-02, 9.7683e-02, 4.7489e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1175, 0.0170, 0.0097, 0.1201, 0.0933, 0.0199, 0.0505, 0.0473, 0.0235,
        0.0173, 0.0917, 0.0155, 0.1166, 0.1125, 0.0636, 0.0480, 0.1114, 0.0097,
        0.1168, 0.0993, 0.1219, 0.0130, 0.0133, 0.0542, 0.1576, 0.0166, 0.0142,
        0.0136, 0.0149, 0.0820, 0.0905, 0.0101, 0.0095, 0.0727, 0.0119, 0.0319,
        0.0120, 0.0544, 0.0480, 0.1470, 0.0132, 0.0612, 0.1050, 0.1199, 0.0110,
        0.1133, 0.1245, 0.0106, 0.0471, 0.0673, 0.0080, 0.0302, 0.0155, 0.1406,
        0.1044, 0.0467, 0.1138, 0.0117, 0.0644, 0.1264, 0.0907, 0.0722, 0.0139,
        0.0034], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.4491e-02, 6.5259e-02, 7.0963e-02, 5.4117e-02, 6.2577e-02, 5.5969e-02,
        6.7705e-02, 5.5972e-02, 6.9211e-02, 8.0918e-02, 5.6537e-02, 6.4114e-02,
        7.2901e-02, 1.0024e-01, 5.3147e-02, 6.2375e-02, 7.3688e-02, 7.9756e-02,
        6.0000e-02, 7.7450e-02, 7.1551e-02, 7.5821e-02, 6.6245e-02, 5.4138e-02,
        8.0351e-02, 6.1945e-02, 5.9717e-02, 7.8938e-02, 6.7948e-02, 7.2645e-02,
        6.6636e-02, 6.9496e-02, 5.8735e-02, 7.8205e-02, 6.6539e-02, 6.2108e-02,
        5.0160e-02, 6.1600e-02, 5.6674e-02, 7.1716e-02, 7.2692e-02, 6.5155e-02,
        6.0094e-02, 6.2080e-02, 6.4247e-02, 6.4568e-02, 8.0381e-02, 7.6745e-02,
        6.0851e-02, 6.2149e-02, 5.6778e-02, 6.0502e-02, 6.9321e-02, 6.2240e-02,
        6.2806e-02, 7.9553e-02, 6.8420e-02, 5.4774e-02, 5.0511e-02, 6.5185e-02,
        7.2688e-02, 7.9105e-02, 5.9004e-02, 7.4283e-02, 1.3280e-07, 6.5733e-02,
        7.2966e-02, 5.7777e-02, 6.8374e-02, 5.3503e-02, 5.9646e-02, 6.8558e-02,
        6.0984e-02, 8.1154e-02, 6.4207e-02, 7.4304e-02, 5.4921e-02, 7.3010e-02,
        7.7714e-02, 7.5822e-02, 6.6602e-02, 7.4515e-02, 5.3509e-02, 5.9478e-02,
        5.9380e-02, 7.2795e-02, 6.3036e-02, 4.6104e-02, 6.2199e-02, 7.0003e-02,
        7.5551e-02, 6.1685e-02, 6.9681e-02, 6.6679e-02, 6.4483e-02, 7.4551e-02,
        6.5952e-02, 7.2668e-02, 7.1857e-02, 6.0287e-02, 6.7009e-02, 3.9259e-02,
        5.4674e-02, 6.9823e-02, 6.9000e-02, 2.6849e-07, 6.6030e-02, 6.4986e-02,
        6.8659e-02, 6.3440e-02, 5.6010e-02, 6.6542e-02, 6.8363e-02, 7.0310e-02,
        6.9388e-02, 6.8562e-02, 4.5811e-02, 6.3675e-02, 5.5242e-02, 7.6363e-02,
        8.1055e-02, 7.3626e-02, 6.5531e-02, 6.0776e-02, 6.3114e-02, 9.7307e-02,
        7.3487e-02, 6.1374e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.5770e-02, 7.1938e-02, 7.7193e-02, 7.9711e-02, 8.1598e-02, 8.2149e-02,
        7.2161e-02, 7.3967e-02, 6.8920e-02, 2.6294e-02, 8.2431e-02, 8.0276e-02,
        6.5623e-02, 7.2704e-02, 7.9952e-02, 7.8698e-02, 7.9936e-02, 8.2076e-02,
        5.7496e-02, 9.4323e-02, 6.4566e-02, 7.7804e-02, 6.7815e-02, 3.8632e-02,
        1.5079e-01, 7.4039e-02, 7.1184e-02, 6.7025e-02, 6.1073e-02, 5.8416e-02,
        1.1669e-01, 7.9221e-02, 7.8552e-02, 4.2015e-02, 4.0004e-03, 9.1910e-02,
        5.2151e-02, 6.4753e-02, 7.8699e-02, 1.1143e-01, 3.5968e-02, 8.7221e-02,
        6.6430e-02, 7.9092e-02, 8.5070e-02, 7.0534e-02, 4.5636e-02, 7.6592e-02,
        7.0335e-02, 7.2985e-02, 6.0617e-02, 6.8745e-02, 6.0679e-02, 2.9074e-02,
        1.3236e-03, 7.7758e-02, 2.9100e-02, 5.6850e-02, 8.1859e-02, 7.4999e-02,
        1.2343e-06, 5.6341e-02, 6.9442e-02, 8.0042e-02, 6.5152e-02, 7.4572e-02,
        7.7834e-02, 5.8447e-02, 7.5940e-02, 5.9188e-02, 7.1748e-02, 5.3962e-02,
        7.3904e-02, 8.4157e-02, 1.2775e-01, 4.5963e-02, 9.2674e-02, 7.5165e-03,
        7.1638e-02, 3.1249e-02, 6.6723e-02, 9.9661e-02, 7.8320e-02, 6.4654e-02,
        7.1348e-02, 1.2962e-01, 8.1291e-02, 7.6669e-02, 6.9406e-02, 7.6078e-02,
        1.3192e-06, 5.3728e-02, 5.4914e-02, 6.9465e-02, 1.6038e-03, 2.2495e-03,
        5.1412e-02, 7.7648e-02, 8.1377e-02, 7.4896e-02, 7.5579e-02, 6.4015e-02,
        7.2612e-02, 5.4488e-02, 9.8083e-02, 6.8835e-02, 7.2775e-02, 7.6274e-02,
        6.5058e-02, 6.4531e-07, 7.6743e-02, 8.4009e-02, 7.0782e-02, 7.8794e-02,
        6.6208e-02, 8.6861e-02, 9.4076e-02, 6.2171e-02, 4.7425e-02, 6.2630e-02,
        7.8591e-02, 5.1980e-02, 8.7936e-02, 3.8119e-02, 9.7095e-02, 7.3308e-02,
        7.7732e-02, 8.2040e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1306e-01, 1.0311e-01, 5.1719e-02, 5.4836e-02, 5.5933e-02, 8.5892e-02,
        9.0123e-02, 6.9435e-02, 6.5551e-02, 2.3213e-01, 8.8823e-02, 8.5411e-02,
        6.3352e-02, 7.5070e-02, 1.0142e-01, 5.2504e-02, 9.3821e-02, 6.1660e-02,
        1.2437e-01, 7.6994e-02, 7.8605e-02, 7.7926e-02, 1.7653e-01, 1.7815e-01,
        4.1598e-02, 6.4638e-02, 5.7784e-02, 1.0544e-01, 9.3524e-02, 1.2887e-01,
        8.2491e-02, 1.2659e-01, 6.8903e-02, 2.1912e-01, 1.5727e-01, 6.9836e-02,
        1.2289e-01, 7.3059e-02, 7.2580e-02, 2.8975e-02, 1.9714e-01, 1.0174e-01,
        1.0780e-01, 9.7010e-02, 5.4439e-02, 7.7308e-02, 1.5672e-01, 1.3469e-01,
        8.6165e-02, 1.4151e-01, 1.7021e-01, 8.4959e-02, 1.3355e-01, 2.2578e-01,
        1.5691e-01, 6.2278e-02, 1.1603e-02, 1.5730e-01, 4.4973e-02, 7.4126e-02,
        4.9585e-09, 8.8803e-02, 7.6204e-02, 6.0711e-02, 1.0101e-01, 4.8515e-02,
        7.8263e-02, 9.9085e-02, 8.8431e-02, 1.4631e-01, 6.0487e-02, 1.4490e-01,
        1.1689e-01, 7.4108e-02, 3.7071e-02, 3.3523e-01, 3.9013e-02, 1.1979e-01,
        8.3321e-02, 1.5079e-01, 8.1578e-02, 5.0338e-02, 1.5787e-01, 6.2339e-02,
        1.1263e-01, 5.9543e-02, 7.8953e-02, 8.1691e-02, 1.3187e-01, 8.8815e-02,
        8.6246e-09, 1.6214e-01, 1.0327e-01, 5.1134e-02, 1.4352e-01, 1.9257e-01,
        2.4543e-01, 5.4149e-02, 1.1094e-01, 8.2210e-02, 7.0836e-02, 8.8921e-02,
        1.3022e-01, 1.3048e-01, 5.9893e-02, 9.7344e-02, 9.2653e-02, 6.4337e-02,
        6.3414e-02, 2.2872e-08, 1.7269e-01, 8.7939e-02, 7.3587e-02, 8.4965e-02,
        1.2424e-01, 1.4943e-01, 9.4122e-02, 8.7888e-02, 3.1539e-02, 6.9179e-02,
        3.9142e-02, 5.1515e-02, 9.1175e-02, 1.8246e-01, 9.0822e-02, 8.9632e-02,
        6.4744e-02, 8.3943e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.4762e-06, 4.0662e-07, 5.8872e-02, 4.8277e-02, 3.2721e-02, 3.6118e-02,
        4.1127e-07, 7.7629e-02, 4.9011e-02, 1.9628e-07, 4.6286e-07, 6.5766e-02,
        6.7033e-02, 5.8959e-02, 9.5453e-02, 4.2108e-02, 2.9072e-06, 6.3407e-02,
        5.6581e-02, 4.7356e-02, 6.5179e-02, 4.0330e-02, 5.5288e-02, 4.1196e-02,
        6.3961e-02, 6.2351e-02, 7.1173e-02, 4.8064e-07, 5.3932e-02, 5.4122e-02,
        4.3003e-02, 4.1127e-07, 1.3786e-06, 5.1432e-02, 5.4635e-02, 2.4438e-07,
        1.0065e-01, 5.4664e-02, 6.0495e-02, 3.9150e-07, 6.2984e-02, 4.7664e-02,
        6.6063e-02, 6.9459e-02, 8.0357e-02, 1.1943e-06, 6.2272e-02, 7.0516e-02,
        6.1052e-02, 2.7551e-07, 3.9771e-02, 7.8448e-07, 5.7610e-02, 6.0253e-02,
        6.7247e-02, 6.5268e-02, 9.4802e-02, 3.3144e-02, 6.4020e-02, 4.4090e-07,
        5.5256e-02, 1.3786e-06, 9.3706e-02, 1.3786e-06, 8.0173e-07, 1.7001e-06,
        5.7631e-02, 4.4606e-02, 4.3412e-02, 5.9756e-02, 8.7754e-07, 1.0475e-06,
        5.4658e-02, 8.7910e-07, 5.5495e-02, 6.4647e-02, 5.9852e-02, 6.3276e-02,
        5.1054e-02, 8.0173e-07, 7.1020e-02, 5.7900e-02, 7.3424e-02, 1.0763e-01,
        6.1777e-02, 5.6918e-02, 4.9253e-07, 5.5216e-02, 2.2921e-06, 6.6706e-02,
        3.5881e-02, 4.3272e-02, 5.4025e-02, 2.3196e-06, 4.0662e-07, 6.4413e-02,
        5.8193e-02, 5.8950e-02, 1.5555e-06, 6.9634e-02, 4.7200e-02, 5.1820e-02,
        6.5372e-02, 5.1314e-02, 1.0622e-06, 7.3751e-02, 6.2104e-02, 3.7065e-02,
        1.0637e-06, 1.0682e-01, 9.5085e-02, 7.8381e-07, 5.2917e-02, 6.8946e-02,
        6.4233e-02, 6.7725e-02, 2.7261e-07, 4.9468e-02, 6.4877e-07, 4.3159e-02,
        5.5843e-02, 5.8311e-02, 6.6348e-02, 6.8054e-02, 5.6391e-02, 6.1506e-02,
        5.7138e-02, 5.7999e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.9778e-03, 4.5479e-02, 7.9151e-02, 6.5340e-02, 5.5934e-02, 6.5678e-02,
        5.7621e-02, 4.3838e-03, 6.7822e-02, 9.3357e-03, 4.1530e-02, 3.9637e-03,
        4.3522e-02, 6.3963e-03, 5.8874e-02, 5.9591e-02, 3.6761e-03, 7.5329e-02,
        4.2278e-03, 3.3116e-03, 6.5599e-02, 1.8819e-03, 4.3849e-02, 3.2612e-02,
        3.6332e-02, 6.1003e-02, 7.1421e-02, 7.0282e-02, 5.1565e-02, 5.7303e-02,
        4.9745e-02, 3.8337e-03, 3.7062e-02, 6.5586e-02, 7.0041e-02, 5.2591e-02,
        5.2722e-02, 6.7013e-02, 7.2362e-02, 6.1493e-02, 8.8991e-04, 7.7184e-03,
        3.7315e-02, 4.2777e-02, 9.4694e-02, 4.5524e-07, 8.6254e-03, 2.1682e-03,
        2.1090e-03, 4.0225e-02, 7.8076e-03, 4.9944e-02, 1.3835e-02, 3.4793e-02,
        7.3995e-02, 5.4220e-02, 7.5476e-02, 5.4636e-02, 5.8700e-02, 1.0471e-01,
        6.7943e-02, 5.9804e-02, 5.6531e-02, 7.1242e-02, 5.5626e-02, 1.1230e-04,
        4.7503e-03, 6.5913e-02, 2.9483e-03, 2.5667e-02, 6.2408e-02, 4.7873e-02,
        5.7162e-02, 8.4152e-02, 6.3017e-02, 5.7894e-02, 6.5622e-02, 4.5757e-02,
        7.9956e-02, 6.0967e-02, 5.7320e-02, 6.0745e-02, 2.0896e-03, 5.3728e-02,
        7.2299e-02, 6.5831e-02, 7.1933e-02, 6.9770e-02, 1.6548e-02, 3.9363e-02,
        8.1518e-02, 7.1312e-03, 5.8630e-02, 6.1529e-02, 3.0259e-07, 3.0916e-03,
        3.0944e-03, 7.1871e-02, 3.9939e-02, 5.8313e-03, 5.6341e-02, 6.9671e-02,
        3.3290e-03, 4.2016e-03, 3.9520e-02, 8.6973e-02, 5.3100e-03, 4.2130e-02,
        5.0512e-02, 8.3565e-02, 5.8872e-03, 5.5365e-02, 3.4714e-03, 5.4496e-02,
        7.6178e-03, 5.7629e-02, 7.0099e-03, 5.6859e-02, 7.8664e-02, 5.3014e-02,
        5.0261e-02, 7.2578e-02, 6.9651e-02, 4.8599e-03, 5.5925e-02, 7.1319e-02,
        5.7730e-02, 8.2206e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.9897e-07, 6.3011e-02, 6.4967e-02, 6.2901e-02, 5.7003e-02, 6.2543e-02,
        6.8912e-02, 6.6800e-02, 6.1389e-02, 1.7913e-06, 6.6426e-02, 6.7035e-02,
        7.1640e-02, 5.6847e-02, 5.8403e-02, 6.1510e-02, 5.8108e-02, 6.7718e-02,
        5.4509e-02, 1.9425e-07, 6.5105e-02, 6.4691e-02, 5.8741e-02, 5.2835e-02,
        5.6112e-02, 3.6395e-07, 6.4468e-02, 6.1305e-02, 6.8382e-02, 6.2752e-02,
        5.5651e-02, 6.4934e-02, 6.8659e-02, 6.4106e-02, 5.0566e-02, 4.6853e-02,
        5.7230e-02, 6.0348e-02, 7.4224e-02, 6.0178e-02, 5.7692e-02, 6.4966e-02,
        7.0059e-02, 6.8641e-02, 5.7700e-02, 6.3755e-02, 6.7676e-02, 7.0097e-02,
        1.7913e-06, 5.5952e-07, 6.1753e-02, 6.2156e-02, 6.0582e-02, 9.8231e-07,
        6.1822e-02, 6.4629e-02, 5.5952e-07, 6.5299e-02, 6.9441e-02, 7.0794e-02,
        6.8675e-02, 6.1515e-02, 5.5301e-02, 6.7451e-02, 5.8141e-02, 5.5952e-07,
        6.5103e-02, 6.1714e-02, 8.9897e-07, 1.1561e-06, 7.2015e-02, 6.3989e-02,
        6.4347e-02, 5.8721e-02, 6.0156e-02, 6.8359e-02, 7.0086e-02, 6.1721e-02,
        6.4656e-02, 5.8219e-02, 6.1536e-02, 2.5956e-07, 6.0839e-02, 5.9647e-02,
        5.7564e-02, 6.8933e-02, 7.0809e-02, 1.9915e-07, 5.5010e-02, 6.3086e-02,
        6.1888e-02, 5.8101e-02, 5.2640e-02, 7.7329e-02, 6.7218e-02, 6.5645e-02,
        6.8655e-02, 6.3407e-02, 6.6625e-02, 5.8170e-02, 6.3166e-02, 6.7042e-02,
        6.2542e-02, 6.7631e-02, 6.9542e-02, 6.2722e-02, 1.2384e-06, 6.6805e-02,
        6.4235e-02, 6.2391e-02, 6.5789e-02, 6.3023e-02, 7.5665e-02, 6.3725e-02,
        7.0514e-02, 7.6547e-02, 1.9915e-07, 6.3292e-02, 5.9435e-02, 7.1924e-02,
        5.9203e-02, 6.5671e-02, 6.2615e-02, 6.6577e-02, 6.6144e-02, 6.8397e-02,
        6.3666e-02, 5.8569e-02, 5.9570e-02, 5.4205e-02, 7.2148e-02, 6.6944e-02,
        6.4042e-02, 6.3747e-02, 1.1301e-06, 7.2474e-02, 6.6249e-02, 6.3232e-02,
        6.1581e-02, 3.6507e-07, 6.3553e-02, 5.9468e-02, 6.6409e-02, 6.7191e-02,
        5.2902e-02, 5.3165e-02, 6.2563e-02, 1.1071e-06, 6.2618e-02, 5.2108e-02,
        7.0843e-02, 5.5856e-02, 5.8019e-02, 6.9060e-02, 7.4288e-02, 6.1858e-02,
        6.5501e-02, 5.6832e-02, 7.1941e-02, 6.4610e-02, 3.8504e-07, 6.0662e-02,
        6.6677e-02, 6.6826e-02, 7.1368e-02, 7.7747e-02, 1.1071e-06, 6.2392e-02,
        6.3872e-02, 7.0259e-02, 6.1629e-02, 7.2183e-02, 7.8517e-02, 5.7327e-02,
        6.8806e-02, 7.2606e-02, 1.9915e-07, 6.5896e-02, 7.0322e-02, 7.1521e-02,
        7.2508e-02, 6.8135e-02, 7.0665e-02, 5.8524e-02, 6.2604e-02, 5.5872e-02,
        6.5955e-02, 6.4718e-02, 6.8122e-02, 6.3321e-02, 6.7583e-02, 5.1516e-02,
        6.4742e-02, 6.3383e-02, 6.2697e-02, 5.6689e-02, 6.2065e-02, 6.2988e-02,
        6.5192e-02, 7.0335e-02, 6.8419e-02, 6.6059e-02, 6.6659e-02, 7.2708e-02,
        5.8591e-02, 1.0981e-06, 5.1245e-02, 6.1631e-02, 5.6487e-02, 6.5267e-02,
        2.5956e-07, 6.8303e-02, 8.6159e-07, 7.2914e-02, 6.5385e-02, 6.9449e-02,
        6.9248e-02, 5.9849e-02, 6.5146e-02, 7.0231e-02, 6.1164e-02, 5.2042e-02,
        5.7574e-02, 6.7873e-02, 5.0159e-07, 6.1213e-02, 6.9981e-02, 5.6612e-02,
        6.9397e-02, 5.5748e-02, 7.3924e-02, 6.6264e-02, 6.2644e-02, 6.3934e-02,
        6.9596e-02, 5.6980e-02, 5.7168e-02, 6.7074e-02, 6.5457e-02, 5.9767e-02,
        6.2240e-02, 5.7330e-02, 6.3965e-02, 7.4354e-02, 6.8549e-02, 6.2776e-02,
        6.2957e-02, 4.4950e-02, 7.3202e-02, 5.5644e-02, 4.4975e-07, 1.1301e-06,
        6.8320e-02, 5.8723e-02, 5.5897e-02, 6.3777e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.6878e-02, 1.7152e-06, 5.7037e-02, 7.4515e-02, 5.4441e-02, 2.0073e-06,
        5.6651e-02, 7.1561e-02, 6.8729e-02, 1.5625e-06, 5.2981e-02, 1.6863e-06,
        6.7648e-02, 2.0063e-06, 1.0709e-01, 8.0150e-02, 4.4589e-06, 5.2860e-02,
        6.6650e-02, 7.3126e-02, 9.9070e-07, 6.9639e-02, 6.2543e-02, 5.5672e-02,
        6.6817e-07, 5.9408e-02, 7.7727e-02, 5.3257e-02, 8.0883e-02, 3.9577e-06,
        5.4811e-02, 4.2356e-02, 2.0062e-06, 5.9471e-02, 7.3690e-02, 8.0091e-07,
        1.3582e-06, 8.8826e-07, 6.4922e-02, 4.3471e-02, 1.4660e-06, 5.2734e-02,
        5.9592e-02, 1.6846e-06, 2.3057e-06, 7.4091e-02, 7.1206e-02, 2.2060e-06,
        1.1047e-06, 2.1751e-06, 1.4482e-06, 5.4968e-02, 6.5980e-02, 7.1115e-02,
        7.7070e-02, 6.0829e-02, 4.3620e-02, 5.2141e-02, 1.0358e-06, 5.7294e-02,
        1.0932e-06, 9.2147e-07, 2.4398e-06, 6.4155e-02, 6.6939e-02, 2.0062e-06,
        6.1150e-07, 2.0707e-06, 5.8723e-02, 5.1010e-02, 4.3326e-06, 2.8287e-06,
        5.6351e-07, 4.7051e-02, 3.9784e-06, 5.8106e-02, 7.0826e-02, 2.4116e-06,
        7.5897e-02, 6.2383e-02, 8.2983e-02, 4.7034e-02, 2.0821e-06, 7.7574e-02,
        2.0065e-06, 6.4152e-02, 9.2709e-07, 6.0467e-02, 3.9615e-04, 4.7231e-06,
        3.2801e-06, 7.7497e-02, 6.3083e-02, 6.1216e-02, 2.9874e-06, 4.9393e-06,
        7.0810e-02, 8.9281e-07, 2.0064e-02, 2.2160e-06, 5.5330e-02, 5.8550e-02,
        7.2915e-02, 7.0282e-02, 8.0438e-02, 7.3949e-02, 3.2796e-06, 9.0604e-07,
        6.4463e-02, 6.2042e-02, 3.1951e-06, 6.6793e-02, 4.4779e-02, 2.0801e-06,
        1.5160e-06, 2.2011e-06, 6.6441e-02, 6.6897e-02, 6.4325e-02, 6.6441e-02,
        1.3879e-06, 5.0181e-02, 6.0098e-02, 1.7209e-06, 6.8729e-02, 5.6179e-02,
        6.2012e-02, 2.0811e-06, 3.6893e-06, 1.5627e-06, 5.5346e-07, 3.9791e-06,
        7.4531e-02, 8.1007e-02, 5.6279e-02, 7.3254e-02, 9.5373e-07, 5.3796e-02,
        4.6140e-02, 4.1961e-02, 6.7248e-02, 4.6620e-07, 5.3039e-02, 1.8727e-06,
        1.1847e-06, 6.2353e-02, 7.3994e-02, 4.3909e-06, 7.2283e-07, 8.0091e-07,
        6.3477e-02, 1.1858e-06, 4.4743e-02, 6.4753e-02, 6.5762e-02, 4.5446e-02,
        5.1015e-02, 7.4939e-02, 4.7277e-02, 5.1700e-02, 1.5635e-06, 7.1581e-02,
        2.0071e-06, 5.0961e-02, 1.5227e-06, 4.4286e-02, 2.8204e-06, 6.1880e-02,
        2.4405e-06, 7.9114e-02, 2.8232e-06, 8.2349e-02, 2.0977e-06, 5.1362e-02,
        5.5508e-02, 5.2826e-02, 2.2160e-06, 6.1026e-02, 5.9103e-02, 5.9640e-02,
        4.4568e-06, 6.3261e-07, 5.0375e-02, 5.3465e-02, 6.3173e-07, 6.1401e-02,
        2.4397e-06, 2.5497e-06, 2.4715e-06, 5.7439e-02, 6.4546e-02, 6.5325e-02,
        7.1929e-02, 5.9359e-02, 7.0485e-02, 5.4106e-02, 7.1030e-02, 6.0434e-02,
        6.1376e-02, 2.8323e-06, 9.1187e-06, 8.9254e-07, 6.9533e-02, 6.4133e-02,
        2.0509e-06, 5.8857e-02, 5.4357e-02, 5.4817e-02, 5.6214e-02, 1.0739e-01,
        1.0949e-06, 5.2246e-02, 4.7301e-02, 3.9264e-02, 5.0511e-02, 6.5275e-02,
        7.7807e-02, 2.0138e-06, 4.9531e-02, 2.2162e-06, 5.5419e-02, 7.0946e-02,
        6.0366e-02, 5.7473e-02, 4.6953e-02, 6.2621e-02, 1.9390e-06, 5.6888e-02,
        4.9071e-02, 5.9581e-02, 6.0436e-02, 6.0057e-02, 1.1133e-06, 6.4483e-02,
        6.6675e-02, 3.4512e-06, 3.5992e-06, 6.8148e-02, 6.2727e-02, 5.3301e-02,
        5.6279e-02, 1.3880e-06, 6.2384e-02, 6.5444e-02, 7.9072e-07, 6.4964e-02,
        2.3487e-06, 7.2240e-02, 6.0714e-02, 5.2713e-02, 4.3475e-02, 1.6326e-04,
        6.5326e-02, 5.9077e-02, 4.8071e-02, 4.9844e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.7218e-02, 5.8496e-08, 3.9151e-02, 2.3883e-02, 4.0144e-02, 3.0221e-02,
        2.9844e-02, 2.4975e-02, 3.4380e-02, 2.0819e-02, 4.6862e-02, 4.3780e-02,
        3.6113e-02, 3.7843e-02, 3.6977e-02, 3.7525e-02, 2.1582e-02, 2.8003e-02,
        5.6809e-02, 4.0122e-02, 5.6785e-02, 4.3441e-02, 3.6705e-02, 3.6885e-02,
        3.8916e-02, 3.8405e-02, 4.5326e-02, 5.8307e-02, 4.8952e-02, 1.2273e-02,
        3.8044e-02, 2.5376e-02, 2.3984e-02, 4.0465e-02, 2.3680e-02, 6.4527e-08,
        8.0818e-08, 3.8562e-02, 4.1656e-02, 5.2736e-02, 3.7784e-02, 4.4538e-02,
        4.9638e-02, 4.3670e-08, 1.9141e-02, 5.3121e-02, 3.2515e-02, 5.8767e-02,
        3.9525e-08, 3.6391e-02, 7.3327e-02, 4.1806e-02, 2.9515e-02, 4.1330e-02,
        3.8586e-02, 3.7676e-02, 4.3624e-02, 5.0117e-02, 5.2132e-08, 3.9830e-02,
        2.2834e-08, 5.8496e-08, 2.0397e-02, 3.0217e-02, 4.4027e-02, 3.3203e-02,
        4.4468e-02, 3.9679e-02, 4.9046e-02, 3.5079e-02, 5.5774e-08, 4.5283e-02,
        4.0393e-02, 2.7416e-02, 5.0448e-02, 5.4821e-02, 2.4559e-02, 4.8853e-02,
        3.4484e-02, 1.7696e-02, 2.3843e-02, 3.2100e-02, 5.4536e-02, 4.4788e-02,
        2.4617e-02, 5.5091e-02, 1.0118e-07, 5.3873e-02, 5.5222e-02, 1.6151e-02,
        1.3535e-07, 6.0889e-02, 4.6940e-02, 4.0177e-02, 1.8537e-07, 6.3357e-02,
        2.3669e-02, 1.4589e-02, 1.7200e-01, 3.3428e-02, 2.0726e-02, 3.7161e-02,
        5.1972e-02, 5.2768e-02, 4.4536e-02, 4.4374e-02, 1.7480e-02, 3.7987e-02,
        5.1937e-02, 3.9069e-02, 3.9243e-02, 4.5911e-02, 6.2701e-02, 4.9819e-02,
        3.3704e-02, 8.4214e-02, 3.9167e-02, 4.7743e-02, 2.1570e-02, 3.6049e-02,
        4.5785e-02, 4.0427e-02, 4.9901e-02, 5.6427e-02, 5.1588e-02, 5.0837e-02,
        4.1511e-02, 6.1087e-02, 8.1943e-08, 4.0433e-02, 3.3633e-02, 8.3653e-02,
        3.8323e-02, 3.8713e-02, 3.8009e-02, 4.1035e-02, 1.4810e-02, 3.8357e-02,
        2.0014e-02, 3.5120e-02, 3.6361e-02, 1.6786e-02, 5.1456e-02, 5.0933e-02,
        2.0279e-02, 4.9314e-02, 7.8285e-02, 5.8496e-08, 1.0432e-02, 6.0553e-08,
        4.9512e-02, 4.6533e-02, 4.9446e-02, 3.6598e-02, 4.2385e-02, 5.1852e-02,
        4.0080e-02, 4.5052e-02, 3.3919e-02, 4.6713e-02, 1.8312e-07, 3.3649e-02,
        2.0427e-02, 2.3476e-02, 5.8849e-03, 6.1080e-02, 1.2204e-03, 3.3194e-02,
        3.5089e-02, 4.7746e-02, 5.4681e-03, 5.7197e-02, 4.1632e-02, 3.9239e-02,
        4.7040e-02, 4.4000e-02, 3.6761e-02, 4.7671e-02, 3.6026e-02, 2.0503e-02,
        2.5735e-02, 3.4362e-08, 3.3670e-02, 2.3198e-02, 1.2558e-07, 2.8314e-02,
        3.9648e-02, 7.3086e-02, 3.7749e-02, 5.8838e-02, 4.8239e-02, 6.3815e-02,
        4.1108e-02, 3.6023e-02, 3.1325e-02, 5.4692e-02, 4.7833e-02, 3.1690e-02,
        4.4032e-02, 5.8632e-08, 5.7308e-02, 5.0648e-03, 5.9577e-02, 4.0364e-02,
        5.1924e-02, 3.0696e-02, 3.5231e-02, 2.5622e-02, 5.1587e-02, 5.7793e-02,
        3.6600e-02, 4.2863e-02, 5.6634e-02, 3.3155e-02, 4.1611e-02, 4.0218e-02,
        3.5559e-02, 4.6745e-02, 4.7479e-02, 2.4323e-02, 3.8519e-02, 4.5042e-02,
        4.0899e-02, 4.3508e-02, 4.1250e-02, 4.9300e-02, 3.6508e-02, 3.2418e-02,
        4.8467e-02, 4.8471e-02, 4.2358e-02, 3.9963e-02, 1.8797e-07, 3.2969e-02,
        3.6957e-02, 1.3514e-02, 2.5408e-02, 6.3440e-02, 3.0076e-02, 3.1707e-02,
        4.9433e-02, 2.8980e-02, 5.9436e-02, 2.9767e-02, 6.0553e-08, 3.6756e-02,
        4.0996e-02, 5.5519e-02, 4.9359e-02, 4.9352e-02, 4.9068e-02, 5.5142e-02,
        4.5134e-02, 5.2510e-02, 2.7579e-02, 2.3642e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.3328e-07, 1.0101e-06, 8.7180e-02, 7.3328e-07, 1.0420e-06, 1.0400e-06,
        7.5089e-02, 7.3950e-02, 1.1498e-06, 4.1467e-06, 6.1912e-07, 7.2686e-02,
        2.2788e-06, 7.1890e-02, 2.5386e-06, 7.7365e-02, 1.7122e-06, 1.8459e-06,
        9.7220e-02, 5.2628e-07, 1.5890e-06, 8.0817e-07, 6.4578e-02, 6.5441e-02,
        2.5386e-06, 1.0472e-06, 4.1467e-06, 9.2432e-07, 1.7004e-06, 2.3779e-06,
        9.1508e-02, 2.5386e-06, 9.7949e-02, 9.2432e-07, 1.0101e-06, 1.2190e-06,
        1.0420e-06, 7.5623e-02, 2.3779e-06, 2.3441e-06, 7.4674e-07, 1.5724e-06,
        8.2190e-02, 2.5999e-06, 6.7265e-07, 5.0131e-07, 1.9580e-06, 8.6598e-02,
        3.6401e-06, 6.1912e-07, 7.9243e-07, 7.9243e-07, 2.3779e-06, 5.1279e-07,
        2.3779e-06, 1.9580e-06, 3.7711e-06, 7.3745e-02, 7.1373e-02, 8.7179e-02,
        8.4353e-02, 6.7265e-07, 1.7004e-06, 9.0131e-02, 2.5386e-06, 9.3803e-02,
        2.5386e-06, 6.1912e-07, 1.5890e-06, 1.1498e-06, 9.1848e-02, 1.1587e-06,
        6.1913e-07, 2.5386e-06, 5.6250e-07, 2.3382e-06, 7.9243e-07, 9.2432e-07,
        1.7004e-06, 7.9243e-07, 1.1392e-06, 7.9243e-07, 6.2707e-02, 6.9889e-02,
        5.6250e-07, 1.0420e-06, 4.1467e-06, 1.1587e-06, 1.1587e-06, 4.1467e-06,
        7.4846e-07, 1.1951e-06, 1.1392e-06, 9.7650e-02, 7.8717e-02, 2.5386e-06,
        3.6401e-06, 7.3328e-07, 1.8459e-06, 1.0101e-06, 6.9395e-07, 6.1889e-02,
        1.5890e-06, 6.7265e-07, 1.1834e-06, 7.2161e-02, 1.5724e-06, 1.8459e-06,
        4.1467e-06, 5.6250e-07, 7.8343e-02, 5.3349e-02, 5.3770e-02, 1.0420e-06,
        8.1681e-02, 2.9982e-02, 1.0105e-01, 7.6706e-02, 3.1885e-06, 5.3270e-07,
        7.6674e-02, 5.3270e-07, 9.2474e-02, 9.7488e-02, 2.1042e-06, 2.5386e-06,
        2.0157e-06, 5.3075e-07, 1.1587e-06, 1.5890e-06, 1.1392e-06, 5.1279e-07,
        7.9243e-07, 5.5384e-02, 7.0297e-02, 5.1279e-07, 5.1279e-07, 4.3520e-02,
        1.8459e-06, 2.3499e-06, 6.7265e-07, 9.2432e-07, 2.3779e-06, 5.6250e-07,
        1.0686e-06, 1.7004e-06, 1.0124e-01, 2.5386e-06, 1.1617e-01, 1.9580e-06,
        4.1467e-06, 5.1279e-07, 8.7704e-02, 1.4010e-06, 1.0101e-06, 6.7265e-07,
        8.0222e-02, 3.5895e-06, 5.6250e-07, 8.4351e-02, 2.3779e-06, 6.7265e-07,
        1.8062e-07, 7.5316e-02, 1.2190e-06, 5.3270e-07, 7.7798e-02, 7.3328e-07,
        1.5724e-06, 1.7139e-06, 3.6401e-06, 1.2190e-06, 8.5544e-02, 1.8459e-06,
        1.2688e-06, 7.9631e-02, 2.5386e-06, 1.0472e-06, 7.9243e-07, 1.2190e-06,
        4.1467e-06, 8.2640e-02, 1.1587e-06, 6.9348e-02, 2.2788e-06, 7.1819e-02,
        2.6617e-08, 9.2432e-07, 1.5890e-06, 7.2861e-02, 5.6250e-07, 5.1279e-07,
        2.3779e-06, 1.1129e-06, 1.9580e-06, 8.9112e-02, 2.2788e-06, 7.0413e-07,
        2.3779e-06, 7.6751e-02, 2.5386e-06, 8.0817e-07, 8.2567e-02, 1.1498e-06,
        1.0472e-06, 9.2432e-07, 5.6250e-07, 2.5386e-06, 7.3328e-07, 1.1392e-06,
        1.0972e-06, 1.5890e-06, 6.5671e-02, 5.6055e-02, 6.7265e-07, 8.0817e-07,
        8.8028e-07, 3.1976e-06, 1.1834e-06, 6.7265e-07, 9.5246e-02, 1.8459e-06,
        5.6250e-07, 3.7215e-06, 1.1498e-06, 7.1549e-02, 3.1885e-06, 1.2190e-06,
        2.2788e-06, 7.6728e-02, 2.5386e-06, 2.3779e-06, 5.8899e-02, 4.1467e-06,
        7.4334e-02, 1.5890e-06, 4.4651e-07, 1.9580e-06, 9.2432e-07, 6.7265e-07,
        6.9970e-02, 1.0101e-06, 8.0016e-02, 2.5386e-06, 6.1912e-07, 5.1279e-07,
        9.3770e-02, 1.4195e-06, 1.0400e-06, 1.0420e-06, 1.6898e-06, 3.1885e-06,
        1.5890e-06, 7.4038e-02, 2.6613e-08, 4.1467e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.8898e-06, 4.3352e-02, 7.1719e-07, 4.2294e-02, 1.4769e-06, 2.9357e-06,
        4.0751e-02, 1.6589e-06, 1.7385e-06, 1.8375e-06, 9.7454e-07, 1.0421e-06,
        2.8679e-02, 3.3469e-06, 4.0152e-02, 1.0567e-06, 3.2023e-06, 2.8351e-02,
        1.3235e-06, 2.0305e-06, 9.6473e-07, 1.3649e-06, 8.1680e-07, 2.6263e-02,
        2.9608e-06, 1.1533e-06, 1.5219e-06, 1.5693e-06, 1.0755e-01, 2.2183e-06,
        4.4361e-06, 3.4073e-06, 1.2290e-06, 9.8922e-02, 4.5214e-06, 1.2899e-06,
        1.5810e-06, 1.3991e-06, 3.2001e-06, 2.0317e-06, 4.6657e-07, 1.3637e-06,
        7.5200e-07, 3.8947e-06, 2.4295e-06, 4.9923e-02, 4.4057e-02, 1.3509e-06,
        4.6001e-02, 8.3818e-07, 1.0639e-06, 1.7248e-06, 4.2541e-06, 5.5488e-02,
        3.0311e-06, 2.0419e-06, 2.2882e-06, 3.1616e-06, 4.4359e-06, 3.3472e-06,
        3.3469e-06, 2.8784e-02, 4.0693e-06, 1.2021e-06, 1.2362e-06, 2.1549e-06,
        2.8789e-06, 9.7706e-07, 4.2265e-02, 3.4452e-02, 2.5378e-07, 2.7041e-06,
        2.2855e-06, 1.7369e-06, 2.2503e-06, 1.6611e-06, 3.0946e-06, 1.4772e-06,
        8.7587e-07, 3.1605e-06, 9.4294e-07, 3.6322e-06, 1.7884e-06, 1.7232e-06,
        1.4975e-06, 1.7306e-02, 5.0678e-02, 3.5128e-02, 3.6319e-06, 4.4358e-06,
        1.4938e-06, 2.2525e-06, 3.6329e-06, 1.2783e-06, 1.1425e-06, 2.2902e-06,
        1.7371e-06, 1.7260e-06, 3.3166e-02, 3.3476e-06, 2.4276e-06, 1.7373e-06,
        3.8615e-06, 1.3806e-06, 2.5627e-06, 1.2774e-06, 1.0245e-06, 2.8073e-06,
        2.5632e-06, 6.9545e-05, 6.7733e-07, 1.8258e-06, 1.1706e-06, 9.7773e-07,
        1.7257e-06, 3.0256e-06, 4.1416e-02, 8.1680e-07, 1.5858e-06, 3.2784e-02,
        1.0240e-06, 1.5221e-06, 2.7105e-06, 1.2791e-06, 1.5219e-06, 4.4678e-02,
        1.6979e-06, 9.7789e-07, 2.6814e-06, 1.6365e-06, 4.7155e-06, 1.0173e-06,
        1.3822e-06, 2.5106e-06, 3.1907e-06, 5.5042e-07, 3.6149e-06, 2.7230e-06,
        1.6842e-06, 1.0419e-06, 3.6899e-02, 1.9429e-06, 2.5320e-06, 3.6367e-06,
        1.4830e-06, 2.7225e-06, 9.6014e-02, 2.2164e-06, 1.5485e-06, 1.7260e-06,
        1.3217e-06, 1.7192e-06, 1.1612e-06, 9.7609e-07, 8.7271e-02, 3.6321e-06,
        1.0481e-06, 4.2876e-02, 1.7400e-06, 4.3174e-02, 1.6855e-06, 1.7257e-06,
        1.4301e-06, 2.6141e-06, 1.8392e-06, 1.5483e-06, 3.6324e-06, 1.2145e-06,
        1.8379e-06, 1.7623e-06, 1.7358e-06, 1.8858e-06, 1.7622e-06, 1.3225e-06,
        8.2820e-07, 1.0096e-06, 1.7587e-06, 8.7854e-07, 8.7115e-07, 1.3211e-06,
        2.3708e-06, 3.4563e-02, 2.2835e-06, 1.3646e-06, 1.4871e-01, 3.0957e-06,
        1.6108e-06, 1.8922e-06, 1.6772e-06, 4.2382e-02, 1.5810e-06, 1.3232e-06,
        3.3499e-02, 3.7801e-02, 1.1633e-06, 2.1272e-06, 1.7580e-06, 7.6494e-07,
        3.1612e-06, 1.6855e-06, 3.3484e-06, 1.3835e-06, 2.9670e-02, 2.0508e-06,
        2.2932e-06, 2.2525e-06, 8.8316e-07, 2.8076e-06, 1.4822e-06, 2.7158e-06,
        2.5321e-06, 6.7721e-07, 4.2624e-06, 1.1280e-06, 8.2054e-07, 1.7626e-06,
        1.5216e-06, 1.9306e-06, 2.2908e-06, 2.5411e-06, 2.7526e-06, 4.1608e-02,
        4.1444e-06, 9.5758e-07, 2.5317e-06, 3.3463e-06, 5.5103e-07, 2.7035e-06,
        2.2168e-06, 2.7036e-06, 2.5366e-06, 3.6322e-06, 1.7622e-06, 1.6603e-06,
        4.4861e-06, 1.0914e-06, 1.8850e-06, 1.3230e-06, 3.6143e-06, 3.2055e-02,
        3.2060e-06, 4.0666e-06, 1.0727e-06, 8.4258e-03, 1.8827e-06, 8.0416e-07,
        9.4285e-07, 4.0198e-02, 7.8920e-07, 1.8370e-06, 1.8838e-06, 9.7938e-07,
        5.3522e-06, 2.4123e-06, 2.7589e-06, 2.7617e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.0986e-07, 1.6220e-06, 9.5463e-07, 4.0190e-02, 6.1244e-07, 2.6591e-06,
        1.9530e-06, 1.5914e-06, 3.0854e-06, 1.1951e-06, 2.6591e-06, 1.2922e-06,
        1.0246e-06, 2.2741e-06, 5.0986e-07, 2.8913e-06, 5.0986e-07, 5.2795e-07,
        1.4855e-06, 1.4639e-06, 5.2795e-07, 4.5827e-06, 1.9686e-06, 3.0668e-06,
        4.6925e-02, 6.1417e-07, 2.3294e-06, 1.7429e-06, 1.6043e-06, 7.0053e-07,
        7.0053e-07, 1.7870e-06, 9.6180e-07, 9.2552e-07, 2.1616e-06, 1.6851e-06,
        3.5528e-06, 4.7600e-06, 5.2795e-07, 7.7394e-07, 2.6591e-06, 4.3519e-06,
        4.3517e-07, 1.9686e-06, 5.7520e-02, 2.6591e-06, 9.5463e-07, 2.6591e-06,
        1.8401e-06, 1.2922e-06, 1.9530e-06, 2.3724e-06, 1.1567e-06, 8.6245e-07,
        4.3519e-06, 3.7476e-02, 1.5259e-06, 1.6220e-06, 1.2906e-06, 5.0986e-07,
        2.6591e-06, 8.6245e-07, 3.3781e-06, 1.9281e-06, 1.0323e-06, 1.6454e-06,
        2.8913e-06, 6.1418e-07, 8.7615e-07, 2.5034e-06, 1.5451e-06, 3.1970e-06,
        1.6220e-06, 7.0053e-07, 1.9686e-06, 2.8392e-06, 3.7653e-02, 2.0031e-06,
        5.8553e-02, 2.5034e-06, 2.1314e-02, 1.9686e-06, 9.6725e-07, 9.5463e-07,
        1.6220e-06, 1.6314e-06, 1.5124e-06, 1.2102e-06, 1.6763e-06, 6.1418e-07,
        9.5463e-07, 5.0589e-02, 3.5214e-06, 1.2996e-06, 3.2664e-06, 4.6760e-02,
        2.8913e-06, 4.1686e-02, 2.2741e-06, 3.7568e-02, 2.3724e-06, 5.2795e-07,
        2.0594e-06, 1.8401e-06, 1.6314e-06, 1.9530e-06, 6.1418e-07, 3.9954e-02,
        9.6725e-07, 3.4708e-02, 3.2664e-06, 8.6245e-07, 1.4639e-06, 2.0277e-06,
        1.7642e-06, 1.2922e-06, 3.1303e-06, 1.8963e-06, 2.1616e-06, 7.0053e-07,
        1.2906e-06, 3.0854e-06, 2.3294e-06, 1.8384e-06, 2.5034e-06, 8.6245e-07,
        7.9582e-07, 1.5126e-06, 5.2795e-07, 1.7202e-06, 3.3781e-06, 1.6314e-06,
        5.6903e-07, 1.8963e-06, 5.2795e-07, 1.4639e-06, 1.9686e-06, 1.8963e-06,
        1.9891e-06, 3.2779e-06, 3.1067e-06, 8.6245e-07, 1.5638e-06, 5.0986e-07,
        1.5966e-06, 1.8185e-06, 1.9281e-06, 9.5463e-07, 7.7394e-07, 1.5966e-06,
        1.5451e-06, 1.6220e-06, 2.0594e-06, 1.3523e-06, 4.3468e-02, 4.6430e-02,
        1.2922e-06, 4.2116e-02, 1.9891e-06, 6.1244e-07, 9.2552e-07, 6.1224e-06,
        1.1855e-06, 2.6591e-06, 1.1053e-06, 4.3289e-06, 4.3519e-06, 1.9530e-06,
        8.6245e-07, 1.4486e-06, 1.7870e-06, 1.7190e-06, 1.1855e-06, 1.5966e-06,
        3.7019e-06, 7.7394e-07, 9.6839e-07, 1.2922e-06, 2.6591e-06, 9.6725e-07,
        9.6725e-07, 5.2795e-07, 2.5161e-06, 5.2795e-07, 1.5517e-06, 2.0031e-06,
        1.7870e-06, 8.6245e-07, 3.2664e-06, 8.6245e-07, 9.9991e-07, 2.0937e-02,
        2.2741e-06, 9.6725e-07, 1.1951e-06, 1.8174e-06, 9.5463e-07, 2.5034e-06,
        3.3993e-06, 1.5638e-06, 1.6103e-06, 1.2922e-06, 1.2766e-06, 1.1951e-06,
        1.9530e-06, 5.0982e-02, 2.8913e-06, 3.1053e-06, 1.1566e-06, 5.2795e-07,
        8.6245e-07, 2.6591e-06, 1.2922e-06, 1.4639e-06, 3.7984e-02, 1.9686e-06,
        3.2664e-06, 3.5528e-06, 2.2923e-06, 1.1053e-06, 2.3724e-06, 2.3724e-06,
        9.6180e-07, 1.9694e-02, 2.5034e-06, 1.1053e-06, 1.7462e-06, 6.1418e-07,
        1.8048e-06, 1.3361e-07, 2.0031e-06, 2.7995e-06, 1.4855e-06, 3.1053e-06,
        6.3382e-07, 1.2016e-06, 4.5828e-06, 1.9686e-06, 3.6623e-02, 9.9546e-07,
        3.2003e-06, 7.9582e-07, 2.8913e-06, 1.9686e-06, 2.0594e-06, 1.2706e-06,
        5.3544e-07, 2.3724e-06, 1.3523e-06, 2.2010e-06, 3.1053e-06, 2.8913e-06,
        1.5451e-06, 5.9239e-07, 2.6944e-06, 8.6245e-07, 3.2664e-06, 9.6725e-07,
        2.4344e-07, 3.0054e-06, 1.5966e-06, 1.1568e-06, 1.3683e-06, 2.5034e-06,
        3.0668e-06, 1.1855e-06, 4.9047e-02, 8.6245e-07, 1.8384e-06, 4.0913e-02,
        1.2922e-06, 3.1053e-06, 3.0854e-06, 1.7642e-06, 4.5828e-06, 3.5528e-06,
        3.1521e-06, 4.5828e-06, 1.5451e-06, 2.2741e-06, 1.6763e-06, 9.2095e-07,
        2.8913e-06, 2.3724e-06, 1.1069e-06, 2.8641e-06, 1.1235e-06, 2.0832e-06,
        2.1914e-06, 9.3555e-07, 1.4296e-06, 5.8412e-02, 2.0931e-06, 1.8048e-06,
        4.8000e-06, 1.1568e-06, 5.0986e-07, 3.3781e-06, 1.2906e-06, 3.1521e-06,
        1.2922e-06, 3.3424e-06, 4.5495e-02, 1.9686e-06, 1.6220e-06, 1.8048e-06,
        9.2552e-07, 1.4891e-06, 1.1053e-06, 2.6944e-06, 3.2664e-06, 1.7462e-06,
        3.1053e-06, 2.3724e-06, 1.5966e-06, 3.3424e-06, 3.1301e-06, 6.1418e-07,
        2.8913e-06, 4.3589e-02, 3.2664e-06, 1.6983e-06, 2.5034e-06, 3.1521e-06,
        1.2102e-06, 8.6245e-07, 3.2664e-06, 3.5528e-06, 1.8963e-06, 3.2664e-06,
        2.8913e-06, 1.4639e-06, 1.5966e-06, 1.2922e-06, 5.0986e-07, 1.2922e-06,
        1.2922e-06, 5.3740e-07, 5.7192e-02, 7.7394e-07, 1.0480e-06, 5.9790e-02,
        3.5528e-06, 9.6180e-07, 3.3993e-06, 1.5966e-06, 3.1053e-06, 9.5218e-07,
        4.8000e-06, 1.5451e-06, 1.3431e-07, 4.1642e-06, 1.1053e-06, 2.2493e-06,
        3.1970e-06, 2.0031e-06, 1.6763e-06, 1.3683e-06, 7.1661e-07, 4.7945e-02,
        3.2779e-06, 2.8913e-06, 7.9582e-07, 2.8913e-06, 2.8913e-06, 3.1970e-06,
        8.3318e-08, 1.6043e-06, 1.5599e-06, 3.6481e-02, 2.3724e-06, 3.2664e-06,
        5.0923e-02, 2.3724e-06, 1.1951e-06, 1.5914e-06, 4.8661e-02, 9.6839e-07,
        1.1053e-06, 3.2664e-06, 1.9891e-06, 1.5124e-06, 9.3555e-07, 1.5124e-06,
        7.0053e-07, 2.9920e-06, 1.2906e-06, 2.5034e-06, 4.0067e-02, 1.9281e-06,
        3.1053e-06, 1.6043e-06, 1.2906e-06, 1.1426e-06, 1.3523e-06, 4.8000e-06,
        4.7659e-06, 1.2922e-06, 1.2906e-06, 6.1417e-07, 1.2922e-06, 2.8913e-06,
        1.0246e-06, 1.5258e-06, 2.2741e-06, 9.6180e-07, 2.2923e-06, 1.6314e-06,
        2.8913e-06, 1.9686e-06, 1.2922e-06, 1.2706e-06, 1.4639e-06, 1.1568e-06,
        1.1951e-06, 3.1053e-06, 7.7394e-07, 5.0986e-07, 4.3289e-06, 2.0931e-06,
        1.8048e-06, 4.4915e-02, 2.5034e-06, 2.5789e-06, 1.0246e-06, 7.9857e-07,
        2.6944e-06, 4.7584e-06, 3.3993e-06, 1.4444e-06, 9.2095e-07, 2.2346e-06,
        1.9530e-06, 1.8011e-06, 9.3555e-07, 1.5451e-06, 1.8048e-06, 1.7429e-06,
        1.6851e-06, 6.1418e-07, 1.5451e-06, 1.7190e-06, 4.6164e-06, 1.8048e-06,
        3.1521e-06, 1.1053e-06, 1.2922e-06, 6.1417e-07, 4.7781e-06, 7.4725e-07,
        1.4639e-06, 1.6220e-06, 2.0594e-06, 3.5528e-06, 2.1967e-06, 8.6245e-07,
        3.3781e-06, 1.1053e-06, 1.6220e-06, 3.2664e-06, 4.8659e-06, 1.9686e-06,
        1.6314e-06, 1.0602e-06, 2.0029e-06, 4.5075e-02, 3.9641e-06, 5.0986e-07,
        2.8709e-06, 3.1970e-06, 8.6245e-07, 2.0031e-06, 3.1053e-06, 1.1855e-06,
        8.6245e-07, 4.1032e-02, 9.5218e-07, 1.0390e-06, 1.6220e-06, 1.5124e-06,
        1.7870e-06, 3.8444e-02, 4.8000e-06, 3.2664e-06, 1.2766e-06, 4.2146e-02,
        6.3382e-07, 7.7598e-07, 3.3424e-06, 1.8174e-06, 3.3506e-02, 5.3800e-07,
        1.7202e-06, 1.3333e-06, 1.5903e-06, 3.1521e-06, 2.8913e-06, 4.5743e-06,
        3.0854e-06, 1.5966e-06, 9.5463e-07, 4.9386e-02, 1.8384e-06, 2.6591e-06,
        2.5034e-06, 7.7394e-07, 5.0986e-07, 3.2779e-06, 2.4287e-06, 2.6591e-06,
        3.0854e-06, 3.0854e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.5815e-06, 4.3662e-07, 9.9240e-06, 1.1714e-05, 1.1714e-05, 7.2805e-06,
        6.2781e-06, 1.3358e-06, 2.5395e-06, 1.6072e-06, 1.2889e-02, 3.5611e-06,
        4.4127e-06, 4.9846e-06, 6.2303e-06, 6.2257e-06, 8.8621e-06, 5.8993e-06,
        4.1578e-06, 4.4283e-06, 9.9240e-06, 1.5519e-05, 7.8877e-06, 2.7404e-06,
        4.3663e-07, 7.2805e-06, 3.5611e-06, 4.3663e-07, 4.0212e-06, 1.8625e-06,
        6.0656e-06, 7.2137e-06, 3.3025e-06, 4.6470e-06, 3.3162e-06, 5.8993e-06,
        4.4815e-06, 7.2137e-06, 3.9430e-06, 6.9251e-02, 3.2804e-06, 3.5611e-06,
        4.9846e-06, 4.9846e-06, 3.3162e-06, 6.6166e-06, 1.1308e-02, 3.7544e-06,
        6.2915e-06, 1.4167e-06, 3.3162e-06, 1.2105e-05, 3.1838e-06, 6.2781e-06,
        8.0186e-03, 7.2137e-06, 5.8993e-06, 9.1027e-06, 4.8756e-06, 7.5815e-06,
        6.0656e-06, 4.4127e-06, 1.2223e-06, 5.9844e-06, 8.3944e-06, 2.5656e-06,
        4.1098e-06, 9.0110e-06, 1.2105e-05, 8.2597e-06, 1.2035e-05, 5.2710e-06,
        1.1714e-05, 4.3663e-07, 4.6138e-06, 2.7404e-06, 9.9951e-03, 4.0761e-06,
        4.7527e-06, 1.2105e-05, 7.0697e-06, 5.8993e-06, 3.3956e-06, 9.0110e-06,
        6.2915e-06, 9.0014e-06, 1.2035e-05, 3.5773e-06, 4.3663e-07, 1.3573e-06,
        6.4457e-06, 1.0740e-06, 4.1098e-06, 1.3082e-06, 3.0228e-06, 4.9846e-06,
        7.2805e-06, 3.7013e-06, 5.9844e-06, 7.7606e-06, 9.6776e-06, 4.6470e-06,
        1.1714e-05, 4.7552e-06, 4.3663e-07, 2.9484e-06, 7.2300e-06, 4.9846e-06,
        3.5611e-06, 5.8234e-06, 1.3159e-02, 6.8529e-06, 6.4456e-06, 8.3944e-06,
        7.5815e-06, 7.2805e-06, 3.3162e-06, 4.3809e-06, 4.6470e-06, 1.3573e-06,
        1.3358e-06, 2.6444e-06, 5.1940e-06, 1.3573e-06, 7.5815e-06, 1.5519e-05,
        8.7049e-06, 4.1661e-06, 6.2781e-06, 2.5656e-06, 4.3663e-07, 6.2781e-06,
        4.3662e-07, 4.9846e-06, 4.9551e-06, 2.5656e-06, 6.4456e-06, 8.8621e-06,
        6.2102e-06, 4.1578e-06, 6.2134e-06, 4.1202e-06, 4.4446e-06, 4.9846e-06,
        3.4408e-06, 4.1578e-06, 4.4815e-06, 2.6152e-06, 4.4815e-06, 1.2035e-05,
        8.3944e-06, 4.7527e-06, 1.1714e-05, 7.7606e-06, 4.9847e-06, 1.8583e-05,
        6.2781e-06, 4.4283e-06, 1.4167e-06, 5.9693e-06, 4.4815e-06, 1.3358e-06,
        6.3342e-06, 2.4168e-06, 1.3100e-05, 1.3052e-05, 3.7014e-06, 7.8400e-06,
        2.6231e-06, 8.8621e-06, 7.9209e-06, 3.5611e-06, 7.2137e-06, 4.0067e-04,
        3.3956e-06, 5.9844e-06, 6.5588e-06, 2.3945e-06, 2.5656e-06, 3.5611e-06,
        4.9847e-06, 9.8305e-07, 8.3944e-06, 7.2805e-06, 3.5933e-06, 3.3162e-06,
        5.9844e-06, 4.7932e-06, 1.3573e-06, 1.9893e-06, 4.3663e-07, 4.8756e-06,
        2.9234e-06, 2.3945e-06, 7.5815e-06, 1.5519e-05, 1.4859e-05, 9.9240e-06,
        5.9844e-06, 2.0224e-06, 6.1675e-06, 9.9240e-06, 6.2915e-06, 6.2781e-06,
        3.1187e-06, 6.2915e-06, 1.1714e-05, 8.6003e-06, 4.1661e-06, 4.7932e-06,
        4.2501e-02, 6.5588e-06, 4.8919e-06, 4.1098e-06, 8.8621e-06, 6.9891e-02,
        7.2805e-06, 9.9240e-06, 6.1675e-06, 1.2899e-05, 3.3162e-06, 6.1140e-06,
        8.6003e-06, 3.3162e-06, 9.9240e-06, 3.6445e-03, 4.6470e-06, 1.2581e-03,
        5.9844e-06, 5.0257e-06, 2.5085e-06, 3.7047e-07, 4.3661e-07, 1.0096e-05,
        2.3989e-06, 2.2116e-06, 8.5099e-06, 4.5351e-06, 1.3573e-06, 4.5351e-06,
        7.1428e-06, 5.1312e-06, 4.1661e-06, 2.6989e-06, 8.9227e-06, 1.6880e-06,
        4.8756e-06, 4.9846e-06, 3.7971e-06, 6.8949e-06, 4.4815e-06, 4.7527e-06,
        7.4006e-02, 2.5194e-06, 4.9846e-06, 3.3162e-06, 3.3162e-06, 5.9844e-06,
        1.3040e-05, 1.1761e-05, 7.2183e-06, 1.3573e-06, 9.8305e-07, 5.6037e-06,
        1.2035e-05, 4.8919e-06, 4.0174e-06, 4.9846e-06, 2.9157e-06, 6.5588e-06,
        1.4167e-06, 4.9846e-06, 3.2735e-06, 4.3663e-07, 4.3236e-06, 4.1661e-06,
        3.3162e-06, 5.9844e-06, 2.7068e-06, 4.5351e-06, 7.2137e-06, 1.4167e-06,
        1.6949e-05, 6.1675e-06, 7.5815e-06, 9.9240e-06, 8.9227e-06, 2.8421e-06,
        9.1998e-06, 7.1919e-07, 4.9846e-06, 3.5611e-06, 3.5611e-06, 4.4815e-06,
        5.2580e-06, 1.7671e-06, 7.9864e-06, 7.5815e-06, 8.3944e-06, 1.2035e-05,
        3.6729e-06, 2.7954e-03, 8.3614e-04, 5.9844e-06, 4.1578e-06, 2.8421e-06,
        3.5611e-06, 6.4900e-06, 3.1838e-06, 5.9844e-06, 6.2160e-06, 3.7013e-06,
        2.6444e-06, 5.9844e-06, 2.5395e-06, 1.4722e-02, 2.8305e-06, 4.4767e-06,
        6.5588e-06, 6.8252e-06, 4.4394e-06, 1.0096e-05, 4.8919e-06, 3.5611e-06,
        7.2137e-06, 8.6435e-06, 3.5611e-06, 2.1229e-06, 3.7014e-06, 4.4127e-06,
        5.4570e-06, 7.1919e-07, 5.1791e-06, 5.8122e-06, 5.9844e-06, 8.9093e-06,
        4.4815e-06, 2.9484e-06, 4.8756e-06, 8.3944e-06, 7.1428e-06, 8.1207e-06,
        5.8234e-06, 1.3082e-06, 4.3662e-07, 2.7940e-06, 7.5815e-06, 8.9227e-06,
        3.1187e-06, 1.7358e-02, 1.7400e-05, 6.4457e-06, 1.5770e-06, 5.8827e-06,
        6.7303e-02, 4.6470e-06, 4.3084e-06, 8.4830e-06, 3.6492e-03, 1.2035e-05,
        3.5588e-06, 5.5480e-06, 5.9844e-06, 1.3573e-06, 6.2233e-06, 3.5611e-06,
        2.5395e-06, 5.9844e-06, 3.3162e-06, 3.5611e-06, 1.4167e-06, 4.9846e-06,
        5.0822e-06, 6.1675e-06, 1.3573e-06, 6.4457e-06, 4.1098e-06, 6.4457e-06,
        7.3130e-06, 3.1838e-06, 8.8621e-06, 4.1578e-06, 2.5656e-06, 4.8919e-06,
        7.5815e-06, 7.7605e-06, 1.1174e-05, 3.5933e-06, 8.9227e-06, 3.3162e-06,
        1.7107e-02, 4.3662e-07, 1.2035e-05, 3.0107e-06, 6.4457e-06, 3.2917e-06,
        4.5870e-06, 3.3162e-06, 4.4446e-06, 8.3131e-06, 4.8919e-06, 7.7605e-06,
        6.8949e-06, 4.7932e-06, 8.3944e-06, 3.5773e-06, 5.1377e-06, 6.2781e-06,
        1.2035e-05, 4.6470e-06, 6.4456e-06, 8.9227e-06, 3.6175e-06, 3.5611e-06,
        2.6444e-06, 4.7202e-06, 3.3162e-06, 3.3162e-06, 4.9846e-06, 3.7013e-06,
        4.9846e-06, 3.4408e-06, 4.7932e-06, 4.8151e-06, 3.8737e-06, 3.1187e-06,
        9.1704e-06, 1.5230e-06, 4.6413e-06, 2.5395e-06, 9.1704e-06, 2.9484e-06,
        4.8756e-06, 1.2482e-05, 3.0107e-06, 3.3162e-06, 5.8827e-06, 4.7527e-06,
        4.9846e-06, 1.5931e-02, 7.9455e-06, 3.5611e-06, 4.8756e-06, 1.3749e-02,
        7.2137e-06, 3.3162e-06, 2.8421e-06, 5.8993e-06, 2.6444e-06, 8.6435e-06,
        4.3662e-07, 1.5519e-05, 5.2710e-06, 7.2137e-06, 6.2915e-06, 3.0914e-02,
        2.8421e-06, 3.6803e-06, 4.9847e-06, 1.3427e-02, 8.6713e-06, 9.9240e-06,
        6.4456e-06, 3.5611e-06, 4.4815e-06, 1.1281e-05, 7.9864e-06, 2.9484e-06,
        3.6729e-06, 4.3662e-07, 1.5470e-06, 4.3663e-07, 7.5815e-06, 8.3944e-06,
        5.8122e-06, 2.6444e-06, 7.7605e-06, 5.5266e-06, 6.1789e-06, 7.5815e-06,
        6.4457e-06, 7.7606e-06, 1.5770e-06, 1.5519e-05, 2.5560e-06, 3.3163e-06,
        2.4168e-06, 7.3972e-06, 4.1661e-06, 4.1609e-06, 9.0220e-06, 7.2805e-06,
        4.9846e-06, 8.8621e-06, 7.1428e-06, 3.8737e-06, 3.7544e-06, 6.2781e-06,
        6.1675e-06, 9.9240e-06, 4.7485e-06, 9.0014e-06, 4.4815e-06, 6.2781e-06,
        4.4815e-06, 3.5590e-06, 1.0414e-06, 6.4457e-06, 8.3944e-06, 6.5588e-06,
        1.4859e-05, 7.5815e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2430e-07, 3.8554e-07, 1.4319e-07, 1.1598e-07, 6.6712e-08, 3.4074e-07,
        4.3669e-07, 1.4063e-07, 1.2157e-07, 2.3554e-07, 4.7830e-02, 1.0849e-07,
        5.7659e-07, 2.6369e-07, 3.2561e-07, 2.6369e-07, 2.0998e-07, 2.0998e-07,
        3.8554e-07, 1.5221e-07, 1.5720e-07, 1.2157e-07, 1.6119e-07, 1.4063e-07,
        6.0176e-08, 2.9469e-07, 6.0178e-08, 1.3081e-07, 1.2157e-07, 1.4319e-07,
        1.5204e-07, 2.4510e-07, 5.7238e-07, 2.1183e-07, 2.5886e-07, 3.2561e-07,
        1.3071e-07, 1.0794e-07, 1.0305e-07, 1.3222e-01, 2.4173e-03, 2.9469e-07,
        1.2157e-07, 2.5900e-07, 1.4063e-07, 1.9117e-07, 6.3816e-02, 1.3371e-07,
        4.1716e-07, 1.3603e-07, 1.2430e-07, 1.1323e-03, 2.6369e-07, 5.5786e-07,
        5.0329e-02, 2.9469e-07, 1.0023e-07, 2.0998e-07, 1.2497e-07, 6.9595e-08,
        3.4390e-07, 6.9595e-08, 2.0998e-07, 1.7813e-07, 1.1613e-07, 1.6978e-07,
        2.1183e-07, 1.9118e-07, 2.1400e-07, 3.9129e-07, 1.5563e-07, 4.9061e-07,
        1.5427e-07, 6.7248e-07, 6.5942e-08, 3.2561e-07, 9.3790e-02, 1.3603e-07,
        6.9595e-08, 6.5942e-08, 4.1722e-07, 4.1716e-07, 3.1829e-07, 6.5942e-08,
        4.1722e-07, 2.2897e-07, 1.2157e-07, 4.9625e-07, 1.3499e-07, 3.2561e-07,
        2.9508e-07, 3.5059e-07, 1.4063e-07, 3.2561e-07, 2.1183e-07, 1.4063e-07,
        4.4878e-07, 1.9235e-07, 6.9595e-08, 2.4631e-07, 5.7258e-07, 1.0023e-07,
        2.0998e-07, 1.4063e-07, 2.1183e-07, 1.5221e-07, 1.9235e-07, 6.9595e-08,
        2.1183e-07, 1.9870e-07, 9.6067e-02, 1.7813e-07, 1.9832e-07, 3.1066e-07,
        1.7348e-07, 2.0998e-07, 1.7813e-07, 2.5481e-07, 1.2439e-07, 6.9595e-08,
        2.9469e-07, 1.4063e-07, 4.9061e-07, 1.0023e-07, 6.7248e-07, 4.9061e-07,
        2.4510e-07, 1.3371e-07, 1.5221e-07, 1.9832e-07, 1.2157e-07, 1.5221e-07,
        9.7593e-08, 1.4319e-07, 6.5942e-08, 3.2871e-07, 1.4063e-07, 1.5221e-07,
        2.6067e-07, 2.1183e-07, 3.1066e-07, 1.7813e-07, 2.5083e-07, 1.3371e-07,
        6.5942e-08, 3.5059e-07, 2.6263e-07, 3.5059e-07, 1.5744e-07, 3.6508e-07,
        2.9970e-07, 3.4071e-07, 1.5001e-07, 6.9595e-08, 3.3757e-07, 6.5942e-08,
        2.1183e-07, 1.5204e-07, 1.5427e-07, 2.7966e-07, 1.0305e-07, 2.5803e-07,
        1.3623e-07, 3.1066e-07, 5.0552e-07, 3.6059e-07, 3.1042e-07, 3.8554e-07,
        2.4510e-07, 2.4510e-07, 9.9002e-08, 1.9859e-07, 1.0984e-07, 9.3580e-03,
        1.9419e-07, 1.3371e-07, 1.0181e-07, 5.4366e-07, 1.9870e-07, 1.5204e-07,
        1.6228e-07, 1.3489e-07, 6.5942e-08, 1.6228e-07, 1.2444e-07, 2.9469e-07,
        1.9235e-07, 6.5942e-08, 6.7248e-07, 3.6943e-07, 1.7090e-07, 1.2430e-07,
        3.2566e-07, 2.4510e-07, 1.0849e-07, 1.4319e-07, 5.7229e-07, 3.6469e-07,
        1.4063e-07, 6.7248e-07, 2.1183e-07, 2.2945e-07, 2.1183e-07, 1.2430e-07,
        2.4510e-07, 1.9094e-07, 2.7967e-07, 8.6288e-08, 1.5236e-07, 3.6695e-07,
        1.3722e-01, 3.6695e-07, 2.1183e-07, 2.9100e-07, 1.4063e-07, 1.0181e-01,
        2.0998e-07, 2.7066e-08, 3.2561e-07, 2.8045e-07, 1.9849e-07, 3.2564e-07,
        1.2430e-07, 2.5887e-07, 3.6694e-07, 2.6292e-02, 1.7813e-07, 1.2355e-02,
        1.9870e-07, 2.1183e-07, 1.2361e-07, 1.5902e-07, 1.3371e-07, 1.5287e-07,
        1.9235e-07, 1.4101e-07, 2.5902e-07, 1.5204e-07, 1.4101e-07, 1.4319e-07,
        1.5519e-07, 1.4063e-07, 1.4063e-07, 1.5229e-07, 2.7318e-07, 2.1183e-07,
        1.7813e-07, 2.2578e-07, 1.3114e-08, 1.4063e-07, 1.9235e-07, 3.2561e-07,
        1.0494e-01, 1.2384e-07, 1.4319e-07, 3.8636e-07, 1.2430e-07, 4.2365e-07,
        2.1183e-07, 2.9629e-07, 1.4319e-07, 2.1183e-07, 4.1722e-07, 2.8328e-07,
        2.1279e-07, 1.5537e-07, 1.0023e-07, 1.2157e-07, 1.5221e-07, 1.9443e-07,
        6.6491e-08, 4.1716e-07, 4.1722e-07, 3.4075e-07, 1.5221e-07, 1.7813e-07,
        1.0939e-07, 7.1612e-07, 1.3371e-07, 1.4063e-07, 2.5902e-07, 1.4319e-07,
        5.7288e-07, 1.3081e-07, 1.5287e-07, 3.1312e-07, 1.7813e-07, 1.2157e-07,
        6.6005e-08, 1.2430e-07, 9.1143e-08, 3.1066e-07, 1.4063e-07, 3.6059e-07,
        1.3371e-07, 2.4510e-07, 1.4100e-07, 2.9469e-07, 3.6059e-07, 1.2157e-07,
        2.1183e-07, 2.2282e-02, 9.6999e-03, 1.5221e-07, 1.5204e-07, 5.7429e-07,
        1.0425e-07, 5.7436e-07, 1.1340e-07, 1.4101e-07, 1.5204e-07, 2.0998e-07,
        3.1066e-07, 1.0023e-07, 5.7307e-07, 9.1144e-02, 3.2561e-07, 1.2600e-07,
        2.0788e-07, 1.7813e-07, 3.7744e-07, 1.0939e-07, 1.2157e-07, 1.4319e-07,
        3.8636e-07, 4.1716e-07, 3.9292e-07, 1.9555e-07, 1.9646e-03, 1.4063e-07,
        1.7813e-07, 2.1183e-07, 1.5121e-07, 2.4510e-07, 1.5160e-07, 1.0023e-07,
        1.9578e-07, 2.6369e-07, 3.9129e-07, 2.1279e-07, 4.1684e-07, 3.1066e-07,
        1.9832e-07, 5.9931e-08, 1.8866e-07, 1.4063e-07, 6.5942e-08, 3.1803e-07,
        2.1183e-07, 1.3425e-01, 4.9061e-07, 1.7813e-07, 2.0996e-07, 1.6221e-07,
        1.1770e-01, 3.6059e-07, 2.9469e-07, 3.6261e-07, 2.6348e-02, 1.9235e-07,
        1.5204e-07, 2.2693e-07, 6.9595e-08, 2.1183e-07, 1.5799e-07, 6.5002e-03,
        3.6059e-07, 7.1801e-07, 3.6059e-07, 1.4319e-07, 1.3371e-07, 1.3371e-07,
        3.1854e-07, 5.7876e-07, 1.2430e-07, 1.0190e-07, 2.5083e-07, 2.0998e-07,
        2.2693e-07, 2.5340e-07, 1.2378e-07, 1.2157e-07, 1.4063e-07, 1.2019e-07,
        6.5942e-08, 1.7813e-07, 3.8401e-07, 2.0290e-07, 1.4319e-07, 1.7813e-07,
        1.1161e-01, 1.0023e-07, 2.1183e-07, 4.9061e-07, 3.4454e-07, 4.1968e-07,
        2.9469e-07, 3.2561e-07, 2.0998e-07, 3.2564e-07, 7.1573e-08, 2.6369e-07,
        1.3529e-07, 1.9236e-07, 1.2360e-07, 1.6152e-07, 1.7563e-07, 3.4075e-07,
        1.9832e-07, 2.1989e-07, 1.0316e-07, 1.2377e-07, 4.1716e-07, 6.9595e-08,
        6.9595e-08, 3.5410e-07, 1.5221e-07, 2.0998e-07, 1.8851e-07, 3.2561e-07,
        3.4082e-07, 2.0998e-07, 4.9061e-07, 3.4076e-07, 2.6369e-07, 1.9832e-07,
        1.5204e-07, 6.5942e-08, 2.0398e-07, 1.5058e-07, 1.7202e-07, 1.6228e-07,
        7.1573e-08, 1.4216e-07, 1.4063e-07, 3.1066e-07, 1.7210e-07, 1.7813e-07,
        3.5059e-07, 1.0433e-01, 2.0998e-07, 3.1742e-07, 1.5287e-07, 6.7268e-02,
        2.5083e-07, 1.2385e-07, 4.1722e-07, 2.1346e-07, 3.5059e-07, 1.5221e-07,
        6.5942e-08, 1.2157e-07, 1.5221e-07, 5.8472e-07, 1.8753e-07, 1.5258e-01,
        1.2157e-07, 1.2430e-07, 7.7912e-08, 5.7542e-02, 3.2561e-07, 1.8133e-07,
        7.1573e-08, 1.8523e-07, 2.6369e-07, 2.2248e-07, 4.9061e-07, 1.2361e-07,
        1.4319e-07, 3.9129e-07, 1.4063e-07, 1.4063e-07, 6.5942e-08, 1.4828e-07,
        1.4319e-07, 4.1722e-07, 3.4075e-07, 1.5598e-07, 1.4063e-07, 3.2565e-07,
        6.9595e-08, 2.1008e-07, 2.0998e-07, 1.0939e-07, 2.7596e-07, 1.1933e-02,
        2.1022e-07, 6.9595e-08, 4.9061e-07, 1.5546e-07, 2.7041e-07, 1.4063e-07,
        6.7248e-07, 3.5480e-07, 2.7649e-07, 2.9469e-07, 3.2561e-07, 3.6059e-07,
        1.5988e-07, 5.7246e-07, 2.2945e-07, 1.3371e-07, 2.9469e-07, 1.1340e-07,
        2.7041e-07, 1.5221e-07, 1.7813e-07, 4.9061e-07, 3.8636e-07, 1.7274e-07,
        1.5040e-07, 9.9002e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0526e-05, 4.6056e-06, 4.6824e-06, 9.1444e-06, 5.1761e-06, 7.5935e-06,
        2.5914e-06, 2.2627e-06, 2.7825e-06, 4.3816e-06, 3.7685e-06, 8.1035e-06,
        8.7414e-06, 3.6800e-06, 5.2759e-06, 1.2069e-05, 5.0848e-06, 5.8920e-06,
        4.4085e-06, 9.8108e-06, 6.4205e-06, 1.9585e-06, 3.1200e-06, 1.2069e-05,
        4.2150e-06, 7.7407e-06, 2.2627e-06, 8.7489e-06, 4.6222e-06, 6.9433e-06,
        3.3942e-06, 4.5600e-06, 3.1121e-06, 6.1656e-06, 5.0733e-06, 3.7684e-06,
        8.7140e-06, 2.4860e-06, 9.1157e-06, 3.9139e-06, 6.4883e-06, 5.6027e-06,
        4.6059e-06, 5.8387e-06, 5.1537e-06, 3.9038e-06, 1.0753e-05, 8.1035e-06,
        5.2509e-06, 4.7439e-06, 3.5121e-06, 6.0289e-06, 7.2868e-06, 3.6800e-06,
        3.7664e-06, 5.1660e-06, 2.4860e-06, 4.2850e-06, 8.3884e-06, 5.3803e-06,
        6.9433e-06, 9.2337e-06, 5.9268e-06, 5.0482e-06, 8.3884e-06, 2.6752e-06,
        5.9268e-06, 8.3427e-06, 7.5696e-06, 9.0625e-06, 3.9139e-06, 3.7168e-06,
        6.2222e-07, 7.5576e-06, 4.4370e-06, 5.4345e-06, 3.0098e-06, 2.3222e-06,
        1.6045e-06, 3.9139e-06, 3.9139e-06, 6.9433e-06, 1.2756e-05, 1.0953e-05,
        4.7398e-06, 2.3495e-06, 4.6924e-06, 6.8182e-06, 3.4838e-06, 6.8182e-06,
        5.2759e-06, 1.2069e-05, 5.2759e-06, 4.7439e-06, 4.9192e-06, 5.1660e-06,
        6.4205e-06, 4.9192e-06, 5.2100e-06, 7.5576e-06, 4.6824e-06, 5.8920e-06,
        1.0230e-05, 9.9372e-06, 1.9351e-06, 8.1687e-06, 1.7566e-06, 4.7439e-06,
        8.3884e-06, 4.8344e-06, 4.2656e-06, 4.9882e-06, 8.9701e-06, 3.7664e-06,
        7.5935e-06, 3.0074e-06, 8.4539e-06, 6.4179e-06, 6.5969e-06, 3.4910e-06,
        4.9906e-06, 6.9634e-06, 1.0526e-05, 3.4812e-06, 5.3287e-06, 1.2749e-05,
        6.8182e-06, 9.3828e-06, 8.1687e-06, 8.1035e-06, 2.1003e-06, 4.9372e-06,
        4.4085e-06, 4.8064e-07, 3.9576e-06, 7.8219e-06, 5.2759e-06, 2.5914e-06,
        4.4085e-06, 5.2759e-06, 1.0290e-06, 1.2659e-06, 6.5300e-06, 7.5684e-06,
        5.5701e-06, 7.5696e-06, 6.4179e-06, 5.1537e-06, 2.8257e-06, 7.2906e-06,
        2.3505e-07, 5.1537e-06, 2.0246e-06, 2.4860e-06, 1.1717e-06, 1.2085e-05,
        4.4062e-06, 3.7684e-06, 8.7414e-06, 5.2759e-06, 3.1247e-06, 3.5342e-06,
        4.0933e-06, 2.3445e-06, 1.5540e-05, 5.1660e-06, 2.5387e-06, 4.6222e-06,
        3.0074e-06, 6.3923e-06, 5.5930e-06, 1.2749e-05, 1.2659e-06, 3.0074e-06,
        4.6222e-06, 7.8219e-06, 7.3411e-06, 3.5477e-06, 7.5684e-06, 7.7765e-06,
        8.1035e-06, 2.3888e-06, 4.8706e-06, 6.9433e-06, 1.6970e-06, 1.3823e-06,
        4.3740e-06, 2.2545e-06, 8.2602e-06, 4.5737e-06, 4.4085e-06, 8.3427e-06,
        8.1137e-07, 8.3769e-06, 5.1537e-06, 6.1719e-06, 1.7294e-05, 7.6951e-06,
        9.6241e-06, 2.5387e-06, 8.8403e-06, 6.3205e-06, 6.3224e-06, 7.8635e-06,
        6.4179e-06, 4.4085e-06, 6.9433e-06, 2.2627e-06, 5.0702e-06, 6.7288e-06,
        7.5935e-06, 1.8550e-06, 1.1205e-05, 8.1035e-06, 4.9192e-06, 4.4062e-06,
        3.9576e-06, 3.9139e-06, 1.2823e-05, 2.2627e-06, 6.5300e-06, 5.1537e-06,
        4.8824e-06, 3.1820e-06, 8.0542e-06, 6.1656e-06, 5.3323e-06, 2.5387e-06,
        1.4417e-05, 5.2436e-06, 3.3056e-06, 1.0683e-05, 7.8219e-06, 5.3886e-06,
        8.3884e-06, 5.9200e-06, 2.1003e-06, 7.5239e-06, 4.0092e-06, 4.1660e-06,
        1.2749e-05, 5.0702e-06, 5.1660e-06, 8.7489e-06, 3.1820e-06, 8.1035e-06,
        1.0773e-05, 7.5696e-06, 2.7665e-06, 3.9139e-06, 5.5508e-06, 6.4179e-06,
        3.1200e-06, 2.9176e-06, 2.8961e-06, 3.9139e-06, 4.1877e-06, 2.0595e-06,
        5.1537e-06, 2.7650e-06, 5.9769e-06, 3.9139e-06, 1.1009e-05, 1.6365e-06,
        6.4179e-06, 5.7939e-06, 1.2414e-05, 8.7572e-06, 3.2561e-06, 4.4062e-06,
        3.5121e-06, 1.7566e-06, 6.9719e-06, 4.9192e-06, 4.9192e-06, 8.0206e-06,
        2.6752e-06, 3.5732e-06, 6.9433e-06, 8.3884e-06, 4.7693e-06, 5.1660e-06,
        5.6496e-06, 8.4539e-06, 4.4062e-06, 5.5126e-06, 5.1761e-06, 1.2749e-05,
        1.0290e-06, 6.2063e-06, 9.8988e-06, 2.7202e-06, 6.4205e-06, 2.5231e-06,
        1.0526e-05, 7.5935e-06, 2.4860e-06, 1.2414e-05, 4.2557e-06, 1.6150e-06,
        4.0134e-06, 6.9433e-06, 5.2759e-06, 3.2988e-06, 3.7607e-06, 3.0074e-06,
        4.0030e-06, 7.5935e-06, 3.9139e-06, 4.4085e-06, 7.8904e-06, 1.6365e-06,
        1.2414e-05, 1.6365e-06, 3.7664e-06, 5.9268e-06, 4.4085e-06, 4.9642e-06,
        4.9906e-06, 4.1765e-06, 3.1247e-06, 5.1537e-06, 4.9372e-06, 2.2545e-06,
        9.9751e-06, 2.5914e-06, 1.4026e-05, 1.0290e-06, 5.1537e-06, 5.2759e-06,
        4.3149e-06, 1.6960e-06, 9.8988e-06, 5.0848e-06, 8.3884e-06, 5.2283e-06,
        4.4096e-06, 4.6222e-06, 1.1717e-06, 6.9433e-06, 8.3884e-06, 2.2545e-06,
        2.2694e-06, 8.3884e-06, 9.6241e-06, 4.1682e-06, 4.3740e-06, 8.0252e-06,
        5.2718e-06, 4.2557e-06, 3.9038e-06, 3.0074e-06, 4.4768e-06, 1.5502e-05,
        2.5387e-06, 7.6951e-06, 1.3823e-06, 5.9268e-06, 2.0254e-06, 1.0230e-05,
        7.7268e-06, 8.8774e-06, 4.1682e-06, 5.8412e-06, 3.9536e-06, 2.8257e-06,
        3.7684e-06, 1.0335e-05, 1.4374e-06, 4.7398e-06, 2.3222e-06, 5.9268e-06,
        3.8239e-06, 5.3323e-06, 7.8219e-06, 8.7414e-06, 4.4085e-06, 1.2749e-05,
        5.2759e-06, 7.7558e-06, 2.5542e-06, 1.1126e-05, 5.2759e-06, 2.0254e-06,
        7.8219e-06, 3.7684e-06, 2.7167e-06, 5.8920e-06, 6.9433e-06, 4.9192e-06,
        1.1717e-06, 3.5732e-06, 1.0953e-05, 1.3848e-05, 5.0702e-06, 2.1503e-06,
        1.6970e-06, 1.2659e-06, 3.2736e-06, 5.2759e-06, 4.7398e-06, 5.2759e-06,
        5.9268e-06, 5.0848e-06, 1.7669e-06, 4.9121e-06, 7.5222e-06, 7.6951e-06,
        3.3056e-06, 8.8403e-06, 7.1299e-06, 3.7900e-06, 5.1761e-06, 7.8219e-06,
        1.2659e-06, 2.5542e-06, 1.0953e-05, 3.9038e-06, 2.3103e-06, 3.9139e-06,
        2.5585e-06, 2.1003e-06, 2.2627e-06, 2.5585e-06, 2.5914e-06, 7.6167e-06,
        2.2545e-06, 2.2194e-06, 6.5300e-06, 1.9618e-06, 6.7289e-06, 5.9268e-06,
        6.9433e-06, 2.0254e-06, 4.2397e-06, 4.2101e-06, 1.0230e-05, 5.1761e-06,
        6.5300e-06, 7.5576e-06, 4.5737e-06, 7.5329e-06, 7.8685e-06, 2.8279e-06,
        5.0482e-06, 3.2988e-06, 1.2085e-05, 6.5969e-06, 4.5737e-06, 5.6027e-06,
        5.6027e-06, 3.4855e-06, 7.2680e-06, 2.7900e-06, 5.3323e-06, 8.3884e-06,
        3.5419e-06, 5.1537e-06, 9.1444e-06, 5.0702e-06, 3.7684e-06, 1.1457e-05,
        6.5006e-06, 9.2337e-06, 3.2357e-06, 1.0290e-06, 4.6881e-06, 8.3884e-06,
        1.4417e-05, 5.8978e-06, 1.0290e-06, 1.2659e-06, 1.6365e-06, 7.8219e-06,
        2.8257e-06, 4.9070e-06, 4.0023e-06, 5.2759e-06, 2.9965e-06, 2.0254e-06,
        1.0290e-06, 7.8219e-06, 8.5952e-06, 1.2749e-05, 6.0875e-06, 1.2749e-05,
        8.2602e-06, 5.6155e-06, 2.2107e-06, 3.9139e-06, 6.9433e-06, 1.2823e-05,
        4.9192e-06, 5.4345e-06, 1.2069e-05, 4.2656e-06, 4.4085e-06, 1.1205e-05,
        5.4154e-06, 5.2759e-06, 8.3884e-06, 4.4085e-06, 3.9139e-06, 5.2759e-06,
        1.2749e-05, 4.9374e-06, 1.0230e-05, 6.1719e-06, 2.0254e-06, 4.3740e-06,
        4.0030e-06, 3.3056e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.0584e-06, 3.5127e-05, 2.4826e-05, 1.8145e-05, 2.2683e-05, 7.2685e-06,
        3.8465e-05, 3.2180e-05, 3.5822e-05, 1.2408e-05, 2.5812e-05, 4.7681e-05,
        2.7298e-05, 2.8559e-05, 2.7600e-05, 3.6252e-05, 2.9007e-05, 4.5553e-05,
        1.9350e-05, 1.9391e-05, 6.1459e-06, 2.2179e-05, 4.2862e-05, 2.9515e-05,
        3.2883e-05, 1.9028e-05, 6.6281e-06, 2.7386e-05, 2.2155e-05, 3.3318e-05,
        9.6038e-06, 2.0519e-05, 1.0563e-05, 1.3266e-05, 2.9632e-05, 2.2352e-05,
        3.5327e-05, 2.1604e-05, 1.6685e-05, 5.7062e-06, 2.1539e-05, 1.9642e-05,
        1.7884e-05, 2.4731e-05, 1.7884e-05, 1.3289e-05, 3.4799e-05, 1.9669e-05,
        2.1542e-05, 7.7107e-06, 3.7231e-05, 1.6271e-05, 2.1340e-05, 2.9507e-05,
        1.1964e-05, 5.4377e-06, 3.5822e-05, 2.2155e-05, 2.9588e-05, 4.7886e-05,
        2.0590e-05, 3.1491e-05, 1.2453e-05, 1.1303e-05, 2.3555e-05, 1.3845e-05,
        1.4534e-05, 3.3237e-05, 2.5709e-05, 3.7344e-05, 1.8881e-05, 3.0990e-05,
        1.1060e-05, 9.5729e-06, 3.8576e-05, 2.5905e-05, 1.8806e-05, 6.6964e-06,
        1.3458e-05, 2.2425e-05, 1.3960e-05, 1.7605e-05, 1.5027e-05, 1.7884e-05,
        5.2959e-06, 2.1595e-05, 3.7669e-05, 4.1268e-05, 1.9504e-05, 3.0721e-05,
        1.8882e-05, 1.5089e-05, 1.2989e-05, 3.5175e-05, 1.7968e-05, 1.5307e-05,
        6.6964e-06, 2.1340e-05, 4.8152e-05, 1.6127e-05, 1.0618e-05, 3.3161e-05,
        4.7546e-06, 1.0102e-05, 1.6243e-05, 2.3114e-05, 2.7964e-05, 1.7179e-05,
        2.3961e-05, 1.2079e-05, 1.2309e-05, 2.6548e-05, 2.6651e-05, 1.1966e-05,
        1.9187e-05, 1.8320e-05, 2.1495e-05, 4.1255e-05, 1.3147e-05, 2.1140e-05,
        1.5997e-05, 3.0580e-05, 3.2999e-05, 2.4162e-05, 1.4872e-05, 1.2077e-05,
        3.9464e-05, 6.8187e-06, 9.4616e-06, 1.8934e-05, 1.2690e-05, 2.1996e-05,
        9.1308e-06, 1.1287e-05, 1.2211e-05, 3.0027e-05, 6.7662e-06, 1.9358e-05,
        1.9861e-05, 2.8215e-05, 9.3040e-06, 4.8895e-06, 1.7925e-05, 1.5307e-05,
        1.5995e-05, 1.4797e-05, 2.9588e-05, 2.4731e-05, 3.5822e-05, 3.2992e-05,
        1.3287e-05, 3.3992e-05, 3.5992e-06, 1.2023e-05, 2.2236e-05, 5.4928e-06,
        6.2033e-06, 2.8827e-05, 1.9585e-05, 1.2453e-05, 1.0238e-05, 1.0500e-05,
        2.0590e-05, 2.2615e-05, 1.7925e-05, 1.8155e-05, 3.2365e-05, 1.2358e-05,
        3.2505e-05, 1.3287e-05, 1.6532e-05, 1.2559e-05, 6.6281e-06, 1.1031e-05,
        2.1225e-05, 1.0665e-05, 3.1841e-05, 1.1239e-05, 2.5709e-05, 1.1859e-05,
        1.7723e-05, 1.4518e-05, 3.6799e-05, 2.0955e-05, 2.2501e-05, 1.5949e-05,
        1.0286e-05, 2.3372e-05, 1.2885e-05, 2.3835e-05, 1.2304e-05, 1.3778e-05,
        3.2136e-05, 1.2304e-05, 3.1730e-05, 1.7872e-05, 2.0590e-05, 3.9960e-06,
        1.3750e-05, 3.5992e-06, 2.7885e-05, 3.2246e-05, 4.0665e-05, 1.9986e-05,
        1.7769e-05, 4.5076e-05, 1.8628e-05, 4.6280e-05, 7.7009e-06, 2.6651e-05,
        1.6962e-05, 1.7823e-05, 1.3147e-05, 6.4001e-06, 4.6280e-05, 1.8608e-05,
        9.0931e-06, 2.9914e-05, 1.1217e-05, 2.2428e-05, 1.3707e-05, 2.3955e-05,
        1.2559e-05, 1.9187e-05, 5.2382e-06, 2.8444e-05, 3.4269e-05, 1.1321e-05,
        2.2161e-05, 6.6281e-06, 7.2374e-06, 1.7925e-05, 1.9986e-05, 1.8541e-05,
        3.8576e-05, 4.6013e-06, 2.0519e-05, 3.9176e-05, 3.0845e-05, 2.6027e-05,
        1.7925e-05, 1.2166e-05, 4.8772e-05, 7.8695e-06, 7.4706e-06, 1.1356e-05,
        2.1834e-05, 2.1619e-05, 1.4141e-05, 7.0370e-06, 1.1858e-05, 2.7341e-05,
        3.6639e-05, 2.9771e-05, 3.0799e-05, 2.5430e-05, 2.3587e-05, 9.0070e-06,
        2.8444e-05, 3.0407e-05, 1.5307e-05, 1.4722e-05, 3.5850e-05, 1.5317e-05,
        1.7925e-05, 3.5187e-05, 4.0742e-05, 1.5219e-05, 2.4009e-05, 5.5059e-05,
        1.7393e-05, 2.6113e-05, 1.8628e-05, 2.9041e-05, 2.8998e-05, 9.5298e-06,
        2.0590e-05, 1.5137e-05, 1.6906e-05, 2.5709e-05, 1.6512e-05, 1.6518e-05,
        1.4407e-05, 6.6383e-06, 2.5872e-05, 1.5159e-05, 1.0472e-05, 9.0594e-06,
        1.8520e-05, 1.7925e-05, 1.3563e-05, 5.4377e-06, 2.8854e-05, 4.5553e-05,
        1.1493e-05, 1.9035e-05, 3.2505e-05, 3.9297e-06, 1.0059e-05, 2.7974e-05,
        3.0407e-05, 1.1932e-05, 1.6427e-05, 1.0152e-05, 1.0596e-05, 2.5801e-05,
        1.5116e-05, 2.2334e-05, 1.9988e-05, 1.6973e-05, 2.4826e-05, 2.9231e-05,
        2.3308e-05, 1.4165e-05, 3.1426e-05, 4.0917e-06, 6.5338e-06, 2.9041e-05,
        2.2032e-05, 1.5486e-05, 1.3531e-05, 1.2304e-05, 1.0373e-05, 1.1085e-05,
        1.8531e-05, 5.6126e-06, 3.4032e-05, 2.4304e-05, 1.9028e-05, 7.5500e-06,
        3.0784e-05, 2.8210e-05, 1.2034e-05, 4.0297e-05, 1.4872e-05, 2.7742e-05,
        2.4295e-05, 3.5822e-05, 1.0186e-05, 8.5958e-06, 3.9648e-05, 5.2287e-06,
        1.9028e-05, 1.7925e-05, 1.2453e-05, 1.2024e-05, 1.1233e-05, 3.1426e-05,
        1.9134e-05, 3.9998e-05, 2.8416e-05, 3.2505e-05, 2.3114e-05, 9.4448e-06,
        2.9507e-05, 8.0278e-06, 2.8894e-05, 2.1708e-05, 9.5299e-06, 5.5500e-06,
        4.7038e-06, 1.4322e-05, 2.0589e-05, 4.0733e-05, 1.8732e-05, 3.4121e-05,
        5.0928e-06, 1.4258e-05, 4.8895e-06, 2.5822e-05, 1.6677e-06, 1.7968e-05,
        2.0702e-05, 4.0360e-06, 1.7077e-05, 2.2236e-05, 2.3308e-05, 3.0281e-05,
        1.2905e-05, 1.4374e-05, 2.4238e-05, 1.4299e-05, 2.4965e-05, 1.9097e-05,
        1.7823e-05, 2.0735e-05, 6.2033e-06, 2.4965e-05, 2.6662e-05, 1.6127e-05,
        3.5822e-05, 2.6431e-06, 3.6473e-05, 2.1675e-05, 2.5595e-05, 3.0415e-05,
        2.9588e-05, 2.6337e-05, 8.8677e-06, 2.3836e-05, 1.3804e-05, 3.0641e-05,
        3.0862e-05, 3.6083e-05, 1.4397e-05, 1.5954e-05, 3.3506e-06, 4.0067e-05,
        2.3938e-05, 2.3308e-05, 1.3309e-05, 1.9028e-05, 1.9239e-05, 4.7003e-05,
        7.4974e-06, 2.6760e-05, 1.6681e-05, 1.2369e-05, 2.2155e-05, 1.5116e-05,
        5.6698e-05, 2.4731e-05, 2.4826e-05, 2.2425e-05, 9.3825e-06, 4.0493e-05,
        1.2419e-05, 1.2907e-05, 1.3317e-05, 2.5309e-05, 1.5307e-05, 1.1233e-05,
        1.3707e-05, 3.5324e-05, 1.0309e-05, 1.1856e-05, 3.2992e-05, 2.0975e-05,
        3.6399e-05, 1.7602e-05, 2.2423e-05, 2.1619e-05, 1.8284e-05, 2.5812e-05,
        9.3985e-06, 1.8098e-05, 4.3039e-05, 2.9138e-05, 2.2551e-05, 1.6273e-05,
        2.5746e-05, 6.8289e-06, 9.4333e-06, 1.1493e-05, 2.1125e-05, 1.6786e-05,
        7.0303e-06, 2.0137e-05, 2.0144e-05, 3.2365e-05, 3.1468e-05, 9.6851e-06,
        2.7341e-05, 3.5103e-05, 4.7659e-05, 2.6247e-05, 1.2211e-05, 1.4532e-05,
        2.5709e-05, 2.9208e-05, 1.2304e-05, 1.2374e-05, 2.2588e-05, 7.7137e-06,
        2.4826e-05, 5.6657e-05, 2.2899e-05, 1.9540e-05, 2.2915e-05, 2.1708e-05,
        2.3684e-05, 2.9588e-05, 3.5822e-05, 1.7995e-05, 7.0767e-06, 1.8466e-05,
        1.4293e-05, 2.1535e-05, 1.0152e-05, 1.8863e-05, 2.3980e-05, 1.1742e-05,
        1.7605e-05, 1.7593e-05, 1.6785e-05, 2.9121e-05, 2.0362e-05, 3.0824e-05,
        3.4799e-05, 1.2989e-05, 3.0297e-05, 2.0410e-05, 3.0824e-05, 3.2883e-05,
        2.4723e-05, 2.4757e-05, 1.0200e-05, 1.0582e-05, 1.1134e-05, 2.6231e-06,
        4.3039e-05, 1.8953e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0936, 1.4324, 1.0611, 1.2579, 1.0229, 1.2777, 1.2344, 1.3106, 1.2912,
        1.4290, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224, 0.0224,
        0.0224], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.671875, 0.6753472089767456, 1.0, 1.0, 1.0, 0.6770833134651184, 0.6736111044883728, 0.6736111044883728, 0.6753472089767456, 0.6736111044883728, 1.0, 0.6736111044883728, 0.671875, 0.6736111044883728, 0.6770833134651184, 0.6736111044883728, 0.678819477558136, 0.6770833134651184, 0.671875, 0.6770833134651184, 0.671875, 0.6736111044883728, 0.6753472089767456, 0.6770833134651184, 0.671875, 0.671875, 0.671875, 0.678819477558136, 1.0, 0.6736111044883728, 1.0, 0.6736111044883728, 0.6736111044883728, 0.6979166865348816, 1.0, 1.0, 0.671875, 0.6736111044883728, 0.671875, 1.0, 0.671875, 1.0, 0.6736111044883728, 0.6753472089767456, 0.6753472089767456, 0.671875, 0.671875, 0.6753472089767456, 1.0, 0.6736111044883728, 0.6753472089767456, 0.6736111044883728, 0.671875, 0.6736111044883728, 1.0, 0.671875, 1.0, 1.0, 0.671875, 1.0, 0.678819477558136, 0.671875, 1.0, 0.671875]

 sparsity of   [1.0, 0.2517361044883728, 0.25, 1.0, 1.0, 0.25, 0.253472238779068, 0.2569444477558136, 0.265625, 0.253472238779068, 0.25, 0.2899305522441864, 0.2552083432674408, 1.0, 0.2638888955116272, 0.2569444477558136, 0.253472238779068, 0.253472238779068, 1.0, 1.0, 1.0, 0.2586805522441864, 0.2586805522441864, 0.2569444477558136, 1.0, 0.2569444477558136, 0.2552083432674408, 0.2638888955116272, 0.2517361044883728, 0.253472238779068, 1.0, 0.25, 0.2604166567325592, 0.2517361044883728, 0.2552083432674408, 0.2569444477558136, 0.2586805522441864, 0.2552083432674408, 0.2517361044883728, 1.0, 0.2586805522441864, 0.265625, 1.0, 1.0, 0.2586805522441864, 1.0, 1.0, 0.2604166567325592, 0.2517361044883728, 0.2517361044883728, 0.2586805522441864, 0.2517361044883728, 0.25, 1.0, 1.0, 0.253472238779068, 1.0, 0.25, 0.2517361044883728, 1.0, 1.0, 0.2621527910232544, 0.25, 0.2604166567325592]

 sparsity of   [1.0, 0.2934027910232544, 1.0, 0.2916666567325592, 0.300347238779068, 0.28125, 1.0, 0.2986111044883728, 1.0, 0.2951388955116272, 0.2864583432674408, 0.2986111044883728, 0.315972238779068, 0.296875, 1.0, 0.2934027910232544, 0.2881944477558136, 0.2916666567325592, 1.0, 0.2916666567325592, 0.2934027910232544, 1.0, 0.2899305522441864, 1.0, 0.2829861044883728, 0.300347238779068, 1.0, 1.0, 0.296875, 0.2951388955116272, 0.2951388955116272, 0.2934027910232544, 0.2881944477558136, 0.2986111044883728, 0.3020833432674408, 0.3663194477558136, 0.2881944477558136, 1.0, 0.2916666567325592, 1.0, 1.0, 0.2951388955116272, 0.2986111044883728, 0.284722238779068, 0.2864583432674408, 0.2899305522441864, 0.2986111044883728, 1.0, 1.0, 1.0, 1.0, 0.2934027910232544, 0.2951388955116272, 0.3177083432674408, 0.3072916567325592, 1.0, 0.3177083432674408, 1.0, 0.300347238779068, 0.2951388955116272, 1.0, 0.2899305522441864, 0.296875, 0.300347238779068]

 sparsity of   [0.3055555522441864, 0.3229166567325592, 0.34375, 0.3072916567325592, 0.3055555522441864, 0.3298611044883728, 0.315972238779068, 0.3194444477558136, 0.331597238779068, 0.3194444477558136, 0.3038194477558136, 0.3263888955116272, 0.3055555522441864, 0.3038194477558136, 0.3090277910232544, 0.3107638955116272, 0.300347238779068, 0.3385416567325592, 0.3055555522441864, 0.3072916567325592, 0.3055555522441864, 0.3194444477558136, 0.328125, 0.3055555522441864, 0.300347238779068, 0.328125, 0.3402777910232544, 0.3333333432674408, 0.3263888955116272, 0.3055555522441864, 0.3038194477558136, 0.3489583432674408, 0.3402777910232544, 0.3072916567325592, 0.331597238779068, 0.3211805522441864, 0.3368055522441864, 0.3020833432674408, 0.3020833432674408, 0.300347238779068, 0.3246527910232544, 0.300347238779068, 0.3038194477558136, 0.3038194477558136, 0.3385416567325592, 0.300347238779068, 0.3020833432674408, 0.3350694477558136, 0.3107638955116272, 0.3055555522441864, 0.3333333432674408, 0.3194444477558136, 0.331597238779068, 0.300347238779068, 0.3038194477558136, 0.3072916567325592, 0.3055555522441864, 0.3420138955116272, 0.3020833432674408, 0.2986111044883728, 0.3020833432674408, 0.2986111044883728, 0.3368055522441864, 0.40625]

 sparsity of   [0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.010416666977107525, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0, 0.0052083334885537624, 0.013888888992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0086805559694767, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0052083334885537624, 0.0069444444961845875, 0.0034722222480922937, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0086805559694767, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.010416666977107525, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0034722222480922937, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0052083334885537624, 0.0034722222480922937, 0.010416666977107525, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.010416666977107525, 0.0017361111240461469, 0.0086805559694767, 0.0034722222480922937, 0.0069444444961845875, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 1.0, 0.0017361111240461469, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937]

 sparsity of   [0.01996527798473835, 0.02083333395421505, 0.02170138992369175, 0.01909722201526165, 0.01996527798473835, 0.01996527798473835, 0.02170138992369175, 0.0225694440305233, 0.0234375, 0.0234375, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.0225694440305233, 0.02083333395421505, 0.01909722201526165, 0.0225694440305233, 0.0164930559694767, 0.02170138992369175, 0.02690972201526165, 0.0164930559694767, 0.02083333395421505, 0.01996527798473835, 0.01996527798473835, 0.0225694440305233, 0.02604166604578495, 0.0234375, 0.01909722201526165, 0.01822916604578495, 0.02864583395421505, 0.1067708358168602, 0.0173611119389534, 0.02170138992369175, 0.0251736119389534, 0.01996527798473835, 0.01909722201526165, 0.02690972201526165, 0.01822916604578495, 0.0234375, 0.0225694440305233, 0.0164930559694767, 0.02170138992369175, 0.01822916604578495, 0.02083333395421505, 0.0234375, 0.01996527798473835, 0.0225694440305233, 0.0225694440305233, 0.0225694440305233, 0.0243055559694767, 0.2994791567325592, 0.01996527798473835, 0.0303819440305233, 0.0225694440305233, 0.01822916604578495, 0.0234375, 1.0, 0.0225694440305233, 0.01822916604578495, 0.0173611119389534, 0.02083333395421505, 0.01822916604578495, 0.02170138992369175, 0.0225694440305233, 0.02170138992369175, 0.02083333395421505, 0.01996527798473835, 0.0225694440305233, 0.0225694440305233, 0.01996527798473835, 0.02170138992369175, 0.02170138992369175, 0.01996527798473835, 0.0651041641831398, 0.0225694440305233, 0.02604166604578495, 0.01996527798473835, 0.01996527798473835, 0.01822916604578495, 0.0234375, 0.01996527798473835, 0.01822916604578495, 0.01909722201526165, 0.01822916604578495, 0.01996527798473835, 0.01822916604578495, 1.0, 0.01909722201526165, 0.02170138992369175, 0.02170138992369175, 0.2578125, 0.1701388955116272, 0.02170138992369175, 0.02170138992369175, 0.01996527798473835, 0.02083333395421505, 0.02170138992369175, 0.01909722201526165, 0.0251736119389534, 0.01909722201526165, 0.02083333395421505, 0.02170138992369175, 0.0225694440305233, 0.01909722201526165, 0.0225694440305233, 1.0, 0.02170138992369175, 0.01909722201526165, 0.0164930559694767, 0.02170138992369175, 0.0173611119389534, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.02083333395421505, 0.01996527798473835, 0.02083333395421505, 0.02083333395421505, 0.01822916604578495, 0.02777777798473835, 0.0173611119389534, 0.02083333395421505, 0.01822916604578495, 0.01822916604578495]

 sparsity of   [0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.015625, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.03125, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0]

 sparsity of   [1.0, 1.0, 0.0303819440305233, 0.02951388992369175, 0.0347222238779068, 0.0303819440305233, 1.0, 0.02864583395421505, 0.02777777798473835, 1.0, 1.0, 0.02690972201526165, 0.02604166604578495, 0.0251736119389534, 0.02604166604578495, 0.02777777798473835, 1.0, 0.0321180559694767, 0.02604166604578495, 0.02951388992369175, 0.02777777798473835, 0.0303819440305233, 0.0243055559694767, 0.0303819440305233, 0.02777777798473835, 0.02777777798473835, 0.02951388992369175, 1.0, 0.0321180559694767, 0.02604166604578495, 0.02864583395421505, 1.0, 1.0, 0.0338541679084301, 0.02864583395421505, 1.0, 0.02951388992369175, 0.02864583395421505, 0.02690972201526165, 1.0, 0.02777777798473835, 0.02864583395421505, 0.02604166604578495, 0.02864583395421505, 0.02690972201526165, 1.0, 0.03125, 0.02951388992369175, 0.03125, 1.0, 0.0321180559694767, 1.0, 0.0321180559694767, 0.0251736119389534, 0.02864583395421505, 0.02604166604578495, 0.0243055559694767, 0.0329861119389534, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 0.02864583395421505, 1.0, 1.0, 1.0, 0.02864583395421505, 0.0355902798473835, 0.0321180559694767, 0.0321180559694767, 1.0, 1.0, 0.02864583395421505, 1.0, 0.03125, 0.02777777798473835, 0.02690972201526165, 0.02951388992369175, 0.03125, 1.0, 0.02864583395421505, 0.02777777798473835, 0.02951388992369175, 0.02777777798473835, 0.02777777798473835, 0.02864583395421505, 1.0, 0.03125, 1.0, 0.02690972201526165, 0.0329861119389534, 0.02690972201526165, 0.02864583395421505, 1.0, 1.0, 0.02604166604578495, 0.02690972201526165, 0.02864583395421505, 1.0, 0.03125, 0.0303819440305233, 0.02951388992369175, 0.02690972201526165, 0.02690972201526165, 1.0, 0.02777777798473835, 0.0321180559694767, 0.02690972201526165, 1.0, 0.02777777798473835, 0.02604166604578495, 1.0, 0.02777777798473835, 0.03125, 0.02951388992369175, 0.02777777798473835, 1.0, 0.02951388992369175, 1.0, 0.02951388992369175, 0.02604166604578495, 0.02864583395421505, 0.02690972201526165, 0.02690972201526165, 0.03125, 0.02690972201526165, 0.02951388992369175, 0.02951388992369175]

 sparsity of   [0.3055555522441864, 0.265625, 0.2595486044883728, 0.2604166567325592, 0.2595486044883728, 0.2630208432674408, 0.2638888955116272, 0.3194444477558136, 0.2630208432674408, 0.2986111044883728, 0.2664930522441864, 0.3246527910232544, 0.265625, 0.2942708432674408, 0.2595486044883728, 0.2630208432674408, 0.3255208432674408, 0.2586805522441864, 0.323784738779068, 0.3203125, 0.261284738779068, 0.3914930522441864, 0.2647569477558136, 0.2647569477558136, 0.2647569477558136, 0.2595486044883728, 0.2621527910232544, 0.2673611044883728, 0.2664930522441864, 0.2595486044883728, 0.2604166567325592, 0.3402777910232544, 0.2638888955116272, 0.2638888955116272, 0.2630208432674408, 0.2630208432674408, 0.265625, 0.2604166567325592, 0.261284738779068, 0.261284738779068, 0.538194477558136, 0.2873263955116272, 0.2647569477558136, 0.2638888955116272, 0.2560763955116272, 1.0, 0.2881944477558136, 0.3871527910232544, 0.4001736044883728, 0.2586805522441864, 0.2881944477558136, 0.2604166567325592, 0.2725694477558136, 0.2682291567325592, 0.2604166567325592, 0.2673611044883728, 0.2595486044883728, 0.261284738779068, 0.261284738779068, 0.2604166567325592, 0.2630208432674408, 0.2604166567325592, 0.2638888955116272, 0.2638888955116272, 0.261284738779068, 0.9973958134651184, 0.315972238779068, 0.2621527910232544, 0.3567708432674408, 0.2673611044883728, 0.2604166567325592, 0.261284738779068, 0.261284738779068, 0.2604166567325592, 0.2630208432674408, 0.2586805522441864, 0.2595486044883728, 0.261284738779068, 0.2621527910232544, 0.2621527910232544, 0.261284738779068, 0.261284738779068, 0.3828125, 0.2604166567325592, 0.2621527910232544, 0.261284738779068, 0.2621527910232544, 0.2604166567325592, 0.2708333432674408, 0.265625, 0.2578125, 0.2864583432674408, 0.2664930522441864, 0.2621527910232544, 1.0, 0.3333333432674408, 0.34375, 0.2604166567325592, 0.265625, 0.2916666567325592, 0.261284738779068, 0.2604166567325592, 0.3376736044883728, 0.3177083432674408, 0.2664930522441864, 0.2604166567325592, 0.3229166567325592, 0.2621527910232544, 0.2621527910232544, 0.2621527910232544, 0.3029513955116272, 0.2595486044883728, 0.3420138955116272, 0.2621527910232544, 0.2916666567325592, 0.2586805522441864, 0.292534738779068, 0.2647569477558136, 0.261284738779068, 0.2621527910232544, 0.2647569477558136, 0.2595486044883728, 0.261284738779068, 0.3151041567325592, 0.2673611044883728, 0.2621527910232544, 0.2673611044883728, 0.2621527910232544]

 sparsity of   [1.0, 0.0069444444961845875, 0.0052083334885537624, 0.009548611007630825, 0.0078125, 0.0069444444961845875, 0.0078125, 0.0069444444961845875, 0.00434027798473835, 1.0, 0.0086805559694767, 0.0034722222480922937, 0.006076388992369175, 0.006076388992369175, 0.0078125, 0.010416666977107525, 0.0026041667442768812, 0.0069444444961845875, 0.009548611007630825, 1.0, 0.0052083334885537624, 0.01128472201526165, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 1.0, 0.0078125, 0.0052083334885537624, 0.0078125, 0.0086805559694767, 0.0086805559694767, 0.00434027798473835, 0.006076388992369175, 0.00434027798473835, 0.01215277798473835, 0.01215277798473835, 0.0069444444961845875, 0.009548611007630825, 0.0052083334885537624, 0.009548611007630825, 0.00434027798473835, 0.0069444444961845875, 0.0086805559694767, 0.0052083334885537624, 0.009548611007630825, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 1.0, 0.009548611007630825, 0.0086805559694767, 1.0, 0.0086805559694767, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.010416666977107525, 0.0078125, 0.006076388992369175, 0.006076388992369175, 1.0, 0.0069444444961845875, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0086805559694767, 0.01128472201526165, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.009548611007630825, 0.006076388992369175, 0.0052083334885537624, 0.006076388992369175, 1.0, 0.006076388992369175, 0.00434027798473835, 0.006076388992369175, 0.0026041667442768812, 0.0034722222480922937, 1.0, 0.0164930559694767, 0.0017361111240461469, 0.009548611007630825, 0.010416666977107525, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175, 0.006076388992369175, 0.0078125, 0.01128472201526165, 0.0086805559694767, 0.01215277798473835, 0.009548611007630825, 0.0052083334885537624, 0.009548611007630825, 0.0008680555620230734, 0.0052083334885537624, 0.0069444444961845875, 1.0, 0.006076388992369175, 0.0078125, 0.00434027798473835, 0.006076388992369175, 0.009548611007630825, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.00434027798473835, 1.0, 0.0052083334885537624, 0.0078125, 0.00434027798473835, 0.0078125, 0.0069444444961845875, 0.0008680555620230734, 0.0078125, 0.00434027798473835, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0078125, 0.00434027798473835, 0.0052083334885537624, 0.0078125, 0.009548611007630825, 1.0, 0.0069444444961845875, 0.00434027798473835, 0.0026041667442768812, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0086805559694767, 0.0069444444961845875, 0.01128472201526165, 0.0078125, 0.0069444444961845875, 0.0052083334885537624, 1.0, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0078125, 0.0086805559694767, 0.006076388992369175, 0.00434027798473835, 0.0052083334885537624, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 1.0, 0.0078125, 0.0026041667442768812, 0.0034722222480922937, 0.00434027798473835, 0.0086805559694767, 1.0, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.009548611007630825, 0.00434027798473835, 0.01215277798473835, 0.006076388992369175, 1.0, 0.0069444444961845875, 0.0008680555620230734, 0.0086805559694767, 0.0069444444961845875, 0.0026041667442768812, 0.0078125, 0.006076388992369175, 0.0034722222480922937, 0.0069444444961845875, 0.0086805559694767, 0.006076388992369175, 0.009548611007630825, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0026041667442768812, 0.0052083334885537624, 0.0078125, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.0078125, 1.0, 0.009548611007630825, 0.00434027798473835, 0.006076388992369175, 0.0034722222480922937, 1.0, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.00434027798473835, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.006076388992369175, 0.009548611007630825, 0.0052083334885537624, 1.0, 0.0078125, 0.00434027798473835, 0.013020833022892475, 0.00434027798473835, 0.01215277798473835, 0.0034722222480922937, 0.0052083334885537624, 0.010416666977107525, 0.009548611007630825, 0.0086805559694767, 0.01128472201526165, 0.0026041667442768812, 0.0078125, 0.0086805559694767, 0.010416666977107525, 0.0034722222480922937, 0.0069444444961845875, 0.00434027798473835, 0.0034722222480922937, 0.006076388992369175, 0.0034722222480922937, 0.01215277798473835, 0.0078125, 0.0086805559694767, 0.0078125, 1.0, 1.0, 0.0069444444961845875, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175]

 sparsity of   [0.110243059694767, 1.0, 0.1115451380610466, 0.1098090261220932, 0.1145833358168602, 1.0, 0.1128472238779068, 0.1115451380610466, 0.1124131977558136, 1.0, 0.1171875, 1.0, 0.1098090261220932, 1.0, 0.1085069477558136, 0.1098090261220932, 1.0, 0.1124131977558136, 0.110243059694767, 0.1106770858168602, 1.0, 0.1098090261220932, 0.1115451380610466, 0.1119791641831398, 1.0, 0.1124131977558136, 0.109375, 0.1119791641831398, 0.1106770858168602, 1.0, 0.1115451380610466, 0.1163194477558136, 1.0, 0.1115451380610466, 0.114149309694767, 1.0, 1.0, 1.0, 0.1137152761220932, 0.1137152761220932, 1.0, 0.1145833358168602, 0.110243059694767, 1.0, 1.0, 0.110243059694767, 0.1089409738779068, 1.0, 1.0, 1.0, 1.0, 0.1115451380610466, 0.1115451380610466, 0.1115451380610466, 0.1098090261220932, 0.1137152761220932, 0.1137152761220932, 0.114149309694767, 1.0, 0.1089409738779068, 1.0, 1.0, 1.0, 0.1106770858168602, 0.1145833358168602, 1.0, 1.0, 1.0, 0.1106770858168602, 0.1119791641831398, 1.0, 1.0, 1.0, 0.1124131977558136, 1.0, 0.11328125, 0.1098090261220932, 1.0, 0.110243059694767, 0.114149309694767, 0.1085069477558136, 0.1106770858168602, 1.0, 0.114149309694767, 1.0, 0.1106770858168602, 1.0, 0.1111111119389534, 0.858506977558136, 1.0, 1.0, 0.1080729141831398, 0.1128472238779068, 0.1137152761220932, 1.0, 1.0, 0.1124131977558136, 1.0, 0.1236979141831398, 1.0, 0.1085069477558136, 0.1154513880610466, 0.1098090261220932, 0.1111111119389534, 0.114149309694767, 0.1128472238779068, 1.0, 1.0, 0.1124131977558136, 0.1128472238779068, 1.0, 0.1115451380610466, 0.1189236119389534, 1.0, 1.0, 1.0, 0.1111111119389534, 0.1145833358168602, 0.109375, 0.1111111119389534, 1.0, 0.1124131977558136, 0.1128472238779068, 1.0, 0.1119791641831398, 0.1119791641831398, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1076388880610466, 0.1115451380610466, 0.1154513880610466, 0.1124131977558136, 1.0, 0.1119791641831398, 0.1150173619389534, 0.1163194477558136, 0.1119791641831398, 1.0, 0.1119791641831398, 1.0, 1.0, 0.1098090261220932, 0.1085069477558136, 1.0, 1.0, 1.0, 0.1128472238779068, 1.0, 0.1137152761220932, 0.109375, 0.1115451380610466, 0.1111111119389534, 0.1171875, 0.1115451380610466, 0.1150173619389534, 0.1111111119389534, 1.0, 0.1119791641831398, 1.0, 0.1115451380610466, 1.0, 0.1128472238779068, 1.0, 0.1106770858168602, 1.0, 0.1089409738779068, 1.0, 0.1111111119389534, 1.0, 0.1167534738779068, 0.114149309694767, 0.1119791641831398, 1.0, 0.109375, 0.1150173619389534, 0.1098090261220932, 1.0, 1.0, 0.1158854141831398, 0.1124131977558136, 1.0, 0.1119791641831398, 1.0, 1.0, 1.0, 0.110243059694767, 0.1119791641831398, 0.1085069477558136, 0.1128472238779068, 0.114149309694767, 0.1128472238779068, 0.1128472238779068, 0.1124131977558136, 0.1111111119389534, 0.1128472238779068, 1.0, 1.0, 1.0, 0.1119791641831398, 0.11328125, 1.0, 0.1098090261220932, 0.1111111119389534, 0.1124131977558136, 0.1111111119389534, 0.1072048619389534, 1.0, 0.110243059694767, 0.1106770858168602, 0.11328125, 0.1150173619389534, 0.1106770858168602, 0.1115451380610466, 1.0, 0.1124131977558136, 1.0, 0.1119791641831398, 0.1119791641831398, 0.1119791641831398, 0.1137152761220932, 0.1137152761220932, 0.1106770858168602, 1.0, 0.1106770858168602, 0.1128472238779068, 0.1124131977558136, 0.109375, 0.11328125, 1.0, 0.1098090261220932, 0.1106770858168602, 1.0, 1.0, 0.1124131977558136, 0.1137152761220932, 0.1128472238779068, 0.1106770858168602, 1.0, 0.1119791641831398, 0.1119791641831398, 1.0, 0.1115451380610466, 1.0, 0.1115451380610466, 0.114149309694767, 0.11328125, 0.1163194477558136, 0.9934895634651184, 0.110243059694767, 0.11328125, 0.1128472238779068, 0.1145833358168602]

 sparsity of   [0.015625, 1.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 1.0, 1.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.015625, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 1.0, 0.0, 0.015625, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.015625, 0.0078125, 0.015625, 1.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.015625, 1.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.03125, 0.015625, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.015625, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0859375, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0078125, 0.0078125, 0.0, 1.0, 0.0, 0.0546875, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0078125, 0.015625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0, 0.0078125, 0.0, 0.0234375, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0]

 sparsity of   [1.0, 1.0, 0.0915798619389534, 1.0, 1.0, 1.0, 0.0889756977558136, 0.0933159738779068, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 0.0889756977558136, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0850694477558136, 1.0, 1.0, 1.0, 0.0911458358168602, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0872395858168602, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.0924479141831398, 0.0872395858168602, 0.0902777761220932, 1.0, 1.0, 0.0872395858168602, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0863715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0924479141831398, 0.090711809694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0855034738779068, 0.0876736119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0920138880610466, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 0.09375, 0.0963541641831398, 1.0, 0.0876736119389534, 0.10546875, 0.0850694477558136, 0.0902777761220932, 1.0, 1.0, 0.0855034738779068, 1.0, 0.0863715261220932, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0928819477558136, 0.0915798619389534, 1.0, 1.0, 0.1006944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0876736119389534, 1.0, 0.082899309694767, 1.0, 1.0, 1.0, 0.0924479141831398, 1.0, 1.0, 1.0, 0.090711809694767, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 0.0967881977558136, 1.0, 0.0920138880610466, 1.0, 1.0, 1.0, 0.0863715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0924479141831398, 0.0915798619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 0.09375, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0911458358168602, 1.0, 0.0855034738779068, 1.0, 1.0, 1.0, 0.0846354141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0]

 sparsity of   [1.0, 0.7400173544883728, 1.0, 0.7387152910232544, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7417534589767456, 1.0, 0.7395833134651184, 1.0, 1.0, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 0.7408854365348816, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 0.7404513955116272, 1.0, 0.7391493320465088, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7426215410232544, 0.7400173544883728, 0.7426215410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 0.7391493320465088, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7408854365348816, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 0.7400173544883728, 0.7408854365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7400173544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 0.7426215410232544, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0681423619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0659722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0794270858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.075086809694767, 1.0, 0.0646701380610466, 1.0, 0.0876736119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 1.0, 0.071180559694767, 1.0, 0.0707465261220932, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078993059694767, 1.0, 0.0815972238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0707465261220932, 0.0716145858168602, 1.0, 0.075086809694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.063368059694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0768229141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0646701380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0690104141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0694444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0655381977558136, 1.0, 1.0, 0.0646701380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0798611119389534, 1.0, 1.0, 0.0677083358168602, 1.0, 1.0, 1.0, 0.0746527761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0768229141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0733506977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0685763880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0772569477558136, 1.0, 1.0, 1.0, 0.0707465261220932, 1.0, 1.0, 1.0, 1.0, 0.0785590261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0724826380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9207899570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9157986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9203559160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9220920205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.921006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9197048544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9446614384651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9255642294883728, 1.0, 0.9286024570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.924913227558136, 0.9299045205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.919053852558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 0.9242621660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9173176884651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.919053852558136, 1.0, 1.0, 1.0, 0.9214409589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9186198115348816, 1.0, 1.0, 1.0, 0.919921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 0.1796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.234375, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.08984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.021484375, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.0234375, 0.01953125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125]

Total parameter pruned: 9775165.002994955 (unstructured) 9440137 (structured)

Test: [0/79]	Time 0.130 (0.130)	Loss 0.4067 (0.4067) ([0.292]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 94.880

 Total elapsed time  1:18:45.384095 
 FINETUNING


 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.671875, 0.6753472089767456, 1.0, 1.0, 1.0, 0.6770833134651184, 0.6736111044883728, 0.6736111044883728, 0.6753472089767456, 0.6736111044883728, 1.0, 0.6736111044883728, 0.671875, 0.6736111044883728, 0.6770833134651184, 0.6736111044883728, 0.678819477558136, 0.6770833134651184, 0.671875, 0.6770833134651184, 0.671875, 0.6736111044883728, 0.6753472089767456, 0.6770833134651184, 0.671875, 0.671875, 0.671875, 0.678819477558136, 1.0, 0.6736111044883728, 1.0, 0.6736111044883728, 0.6736111044883728, 0.6979166865348816, 1.0, 1.0, 0.671875, 0.6736111044883728, 0.671875, 1.0, 0.671875, 1.0, 0.6736111044883728, 0.6753472089767456, 0.6753472089767456, 0.671875, 0.671875, 0.6753472089767456, 1.0, 0.6736111044883728, 0.6753472089767456, 0.6736111044883728, 0.671875, 0.6736111044883728, 1.0, 0.671875, 1.0, 1.0, 0.671875, 1.0, 0.678819477558136, 0.671875, 1.0, 0.671875]

 sparsity of   [1.0, 0.2517361044883728, 0.25, 1.0, 1.0, 0.25, 0.253472238779068, 0.2569444477558136, 0.265625, 0.253472238779068, 0.25, 0.2899305522441864, 0.2552083432674408, 1.0, 0.2638888955116272, 0.2569444477558136, 0.253472238779068, 0.253472238779068, 1.0, 1.0, 1.0, 0.2586805522441864, 0.2586805522441864, 0.2569444477558136, 1.0, 0.2569444477558136, 0.2552083432674408, 0.2638888955116272, 0.2517361044883728, 0.253472238779068, 1.0, 0.25, 0.2604166567325592, 0.2517361044883728, 0.2552083432674408, 0.2569444477558136, 0.2586805522441864, 0.2552083432674408, 0.2517361044883728, 1.0, 0.2586805522441864, 0.265625, 1.0, 1.0, 0.2586805522441864, 1.0, 1.0, 0.2604166567325592, 0.2517361044883728, 0.2517361044883728, 0.2586805522441864, 0.2517361044883728, 0.25, 1.0, 1.0, 0.253472238779068, 1.0, 0.25, 0.2517361044883728, 1.0, 1.0, 0.2621527910232544, 0.25, 0.2604166567325592]

 sparsity of   [1.0, 0.2934027910232544, 1.0, 0.2916666567325592, 0.300347238779068, 0.28125, 1.0, 0.2986111044883728, 1.0, 0.2951388955116272, 0.2864583432674408, 0.2986111044883728, 0.315972238779068, 0.296875, 1.0, 0.2934027910232544, 0.2881944477558136, 0.2916666567325592, 1.0, 0.2916666567325592, 0.2934027910232544, 1.0, 0.2899305522441864, 1.0, 0.2829861044883728, 0.300347238779068, 1.0, 1.0, 0.296875, 0.2951388955116272, 0.2951388955116272, 0.2934027910232544, 0.2881944477558136, 0.2986111044883728, 0.3020833432674408, 0.3663194477558136, 0.2881944477558136, 1.0, 0.2916666567325592, 1.0, 1.0, 0.2951388955116272, 0.2986111044883728, 0.284722238779068, 0.2864583432674408, 0.2899305522441864, 0.2986111044883728, 1.0, 1.0, 1.0, 1.0, 0.2934027910232544, 0.2951388955116272, 0.3177083432674408, 0.3072916567325592, 1.0, 0.3177083432674408, 1.0, 0.300347238779068, 0.2951388955116272, 1.0, 0.2899305522441864, 0.296875, 0.300347238779068]

 sparsity of   [0.3055555522441864, 0.3229166567325592, 0.34375, 0.3072916567325592, 0.3055555522441864, 0.3298611044883728, 0.315972238779068, 0.3194444477558136, 0.331597238779068, 0.3194444477558136, 0.3038194477558136, 0.3263888955116272, 0.3055555522441864, 0.3038194477558136, 0.3090277910232544, 0.3107638955116272, 0.300347238779068, 0.3385416567325592, 0.3055555522441864, 0.3072916567325592, 0.3055555522441864, 0.3194444477558136, 0.328125, 0.3055555522441864, 0.300347238779068, 0.328125, 0.3402777910232544, 0.3333333432674408, 0.3263888955116272, 0.3055555522441864, 0.3038194477558136, 0.3489583432674408, 0.3402777910232544, 0.3072916567325592, 0.331597238779068, 0.3211805522441864, 0.3368055522441864, 0.3020833432674408, 0.3020833432674408, 0.300347238779068, 0.3246527910232544, 0.300347238779068, 0.3038194477558136, 0.3038194477558136, 0.3385416567325592, 0.300347238779068, 0.3020833432674408, 0.3350694477558136, 0.3107638955116272, 0.3055555522441864, 0.3333333432674408, 0.3194444477558136, 0.331597238779068, 0.300347238779068, 0.3038194477558136, 0.3072916567325592, 0.3055555522441864, 0.3420138955116272, 0.3020833432674408, 0.2986111044883728, 0.3020833432674408, 0.2986111044883728, 0.3368055522441864, 0.40625]

 sparsity of   [0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.010416666977107525, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0, 0.0052083334885537624, 0.013888888992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0086805559694767, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0052083334885537624, 0.0069444444961845875, 0.0034722222480922937, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0086805559694767, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.010416666977107525, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0034722222480922937, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0052083334885537624, 0.0034722222480922937, 0.010416666977107525, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0069444444961845875, 0.010416666977107525, 0.0017361111240461469, 0.0086805559694767, 0.0034722222480922937, 0.0069444444961845875, 0.010416666977107525, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 1.0, 0.0017361111240461469, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937]

 sparsity of   [0.01996527798473835, 0.02083333395421505, 0.02170138992369175, 0.01909722201526165, 0.01996527798473835, 0.01996527798473835, 0.02170138992369175, 0.0225694440305233, 0.0234375, 0.0234375, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.0225694440305233, 0.02083333395421505, 0.01909722201526165, 0.0225694440305233, 0.0164930559694767, 0.02170138992369175, 0.02690972201526165, 0.0164930559694767, 0.02083333395421505, 0.01996527798473835, 0.01996527798473835, 0.0225694440305233, 0.02604166604578495, 0.0234375, 0.01909722201526165, 0.01822916604578495, 0.02864583395421505, 0.1067708358168602, 0.0173611119389534, 0.02170138992369175, 0.0251736119389534, 0.01996527798473835, 0.01909722201526165, 0.02690972201526165, 0.01822916604578495, 0.0234375, 0.0225694440305233, 0.0164930559694767, 0.02170138992369175, 0.01822916604578495, 0.02083333395421505, 0.0234375, 0.01996527798473835, 0.0225694440305233, 0.0225694440305233, 0.0225694440305233, 0.0243055559694767, 0.2994791567325592, 0.01996527798473835, 0.0303819440305233, 0.0225694440305233, 0.01822916604578495, 0.0234375, 1.0, 0.0225694440305233, 0.01822916604578495, 0.0173611119389534, 0.02083333395421505, 0.01822916604578495, 0.02170138992369175, 0.0225694440305233, 0.02170138992369175, 0.02083333395421505, 0.01996527798473835, 0.0225694440305233, 0.0225694440305233, 0.01996527798473835, 0.02170138992369175, 0.02170138992369175, 0.01996527798473835, 0.0651041641831398, 0.0225694440305233, 0.02604166604578495, 0.01996527798473835, 0.01996527798473835, 0.01822916604578495, 0.0234375, 0.01996527798473835, 0.01822916604578495, 0.01909722201526165, 0.01822916604578495, 0.01996527798473835, 0.01822916604578495, 1.0, 0.01909722201526165, 0.02170138992369175, 0.02170138992369175, 0.2578125, 0.1701388955116272, 0.02170138992369175, 0.02170138992369175, 0.01996527798473835, 0.02083333395421505, 0.02170138992369175, 0.01909722201526165, 0.0251736119389534, 0.01909722201526165, 0.02083333395421505, 0.02170138992369175, 0.0225694440305233, 0.01909722201526165, 0.0225694440305233, 1.0, 0.02170138992369175, 0.01909722201526165, 0.0164930559694767, 0.02170138992369175, 0.0173611119389534, 0.02170138992369175, 0.01909722201526165, 0.02170138992369175, 0.02083333395421505, 0.01996527798473835, 0.02083333395421505, 0.02083333395421505, 0.01822916604578495, 0.02777777798473835, 0.0173611119389534, 0.02083333395421505, 0.01822916604578495, 0.01822916604578495]

 sparsity of   [0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.015625, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.03125, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0]

 sparsity of   [1.0, 1.0, 0.0303819440305233, 0.02951388992369175, 0.0347222238779068, 0.0303819440305233, 1.0, 0.02864583395421505, 0.02777777798473835, 1.0, 1.0, 0.02690972201526165, 0.02604166604578495, 0.0251736119389534, 0.02604166604578495, 0.02777777798473835, 1.0, 0.0321180559694767, 0.02604166604578495, 0.02951388992369175, 0.02777777798473835, 0.0303819440305233, 0.0243055559694767, 0.0303819440305233, 0.02777777798473835, 0.02777777798473835, 0.02951388992369175, 1.0, 0.0321180559694767, 0.02604166604578495, 0.02864583395421505, 1.0, 1.0, 0.0338541679084301, 0.02864583395421505, 1.0, 0.02951388992369175, 0.02864583395421505, 0.02690972201526165, 1.0, 0.02777777798473835, 0.02864583395421505, 0.02604166604578495, 0.02864583395421505, 0.02690972201526165, 1.0, 0.03125, 0.02951388992369175, 0.03125, 1.0, 0.0321180559694767, 1.0, 0.0321180559694767, 0.0251736119389534, 0.02864583395421505, 0.02604166604578495, 0.0243055559694767, 0.0329861119389534, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 0.02864583395421505, 1.0, 1.0, 1.0, 0.02864583395421505, 0.0355902798473835, 0.0321180559694767, 0.0321180559694767, 1.0, 1.0, 0.02864583395421505, 1.0, 0.03125, 0.02777777798473835, 0.02690972201526165, 0.02951388992369175, 0.03125, 1.0, 0.02864583395421505, 0.02777777798473835, 0.02951388992369175, 0.02777777798473835, 0.02777777798473835, 0.02864583395421505, 1.0, 0.03125, 1.0, 0.02690972201526165, 0.0329861119389534, 0.02690972201526165, 0.02864583395421505, 1.0, 1.0, 0.02604166604578495, 0.02690972201526165, 0.02864583395421505, 1.0, 0.03125, 0.0303819440305233, 0.02951388992369175, 0.02690972201526165, 0.02690972201526165, 1.0, 0.02777777798473835, 0.0321180559694767, 0.02690972201526165, 1.0, 0.02777777798473835, 0.02604166604578495, 1.0, 0.02777777798473835, 0.03125, 0.02951388992369175, 0.02777777798473835, 1.0, 0.02951388992369175, 1.0, 0.02951388992369175, 0.02604166604578495, 0.02864583395421505, 0.02690972201526165, 0.02690972201526165, 0.03125, 0.02690972201526165, 0.02951388992369175, 0.02951388992369175]

 sparsity of   [0.3055555522441864, 0.265625, 0.2595486044883728, 0.2604166567325592, 0.2595486044883728, 0.2630208432674408, 0.2638888955116272, 0.3194444477558136, 0.2630208432674408, 0.2986111044883728, 0.2664930522441864, 0.3246527910232544, 0.265625, 0.2942708432674408, 0.2595486044883728, 0.2630208432674408, 0.3255208432674408, 0.2586805522441864, 0.323784738779068, 0.3203125, 0.261284738779068, 0.3914930522441864, 0.2647569477558136, 0.2647569477558136, 0.2647569477558136, 0.2595486044883728, 0.2621527910232544, 0.2673611044883728, 0.2664930522441864, 0.2595486044883728, 0.2604166567325592, 0.3402777910232544, 0.2638888955116272, 0.2638888955116272, 0.2630208432674408, 0.2630208432674408, 0.265625, 0.2604166567325592, 0.261284738779068, 0.261284738779068, 0.538194477558136, 0.2873263955116272, 0.2647569477558136, 0.2638888955116272, 0.2560763955116272, 1.0, 0.2881944477558136, 0.3871527910232544, 0.4001736044883728, 0.2586805522441864, 0.2881944477558136, 0.2604166567325592, 0.2725694477558136, 0.2682291567325592, 0.2604166567325592, 0.2673611044883728, 0.2595486044883728, 0.261284738779068, 0.261284738779068, 0.2604166567325592, 0.2630208432674408, 0.2604166567325592, 0.2638888955116272, 0.2638888955116272, 0.261284738779068, 0.9973958134651184, 0.315972238779068, 0.2621527910232544, 0.3567708432674408, 0.2673611044883728, 0.2604166567325592, 0.261284738779068, 0.261284738779068, 0.2604166567325592, 0.2630208432674408, 0.2586805522441864, 0.2595486044883728, 0.261284738779068, 0.2621527910232544, 0.2621527910232544, 0.261284738779068, 0.261284738779068, 0.3828125, 0.2604166567325592, 0.2621527910232544, 0.261284738779068, 0.2621527910232544, 0.2604166567325592, 0.2708333432674408, 0.265625, 0.2578125, 0.2864583432674408, 0.2664930522441864, 0.2621527910232544, 1.0, 0.3333333432674408, 0.34375, 0.2604166567325592, 0.265625, 0.2916666567325592, 0.261284738779068, 0.2604166567325592, 0.3376736044883728, 0.3177083432674408, 0.2664930522441864, 0.2604166567325592, 0.3229166567325592, 0.2621527910232544, 0.2621527910232544, 0.2621527910232544, 0.3029513955116272, 0.2595486044883728, 0.3420138955116272, 0.2621527910232544, 0.2916666567325592, 0.2586805522441864, 0.292534738779068, 0.2647569477558136, 0.261284738779068, 0.2621527910232544, 0.2647569477558136, 0.2595486044883728, 0.261284738779068, 0.3151041567325592, 0.2673611044883728, 0.2621527910232544, 0.2673611044883728, 0.2621527910232544]

 sparsity of   [1.0, 0.0069444444961845875, 0.0052083334885537624, 0.009548611007630825, 0.0078125, 0.0069444444961845875, 0.0078125, 0.0069444444961845875, 0.00434027798473835, 1.0, 0.0086805559694767, 0.0034722222480922937, 0.006076388992369175, 0.006076388992369175, 0.0078125, 0.010416666977107525, 0.0026041667442768812, 0.0069444444961845875, 0.009548611007630825, 1.0, 0.0052083334885537624, 0.01128472201526165, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 1.0, 0.0078125, 0.0052083334885537624, 0.0078125, 0.0086805559694767, 0.0086805559694767, 0.00434027798473835, 0.006076388992369175, 0.00434027798473835, 0.01215277798473835, 0.01215277798473835, 0.0069444444961845875, 0.009548611007630825, 0.0052083334885537624, 0.009548611007630825, 0.00434027798473835, 0.0069444444961845875, 0.0086805559694767, 0.0052083334885537624, 0.009548611007630825, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 1.0, 0.009548611007630825, 0.0086805559694767, 1.0, 0.0086805559694767, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.010416666977107525, 0.0078125, 0.006076388992369175, 0.006076388992369175, 1.0, 0.0069444444961845875, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0086805559694767, 0.01128472201526165, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.009548611007630825, 0.006076388992369175, 0.0052083334885537624, 0.006076388992369175, 1.0, 0.006076388992369175, 0.00434027798473835, 0.006076388992369175, 0.0026041667442768812, 0.0034722222480922937, 1.0, 0.0164930559694767, 0.0017361111240461469, 0.009548611007630825, 0.010416666977107525, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175, 0.006076388992369175, 0.0078125, 0.01128472201526165, 0.0086805559694767, 0.01215277798473835, 0.009548611007630825, 0.0052083334885537624, 0.009548611007630825, 0.0008680555620230734, 0.0052083334885537624, 0.0069444444961845875, 1.0, 0.006076388992369175, 0.0078125, 0.00434027798473835, 0.006076388992369175, 0.009548611007630825, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.00434027798473835, 1.0, 0.0052083334885537624, 0.0078125, 0.00434027798473835, 0.0078125, 0.0069444444961845875, 0.0008680555620230734, 0.0078125, 0.00434027798473835, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0078125, 0.00434027798473835, 0.0052083334885537624, 0.0078125, 0.009548611007630825, 1.0, 0.0069444444961845875, 0.00434027798473835, 0.0026041667442768812, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0086805559694767, 0.0069444444961845875, 0.01128472201526165, 0.0078125, 0.0069444444961845875, 0.0052083334885537624, 1.0, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0078125, 0.0086805559694767, 0.006076388992369175, 0.00434027798473835, 0.0052083334885537624, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 1.0, 0.0078125, 0.0026041667442768812, 0.0034722222480922937, 0.00434027798473835, 0.0086805559694767, 1.0, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.009548611007630825, 0.00434027798473835, 0.01215277798473835, 0.006076388992369175, 1.0, 0.0069444444961845875, 0.0008680555620230734, 0.0086805559694767, 0.0069444444961845875, 0.0026041667442768812, 0.0078125, 0.006076388992369175, 0.0034722222480922937, 0.0069444444961845875, 0.0086805559694767, 0.006076388992369175, 0.009548611007630825, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.0026041667442768812, 0.0052083334885537624, 0.0078125, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.0078125, 1.0, 0.009548611007630825, 0.00434027798473835, 0.006076388992369175, 0.0034722222480922937, 1.0, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.00434027798473835, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.006076388992369175, 0.009548611007630825, 0.0052083334885537624, 1.0, 0.0078125, 0.00434027798473835, 0.013020833022892475, 0.00434027798473835, 0.01215277798473835, 0.0034722222480922937, 0.0052083334885537624, 0.010416666977107525, 0.009548611007630825, 0.0086805559694767, 0.01128472201526165, 0.0026041667442768812, 0.0078125, 0.0086805559694767, 0.010416666977107525, 0.0034722222480922937, 0.0069444444961845875, 0.00434027798473835, 0.0034722222480922937, 0.006076388992369175, 0.0034722222480922937, 0.01215277798473835, 0.0078125, 0.0086805559694767, 0.0078125, 1.0, 1.0, 0.0069444444961845875, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175]

 sparsity of   [0.110243059694767, 1.0, 0.1115451380610466, 0.1098090261220932, 0.1145833358168602, 1.0, 0.1128472238779068, 0.1115451380610466, 0.1124131977558136, 1.0, 0.1171875, 1.0, 0.1098090261220932, 1.0, 0.1085069477558136, 0.1098090261220932, 1.0, 0.1124131977558136, 0.110243059694767, 0.1106770858168602, 1.0, 0.1098090261220932, 0.1115451380610466, 0.1119791641831398, 1.0, 0.1124131977558136, 0.109375, 0.1119791641831398, 0.1106770858168602, 1.0, 0.1115451380610466, 0.1163194477558136, 1.0, 0.1115451380610466, 0.114149309694767, 1.0, 1.0, 1.0, 0.1137152761220932, 0.1137152761220932, 1.0, 0.1145833358168602, 0.110243059694767, 1.0, 1.0, 0.110243059694767, 0.1089409738779068, 1.0, 1.0, 1.0, 1.0, 0.1115451380610466, 0.1115451380610466, 0.1115451380610466, 0.1098090261220932, 0.1137152761220932, 0.1137152761220932, 0.114149309694767, 1.0, 0.1089409738779068, 1.0, 1.0, 1.0, 0.1106770858168602, 0.1145833358168602, 1.0, 1.0, 1.0, 0.1106770858168602, 0.1119791641831398, 1.0, 1.0, 1.0, 0.1124131977558136, 1.0, 0.11328125, 0.1098090261220932, 1.0, 0.110243059694767, 0.114149309694767, 0.1085069477558136, 0.1106770858168602, 1.0, 0.114149309694767, 1.0, 0.1106770858168602, 1.0, 0.1111111119389534, 0.858506977558136, 1.0, 1.0, 0.1080729141831398, 0.1128472238779068, 0.1137152761220932, 1.0, 1.0, 0.1124131977558136, 1.0, 0.1236979141831398, 1.0, 0.1085069477558136, 0.1154513880610466, 0.1098090261220932, 0.1111111119389534, 0.114149309694767, 0.1128472238779068, 1.0, 1.0, 0.1124131977558136, 0.1128472238779068, 1.0, 0.1115451380610466, 0.1189236119389534, 1.0, 1.0, 1.0, 0.1111111119389534, 0.1145833358168602, 0.109375, 0.1111111119389534, 1.0, 0.1124131977558136, 0.1128472238779068, 1.0, 0.1119791641831398, 0.1119791641831398, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1076388880610466, 0.1115451380610466, 0.1154513880610466, 0.1124131977558136, 1.0, 0.1119791641831398, 0.1150173619389534, 0.1163194477558136, 0.1119791641831398, 1.0, 0.1119791641831398, 1.0, 1.0, 0.1098090261220932, 0.1085069477558136, 1.0, 1.0, 1.0, 0.1128472238779068, 1.0, 0.1137152761220932, 0.109375, 0.1115451380610466, 0.1111111119389534, 0.1171875, 0.1115451380610466, 0.1150173619389534, 0.1111111119389534, 1.0, 0.1119791641831398, 1.0, 0.1115451380610466, 1.0, 0.1128472238779068, 1.0, 0.1106770858168602, 1.0, 0.1089409738779068, 1.0, 0.1111111119389534, 1.0, 0.1167534738779068, 0.114149309694767, 0.1119791641831398, 1.0, 0.109375, 0.1150173619389534, 0.1098090261220932, 1.0, 1.0, 0.1158854141831398, 0.1124131977558136, 1.0, 0.1119791641831398, 1.0, 1.0, 1.0, 0.110243059694767, 0.1119791641831398, 0.1085069477558136, 0.1128472238779068, 0.114149309694767, 0.1128472238779068, 0.1128472238779068, 0.1124131977558136, 0.1111111119389534, 0.1128472238779068, 1.0, 1.0, 1.0, 0.1119791641831398, 0.11328125, 1.0, 0.1098090261220932, 0.1111111119389534, 0.1124131977558136, 0.1111111119389534, 0.1072048619389534, 1.0, 0.110243059694767, 0.1106770858168602, 0.11328125, 0.1150173619389534, 0.1106770858168602, 0.1115451380610466, 1.0, 0.1124131977558136, 1.0, 0.1119791641831398, 0.1119791641831398, 0.1119791641831398, 0.1137152761220932, 0.1137152761220932, 0.1106770858168602, 1.0, 0.1106770858168602, 0.1128472238779068, 0.1124131977558136, 0.109375, 0.11328125, 1.0, 0.1098090261220932, 0.1106770858168602, 1.0, 1.0, 0.1124131977558136, 0.1137152761220932, 0.1128472238779068, 0.1106770858168602, 1.0, 0.1119791641831398, 0.1119791641831398, 1.0, 0.1115451380610466, 1.0, 0.1115451380610466, 0.114149309694767, 0.11328125, 0.1163194477558136, 0.9934895634651184, 0.110243059694767, 0.11328125, 0.1128472238779068, 0.1145833358168602]

 sparsity of   [0.015625, 1.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 1.0, 1.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.015625, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 1.0, 0.0, 0.015625, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.015625, 0.0078125, 0.015625, 1.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.015625, 1.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.03125, 0.015625, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.015625, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0859375, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0078125, 0.0078125, 0.0, 1.0, 0.0, 0.0546875, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0078125, 0.015625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.015625, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0, 0.0078125, 0.0, 0.0234375, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0]

 sparsity of   [1.0, 1.0, 0.0915798619389534, 1.0, 1.0, 1.0, 0.0889756977558136, 0.0933159738779068, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 0.0889756977558136, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0850694477558136, 1.0, 1.0, 1.0, 0.0911458358168602, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0872395858168602, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 0.0924479141831398, 0.0872395858168602, 0.0902777761220932, 1.0, 1.0, 0.0872395858168602, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0863715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0924479141831398, 0.090711809694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0855034738779068, 0.0876736119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0920138880610466, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 0.09375, 0.0963541641831398, 1.0, 0.0876736119389534, 0.10546875, 0.0850694477558136, 0.0902777761220932, 1.0, 1.0, 0.0855034738779068, 1.0, 0.0863715261220932, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0928819477558136, 0.0915798619389534, 1.0, 1.0, 0.1006944477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0876736119389534, 1.0, 0.082899309694767, 1.0, 1.0, 1.0, 0.0924479141831398, 1.0, 1.0, 1.0, 0.090711809694767, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 0.0967881977558136, 1.0, 0.0920138880610466, 1.0, 1.0, 1.0, 0.0863715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0924479141831398, 0.0915798619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 0.0902777761220932, 1.0, 1.0, 0.09375, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0911458358168602, 1.0, 0.0855034738779068, 1.0, 1.0, 1.0, 0.0846354141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 1.0, 1.0]

 sparsity of   [1.0, 0.7400173544883728, 1.0, 0.7387152910232544, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7417534589767456, 1.0, 0.7395833134651184, 1.0, 1.0, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 0.7408854365348816, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 0.7404513955116272, 1.0, 0.7391493320465088, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7426215410232544, 0.7400173544883728, 0.7426215410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.741319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 0.7391493320465088, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7408854365348816, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 0.7395833134651184, 1.0, 1.0, 0.7400173544883728, 0.7408854365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7400173544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7404513955116272, 1.0, 1.0, 1.0, 0.7426215410232544, 1.0, 1.0, 1.0, 0.7387152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0681423619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0659722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0794270858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.075086809694767, 1.0, 0.0646701380610466, 1.0, 0.0876736119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 1.0, 0.071180559694767, 1.0, 0.0707465261220932, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078993059694767, 1.0, 0.0815972238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0707465261220932, 0.0716145858168602, 1.0, 0.075086809694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.063368059694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0768229141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0729166641831398, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0646701380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0690104141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0694444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0655381977558136, 1.0, 1.0, 0.0646701380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0798611119389534, 1.0, 1.0, 0.0677083358168602, 1.0, 1.0, 1.0, 0.0746527761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0768229141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0733506977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0685763880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0772569477558136, 1.0, 1.0, 1.0, 0.0707465261220932, 1.0, 1.0, 1.0, 1.0, 0.0785590261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0724826380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9207899570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9157986044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9203559160232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9220920205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.921006977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9197048544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9446614384651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9255642294883728, 1.0, 0.9286024570465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.924913227558136, 0.9299045205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.919053852558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9184027910232544, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 0.9242621660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9173176884651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.919053852558136, 1.0, 1.0, 1.0, 0.9214409589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9186198115348816, 1.0, 1.0, 1.0, 0.919921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 0.1796875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.234375, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.08984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16015625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03515625, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.021484375, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.0234375, 0.01953125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125]

Total parameter pruned: 9775165.002994955 (unstructured) 9440137 (structured)

Test: [0/79]	Time 0.136 (0.136)	Loss 0.4067 (0.4067) ([0.292]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 94.860
current lr 1.00000e-03
Grad=  tensor(0.2444, device='cuda:0')
Epoch: [300][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [300][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [300][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [300][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0026 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2970 (0.2970) ([0.297]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-03
Grad=  tensor(0.0152, device='cuda:0')
Epoch: [301][0/391]	Time 0.148 (0.148)	Data 0.122 (0.122)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [301][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [301][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3030 (0.3030) ([0.303]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.0098, device='cuda:0')
Epoch: [302][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [302][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0077 (0.0025) ([0.008]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [302][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2799 (0.2799) ([0.280]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.0005, device='cuda:0')
Epoch: [303][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [303][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [303][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3252 (0.3252) ([0.325]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.660
current lr 1.00000e-03
Grad=  tensor(0.0102, device='cuda:0')
Epoch: [304][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [304][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [304][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2778 (0.2778) ([0.278]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.680
current lr 1.00000e-03
Grad=  tensor(0.1769, device='cuda:0')
Epoch: [305][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [305][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [305][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2801 (0.2801) ([0.280]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-03
Grad=  tensor(0.2199, device='cuda:0')
Epoch: [306][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [306][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [306][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0085 (0.0026) ([0.008]+[0.000])	Prec@1 99.219 (99.971)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3003 (0.3003) ([0.300]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.860
current lr 1.00000e-03
Grad=  tensor(0.0323, device='cuda:0')
Epoch: [307][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0080 (0.0024) ([0.008]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [307][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0044 (0.0028) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [307][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2875 (0.2875) ([0.288]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0030, device='cuda:0')
Epoch: [308][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [308][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0047 (0.0026) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [308][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2917 (0.2917) ([0.292]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-03
Grad=  tensor(0.0082, device='cuda:0')
Epoch: [309][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [309][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [309][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3089 (0.3089) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-03
Grad=  tensor(1.1636, device='cuda:0')
Epoch: [310][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [310][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [310][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3076 (0.3076) ([0.308]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-03
Grad=  tensor(0.0078, device='cuda:0')
Epoch: [311][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [311][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [311][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2765 (0.2765) ([0.277]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.800
current lr 1.00000e-03
Grad=  tensor(0.0113, device='cuda:0')
Epoch: [312][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [312][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0046 (0.0025) ([0.005]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [312][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0026) ([0.004]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.2895 (0.2895) ([0.289]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0069, device='cuda:0')
Epoch: [313][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [313][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [313][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2770 (0.2770) ([0.277]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-03
Grad=  tensor(0.0062, device='cuda:0')
Epoch: [314][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [314][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [314][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2899 (0.2899) ([0.290]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.0105, device='cuda:0')
Epoch: [315][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [315][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [315][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3087 (0.3087) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.740
current lr 1.00000e-03
Grad=  tensor(0.0252, device='cuda:0')
Epoch: [316][0/391]	Time 0.150 (0.150)	Data 0.123 (0.123)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0061 (0.0023) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [316][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [316][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3030 (0.3030) ([0.303]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-03
Grad=  tensor(0.0068, device='cuda:0')
Epoch: [317][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [317][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [317][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2910 (0.2910) ([0.291]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.840
current lr 1.00000e-03
Grad=  tensor(0.0155, device='cuda:0')
Epoch: [318][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [318][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0044 (0.0024) ([0.004]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [318][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2996 (0.2996) ([0.300]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.780
current lr 1.00000e-03
Grad=  tensor(0.6124, device='cuda:0')
Epoch: [319][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [319][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [319][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [319][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0030 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2984 (0.2984) ([0.298]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.0367, device='cuda:0')
Epoch: [320][0/391]	Time 0.148 (0.148)	Data 0.122 (0.122)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [320][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [320][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0028 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2994 (0.2994) ([0.299]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0011, device='cuda:0')
Epoch: [321][0/391]	Time 0.148 (0.148)	Data 0.123 (0.123)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [321][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [321][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3018 (0.3018) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.2093, device='cuda:0')
Epoch: [322][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [322][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [322][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2897 (0.2897) ([0.290]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-03
Grad=  tensor(2.8327, device='cuda:0')
Epoch: [323][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0083 (0.0083) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [323][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [323][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [323][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0030 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2967 (0.2967) ([0.297]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.0352, device='cuda:0')
Epoch: [324][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [324][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [324][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2914 (0.2914) ([0.291]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [325][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [325][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [325][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3035 (0.3035) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0223, device='cuda:0')
Epoch: [326][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [326][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [326][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3208 (0.3208) ([0.321]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.0476, device='cuda:0')
Epoch: [327][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [327][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [327][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3159 (0.3159) ([0.316]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0573, device='cuda:0')
Epoch: [328][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [328][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [328][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3201 (0.3201) ([0.320]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-03
Grad=  tensor(1.7918, device='cuda:0')
Epoch: [329][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [329][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [329][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3063 (0.3063) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.860
current lr 1.00000e-03
Grad=  tensor(0.0127, device='cuda:0')
Epoch: [330][0/391]	Time 0.152 (0.152)	Data 0.128 (0.128)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [330][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [330][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3069 (0.3069) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-03
Grad=  tensor(0.2142, device='cuda:0')
Epoch: [331][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [331][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3133 (0.3133) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-03
Grad=  tensor(0.0084, device='cuda:0')
Epoch: [332][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [332][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0066 (0.0021) ([0.007]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [332][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0071 (0.0021) ([0.007]+[0.000])	Prec@1 99.219 (99.984)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3121 (0.3121) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-03
Grad=  tensor(0.0320, device='cuda:0')
Epoch: [333][0/391]	Time 0.160 (0.160)	Data 0.135 (0.135)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [333][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [333][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3081 (0.3081) ([0.308]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-03
Grad=  tensor(0.0150, device='cuda:0')
Epoch: [334][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [334][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [334][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3072 (0.3072) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-03
Grad=  tensor(0.0038, device='cuda:0')
Epoch: [335][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [335][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [335][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [335][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3106 (0.3106) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-03
Grad=  tensor(0.0160, device='cuda:0')
Epoch: [336][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [336][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [336][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3122 (0.3122) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0009, device='cuda:0')
Epoch: [337][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [337][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [337][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0080 (0.0022) ([0.008]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3088 (0.3088) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-03
Grad=  tensor(0.0022, device='cuda:0')
Epoch: [338][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [338][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [338][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3171 (0.3171) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-03
Grad=  tensor(0.0070, device='cuda:0')
Epoch: [339][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0050 (0.0019) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [339][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3089 (0.3089) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.860
current lr 1.00000e-03
Grad=  tensor(0.0081, device='cuda:0')
Epoch: [340][0/391]	Time 0.160 (0.160)	Data 0.134 (0.134)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [340][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [340][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3209 (0.3209) ([0.321]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-03
Grad=  tensor(0.0463, device='cuda:0')
Epoch: [341][0/391]	Time 0.159 (0.159)	Data 0.133 (0.133)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3127 (0.3127) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-03
Grad=  tensor(0.0037, device='cuda:0')
Epoch: [342][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [342][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [342][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3058 (0.3058) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-03
Grad=  tensor(0.0175, device='cuda:0')
Epoch: [343][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [343][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3168 (0.3168) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-03
Grad=  tensor(0.0325, device='cuda:0')
Epoch: [344][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [344][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [344][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3124 (0.3124) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-03
Grad=  tensor(0.0113, device='cuda:0')
Epoch: [345][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [345][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [345][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3058 (0.3058) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-03
Grad=  tensor(0.0912, device='cuda:0')
Epoch: [346][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [346][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0062 (0.0019) ([0.006]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3129 (0.3129) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.800
current lr 1.00000e-03
Grad=  tensor(0.0017, device='cuda:0')
Epoch: [347][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [347][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [347][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.115 (0.115)	Loss 0.3181 (0.3181) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-03
Grad=  tensor(0.0496, device='cuda:0')
Epoch: [348][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [348][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [348][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0034 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3275 (0.3275) ([0.328]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.0014, device='cuda:0')
Epoch: [349][0/391]	Time 0.160 (0.160)	Data 0.135 (0.135)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3475 (0.3475) ([0.347]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-04
Grad=  tensor(0.0045, device='cuda:0')
Epoch: [350][0/391]	Time 0.153 (0.153)	Data 0.129 (0.129)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [350][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [350][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3429 (0.3429) ([0.343]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [351][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0076 (0.0019) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0020) ([0.004]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3132 (0.3132) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-04
Grad=  tensor(0.2308, device='cuda:0')
Epoch: [352][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [352][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3291 (0.3291) ([0.329]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.960
current lr 1.00000e-04
Grad=  tensor(0.0010, device='cuda:0')
Epoch: [353][0/391]	Time 0.163 (0.163)	Data 0.137 (0.137)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [353][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [353][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3125 (0.3125) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-04
Grad=  tensor(0.0045, device='cuda:0')
Epoch: [354][0/391]	Time 0.154 (0.154)	Data 0.127 (0.127)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [354][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [354][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3021 (0.3021) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-04
Grad=  tensor(0.0633, device='cuda:0')
Epoch: [355][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [355][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [355][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3010 (0.3010) ([0.301]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-04
Grad=  tensor(0.0176, device='cuda:0')
Epoch: [356][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0032 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][300/391]	Time 0.020 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3318 (0.3318) ([0.332]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-04
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [357][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0046 (0.0020) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [357][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [357][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3178 (0.3178) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-04
Grad=  tensor(0.0056, device='cuda:0')
Epoch: [358][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0251 (0.0018) ([0.025]+[0.000])	Prec@1 99.219 (99.996)
Epoch: [358][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3051 (0.3051) ([0.305]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-04
Grad=  tensor(0.0013, device='cuda:0')
Epoch: [359][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [359][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [359][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3365 (0.3365) ([0.336]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-04
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [360][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0006 (0.0006) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [360][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0047 (0.0018) ([0.005]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [360][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3081 (0.3081) ([0.308]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-04
Grad=  tensor(0.0515, device='cuda:0')
Epoch: [361][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3213 (0.3213) ([0.321]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-04
Grad=  tensor(0.0095, device='cuda:0')
Epoch: [362][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [362][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [362][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3294 (0.3294) ([0.329]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-04
Grad=  tensor(0.0039, device='cuda:0')
Epoch: [363][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [363][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [363][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3156 (0.3156) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-04
Grad=  tensor(0.0036, device='cuda:0')
Epoch: [364][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [364][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [364][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3176 (0.3176) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 95.020
current lr 1.00000e-04
Grad=  tensor(0.0343, device='cuda:0')
Epoch: [365][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0059 (0.0021) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [365][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [365][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3133 (0.3133) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-04
Grad=  tensor(0.0044, device='cuda:0')
Epoch: [366][0/391]	Time 0.159 (0.159)	Data 0.133 (0.133)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [366][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [366][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3163 (0.3163) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.960
current lr 1.00000e-04
Grad=  tensor(0.0090, device='cuda:0')
Epoch: [367][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [367][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [367][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3125 (0.3125) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-04
Grad=  tensor(0.0170, device='cuda:0')
Epoch: [368][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0063 (0.0019) ([0.006]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3101 (0.3101) ([0.310]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-04
Grad=  tensor(0.0014, device='cuda:0')
Epoch: [369][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [369][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [369][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3191 (0.3191) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-04
Grad=  tensor(0.2592, device='cuda:0')
Epoch: [370][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [370][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [370][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3111 (0.3111) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-04
Grad=  tensor(0.0029, device='cuda:0')
Epoch: [371][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0034 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3045 (0.3045) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-04
Grad=  tensor(0.0728, device='cuda:0')
Epoch: [372][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [372][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [372][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3152 (0.3152) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-04
Grad=  tensor(0.0542, device='cuda:0')
Epoch: [373][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [373][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3261 (0.3261) ([0.326]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-04
Grad=  tensor(0.1061, device='cuda:0')
Epoch: [374][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3088 (0.3088) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-04
Grad=  tensor(0.0053, device='cuda:0')
Epoch: [375][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [375][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0123 (0.0020) ([0.012]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [375][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3112 (0.3112) ([0.311]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.840
current lr 1.00000e-04
Grad=  tensor(0.0282, device='cuda:0')
Epoch: [376][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [376][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [376][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3192 (0.3192) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-04
Grad=  tensor(0.0227, device='cuda:0')
Epoch: [377][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3162 (0.3162) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.780
current lr 1.00000e-04
Grad=  tensor(0.0040, device='cuda:0')
Epoch: [378][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [378][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [378][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3058 (0.3058) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-04
Grad=  tensor(0.0166, device='cuda:0')
Epoch: [379][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [379][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0206 (0.0019) ([0.021]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [379][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3020 (0.3020) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-04
Grad=  tensor(0.0038, device='cuda:0')
Epoch: [380][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [380][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [380][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3127 (0.3127) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-04
Grad=  tensor(0.2048, device='cuda:0')
Epoch: [381][0/391]	Time 0.150 (0.150)	Data 0.126 (0.126)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0048 (0.0020) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [381][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0031 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2997 (0.2997) ([0.300]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-04
Grad=  tensor(0.0093, device='cuda:0')
Epoch: [382][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0027 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3088 (0.3088) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.780
current lr 1.00000e-04
Grad=  tensor(0.0774, device='cuda:0')
Epoch: [383][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [383][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0119 (0.0020) ([0.012]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [383][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2918 (0.2918) ([0.292]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-04
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [384][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [384][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3131 (0.3131) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-04
Grad=  tensor(0.0129, device='cuda:0')
Epoch: [385][0/391]	Time 0.147 (0.147)	Data 0.122 (0.122)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0043 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [385][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3101 (0.3101) ([0.310]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-04
Grad=  tensor(0.0374, device='cuda:0')
Epoch: [386][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3022 (0.3022) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-04
Grad=  tensor(0.0123, device='cuda:0')
Epoch: [387][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [387][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [387][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [387][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3286 (0.3286) ([0.329]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.800
current lr 1.00000e-04
Grad=  tensor(0.0367, device='cuda:0')
Epoch: [388][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3146 (0.3146) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.800
current lr 1.00000e-04
Grad=  tensor(0.0439, device='cuda:0')
Epoch: [389][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [389][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [389][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3035 (0.3035) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-04
Grad=  tensor(0.0142, device='cuda:0')
Epoch: [390][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3132 (0.3132) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-04
Grad=  tensor(0.0013, device='cuda:0')
Epoch: [391][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [391][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [391][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3133 (0.3133) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-04
Grad=  tensor(0.0067, device='cuda:0')
Epoch: [392][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3066 (0.3066) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-04
Grad=  tensor(0.3784, device='cuda:0')
Epoch: [393][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3175 (0.3175) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-04
Grad=  tensor(0.1144, device='cuda:0')
Epoch: [394][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [394][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [394][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3126 (0.3126) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [395][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [395][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3055 (0.3055) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-04
Grad=  tensor(0.0012, device='cuda:0')
Epoch: [396][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [396][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [396][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0134 (0.0019) ([0.013]+[0.000])	Prec@1 99.219 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2971 (0.2971) ([0.297]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-04
Grad=  tensor(0.0010, device='cuda:0')
Epoch: [397][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [397][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3264 (0.3264) ([0.326]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-04
Grad=  tensor(0.0637, device='cuda:0')
Epoch: [398][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3114 (0.3114) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 95.090
current lr 1.00000e-04
Grad=  tensor(0.0417, device='cuda:0')
Epoch: [399][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2938 (0.2938) ([0.294]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.780
current lr 1.00000e-05
Grad=  tensor(0.0109, device='cuda:0')
Epoch: [400][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [400][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [400][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3108 (0.3108) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.0300, device='cuda:0')
Epoch: [401][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.019 (0.021)	Data 0.000 (0.001)	Loss 0.0009 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [401][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [401][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3100 (0.3100) ([0.310]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-05
Grad=  tensor(0.0066, device='cuda:0')
Epoch: [402][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [402][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [402][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.115 (0.115)	Loss 0.3118 (0.3118) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-05
Grad=  tensor(0.0062, device='cuda:0')
Epoch: [403][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [403][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [403][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3139 (0.3139) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-05
Grad=  tensor(0.0009, device='cuda:0')
Epoch: [404][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [404][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [404][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3295 (0.3295) ([0.329]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.800
current lr 1.00000e-05
Grad=  tensor(2.3239, device='cuda:0')
Epoch: [405][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0083 (0.0083) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [405][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3227 (0.3227) ([0.323]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-05
Grad=  tensor(0.0897, device='cuda:0')
Epoch: [406][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0058 (0.0020) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [406][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [406][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3027 (0.3027) ([0.303]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.950
current lr 1.00000e-05
Grad=  tensor(0.0160, device='cuda:0')
Epoch: [407][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3132 (0.3132) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-05
Grad=  tensor(0.0992, device='cuda:0')
Epoch: [408][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3023 (0.3023) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.950
current lr 1.00000e-05
Grad=  tensor(0.0050, device='cuda:0')
Epoch: [409][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [409][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3067 (0.3067) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-05
Grad=  tensor(0.2002, device='cuda:0')
Epoch: [410][0/391]	Time 0.166 (0.166)	Data 0.141 (0.141)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss 0.0031 (0.0016) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0035 (0.0017) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3180 (0.3180) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0182, device='cuda:0')
Epoch: [411][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [411][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [411][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3017 (0.3017) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-05
Grad=  tensor(0.0237, device='cuda:0')
Epoch: [412][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [412][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0028 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3174 (0.3174) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-05
Grad=  tensor(0.0062, device='cuda:0')
Epoch: [413][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3145 (0.3145) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-05
Grad=  tensor(0.0154, device='cuda:0')
Epoch: [414][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [414][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3162 (0.3162) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-05
Grad=  tensor(0.0260, device='cuda:0')
Epoch: [415][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0032 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3142 (0.3142) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-05
Grad=  tensor(0.0113, device='cuda:0')
Epoch: [416][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [416][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3244 (0.3244) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.0144, device='cuda:0')
Epoch: [417][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0016) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [417][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [417][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3084 (0.3084) ([0.308]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-05
Grad=  tensor(0.1081, device='cuda:0')
Epoch: [418][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [418][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3196 (0.3196) ([0.320]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-05
Grad=  tensor(0.0109, device='cuda:0')
Epoch: [419][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [419][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3082 (0.3082) ([0.308]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-05
Grad=  tensor(0.0071, device='cuda:0')
Epoch: [420][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0048 (0.0019) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [420][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0046 (0.0019) ([0.005]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3176 (0.3176) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-05
Grad=  tensor(0.0140, device='cuda:0')
Epoch: [421][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [421][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3086 (0.3086) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [422][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [422][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3065 (0.3065) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-05
Grad=  tensor(0.0030, device='cuda:0')
Epoch: [423][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [423][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [423][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3241 (0.3241) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-05
Grad=  tensor(0.0010, device='cuda:0')
Epoch: [424][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0065 (0.0020) ([0.007]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [424][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3123 (0.3123) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-05
Grad=  tensor(0.0095, device='cuda:0')
Epoch: [425][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [425][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [425][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3156 (0.3156) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.0112, device='cuda:0')
Epoch: [426][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [426][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3072 (0.3072) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-05
Grad=  tensor(0.0011, device='cuda:0')
Epoch: [427][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3118 (0.3118) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [428][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [428][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3192 (0.3192) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0122, device='cuda:0')
Epoch: [429][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [429][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [429][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3286 (0.3286) ([0.329]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.860
current lr 1.00000e-05
Grad=  tensor(0.0303, device='cuda:0')
Epoch: [430][0/391]	Time 0.198 (0.198)	Data 0.173 (0.173)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss 0.0043 (0.0020) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [430][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [430][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3125 (0.3125) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-05
Grad=  tensor(0.2260, device='cuda:0')
Epoch: [431][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [431][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3202 (0.3202) ([0.320]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0435, device='cuda:0')
Epoch: [432][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [432][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [432][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3110 (0.3110) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0650, device='cuda:0')
Epoch: [433][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3115 (0.3115) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-05
Grad=  tensor(0.0116, device='cuda:0')
Epoch: [434][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [434][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0028 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3242 (0.3242) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(6.2769, device='cuda:0')
Epoch: [435][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0091 (0.0091) ([0.009]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [435][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [435][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0048 (0.0018) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [435][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3148 (0.3148) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0259, device='cuda:0')
Epoch: [436][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [436][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [436][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3147 (0.3147) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-05
Grad=  tensor(0.2667, device='cuda:0')
Epoch: [437][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3035 (0.3035) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-05
Grad=  tensor(0.0086, device='cuda:0')
Epoch: [438][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3296 (0.3296) ([0.330]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-05
Grad=  tensor(0.0057, device='cuda:0')
Epoch: [439][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3167 (0.3167) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.8090, device='cuda:0')
Epoch: [440][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [440][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3362 (0.3362) ([0.336]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.0285, device='cuda:0')
Epoch: [441][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [441][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3262 (0.3262) ([0.326]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-05
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [442][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0016) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [442][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3239 (0.3239) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-05
Grad=  tensor(0.0055, device='cuda:0')
Epoch: [443][0/391]	Time 0.152 (0.152)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [443][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [443][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3042 (0.3042) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.950
current lr 1.00000e-05
Grad=  tensor(0.0141, device='cuda:0')
Epoch: [444][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [444][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3122 (0.3122) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-05
Grad=  tensor(0.0136, device='cuda:0')
Epoch: [445][0/391]	Time 0.148 (0.148)	Data 0.123 (0.123)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [445][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3088 (0.3088) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 95.000
current lr 1.00000e-05
Grad=  tensor(0.0087, device='cuda:0')
Epoch: [446][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [446][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [446][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3069 (0.3069) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-05
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [447][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [447][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3157 (0.3157) ([0.316]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
current lr 1.00000e-05
Grad=  tensor(0.0011, device='cuda:0')
Epoch: [448][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [448][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [448][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0048 (0.0018) ([0.005]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3142 (0.3142) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-05
Grad=  tensor(0.0554, device='cuda:0')
Epoch: [449][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [449][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0040 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [449][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3229 (0.3229) ([0.323]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-06
Grad=  tensor(0.0276, device='cuda:0')
Epoch: [450][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [450][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [450][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3307 (0.3307) ([0.331]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0045, device='cuda:0')
Epoch: [451][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3188 (0.3188) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.950
current lr 1.00000e-06
Grad=  tensor(0.0030, device='cuda:0')
Epoch: [452][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3048 (0.3048) ([0.305]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-06
Grad=  tensor(0.0056, device='cuda:0')
Epoch: [453][0/391]	Time 0.154 (0.154)	Data 0.127 (0.127)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [453][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [453][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3147 (0.3147) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.0034, device='cuda:0')
Epoch: [454][0/391]	Time 0.159 (0.159)	Data 0.132 (0.132)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.019 (0.021)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [454][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [454][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3044 (0.3044) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0244, device='cuda:0')
Epoch: [455][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0042 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [455][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [455][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3225 (0.3225) ([0.322]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.7396, device='cuda:0')
Epoch: [456][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [456][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [456][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0045 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3264 (0.3264) ([0.326]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0141, device='cuda:0')
Epoch: [457][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0036 (0.0020) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3187 (0.3187) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.0041, device='cuda:0')
Epoch: [458][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0063 (0.0019) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [458][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3110 (0.3110) ([0.311]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-06
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [459][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [459][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3217 (0.3217) ([0.322]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-06
Grad=  tensor(0.0074, device='cuda:0')
Epoch: [460][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3032 (0.3032) ([0.303]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-06
Grad=  tensor(0.0630, device='cuda:0')
Epoch: [461][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [461][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [461][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3194 (0.3194) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.770
current lr 1.00000e-06
Grad=  tensor(0.0241, device='cuda:0')
Epoch: [462][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [462][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [462][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3047 (0.3047) ([0.305]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-06
Grad=  tensor(0.0133, device='cuda:0')
Epoch: [463][0/391]	Time 0.150 (0.150)	Data 0.126 (0.126)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [463][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0027 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3256 (0.3256) ([0.326]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-06
Grad=  tensor(0.1749, device='cuda:0')
Epoch: [464][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [464][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0038 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [464][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3024 (0.3024) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0044, device='cuda:0')
Epoch: [465][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3238 (0.3238) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-06
Grad=  tensor(0.1332, device='cuda:0')
Epoch: [466][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [466][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [466][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3396 (0.3396) ([0.340]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-06
Grad=  tensor(0.0067, device='cuda:0')
Epoch: [467][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [467][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3096 (0.3096) ([0.310]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.1278, device='cuda:0')
Epoch: [468][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [468][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [468][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3016 (0.3016) ([0.302]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.0036, device='cuda:0')
Epoch: [469][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [469][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3041 (0.3041) ([0.304]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-06
Grad=  tensor(0.1024, device='cuda:0')
Epoch: [470][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [470][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2997 (0.2997) ([0.300]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.830
current lr 1.00000e-06
Grad=  tensor(0.0111, device='cuda:0')
Epoch: [471][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [471][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3124 (0.3124) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.930
current lr 1.00000e-06
Grad=  tensor(0.0052, device='cuda:0')
Epoch: [472][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3214 (0.3214) ([0.321]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.0041, device='cuda:0')
Epoch: [473][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [473][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [473][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3093 (0.3093) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.920
current lr 1.00000e-06
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [474][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [474][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [474][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0006 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3139 (0.3139) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.0021, device='cuda:0')
Epoch: [475][0/391]	Time 0.147 (0.147)	Data 0.121 (0.121)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [475][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [475][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0040 (0.0020) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3074 (0.3074) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-06
Grad=  tensor(0.0685, device='cuda:0')
Epoch: [476][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [476][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [476][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3142 (0.3142) ([0.314]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-06
Grad=  tensor(0.0024, device='cuda:0')
Epoch: [477][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [477][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [477][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.3103 (0.3103) ([0.310]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.0165, device='cuda:0')
Epoch: [478][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [478][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [478][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3170 (0.3170) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-06
Grad=  tensor(0.0020, device='cuda:0')
Epoch: [479][0/391]	Time 0.165 (0.165)	Data 0.139 (0.139)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss 0.0020 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [479][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3062 (0.3062) ([0.306]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-06
Grad=  tensor(0.0029, device='cuda:0')
Epoch: [480][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [480][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [480][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3095 (0.3095) ([0.309]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.880
current lr 1.00000e-06
Grad=  tensor(0.0122, device='cuda:0')
Epoch: [481][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0072 (0.0019) ([0.007]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3135 (0.3135) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.0052, device='cuda:0')
Epoch: [482][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [482][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [482][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.3178 (0.3178) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-06
Grad=  tensor(0.0031, device='cuda:0')
Epoch: [483][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [483][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [483][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3249 (0.3249) ([0.325]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.790
current lr 1.00000e-06
Grad=  tensor(0.0316, device='cuda:0')
Epoch: [484][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [484][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [484][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2983 (0.2983) ([0.298]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.960
current lr 1.00000e-06
Grad=  tensor(0.2807, device='cuda:0')
Epoch: [485][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [485][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.3147 (0.3147) ([0.315]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0741, device='cuda:0')
Epoch: [486][0/391]	Time 0.164 (0.164)	Data 0.138 (0.138)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.019 (0.020)	Data 0.000 (0.002)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [486][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [486][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0045 (0.0018) ([0.005]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3175 (0.3175) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.910
current lr 1.00000e-06
Grad=  tensor(0.0016, device='cuda:0')
Epoch: [487][0/391]	Time 0.152 (0.152)	Data 0.125 (0.125)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3241 (0.3241) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.900
current lr 1.00000e-06
Grad=  tensor(0.0088, device='cuda:0')
Epoch: [488][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3129 (0.3129) ([0.313]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.840
current lr 1.00000e-06
Grad=  tensor(0.0047, device='cuda:0')
Epoch: [489][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [489][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [489][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3217 (0.3217) ([0.322]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.970
current lr 1.00000e-06
Grad=  tensor(0.0125, device='cuda:0')
Epoch: [490][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3185 (0.3185) ([0.319]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.950
current lr 1.00000e-06
Grad=  tensor(0.0175, device='cuda:0')
Epoch: [491][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3066 (0.3066) ([0.307]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.0101, device='cuda:0')
Epoch: [492][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [492][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [492][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.3184 (0.3184) ([0.318]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-06
Grad=  tensor(0.0080, device='cuda:0')
Epoch: [493][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [493][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3208 (0.3208) ([0.321]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-06
Grad=  tensor(0.0488, device='cuda:0')
Epoch: [494][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0042 (0.0017) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3172 (0.3172) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.0258, device='cuda:0')
Epoch: [495][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3246 (0.3246) ([0.325]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.940
current lr 1.00000e-06
Grad=  tensor(0.1563, device='cuda:0')
Epoch: [496][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3236 (0.3236) ([0.324]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.870
current lr 1.00000e-06
Grad=  tensor(0.0106, device='cuda:0')
Epoch: [497][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3121 (0.3121) ([0.312]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.850
current lr 1.00000e-06
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [498][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.3231 (0.3231) ([0.323]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.980
current lr 1.00000e-06
Grad=  tensor(0.0005, device='cuda:0')
Epoch: [499][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [499][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3168 (0.3168) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890

 Elapsed time for training  1:46:32.580080

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.021484375, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.017578125, 0.017578125, 0.021484375, 0.0234375, 0.01953125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125, 0.205078125]
Total parameter pruned: 9453145.0 (unstructured) 9443593 (structured)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3168 (0.3168) ([0.317]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.890
Best accuracy:  95.09
