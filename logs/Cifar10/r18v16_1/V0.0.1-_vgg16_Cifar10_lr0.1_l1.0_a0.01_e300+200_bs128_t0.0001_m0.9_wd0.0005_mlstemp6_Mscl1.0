V0.0.1-_vgg16_Cifar10_lr0.1_l1.0_a0.01_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp6_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.6437135934829712, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.3027346432209015, Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.1490456759929657, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.13979694247245789, Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.20777323842048645, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.10516299307346344, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.10126874595880508, Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.09506335854530334, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.05375269055366516, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.050536274909973145, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.04137488082051277, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.027590204030275345, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.03216041252017021, Linear(in_features=512, out_features=10, bias=True): 0.3597226142883301}
current lr 1.00000e-01
Grad=  tensor(70.3762, device='cuda:0')
Epoch: [0][0/391]	Time 0.228 (0.228)	Data 0.195 (0.195)	Loss 3.5139 (3.5139) ([2.316]+[1.198])	Prec@1 11.719 (11.719)
Epoch: [0][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 2.9661 (3.2687) ([2.059]+[0.907])	Prec@1 21.875 (14.944)
Epoch: [0][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 2.6725 (3.0338) ([1.912]+[0.760])	Prec@1 28.125 (18.699)
Epoch: [0][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 2.3988 (2.8742) ([1.773]+[0.625])	Prec@1 26.562 (21.449)
Test: [0/79]	Time 0.206 (0.206)	Loss 2.1978 (2.1978) ([1.680]+[0.518])	Prec@1 33.594 (33.594)
 * Prec@1 32.570
current lr 1.00000e-01
Grad=  tensor(0.5584, device='cuda:0')
Epoch: [1][0/391]	Time 0.233 (0.233)	Data 0.201 (0.201)	Loss 2.1612 (2.1612) ([1.644]+[0.518])	Prec@1 29.688 (29.688)
Epoch: [1][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 2.0535 (2.1452) ([1.641]+[0.412])	Prec@1 36.719 (34.367)
Epoch: [1][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.9026 (2.0586) ([1.571]+[0.332])	Prec@1 34.375 (36.377)
Epoch: [1][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 1.9618 (1.9752) ([1.680]+[0.282])	Prec@1 36.719 (38.738)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.0923 (2.0923) ([1.816]+[0.276])	Prec@1 35.938 (35.938)
 * Prec@1 42.230
current lr 1.00000e-01
Grad=  tensor(1.9419, device='cuda:0')
Epoch: [2][0/391]	Time 0.260 (0.260)	Data 0.227 (0.227)	Loss 1.6102 (1.6102) ([1.334]+[0.276])	Prec@1 49.219 (49.219)
Epoch: [2][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 1.4718 (1.6093) ([1.198]+[0.274])	Prec@1 59.375 (51.725)
Epoch: [2][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 1.4977 (1.5682) ([1.223]+[0.275])	Prec@1 54.688 (53.238)
Epoch: [2][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.5533 (1.5310) ([1.278]+[0.275])	Prec@1 56.250 (54.708)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3767 (2.3767) ([2.095]+[0.282])	Prec@1 39.062 (39.062)
 * Prec@1 34.600
current lr 1.00000e-01
Grad=  tensor(1.7569, device='cuda:0')
Epoch: [3][0/391]	Time 0.232 (0.232)	Data 0.200 (0.200)	Loss 1.2549 (1.2549) ([0.973]+[0.282])	Prec@1 72.656 (72.656)
Epoch: [3][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 1.4125 (1.4113) ([1.120]+[0.293])	Prec@1 62.500 (60.620)
Epoch: [3][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 1.2808 (1.3781) ([0.990]+[0.290])	Prec@1 64.062 (61.944)
Epoch: [3][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.3498 (1.3589) ([1.056]+[0.294])	Prec@1 65.625 (62.876)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.0450 (2.0450) ([1.747]+[0.298])	Prec@1 44.531 (44.531)
 * Prec@1 44.530
current lr 1.00000e-01
Grad=  tensor(1.8168, device='cuda:0')
Epoch: [4][0/391]	Time 0.225 (0.225)	Data 0.193 (0.193)	Loss 1.1188 (1.1188) ([0.821]+[0.298])	Prec@1 74.219 (74.219)
Epoch: [4][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 1.3185 (1.2598) ([1.019]+[0.300])	Prec@1 67.188 (67.087)
Epoch: [4][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.1240 (1.2453) ([0.823]+[0.301])	Prec@1 71.875 (67.631)
Epoch: [4][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.3447 (1.2392) ([1.047]+[0.297])	Prec@1 66.406 (67.969)
Test: [0/79]	Time 0.197 (0.197)	Loss 1.6317 (1.6317) ([1.334]+[0.298])	Prec@1 57.031 (57.031)
 * Prec@1 59.160
current lr 1.00000e-01
Grad=  tensor(2.1454, device='cuda:0')
Epoch: [5][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 1.1631 (1.1631) ([0.865]+[0.298])	Prec@1 71.875 (71.875)
Epoch: [5][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 1.2266 (1.1869) ([0.928]+[0.299])	Prec@1 70.312 (70.560)
Epoch: [5][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0188 (1.1755) ([0.715]+[0.304])	Prec@1 75.781 (70.938)
Epoch: [5][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9336 (1.1723) ([0.626]+[0.308])	Prec@1 82.812 (71.205)
Test: [0/79]	Time 0.194 (0.194)	Loss 1.6555 (1.6555) ([1.346]+[0.310])	Prec@1 57.031 (57.031)
 * Prec@1 60.340
current lr 1.00000e-01
Grad=  tensor(2.0007, device='cuda:0')
Epoch: [6][0/391]	Time 0.209 (0.209)	Data 0.181 (0.181)	Loss 1.3043 (1.3043) ([0.995]+[0.310])	Prec@1 72.656 (72.656)
Epoch: [6][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.9986 (1.1164) ([0.690]+[0.309])	Prec@1 78.125 (73.291)
Epoch: [6][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.1689 (1.1133) ([0.867]+[0.302])	Prec@1 72.656 (73.488)
Epoch: [6][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9075 (1.1091) ([0.604]+[0.304])	Prec@1 78.125 (73.622)
Test: [0/79]	Time 0.193 (0.193)	Loss 1.7352 (1.7352) ([1.430]+[0.305])	Prec@1 53.906 (53.906)
 * Prec@1 54.850
current lr 1.00000e-01
Grad=  tensor(2.9953, device='cuda:0')
Epoch: [7][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 1.1890 (1.1890) ([0.884]+[0.305])	Prec@1 73.438 (73.438)
Epoch: [7][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 1.0231 (1.1050) ([0.714]+[0.309])	Prec@1 77.344 (73.778)
Epoch: [7][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.1694 (1.0846) ([0.869]+[0.300])	Prec@1 71.094 (74.409)
Epoch: [7][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 1.0379 (1.0750) ([0.729]+[0.309])	Prec@1 74.219 (74.813)
Test: [0/79]	Time 0.212 (0.212)	Loss 1.2029 (1.2029) ([0.896]+[0.307])	Prec@1 73.438 (73.438)
 * Prec@1 70.020
current lr 1.00000e-01
Grad=  tensor(1.7428, device='cuda:0')
Epoch: [8][0/391]	Time 0.259 (0.259)	Data 0.225 (0.225)	Loss 0.9468 (0.9468) ([0.640]+[0.307])	Prec@1 78.125 (78.125)
Epoch: [8][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.9380 (1.0343) ([0.634]+[0.304])	Prec@1 76.562 (76.222)
Epoch: [8][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.0184 (1.0445) ([0.713]+[0.306])	Prec@1 79.688 (76.034)
Epoch: [8][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.0787 (1.0383) ([0.777]+[0.302])	Prec@1 74.219 (76.264)
Test: [0/79]	Time 0.198 (0.198)	Loss 1.7989 (1.7989) ([1.491]+[0.308])	Prec@1 58.594 (58.594)
 * Prec@1 58.190
current lr 1.00000e-01
Grad=  tensor(1.4866, device='cuda:0')
Epoch: [9][0/391]	Time 0.222 (0.222)	Data 0.191 (0.191)	Loss 0.8898 (0.8898) ([0.582]+[0.308])	Prec@1 84.375 (84.375)
Epoch: [9][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8725 (0.9982) ([0.570]+[0.303])	Prec@1 78.906 (78.187)
Epoch: [9][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.9265 (1.0025) ([0.628]+[0.299])	Prec@1 77.344 (77.604)
Epoch: [9][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.1257 (1.0070) ([0.820]+[0.306])	Prec@1 75.781 (77.422)
Test: [0/79]	Time 0.187 (0.187)	Loss 1.5335 (1.5335) ([1.232]+[0.301])	Prec@1 65.625 (65.625)
 * Prec@1 57.940
current lr 1.00000e-01
Grad=  tensor(2.6047, device='cuda:0')
Epoch: [10][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.8996 (0.8996) ([0.599]+[0.301])	Prec@1 77.344 (77.344)
Epoch: [10][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 1.0068 (0.9866) ([0.707]+[0.300])	Prec@1 74.219 (77.885)
Epoch: [10][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.9236 (0.9895) ([0.622]+[0.301])	Prec@1 78.906 (77.678)
Epoch: [10][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.1384 (0.9780) ([0.840]+[0.298])	Prec@1 71.875 (78.203)
Test: [0/79]	Time 0.194 (0.194)	Loss 1.2270 (1.2270) ([0.932]+[0.295])	Prec@1 71.875 (71.875)
 * Prec@1 71.900
current lr 1.00000e-01
Grad=  tensor(1.9277, device='cuda:0')
Epoch: [11][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.8800 (0.8800) ([0.585]+[0.295])	Prec@1 78.906 (78.906)
Epoch: [11][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 1.1002 (0.9629) ([0.801]+[0.299])	Prec@1 72.656 (78.744)
Epoch: [11][200/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 1.1249 (0.9609) ([0.824]+[0.301])	Prec@1 72.656 (78.716)
Epoch: [11][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.9963 (0.9712) ([0.696]+[0.300])	Prec@1 73.438 (78.429)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.9513 (0.9513) ([0.652]+[0.299])	Prec@1 79.688 (79.688)
 * Prec@1 73.420
current lr 1.00000e-01
Grad=  tensor(1.8150, device='cuda:0')
Epoch: [12][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.9461 (0.9461) ([0.647]+[0.299])	Prec@1 78.906 (78.906)
Epoch: [12][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 1.0618 (0.9502) ([0.760]+[0.302])	Prec@1 79.688 (79.448)
Epoch: [12][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.8499 (0.9558) ([0.551]+[0.299])	Prec@1 81.250 (79.171)
Epoch: [12][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.8120 (0.9538) ([0.513]+[0.299])	Prec@1 84.375 (79.241)
Test: [0/79]	Time 0.216 (0.216)	Loss 1.2178 (1.2178) ([0.922]+[0.296])	Prec@1 70.312 (70.312)
 * Prec@1 70.560
current lr 1.00000e-01
Grad=  tensor(1.4793, device='cuda:0')
Epoch: [13][0/391]	Time 0.203 (0.203)	Data 0.174 (0.174)	Loss 0.8950 (0.8950) ([0.599]+[0.296])	Prec@1 81.250 (81.250)
Epoch: [13][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7350 (0.9311) ([0.440]+[0.295])	Prec@1 85.938 (79.347)
Epoch: [13][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.8134 (0.9397) ([0.519]+[0.295])	Prec@1 82.031 (79.120)
Epoch: [13][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0800 (0.9421) ([0.786]+[0.294])	Prec@1 74.219 (79.122)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.2222 (1.2222) ([0.930]+[0.292])	Prec@1 75.781 (75.781)
 * Prec@1 72.640
current lr 1.00000e-01
Grad=  tensor(1.6477, device='cuda:0')
Epoch: [14][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.7517 (0.7517) ([0.460]+[0.292])	Prec@1 86.719 (86.719)
Epoch: [14][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8636 (0.9103) ([0.570]+[0.294])	Prec@1 81.250 (79.780)
Epoch: [14][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0841 (0.9160) ([0.792]+[0.292])	Prec@1 73.438 (79.804)
Epoch: [14][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9234 (0.9193) ([0.634]+[0.289])	Prec@1 78.125 (79.682)
Test: [0/79]	Time 0.182 (0.182)	Loss 1.2942 (1.2942) ([1.007]+[0.287])	Prec@1 72.656 (72.656)
 * Prec@1 67.880
current lr 1.00000e-01
Grad=  tensor(2.4321, device='cuda:0')
Epoch: [15][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.8955 (0.8955) ([0.608]+[0.287])	Prec@1 81.250 (81.250)
Epoch: [15][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.9933 (0.8927) ([0.705]+[0.289])	Prec@1 78.125 (80.260)
Epoch: [15][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9534 (0.9060) ([0.666]+[0.287])	Prec@1 81.250 (79.998)
Epoch: [15][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.9847 (0.9063) ([0.695]+[0.290])	Prec@1 78.906 (80.004)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.9091 (0.9091) ([0.625]+[0.285])	Prec@1 78.906 (78.906)
 * Prec@1 76.170
current lr 1.00000e-01
Grad=  tensor(2.3325, device='cuda:0')
Epoch: [16][0/391]	Time 0.245 (0.245)	Data 0.215 (0.215)	Loss 0.9189 (0.9189) ([0.634]+[0.285])	Prec@1 79.688 (79.688)
Epoch: [16][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.9598 (0.8860) ([0.674]+[0.286])	Prec@1 82.031 (80.995)
Epoch: [16][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8805 (0.8977) ([0.593]+[0.287])	Prec@1 81.250 (80.379)
Epoch: [16][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.8728 (0.9016) ([0.588]+[0.285])	Prec@1 76.562 (80.264)
Test: [0/79]	Time 0.186 (0.186)	Loss 1.2448 (1.2448) ([0.956]+[0.288])	Prec@1 67.188 (67.188)
 * Prec@1 70.340
current lr 1.00000e-01
Grad=  tensor(2.6177, device='cuda:0')
Epoch: [17][0/391]	Time 0.227 (0.227)	Data 0.198 (0.198)	Loss 0.9509 (0.9509) ([0.662]+[0.288])	Prec@1 78.125 (78.125)
Epoch: [17][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7537 (0.8956) ([0.467]+[0.286])	Prec@1 85.938 (80.554)
Epoch: [17][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8338 (0.8851) ([0.547]+[0.286])	Prec@1 84.375 (80.803)
Epoch: [17][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8804 (0.8876) ([0.596]+[0.284])	Prec@1 82.031 (80.778)
Test: [0/79]	Time 0.226 (0.226)	Loss 1.1436 (1.1436) ([0.861]+[0.283])	Prec@1 73.438 (73.438)
 * Prec@1 75.610
current lr 1.00000e-01
Grad=  tensor(1.2946, device='cuda:0')
Epoch: [18][0/391]	Time 0.227 (0.227)	Data 0.194 (0.194)	Loss 0.7415 (0.7415) ([0.459]+[0.283])	Prec@1 84.375 (84.375)
Epoch: [18][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8566 (0.8711) ([0.572]+[0.285])	Prec@1 80.469 (81.196)
Epoch: [18][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7850 (0.8833) ([0.501]+[0.284])	Prec@1 84.375 (80.889)
Epoch: [18][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.9440 (0.8795) ([0.663]+[0.281])	Prec@1 81.250 (80.954)
Test: [0/79]	Time 0.182 (0.182)	Loss 1.3667 (1.3667) ([1.086]+[0.280])	Prec@1 67.188 (67.188)
 * Prec@1 70.620
current lr 1.00000e-01
Grad=  tensor(1.5899, device='cuda:0')
Epoch: [19][0/391]	Time 0.232 (0.232)	Data 0.200 (0.200)	Loss 0.8195 (0.8195) ([0.539]+[0.280])	Prec@1 82.812 (82.812)
Epoch: [19][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7880 (0.8704) ([0.508]+[0.280])	Prec@1 81.250 (81.211)
Epoch: [19][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8137 (0.8664) ([0.534]+[0.279])	Prec@1 83.594 (81.234)
Epoch: [19][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7442 (0.8694) ([0.463]+[0.282])	Prec@1 82.812 (81.206)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.1656 (1.1656) ([0.883]+[0.283])	Prec@1 71.094 (71.094)
 * Prec@1 70.070
current lr 1.00000e-01
Grad=  tensor(1.9166, device='cuda:0')
Epoch: [20][0/391]	Time 0.207 (0.207)	Data 0.176 (0.176)	Loss 0.8378 (0.8378) ([0.555]+[0.283])	Prec@1 81.250 (81.250)
Epoch: [20][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8449 (0.8662) ([0.565]+[0.280])	Prec@1 84.375 (81.289)
Epoch: [20][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6971 (0.8656) ([0.414]+[0.283])	Prec@1 90.625 (81.234)
Epoch: [20][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8219 (0.8694) ([0.544]+[0.278])	Prec@1 79.688 (81.107)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.2113 (1.2113) ([0.936]+[0.275])	Prec@1 68.750 (68.750)
 * Prec@1 70.580
current lr 1.00000e-01
Grad=  tensor(2.3805, device='cuda:0')
Epoch: [21][0/391]	Time 0.256 (0.256)	Data 0.225 (0.225)	Loss 0.9860 (0.9860) ([0.711]+[0.275])	Prec@1 79.688 (79.688)
Epoch: [21][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8276 (0.8354) ([0.553]+[0.274])	Prec@1 82.031 (82.070)
Epoch: [21][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7756 (0.8520) ([0.500]+[0.276])	Prec@1 83.594 (81.557)
Epoch: [21][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8514 (0.8485) ([0.577]+[0.275])	Prec@1 81.250 (81.598)
Test: [0/79]	Time 0.209 (0.209)	Loss 1.1716 (1.1716) ([0.897]+[0.274])	Prec@1 74.219 (74.219)
 * Prec@1 70.410
current lr 1.00000e-01
Grad=  tensor(2.0039, device='cuda:0')
Epoch: [22][0/391]	Time 0.249 (0.249)	Data 0.217 (0.217)	Loss 0.8176 (0.8176) ([0.543]+[0.274])	Prec@1 84.375 (84.375)
Epoch: [22][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.9408 (0.8505) ([0.664]+[0.276])	Prec@1 74.219 (81.614)
Epoch: [22][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7661 (0.8497) ([0.490]+[0.276])	Prec@1 83.594 (81.518)
Epoch: [22][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8577 (0.8588) ([0.583]+[0.274])	Prec@1 76.562 (81.201)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.0627 (1.0627) ([0.790]+[0.273])	Prec@1 76.562 (76.562)
 * Prec@1 74.610
current lr 1.00000e-01
Grad=  tensor(3.1286, device='cuda:0')
Epoch: [23][0/391]	Time 0.256 (0.256)	Data 0.222 (0.222)	Loss 0.9992 (0.9992) ([0.726]+[0.273])	Prec@1 72.656 (72.656)
Epoch: [23][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.9256 (0.8397) ([0.653]+[0.272])	Prec@1 76.562 (81.784)
Epoch: [23][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6723 (0.8382) ([0.400]+[0.272])	Prec@1 83.594 (81.732)
Epoch: [23][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8090 (0.8357) ([0.536]+[0.273])	Prec@1 82.031 (81.901)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.1232 (1.1232) ([0.853]+[0.270])	Prec@1 72.656 (72.656)
 * Prec@1 74.550
current lr 1.00000e-01
Grad=  tensor(1.9906, device='cuda:0')
Epoch: [24][0/391]	Time 0.214 (0.214)	Data 0.182 (0.182)	Loss 0.7497 (0.7497) ([0.479]+[0.270])	Prec@1 84.375 (84.375)
Epoch: [24][100/391]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 0.9608 (0.8282) ([0.692]+[0.269])	Prec@1 78.125 (82.085)
Epoch: [24][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6104 (0.8318) ([0.342]+[0.269])	Prec@1 89.844 (82.012)
Epoch: [24][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6608 (0.8291) ([0.394]+[0.267])	Prec@1 85.938 (82.018)
Test: [0/79]	Time 0.221 (0.221)	Loss 1.5282 (1.5282) ([1.262]+[0.266])	Prec@1 67.969 (67.969)
 * Prec@1 65.340
current lr 1.00000e-01
Grad=  tensor(3.1347, device='cuda:0')
Epoch: [25][0/391]	Time 0.251 (0.251)	Data 0.219 (0.219)	Loss 1.0062 (1.0062) ([0.740]+[0.266])	Prec@1 71.875 (71.875)
Epoch: [25][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.8564 (0.8263) ([0.589]+[0.267])	Prec@1 80.469 (81.691)
Epoch: [25][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 1.0151 (0.8374) ([0.747]+[0.268])	Prec@1 81.250 (81.413)
Epoch: [25][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7561 (0.8365) ([0.489]+[0.267])	Prec@1 85.938 (81.580)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.3676 (1.3676) ([1.102]+[0.266])	Prec@1 64.844 (64.844)
 * Prec@1 69.860
current lr 1.00000e-01
Grad=  tensor(2.1352, device='cuda:0')
Epoch: [26][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.8264 (0.8264) ([0.560]+[0.266])	Prec@1 84.375 (84.375)
Epoch: [26][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7603 (0.8218) ([0.496]+[0.264])	Prec@1 82.812 (81.784)
Epoch: [26][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8068 (0.8197) ([0.542]+[0.264])	Prec@1 79.688 (81.946)
Epoch: [26][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6837 (0.8245) ([0.420]+[0.264])	Prec@1 82.812 (81.831)
Test: [0/79]	Time 0.175 (0.175)	Loss 1.0129 (1.0129) ([0.751]+[0.261])	Prec@1 78.125 (78.125)
 * Prec@1 75.590
current lr 1.00000e-01
Grad=  tensor(2.3755, device='cuda:0')
Epoch: [27][0/391]	Time 0.255 (0.255)	Data 0.221 (0.221)	Loss 0.7256 (0.7256) ([0.464]+[0.261])	Prec@1 85.938 (85.938)
Epoch: [27][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.8242 (0.8291) ([0.561]+[0.263])	Prec@1 84.375 (81.753)
Epoch: [27][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6576 (0.8279) ([0.395]+[0.262])	Prec@1 88.281 (81.919)
Epoch: [27][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7056 (0.8246) ([0.445]+[0.261])	Prec@1 84.375 (82.075)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.9477 (0.9477) ([0.688]+[0.259])	Prec@1 78.125 (78.125)
 * Prec@1 76.130
current lr 1.00000e-01
Grad=  tensor(1.7922, device='cuda:0')
Epoch: [28][0/391]	Time 0.217 (0.217)	Data 0.184 (0.184)	Loss 0.7675 (0.7675) ([0.508]+[0.259])	Prec@1 85.156 (85.156)
Epoch: [28][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.6487 (0.7911) ([0.390]+[0.259])	Prec@1 89.844 (82.843)
Epoch: [28][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8392 (0.8017) ([0.580]+[0.259])	Prec@1 80.469 (82.591)
Epoch: [28][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8264 (0.8079) ([0.568]+[0.258])	Prec@1 82.031 (82.392)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.8511 (0.8511) ([0.593]+[0.258])	Prec@1 78.125 (78.125)
 * Prec@1 78.270
current lr 1.00000e-01
Grad=  tensor(1.4745, device='cuda:0')
Epoch: [29][0/391]	Time 0.230 (0.230)	Data 0.200 (0.200)	Loss 0.6047 (0.6047) ([0.347]+[0.258])	Prec@1 88.281 (88.281)
Epoch: [29][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7768 (0.8075) ([0.521]+[0.256])	Prec@1 85.938 (82.232)
Epoch: [29][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.6879 (0.8055) ([0.433]+[0.255])	Prec@1 85.156 (82.257)
Epoch: [29][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7971 (0.8125) ([0.540]+[0.257])	Prec@1 82.031 (82.125)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.8096 (0.8096) ([0.556]+[0.254])	Prec@1 80.469 (80.469)
 * Prec@1 80.350
current lr 1.00000e-01
Grad=  tensor(1.3943, device='cuda:0')
Epoch: [30][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.7431 (0.7431) ([0.489]+[0.254])	Prec@1 85.156 (85.156)
Epoch: [30][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.8031 (0.8026) ([0.549]+[0.254])	Prec@1 81.250 (82.271)
Epoch: [30][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8795 (0.8048) ([0.626]+[0.253])	Prec@1 82.031 (82.276)
Epoch: [30][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.8555 (0.8028) ([0.604]+[0.252])	Prec@1 78.906 (82.488)
Test: [0/79]	Time 0.198 (0.198)	Loss 1.2722 (1.2722) ([1.021]+[0.251])	Prec@1 73.438 (73.438)
 * Prec@1 72.630
current lr 1.00000e-01
Grad=  tensor(2.9219, device='cuda:0')
Epoch: [31][0/391]	Time 0.211 (0.211)	Data 0.180 (0.180)	Loss 0.8118 (0.8118) ([0.561]+[0.251])	Prec@1 82.812 (82.812)
Epoch: [31][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.6871 (0.8031) ([0.437]+[0.250])	Prec@1 84.375 (82.124)
Epoch: [31][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7164 (0.7871) ([0.468]+[0.248])	Prec@1 83.594 (82.548)
Epoch: [31][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9390 (0.7905) ([0.691]+[0.248])	Prec@1 81.250 (82.478)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.9871 (0.9871) ([0.741]+[0.246])	Prec@1 76.562 (76.562)
 * Prec@1 75.330
current lr 1.00000e-01
Grad=  tensor(1.6995, device='cuda:0')
Epoch: [32][0/391]	Time 0.219 (0.219)	Data 0.189 (0.189)	Loss 0.7079 (0.7079) ([0.462]+[0.246])	Prec@1 85.156 (85.156)
Epoch: [32][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7304 (0.7700) ([0.484]+[0.246])	Prec@1 85.938 (82.882)
Epoch: [32][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7760 (0.7763) ([0.530]+[0.246])	Prec@1 83.594 (82.746)
Epoch: [32][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7909 (0.7758) ([0.546]+[0.245])	Prec@1 80.469 (82.846)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.0554 (1.0554) ([0.810]+[0.246])	Prec@1 76.562 (76.562)
 * Prec@1 72.850
current lr 1.00000e-01
Grad=  tensor(2.1209, device='cuda:0')
Epoch: [33][0/391]	Time 0.256 (0.256)	Data 0.223 (0.223)	Loss 0.7110 (0.7110) ([0.465]+[0.246])	Prec@1 85.156 (85.156)
Epoch: [33][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7386 (0.7742) ([0.494]+[0.245])	Prec@1 84.375 (83.029)
Epoch: [33][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8590 (0.7759) ([0.613]+[0.246])	Prec@1 80.469 (82.801)
Epoch: [33][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8445 (0.7818) ([0.599]+[0.245])	Prec@1 78.906 (82.626)
Test: [0/79]	Time 0.189 (0.189)	Loss 1.3524 (1.3524) ([1.109]+[0.244])	Prec@1 67.188 (67.188)
 * Prec@1 69.330
current lr 1.00000e-01
Grad=  tensor(2.4102, device='cuda:0')
Epoch: [34][0/391]	Time 0.227 (0.227)	Data 0.199 (0.199)	Loss 0.7330 (0.7330) ([0.490]+[0.244])	Prec@1 81.250 (81.250)
Epoch: [34][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.6821 (0.7836) ([0.438]+[0.244])	Prec@1 86.719 (82.611)
Epoch: [34][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7809 (0.7795) ([0.538]+[0.243])	Prec@1 83.594 (82.754)
Epoch: [34][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.7229 (0.7786) ([0.480]+[0.243])	Prec@1 84.375 (82.776)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.7991 (0.7991) ([0.557]+[0.243])	Prec@1 83.594 (83.594)
 * Prec@1 79.510
current lr 1.00000e-01
Grad=  tensor(2.8693, device='cuda:0')
Epoch: [35][0/391]	Time 0.253 (0.253)	Data 0.221 (0.221)	Loss 0.7856 (0.7856) ([0.543]+[0.243])	Prec@1 84.375 (84.375)
Epoch: [35][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.7679 (0.7705) ([0.525]+[0.243])	Prec@1 81.250 (83.014)
Epoch: [35][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6195 (0.7659) ([0.378]+[0.242])	Prec@1 89.844 (83.302)
Epoch: [35][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7408 (0.7700) ([0.500]+[0.241])	Prec@1 83.594 (83.140)
Test: [0/79]	Time 0.190 (0.190)	Loss 1.6773 (1.6773) ([1.436]+[0.241])	Prec@1 59.375 (59.375)
 * Prec@1 60.040
current lr 1.00000e-01
Grad=  tensor(3.4884, device='cuda:0')
Epoch: [36][0/391]	Time 0.220 (0.220)	Data 0.191 (0.191)	Loss 0.8589 (0.8589) ([0.618]+[0.241])	Prec@1 75.000 (75.000)
Epoch: [36][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.7700 (0.7692) ([0.529]+[0.241])	Prec@1 85.156 (83.099)
Epoch: [36][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6684 (0.7760) ([0.428]+[0.240])	Prec@1 83.594 (82.789)
Epoch: [36][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9002 (0.7723) ([0.661]+[0.239])	Prec@1 79.688 (82.859)
Test: [0/79]	Time 0.193 (0.193)	Loss 1.1207 (1.1207) ([0.882]+[0.239])	Prec@1 71.094 (71.094)
 * Prec@1 73.340
current lr 1.00000e-01
Grad=  tensor(1.9847, device='cuda:0')
Epoch: [37][0/391]	Time 0.227 (0.227)	Data 0.197 (0.197)	Loss 0.7687 (0.7687) ([0.530]+[0.239])	Prec@1 81.250 (81.250)
Epoch: [37][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.8076 (0.7590) ([0.569]+[0.239])	Prec@1 81.250 (83.308)
Epoch: [37][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.6529 (0.7665) ([0.414]+[0.239])	Prec@1 86.719 (83.073)
Epoch: [37][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.6446 (0.7746) ([0.406]+[0.238])	Prec@1 82.031 (82.623)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.9964 (0.9964) ([0.759]+[0.238])	Prec@1 78.906 (78.906)
 * Prec@1 76.410
current lr 1.00000e-01
Grad=  tensor(2.3744, device='cuda:0')
Epoch: [38][0/391]	Time 0.248 (0.248)	Data 0.214 (0.214)	Loss 0.7493 (0.7493) ([0.511]+[0.238])	Prec@1 82.031 (82.031)
Epoch: [38][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.7993 (0.7598) ([0.561]+[0.238])	Prec@1 80.469 (83.192)
Epoch: [38][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.5648 (0.7407) ([0.328]+[0.237])	Prec@1 91.406 (83.714)
Epoch: [38][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7056 (0.7477) ([0.469]+[0.236])	Prec@1 86.719 (83.508)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.8862 (0.8862) ([0.649]+[0.237])	Prec@1 74.219 (74.219)
 * Prec@1 76.470
current lr 1.00000e-01
Grad=  tensor(1.7483, device='cuda:0')
Epoch: [39][0/391]	Time 0.217 (0.217)	Data 0.187 (0.187)	Loss 0.7849 (0.7849) ([0.547]+[0.237])	Prec@1 81.250 (81.250)
Epoch: [39][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.6466 (0.7588) ([0.408]+[0.238])	Prec@1 88.281 (83.338)
Epoch: [39][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.7571 (0.7595) ([0.519]+[0.238])	Prec@1 83.594 (83.364)
Epoch: [39][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9037 (0.7694) ([0.666]+[0.238])	Prec@1 80.469 (82.960)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.7515 (0.7515) ([0.515]+[0.237])	Prec@1 82.031 (82.031)
 * Prec@1 81.010
current lr 1.00000e-01
Grad=  tensor(1.4066, device='cuda:0')
Epoch: [40][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.6207 (0.6207) ([0.384]+[0.237])	Prec@1 88.281 (88.281)
Epoch: [40][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8942 (0.7592) ([0.655]+[0.239])	Prec@1 80.469 (83.184)
Epoch: [40][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.8397 (0.7559) ([0.602]+[0.237])	Prec@1 80.469 (83.252)
Epoch: [40][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7404 (0.7594) ([0.503]+[0.237])	Prec@1 78.125 (83.075)
Test: [0/79]	Time 0.225 (0.225)	Loss 1.1507 (1.1507) ([0.914]+[0.236])	Prec@1 68.750 (68.750)
 * Prec@1 71.340
current lr 1.00000e-01
Grad=  tensor(2.8871, device='cuda:0')
Epoch: [41][0/391]	Time 0.242 (0.242)	Data 0.210 (0.210)	Loss 0.7988 (0.7988) ([0.562]+[0.236])	Prec@1 82.031 (82.031)
Epoch: [41][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.6966 (0.7463) ([0.460]+[0.236])	Prec@1 87.500 (83.308)
Epoch: [41][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7122 (0.7542) ([0.476]+[0.236])	Prec@1 87.500 (83.244)
Epoch: [41][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8837 (0.7519) ([0.648]+[0.236])	Prec@1 84.375 (83.334)
Test: [0/79]	Time 0.186 (0.186)	Loss 1.1377 (1.1377) ([0.903]+[0.235])	Prec@1 75.000 (75.000)
 * Prec@1 73.750
current lr 1.00000e-01
Grad=  tensor(1.5700, device='cuda:0')
Epoch: [42][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.6740 (0.6740) ([0.439]+[0.235])	Prec@1 82.812 (82.812)
Epoch: [42][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.6828 (0.7353) ([0.448]+[0.235])	Prec@1 82.812 (83.532)
Epoch: [42][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9160 (0.7463) ([0.681]+[0.235])	Prec@1 77.344 (83.489)
Epoch: [42][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7401 (0.7434) ([0.506]+[0.234])	Prec@1 84.375 (83.464)
Test: [0/79]	Time 0.215 (0.215)	Loss 1.2496 (1.2496) ([1.015]+[0.234])	Prec@1 71.875 (71.875)
 * Prec@1 70.320
current lr 1.00000e-01
Grad=  tensor(3.1429, device='cuda:0')
Epoch: [43][0/391]	Time 0.229 (0.229)	Data 0.197 (0.197)	Loss 0.9267 (0.9267) ([0.692]+[0.234])	Prec@1 75.000 (75.000)
Epoch: [43][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6883 (0.7550) ([0.454]+[0.234])	Prec@1 80.469 (83.199)
Epoch: [43][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7857 (0.7445) ([0.553]+[0.233])	Prec@1 83.594 (83.773)
Epoch: [43][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7669 (0.7507) ([0.534]+[0.233])	Prec@1 82.812 (83.490)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.7943 (0.7943) ([0.561]+[0.233])	Prec@1 82.812 (82.812)
 * Prec@1 76.890
current lr 1.00000e-01
Grad=  tensor(1.5004, device='cuda:0')
Epoch: [44][0/391]	Time 0.213 (0.213)	Data 0.182 (0.182)	Loss 0.6885 (0.6885) ([0.455]+[0.233])	Prec@1 83.594 (83.594)
Epoch: [44][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8060 (0.7476) ([0.572]+[0.234])	Prec@1 82.812 (83.377)
Epoch: [44][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7692 (0.7377) ([0.536]+[0.234])	Prec@1 82.031 (83.629)
Epoch: [44][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8754 (0.7510) ([0.642]+[0.233])	Prec@1 77.344 (83.223)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.9041 (0.9041) ([0.671]+[0.233])	Prec@1 76.562 (76.562)
 * Prec@1 79.050
current lr 1.00000e-01
Grad=  tensor(1.1004, device='cuda:0')
Epoch: [45][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.5597 (0.5597) ([0.327]+[0.233])	Prec@1 91.406 (91.406)
Epoch: [45][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8019 (0.7198) ([0.570]+[0.232])	Prec@1 83.594 (83.988)
Epoch: [45][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7014 (0.7296) ([0.471]+[0.230])	Prec@1 83.594 (83.827)
Epoch: [45][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7274 (0.7344) ([0.497]+[0.230])	Prec@1 85.938 (83.661)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.9858 (0.9858) ([0.756]+[0.230])	Prec@1 76.562 (76.562)
 * Prec@1 76.580
current lr 1.00000e-01
Grad=  tensor(1.7872, device='cuda:0')
Epoch: [46][0/391]	Time 0.203 (0.203)	Data 0.174 (0.174)	Loss 0.6671 (0.6671) ([0.437]+[0.230])	Prec@1 86.719 (86.719)
Epoch: [46][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.8722 (0.7321) ([0.642]+[0.230])	Prec@1 78.125 (83.834)
Epoch: [46][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8764 (0.7267) ([0.647]+[0.229])	Prec@1 78.906 (83.979)
Epoch: [46][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6156 (0.7392) ([0.385]+[0.230])	Prec@1 87.500 (83.560)
Test: [0/79]	Time 0.205 (0.205)	Loss 1.1368 (1.1368) ([0.907]+[0.229])	Prec@1 74.219 (74.219)
 * Prec@1 75.220
current lr 1.00000e-01
Grad=  tensor(2.1528, device='cuda:0')
Epoch: [47][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.7532 (0.7532) ([0.524]+[0.229])	Prec@1 80.469 (80.469)
Epoch: [47][100/391]	Time 0.028 (0.027)	Data 0.000 (0.002)	Loss 0.8337 (0.7225) ([0.605]+[0.229])	Prec@1 78.906 (83.772)
Epoch: [47][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5759 (0.7393) ([0.347]+[0.229])	Prec@1 85.156 (83.333)
Epoch: [47][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6246 (0.7385) ([0.396]+[0.229])	Prec@1 87.500 (83.472)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.9852 (0.9852) ([0.757]+[0.228])	Prec@1 78.906 (78.906)
 * Prec@1 78.290
current lr 1.00000e-01
Grad=  tensor(2.6413, device='cuda:0')
Epoch: [48][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.6931 (0.6931) ([0.465]+[0.228])	Prec@1 89.062 (89.062)
Epoch: [48][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7422 (0.7023) ([0.514]+[0.228])	Prec@1 85.938 (84.677)
Epoch: [48][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7301 (0.7142) ([0.502]+[0.228])	Prec@1 85.156 (84.426)
Epoch: [48][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7512 (0.7272) ([0.522]+[0.229])	Prec@1 82.031 (83.973)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.9448 (0.9448) ([0.716]+[0.229])	Prec@1 76.562 (76.562)
 * Prec@1 77.350
current lr 1.00000e-01
Grad=  tensor(2.0155, device='cuda:0')
Epoch: [49][0/391]	Time 0.258 (0.258)	Data 0.226 (0.226)	Loss 0.6353 (0.6353) ([0.406]+[0.229])	Prec@1 86.719 (86.719)
Epoch: [49][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7642 (0.7320) ([0.535]+[0.229])	Prec@1 80.469 (83.648)
Epoch: [49][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7524 (0.7337) ([0.524]+[0.229])	Prec@1 83.594 (83.555)
Epoch: [49][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.6536 (0.7337) ([0.425]+[0.229])	Prec@1 88.281 (83.635)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.0932 (1.0932) ([0.865]+[0.228])	Prec@1 70.312 (70.312)
 * Prec@1 75.110
current lr 1.00000e-01
Grad=  tensor(1.9220, device='cuda:0')
Epoch: [50][0/391]	Time 0.245 (0.245)	Data 0.215 (0.215)	Loss 0.7102 (0.7102) ([0.482]+[0.228])	Prec@1 85.156 (85.156)
Epoch: [50][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7201 (0.7267) ([0.491]+[0.229])	Prec@1 85.938 (83.640)
Epoch: [50][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7487 (0.7223) ([0.521]+[0.228])	Prec@1 81.250 (83.846)
Epoch: [50][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6530 (0.7258) ([0.425]+[0.228])	Prec@1 86.719 (83.846)
Test: [0/79]	Time 0.174 (0.174)	Loss 1.0348 (1.0348) ([0.807]+[0.227])	Prec@1 75.000 (75.000)
 * Prec@1 73.150
current lr 1.00000e-01
Grad=  tensor(2.1249, device='cuda:0')
Epoch: [51][0/391]	Time 0.200 (0.200)	Data 0.171 (0.171)	Loss 0.8678 (0.8678) ([0.640]+[0.227])	Prec@1 82.031 (82.031)
Epoch: [51][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.7265 (0.7383) ([0.498]+[0.228])	Prec@1 84.375 (83.369)
Epoch: [51][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7551 (0.7319) ([0.528]+[0.228])	Prec@1 82.812 (83.753)
Epoch: [51][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.7394 (0.7354) ([0.511]+[0.229])	Prec@1 80.469 (83.487)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.8824 (0.8824) ([0.654]+[0.228])	Prec@1 79.688 (79.688)
 * Prec@1 75.930
current lr 1.00000e-01
Grad=  tensor(2.1745, device='cuda:0')
Epoch: [52][0/391]	Time 0.261 (0.261)	Data 0.228 (0.228)	Loss 0.6494 (0.6494) ([0.421]+[0.228])	Prec@1 86.719 (86.719)
Epoch: [52][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7746 (0.7402) ([0.546]+[0.229])	Prec@1 80.469 (83.346)
Epoch: [52][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7348 (0.7461) ([0.506]+[0.229])	Prec@1 87.500 (83.279)
Epoch: [52][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.7499 (0.7395) ([0.522]+[0.228])	Prec@1 84.375 (83.550)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.9521 (0.9521) ([0.724]+[0.228])	Prec@1 79.688 (79.688)
 * Prec@1 73.870
current lr 1.00000e-01
Grad=  tensor(2.6404, device='cuda:0')
Epoch: [53][0/391]	Time 0.223 (0.223)	Data 0.193 (0.193)	Loss 0.7015 (0.7015) ([0.473]+[0.228])	Prec@1 83.594 (83.594)
Epoch: [53][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7870 (0.7382) ([0.558]+[0.230])	Prec@1 80.469 (83.455)
Epoch: [53][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5642 (0.7375) ([0.335]+[0.229])	Prec@1 89.844 (83.594)
Epoch: [53][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.8264 (0.7384) ([0.598]+[0.228])	Prec@1 77.344 (83.474)
Test: [0/79]	Time 0.190 (0.190)	Loss 1.1163 (1.1163) ([0.888]+[0.228])	Prec@1 72.656 (72.656)
 * Prec@1 72.830
current lr 1.00000e-01
Grad=  tensor(2.4659, device='cuda:0')
Epoch: [54][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.6404 (0.6404) ([0.412]+[0.228])	Prec@1 85.938 (85.938)
Epoch: [54][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.8582 (0.7189) ([0.630]+[0.228])	Prec@1 77.344 (83.926)
Epoch: [54][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.6731 (0.7242) ([0.445]+[0.228])	Prec@1 82.031 (83.780)
Epoch: [54][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7502 (0.7301) ([0.522]+[0.228])	Prec@1 81.250 (83.518)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.8729 (0.8729) ([0.645]+[0.228])	Prec@1 78.906 (78.906)
 * Prec@1 79.860
current lr 1.00000e-01
Grad=  tensor(1.7064, device='cuda:0')
Epoch: [55][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.6220 (0.6220) ([0.394]+[0.228])	Prec@1 89.062 (89.062)
Epoch: [55][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.6717 (0.7234) ([0.444]+[0.228])	Prec@1 87.500 (83.981)
Epoch: [55][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.6979 (0.7215) ([0.470]+[0.227])	Prec@1 82.812 (83.924)
Epoch: [55][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.8164 (0.7212) ([0.590]+[0.227])	Prec@1 78.906 (83.910)
Test: [0/79]	Time 0.231 (0.231)	Loss 1.0135 (1.0135) ([0.787]+[0.226])	Prec@1 73.438 (73.438)
 * Prec@1 72.750
current lr 1.00000e-01
Grad=  tensor(1.9897, device='cuda:0')
Epoch: [56][0/391]	Time 0.252 (0.252)	Data 0.220 (0.220)	Loss 0.6260 (0.6260) ([0.400]+[0.226])	Prec@1 89.062 (89.062)
Epoch: [56][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.7339 (0.7205) ([0.507]+[0.227])	Prec@1 81.250 (83.926)
Epoch: [56][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6512 (0.7230) ([0.424]+[0.228])	Prec@1 85.156 (83.776)
Epoch: [56][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7486 (0.7242) ([0.521]+[0.228])	Prec@1 83.594 (83.778)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.8929 (0.8929) ([0.665]+[0.228])	Prec@1 79.688 (79.688)
 * Prec@1 77.960
current lr 1.00000e-01
Grad=  tensor(2.6567, device='cuda:0')
Epoch: [57][0/391]	Time 0.238 (0.238)	Data 0.208 (0.208)	Loss 0.8003 (0.8003) ([0.572]+[0.228])	Prec@1 81.250 (81.250)
Epoch: [57][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8984 (0.7235) ([0.671]+[0.227])	Prec@1 74.219 (83.849)
Epoch: [57][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7961 (0.7227) ([0.568]+[0.228])	Prec@1 81.250 (83.819)
Epoch: [57][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.8888 (0.7312) ([0.661]+[0.228])	Prec@1 81.250 (83.742)
Test: [0/79]	Time 0.229 (0.229)	Loss 1.0279 (1.0279) ([0.801]+[0.227])	Prec@1 75.000 (75.000)
 * Prec@1 72.920
current lr 1.00000e-01
Grad=  tensor(3.8045, device='cuda:0')
Epoch: [58][0/391]	Time 0.240 (0.240)	Data 0.204 (0.204)	Loss 0.9110 (0.9110) ([0.684]+[0.227])	Prec@1 78.906 (78.906)
Epoch: [58][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.6608 (0.7105) ([0.434]+[0.226])	Prec@1 85.156 (84.011)
Epoch: [58][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.8395 (0.7151) ([0.614]+[0.226])	Prec@1 78.125 (83.955)
Epoch: [58][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8498 (0.7245) ([0.624]+[0.226])	Prec@1 80.469 (83.848)
Test: [0/79]	Time 0.194 (0.194)	Loss 1.1726 (1.1726) ([0.947]+[0.225])	Prec@1 72.656 (72.656)
 * Prec@1 69.140
current lr 1.00000e-01
Grad=  tensor(2.4219, device='cuda:0')
Epoch: [59][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.8075 (0.8075) ([0.582]+[0.225])	Prec@1 84.375 (84.375)
Epoch: [59][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7696 (0.7324) ([0.543]+[0.226])	Prec@1 81.250 (83.671)
Epoch: [59][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7146 (0.7219) ([0.489]+[0.226])	Prec@1 82.812 (83.959)
Epoch: [59][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7370 (0.7272) ([0.511]+[0.226])	Prec@1 82.812 (83.755)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.8477 (0.8477) ([0.621]+[0.226])	Prec@1 81.250 (81.250)
 * Prec@1 78.800
current lr 1.00000e-01
Grad=  tensor(2.3007, device='cuda:0')
Epoch: [60][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.6897 (0.6897) ([0.463]+[0.226])	Prec@1 85.156 (85.156)
Epoch: [60][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.9605 (0.7058) ([0.733]+[0.227])	Prec@1 81.250 (84.452)
Epoch: [60][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7136 (0.7245) ([0.486]+[0.228])	Prec@1 84.375 (83.839)
Epoch: [60][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7207 (0.7194) ([0.495]+[0.226])	Prec@1 78.906 (84.025)
Test: [0/79]	Time 0.201 (0.201)	Loss 1.0890 (1.0890) ([0.863]+[0.226])	Prec@1 73.438 (73.438)
 * Prec@1 75.930
current lr 1.00000e-01
Grad=  tensor(1.5181, device='cuda:0')
Epoch: [61][0/391]	Time 0.210 (0.210)	Data 0.181 (0.181)	Loss 0.5965 (0.5965) ([0.370]+[0.226])	Prec@1 88.281 (88.281)
Epoch: [61][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.6968 (0.6947) ([0.471]+[0.226])	Prec@1 81.250 (84.669)
Epoch: [61][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.6239 (0.7199) ([0.398]+[0.226])	Prec@1 88.281 (83.881)
Epoch: [61][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.6879 (0.7212) ([0.462]+[0.226])	Prec@1 83.594 (83.929)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.8672 (0.8672) ([0.642]+[0.225])	Prec@1 82.031 (82.031)
 * Prec@1 78.510
current lr 1.00000e-01
Grad=  tensor(1.5355, device='cuda:0')
Epoch: [62][0/391]	Time 0.241 (0.241)	Data 0.208 (0.208)	Loss 0.6573 (0.6573) ([0.432]+[0.225])	Prec@1 85.938 (85.938)
Epoch: [62][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.7661 (0.7083) ([0.540]+[0.226])	Prec@1 79.688 (84.236)
Epoch: [62][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6840 (0.7178) ([0.458]+[0.226])	Prec@1 85.938 (84.045)
Epoch: [62][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.9986 (0.7248) ([0.773]+[0.226])	Prec@1 75.000 (83.830)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.7549 (0.7549) ([0.529]+[0.226])	Prec@1 85.156 (85.156)
 * Prec@1 80.800
current lr 1.00000e-01
Grad=  tensor(2.2167, device='cuda:0')
Epoch: [63][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.7700 (0.7700) ([0.544]+[0.226])	Prec@1 82.031 (82.031)
Epoch: [63][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.6642 (0.7272) ([0.437]+[0.227])	Prec@1 88.281 (83.787)
Epoch: [63][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.9249 (0.7207) ([0.698]+[0.227])	Prec@1 80.469 (84.060)
Epoch: [63][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8240 (0.7231) ([0.598]+[0.226])	Prec@1 78.125 (83.861)
Test: [0/79]	Time 0.194 (0.194)	Loss 1.1684 (1.1684) ([0.943]+[0.225])	Prec@1 75.000 (75.000)
 * Prec@1 73.440
current lr 1.00000e-01
Grad=  tensor(1.8417, device='cuda:0')
Epoch: [64][0/391]	Time 0.217 (0.217)	Data 0.186 (0.186)	Loss 0.6795 (0.6795) ([0.454]+[0.225])	Prec@1 85.938 (85.938)
Epoch: [64][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6709 (0.6932) ([0.446]+[0.225])	Prec@1 83.594 (84.947)
Epoch: [64][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8076 (0.7148) ([0.582]+[0.226])	Prec@1 78.125 (84.262)
Epoch: [64][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.5350 (0.7168) ([0.309]+[0.226])	Prec@1 89.062 (84.121)
Test: [0/79]	Time 0.228 (0.228)	Loss 1.1119 (1.1119) ([0.887]+[0.225])	Prec@1 71.094 (71.094)
 * Prec@1 69.980
current lr 1.00000e-01
Grad=  tensor(1.6399, device='cuda:0')
Epoch: [65][0/391]	Time 0.256 (0.256)	Data 0.223 (0.223)	Loss 0.6636 (0.6636) ([0.438]+[0.225])	Prec@1 85.156 (85.156)
Epoch: [65][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.8106 (0.7193) ([0.585]+[0.225])	Prec@1 80.469 (83.911)
Epoch: [65][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.7662 (0.7165) ([0.541]+[0.225])	Prec@1 81.250 (84.099)
Epoch: [65][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.7790 (0.7156) ([0.555]+[0.224])	Prec@1 82.031 (84.136)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.8410 (0.8410) ([0.617]+[0.224])	Prec@1 75.000 (75.000)
 * Prec@1 77.270
current lr 1.00000e-01
Grad=  tensor(1.7571, device='cuda:0')
Epoch: [66][0/391]	Time 0.245 (0.245)	Data 0.215 (0.215)	Loss 0.6449 (0.6449) ([0.421]+[0.224])	Prec@1 85.938 (85.938)
Epoch: [66][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7601 (0.7120) ([0.535]+[0.225])	Prec@1 83.594 (84.066)
Epoch: [66][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8766 (0.7107) ([0.652]+[0.225])	Prec@1 78.125 (84.049)
Epoch: [66][300/391]	Time 0.030 (0.026)	Data 0.000 (0.001)	Loss 0.9926 (0.7203) ([0.767]+[0.226])	Prec@1 75.781 (83.757)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.6821 (0.6821) ([0.457]+[0.225])	Prec@1 83.594 (83.594)
 * Prec@1 83.000
current lr 1.00000e-01
Grad=  tensor(1.5642, device='cuda:0')
Epoch: [67][0/391]	Time 0.222 (0.222)	Data 0.189 (0.189)	Loss 0.5432 (0.5432) ([0.318]+[0.225])	Prec@1 89.062 (89.062)
Epoch: [67][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8783 (0.7126) ([0.653]+[0.225])	Prec@1 78.125 (83.934)
Epoch: [67][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6133 (0.7308) ([0.387]+[0.226])	Prec@1 89.062 (83.516)
Epoch: [67][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8316 (0.7297) ([0.606]+[0.225])	Prec@1 81.250 (83.646)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.8447 (0.8447) ([0.620]+[0.224])	Prec@1 76.562 (76.562)
 * Prec@1 78.020
current lr 1.00000e-01
Grad=  tensor(2.2818, device='cuda:0')
Epoch: [68][0/391]	Time 0.210 (0.210)	Data 0.178 (0.178)	Loss 0.6599 (0.6599) ([0.435]+[0.224])	Prec@1 82.031 (82.031)
Epoch: [68][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7133 (0.7019) ([0.488]+[0.225])	Prec@1 82.812 (84.862)
Epoch: [68][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7878 (0.7061) ([0.563]+[0.225])	Prec@1 83.594 (84.721)
Epoch: [68][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7758 (0.7161) ([0.551]+[0.225])	Prec@1 78.125 (84.131)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.7845 (0.7845) ([0.559]+[0.225])	Prec@1 82.812 (82.812)
 * Prec@1 80.570
current lr 1.00000e-01
Grad=  tensor(2.3038, device='cuda:0')
Epoch: [69][0/391]	Time 0.213 (0.213)	Data 0.185 (0.185)	Loss 0.8035 (0.8035) ([0.578]+[0.225])	Prec@1 83.594 (83.594)
Epoch: [69][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.7050 (0.7184) ([0.480]+[0.225])	Prec@1 83.594 (84.104)
Epoch: [69][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.7720 (0.7148) ([0.546]+[0.226])	Prec@1 80.469 (84.239)
Epoch: [69][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.5549 (0.7180) ([0.330]+[0.225])	Prec@1 85.938 (84.147)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.0110 (1.0110) ([0.786]+[0.225])	Prec@1 73.438 (73.438)
 * Prec@1 77.720
current lr 1.00000e-01
Grad=  tensor(2.5952, device='cuda:0')
Epoch: [70][0/391]	Time 0.215 (0.215)	Data 0.176 (0.176)	Loss 0.7270 (0.7270) ([0.502]+[0.225])	Prec@1 82.031 (82.031)
Epoch: [70][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.8742 (0.7082) ([0.649]+[0.226])	Prec@1 77.344 (84.414)
Epoch: [70][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6904 (0.7144) ([0.465]+[0.226])	Prec@1 85.156 (84.146)
Epoch: [70][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7419 (0.7213) ([0.515]+[0.227])	Prec@1 81.250 (83.936)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.8436 (0.8436) ([0.618]+[0.225])	Prec@1 81.250 (81.250)
 * Prec@1 78.040
current lr 1.00000e-01
Grad=  tensor(2.8064, device='cuda:0')
Epoch: [71][0/391]	Time 0.213 (0.213)	Data 0.182 (0.182)	Loss 0.7974 (0.7974) ([0.572]+[0.225])	Prec@1 82.031 (82.031)
Epoch: [71][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7447 (0.7274) ([0.519]+[0.226])	Prec@1 81.250 (83.756)
Epoch: [71][200/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.7426 (0.7245) ([0.516]+[0.227])	Prec@1 85.938 (83.590)
Epoch: [71][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6031 (0.7277) ([0.376]+[0.227])	Prec@1 90.625 (83.503)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.9259 (0.9259) ([0.700]+[0.226])	Prec@1 78.125 (78.125)
 * Prec@1 77.770
current lr 1.00000e-01
Grad=  tensor(2.9635, device='cuda:0')
Epoch: [72][0/391]	Time 0.262 (0.262)	Data 0.229 (0.229)	Loss 0.6646 (0.6646) ([0.439]+[0.226])	Prec@1 82.812 (82.812)
Epoch: [72][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.6215 (0.6959) ([0.397]+[0.225])	Prec@1 89.062 (84.847)
Epoch: [72][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.7603 (0.7072) ([0.535]+[0.225])	Prec@1 83.594 (84.321)
Epoch: [72][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8518 (0.7172) ([0.627]+[0.225])	Prec@1 77.344 (83.996)
Test: [0/79]	Time 0.199 (0.199)	Loss 1.6292 (1.6292) ([1.405]+[0.224])	Prec@1 60.938 (60.938)
 * Prec@1 61.180
current lr 1.00000e-01
Grad=  tensor(1.6680, device='cuda:0')
Epoch: [73][0/391]	Time 0.229 (0.229)	Data 0.192 (0.192)	Loss 0.5773 (0.5773) ([0.353]+[0.224])	Prec@1 89.062 (89.062)
Epoch: [73][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7344 (0.7086) ([0.510]+[0.225])	Prec@1 80.469 (84.530)
Epoch: [73][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.7421 (0.7099) ([0.518]+[0.224])	Prec@1 85.156 (84.391)
Epoch: [73][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.5726 (0.7116) ([0.349]+[0.224])	Prec@1 90.625 (84.354)
Test: [0/79]	Time 0.178 (0.178)	Loss 1.0991 (1.0991) ([0.876]+[0.223])	Prec@1 75.000 (75.000)
 * Prec@1 73.000
current lr 1.00000e-01
Grad=  tensor(2.5882, device='cuda:0')
Epoch: [74][0/391]	Time 0.260 (0.260)	Data 0.228 (0.228)	Loss 0.7879 (0.7879) ([0.565]+[0.223])	Prec@1 82.031 (82.031)
Epoch: [74][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.8237 (0.7083) ([0.600]+[0.224])	Prec@1 83.594 (84.174)
Epoch: [74][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.6551 (0.7170) ([0.431]+[0.224])	Prec@1 85.156 (84.095)
Epoch: [74][300/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.7150 (0.7138) ([0.492]+[0.223])	Prec@1 79.688 (84.211)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.8332 (0.8332) ([0.611]+[0.222])	Prec@1 79.688 (79.688)
 * Prec@1 75.750
current lr 1.00000e-02
Grad=  tensor(2.7158, device='cuda:0')
Epoch: [75][0/391]	Time 0.206 (0.206)	Data 0.174 (0.174)	Loss 0.7637 (0.7637) ([0.541]+[0.222])	Prec@1 81.250 (81.250)
Epoch: [75][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.4048 (0.5401) ([0.205]+[0.200])	Prec@1 94.531 (89.132)
Epoch: [75][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5464 (0.5082) ([0.348]+[0.198])	Prec@1 87.500 (89.999)
Epoch: [75][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5023 (0.4954) ([0.305]+[0.197])	Prec@1 87.500 (90.410)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.4159 (0.4159) ([0.221]+[0.195])	Prec@1 93.750 (93.750)
 * Prec@1 90.840
current lr 1.00000e-02
Grad=  tensor(0.9230, device='cuda:0')
Epoch: [76][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.3659 (0.3659) ([0.171]+[0.195])	Prec@1 95.312 (95.312)
Epoch: [76][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3520 (0.4289) ([0.158]+[0.194])	Prec@1 92.969 (92.141)
Epoch: [76][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.4089 (0.4198) ([0.217]+[0.192])	Prec@1 92.969 (92.475)
Epoch: [76][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3757 (0.4225) ([0.185]+[0.191])	Prec@1 92.969 (92.372)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.3766 (0.3766) ([0.187]+[0.189])	Prec@1 92.969 (92.969)
 * Prec@1 91.320
current lr 1.00000e-02
Grad=  tensor(1.1023, device='cuda:0')
Epoch: [77][0/391]	Time 0.264 (0.264)	Data 0.230 (0.230)	Loss 0.3702 (0.3702) ([0.181]+[0.189])	Prec@1 92.969 (92.969)
Epoch: [77][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.4746 (0.3939) ([0.287]+[0.188])	Prec@1 88.281 (93.201)
Epoch: [77][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.4814 (0.3993) ([0.295]+[0.187])	Prec@1 88.281 (92.969)
Epoch: [77][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3953 (0.3924) ([0.210]+[0.185])	Prec@1 91.406 (93.223)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.3718 (0.3718) ([0.188]+[0.184])	Prec@1 94.531 (94.531)
 * Prec@1 91.380
current lr 1.00000e-02
Grad=  tensor(2.2089, device='cuda:0')
Epoch: [78][0/391]	Time 0.243 (0.243)	Data 0.210 (0.210)	Loss 0.4248 (0.4248) ([0.241]+[0.184])	Prec@1 91.406 (91.406)
Epoch: [78][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3482 (0.3658) ([0.166]+[0.183])	Prec@1 94.531 (94.052)
Epoch: [78][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3573 (0.3700) ([0.176]+[0.181])	Prec@1 95.312 (93.766)
Epoch: [78][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3240 (0.3730) ([0.144]+[0.180])	Prec@1 93.750 (93.649)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.3374 (0.3374) ([0.159]+[0.179])	Prec@1 95.312 (95.312)
 * Prec@1 91.220
current lr 1.00000e-02
Grad=  tensor(2.9586, device='cuda:0')
Epoch: [79][0/391]	Time 0.253 (0.253)	Data 0.212 (0.212)	Loss 0.4460 (0.4460) ([0.267]+[0.179])	Prec@1 92.188 (92.188)
Epoch: [79][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.3450 (0.3552) ([0.167]+[0.178])	Prec@1 94.531 (94.268)
Epoch: [79][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.3817 (0.3550) ([0.205]+[0.176])	Prec@1 94.531 (94.197)
Epoch: [79][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3881 (0.3507) ([0.213]+[0.175])	Prec@1 94.531 (94.292)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3761 (0.3761) ([0.202]+[0.174])	Prec@1 93.750 (93.750)
 * Prec@1 91.470
current lr 1.00000e-02
Grad=  tensor(3.3395, device='cuda:0')
Epoch: [80][0/391]	Time 0.228 (0.228)	Data 0.196 (0.196)	Loss 0.4874 (0.4874) ([0.313]+[0.174])	Prec@1 90.625 (90.625)
Epoch: [80][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3556 (0.3411) ([0.183]+[0.173])	Prec@1 96.094 (94.701)
Epoch: [80][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3097 (0.3384) ([0.138]+[0.172])	Prec@1 95.312 (94.566)
Epoch: [80][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2315 (0.3362) ([0.061]+[0.171])	Prec@1 99.219 (94.599)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.3217 (0.3217) ([0.152]+[0.170])	Prec@1 94.531 (94.531)
 * Prec@1 91.480
current lr 1.00000e-02
Grad=  tensor(2.0789, device='cuda:0')
Epoch: [81][0/391]	Time 0.236 (0.236)	Data 0.204 (0.204)	Loss 0.3639 (0.3639) ([0.194]+[0.170])	Prec@1 94.531 (94.531)
Epoch: [81][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3990 (0.3209) ([0.230]+[0.169])	Prec@1 92.188 (95.050)
Epoch: [81][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.4107 (0.3199) ([0.243]+[0.168])	Prec@1 92.188 (95.064)
Epoch: [81][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2720 (0.3193) ([0.105]+[0.167])	Prec@1 96.094 (95.030)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.3537 (0.3537) ([0.188]+[0.166])	Prec@1 94.531 (94.531)
 * Prec@1 91.880
current lr 1.00000e-02
Grad=  tensor(1.2692, device='cuda:0')
Epoch: [82][0/391]	Time 0.225 (0.225)	Data 0.195 (0.195)	Loss 0.2426 (0.2426) ([0.077]+[0.166])	Prec@1 96.875 (96.875)
Epoch: [82][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.2819 (0.3002) ([0.117]+[0.165])	Prec@1 96.875 (95.490)
Epoch: [82][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2293 (0.3010) ([0.066]+[0.164])	Prec@1 98.438 (95.445)
Epoch: [82][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.3274 (0.3015) ([0.165]+[0.163])	Prec@1 95.312 (95.460)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.3292 (0.3292) ([0.167]+[0.162])	Prec@1 94.531 (94.531)
 * Prec@1 91.670
current lr 1.00000e-02
Grad=  tensor(2.2389, device='cuda:0')
Epoch: [83][0/391]	Time 0.211 (0.211)	Data 0.179 (0.179)	Loss 0.2381 (0.2381) ([0.076]+[0.162])	Prec@1 96.875 (96.875)
Epoch: [83][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.3150 (0.2941) ([0.154]+[0.161])	Prec@1 96.094 (95.429)
Epoch: [83][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2953 (0.2954) ([0.135]+[0.160])	Prec@1 94.531 (95.301)
Epoch: [83][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.3251 (0.2970) ([0.166]+[0.159])	Prec@1 94.531 (95.336)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3642 (0.3642) ([0.206]+[0.158])	Prec@1 92.969 (92.969)
 * Prec@1 91.820
current lr 1.00000e-02
Grad=  tensor(4.7698, device='cuda:0')
Epoch: [84][0/391]	Time 0.203 (0.203)	Data 0.171 (0.171)	Loss 0.2622 (0.2622) ([0.104]+[0.158])	Prec@1 93.750 (93.750)
Epoch: [84][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.2363 (0.2850) ([0.079]+[0.157])	Prec@1 98.438 (95.761)
Epoch: [84][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3830 (0.2842) ([0.226]+[0.157])	Prec@1 92.969 (95.717)
Epoch: [84][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2966 (0.2860) ([0.141]+[0.156])	Prec@1 93.750 (95.645)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.3494 (0.3494) ([0.194]+[0.155])	Prec@1 92.188 (92.188)
 * Prec@1 91.290
current lr 1.00000e-02
Grad=  tensor(1.0286, device='cuda:0')
Epoch: [85][0/391]	Time 0.252 (0.252)	Data 0.215 (0.215)	Loss 0.2055 (0.2055) ([0.050]+[0.155])	Prec@1 98.438 (98.438)
Epoch: [85][100/391]	Time 0.031 (0.029)	Data 0.000 (0.002)	Loss 0.1879 (0.2731) ([0.033]+[0.154])	Prec@1 99.219 (96.140)
Epoch: [85][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.3398 (0.2773) ([0.186]+[0.154])	Prec@1 95.312 (95.931)
Epoch: [85][300/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2957 (0.2777) ([0.143]+[0.153])	Prec@1 95.312 (95.956)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3520 (0.3520) ([0.200]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 91.530
current lr 1.00000e-02
Grad=  tensor(7.1488, device='cuda:0')
Epoch: [86][0/391]	Time 0.240 (0.240)	Data 0.208 (0.208)	Loss 0.3328 (0.3328) ([0.180]+[0.152])	Prec@1 93.750 (93.750)
Epoch: [86][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3086 (0.2716) ([0.157]+[0.152])	Prec@1 95.312 (96.024)
Epoch: [86][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2700 (0.2744) ([0.119]+[0.151])	Prec@1 96.094 (95.915)
Epoch: [86][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3337 (0.2760) ([0.183]+[0.150])	Prec@1 92.969 (95.816)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3031 (0.3031) ([0.153]+[0.150])	Prec@1 93.750 (93.750)
 * Prec@1 91.320
current lr 1.00000e-02
Grad=  tensor(2.2805, device='cuda:0')
Epoch: [87][0/391]	Time 0.259 (0.259)	Data 0.224 (0.224)	Loss 0.2222 (0.2222) ([0.072]+[0.150])	Prec@1 97.656 (97.656)
Epoch: [87][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.3185 (0.2611) ([0.169]+[0.149])	Prec@1 95.312 (96.163)
Epoch: [87][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2654 (0.2636) ([0.117]+[0.148])	Prec@1 95.312 (96.094)
Epoch: [87][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2132 (0.2673) ([0.065]+[0.148])	Prec@1 98.438 (96.018)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.3583 (0.3583) ([0.211]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 91.160
current lr 1.00000e-02
Grad=  tensor(10.3227, device='cuda:0')
Epoch: [88][0/391]	Time 0.243 (0.243)	Data 0.210 (0.210)	Loss 0.3420 (0.3420) ([0.195]+[0.147])	Prec@1 90.625 (90.625)
Epoch: [88][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2741 (0.2604) ([0.127]+[0.147])	Prec@1 96.094 (95.955)
Epoch: [88][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2541 (0.2618) ([0.108]+[0.146])	Prec@1 96.094 (96.070)
Epoch: [88][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2662 (0.2631) ([0.121]+[0.146])	Prec@1 96.875 (96.000)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.3105 (0.3105) ([0.165]+[0.145])	Prec@1 93.750 (93.750)
 * Prec@1 91.000
current lr 1.00000e-02
Grad=  tensor(2.3234, device='cuda:0')
Epoch: [89][0/391]	Time 0.233 (0.233)	Data 0.199 (0.199)	Loss 0.2428 (0.2428) ([0.098]+[0.145])	Prec@1 97.656 (97.656)
Epoch: [89][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2012 (0.2606) ([0.056]+[0.145])	Prec@1 99.219 (96.194)
Epoch: [89][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2439 (0.2657) ([0.100]+[0.144])	Prec@1 96.875 (95.969)
Epoch: [89][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3656 (0.2651) ([0.222]+[0.144])	Prec@1 92.188 (95.967)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.3711 (0.3711) ([0.228]+[0.143])	Prec@1 91.406 (91.406)
 * Prec@1 91.250
current lr 1.00000e-02
Grad=  tensor(6.3743, device='cuda:0')
Epoch: [90][0/391]	Time 0.233 (0.233)	Data 0.201 (0.201)	Loss 0.3037 (0.3037) ([0.160]+[0.143])	Prec@1 94.531 (94.531)
Epoch: [90][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2123 (0.2592) ([0.069]+[0.143])	Prec@1 96.094 (96.248)
Epoch: [90][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1959 (0.2614) ([0.053]+[0.142])	Prec@1 97.656 (96.086)
Epoch: [90][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2213 (0.2596) ([0.079]+[0.142])	Prec@1 96.875 (96.177)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.3522 (0.3522) ([0.211]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 90.930
current lr 1.00000e-02
Grad=  tensor(7.0751, device='cuda:0')
Epoch: [91][0/391]	Time 0.239 (0.239)	Data 0.204 (0.204)	Loss 0.2492 (0.2492) ([0.108]+[0.142])	Prec@1 96.875 (96.875)
Epoch: [91][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2031 (0.2575) ([0.062]+[0.141])	Prec@1 98.438 (96.140)
Epoch: [91][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.3215 (0.2595) ([0.181]+[0.141])	Prec@1 90.625 (96.012)
Epoch: [91][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2716 (0.2603) ([0.131]+[0.140])	Prec@1 95.312 (95.959)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3024 (0.3024) ([0.162]+[0.140])	Prec@1 92.188 (92.188)
 * Prec@1 90.750
current lr 1.00000e-02
Grad=  tensor(5.8737, device='cuda:0')
Epoch: [92][0/391]	Time 0.229 (0.229)	Data 0.198 (0.198)	Loss 0.2893 (0.2893) ([0.149]+[0.140])	Prec@1 96.094 (96.094)
Epoch: [92][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2592 (0.2445) ([0.120]+[0.140])	Prec@1 94.531 (96.496)
Epoch: [92][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2084 (0.2534) ([0.069]+[0.139])	Prec@1 98.438 (96.203)
Epoch: [92][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3842 (0.2589) ([0.245]+[0.139])	Prec@1 89.844 (95.977)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2356 (0.2356) ([0.097]+[0.139])	Prec@1 96.094 (96.094)
 * Prec@1 90.980
current lr 1.00000e-02
Grad=  tensor(2.7610, device='cuda:0')
Epoch: [93][0/391]	Time 0.227 (0.227)	Data 0.195 (0.195)	Loss 0.2114 (0.2114) ([0.072]+[0.139])	Prec@1 98.438 (98.438)
Epoch: [93][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2767 (0.2412) ([0.138]+[0.138])	Prec@1 96.875 (96.496)
Epoch: [93][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2281 (0.2472) ([0.090]+[0.138])	Prec@1 96.875 (96.300)
Epoch: [93][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3905 (0.2512) ([0.253]+[0.138])	Prec@1 92.188 (96.187)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2922 (0.2922) ([0.154]+[0.138])	Prec@1 93.750 (93.750)
 * Prec@1 89.860
current lr 1.00000e-02
Grad=  tensor(5.5180, device='cuda:0')
Epoch: [94][0/391]	Time 0.231 (0.231)	Data 0.198 (0.198)	Loss 0.2554 (0.2554) ([0.118]+[0.138])	Prec@1 96.875 (96.875)
Epoch: [94][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2413 (0.2538) ([0.104]+[0.138])	Prec@1 97.656 (96.055)
Epoch: [94][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2501 (0.2517) ([0.113]+[0.137])	Prec@1 93.750 (96.105)
Epoch: [94][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2341 (0.2508) ([0.097]+[0.137])	Prec@1 96.875 (96.148)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.3938 (0.3938) ([0.257]+[0.137])	Prec@1 92.188 (92.188)
 * Prec@1 90.180
current lr 1.00000e-02
Grad=  tensor(9.6241, device='cuda:0')
Epoch: [95][0/391]	Time 0.226 (0.226)	Data 0.193 (0.193)	Loss 0.2824 (0.2824) ([0.146]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [95][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2364 (0.2455) ([0.100]+[0.136])	Prec@1 96.094 (96.465)
Epoch: [95][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3192 (0.2485) ([0.183]+[0.136])	Prec@1 94.531 (96.296)
Epoch: [95][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2307 (0.2540) ([0.095]+[0.136])	Prec@1 98.438 (96.091)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3675 (0.3675) ([0.232]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 90.440
current lr 1.00000e-02
Grad=  tensor(7.1035, device='cuda:0')
Epoch: [96][0/391]	Time 0.227 (0.227)	Data 0.195 (0.195)	Loss 0.3114 (0.3114) ([0.176]+[0.136])	Prec@1 95.312 (95.312)
Epoch: [96][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1776 (0.2403) ([0.042]+[0.136])	Prec@1 98.438 (96.419)
Epoch: [96][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2322 (0.2464) ([0.097]+[0.135])	Prec@1 96.094 (96.241)
Epoch: [96][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2937 (0.2461) ([0.159]+[0.135])	Prec@1 96.094 (96.281)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3365 (0.3365) ([0.201]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 90.680
current lr 1.00000e-02
Grad=  tensor(5.2273, device='cuda:0')
Epoch: [97][0/391]	Time 0.219 (0.219)	Data 0.187 (0.187)	Loss 0.1907 (0.1907) ([0.056]+[0.135])	Prec@1 96.875 (96.875)
Epoch: [97][100/391]	Time 0.024 (0.028)	Data 0.000 (0.002)	Loss 0.2281 (0.2580) ([0.093]+[0.135])	Prec@1 96.094 (95.831)
Epoch: [97][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2531 (0.2563) ([0.118]+[0.135])	Prec@1 95.312 (95.853)
Epoch: [97][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2637 (0.2516) ([0.129]+[0.135])	Prec@1 93.750 (96.006)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3275 (0.3275) ([0.193]+[0.134])	Prec@1 94.531 (94.531)
 * Prec@1 89.730
current lr 1.00000e-02
Grad=  tensor(5.7130, device='cuda:0')
Epoch: [98][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.2404 (0.2404) ([0.106]+[0.134])	Prec@1 96.875 (96.875)
Epoch: [98][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1626 (0.2420) ([0.028]+[0.134])	Prec@1 100.000 (96.295)
Epoch: [98][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2692 (0.2456) ([0.135]+[0.134])	Prec@1 96.094 (96.214)
Epoch: [98][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1943 (0.2503) ([0.060]+[0.134])	Prec@1 98.438 (96.050)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.3939 (0.3939) ([0.260]+[0.134])	Prec@1 91.406 (91.406)
 * Prec@1 90.110
current lr 1.00000e-02
Grad=  tensor(4.4068, device='cuda:0')
Epoch: [99][0/391]	Time 0.260 (0.260)	Data 0.227 (0.227)	Loss 0.2240 (0.2240) ([0.090]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [99][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2317 (0.2443) ([0.098]+[0.134])	Prec@1 95.312 (96.310)
Epoch: [99][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.2240 (0.2527) ([0.090]+[0.134])	Prec@1 98.438 (96.032)
Epoch: [99][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2378 (0.2526) ([0.104]+[0.134])	Prec@1 96.094 (96.031)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.3297 (0.3297) ([0.196]+[0.133])	Prec@1 92.969 (92.969)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(7.5862, device='cuda:0')
Epoch: [100][0/391]	Time 0.256 (0.256)	Data 0.224 (0.224)	Loss 0.2507 (0.2507) ([0.117]+[0.133])	Prec@1 96.875 (96.875)
Epoch: [100][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2741 (0.2349) ([0.141]+[0.133])	Prec@1 94.531 (96.682)
Epoch: [100][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2887 (0.2463) ([0.156]+[0.133])	Prec@1 92.969 (96.183)
Epoch: [100][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.2455 (0.2515) ([0.112]+[0.133])	Prec@1 96.875 (95.995)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2913 (0.2913) ([0.158]+[0.133])	Prec@1 94.531 (94.531)
 * Prec@1 90.330
current lr 1.00000e-02
Grad=  tensor(7.7310, device='cuda:0')
Epoch: [101][0/391]	Time 0.258 (0.258)	Data 0.225 (0.225)	Loss 0.2554 (0.2554) ([0.123]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [101][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.3699 (0.2461) ([0.237]+[0.133])	Prec@1 92.969 (96.357)
Epoch: [101][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2611 (0.2527) ([0.128]+[0.133])	Prec@1 96.875 (95.997)
Epoch: [101][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1997 (0.2489) ([0.067]+[0.133])	Prec@1 96.875 (96.107)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2875 (0.2875) ([0.155]+[0.133])	Prec@1 93.750 (93.750)
 * Prec@1 88.830
current lr 1.00000e-02
Grad=  tensor(6.3743, device='cuda:0')
Epoch: [102][0/391]	Time 0.261 (0.261)	Data 0.230 (0.230)	Loss 0.2175 (0.2175) ([0.085]+[0.133])	Prec@1 96.094 (96.094)
Epoch: [102][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2039 (0.2430) ([0.071]+[0.133])	Prec@1 98.438 (96.403)
Epoch: [102][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2949 (0.2517) ([0.162]+[0.133])	Prec@1 95.312 (96.102)
Epoch: [102][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3069 (0.2542) ([0.174]+[0.132])	Prec@1 94.531 (95.982)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.3366 (0.3366) ([0.204]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(5.3509, device='cuda:0')
Epoch: [103][0/391]	Time 0.226 (0.226)	Data 0.193 (0.193)	Loss 0.2096 (0.2096) ([0.077]+[0.132])	Prec@1 96.875 (96.875)
Epoch: [103][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2346 (0.2397) ([0.102]+[0.132])	Prec@1 96.875 (96.442)
Epoch: [103][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1740 (0.2422) ([0.042]+[0.132])	Prec@1 98.438 (96.292)
Epoch: [103][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2107 (0.2448) ([0.079]+[0.132])	Prec@1 99.219 (96.273)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.4153 (0.4153) ([0.283]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 89.300
current lr 1.00000e-02
Grad=  tensor(8.6558, device='cuda:0')
Epoch: [104][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.2748 (0.2748) ([0.143]+[0.132])	Prec@1 93.750 (93.750)
Epoch: [104][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.3650 (0.2426) ([0.233]+[0.132])	Prec@1 92.969 (96.419)
Epoch: [104][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2264 (0.2434) ([0.094]+[0.132])	Prec@1 96.094 (96.269)
Epoch: [104][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2462 (0.2464) ([0.114]+[0.132])	Prec@1 96.094 (96.153)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3838 (0.3838) ([0.252]+[0.132])	Prec@1 89.844 (89.844)
 * Prec@1 89.340
current lr 1.00000e-02
Grad=  tensor(11.0916, device='cuda:0')
Epoch: [105][0/391]	Time 0.253 (0.253)	Data 0.220 (0.220)	Loss 0.3547 (0.3547) ([0.223]+[0.132])	Prec@1 92.969 (92.969)
Epoch: [105][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2106 (0.2445) ([0.079]+[0.132])	Prec@1 97.656 (96.117)
Epoch: [105][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2625 (0.2497) ([0.131]+[0.132])	Prec@1 96.094 (95.934)
Epoch: [105][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2254 (0.2569) ([0.093]+[0.132])	Prec@1 96.094 (95.699)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.3913 (0.3913) ([0.259]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 90.350
current lr 1.00000e-02
Grad=  tensor(6.7822, device='cuda:0')
Epoch: [106][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.2471 (0.2471) ([0.115]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [106][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.3231 (0.2397) ([0.191]+[0.132])	Prec@1 93.750 (96.419)
Epoch: [106][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2306 (0.2395) ([0.099]+[0.132])	Prec@1 96.875 (96.420)
Epoch: [106][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3098 (0.2436) ([0.178]+[0.132])	Prec@1 96.094 (96.247)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.4931 (0.4931) ([0.361]+[0.132])	Prec@1 89.844 (89.844)
 * Prec@1 89.600
current lr 1.00000e-02
Grad=  tensor(4.7855, device='cuda:0')
Epoch: [107][0/391]	Time 0.200 (0.200)	Data 0.172 (0.172)	Loss 0.2237 (0.2237) ([0.092]+[0.132])	Prec@1 96.875 (96.875)
Epoch: [107][100/391]	Time 0.026 (0.026)	Data 0.000 (0.002)	Loss 0.1712 (0.2435) ([0.039]+[0.132])	Prec@1 99.219 (96.078)
Epoch: [107][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2272 (0.2470) ([0.095]+[0.132])	Prec@1 96.094 (95.989)
Epoch: [107][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2718 (0.2488) ([0.140]+[0.132])	Prec@1 95.312 (95.951)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3127 (0.3127) ([0.181]+[0.132])	Prec@1 95.312 (95.312)
 * Prec@1 90.370
current lr 1.00000e-02
Grad=  tensor(4.3505, device='cuda:0')
Epoch: [108][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.2116 (0.2116) ([0.080]+[0.132])	Prec@1 97.656 (97.656)
Epoch: [108][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2104 (0.2366) ([0.079]+[0.132])	Prec@1 98.438 (96.527)
Epoch: [108][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3607 (0.2510) ([0.229]+[0.132])	Prec@1 93.750 (96.043)
Epoch: [108][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2271 (0.2526) ([0.095]+[0.132])	Prec@1 96.875 (95.907)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4840 (0.4840) ([0.352]+[0.132])	Prec@1 88.281 (88.281)
 * Prec@1 89.590
current lr 1.00000e-02
Grad=  tensor(14.5988, device='cuda:0')
Epoch: [109][0/391]	Time 0.239 (0.239)	Data 0.210 (0.210)	Loss 0.2753 (0.2753) ([0.143]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [109][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.3069 (0.2501) ([0.175]+[0.132])	Prec@1 96.094 (95.931)
Epoch: [109][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2742 (0.2504) ([0.142]+[0.132])	Prec@1 96.094 (96.000)
Epoch: [109][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.2647 (0.2510) ([0.133]+[0.132])	Prec@1 96.094 (95.980)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.4631 (0.4631) ([0.331]+[0.132])	Prec@1 89.062 (89.062)
 * Prec@1 88.870
current lr 1.00000e-02
Grad=  tensor(3.5204, device='cuda:0')
Epoch: [110][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.2084 (0.2084) ([0.076]+[0.132])	Prec@1 97.656 (97.656)
Epoch: [110][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2734 (0.2470) ([0.141]+[0.132])	Prec@1 94.531 (96.225)
Epoch: [110][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2269 (0.2456) ([0.095]+[0.132])	Prec@1 96.094 (96.315)
Epoch: [110][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1743 (0.2474) ([0.042]+[0.132])	Prec@1 99.219 (96.216)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.3216 (0.3216) ([0.190]+[0.132])	Prec@1 93.750 (93.750)
 * Prec@1 90.170
current lr 1.00000e-02
Grad=  tensor(9.1943, device='cuda:0')
Epoch: [111][0/391]	Time 0.217 (0.217)	Data 0.187 (0.187)	Loss 0.2481 (0.2481) ([0.116]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [111][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2257 (0.2459) ([0.094]+[0.132])	Prec@1 97.656 (96.295)
Epoch: [111][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2730 (0.2480) ([0.141]+[0.132])	Prec@1 94.531 (96.187)
Epoch: [111][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2105 (0.2508) ([0.078]+[0.132])	Prec@1 96.094 (96.031)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.4262 (0.4262) ([0.294]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(9.2855, device='cuda:0')
Epoch: [112][0/391]	Time 0.253 (0.253)	Data 0.220 (0.220)	Loss 0.2554 (0.2554) ([0.123]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [112][100/391]	Time 0.028 (0.028)	Data 0.000 (0.002)	Loss 0.2033 (0.2372) ([0.071]+[0.132])	Prec@1 98.438 (96.488)
Epoch: [112][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.3536 (0.2479) ([0.222]+[0.132])	Prec@1 92.969 (96.140)
Epoch: [112][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2133 (0.2511) ([0.081]+[0.132])	Prec@1 96.875 (96.039)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.4724 (0.4724) ([0.340]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 90.390
current lr 1.00000e-02
Grad=  tensor(8.0144, device='cuda:0')
Epoch: [113][0/391]	Time 0.231 (0.231)	Data 0.200 (0.200)	Loss 0.2284 (0.2284) ([0.096]+[0.132])	Prec@1 96.875 (96.875)
Epoch: [113][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2061 (0.2386) ([0.074]+[0.132])	Prec@1 96.875 (96.372)
Epoch: [113][200/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.2162 (0.2447) ([0.084]+[0.132])	Prec@1 98.438 (96.199)
Epoch: [113][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1855 (0.2441) ([0.054]+[0.132])	Prec@1 99.219 (96.221)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4417 (0.4417) ([0.309]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 89.890
current lr 1.00000e-02
Grad=  tensor(14.2524, device='cuda:0')
Epoch: [114][0/391]	Time 0.254 (0.254)	Data 0.220 (0.220)	Loss 0.2422 (0.2422) ([0.110]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [114][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2489 (0.2384) ([0.117]+[0.132])	Prec@1 95.312 (96.481)
Epoch: [114][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3263 (0.2433) ([0.194]+[0.132])	Prec@1 92.188 (96.238)
Epoch: [114][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3036 (0.2504) ([0.171]+[0.132])	Prec@1 92.188 (95.974)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3034 (0.3034) ([0.171]+[0.132])	Prec@1 96.094 (96.094)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(10.0410, device='cuda:0')
Epoch: [115][0/391]	Time 0.225 (0.225)	Data 0.193 (0.193)	Loss 0.3287 (0.3287) ([0.196]+[0.132])	Prec@1 94.531 (94.531)
Epoch: [115][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2101 (0.2408) ([0.078]+[0.132])	Prec@1 97.656 (96.403)
Epoch: [115][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2671 (0.2453) ([0.135]+[0.132])	Prec@1 95.312 (96.234)
Epoch: [115][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2308 (0.2452) ([0.098]+[0.132])	Prec@1 96.094 (96.208)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.4831 (0.4831) ([0.351]+[0.132])	Prec@1 88.281 (88.281)
 * Prec@1 90.160
current lr 1.00000e-02
Grad=  tensor(9.6731, device='cuda:0')
Epoch: [116][0/391]	Time 0.259 (0.259)	Data 0.225 (0.225)	Loss 0.2671 (0.2671) ([0.135]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [116][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2955 (0.2410) ([0.163]+[0.132])	Prec@1 92.969 (96.279)
Epoch: [116][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2220 (0.2396) ([0.090]+[0.132])	Prec@1 96.875 (96.327)
Epoch: [116][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2518 (0.2436) ([0.119]+[0.132])	Prec@1 96.875 (96.229)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.3533 (0.3533) ([0.221]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 90.370
current lr 1.00000e-02
Grad=  tensor(6.6336, device='cuda:0')
Epoch: [117][0/391]	Time 0.223 (0.223)	Data 0.190 (0.190)	Loss 0.2193 (0.2193) ([0.087]+[0.133])	Prec@1 96.094 (96.094)
Epoch: [117][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2992 (0.2388) ([0.167]+[0.133])	Prec@1 93.750 (96.542)
Epoch: [117][200/391]	Time 0.030 (0.026)	Data 0.000 (0.001)	Loss 0.2444 (0.2371) ([0.112]+[0.132])	Prec@1 97.656 (96.475)
Epoch: [117][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3649 (0.2435) ([0.232]+[0.133])	Prec@1 94.531 (96.270)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3125 (0.3125) ([0.180]+[0.133])	Prec@1 91.406 (91.406)
 * Prec@1 90.950
current lr 1.00000e-02
Grad=  tensor(4.8354, device='cuda:0')
Epoch: [118][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.2130 (0.2130) ([0.080]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [118][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.2564 (0.2496) ([0.124]+[0.133])	Prec@1 95.312 (96.101)
Epoch: [118][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2228 (0.2509) ([0.090]+[0.133])	Prec@1 96.875 (96.098)
Epoch: [118][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2118 (0.2475) ([0.079]+[0.133])	Prec@1 96.094 (96.224)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.4133 (0.4133) ([0.281]+[0.133])	Prec@1 89.062 (89.062)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(7.8430, device='cuda:0')
Epoch: [119][0/391]	Time 0.243 (0.243)	Data 0.212 (0.212)	Loss 0.2204 (0.2204) ([0.088]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [119][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.3588 (0.2344) ([0.226]+[0.133])	Prec@1 91.406 (96.597)
Epoch: [119][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2613 (0.2444) ([0.129]+[0.133])	Prec@1 96.875 (96.335)
Epoch: [119][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1886 (0.2480) ([0.056]+[0.133])	Prec@1 96.875 (96.224)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.3223 (0.3223) ([0.190]+[0.133])	Prec@1 95.312 (95.312)
 * Prec@1 90.060
current lr 1.00000e-02
Grad=  tensor(4.4643, device='cuda:0')
Epoch: [120][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.1844 (0.1844) ([0.052]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [120][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.2789 (0.2355) ([0.146]+[0.133])	Prec@1 95.312 (96.488)
Epoch: [120][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.3093 (0.2386) ([0.177]+[0.133])	Prec@1 94.531 (96.381)
Epoch: [120][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2182 (0.2448) ([0.085]+[0.133])	Prec@1 97.656 (96.195)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.3425 (0.3425) ([0.210]+[0.133])	Prec@1 95.312 (95.312)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(6.7375, device='cuda:0')
Epoch: [121][0/391]	Time 0.239 (0.239)	Data 0.208 (0.208)	Loss 0.2113 (0.2113) ([0.078]+[0.133])	Prec@1 96.875 (96.875)
Epoch: [121][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.3000 (0.2431) ([0.167]+[0.133])	Prec@1 92.969 (96.287)
Epoch: [121][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2597 (0.2432) ([0.127]+[0.133])	Prec@1 96.094 (96.343)
Epoch: [121][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2483 (0.2471) ([0.115]+[0.133])	Prec@1 96.875 (96.200)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4706 (0.4706) ([0.338]+[0.133])	Prec@1 89.062 (89.062)
 * Prec@1 89.110
current lr 1.00000e-02
Grad=  tensor(14.2757, device='cuda:0')
Epoch: [122][0/391]	Time 0.258 (0.258)	Data 0.225 (0.225)	Loss 0.2863 (0.2863) ([0.153]+[0.133])	Prec@1 95.312 (95.312)
Epoch: [122][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2075 (0.2384) ([0.075]+[0.133])	Prec@1 97.656 (96.473)
Epoch: [122][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2197 (0.2455) ([0.087]+[0.133])	Prec@1 96.875 (96.203)
Epoch: [122][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2423 (0.2486) ([0.109]+[0.133])	Prec@1 96.094 (96.161)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2733 (0.2733) ([0.140]+[0.133])	Prec@1 93.750 (93.750)
 * Prec@1 90.180
current lr 1.00000e-02
Grad=  tensor(15.9063, device='cuda:0')
Epoch: [123][0/391]	Time 0.222 (0.222)	Data 0.190 (0.190)	Loss 0.3324 (0.3324) ([0.199]+[0.133])	Prec@1 92.969 (92.969)
Epoch: [123][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2454 (0.2388) ([0.112]+[0.133])	Prec@1 96.094 (96.303)
Epoch: [123][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1958 (0.2423) ([0.063]+[0.133])	Prec@1 97.656 (96.234)
Epoch: [123][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2190 (0.2447) ([0.086]+[0.133])	Prec@1 96.094 (96.156)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3293 (0.3293) ([0.196]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 90.540
current lr 1.00000e-02
Grad=  tensor(5.9387, device='cuda:0')
Epoch: [124][0/391]	Time 0.204 (0.204)	Data 0.173 (0.173)	Loss 0.2082 (0.2082) ([0.075]+[0.133])	Prec@1 96.875 (96.875)
Epoch: [124][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2365 (0.2445) ([0.103]+[0.133])	Prec@1 97.656 (96.218)
Epoch: [124][200/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.2377 (0.2485) ([0.104]+[0.133])	Prec@1 97.656 (96.187)
Epoch: [124][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2149 (0.2480) ([0.082]+[0.133])	Prec@1 98.438 (96.226)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.3555 (0.3555) ([0.222]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(9.0833, device='cuda:0')
Epoch: [125][0/391]	Time 0.254 (0.254)	Data 0.222 (0.222)	Loss 0.2403 (0.2403) ([0.107]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [125][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2888 (0.2354) ([0.155]+[0.133])	Prec@1 97.656 (96.442)
Epoch: [125][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2844 (0.2425) ([0.151]+[0.133])	Prec@1 95.312 (96.245)
Epoch: [125][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2332 (0.2439) ([0.100]+[0.133])	Prec@1 95.312 (96.182)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3946 (0.3946) ([0.261]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(6.4443, device='cuda:0')
Epoch: [126][0/391]	Time 0.208 (0.208)	Data 0.174 (0.174)	Loss 0.2491 (0.2491) ([0.115]+[0.134])	Prec@1 96.875 (96.875)
Epoch: [126][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1935 (0.2445) ([0.060]+[0.134])	Prec@1 98.438 (96.202)
Epoch: [126][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2273 (0.2482) ([0.094]+[0.134])	Prec@1 95.312 (96.164)
Epoch: [126][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2231 (0.2511) ([0.089]+[0.134])	Prec@1 97.656 (96.013)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.4321 (0.4321) ([0.298]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 89.120
current lr 1.00000e-02
Grad=  tensor(18.1374, device='cuda:0')
Epoch: [127][0/391]	Time 0.252 (0.252)	Data 0.218 (0.218)	Loss 0.3257 (0.3257) ([0.192]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [127][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.2041 (0.2433) ([0.070]+[0.134])	Prec@1 97.656 (96.187)
Epoch: [127][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2138 (0.2402) ([0.080]+[0.134])	Prec@1 96.875 (96.428)
Epoch: [127][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2067 (0.2426) ([0.073]+[0.134])	Prec@1 97.656 (96.371)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.4203 (0.4203) ([0.287]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(19.7493, device='cuda:0')
Epoch: [128][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.2457 (0.2457) ([0.112]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [128][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.2034 (0.2324) ([0.070]+[0.134])	Prec@1 96.875 (96.573)
Epoch: [128][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2536 (0.2372) ([0.120]+[0.133])	Prec@1 93.750 (96.409)
Epoch: [128][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3051 (0.2428) ([0.171]+[0.134])	Prec@1 92.969 (96.257)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.4844 (0.4844) ([0.351]+[0.134])	Prec@1 87.500 (87.500)
 * Prec@1 89.040
current lr 1.00000e-02
Grad=  tensor(13.9345, device='cuda:0')
Epoch: [129][0/391]	Time 0.231 (0.231)	Data 0.197 (0.197)	Loss 0.2446 (0.2446) ([0.111]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [129][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2165 (0.2364) ([0.083]+[0.134])	Prec@1 96.875 (96.720)
Epoch: [129][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3424 (0.2408) ([0.209]+[0.134])	Prec@1 95.312 (96.475)
Epoch: [129][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1932 (0.2414) ([0.060]+[0.134])	Prec@1 96.875 (96.486)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4478 (0.4478) ([0.314]+[0.134])	Prec@1 91.406 (91.406)
 * Prec@1 89.710
current lr 1.00000e-02
Grad=  tensor(3.3951, device='cuda:0')
Epoch: [130][0/391]	Time 0.218 (0.218)	Data 0.186 (0.186)	Loss 0.1994 (0.1994) ([0.066]+[0.134])	Prec@1 97.656 (97.656)
Epoch: [130][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2598 (0.2271) ([0.126]+[0.134])	Prec@1 96.875 (96.867)
Epoch: [130][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2463 (0.2381) ([0.113]+[0.134])	Prec@1 96.875 (96.529)
Epoch: [130][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3440 (0.2411) ([0.210]+[0.134])	Prec@1 93.750 (96.400)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3717 (0.3717) ([0.238]+[0.134])	Prec@1 89.062 (89.062)
 * Prec@1 89.430
current lr 1.00000e-02
Grad=  tensor(8.4494, device='cuda:0')
Epoch: [131][0/391]	Time 0.203 (0.203)	Data 0.174 (0.174)	Loss 0.2824 (0.2824) ([0.149]+[0.134])	Prec@1 92.969 (92.969)
Epoch: [131][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2726 (0.2450) ([0.139]+[0.134])	Prec@1 94.531 (96.148)
Epoch: [131][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2248 (0.2429) ([0.091]+[0.134])	Prec@1 96.875 (96.234)
Epoch: [131][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2995 (0.2460) ([0.166]+[0.134])	Prec@1 96.094 (96.107)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3129 (0.3129) ([0.179]+[0.134])	Prec@1 92.969 (92.969)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(7.7676, device='cuda:0')
Epoch: [132][0/391]	Time 0.211 (0.211)	Data 0.178 (0.178)	Loss 0.2295 (0.2295) ([0.096]+[0.134])	Prec@1 96.875 (96.875)
Epoch: [132][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2807 (0.2331) ([0.147]+[0.134])	Prec@1 95.312 (96.581)
Epoch: [132][200/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.2792 (0.2350) ([0.145]+[0.134])	Prec@1 92.969 (96.618)
Epoch: [132][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2024 (0.2409) ([0.068]+[0.134])	Prec@1 97.656 (96.431)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.3721 (0.3721) ([0.238]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 90.450
current lr 1.00000e-02
Grad=  tensor(11.1392, device='cuda:0')
Epoch: [133][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.2584 (0.2584) ([0.124]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [133][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1925 (0.2370) ([0.059]+[0.134])	Prec@1 97.656 (96.550)
Epoch: [133][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1931 (0.2429) ([0.059]+[0.134])	Prec@1 96.875 (96.323)
Epoch: [133][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2554 (0.2448) ([0.121]+[0.134])	Prec@1 94.531 (96.242)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.3709 (0.3709) ([0.237]+[0.134])	Prec@1 92.969 (92.969)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(3.5348, device='cuda:0')
Epoch: [134][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.1838 (0.1838) ([0.050]+[0.134])	Prec@1 99.219 (99.219)
Epoch: [134][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.2836 (0.2320) ([0.150]+[0.134])	Prec@1 93.750 (96.713)
Epoch: [134][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2184 (0.2359) ([0.084]+[0.134])	Prec@1 97.656 (96.618)
Epoch: [134][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2452 (0.2398) ([0.111]+[0.134])	Prec@1 94.531 (96.480)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.3448 (0.3448) ([0.211]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(8.5081, device='cuda:0')
Epoch: [135][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.2286 (0.2286) ([0.095]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [135][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2273 (0.2375) ([0.093]+[0.134])	Prec@1 96.094 (96.465)
Epoch: [135][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2286 (0.2419) ([0.095]+[0.134])	Prec@1 96.094 (96.381)
Epoch: [135][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2497 (0.2439) ([0.116]+[0.134])	Prec@1 96.094 (96.343)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.2998 (0.2998) ([0.166]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 89.540
current lr 1.00000e-02
Grad=  tensor(8.5574, device='cuda:0')
Epoch: [136][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.2238 (0.2238) ([0.090]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [136][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2548 (0.2256) ([0.121]+[0.134])	Prec@1 96.875 (96.844)
Epoch: [136][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2289 (0.2367) ([0.095]+[0.134])	Prec@1 97.656 (96.580)
Epoch: [136][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2577 (0.2419) ([0.124]+[0.134])	Prec@1 96.094 (96.429)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.4556 (0.4556) ([0.322]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 87.740
current lr 1.00000e-02
Grad=  tensor(13.5317, device='cuda:0')
Epoch: [137][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.3113 (0.3113) ([0.177]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [137][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2233 (0.2410) ([0.089]+[0.134])	Prec@1 96.875 (96.380)
Epoch: [137][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2740 (0.2393) ([0.140]+[0.134])	Prec@1 95.312 (96.385)
Epoch: [137][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2503 (0.2430) ([0.116]+[0.134])	Prec@1 96.094 (96.268)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3863 (0.3863) ([0.252]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 89.970
current lr 1.00000e-02
Grad=  tensor(9.1253, device='cuda:0')
Epoch: [138][0/391]	Time 0.259 (0.259)	Data 0.226 (0.226)	Loss 0.3058 (0.3058) ([0.172]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [138][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.3185 (0.2336) ([0.184]+[0.134])	Prec@1 94.531 (96.488)
Epoch: [138][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2505 (0.2378) ([0.116]+[0.134])	Prec@1 96.094 (96.416)
Epoch: [138][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2865 (0.2394) ([0.152]+[0.134])	Prec@1 93.750 (96.410)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.3151 (0.3151) ([0.181]+[0.134])	Prec@1 94.531 (94.531)
 * Prec@1 89.320
current lr 1.00000e-02
Grad=  tensor(5.3005, device='cuda:0')
Epoch: [139][0/391]	Time 0.240 (0.240)	Data 0.208 (0.208)	Loss 0.1903 (0.1903) ([0.056]+[0.134])	Prec@1 98.438 (98.438)
Epoch: [139][100/391]	Time 0.027 (0.027)	Data 0.000 (0.002)	Loss 0.1625 (0.2390) ([0.028]+[0.134])	Prec@1 100.000 (96.380)
Epoch: [139][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2218 (0.2439) ([0.087]+[0.134])	Prec@1 96.875 (96.222)
Epoch: [139][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2714 (0.2444) ([0.137]+[0.134])	Prec@1 96.875 (96.249)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.3009 (0.3009) ([0.167]+[0.134])	Prec@1 96.875 (96.875)
 * Prec@1 89.690
current lr 1.00000e-02
Grad=  tensor(12.9086, device='cuda:0')
Epoch: [140][0/391]	Time 0.258 (0.258)	Data 0.223 (0.223)	Loss 0.3070 (0.3070) ([0.173]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [140][100/391]	Time 0.027 (0.028)	Data 0.000 (0.002)	Loss 0.2895 (0.2364) ([0.155]+[0.134])	Prec@1 94.531 (96.643)
Epoch: [140][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2493 (0.2405) ([0.115]+[0.134])	Prec@1 95.312 (96.514)
Epoch: [140][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.3345 (0.2435) ([0.200]+[0.135])	Prec@1 92.969 (96.434)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.4280 (0.4280) ([0.293]+[0.135])	Prec@1 90.625 (90.625)
 * Prec@1 90.130
current lr 1.00000e-02
Grad=  tensor(0.6980, device='cuda:0')
Epoch: [141][0/391]	Time 0.227 (0.227)	Data 0.196 (0.196)	Loss 0.1536 (0.1536) ([0.019]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [141][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2250 (0.2413) ([0.090]+[0.135])	Prec@1 97.656 (96.558)
Epoch: [141][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3019 (0.2469) ([0.167]+[0.135])	Prec@1 94.531 (96.319)
Epoch: [141][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2761 (0.2460) ([0.141]+[0.135])	Prec@1 95.312 (96.314)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.3272 (0.3272) ([0.192]+[0.135])	Prec@1 95.312 (95.312)
 * Prec@1 90.490
current lr 1.00000e-02
Grad=  tensor(8.2445, device='cuda:0')
Epoch: [142][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.2390 (0.2390) ([0.104]+[0.135])	Prec@1 96.875 (96.875)
Epoch: [142][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2093 (0.2331) ([0.075]+[0.135])	Prec@1 97.656 (96.782)
Epoch: [142][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2015 (0.2351) ([0.067]+[0.135])	Prec@1 98.438 (96.681)
Epoch: [142][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2831 (0.2371) ([0.148]+[0.135])	Prec@1 95.312 (96.600)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2794 (0.2794) ([0.145]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 89.720
current lr 1.00000e-02
Grad=  tensor(7.9250, device='cuda:0')
Epoch: [143][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.2460 (0.2460) ([0.111]+[0.135])	Prec@1 95.312 (95.312)
Epoch: [143][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1969 (0.2302) ([0.062]+[0.135])	Prec@1 97.656 (96.860)
Epoch: [143][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2633 (0.2387) ([0.129]+[0.135])	Prec@1 93.750 (96.447)
Epoch: [143][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2553 (0.2397) ([0.120]+[0.135])	Prec@1 96.094 (96.410)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3254 (0.3254) ([0.191]+[0.135])	Prec@1 89.844 (89.844)
 * Prec@1 89.790
current lr 1.00000e-02
Grad=  tensor(20.5612, device='cuda:0')
Epoch: [144][0/391]	Time 0.214 (0.214)	Data 0.182 (0.182)	Loss 0.3242 (0.3242) ([0.189]+[0.135])	Prec@1 93.750 (93.750)
Epoch: [144][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1914 (0.2391) ([0.056]+[0.135])	Prec@1 98.438 (96.511)
Epoch: [144][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1932 (0.2401) ([0.058]+[0.135])	Prec@1 97.656 (96.416)
Epoch: [144][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2107 (0.2451) ([0.076]+[0.135])	Prec@1 97.656 (96.255)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.3719 (0.3719) ([0.237]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 89.720
current lr 1.00000e-02
Grad=  tensor(5.2057, device='cuda:0')
Epoch: [145][0/391]	Time 0.251 (0.251)	Data 0.218 (0.218)	Loss 0.2017 (0.2017) ([0.067]+[0.135])	Prec@1 97.656 (97.656)
Epoch: [145][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2564 (0.2220) ([0.122]+[0.135])	Prec@1 96.875 (97.115)
Epoch: [145][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1971 (0.2297) ([0.062]+[0.135])	Prec@1 97.656 (96.801)
Epoch: [145][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2275 (0.2342) ([0.093]+[0.135])	Prec@1 98.438 (96.675)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.3632 (0.3632) ([0.228]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 89.590
current lr 1.00000e-02
Grad=  tensor(7.3843, device='cuda:0')
Epoch: [146][0/391]	Time 0.223 (0.223)	Data 0.190 (0.190)	Loss 0.1981 (0.1981) ([0.063]+[0.135])	Prec@1 97.656 (97.656)
Epoch: [146][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2199 (0.2342) ([0.085]+[0.135])	Prec@1 98.438 (96.581)
Epoch: [146][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1813 (0.2331) ([0.046]+[0.135])	Prec@1 98.438 (96.661)
Epoch: [146][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2761 (0.2390) ([0.141]+[0.135])	Prec@1 93.750 (96.496)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.3659 (0.3659) ([0.231]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 89.350
current lr 1.00000e-02
Grad=  tensor(5.1064, device='cuda:0')
Epoch: [147][0/391]	Time 0.231 (0.231)	Data 0.200 (0.200)	Loss 0.2252 (0.2252) ([0.090]+[0.135])	Prec@1 96.875 (96.875)
Epoch: [147][100/391]	Time 0.025 (0.029)	Data 0.000 (0.002)	Loss 0.2023 (0.2294) ([0.067]+[0.135])	Prec@1 97.656 (96.867)
Epoch: [147][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1979 (0.2327) ([0.063]+[0.135])	Prec@1 98.438 (96.856)
Epoch: [147][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1955 (0.2382) ([0.060]+[0.135])	Prec@1 97.656 (96.626)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.3677 (0.3677) ([0.232]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 89.710
current lr 1.00000e-02
Grad=  tensor(4.1924, device='cuda:0')
Epoch: [148][0/391]	Time 0.211 (0.211)	Data 0.180 (0.180)	Loss 0.1795 (0.1795) ([0.044]+[0.135])	Prec@1 99.219 (99.219)
Epoch: [148][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.3381 (0.2341) ([0.203]+[0.135])	Prec@1 94.531 (96.875)
Epoch: [148][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2289 (0.2341) ([0.094]+[0.135])	Prec@1 96.094 (96.801)
Epoch: [148][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3645 (0.2335) ([0.229]+[0.135])	Prec@1 94.531 (96.823)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.3400 (0.3400) ([0.205]+[0.135])	Prec@1 95.312 (95.312)
 * Prec@1 89.130
current lr 1.00000e-02
Grad=  tensor(4.9073, device='cuda:0')
Epoch: [149][0/391]	Time 0.234 (0.234)	Data 0.204 (0.204)	Loss 0.2129 (0.2129) ([0.078]+[0.135])	Prec@1 97.656 (97.656)
Epoch: [149][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3419 (0.2501) ([0.207]+[0.135])	Prec@1 92.969 (96.349)
Epoch: [149][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2266 (0.2412) ([0.091]+[0.135])	Prec@1 97.656 (96.591)
Epoch: [149][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1735 (0.2416) ([0.038]+[0.135])	Prec@1 98.438 (96.532)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.3754 (0.3754) ([0.240]+[0.135])	Prec@1 94.531 (94.531)
 * Prec@1 89.200
current lr 1.00000e-03
Grad=  tensor(6.2080, device='cuda:0')
Epoch: [150][0/391]	Time 0.205 (0.205)	Data 0.177 (0.177)	Loss 0.2115 (0.2115) ([0.076]+[0.135])	Prec@1 98.438 (98.438)
Epoch: [150][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1857 (0.2025) ([0.053]+[0.133])	Prec@1 98.438 (97.803)
Epoch: [150][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1608 (0.1915) ([0.028]+[0.133])	Prec@1 99.219 (98.185)
Epoch: [150][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1556 (0.1872) ([0.023]+[0.133])	Prec@1 98.438 (98.305)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2470 (0.2470) ([0.114]+[0.133])	Prec@1 96.875 (96.875)
 * Prec@1 92.420
current lr 1.00000e-03
Grad=  tensor(0.6561, device='cuda:0')
Epoch: [151][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1468 (0.1468) ([0.014]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [151][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1670 (0.1671) ([0.034]+[0.133])	Prec@1 99.219 (99.010)
Epoch: [151][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1598 (0.1670) ([0.027]+[0.133])	Prec@1 100.000 (98.951)
Epoch: [151][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1436 (0.1662) ([0.011]+[0.132])	Prec@1 100.000 (98.985)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2469 (0.2469) ([0.115]+[0.132])	Prec@1 96.875 (96.875)
 * Prec@1 92.830
current lr 1.00000e-03
Grad=  tensor(4.2467, device='cuda:0')
Epoch: [152][0/391]	Time 0.222 (0.222)	Data 0.193 (0.193)	Loss 0.1673 (0.1673) ([0.035]+[0.132])	Prec@1 98.438 (98.438)
Epoch: [152][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1468 (0.1604) ([0.014]+[0.132])	Prec@1 99.219 (99.157)
Epoch: [152][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2022 (0.1612) ([0.070]+[0.132])	Prec@1 97.656 (99.153)
Epoch: [152][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2017 (0.1604) ([0.070]+[0.132])	Prec@1 98.438 (99.180)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2280 (0.2280) ([0.096]+[0.132])	Prec@1 96.875 (96.875)
 * Prec@1 92.860
current lr 1.00000e-03
Grad=  tensor(1.2758, device='cuda:0')
Epoch: [153][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.1452 (0.1452) ([0.013]+[0.132])	Prec@1 99.219 (99.219)
Epoch: [153][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1686 (0.1560) ([0.037]+[0.132])	Prec@1 99.219 (99.288)
Epoch: [153][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1410 (0.1551) ([0.009]+[0.132])	Prec@1 100.000 (99.355)
Epoch: [153][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1497 (0.1557) ([0.018]+[0.132])	Prec@1 99.219 (99.315)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.2513 (0.2513) ([0.120]+[0.132])	Prec@1 96.875 (96.875)
 * Prec@1 92.990
current lr 1.00000e-03
Grad=  tensor(2.1680, device='cuda:0')
Epoch: [154][0/391]	Time 0.228 (0.228)	Data 0.198 (0.198)	Loss 0.1475 (0.1475) ([0.016]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [154][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1468 (0.1510) ([0.015]+[0.132])	Prec@1 100.000 (99.451)
Epoch: [154][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1473 (0.1508) ([0.016]+[0.131])	Prec@1 100.000 (99.436)
Epoch: [154][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1591 (0.1511) ([0.028]+[0.131])	Prec@1 99.219 (99.434)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2584 (0.2584) ([0.127]+[0.131])	Prec@1 96.875 (96.875)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(4.8209, device='cuda:0')
Epoch: [155][0/391]	Time 0.199 (0.199)	Data 0.170 (0.170)	Loss 0.1651 (0.1651) ([0.034]+[0.131])	Prec@1 99.219 (99.219)
Epoch: [155][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1516 (0.1497) ([0.020]+[0.131])	Prec@1 99.219 (99.459)
Epoch: [155][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1387 (0.1506) ([0.008]+[0.131])	Prec@1 100.000 (99.390)
Epoch: [155][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1352 (0.1504) ([0.004]+[0.131])	Prec@1 100.000 (99.398)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2679 (0.2679) ([0.137]+[0.131])	Prec@1 96.094 (96.094)
 * Prec@1 93.030
current lr 1.00000e-03
Grad=  tensor(1.1330, device='cuda:0')
Epoch: [156][0/391]	Time 0.259 (0.259)	Data 0.226 (0.226)	Loss 0.1430 (0.1430) ([0.012]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [156][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1394 (0.1497) ([0.009]+[0.131])	Prec@1 100.000 (99.420)
Epoch: [156][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1426 (0.1479) ([0.012]+[0.131])	Prec@1 99.219 (99.502)
Epoch: [156][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1405 (0.1473) ([0.010]+[0.131])	Prec@1 100.000 (99.512)
Test: [0/79]	Time 0.230 (0.230)	Loss 0.2279 (0.2279) ([0.097]+[0.130])	Prec@1 96.875 (96.875)
 * Prec@1 93.090
current lr 1.00000e-03
Grad=  tensor(0.8214, device='cuda:0')
Epoch: [157][0/391]	Time 0.256 (0.256)	Data 0.220 (0.220)	Loss 0.1387 (0.1387) ([0.008]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [157][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1334 (0.1457) ([0.003]+[0.130])	Prec@1 100.000 (99.544)
Epoch: [157][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1372 (0.1460) ([0.007]+[0.130])	Prec@1 100.000 (99.534)
Epoch: [157][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1617 (0.1459) ([0.032]+[0.130])	Prec@1 99.219 (99.543)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.2646 (0.2646) ([0.134]+[0.130])	Prec@1 96.875 (96.875)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(1.7190, device='cuda:0')
Epoch: [158][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.1423 (0.1423) ([0.012]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [158][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1394 (0.1457) ([0.009]+[0.130])	Prec@1 99.219 (99.559)
Epoch: [158][200/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.1444 (0.1457) ([0.014]+[0.130])	Prec@1 99.219 (99.526)
Epoch: [158][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.1444 (0.1448) ([0.015]+[0.130])	Prec@1 99.219 (99.556)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.2533 (0.2533) ([0.124]+[0.130])	Prec@1 96.875 (96.875)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(3.7478, device='cuda:0')
Epoch: [159][0/391]	Time 0.222 (0.222)	Data 0.189 (0.189)	Loss 0.1410 (0.1410) ([0.011]+[0.130])	Prec@1 99.219 (99.219)
Epoch: [159][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1428 (0.1429) ([0.013]+[0.130])	Prec@1 99.219 (99.590)
Epoch: [159][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1439 (0.1450) ([0.014]+[0.130])	Prec@1 99.219 (99.537)
Epoch: [159][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1464 (0.1449) ([0.017]+[0.129])	Prec@1 99.219 (99.543)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.2460 (0.2460) ([0.117]+[0.129])	Prec@1 96.094 (96.094)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(0.2789, device='cuda:0')
Epoch: [160][0/391]	Time 0.216 (0.216)	Data 0.187 (0.187)	Loss 0.1339 (0.1339) ([0.005]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [160][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1526 (0.1431) ([0.023]+[0.129])	Prec@1 99.219 (99.621)
Epoch: [160][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1580 (0.1437) ([0.029]+[0.129])	Prec@1 99.219 (99.596)
Epoch: [160][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1396 (0.1437) ([0.010]+[0.129])	Prec@1 100.000 (99.593)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.2410 (0.2410) ([0.112]+[0.129])	Prec@1 96.875 (96.875)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(9.6418, device='cuda:0')
Epoch: [161][0/391]	Time 0.235 (0.235)	Data 0.204 (0.204)	Loss 0.1553 (0.1553) ([0.026]+[0.129])	Prec@1 99.219 (99.219)
Epoch: [161][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1311 (0.1415) ([0.002]+[0.129])	Prec@1 100.000 (99.675)
Epoch: [161][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1336 (0.1417) ([0.005]+[0.129])	Prec@1 100.000 (99.658)
Epoch: [161][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1367 (0.1418) ([0.008]+[0.129])	Prec@1 100.000 (99.660)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2664 (0.2664) ([0.138]+[0.129])	Prec@1 96.094 (96.094)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(18.7077, device='cuda:0')
Epoch: [162][0/391]	Time 0.211 (0.211)	Data 0.179 (0.179)	Loss 0.1974 (0.1974) ([0.069]+[0.129])	Prec@1 97.656 (97.656)
Epoch: [162][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1319 (0.1408) ([0.003]+[0.129])	Prec@1 100.000 (99.667)
Epoch: [162][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1447 (0.1412) ([0.016]+[0.128])	Prec@1 98.438 (99.646)
Epoch: [162][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1370 (0.1405) ([0.009]+[0.128])	Prec@1 100.000 (99.683)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2356 (0.2356) ([0.107]+[0.128])	Prec@1 96.875 (96.875)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.2345, device='cuda:0')
Epoch: [163][0/391]	Time 0.263 (0.263)	Data 0.230 (0.230)	Loss 0.1317 (0.1317) ([0.003]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [163][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1437 (0.1394) ([0.016]+[0.128])	Prec@1 99.219 (99.683)
Epoch: [163][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1351 (0.1393) ([0.007]+[0.128])	Prec@1 100.000 (99.674)
Epoch: [163][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1429 (0.1390) ([0.015]+[0.128])	Prec@1 99.219 (99.683)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.2466 (0.2466) ([0.119]+[0.128])	Prec@1 97.656 (97.656)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.3435, device='cuda:0')
Epoch: [164][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.1317 (0.1317) ([0.004]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [164][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1439 (0.1395) ([0.016]+[0.128])	Prec@1 99.219 (99.660)
Epoch: [164][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1696 (0.1393) ([0.042]+[0.128])	Prec@1 98.438 (99.650)
Epoch: [164][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1330 (0.1389) ([0.005]+[0.128])	Prec@1 100.000 (99.670)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2520 (0.2520) ([0.125]+[0.128])	Prec@1 97.656 (97.656)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(18.8797, device='cuda:0')
Epoch: [165][0/391]	Time 0.204 (0.204)	Data 0.177 (0.177)	Loss 0.1693 (0.1693) ([0.042]+[0.128])	Prec@1 98.438 (98.438)
Epoch: [165][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1436 (0.1382) ([0.016]+[0.127])	Prec@1 99.219 (99.675)
Epoch: [165][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1327 (0.1380) ([0.005]+[0.127])	Prec@1 100.000 (99.677)
Epoch: [165][300/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 0.1415 (0.1373) ([0.014]+[0.127])	Prec@1 100.000 (99.722)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.2381 (0.2381) ([0.111]+[0.127])	Prec@1 96.875 (96.875)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(17.2815, device='cuda:0')
Epoch: [166][0/391]	Time 0.236 (0.236)	Data 0.204 (0.204)	Loss 0.1604 (0.1604) ([0.033]+[0.127])	Prec@1 98.438 (98.438)
Epoch: [166][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1462 (0.1355) ([0.019]+[0.127])	Prec@1 99.219 (99.799)
Epoch: [166][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1288 (0.1359) ([0.002]+[0.127])	Prec@1 100.000 (99.775)
Epoch: [166][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1298 (0.1360) ([0.003]+[0.127])	Prec@1 100.000 (99.764)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2474 (0.2474) ([0.121]+[0.127])	Prec@1 97.656 (97.656)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(0.2852, device='cuda:0')
Epoch: [167][0/391]	Time 0.231 (0.231)	Data 0.199 (0.199)	Loss 0.1296 (0.1296) ([0.003]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [167][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1296 (0.1355) ([0.003]+[0.127])	Prec@1 100.000 (99.737)
Epoch: [167][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1285 (0.1360) ([0.002]+[0.127])	Prec@1 100.000 (99.724)
Epoch: [167][300/391]	Time 0.027 (0.025)	Data 0.000 (0.001)	Loss 0.1286 (0.1355) ([0.002]+[0.127])	Prec@1 100.000 (99.743)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2718 (0.2718) ([0.145]+[0.126])	Prec@1 96.094 (96.094)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(10.5041, device='cuda:0')
Epoch: [168][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.1434 (0.1434) ([0.017]+[0.126])	Prec@1 99.219 (99.219)
Epoch: [168][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1330 (0.1354) ([0.007]+[0.126])	Prec@1 100.000 (99.760)
Epoch: [168][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1305 (0.1351) ([0.004]+[0.126])	Prec@1 100.000 (99.775)
Epoch: [168][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1492 (0.1353) ([0.023]+[0.126])	Prec@1 99.219 (99.766)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.2442 (0.2442) ([0.118]+[0.126])	Prec@1 97.656 (97.656)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(0.2402, device='cuda:0')
Epoch: [169][0/391]	Time 0.266 (0.266)	Data 0.233 (0.233)	Loss 0.1284 (0.1284) ([0.002]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [169][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1408 (0.1330) ([0.015]+[0.126])	Prec@1 99.219 (99.853)
Epoch: [169][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1279 (0.1342) ([0.002]+[0.126])	Prec@1 100.000 (99.806)
Epoch: [169][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1421 (0.1340) ([0.016]+[0.126])	Prec@1 100.000 (99.805)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2395 (0.2395) ([0.114]+[0.126])	Prec@1 97.656 (97.656)
 * Prec@1 93.450
current lr 1.00000e-03
Grad=  tensor(0.2482, device='cuda:0')
Epoch: [170][0/391]	Time 0.215 (0.215)	Data 0.183 (0.183)	Loss 0.1285 (0.1285) ([0.003]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [170][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1363 (0.1330) ([0.011]+[0.126])	Prec@1 100.000 (99.783)
Epoch: [170][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1327 (0.1330) ([0.007]+[0.126])	Prec@1 100.000 (99.798)
Epoch: [170][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1288 (0.1328) ([0.003]+[0.125])	Prec@1 100.000 (99.805)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.2512 (0.2512) ([0.126]+[0.125])	Prec@1 96.094 (96.094)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.5560, device='cuda:0')
Epoch: [171][0/391]	Time 0.219 (0.219)	Data 0.186 (0.186)	Loss 0.1286 (0.1286) ([0.003]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [171][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1404 (0.1345) ([0.015]+[0.125])	Prec@1 99.219 (99.783)
Epoch: [171][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1377 (0.1336) ([0.013]+[0.125])	Prec@1 99.219 (99.775)
Epoch: [171][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1306 (0.1339) ([0.006]+[0.125])	Prec@1 100.000 (99.766)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.2528 (0.2528) ([0.128]+[0.125])	Prec@1 96.875 (96.875)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.3076, device='cuda:0')
Epoch: [172][0/391]	Time 0.224 (0.224)	Data 0.194 (0.194)	Loss 0.1293 (0.1293) ([0.004]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [172][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1257 (0.1336) ([0.001]+[0.125])	Prec@1 100.000 (99.783)
Epoch: [172][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1354 (0.1334) ([0.011]+[0.125])	Prec@1 100.000 (99.817)
Epoch: [172][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1298 (0.1336) ([0.005]+[0.125])	Prec@1 100.000 (99.787)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2220 (0.2220) ([0.097]+[0.125])	Prec@1 97.656 (97.656)
 * Prec@1 93.300
current lr 1.00000e-03
Grad=  tensor(0.2708, device='cuda:0')
Epoch: [173][0/391]	Time 0.252 (0.252)	Data 0.220 (0.220)	Loss 0.1281 (0.1281) ([0.003]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [173][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1307 (0.1318) ([0.006]+[0.125])	Prec@1 100.000 (99.830)
Epoch: [173][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1320 (0.1319) ([0.007]+[0.124])	Prec@1 100.000 (99.825)
Epoch: [173][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1462 (0.1318) ([0.022]+[0.124])	Prec@1 99.219 (99.816)
Test: [0/79]	Time 0.233 (0.233)	Loss 0.2261 (0.2261) ([0.102]+[0.124])	Prec@1 96.094 (96.094)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.1916, device='cuda:0')
Epoch: [174][0/391]	Time 0.256 (0.256)	Data 0.223 (0.223)	Loss 0.1261 (0.1261) ([0.002]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [174][100/391]	Time 0.025 (0.029)	Data 0.000 (0.002)	Loss 0.1277 (0.1305) ([0.004]+[0.124])	Prec@1 100.000 (99.838)
Epoch: [174][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.1371 (0.1317) ([0.013]+[0.124])	Prec@1 99.219 (99.802)
Epoch: [174][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1249 (0.1317) ([0.001]+[0.124])	Prec@1 100.000 (99.795)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2385 (0.2385) ([0.115]+[0.124])	Prec@1 97.656 (97.656)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.2148, device='cuda:0')
Epoch: [175][0/391]	Time 0.202 (0.202)	Data 0.173 (0.173)	Loss 0.1263 (0.1263) ([0.002]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [175][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1275 (0.1308) ([0.004]+[0.124])	Prec@1 100.000 (99.814)
Epoch: [175][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1483 (0.1305) ([0.025]+[0.124])	Prec@1 99.219 (99.825)
Epoch: [175][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1294 (0.1305) ([0.006]+[0.124])	Prec@1 100.000 (99.826)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2479 (0.2479) ([0.124]+[0.124])	Prec@1 96.875 (96.875)
 * Prec@1 93.070
current lr 1.00000e-03
Grad=  tensor(0.2562, device='cuda:0')
Epoch: [176][0/391]	Time 0.217 (0.217)	Data 0.185 (0.185)	Loss 0.1272 (0.1272) ([0.004]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [176][100/391]	Time 0.030 (0.027)	Data 0.000 (0.002)	Loss 0.1250 (0.1300) ([0.002]+[0.124])	Prec@1 100.000 (99.830)
Epoch: [176][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1245 (0.1308) ([0.001]+[0.123])	Prec@1 100.000 (99.806)
Epoch: [176][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1304 (0.1310) ([0.007]+[0.123])	Prec@1 99.219 (99.798)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2552 (0.2552) ([0.132]+[0.123])	Prec@1 95.312 (95.312)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(0.1936, device='cuda:0')
Epoch: [177][0/391]	Time 0.254 (0.254)	Data 0.222 (0.222)	Loss 0.1246 (0.1246) ([0.001]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [177][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1341 (0.1290) ([0.011]+[0.123])	Prec@1 100.000 (99.869)
Epoch: [177][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1517 (0.1290) ([0.029]+[0.123])	Prec@1 99.219 (99.880)
Epoch: [177][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1263 (0.1291) ([0.003]+[0.123])	Prec@1 100.000 (99.870)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.2463 (0.2463) ([0.123]+[0.123])	Prec@1 97.656 (97.656)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.2737, device='cuda:0')
Epoch: [178][0/391]	Time 0.211 (0.211)	Data 0.182 (0.182)	Loss 0.1257 (0.1257) ([0.003]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [178][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1272 (0.1284) ([0.004]+[0.123])	Prec@1 100.000 (99.861)
Epoch: [178][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1322 (0.1287) ([0.009]+[0.123])	Prec@1 99.219 (99.856)
Epoch: [178][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1258 (0.1290) ([0.003]+[0.123])	Prec@1 100.000 (99.836)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2253 (0.2253) ([0.103]+[0.123])	Prec@1 96.875 (96.875)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(0.8425, device='cuda:0')
Epoch: [179][0/391]	Time 0.206 (0.206)	Data 0.174 (0.174)	Loss 0.1284 (0.1284) ([0.006]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [179][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1262 (0.1288) ([0.004]+[0.122])	Prec@1 100.000 (99.830)
Epoch: [179][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1239 (0.1289) ([0.002]+[0.122])	Prec@1 100.000 (99.810)
Epoch: [179][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1242 (0.1289) ([0.002]+[0.122])	Prec@1 100.000 (99.818)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2189 (0.2189) ([0.097]+[0.122])	Prec@1 96.875 (96.875)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(2.8610, device='cuda:0')
Epoch: [180][0/391]	Time 0.216 (0.216)	Data 0.185 (0.185)	Loss 0.1324 (0.1324) ([0.010]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [180][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1369 (0.1284) ([0.015]+[0.122])	Prec@1 99.219 (99.845)
Epoch: [180][200/391]	Time 0.024 (0.027)	Data 0.000 (0.001)	Loss 0.1327 (0.1283) ([0.011]+[0.122])	Prec@1 99.219 (99.837)
Epoch: [180][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1256 (0.1284) ([0.004]+[0.122])	Prec@1 100.000 (99.831)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.2231 (0.2231) ([0.101]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(0.1863, device='cuda:0')
Epoch: [181][0/391]	Time 0.242 (0.242)	Data 0.211 (0.211)	Loss 0.1230 (0.1230) ([0.001]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [181][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1253 (0.1285) ([0.004]+[0.122])	Prec@1 100.000 (99.822)
Epoch: [181][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1282 (0.1291) ([0.007]+[0.122])	Prec@1 100.000 (99.806)
Epoch: [181][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1403 (0.1288) ([0.019]+[0.122])	Prec@1 99.219 (99.818)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.2528 (0.2528) ([0.131]+[0.122])	Prec@1 96.094 (96.094)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.1835, device='cuda:0')
Epoch: [182][0/391]	Time 0.259 (0.259)	Data 0.228 (0.228)	Loss 0.1231 (0.1231) ([0.002]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [182][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1227 (0.1256) ([0.001]+[0.121])	Prec@1 100.000 (99.907)
Epoch: [182][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1221 (0.1267) ([0.001]+[0.121])	Prec@1 100.000 (99.872)
Epoch: [182][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1312 (0.1270) ([0.010]+[0.121])	Prec@1 99.219 (99.865)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2317 (0.2317) ([0.111]+[0.121])	Prec@1 96.875 (96.875)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(1.1650, device='cuda:0')
Epoch: [183][0/391]	Time 0.217 (0.217)	Data 0.185 (0.185)	Loss 0.1273 (0.1273) ([0.006]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [183][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1230 (0.1266) ([0.002]+[0.121])	Prec@1 100.000 (99.876)
Epoch: [183][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1278 (0.1269) ([0.007]+[0.121])	Prec@1 99.219 (99.868)
Epoch: [183][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1219 (0.1275) ([0.001]+[0.121])	Prec@1 100.000 (99.842)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.1868 (0.1868) ([0.066]+[0.121])	Prec@1 98.438 (98.438)
 * Prec@1 93.350
current lr 1.00000e-03
Grad=  tensor(0.2669, device='cuda:0')
Epoch: [184][0/391]	Time 0.257 (0.257)	Data 0.221 (0.221)	Loss 0.1231 (0.1231) ([0.002]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [184][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1291 (0.1262) ([0.008]+[0.121])	Prec@1 100.000 (99.884)
Epoch: [184][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1310 (0.1255) ([0.010]+[0.121])	Prec@1 100.000 (99.887)
Epoch: [184][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1220 (0.1257) ([0.001]+[0.121])	Prec@1 100.000 (99.878)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.1949 (0.1949) ([0.074]+[0.121])	Prec@1 98.438 (98.438)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.6963, device='cuda:0')
Epoch: [185][0/391]	Time 0.205 (0.205)	Data 0.173 (0.173)	Loss 0.1256 (0.1256) ([0.005]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [185][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1215 (0.1264) ([0.001]+[0.120])	Prec@1 100.000 (99.861)
Epoch: [185][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1215 (0.1258) ([0.001]+[0.120])	Prec@1 100.000 (99.864)
Epoch: [185][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1244 (0.1258) ([0.004]+[0.120])	Prec@1 100.000 (99.849)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.1986 (0.1986) ([0.078]+[0.120])	Prec@1 98.438 (98.438)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.1905, device='cuda:0')
Epoch: [186][0/391]	Time 0.200 (0.200)	Data 0.171 (0.171)	Loss 0.1216 (0.1216) ([0.001]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [186][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1214 (0.1247) ([0.001]+[0.120])	Prec@1 100.000 (99.853)
Epoch: [186][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1324 (0.1247) ([0.012]+[0.120])	Prec@1 99.219 (99.864)
Epoch: [186][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1210 (0.1251) ([0.001]+[0.120])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.2073 (0.2073) ([0.088]+[0.120])	Prec@1 97.656 (97.656)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(4.8474, device='cuda:0')
Epoch: [187][0/391]	Time 0.220 (0.220)	Data 0.189 (0.189)	Loss 0.1280 (0.1280) ([0.008]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [187][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1221 (0.1250) ([0.002]+[0.120])	Prec@1 100.000 (99.861)
Epoch: [187][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1209 (0.1255) ([0.001]+[0.120])	Prec@1 100.000 (99.852)
Epoch: [187][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1269 (0.1255) ([0.007]+[0.120])	Prec@1 99.219 (99.844)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2177 (0.2177) ([0.098]+[0.119])	Prec@1 96.875 (96.875)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(1.3643, device='cuda:0')
Epoch: [188][0/391]	Time 0.205 (0.205)	Data 0.171 (0.171)	Loss 0.1291 (0.1291) ([0.010]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [188][100/391]	Time 0.024 (0.028)	Data 0.000 (0.002)	Loss 0.1235 (0.1241) ([0.004]+[0.119])	Prec@1 100.000 (99.876)
Epoch: [188][200/391]	Time 0.023 (0.026)	Data 0.000 (0.001)	Loss 0.1201 (0.1238) ([0.001]+[0.119])	Prec@1 100.000 (99.880)
Epoch: [188][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1237 (0.1241) ([0.004]+[0.119])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2324 (0.2324) ([0.113]+[0.119])	Prec@1 96.875 (96.875)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.4778, device='cuda:0')
Epoch: [189][0/391]	Time 0.258 (0.258)	Data 0.226 (0.226)	Loss 0.1224 (0.1224) ([0.003]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [189][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1243 (0.1238) ([0.005]+[0.119])	Prec@1 100.000 (99.861)
Epoch: [189][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1421 (0.1239) ([0.023]+[0.119])	Prec@1 99.219 (99.868)
Epoch: [189][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1244 (0.1236) ([0.005]+[0.119])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2672 (0.2672) ([0.148]+[0.119])	Prec@1 95.312 (95.312)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.2932, device='cuda:0')
Epoch: [190][0/391]	Time 0.256 (0.256)	Data 0.223 (0.223)	Loss 0.1203 (0.1203) ([0.001]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [190][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1216 (0.1240) ([0.003]+[0.119])	Prec@1 100.000 (99.869)
Epoch: [190][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1201 (0.1242) ([0.001]+[0.119])	Prec@1 100.000 (99.841)
Epoch: [190][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1210 (0.1236) ([0.002]+[0.119])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2239 (0.2239) ([0.105]+[0.118])	Prec@1 97.656 (97.656)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(0.1926, device='cuda:0')
Epoch: [191][0/391]	Time 0.218 (0.218)	Data 0.188 (0.188)	Loss 0.1200 (0.1200) ([0.002]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [191][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1199 (0.1230) ([0.002]+[0.118])	Prec@1 100.000 (99.899)
Epoch: [191][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1195 (0.1228) ([0.001]+[0.118])	Prec@1 100.000 (99.899)
Epoch: [191][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1189 (0.1230) ([0.001]+[0.118])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.1927 (0.1927) ([0.075]+[0.118])	Prec@1 98.438 (98.438)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(2.3283, device='cuda:0')
Epoch: [192][0/391]	Time 0.258 (0.258)	Data 0.223 (0.223)	Loss 0.1261 (0.1261) ([0.008]+[0.118])	Prec@1 99.219 (99.219)
Epoch: [192][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1245 (0.1223) ([0.006]+[0.118])	Prec@1 99.219 (99.861)
Epoch: [192][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1242 (0.1225) ([0.006]+[0.118])	Prec@1 100.000 (99.872)
Epoch: [192][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1240 (0.1226) ([0.006]+[0.118])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.1936 (0.1936) ([0.076]+[0.118])	Prec@1 97.656 (97.656)
 * Prec@1 93.340
current lr 1.00000e-03
Grad=  tensor(0.3099, device='cuda:0')
Epoch: [193][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1202 (0.1202) ([0.002]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [193][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1231 (0.1225) ([0.005]+[0.118])	Prec@1 100.000 (99.869)
Epoch: [193][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1203 (0.1223) ([0.003]+[0.118])	Prec@1 100.000 (99.872)
Epoch: [193][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1181 (0.1221) ([0.001]+[0.118])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.1998 (0.1998) ([0.082]+[0.117])	Prec@1 96.875 (96.875)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.1804, device='cuda:0')
Epoch: [194][0/391]	Time 0.219 (0.219)	Data 0.189 (0.189)	Loss 0.1186 (0.1186) ([0.001]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [194][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1318 (0.1220) ([0.014]+[0.117])	Prec@1 99.219 (99.892)
Epoch: [194][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1180 (0.1215) ([0.001]+[0.117])	Prec@1 100.000 (99.911)
Epoch: [194][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1189 (0.1214) ([0.002]+[0.117])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2263 (0.2263) ([0.109]+[0.117])	Prec@1 96.875 (96.875)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(2.1808, device='cuda:0')
Epoch: [195][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.1247 (0.1247) ([0.008]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [195][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1180 (0.1202) ([0.001]+[0.117])	Prec@1 100.000 (99.930)
Epoch: [195][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1176 (0.1208) ([0.001]+[0.117])	Prec@1 100.000 (99.891)
Epoch: [195][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1179 (0.1209) ([0.001]+[0.117])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.2436 (0.2436) ([0.127]+[0.117])	Prec@1 96.094 (96.094)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(1.3773, device='cuda:0')
Epoch: [196][0/391]	Time 0.210 (0.210)	Data 0.178 (0.178)	Loss 0.1201 (0.1201) ([0.003]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [196][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1178 (0.1205) ([0.001]+[0.117])	Prec@1 100.000 (99.899)
Epoch: [196][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1360 (0.1207) ([0.019]+[0.117])	Prec@1 99.219 (99.891)
Epoch: [196][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1176 (0.1205) ([0.001]+[0.117])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.2188 (0.2188) ([0.102]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(0.2080, device='cuda:0')
Epoch: [197][0/391]	Time 0.251 (0.251)	Data 0.214 (0.214)	Loss 0.1174 (0.1174) ([0.001]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [197][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1173 (0.1203) ([0.001]+[0.116])	Prec@1 100.000 (99.899)
Epoch: [197][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1175 (0.1211) ([0.001]+[0.116])	Prec@1 100.000 (99.876)
Epoch: [197][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1168 (0.1214) ([0.001]+[0.116])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2688 (0.2688) ([0.153]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.2401, device='cuda:0')
Epoch: [198][0/391]	Time 0.215 (0.215)	Data 0.184 (0.184)	Loss 0.1174 (0.1174) ([0.001]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [198][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1170 (0.1205) ([0.001]+[0.116])	Prec@1 100.000 (99.899)
Epoch: [198][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1165 (0.1201) ([0.001]+[0.116])	Prec@1 100.000 (99.903)
Epoch: [198][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1166 (0.1201) ([0.001]+[0.116])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2725 (0.2725) ([0.157]+[0.116])	Prec@1 96.094 (96.094)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.2129, device='cuda:0')
Epoch: [199][0/391]	Time 0.261 (0.261)	Data 0.227 (0.227)	Loss 0.1172 (0.1172) ([0.001]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [199][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1167 (0.1192) ([0.001]+[0.116])	Prec@1 100.000 (99.954)
Epoch: [199][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1167 (0.1196) ([0.001]+[0.116])	Prec@1 100.000 (99.922)
Epoch: [199][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1216 (0.1201) ([0.006]+[0.116])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2578 (0.2578) ([0.142]+[0.116])	Prec@1 95.312 (95.312)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(2.6101, device='cuda:0')
Epoch: [200][0/391]	Time 0.216 (0.216)	Data 0.185 (0.185)	Loss 0.1225 (0.1225) ([0.007]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [200][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1214 (0.1192) ([0.006]+[0.115])	Prec@1 100.000 (99.907)
Epoch: [200][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1162 (0.1200) ([0.001]+[0.115])	Prec@1 100.000 (99.880)
Epoch: [200][300/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.1164 (0.1201) ([0.001]+[0.115])	Prec@1 100.000 (99.878)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2847 (0.2847) ([0.170]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 92.970
current lr 1.00000e-03
Grad=  tensor(0.1808, device='cuda:0')
Epoch: [201][0/391]	Time 0.198 (0.198)	Data 0.168 (0.168)	Loss 0.1161 (0.1161) ([0.001]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [201][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1238 (0.1186) ([0.009]+[0.115])	Prec@1 100.000 (99.923)
Epoch: [201][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1157 (0.1187) ([0.001]+[0.115])	Prec@1 100.000 (99.911)
Epoch: [201][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1350 (0.1190) ([0.020]+[0.115])	Prec@1 99.219 (99.904)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2525 (0.2525) ([0.138]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.2015, device='cuda:0')
Epoch: [202][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1161 (0.1161) ([0.001]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [202][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1221 (0.1186) ([0.007]+[0.115])	Prec@1 100.000 (99.899)
Epoch: [202][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1160 (0.1187) ([0.001]+[0.115])	Prec@1 100.000 (99.891)
Epoch: [202][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1152 (0.1190) ([0.001]+[0.115])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2745 (0.2745) ([0.160]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.1768, device='cuda:0')
Epoch: [203][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1153 (0.1153) ([0.001]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [203][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1694 (0.1194) ([0.055]+[0.114])	Prec@1 98.438 (99.876)
Epoch: [203][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1153 (0.1190) ([0.001]+[0.114])	Prec@1 100.000 (99.876)
Epoch: [203][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1169 (0.1192) ([0.003]+[0.114])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.2123 (0.2123) ([0.098]+[0.114])	Prec@1 96.875 (96.875)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.2014, device='cuda:0')
Epoch: [204][0/391]	Time 0.204 (0.204)	Data 0.172 (0.172)	Loss 0.1154 (0.1154) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [204][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1152 (0.1177) ([0.001]+[0.114])	Prec@1 100.000 (99.923)
Epoch: [204][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1160 (0.1179) ([0.002]+[0.114])	Prec@1 100.000 (99.911)
Epoch: [204][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1154 (0.1178) ([0.001]+[0.114])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2494 (0.2494) ([0.135]+[0.114])	Prec@1 95.312 (95.312)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(0.2388, device='cuda:0')
Epoch: [205][0/391]	Time 0.218 (0.218)	Data 0.179 (0.179)	Loss 0.1150 (0.1150) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [205][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1151 (0.1187) ([0.001]+[0.114])	Prec@1 100.000 (99.907)
Epoch: [205][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1191 (0.1179) ([0.005]+[0.114])	Prec@1 100.000 (99.918)
Epoch: [205][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1155 (0.1180) ([0.002]+[0.114])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2306 (0.2306) ([0.117]+[0.114])	Prec@1 96.875 (96.875)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.2308, device='cuda:0')
Epoch: [206][0/391]	Time 0.249 (0.249)	Data 0.217 (0.217)	Loss 0.1148 (0.1148) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [206][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.1396 (0.1173) ([0.026]+[0.113])	Prec@1 98.438 (99.923)
Epoch: [206][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1177 (0.1175) ([0.004]+[0.113])	Prec@1 100.000 (99.907)
Epoch: [206][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1146 (0.1175) ([0.001]+[0.113])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2263 (0.2263) ([0.113]+[0.113])	Prec@1 96.875 (96.875)
 * Prec@1 93.040
current lr 1.00000e-03
Grad=  tensor(0.1822, device='cuda:0')
Epoch: [207][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.1139 (0.1139) ([0.001]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [207][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1408 (0.1183) ([0.028]+[0.113])	Prec@1 99.219 (99.861)
Epoch: [207][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1142 (0.1178) ([0.001]+[0.113])	Prec@1 100.000 (99.880)
Epoch: [207][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1177 (0.1177) ([0.005]+[0.113])	Prec@1 100.000 (99.870)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2436 (0.2436) ([0.131]+[0.113])	Prec@1 95.312 (95.312)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.1802, device='cuda:0')
Epoch: [208][0/391]	Time 0.259 (0.259)	Data 0.227 (0.227)	Loss 0.1137 (0.1137) ([0.001]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [208][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1140 (0.1183) ([0.001]+[0.113])	Prec@1 100.000 (99.845)
Epoch: [208][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1136 (0.1172) ([0.001]+[0.113])	Prec@1 100.000 (99.883)
Epoch: [208][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1163 (0.1169) ([0.004]+[0.113])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2994 (0.2994) ([0.187]+[0.113])	Prec@1 96.094 (96.094)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.2109, device='cuda:0')
Epoch: [209][0/391]	Time 0.251 (0.251)	Data 0.215 (0.215)	Loss 0.1137 (0.1137) ([0.001]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [209][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1130 (0.1181) ([0.000]+[0.113])	Prec@1 100.000 (99.861)
Epoch: [209][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1155 (0.1179) ([0.003]+[0.112])	Prec@1 100.000 (99.868)
Epoch: [209][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1256 (0.1173) ([0.013]+[0.112])	Prec@1 99.219 (99.886)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.2834 (0.2834) ([0.171]+[0.112])	Prec@1 95.312 (95.312)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(11.8011, device='cuda:0')
Epoch: [210][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.1208 (0.1208) ([0.008]+[0.112])	Prec@1 99.219 (99.219)
Epoch: [210][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1154 (0.1150) ([0.003]+[0.112])	Prec@1 100.000 (99.954)
Epoch: [210][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1187 (0.1154) ([0.007]+[0.112])	Prec@1 100.000 (99.930)
Epoch: [210][300/391]	Time 0.026 (0.024)	Data 0.000 (0.001)	Loss 0.1252 (0.1155) ([0.013]+[0.112])	Prec@1 99.219 (99.917)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.2824 (0.2824) ([0.170]+[0.112])	Prec@1 94.531 (94.531)
 * Prec@1 93.350
current lr 1.00000e-03
Grad=  tensor(0.1947, device='cuda:0')
Epoch: [211][0/391]	Time 0.249 (0.249)	Data 0.217 (0.217)	Loss 0.1133 (0.1133) ([0.001]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [211][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1133 (0.1150) ([0.001]+[0.112])	Prec@1 100.000 (99.923)
Epoch: [211][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1171 (0.1151) ([0.005]+[0.112])	Prec@1 100.000 (99.918)
Epoch: [211][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1131 (0.1155) ([0.001]+[0.112])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.3152 (0.3152) ([0.204]+[0.112])	Prec@1 95.312 (95.312)
 * Prec@1 93.320
current lr 1.00000e-03
Grad=  tensor(0.1790, device='cuda:0')
Epoch: [212][0/391]	Time 0.214 (0.214)	Data 0.184 (0.184)	Loss 0.1123 (0.1123) ([0.001]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [212][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1181 (0.1161) ([0.006]+[0.112])	Prec@1 99.219 (99.876)
Epoch: [212][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1159 (0.1168) ([0.004]+[0.112])	Prec@1 100.000 (99.856)
Epoch: [212][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.1191 (0.1163) ([0.008]+[0.111])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.3201 (0.3201) ([0.209]+[0.111])	Prec@1 94.531 (94.531)
 * Prec@1 93.120
current lr 1.00000e-03
Grad=  tensor(0.7788, device='cuda:0')
Epoch: [213][0/391]	Time 0.236 (0.236)	Data 0.205 (0.205)	Loss 0.1140 (0.1140) ([0.003]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [213][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1130 (0.1153) ([0.002]+[0.111])	Prec@1 100.000 (99.915)
Epoch: [213][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1139 (0.1156) ([0.003]+[0.111])	Prec@1 100.000 (99.883)
Epoch: [213][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1138 (0.1155) ([0.003]+[0.111])	Prec@1 100.000 (99.878)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.2808 (0.2808) ([0.170]+[0.111])	Prec@1 93.750 (93.750)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(0.1762, device='cuda:0')
Epoch: [214][0/391]	Time 0.199 (0.199)	Data 0.171 (0.171)	Loss 0.1116 (0.1116) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [214][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1119 (0.1148) ([0.001]+[0.111])	Prec@1 100.000 (99.923)
Epoch: [214][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1322 (0.1151) ([0.021]+[0.111])	Prec@1 99.219 (99.903)
Epoch: [214][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1126 (0.1149) ([0.002]+[0.111])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.2684 (0.2684) ([0.158]+[0.111])	Prec@1 94.531 (94.531)
 * Prec@1 92.920
current lr 1.00000e-03
Grad=  tensor(0.1894, device='cuda:0')
Epoch: [215][0/391]	Time 0.263 (0.263)	Data 0.231 (0.231)	Loss 0.1116 (0.1116) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [215][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1169 (0.1146) ([0.006]+[0.111])	Prec@1 100.000 (99.899)
Epoch: [215][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1121 (0.1144) ([0.002]+[0.111])	Prec@1 100.000 (99.895)
Epoch: [215][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1134 (0.1152) ([0.003]+[0.110])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.2625 (0.2625) ([0.152]+[0.110])	Prec@1 95.312 (95.312)
 * Prec@1 93.100
current lr 1.00000e-03
Grad=  tensor(0.3259, device='cuda:0')
Epoch: [216][0/391]	Time 0.222 (0.222)	Data 0.191 (0.191)	Loss 0.1119 (0.1119) ([0.001]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [216][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1116 (0.1144) ([0.001]+[0.110])	Prec@1 100.000 (99.853)
Epoch: [216][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1145 (0.1142) ([0.004]+[0.110])	Prec@1 100.000 (99.868)
Epoch: [216][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1115 (0.1143) ([0.001]+[0.110])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.2425 (0.2425) ([0.132]+[0.110])	Prec@1 96.875 (96.875)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.1822, device='cuda:0')
Epoch: [217][0/391]	Time 0.254 (0.254)	Data 0.220 (0.220)	Loss 0.1114 (0.1114) ([0.001]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [217][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1109 (0.1141) ([0.001]+[0.110])	Prec@1 100.000 (99.876)
Epoch: [217][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.1108 (0.1140) ([0.001]+[0.110])	Prec@1 100.000 (99.887)
Epoch: [217][300/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.1171 (0.1137) ([0.007]+[0.110])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2569 (0.2569) ([0.147]+[0.110])	Prec@1 96.094 (96.094)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.4532, device='cuda:0')
Epoch: [218][0/391]	Time 0.237 (0.237)	Data 0.205 (0.205)	Loss 0.1128 (0.1128) ([0.003]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [218][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1110 (0.1134) ([0.001]+[0.110])	Prec@1 100.000 (99.915)
Epoch: [218][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1194 (0.1135) ([0.010]+[0.110])	Prec@1 99.219 (99.887)
Epoch: [218][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1105 (0.1134) ([0.001]+[0.110])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.2426 (0.2426) ([0.133]+[0.109])	Prec@1 95.312 (95.312)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.1762, device='cuda:0')
Epoch: [219][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.1101 (0.1101) ([0.001]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [219][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1155 (0.1131) ([0.006]+[0.109])	Prec@1 100.000 (99.930)
Epoch: [219][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1280 (0.1134) ([0.019]+[0.109])	Prec@1 99.219 (99.903)
Epoch: [219][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1123 (0.1137) ([0.003]+[0.109])	Prec@1 100.000 (99.886)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2614 (0.2614) ([0.152]+[0.109])	Prec@1 95.312 (95.312)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(0.1795, device='cuda:0')
Epoch: [220][0/391]	Time 0.203 (0.203)	Data 0.172 (0.172)	Loss 0.1098 (0.1098) ([0.001]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [220][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1103 (0.1126) ([0.001]+[0.109])	Prec@1 100.000 (99.899)
Epoch: [220][200/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.1230 (0.1130) ([0.014]+[0.109])	Prec@1 99.219 (99.891)
Epoch: [220][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1117 (0.1128) ([0.003]+[0.109])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2406 (0.2406) ([0.132]+[0.109])	Prec@1 94.531 (94.531)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.4283, device='cuda:0')
Epoch: [221][0/391]	Time 0.225 (0.225)	Data 0.193 (0.193)	Loss 0.1111 (0.1111) ([0.002]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [221][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1092 (0.1128) ([0.000]+[0.109])	Prec@1 100.000 (99.899)
Epoch: [221][200/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.1097 (0.1132) ([0.001]+[0.109])	Prec@1 100.000 (99.880)
Epoch: [221][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1095 (0.1130) ([0.001]+[0.109])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2656 (0.2656) ([0.157]+[0.109])	Prec@1 95.312 (95.312)
 * Prec@1 93.160
current lr 1.00000e-03
Grad=  tensor(0.5412, device='cuda:0')
Epoch: [222][0/391]	Time 0.222 (0.222)	Data 0.185 (0.185)	Loss 0.1110 (0.1110) ([0.002]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [222][100/391]	Time 0.029 (0.029)	Data 0.000 (0.002)	Loss 0.1110 (0.1111) ([0.003]+[0.108])	Prec@1 100.000 (99.961)
Epoch: [222][200/391]	Time 0.025 (0.028)	Data 0.000 (0.001)	Loss 0.1111 (0.1115) ([0.003]+[0.108])	Prec@1 100.000 (99.942)
Epoch: [222][300/391]	Time 0.024 (0.027)	Data 0.000 (0.001)	Loss 0.1149 (0.1119) ([0.007]+[0.108])	Prec@1 99.219 (99.917)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2546 (0.2546) ([0.146]+[0.108])	Prec@1 94.531 (94.531)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.1779, device='cuda:0')
Epoch: [223][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.1089 (0.1089) ([0.001]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [223][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1092 (0.1118) ([0.001]+[0.108])	Prec@1 100.000 (99.892)
Epoch: [223][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1088 (0.1114) ([0.001]+[0.108])	Prec@1 100.000 (99.918)
Epoch: [223][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.1085 (0.1113) ([0.000]+[0.108])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2415 (0.2415) ([0.134]+[0.108])	Prec@1 96.094 (96.094)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.1902, device='cuda:0')
Epoch: [224][0/391]	Time 0.199 (0.199)	Data 0.169 (0.169)	Loss 0.1092 (0.1092) ([0.001]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [224][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1085 (0.1106) ([0.001]+[0.108])	Prec@1 100.000 (99.961)
Epoch: [224][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1089 (0.1113) ([0.001]+[0.108])	Prec@1 100.000 (99.914)
Epoch: [224][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1089 (0.1113) ([0.001]+[0.108])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.2564 (0.2564) ([0.149]+[0.108])	Prec@1 93.750 (93.750)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.1775, device='cuda:0')
Epoch: [225][0/391]	Time 0.264 (0.264)	Data 0.229 (0.229)	Loss 0.1085 (0.1085) ([0.001]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [225][100/391]	Time 0.030 (0.028)	Data 0.000 (0.002)	Loss 0.1083 (0.1103) ([0.001]+[0.108])	Prec@1 100.000 (99.946)
Epoch: [225][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1083 (0.1105) ([0.001]+[0.107])	Prec@1 100.000 (99.926)
Epoch: [225][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1111 (0.1106) ([0.004]+[0.107])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3338 (0.3338) ([0.226]+[0.107])	Prec@1 94.531 (94.531)
 * Prec@1 93.410
current lr 1.00000e-03
Grad=  tensor(12.6304, device='cuda:0')
Epoch: [226][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.1237 (0.1237) ([0.016]+[0.107])	Prec@1 99.219 (99.219)
Epoch: [226][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1124 (0.1117) ([0.005]+[0.107])	Prec@1 100.000 (99.876)
Epoch: [226][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1086 (0.1114) ([0.001]+[0.107])	Prec@1 100.000 (99.883)
Epoch: [226][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1078 (0.1113) ([0.001]+[0.107])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2961 (0.2961) ([0.189]+[0.107])	Prec@1 95.312 (95.312)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.4283, device='cuda:0')
Epoch: [227][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.1087 (0.1087) ([0.002]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [227][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1078 (0.1111) ([0.001]+[0.107])	Prec@1 100.000 (99.892)
Epoch: [227][200/391]	Time 0.025 (0.028)	Data 0.000 (0.001)	Loss 0.1156 (0.1110) ([0.009]+[0.107])	Prec@1 100.000 (99.903)
Epoch: [227][300/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1115 (0.1109) ([0.005]+[0.107])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.3205 (0.3205) ([0.214]+[0.107])	Prec@1 94.531 (94.531)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.1770, device='cuda:0')
Epoch: [228][0/391]	Time 0.258 (0.258)	Data 0.226 (0.226)	Loss 0.1074 (0.1074) ([0.001]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [228][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1077 (0.1114) ([0.001]+[0.107])	Prec@1 100.000 (99.845)
Epoch: [228][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1128 (0.1106) ([0.006]+[0.107])	Prec@1 100.000 (99.876)
Epoch: [228][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1199 (0.1107) ([0.013]+[0.106])	Prec@1 99.219 (99.868)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.3006 (0.3006) ([0.194]+[0.106])	Prec@1 94.531 (94.531)
 * Prec@1 93.100
current lr 1.00000e-03
Grad=  tensor(0.4943, device='cuda:0')
Epoch: [229][0/391]	Time 0.219 (0.219)	Data 0.188 (0.188)	Loss 0.1092 (0.1092) ([0.003]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [229][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1069 (0.1096) ([0.001]+[0.106])	Prec@1 100.000 (99.923)
Epoch: [229][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1088 (0.1094) ([0.002]+[0.106])	Prec@1 100.000 (99.922)
Epoch: [229][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1077 (0.1096) ([0.001]+[0.106])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.2936 (0.2936) ([0.188]+[0.106])	Prec@1 94.531 (94.531)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(6.7677, device='cuda:0')
Epoch: [230][0/391]	Time 0.235 (0.235)	Data 0.206 (0.206)	Loss 0.1127 (0.1127) ([0.007]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [230][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1067 (0.1090) ([0.001]+[0.106])	Prec@1 100.000 (99.930)
Epoch: [230][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1066 (0.1093) ([0.001]+[0.106])	Prec@1 100.000 (99.914)
Epoch: [230][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1162 (0.1094) ([0.010]+[0.106])	Prec@1 99.219 (99.914)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3383 (0.3383) ([0.232]+[0.106])	Prec@1 95.312 (95.312)
 * Prec@1 93.030
current lr 1.00000e-03
Grad=  tensor(0.1936, device='cuda:0')
Epoch: [231][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.1067 (0.1067) ([0.001]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [231][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1077 (0.1087) ([0.002]+[0.106])	Prec@1 100.000 (99.954)
Epoch: [231][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1070 (0.1084) ([0.001]+[0.106])	Prec@1 100.000 (99.961)
Epoch: [231][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1086 (0.1088) ([0.003]+[0.106])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.2438 (0.2438) ([0.138]+[0.105])	Prec@1 95.312 (95.312)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.3708, device='cuda:0')
Epoch: [232][0/391]	Time 0.234 (0.234)	Data 0.204 (0.204)	Loss 0.1076 (0.1076) ([0.002]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [232][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1060 (0.1097) ([0.001]+[0.105])	Prec@1 100.000 (99.892)
Epoch: [232][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1064 (0.1094) ([0.001]+[0.105])	Prec@1 100.000 (99.891)
Epoch: [232][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1058 (0.1094) ([0.001]+[0.105])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2859 (0.2859) ([0.181]+[0.105])	Prec@1 93.750 (93.750)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(26.2557, device='cuda:0')
Epoch: [233][0/391]	Time 0.217 (0.217)	Data 0.188 (0.188)	Loss 0.1133 (0.1133) ([0.008]+[0.105])	Prec@1 99.219 (99.219)
Epoch: [233][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1060 (0.1089) ([0.001]+[0.105])	Prec@1 100.000 (99.907)
Epoch: [233][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1056 (0.1088) ([0.001]+[0.105])	Prec@1 100.000 (99.895)
Epoch: [233][300/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.1059 (0.1089) ([0.001]+[0.105])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2492 (0.2492) ([0.144]+[0.105])	Prec@1 95.312 (95.312)
 * Prec@1 92.870
current lr 1.00000e-03
Grad=  tensor(0.1831, device='cuda:0')
Epoch: [234][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.1057 (0.1057) ([0.001]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [234][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1096 (0.1089) ([0.005]+[0.105])	Prec@1 100.000 (99.907)
Epoch: [234][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1110 (0.1085) ([0.006]+[0.105])	Prec@1 100.000 (99.918)
Epoch: [234][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1052 (0.1086) ([0.001]+[0.105])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3366 (0.3366) ([0.232]+[0.105])	Prec@1 92.969 (92.969)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(17.6563, device='cuda:0')
Epoch: [235][0/391]	Time 0.203 (0.203)	Data 0.174 (0.174)	Loss 0.1233 (0.1233) ([0.019]+[0.105])	Prec@1 99.219 (99.219)
Epoch: [235][100/391]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 0.1054 (0.1094) ([0.001]+[0.104])	Prec@1 100.000 (99.869)
Epoch: [235][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1050 (0.1088) ([0.001]+[0.104])	Prec@1 100.000 (99.876)
Epoch: [235][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1054 (0.1084) ([0.001]+[0.104])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.3083 (0.3083) ([0.204]+[0.104])	Prec@1 95.312 (95.312)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(0.1804, device='cuda:0')
Epoch: [236][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1050 (0.1050) ([0.001]+[0.104])	Prec@1 100.000 (100.000)
Epoch: [236][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1050 (0.1075) ([0.001]+[0.104])	Prec@1 100.000 (99.930)
Epoch: [236][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1052 (0.1073) ([0.001]+[0.104])	Prec@1 100.000 (99.918)
Epoch: [236][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1146 (0.1078) ([0.011]+[0.104])	Prec@1 99.219 (99.894)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.2516 (0.2516) ([0.148]+[0.104])	Prec@1 96.875 (96.875)
 * Prec@1 93.080
current lr 1.00000e-03
Grad=  tensor(29.3068, device='cuda:0')
Epoch: [237][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.1218 (0.1218) ([0.018]+[0.104])	Prec@1 99.219 (99.219)
Epoch: [237][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1060 (0.1095) ([0.002]+[0.104])	Prec@1 100.000 (99.869)
Epoch: [237][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1046 (0.1091) ([0.001]+[0.104])	Prec@1 100.000 (99.860)
Epoch: [237][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1061 (0.1085) ([0.002]+[0.104])	Prec@1 100.000 (99.868)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.3269 (0.3269) ([0.223]+[0.104])	Prec@1 96.094 (96.094)
 * Prec@1 92.960
current lr 1.00000e-03
Grad=  tensor(0.1789, device='cuda:0')
Epoch: [238][0/391]	Time 0.254 (0.254)	Data 0.219 (0.219)	Loss 0.1043 (0.1043) ([0.001]+[0.104])	Prec@1 100.000 (100.000)
Epoch: [238][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1268 (0.1077) ([0.023]+[0.104])	Prec@1 99.219 (99.923)
Epoch: [238][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1096 (0.1079) ([0.006]+[0.104])	Prec@1 100.000 (99.911)
Epoch: [238][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1044 (0.1078) ([0.001]+[0.103])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.3245 (0.3245) ([0.221]+[0.103])	Prec@1 94.531 (94.531)
 * Prec@1 92.770
current lr 1.00000e-03
Grad=  tensor(9.4874, device='cuda:0')
Epoch: [239][0/391]	Time 0.222 (0.222)	Data 0.191 (0.191)	Loss 0.1105 (0.1105) ([0.007]+[0.103])	Prec@1 100.000 (100.000)
Epoch: [239][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1043 (0.1076) ([0.001]+[0.103])	Prec@1 100.000 (99.899)
Epoch: [239][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1040 (0.1071) ([0.001]+[0.103])	Prec@1 100.000 (99.907)
Epoch: [239][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1051 (0.1070) ([0.002]+[0.103])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.3318 (0.3318) ([0.229]+[0.103])	Prec@1 95.312 (95.312)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.5091, device='cuda:0')
Epoch: [240][0/391]	Time 0.258 (0.258)	Data 0.220 (0.220)	Loss 0.1066 (0.1066) ([0.003]+[0.103])	Prec@1 100.000 (100.000)
Epoch: [240][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1038 (0.1071) ([0.001]+[0.103])	Prec@1 100.000 (99.923)
Epoch: [240][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1037 (0.1074) ([0.001]+[0.103])	Prec@1 100.000 (99.891)
Epoch: [240][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1041 (0.1069) ([0.001]+[0.103])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.2794 (0.2794) ([0.177]+[0.103])	Prec@1 96.094 (96.094)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(0.4439, device='cuda:0')
Epoch: [241][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.1047 (0.1047) ([0.002]+[0.103])	Prec@1 100.000 (100.000)
Epoch: [241][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1075 (0.1062) ([0.005]+[0.103])	Prec@1 100.000 (99.876)
Epoch: [241][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1040 (0.1065) ([0.001]+[0.103])	Prec@1 100.000 (99.872)
Epoch: [241][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1052 (0.1067) ([0.003]+[0.103])	Prec@1 100.000 (99.881)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.3049 (0.3049) ([0.202]+[0.102])	Prec@1 94.531 (94.531)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.1769, device='cuda:0')
Epoch: [242][0/391]	Time 0.236 (0.236)	Data 0.200 (0.200)	Loss 0.1031 (0.1031) ([0.001]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [242][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1429 (0.1062) ([0.041]+[0.102])	Prec@1 99.219 (99.899)
Epoch: [242][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1318 (0.1066) ([0.029]+[0.102])	Prec@1 98.438 (99.883)
Epoch: [242][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1044 (0.1064) ([0.002]+[0.102])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.3798 (0.3798) ([0.278]+[0.102])	Prec@1 92.969 (92.969)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(0.4735, device='cuda:0')
Epoch: [243][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1038 (0.1038) ([0.002]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [243][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1079 (0.1063) ([0.006]+[0.102])	Prec@1 100.000 (99.876)
Epoch: [243][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1052 (0.1065) ([0.003]+[0.102])	Prec@1 100.000 (99.864)
Epoch: [243][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1062 (0.1070) ([0.004]+[0.102])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.3073 (0.3073) ([0.205]+[0.102])	Prec@1 94.531 (94.531)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(2.8412, device='cuda:0')
Epoch: [244][0/391]	Time 0.220 (0.220)	Data 0.189 (0.189)	Loss 0.1066 (0.1066) ([0.005]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [244][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1034 (0.1054) ([0.002]+[0.102])	Prec@1 100.000 (99.946)
Epoch: [244][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1031 (0.1055) ([0.001]+[0.102])	Prec@1 100.000 (99.918)
Epoch: [244][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1072 (0.1054) ([0.005]+[0.102])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.2784 (0.2784) ([0.177]+[0.102])	Prec@1 95.312 (95.312)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(0.1903, device='cuda:0')
Epoch: [245][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.1028 (0.1028) ([0.001]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [245][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1024 (0.1053) ([0.001]+[0.102])	Prec@1 100.000 (99.899)
Epoch: [245][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1022 (0.1053) ([0.001]+[0.101])	Prec@1 100.000 (99.895)
Epoch: [245][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1077 (0.1053) ([0.006]+[0.101])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.2609 (0.2609) ([0.160]+[0.101])	Prec@1 96.094 (96.094)
 * Prec@1 92.850
current lr 1.00000e-03
Grad=  tensor(0.2637, device='cuda:0')
Epoch: [246][0/391]	Time 0.211 (0.211)	Data 0.182 (0.182)	Loss 0.1027 (0.1027) ([0.001]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [246][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1060 (0.1052) ([0.005]+[0.101])	Prec@1 100.000 (99.892)
Epoch: [246][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1027 (0.1054) ([0.002]+[0.101])	Prec@1 100.000 (99.880)
Epoch: [246][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1021 (0.1054) ([0.001]+[0.101])	Prec@1 100.000 (99.875)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.2939 (0.2939) ([0.193]+[0.101])	Prec@1 94.531 (94.531)
 * Prec@1 92.810
current lr 1.00000e-03
Grad=  tensor(0.8378, device='cuda:0')
Epoch: [247][0/391]	Time 0.214 (0.214)	Data 0.185 (0.185)	Loss 0.1033 (0.1033) ([0.002]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [247][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1028 (0.1049) ([0.002]+[0.101])	Prec@1 100.000 (99.907)
Epoch: [247][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1016 (0.1044) ([0.001]+[0.101])	Prec@1 100.000 (99.930)
Epoch: [247][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1031 (0.1044) ([0.002]+[0.101])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.2519 (0.2519) ([0.151]+[0.101])	Prec@1 95.312 (95.312)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.2800, device='cuda:0')
Epoch: [248][0/391]	Time 0.254 (0.254)	Data 0.221 (0.221)	Loss 0.1027 (0.1027) ([0.002]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [248][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1018 (0.1036) ([0.001]+[0.101])	Prec@1 100.000 (99.907)
Epoch: [248][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1029 (0.1037) ([0.002]+[0.101])	Prec@1 100.000 (99.911)
Epoch: [248][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1018 (0.1040) ([0.001]+[0.101])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.3306 (0.3306) ([0.230]+[0.100])	Prec@1 92.969 (92.969)
 * Prec@1 93.060
current lr 1.00000e-03
Grad=  tensor(27.0143, device='cuda:0')
Epoch: [249][0/391]	Time 0.213 (0.213)	Data 0.181 (0.181)	Loss 0.1379 (0.1379) ([0.037]+[0.100])	Prec@1 99.219 (99.219)
Epoch: [249][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1012 (0.1049) ([0.001]+[0.100])	Prec@1 100.000 (99.876)
Epoch: [249][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1012 (0.1044) ([0.001]+[0.100])	Prec@1 100.000 (99.891)
Epoch: [249][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1011 (0.1042) ([0.001]+[0.100])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2769 (0.2769) ([0.177]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.100
current lr 1.00000e-04
Grad=  tensor(0.1892, device='cuda:0')
Epoch: [250][0/391]	Time 0.216 (0.216)	Data 0.187 (0.187)	Loss 0.1012 (0.1012) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [250][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1009 (0.1042) ([0.001]+[0.100])	Prec@1 100.000 (99.869)
Epoch: [250][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1021 (0.1036) ([0.002]+[0.100])	Prec@1 100.000 (99.899)
Epoch: [250][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1018 (0.1035) ([0.002]+[0.100])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.2527 (0.2527) ([0.153]+[0.100])	Prec@1 95.312 (95.312)
 * Prec@1 93.190
current lr 1.00000e-04
Grad=  tensor(64.8956, device='cuda:0')
Epoch: [251][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.1579 (0.1579) ([0.058]+[0.100])	Prec@1 98.438 (98.438)
Epoch: [251][100/391]	Time 0.029 (0.026)	Data 0.000 (0.002)	Loss 0.1044 (0.1041) ([0.004]+[0.100])	Prec@1 100.000 (99.915)
Epoch: [251][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1024 (0.1039) ([0.002]+[0.100])	Prec@1 100.000 (99.907)
Epoch: [251][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1019 (0.1036) ([0.002]+[0.100])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2848 (0.2848) ([0.185]+[0.100])	Prec@1 95.312 (95.312)
 * Prec@1 93.100
current lr 1.00000e-04
Grad=  tensor(0.3805, device='cuda:0')
Epoch: [252][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.1018 (0.1018) ([0.002]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [252][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1018 (0.1033) ([0.002]+[0.100])	Prec@1 100.000 (99.930)
Epoch: [252][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1009 (0.1032) ([0.001]+[0.100])	Prec@1 100.000 (99.934)
Epoch: [252][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1006 (0.1031) ([0.001]+[0.100])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.2904 (0.2904) ([0.190]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.220
current lr 1.00000e-04
Grad=  tensor(6.4757, device='cuda:0')
Epoch: [253][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.1052 (0.1052) ([0.005]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [253][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1004 (0.1036) ([0.000]+[0.100])	Prec@1 100.000 (99.892)
Epoch: [253][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1010 (0.1034) ([0.001]+[0.100])	Prec@1 100.000 (99.911)
Epoch: [253][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1008 (0.1031) ([0.001]+[0.100])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2811 (0.2811) ([0.181]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.210
current lr 1.00000e-04
Grad=  tensor(0.2340, device='cuda:0')
Epoch: [254][0/391]	Time 0.215 (0.215)	Data 0.178 (0.178)	Loss 0.1009 (0.1009) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1007 (0.1026) ([0.001]+[0.100])	Prec@1 100.000 (99.915)
Epoch: [254][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1010 (0.1026) ([0.001]+[0.100])	Prec@1 100.000 (99.930)
Epoch: [254][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1004 (0.1025) ([0.000]+[0.100])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.2602 (0.2602) ([0.160]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.110
current lr 1.00000e-04
Grad=  tensor(0.1768, device='cuda:0')
Epoch: [255][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.1005 (0.1005) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1020 (0.1024) ([0.002]+[0.100])	Prec@1 100.000 (99.938)
Epoch: [255][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1005 (0.1022) ([0.001]+[0.100])	Prec@1 100.000 (99.949)
Epoch: [255][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1005 (0.1021) ([0.001]+[0.100])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2652 (0.2652) ([0.165]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.180
current lr 1.00000e-04
Grad=  tensor(0.2414, device='cuda:0')
Epoch: [256][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.1010 (0.1010) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1009 (0.1019) ([0.001]+[0.100])	Prec@1 100.000 (99.961)
Epoch: [256][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1006 (0.1021) ([0.001]+[0.100])	Prec@1 100.000 (99.953)
Epoch: [256][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1024 (0.1024) ([0.003]+[0.100])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.2660 (0.2660) ([0.166]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.290
current lr 1.00000e-04
Grad=  tensor(0.1784, device='cuda:0')
Epoch: [257][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.1005 (0.1005) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [257][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1104 (0.1024) ([0.011]+[0.100])	Prec@1 99.219 (99.954)
Epoch: [257][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1036 (0.1021) ([0.004]+[0.100])	Prec@1 100.000 (99.961)
Epoch: [257][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1005 (0.1020) ([0.001]+[0.100])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2763 (0.2763) ([0.177]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.290
current lr 1.00000e-04
Grad=  tensor(0.1769, device='cuda:0')
Epoch: [258][0/391]	Time 0.207 (0.207)	Data 0.176 (0.176)	Loss 0.1004 (0.1004) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1004 (0.1015) ([0.001]+[0.100])	Prec@1 100.000 (99.969)
Epoch: [258][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1005 (0.1021) ([0.001]+[0.100])	Prec@1 100.000 (99.946)
Epoch: [258][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1021 (0.1020) ([0.002]+[0.100])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.210 (0.210)	Loss 0.2661 (0.2661) ([0.166]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.1767, device='cuda:0')
Epoch: [259][0/391]	Time 0.220 (0.220)	Data 0.188 (0.188)	Loss 0.1004 (0.1004) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [259][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1008 (0.1019) ([0.001]+[0.100])	Prec@1 100.000 (99.954)
Epoch: [259][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1005 (0.1020) ([0.001]+[0.100])	Prec@1 100.000 (99.957)
Epoch: [259][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1023 (0.1019) ([0.003]+[0.100])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2419 (0.2419) ([0.142]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.170
current lr 1.00000e-04
Grad=  tensor(0.1769, device='cuda:0')
Epoch: [260][0/391]	Time 0.235 (0.235)	Data 0.204 (0.204)	Loss 0.1003 (0.1003) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1005 (0.1019) ([0.001]+[0.100])	Prec@1 100.000 (99.954)
Epoch: [260][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1004 (0.1020) ([0.001]+[0.100])	Prec@1 100.000 (99.942)
Epoch: [260][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1029 (0.1019) ([0.003]+[0.100])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.2618 (0.2618) ([0.162]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.4718, device='cuda:0')
Epoch: [261][0/391]	Time 0.248 (0.248)	Data 0.215 (0.215)	Loss 0.1012 (0.1012) ([0.002]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1168 (0.1022) ([0.017]+[0.100])	Prec@1 99.219 (99.961)
Epoch: [261][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1072 (0.1018) ([0.008]+[0.100])	Prec@1 100.000 (99.965)
Epoch: [261][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1034 (0.1021) ([0.004]+[0.100])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2583 (0.2583) ([0.159]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.1773, device='cuda:0')
Epoch: [262][0/391]	Time 0.220 (0.220)	Data 0.188 (0.188)	Loss 0.1005 (0.1005) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1141 (0.1021) ([0.014]+[0.100])	Prec@1 100.000 (99.946)
Epoch: [262][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1021 (0.1020) ([0.002]+[0.100])	Prec@1 100.000 (99.961)
Epoch: [262][300/391]	Time 0.028 (0.025)	Data 0.000 (0.001)	Loss 0.1002 (0.1019) ([0.001]+[0.100])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.2635 (0.2635) ([0.164]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.2578, device='cuda:0')
Epoch: [263][0/391]	Time 0.207 (0.207)	Data 0.176 (0.176)	Loss 0.1008 (0.1008) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1009 (0.1029) ([0.001]+[0.100])	Prec@1 100.000 (99.946)
Epoch: [263][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1005 (0.1025) ([0.001]+[0.100])	Prec@1 100.000 (99.938)
Epoch: [263][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1020 (0.1023) ([0.002]+[0.100])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.2696 (0.2696) ([0.170]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-04
Grad=  tensor(0.1771, device='cuda:0')
Epoch: [264][0/391]	Time 0.253 (0.253)	Data 0.221 (0.221)	Loss 0.1003 (0.1003) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1003 (0.1018) ([0.001]+[0.100])	Prec@1 100.000 (99.954)
Epoch: [264][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1002 (0.1016) ([0.001]+[0.100])	Prec@1 100.000 (99.965)
Epoch: [264][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1005 (0.1015) ([0.001]+[0.100])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.2689 (0.2689) ([0.169]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.320
current lr 1.00000e-04
Grad=  tensor(0.1791, device='cuda:0')
Epoch: [265][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.1004 (0.1004) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1031 (0.1022) ([0.004]+[0.100])	Prec@1 100.000 (99.946)
Epoch: [265][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1005 (0.1020) ([0.001]+[0.100])	Prec@1 100.000 (99.957)
Epoch: [265][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1004 (0.1017) ([0.001]+[0.100])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.2636 (0.2636) ([0.164]+[0.100])	Prec@1 94.531 (94.531)
 * Prec@1 93.230
current lr 1.00000e-04
Grad=  tensor(0.1998, device='cuda:0')
Epoch: [266][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.1006 (0.1006) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.026 (0.026)	Data 0.000 (0.002)	Loss 0.1013 (0.1023) ([0.002]+[0.100])	Prec@1 100.000 (99.946)
Epoch: [266][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1003 (0.1022) ([0.001]+[0.100])	Prec@1 100.000 (99.942)
Epoch: [266][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1013 (0.1023) ([0.002]+[0.100])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2650 (0.2650) ([0.165]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-04
Grad=  tensor(0.1906, device='cuda:0')
Epoch: [267][0/391]	Time 0.203 (0.203)	Data 0.172 (0.172)	Loss 0.1004 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1009 (0.1025) ([0.001]+[0.099])	Prec@1 100.000 (99.923)
Epoch: [267][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1001 (0.1021) ([0.001]+[0.099])	Prec@1 100.000 (99.934)
Epoch: [267][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1011 (0.1020) ([0.002]+[0.099])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.2658 (0.2658) ([0.166]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.2080, device='cuda:0')
Epoch: [268][0/391]	Time 0.206 (0.206)	Data 0.174 (0.174)	Loss 0.1006 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1007 (0.1025) ([0.001]+[0.099])	Prec@1 100.000 (99.930)
Epoch: [268][200/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.1003 (0.1017) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [268][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1030 (0.1015) ([0.004]+[0.099])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2738 (0.2738) ([0.174]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.320
current lr 1.00000e-04
Grad=  tensor(0.1766, device='cuda:0')
Epoch: [269][0/391]	Time 0.247 (0.247)	Data 0.215 (0.215)	Loss 0.1001 (0.1001) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1002 (0.1021) ([0.001]+[0.099])	Prec@1 100.000 (99.938)
Epoch: [269][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1000 (0.1021) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [269][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1006 (0.1020) ([0.001]+[0.099])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.2588 (0.2588) ([0.159]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-04
Grad=  tensor(0.5342, device='cuda:0')
Epoch: [270][0/391]	Time 0.245 (0.245)	Data 0.211 (0.211)	Loss 0.1053 (0.1053) ([0.006]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0999 (0.1015) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [270][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1001 (0.1014) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [270][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1068 (0.1014) ([0.007]+[0.099])	Prec@1 99.219 (99.958)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.2545 (0.2545) ([0.155]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.350
current lr 1.00000e-04
Grad=  tensor(0.1815, device='cuda:0')
Epoch: [271][0/391]	Time 0.205 (0.205)	Data 0.174 (0.174)	Loss 0.1002 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1001 (0.1013) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [271][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1165 (0.1015) ([0.017]+[0.099])	Prec@1 99.219 (99.949)
Epoch: [271][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1001 (0.1015) ([0.001]+[0.099])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.2692 (0.2692) ([0.170]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.370
current lr 1.00000e-04
Grad=  tensor(0.1789, device='cuda:0')
Epoch: [272][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.1004 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1001 (0.1014) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [272][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1004 (0.1018) ([0.001]+[0.099])	Prec@1 100.000 (99.949)
Epoch: [272][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1009 (0.1016) ([0.002]+[0.099])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.2527 (0.2527) ([0.153]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.230
current lr 1.00000e-04
Grad=  tensor(0.1760, device='cuda:0')
Epoch: [273][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.0998 (0.0998) ([0.000]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1015 (0.1017) ([0.002]+[0.099])	Prec@1 100.000 (99.954)
Epoch: [273][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0999 (0.1012) ([0.001]+[0.099])	Prec@1 100.000 (99.965)
Epoch: [273][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1004 (0.1011) ([0.001]+[0.099])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.2522 (0.2522) ([0.153]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.320
current lr 1.00000e-04
Grad=  tensor(0.1761, device='cuda:0')
Epoch: [274][0/391]	Time 0.201 (0.201)	Data 0.172 (0.172)	Loss 0.0999 (0.0999) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1007 (0.1008) ([0.001]+[0.099])	Prec@1 100.000 (99.992)
Epoch: [274][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0998 (0.1014) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [274][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1027 (0.1011) ([0.003]+[0.099])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.2641 (0.2641) ([0.165]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.340
current lr 1.00000e-04
Grad=  tensor(0.2290, device='cuda:0')
Epoch: [275][0/391]	Time 0.204 (0.204)	Data 0.175 (0.175)	Loss 0.1003 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0999 (0.1011) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [275][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1002 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [275][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0999 (0.1010) ([0.001]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.2473 (0.2473) ([0.148]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.280
current lr 1.00000e-04
Grad=  tensor(0.2472, device='cuda:0')
Epoch: [276][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1006 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1212 (0.1013) ([0.022]+[0.099])	Prec@1 99.219 (99.954)
Epoch: [276][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0998 (0.1010) ([0.001]+[0.099])	Prec@1 100.000 (99.973)
Epoch: [276][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1001 (0.1010) ([0.001]+[0.099])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.2791 (0.2791) ([0.180]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.2882, device='cuda:0')
Epoch: [277][0/391]	Time 0.246 (0.246)	Data 0.216 (0.216)	Loss 0.1006 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1000 (0.1010) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [277][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0999 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.973)
Epoch: [277][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0999 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.2665 (0.2665) ([0.167]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.1858, device='cuda:0')
Epoch: [278][0/391]	Time 0.207 (0.207)	Data 0.176 (0.176)	Loss 0.1003 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0998 (0.1012) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [278][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1013 (0.1010) ([0.002]+[0.099])	Prec@1 100.000 (99.953)
Epoch: [278][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1105 (0.1012) ([0.011]+[0.099])	Prec@1 99.219 (99.951)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.2727 (0.2727) ([0.174]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.1796, device='cuda:0')
Epoch: [279][0/391]	Time 0.211 (0.211)	Data 0.182 (0.182)	Loss 0.0998 (0.0998) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0998 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [279][200/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.0997 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.965)
Epoch: [279][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.0999 (0.1010) ([0.001]+[0.099])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.2644 (0.2644) ([0.165]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.340
current lr 1.00000e-04
Grad=  tensor(0.4660, device='cuda:0')
Epoch: [280][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.1007 (0.1007) ([0.002]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0999 (0.1007) ([0.001]+[0.099])	Prec@1 100.000 (99.985)
Epoch: [280][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0996 (0.1008) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [280][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0996 (0.1009) ([0.000]+[0.099])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2493 (0.2493) ([0.150]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.310
current lr 1.00000e-04
Grad=  tensor(0.1954, device='cuda:0')
Epoch: [281][0/391]	Time 0.205 (0.205)	Data 0.173 (0.173)	Loss 0.1001 (0.1001) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1010 (0.1014) ([0.002]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [281][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1001 (0.1011) ([0.001]+[0.099])	Prec@1 100.000 (99.949)
Epoch: [281][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0998 (0.1011) ([0.001]+[0.099])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2606 (0.2606) ([0.162]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.410
current lr 1.00000e-04
Grad=  tensor(0.1945, device='cuda:0')
Epoch: [282][0/391]	Time 0.203 (0.203)	Data 0.172 (0.172)	Loss 0.0999 (0.0999) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1092 (0.1010) ([0.010]+[0.099])	Prec@1 99.219 (99.969)
Epoch: [282][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0998 (0.1008) ([0.001]+[0.099])	Prec@1 100.000 (99.973)
Epoch: [282][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1003 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.2603 (0.2603) ([0.161]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(25.6607, device='cuda:0')
Epoch: [283][0/391]	Time 0.221 (0.221)	Data 0.191 (0.191)	Loss 0.1215 (0.1215) ([0.022]+[0.099])	Prec@1 99.219 (99.219)
Epoch: [283][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1000 (0.1014) ([0.001]+[0.099])	Prec@1 100.000 (99.954)
Epoch: [283][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1039 (0.1013) ([0.005]+[0.099])	Prec@1 100.000 (99.965)
Epoch: [283][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0998 (0.1013) ([0.001]+[0.099])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.2371 (0.2371) ([0.138]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.330
current lr 1.00000e-04
Grad=  tensor(7.7764, device='cuda:0')
Epoch: [284][0/391]	Time 0.255 (0.255)	Data 0.223 (0.223)	Loss 0.1071 (0.1071) ([0.008]+[0.099])	Prec@1 99.219 (99.219)
Epoch: [284][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1022 (0.1004) ([0.003]+[0.099])	Prec@1 100.000 (99.985)
Epoch: [284][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.0998 (0.1007) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [284][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0997 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2469 (0.2469) ([0.148]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.370
current lr 1.00000e-04
Grad=  tensor(0.1770, device='cuda:0')
Epoch: [285][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.0996 (0.0996) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0994 (0.1011) ([0.000]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [285][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1002 (0.1007) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [285][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1015 (0.1007) ([0.003]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.2501 (0.2501) ([0.151]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-04
Grad=  tensor(0.1865, device='cuda:0')
Epoch: [286][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.0999 (0.0999) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1006 (0.1009) ([0.002]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [286][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1290 (0.1010) ([0.030]+[0.099])	Prec@1 99.219 (99.965)
Epoch: [286][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0994 (0.1008) ([0.000]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2448 (0.2448) ([0.146]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.1766, device='cuda:0')
Epoch: [287][0/391]	Time 0.217 (0.217)	Data 0.187 (0.187)	Loss 0.0996 (0.0996) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.0995 (0.1005) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [287][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0995 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [287][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1043 (0.1007) ([0.005]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.2584 (0.2584) ([0.160]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.490
current lr 1.00000e-04
Grad=  tensor(0.2225, device='cuda:0')
Epoch: [288][0/391]	Time 0.212 (0.212)	Data 0.181 (0.181)	Loss 0.1001 (0.1001) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0995 (0.1008) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [288][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0997 (0.1007) ([0.001]+[0.099])	Prec@1 100.000 (99.957)
Epoch: [288][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0993 (0.1007) ([0.000]+[0.099])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.2564 (0.2564) ([0.158]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(4.9192, device='cuda:0')
Epoch: [289][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.1075 (0.1075) ([0.009]+[0.099])	Prec@1 99.219 (99.219)
Epoch: [289][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1011 (0.1006) ([0.002]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [289][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0996 (0.1005) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [289][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.0996 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2560 (0.2560) ([0.157]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.400
current lr 1.00000e-04
Grad=  tensor(0.3020, device='cuda:0')
Epoch: [290][0/391]	Time 0.242 (0.242)	Data 0.209 (0.209)	Loss 0.1011 (0.1011) ([0.002]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0994 (0.1005) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [290][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0994 (0.1007) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [290][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1050 (0.1006) ([0.006]+[0.099])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.2619 (0.2619) ([0.163]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(0.7465, device='cuda:0')
Epoch: [291][0/391]	Time 0.255 (0.255)	Data 0.223 (0.223)	Loss 0.1023 (0.1023) ([0.004]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0992 (0.1004) ([0.000]+[0.099])	Prec@1 100.000 (99.985)
Epoch: [291][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0996 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (99.992)
Epoch: [291][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0994 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.2482 (0.2482) ([0.149]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.360
current lr 1.00000e-04
Grad=  tensor(7.5910, device='cuda:0')
Epoch: [292][0/391]	Time 0.212 (0.212)	Data 0.179 (0.179)	Loss 0.1024 (0.1024) ([0.004]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0995 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [292][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0993 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (99.957)
Epoch: [292][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0996 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.2678 (0.2678) ([0.169]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.410
current lr 1.00000e-04
Grad=  tensor(0.1767, device='cuda:0')
Epoch: [293][0/391]	Time 0.250 (0.250)	Data 0.215 (0.215)	Loss 0.0993 (0.0993) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0994 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [293][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0994 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (99.973)
Epoch: [293][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0998 (0.1004) ([0.001]+[0.099])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.2454 (0.2454) ([0.147]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.260
current lr 1.00000e-04
Grad=  tensor(0.1791, device='cuda:0')
Epoch: [294][0/391]	Time 0.217 (0.217)	Data 0.187 (0.187)	Loss 0.0996 (0.0996) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0994 (0.1005) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [294][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1001 (0.1009) ([0.001]+[0.099])	Prec@1 100.000 (99.942)
Epoch: [294][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0993 (0.1006) ([0.001]+[0.099])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.2530 (0.2530) ([0.154]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.320
current lr 1.00000e-04
Grad=  tensor(1.0704, device='cuda:0')
Epoch: [295][0/391]	Time 0.265 (0.265)	Data 0.234 (0.234)	Loss 0.1011 (0.1011) ([0.002]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0995 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [295][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0992 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [295][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0991 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.2592 (0.2592) ([0.161]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.290
current lr 1.00000e-04
Grad=  tensor(0.2334, device='cuda:0')
Epoch: [296][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.0999 (0.0999) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0994 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [296][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0993 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [296][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0990 (0.1003) ([0.000]+[0.099])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.2591 (0.2591) ([0.160]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(0.1778, device='cuda:0')
Epoch: [297][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.0993 (0.0993) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1000 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (99.969)
Epoch: [297][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0991 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [297][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0990 (0.1001) ([0.000]+[0.099])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2619 (0.2619) ([0.163]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(5.3518, device='cuda:0')
Epoch: [298][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.1032 (0.1032) ([0.005]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0992 (0.1002) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [298][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0993 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (99.984)
Epoch: [298][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0992 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.2620 (0.2620) ([0.163]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.410
current lr 1.00000e-04
Grad=  tensor(0.1867, device='cuda:0')
Epoch: [299][0/391]	Time 0.259 (0.259)	Data 0.225 (0.225)	Loss 0.0993 (0.0993) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.025 (0.029)	Data 0.000 (0.002)	Loss 0.0991 (0.0997) ([0.001]+[0.099])	Prec@1 100.000 (99.992)
Epoch: [299][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0994 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (99.977)
Epoch: [299][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0992 (0.0999) ([0.001]+[0.098])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.2672 (0.2672) ([0.169]+[0.098])	Prec@1 94.531 (94.531)
 * Prec@1 93.390

 Elapsed time for training  0:54:11.796009

 sparsity of   [0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.6666666865348816, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.8518518805503845, 0.0, 0.8888888955116272, 0.0, 0.9629629850387573, 0.6296296119689941, 0.0, 0.9629629850387573, 0.9629629850387573]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0703125, 0.0, 0.1328125, 0.0855034738779068, 0.0, 0.0546875, 0.0529513880610466, 0.208767369389534, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.3016493022441864, 0.9986979365348816, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.16015625, 0.999131977558136, 0.2921006977558136, 0.999131977558136, 0.0, 0.110243059694767, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.2469618022441864, 0.9995659589767456, 0.23828125, 0.9995659589767456, 0.03515625, 0.0, 0.1028645858168602, 0.0, 0.0, 0.0, 0.0, 0.764756977558136, 0.0629340261220932, 0.8407118320465088, 0.999131977558136, 0.1532118022441864, 0.999131977558136, 0.0, 0.999131977558136, 0.08203125, 0.0013020833721384406, 0.999131977558136, 0.01996527798473835, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1119791641831398, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.02387152798473835, 0.0, 0.0, 0.999131977558136, 0.1206597238779068, 0.0607638880610466, 0.0616319440305233, 0.999131977558136, 0.999131977558136, 0.0611979179084301, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.3732638955116272, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0924479141831398, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0815972238779068, 0.1145833358168602, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.02213541604578495, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0520833320915699, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.0, 0.7677951455116272, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0651041641831398, 0.0, 0.0, 0.126736119389534, 0.0, 0.0, 0.999131977558136, 0.0, 0.1927083283662796, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0364583320915699, 0.0, 0.0, 0.1710069477558136, 0.9995659589767456, 0.0516493059694767, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0625, 0.0, 0.0251736119389534, 0.0776909738779068, 0.2556423544883728, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0590277798473835, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0690104141831398, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0928819477558136, 0.999131977558136, 0.0, 0.118055559694767, 0.0, 0.0, 0.0894097238779068, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.9986979365348816, 0.0, 0.0, 0.0, 0.9986979365348816, 0.0, 0.0438368059694767, 0.0, 0.2039930522441864, 0.0885416641831398, 0.0, 0.0, 0.0, 0.999131977558136, 0.0412326380610466, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0993923619389534, 0.0, 0.0651041641831398, 0.0, 0.9995659589767456, 0.831163227558136, 0.0, 0.999131977558136, 0.0, 0.118055559694767, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1254340261220932, 0.0, 0.0850694477558136, 0.0798611119389534, 0.87109375, 0.0, 0.2543402910232544, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0668402761220932, 0.0690104141831398, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.2413194477558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.5234375, 0.08984375, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.9986979365348816, 0.999131977558136, 0.0920138880610466, 0.9995659589767456, 0.999131977558136, 0.0538194440305233, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.0815972238779068, 0.999131977558136, 0.04296875, 0.9995659589767456, 0.0967881977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.5234375, 0.0, 0.999131977558136, 0.0, 0.0889756977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0542534738779068, 0.0, 0.9986979365348816, 0.5234375, 0.0421006940305233, 0.0, 0.1401909738779068, 0.0, 0.0677083358168602, 0.5234375, 0.0638020858168602, 0.0, 0.0, 0.0, 0.9995659589767456, 0.1119791641831398, 0.0, 0.1171875, 0.9995659589767456, 0.999131977558136, 0.9084201455116272, 0.5221354365348816, 0.999131977558136, 0.5234375, 0.9995659589767456, 0.5234375, 0.0377604179084301, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.067274309694767, 0.0850694477558136, 0.0, 0.0681423619389534, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0559895820915699, 0.999131977558136, 0.0, 0.1983506977558136, 0.0386284738779068, 0.5199652910232544, 0.1284722238779068, 0.1401909738779068, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.1080729141831398, 0.999131977558136, 0.0, 0.0, 0.0, 0.0364583320915699, 0.9986979365348816, 0.0, 0.0807291641831398, 0.0, 0.999131977558136, 0.1271701455116272, 0.07421875, 0.1271701455116272, 0.1050347238779068, 0.0, 0.999131977558136, 0.0, 0.0, 0.4596354067325592, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9986979365348816, 0.0, 0.8563368320465088, 0.0616319440305233, 0.0477430559694767, 0.9995659589767456, 0.999131977558136, 0.0798611119389534, 0.0681423619389534, 0.9105902910232544, 0.9986979365348816, 0.1085069477558136, 0.999131977558136, 0.0920138880610466, 0.5234375, 0.0, 0.2560763955116272, 0.1028645858168602, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.2829861044883728, 0.999131977558136, 0.0, 0.08203125, 0.02864583395421505, 0.999131977558136, 0.0538194440305233, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.1197916641831398, 0.1310763955116272, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.9986979365348816, 0.0, 0.9995659589767456, 0.8862847089767456, 0.9995659589767456, 0.1098090261220932, 0.0, 0.0833333358168602, 0.0, 0.0625, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.0, 0.0885416641831398, 0.0, 0.228298619389534, 0.5234375, 0.075086809694767, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0980902761220932, 0.1158854141831398, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.082899309694767, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.483940988779068, 0.12109375, 0.8919270634651184, 0.999131977558136, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.0546875, 0.0707465261220932, 0.0794270858168602, 0.0546875, 0.094618059694767, 0.0]

 sparsity of   [0.999131977558136, 0.1046006977558136, 0.999131977558136, 0.8862847089767456, 0.0377604179084301, 0.999131977558136, 0.013020833022892475, 0.0, 0.1527777761220932, 0.0473090298473835, 0.0473090298473835, 0.0360243059694767, 0.999131977558136, 0.0607638880610466, 0.999131977558136, 0.772569477558136, 0.0455729179084301, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8402777910232544, 0.1163194477558136, 0.999131977558136, 0.2786458432674408, 0.142361119389534, 0.8993055820465088, 0.9995659589767456, 0.05859375, 0.9995659589767456, 0.9986979365348816, 0.0243055559694767, 0.1592881977558136, 0.03515625, 0.2317708283662796, 0.0850694477558136, 0.1158854141831398, 0.999131977558136, 0.1475694477558136, 0.999131977558136, 0.0447048619389534, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.7964409589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.0421006940305233, 0.03515625, 0.0381944440305233, 0.063368059694767, 0.0303819440305233, 0.999131977558136, 0.189236119389534, 0.03125, 0.999131977558136, 0.999131977558136, 0.1401909738779068, 0.999131977558136, 0.0499131940305233, 0.2326388955116272, 0.078125, 0.999131977558136, 0.7673611044883728, 0.999131977558136, 0.0598958320915699, 0.078993059694767, 0.0, 0.8953993320465088, 0.9995659589767456, 0.9986979365348816, 0.1115451380610466, 0.0347222238779068, 0.067274309694767, 0.0, 0.0529513880610466, 0.02604166604578495, 0.0533854179084301, 0.6909722089767456, 0.0920138880610466, 0.999131977558136, 0.999131977558136, 0.0303819440305233, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.110243059694767, 0.999131977558136, 0.02560763992369175, 0.1154513880610466, 0.9995659589767456, 0.0442708320915699, 0.0342881940305233, 0.0659722238779068, 0.2174479216337204, 0.0533854179084301, 0.0203993059694767, 0.999131977558136, 0.171875, 0.2217881977558136, 0.9995659589767456, 0.1536458283662796, 0.999131977558136, 0.0399305559694767, 0.0551215298473835, 0.0516493059694767, 0.0360243059694767, 0.1393229216337204, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8346354365348816, 0.0564236119389534, 0.0368923619389534, 0.0911458358168602, 0.999131977558136, 0.0364583320915699, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.765625, 0.0421006940305233, 0.086805559694767, 0.0, 0.999131977558136, 0.4301215410232544, 0.0, 0.999131977558136, 0.1050347238779068, 0.999131977558136, 0.1684027761220932, 0.999131977558136, 0.999131977558136, 0.0355902798473835, 0.999131977558136, 0.8151041865348816, 0.0590277798473835, 0.0, 0.1124131977558136, 0.0403645820915699, 0.999131977558136, 0.0342881940305233, 0.999131977558136, 0.1284722238779068, 0.1458333283662796, 0.999131977558136, 0.0364583320915699, 0.05859375, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.02734375, 0.999131977558136, 0.88671875, 0.999131977558136, 0.1701388955116272, 0.1145833358168602, 0.0334201380610466, 0.2239583283662796, 0.0737847238779068, 0.999131977558136, 0.0503472238779068, 0.0, 0.999131977558136, 0.1154513880610466, 0.0, 0.0542534738779068, 0.0377604179084301, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9097222089767456, 0.0381944440305233, 0.8758680820465088, 0.999131977558136, 0.999131977558136, 0.0394965298473835, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.953125, 0.6449652910232544, 0.9995659589767456, 0.0225694440305233, 0.999131977558136, 0.999131977558136, 0.1566840261220932, 0.0559895820915699, 0.04296875, 0.8997395634651184, 0.1675347238779068, 0.999131977558136, 0.0980902761220932, 0.0, 0.0989583358168602, 0.936631977558136, 0.01519097201526165, 0.7703993320465088, 0.999131977558136, 0.999131977558136, 0.3068576455116272, 0.0347222238779068, 0.9995659589767456, 0.0334201380610466, 0.0, 0.0520833320915699, 0.1050347238779068, 0.999131977558136, 0.1440972238779068, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1571180522441864, 0.1037326380610466, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.01605902798473835, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0460069440305233, 0.999131977558136, 0.0, 0.9995659589767456, 0.0377604179084301, 0.999131977558136, 0.999131977558136, 0.0755208358168602, 0.999131977558136, 0.8003472089767456, 0.2074652761220932, 0.1731770783662796, 0.0, 0.0, 0.999131977558136, 0.0785590261220932, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.2278645783662796, 0.999131977558136, 0.0434027798473835, 0.999131977558136, 0.9032118320465088, 0.0481770820915699, 0.0334201380610466, 0.999131977558136, 0.999131977558136, 0.0360243059694767, 0.0381944440305233, 0.999131977558136, 0.999131977558136, 0.0538194440305233, 0.0876736119389534, 0.7717013955116272, 0.0512152798473835, 0.999131977558136, 0.6401909589767456, 0.9283854365348816, 0.0, 0.05078125, 0.9986979365348816, 0.999131977558136, 0.9986979365348816, 0.0, 0.9422743320465088, 0.1427951455116272, 0.999131977558136, 0.0516493059694767, 0.2131076455116272, 0.6675347089767456, 0.8415798544883728, 0.1961805522441864, 0.999131977558136, 0.9995659589767456, 0.078993059694767, 0.0, 0.1080729141831398, 0.0503472238779068, 0.999131977558136, 0.9995659589767456, 0.0512152798473835, 0.7682291865348816, 0.1961805522441864, 0.999131977558136, 0.0399305559694767, 0.9995659589767456, 0.0407986119389534, 0.999131977558136, 0.999131977558136, 0.1206597238779068, 0.8519965410232544, 0.999131977558136, 0.8033854365348816, 0.046875, 0.1080729141831398, 0.078993059694767, 0.0564236119389534, 0.0, 0.7760416865348816, 0.2174479216337204, 0.999131977558136, 0.999131977558136, 0.3363715410232544, 0.0837673619389534, 0.9244791865348816, 0.1280381977558136, 0.0421006940305233, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.999131977558136, 0.9496527910232544, 0.9379340410232544, 0.999131977558136, 0.73828125, 0.999131977558136, 0.086805559694767, 0.0, 0.0399305559694767, 0.999131977558136, 0.0173611119389534, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.1276041716337204, 0.2526041567325592, 0.999131977558136, 0.9995659589767456, 0.0442708320915699, 0.999131977558136, 0.0824652761220932, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.02604166604578495, 0.181423619389534, 0.999131977558136, 0.0460069440305233, 0.999131977558136, 0.0876736119389534, 0.0, 0.1935763955116272, 0.0, 0.0859375, 0.9205729365348816, 0.0, 0.02170138992369175, 0.9986979365348816, 0.999131977558136, 0.2647569477558136, 0.999131977558136, 0.0407986119389534, 0.0342881940305233, 0.0, 0.90234375, 0.999131977558136, 0.9982638955116272, 0.0690104141831398, 0.999131977558136, 0.01692708395421505, 0.1801215261220932, 0.0837673619389534, 0.0455729179084301, 0.0394965298473835, 0.046875, 0.0455729179084301, 0.9496527910232544, 0.150173619389534, 0.0590277798473835, 0.0282118059694767, 0.1336805522441864, 0.9986979365348816, 0.0, 0.9995659589767456, 0.9431423544883728, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9578993320465088, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.078125, 0.0368923619389534, 0.0824652761220932, 0.999131977558136, 0.0421006940305233, 0.1588541716337204, 0.0447048619389534, 0.0655381977558136, 0.0551215298473835, 0.0416666679084301, 0.0737847238779068, 0.0394965298473835, 0.134548619389534, 0.04296875, 0.1193576380610466, 0.0386284738779068, 0.3311631977558136, 0.999131977558136, 0.0737847238779068, 0.1987847238779068, 0.9995659589767456, 0.0, 0.7699652910232544, 0.02473958395421505, 0.999131977558136, 0.999131977558136, 0.9157986044883728, 0.0329861119389534, 0.999131977558136, 0.0620659738779068, 0.0364583320915699, 0.02777777798473835, 0.999131977558136, 0.999131977558136, 0.0815972238779068, 0.1263020783662796, 0.0629340261220932, 0.0425347238779068, 0.0386284738779068, 0.0164930559694767, 0.999131977558136, 0.999131977558136, 0.0772569477558136, 0.02864583395421505, 0.999131977558136, 0.0755208358168602, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1202256977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.0516493059694767, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1028645858168602, 0.999131977558136, 0.0421006940305233, 0.9309895634651184, 0.0407986119389534, 0.1037326380610466, 0.9995659589767456, 0.1171875, 0.1684027761220932, 0.0394965298473835, 0.0889756977558136, 0.999131977558136, 0.0, 0.0638020858168602, 0.02300347201526165, 0.7400173544883728, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0394965298473835, 0.0381944440305233, 0.0, 0.0998263880610466, 0.1527777761220932, 0.0473090298473835, 0.999131977558136, 0.9986979365348816, 0.03125, 0.999131977558136, 0.0377604179084301, 0.02170138992369175, 0.92578125, 0.0, 0.999131977558136, 0.1519097238779068, 0.999131977558136, 0.9270833134651184, 0.0234375, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0434027798473835, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.2421875, 0.0629340261220932, 0.639756977558136, 0.0, 0.0694444477558136, 0.02300347201526165, 0.999131977558136, 0.02170138992369175, 0.0885416641831398, 0.0212673619389534, 0.02864583395421505, 0.0186631940305233, 0.8654513955116272]

 sparsity of   [0.1037326380610466, 0.0861545130610466, 0.0262586809694767, 0.0360243059694767, 0.0345052070915699, 0.025390625, 0.0700954869389534, 0.0379774309694767, 0.01714409701526165, 0.0642361119389534, 0.069227434694767, 0.1508246511220932, 0.4926215410232544, 0.0303819440305233, 0.02690972201526165, 0.1087239608168602, 0.9997829794883728, 0.273003488779068, 0.0460069440305233, 0.0785590261220932, 0.0748697891831398, 0.0631510391831398, 0.2994791567325592, 0.0364583320915699, 0.0831163227558136, 0.0405815988779068, 0.0646701380610466, 0.0407986119389534, 0.080078125, 0.0772569477558136, 0.0347222238779068, 0.0106336809694767, 0.0668402761220932, 0.0696614608168602, 0.0397135429084301, 0.0627170130610466, 0.03515625, 0.9997829794883728, 0.0622829869389534, 0.0529513880610466, 0.0345052070915699, 0.1512586772441864, 0.0718315988779068, 0.0640190988779068, 0.0870225727558136, 0.108289934694767, 0.0520833320915699, 0.0622829869389534, 0.0386284738779068, 0.0518663190305233, 0.0447048619389534, 0.052734375, 0.0442708320915699, 0.9997829794883728, 0.0388454869389534, 0.0794270858168602, 0.067274309694767, 0.0473090298473835, 0.0941840261220932, 0.0338541679084301, 0.0464409738779068, 0.0707465261220932, 0.3216145932674408, 0.02170138992369175, 0.0638020858168602, 0.0490451380610466, 0.011067708022892475, 0.0583767369389534, 0.086805559694767, 0.0496961809694767, 0.0536024309694767, 0.609375, 0.0577256940305233, 0.0872395858168602, 0.0373263880610466, 0.0855034738779068, 0.01584201492369175, 0.0785590261220932, 0.1174045130610466, 0.0581597238779068, 0.0622829869389534, 0.0776909738779068, 0.7137587070465088, 0.9216579794883728, 0.0499131940305233, 0.0316840298473835, 0.0779079869389534, 0.0909288227558136, 0.095703125, 0.071180559694767, 0.840928852558136, 0.5119357705116272, 0.1072048619389534, 0.0379774309694767, 0.333550363779068, 0.0412326380610466, 0.02994791604578495, 0.0850694477558136, 0.1019965261220932, 0.1067708358168602, 0.072265625, 0.0670572891831398, 0.05859375, 0.0631510391831398, 0.1156684011220932, 0.082899309694767, 0.0475260429084301, 0.0381944440305233, 0.0815972238779068, 0.0386284738779068, 0.029296875, 0.0327690988779068, 0.0870225727558136, 0.0807291641831398, 0.0347222238779068, 0.5182291865348816, 0.02886284701526165, 0.0709635391831398, 0.0438368059694767, 0.0635850727558136, 0.6744791865348816, 0.183376744389534, 0.6796875, 0.0379774309694767, 0.1154513880610466, 0.078993059694767, 0.1605902761220932, 0.0518663190305233, 0.0596788190305233, 0.1221788227558136, 0.0486111119389534, 0.01909722201526165, 0.0251736119389534, 0.072265625, 0.0390625, 0.71484375, 0.4563802182674408, 0.02495659701526165, 0.104383684694767, 0.0381944440305233, 0.7157118320465088, 0.0557725690305233, 0.1701388955116272, 0.0842013880610466, 0.044921875, 0.0954861119389534, 0.6566840410232544, 0.0518663190305233, 0.9997829794883728, 0.0518663190305233, 0.0763888880610466, 0.02690972201526165, 0.03059895895421505, 0.0559895820915699, 0.7378472089767456, 0.0362413190305233, 0.0412326380610466, 0.0776909738779068, 0.0616319440305233, 0.0833333358168602, 0.1124131977558136, 0.5946180820465088, 0.0, 0.0427517369389534, 0.590928852558136, 0.02799479104578495, 0.0373263880610466, 0.086805559694767, 0.4698350727558136, 0.860460102558136, 0.0466579869389534, 0.1744791716337204, 0.7115885615348816, 0.22265625, 0.0460069440305233, 0.0872395858168602, 0.0301649309694767, 0.0325520820915699, 0.0729166641831398, 0.0338541679084301, 0.0425347238779068, 0.1256510466337204, 0.8713107705116272, 0.01714409701526165, 0.08984375, 0.0284288190305233, 0.5130208134651184, 0.138671875, 0.02756076492369175, 0.0, 0.1078559011220932, 0.0, 0.0540364570915699, 0.9997829794883728, 0.0794270858168602, 0.0891927108168602, 0.065321184694767, 0.0436197929084301, 0.01584201492369175, 0.6879340410232544, 0.4016927182674408, 0.0310329869389534, 0.0321180559694767, 0.11328125, 0.6861979365348816, 0.7170138955116272, 0.0399305559694767, 0.0924479141831398, 0.2137586772441864, 0.480034738779068, 0.0538194440305233, 0.0436197929084301, 0.064453125, 0.0852864608168602, 0.09375, 0.0349392369389534, 0.0685763880610466, 0.548828125, 0.0490451380610466, 0.01128472201526165, 0.0483940988779068, 0.03515625, 0.0405815988779068, 0.0598958320915699, 0.0423177070915699, 0.0575086809694767, 0.0811631977558136, 0.0466579869389534, 0.0579427070915699, 0.0501302070915699, 0.090711809694767, 0.1410590261220932, 0.0546875, 0.02734375, 0.0355902798473835, 0.158203125, 0.0687934011220932, 0.6888020634651184, 0.3936631977558136, 0.0384114570915699, 0.0301649309694767, 0.0, 0.6080729365348816, 0.0373263880610466, 0.1694878488779068, 0.5308159589767456, 0.0421006940305233, 0.037109375, 0.125, 0.5509982705116272, 0.179470494389534, 0.0592447929084301, 0.0783420130610466, 0.0145399309694767, 0.0353732630610466, 0.096571184694767, 0.3387586772441864, 0.0774739608168602, 0.294487863779068, 0.748046875, 0.1104600727558136, 0.1753472238779068, 0.087890625, 0.0414496548473835, 0.0720486119389534, 0.1788194477558136, 0.0553385429084301, 0.4678819477558136, 0.068359375, 0.0232204869389534, 0.0314670130610466, 0.0340711809694767, 0.0477430559694767, 0.1100260391831398, 0.086805559694767, 0.3059895932674408, 0.9997829794883728, 0.0381944440305233, 0.0681423619389534, 0.3897569477558136, 0.0601128488779068, 0.0700954869389534, 0.0583767369389534, 0.0440538190305233, 0.014973958022892475, 0.0325520820915699, 0.0881076380610466, 0.3168402910232544, 0.0186631940305233, 0.0342881940305233, 0.0544704869389534, 0.0193142369389534, 0.1280381977558136, 0.0193142369389534, 0.0970052108168602, 0.03081597201526165, 0.6158854365348816, 0.0759548619389534, 0.0718315988779068, 0.046875, 0.0434027798473835, 0.2536892294883728, 0.0598958320915699, 0.072265625, 0.0594618059694767, 0.0661892369389534, 0.5855034589767456, 0.4854600727558136, 0.0444878488779068, 0.02408854104578495, 0.4752604067325592, 0.0696614608168602, 0.046875, 0.0629340261220932, 0.0366753488779068, 0.0648871511220932, 0.0321180559694767, 0.09765625, 0.0, 0.0551215298473835, 0.103515625, 0.0670572891831398, 0.048828125, 0.0436197929084301, 0.6302083134651184, 0.5835503339767456, 0.02018229104578495, 0.1790364533662796, 0.1341145783662796, 0.0403645820915699, 0.1013454869389534, 0.0399305559694767, 0.1569010466337204, 0.1256510466337204, 0.0690104141831398, 0.0290798619389534, 0.01584201492369175, 0.0447048619389534, 0.0173611119389534, 0.686631977558136, 0.2044270783662796, 0.0785590261220932, 0.6037326455116272, 0.7560763955116272, 0.1206597238779068, 0.0557725690305233, 0.09375, 0.0, 0.0323350690305233, 0.0779079869389534, 0.0470920130610466, 0.0392795130610466, 0.4654947817325592, 0.078125, 0.0655381977558136, 0.0564236119389534, 0.1236979141831398, 0.5703125, 0.0, 0.0737847238779068, 0.0349392369389534, 0.1072048619389534, 0.0518663190305233, 0.3146701455116272, 0.0909288227558136, 0.0394965298473835, 0.1812065988779068, 0.0375434048473835, 0.0392795130610466, 0.01584201492369175, 0.046875, 0.0707465261220932, 0.0651041641831398, 0.4776475727558136, 0.1330295205116272, 0.02690972201526165, 0.0583767369389534, 0.17578125, 0.0729166641831398, 0.1050347238779068, 0.0930989608168602, 0.0470920130610466, 0.0394965298473835, 0.0336371548473835, 0.0447048619389534, 0.236111119389534, 0.0334201380610466, 0.0713975727558136, 0.0501302070915699, 0.0989583358168602, 0.080946184694767, 0.0629340261220932, 0.0813802108168602, 0.0774739608168602, 0.8576388955116272, 0.0787760391831398, 0.10546875, 0.1605902761220932, 0.0323350690305233, 0.0959201380610466, 0.0436197929084301, 0.1467013955116272, 0.0071614584885537624, 0.0583767369389534, 0.0967881977558136, 0.0366753488779068, 0.0638020858168602, 0.080078125, 0.0833333358168602, 0.0607638880610466, 0.2217881977558136, 0.041015625, 0.0030381944961845875, 0.0, 0.0581597238779068, 0.5798611044883728, 0.02777777798473835, 0.0609809048473835, 0.1471354216337204, 0.0546875, 0.7135416865348816, 0.5815972089767456, 0.0792100727558136, 0.0557725690305233, 0.0464409738779068, 0.0390625, 0.0499131940305233, 0.5852864384651184, 0.060546875, 0.00021701389050576836, 0.0648871511220932, 0.0536024309694767, 0.0963541641831398, 0.0284288190305233, 0.2864583432674408, 0.0392795130610466, 0.0184461809694767, 0.0325520820915699, 0.9997829794883728, 0.0679253488779068, 0.0427517369389534, 0.1549479216337204, 0.041015625, 0.278862863779068, 0.0611979179084301, 0.0546875, 0.0405815988779068, 0.052734375, 0.615234375, 0.1206597238779068, 0.1818576455116272, 0.0, 0.7263454794883728, 0.091796875, 0.0397135429084301, 0.82421875, 0.0607638880610466, 0.1009114608168602, 0.0421006940305233, 0.3656684160232544, 0.0479600690305233, 0.0551215298473835, 0.100477434694767, 0.0661892369389534, 0.0364583320915699, 0.0620659738779068, 0.099609375, 0.0405815988779068, 0.0757378488779068, 0.060546875, 0.1119791641831398, 0.04296875, 0.0807291641831398, 0.0397135429084301, 0.0321180559694767, 0.03125, 0.0924479141831398, 0.0384114570915699, 0.063368059694767, 0.0542534738779068, 0.7964409589767456, 0.1382378488779068, 0.0590277798473835, 0.111328125, 0.0685763880610466, 0.0581597238779068, 0.0373263880610466, 0.0757378488779068, 0.0470920130610466, 0.103515625, 0.0627170130610466, 0.0668402761220932, 0.0970052108168602, 0.0681423619389534, 0.0516493059694767, 0.06640625, 0.8146701455116272, 0.1158854141831398, 0.3103298544883728, 0.0737847238779068, 0.02690972201526165, 0.0392795130610466, 0.0386284738779068, 0.4422743022441864, 0.0575086809694767, 0.0746527761220932, 0.1145833358168602, 0.5651041865348816, 0.693359375, 0.0375434048473835, 0.0414496548473835, 0.0625]

 sparsity of   [0.2098524272441864, 0.1493055522441864, 0.0861545130610466, 0.0470920130610466, 0.1510416716337204, 0.0473090298473835, 0.0271267369389534, 0.065321184694767, 0.0481770820915699, 0.05078125, 0.0323350690305233, 0.0362413190305233, 0.2217881977558136, 0.0562065988779068, 0.0533854179084301, 0.4548611044883728, 0.0538194440305233, 0.7267795205116272, 0.0394965298473835, 0.0492621548473835, 0.077039934694767, 0.0685763880610466, 0.0679253488779068, 0.1319444477558136, 0.0512152798473835, 0.1176215261220932, 0.2387152761220932, 0.0145399309694767, 0.0557725690305233, 0.0516493059694767, 0.1842447966337204, 0.1351996511220932, 0.0642361119389534, 0.0245225690305233, 0.02300347201526165, 0.0536024309694767, 0.0329861119389534, 0.3621961772441864, 0.0444878488779068, 0.0475260429084301, 0.9995659589767456, 0.0486111119389534, 0.0620659738779068, 0.075086809694767, 0.0755208358168602, 0.0746527761220932, 0.0924479141831398, 0.3075086772441864, 0.0592447929084301, 0.0466579869389534, 0.1840277761220932, 0.0705295130610466, 0.0572916679084301, 0.0863715261220932, 0.01888020895421505, 0.0889756977558136, 0.672960102558136, 0.1104600727558136, 0.1453993022441864, 0.8513454794883728, 0.6957465410232544, 0.0345052070915699, 0.060546875, 0.0, 0.0336371548473835, 0.0959201380610466, 0.1881510466337204, 0.0442708320915699, 0.0755208358168602, 0.0290798619389534, 0.0924479141831398, 0.1766493022441864, 0.0553385429084301, 0.0861545130610466, 0.2825520932674408, 0.1432291716337204, 0.0687934011220932, 0.0349392369389534, 0.1903211772441864, 0.2725694477558136, 0.065321184694767, 0.02690972201526165, 0.052734375, 0.0970052108168602, 0.0698784738779068, 0.210720494389534, 0.123046875, 0.090711809694767, 0.0414496548473835, 0.0635850727558136, 0.2502170205116272, 0.1252170205116272, 0.0950520858168602, 0.3463541567325592, 0.1087239608168602, 0.1143663227558136, 0.2328559011220932, 0.1263020783662796, 0.0989583358168602, 0.063368059694767, 0.1506076455116272, 0.0601128488779068, 0.0416666679084301, 0.0713975727558136, 0.0785590261220932, 0.067274309694767, 0.1922743022441864, 0.0705295130610466, 0.0657552108168602, 0.0540364570915699, 0.0935329869389534, 0.0334201380610466, 0.0327690988779068, 0.0635850727558136, 0.0842013880610466, 0.0457899309694767, 0.0407986119389534, 0.0, 0.0583767369389534, 0.0614149309694767, 0.064453125, 0.0646701380610466, 0.0470920130610466, 0.0551215298473835, 0.0008680555620230734, 0.0284288190305233, 0.0759548619389534, 0.080946184694767, 0.0707465261220932, 0.411675363779068, 0.1408420205116272, 0.6117621660232544, 0.1380208283662796, 0.0436197929084301, 0.0659722238779068, 0.0399305559694767, 0.3953993022441864, 0.012803819961845875, 0.0666232630610466, 0.07421875, 0.1412760466337204, 0.0902777761220932, 0.02669270895421505, 0.12109375, 0.4186197817325592, 0.065321184694767, 0.0392795130610466, 0.0407986119389534, 0.0462239570915699, 0.0818142369389534, 0.0336371548473835, 0.265190988779068, 0.0674913227558136, 0.2144097238779068, 0.0648871511220932, 0.0796440988779068, 0.0920138880610466, 0.1929253488779068, 0.1321614533662796, 0.0783420130610466, 0.1334635466337204, 0.3723958432674408, 0.189236119389534, 0.0538194440305233, 0.1187065988779068, 0.1586371511220932, 0.01909722201526165, 0.3185763955116272, 0.1150173619389534, 0.048828125, 0.0659722238779068, 0.0414496548473835, 0.1278211772441864, 0.07421875, 0.775390625, 0.0594618059694767, 0.0609809048473835, 0.12890625, 0.01888020895421505, 0.7029079794883728, 0.1927083283662796, 0.0414496548473835, 0.6998698115348816, 0.0876736119389534, 0.084852434694767, 0.0234375, 0.724609375, 0.078125, 0.1334635466337204, 0.130642369389534, 0.0559895820915699, 0.185546875, 0.0833333358168602, 0.3370225727558136, 0.9075520634651184, 0.0, 0.1742621511220932, 0.92578125, 0.3001302182674408, 0.1284722238779068, 0.0523003488779068, 0.0766059011220932, 0.0490451380610466, 0.0303819440305233, 0.0705295130610466, 0.0861545130610466, 0.1545138955116272, 0.9995659589767456, 0.065321184694767, 0.2020399272441864, 0.0, 0.0572916679084301, 0.0473090298473835, 0.0922309011220932, 0.0516493059694767, 0.3240017294883728, 0.7534722089767456, 0.05859375, 0.0388454869389534, 0.0575086809694767, 0.0388454869389534, 0.02864583395421505, 0.0353732630610466, 0.0451388880610466, 0.0546875, 0.0564236119389534, 0.0, 0.071180559694767, 0.0681423619389534, 0.0355902798473835, 0.2300347238779068, 0.4340277910232544, 0.0891927108168602, 0.0538194440305233, 0.052734375, 0.7437065839767456, 0.09375, 0.152126744389534, 0.0264756940305233, 0.02387152798473835, 0.1013454869389534, 0.0944010391831398, 0.0321180559694767, 0.1382378488779068, 0.02690972201526165, 0.0531684048473835, 0.0516493059694767, 0.0941840261220932, 0.01128472201526165, 0.1019965261220932, 0.0568576380610466, 0.6111111044883728, 0.0766059011220932, 0.1069878488779068, 0.0386284738779068, 0.0744357630610466, 0.0329861119389534, 0.120008684694767, 0.011935763992369175, 0.1829427033662796, 0.9997829794883728, 0.029296875, 0.0755208358168602, 0.0496961809694767, 0.02560763992369175, 0.0533854179084301, 0.084852434694767, 0.4720052182674408, 0.4229600727558136, 0.7934027910232544, 0.103515625, 0.0477430559694767, 0.0594618059694767, 0.1263020783662796, 0.0, 0.082899309694767, 0.046875, 0.0844184011220932, 0.02604166604578495, 0.052734375, 0.02365451492369175, 0.0533854179084301, 0.2332899272441864, 0.3196614682674408, 0.025390625, 0.0596788190305233, 0.0694444477558136, 0.0336371548473835, 0.0564236119389534, 0.0668402761220932, 0.0438368059694767, 0.179470494389534, 0.0592447929084301, 0.9926215410232544, 0.0381944440305233, 0.111328125, 0.0403645820915699, 0.0427517369389534, 0.0874565988779068, 0.0850694477558136, 0.1254340261220932, 0.0321180559694767, 0.0503472238779068, 0.0353732630610466, 0.011935763992369175, 0.2775607705116272, 0.0, 0.4919704794883728, 0.2506510317325592, 0.078125, 0.1573350727558136, 0.0690104141831398, 0.0635850727558136, 0.0405815988779068, 0.052734375, 0.0659722238779068, 0.2456597238779068, 0.0779079869389534, 0.2293836772441864, 0.2975260317325592, 0.056640625, 0.0533854179084301, 0.01692708395421505, 0.0264756940305233, 0.0883246511220932, 0.2482638955116272, 0.1725260466337204, 0.1885850727558136, 0.159939244389534, 0.0655381977558136, 0.0709635391831398, 0.0685763880610466, 0.0572916679084301, 0.2369791716337204, 0.8209635615348816, 0.6256510615348816, 0.0, 0.0570746548473835, 0.0336371548473835, 0.0536024309694767, 0.300347238779068, 0.0, 0.090711809694767, 0.0824652761220932, 0.0787760391831398, 0.0757378488779068, 0.1471354216337204, 0.0724826380610466, 0.02864583395421505, 0.0894097238779068, 0.1985677033662796, 0.1156684011220932, 0.1909722238779068, 0.1612413227558136, 0.0453559048473835, 0.2688802182674408, 0.0776909738779068, 0.1744791716337204, 0.2799479067325592, 0.1154513880610466, 0.037109375, 0.0577256940305233, 0.0857204869389534, 0.3229166567325592, 0.0763888880610466, 0.0685763880610466, 0.0481770820915699, 0.9995659589767456, 0.0911458358168602, 0.0687934011220932, 0.0909288227558136, 0.0340711809694767, 0.075086809694767, 0.1263020783662796, 0.8700087070465088, 0.02300347201526165, 0.1334635466337204, 0.0687934011220932, 0.1013454869389534, 0.094618059694767, 0.1371527761220932, 0.02213541604578495, 0.068359375, 0.0455729179084301, 0.01888020895421505, 0.1532118022441864, 0.0657552108168602, 0.150173619389534, 0.060546875, 0.0930989608168602, 0.1506076455116272, 0.2081163227558136, 0.0837673619389534, 0.1158854141831398, 0.0505642369389534, 0.587890625, 0.0421006940305233, 0.1341145783662796, 0.0234375, 0.0206163190305233, 0.0659722238779068, 0.0583767369389534, 0.1957465261220932, 0.9997829794883728, 0.0685763880610466, 0.080078125, 0.875, 0.0870225727558136, 0.116102434694767, 0.0740017369389534, 0.0, 0.02951388992369175, 0.0, 0.046875, 0.00021701389050576836, 0.0282118059694767, 0.1360677033662796, 0.7300347089767456, 0.1343315988779068, 0.067274309694767, 0.0421006940305233, 0.0555555559694767, 0.1547309011220932, 0.4379340410232544, 0.0284288190305233, 0.0792100727558136, 0.08203125, 0.0431857630610466, 0.3621961772441864, 0.0716145858168602, 0.166015625, 0.0514322929084301, 0.08984375, 0.0718315988779068, 0.0815972238779068, 0.156032994389534, 0.1098090261220932, 0.1610243022441864, 0.0355902798473835, 0.9997829794883728, 0.0544704869389534, 0.0579427070915699, 0.130642369389534, 0.6150173544883728, 0.0733506977558136, 0.0416666679084301, 0.0594618059694767, 0.0818142369389534, 0.0533854179084301, 0.0865885391831398, 0.0581597238779068, 0.0494791679084301, 0.1753472238779068, 0.1174045130610466, 0.0807291641831398, 0.5345051884651184, 0.0657552108168602, 0.2853732705116272, 0.02018229104578495, 0.2211371511220932, 0.156032994389534, 0.2725694477558136, 0.0963541641831398, 0.4865451455116272, 0.0611979179084301, 0.1111111119389534, 0.0659722238779068, 0.2549913227558136, 0.490234375, 0.0045572915114462376, 0.1009114608168602, 0.296440988779068, 0.0744357630610466, 0.0859375, 0.161892369389534, 0.0696614608168602, 0.0690104141831398, 0.0529513880610466, 0.1213107630610466, 0.0470920130610466, 0.041015625, 0.0833333358168602, 0.19921875, 0.130859375, 0.6176215410232544, 0.1714409738779068, 0.0434027798473835, 0.075086809694767, 0.0588107630610466, 0.0620659738779068, 0.0674913227558136, 0.076171875, 0.1030815988779068, 0.3754340410232544, 0.0900607630610466, 0.1907552033662796, 0.0570746548473835, 0.1338975727558136, 0.01888020895421505, 0.8552517294883728, 0.9997829794883728, 0.5496962070465088, 0.0538194440305233, 0.1725260466337204, 0.0638020858168602, 0.0590277798473835, 0.0, 0.0618489570915699, 0.0490451380610466, 0.171875, 0.08203125, 0.425347238779068]

 sparsity of   [0.02170138992369175, 0.0518663190305233, 0.2584635317325592, 0.3229166567325592, 0.04296875, 0.0677083358168602, 0.0006510416860692203, 0.0173611119389534, 0.0340711809694767, 0.0677083358168602, 0.011501736007630825, 0.0434027798473835, 0.0540364570915699, 0.0310329869389534, 0.0412326380610466, 0.013671875, 0.0004340277810115367, 0.1202256977558136, 0.0588107630610466, 0.0598958320915699, 0.0470920130610466, 0.0451388880610466, 0.0, 0.025390625, 0.0455729179084301, 0.0462239570915699, 0.0069444444961845875, 0.011067708022892475, 0.0023871527519077063, 0.008897569961845875, 0.0284288190305233, 0.0737847238779068, 0.0017361111240461469, 0.3331163227558136, 0.1052517369389534, 0.0, 0.01128472201526165, 0.01888020895421505, 0.01019965298473835, 0.0301649309694767, 0.0824652761220932, 0.4646267294883728, 0.0640190988779068, 0.300347238779068, 0.0006510416860692203, 0.01974826492369175, 0.010416666977107525, 0.0629340261220932, 0.013020833022892475, 0.0036892362404614687, 0.107421875, 0.0, 0.1820746511220932, 0.0616319440305233, 0.008029513992369175, 0.021484375, 0.004123263992369175, 0.0243055559694767, 0.0034722222480922937, 0.9997829794883728, 0.9997829794883728, 0.0186631940305233, 0.0327690988779068, 0.01996527798473835, 0.00824652798473835, 0.0388454869389534, 0.0579427070915699, 0.0264756940305233, 0.4516059160232544, 0.0598958320915699, 0.0729166641831398, 0.0455729179084301, 0.0601128488779068, 0.0492621548473835, 0.0451388880610466, 0.0451388880610466, 0.072265625, 0.0733506977558136, 0.011935763992369175, 0.1388888955116272, 0.0377604179084301, 0.5904948115348816, 0.0071614584885537624, 0.0379774309694767, 0.0668402761220932, 0.2658420205116272, 0.0431857630610466, 0.0, 0.01215277798473835, 0.0505642369389534, 0.0753038227558136, 0.0, 0.0310329869389534, 0.8229166865348816, 0.2378472238779068, 0.006076388992369175, 0.0223524309694767, 0.0972222238779068, 0.02191840298473835, 0.0334201380610466, 0.011935763992369175, 0.008029513992369175, 0.6161024570465088, 0.2658420205116272, 0.00933159701526165, 0.0284288190305233, 0.009548611007630825, 0.0336371548473835, 0.02560763992369175, 0.0234375, 0.240017369389534, 0.0648871511220932, 0.0026041667442768812, 0.0310329869389534, 0.01888020895421505, 0.0520833320915699, 0.056640625, 0.01627604104578495, 0.0310329869389534, 0.0588107630610466, 0.02170138992369175, 0.02604166604578495, 0.0, 0.0212673619389534, 0.0540364570915699, 0.02278645895421505, 0.1302083283662796, 0.0494791679084301, 0.1629774272441864, 0.0392795130610466, 0.011935763992369175, 0.009114583022892475, 0.025390625, 0.0375434048473835, 0.0564236119389534, 0.0397135429084301, 0.0, 0.121961809694767, 0.02018229104578495, 0.013454861007630825, 0.0631510391831398, 0.01909722201526165, 0.01779513992369175, 0.012803819961845875, 0.3819444477558136, 0.0551215298473835, 0.009982638992369175, 0.0, 0.008463541977107525, 0.01909722201526165, 0.0533854179084301, 0.0173611119389534, 0.1686197966337204, 0.064453125, 0.01996527798473835, 0.02365451492369175, 0.9852430820465088, 0.0381944440305233, 0.012803819961845875, 0.0438368059694767, 0.0078125, 0.0368923619389534, 0.008897569961845875, 0.0203993059694767, 0.03059895895421505, 0.0206163190305233, 0.02994791604578495, 0.0, 0.01692708395421505, 0.0596788190305233, 0.072265625, 0.0785590261220932, 0.0, 0.012803819961845875, 0.0640190988779068, 0.009982638992369175, 0.0700954869389534, 0.0145399309694767, 0.0492621548473835, 0.02582465298473835, 0.0013020833721384406, 0.203125, 0.01584201492369175, 0.029296875, 0.838975727558136, 0.0496961809694767, 0.0, 0.0086805559694767, 0.014756944961845875, 0.01779513992369175, 0.0125868059694767, 0.02387152798473835, 0.279296875, 0.0963541641831398, 0.01171875, 0.011067708022892475, 0.0262586809694767, 0.01627604104578495, 0.1449652761220932, 0.005859375, 0.0, 0.014322916977107525, 0.0726996511220932, 0.0106336809694767, 0.01953125, 0.013671875, 0.0262586809694767, 0.0598958320915699, 0.071180559694767, 0.0316840298473835, 0.0026041667442768812, 0.0, 0.0546875, 0.0737847238779068, 0.0418836809694767, 0.0423177070915699, 0.0164930559694767, 0.02387152798473835, 0.02495659701526165, 0.0023871527519077063, 0.0564236119389534, 0.12890625, 0.00629340298473835, 0.0, 0.29296875, 0.0, 0.1019965261220932, 0.0577256940305233, 0.0, 0.1176215261220932, 0.005642361007630825, 0.013888888992369175, 0.0616319440305233, 0.004123263992369175, 0.013454861007630825, 0.1321614533662796, 0.0544704869389534, 0.0212673619389534, 0.0503472238779068, 0.0529513880610466, 0.0492621548473835, 0.013671875, 0.006076388992369175, 0.0729166641831398, 0.00434027798473835, 0.0193142369389534, 0.0733506977558136, 0.0069444444961845875, 0.2764756977558136, 0.8552517294883728, 0.01714409701526165, 0.0, 0.0173611119389534, 0.01909722201526165, 0.0679253488779068, 0.094618059694767, 0.0783420130610466, 0.0390625, 0.02951388992369175, 0.0434027798473835, 0.00021701389050576836, 0.6705729365348816, 0.012803819961845875, 0.044921875, 0.0542534738779068, 0.011067708022892475, 0.102430559694767, 0.002170138992369175, 0.0444878488779068, 0.0321180559694767, 0.02495659701526165, 0.7135416865348816, 0.0505642369389534, 0.001953125, 0.1128472238779068, 0.0353732630610466, 0.02604166604578495, 0.01953125, 0.02495659701526165, 0.0457899309694767, 0.2805989682674408, 0.013020833022892475, 0.0321180559694767, 0.0004340277810115367, 0.0028211805038154125, 0.065321184694767, 0.0314670130610466, 0.0967881977558136, 0.0477430559694767, 0.0590277798473835, 0.0614149309694767, 0.0362413190305233, 0.1924913227558136, 0.154079869389534, 0.01323784701526165, 0.0536024309694767, 0.00021701389050576836, 0.0045572915114462376, 0.041015625, 0.009114583022892475, 0.010416666977107525, 0.0184461809694767, 0.0065104165114462376, 0.0603298619389534, 0.0451388880610466, 0.0959201380610466, 0.009548611007630825, 0.005642361007630825, 0.0455729179084301, 0.0030381944961845875, 0.0416666679084301, 0.02799479104578495, 0.013671875, 0.0397135429084301, 0.1471354216337204, 0.0327690988779068, 0.3328993022441864, 0.0086805559694767, 0.01953125, 0.0525173619389534, 0.04296875, 0.0, 0.0388454869389534, 0.0, 0.0212673619389534, 0.0052083334885537624, 0.00021701389050576836, 0.0381944440305233, 0.0243055559694767, 0.0746527761220932, 0.013454861007630825, 0.004123263992369175, 0.025390625, 0.0622829869389534, 0.0334201380610466, 0.0345052070915699, 0.0774739608168602, 0.0499131940305233, 0.0386284738779068, 0.02191840298473835, 0.4368489682674408, 0.0434027798473835, 0.01974826492369175, 0.0223524309694767, 0.0436197929084301, 0.1651475727558136, 0.007595486007630825, 0.0431857630610466, 0.0562065988779068, 0.0290798619389534, 0.6176215410232544, 0.014756944961845875, 0.0071614584885537624, 0.02690972201526165, 0.6206597089767456, 0.14453125, 0.0368923619389534, 0.02191840298473835, 0.0713975727558136, 0.011935763992369175, 0.0049913194961845875, 0.015625, 0.01801215298473835, 0.2803819477558136, 0.0, 0.02213541604578495, 0.01019965298473835, 0.02105034701526165, 0.0334201380610466, 0.1920572966337204, 0.0271267369389534, 0.0798611119389534, 0.3205295205116272, 0.0635850727558136, 0.0340711809694767, 0.011067708022892475, 0.0, 0.02864583395421505, 0.02734375, 0.1091579869389534, 0.2306857705116272, 0.0, 0.0193142369389534, 0.2727864682674408, 0.0592447929084301, 0.0, 0.0026041667442768812, 0.0, 0.01627604104578495, 0.009765625, 0.0685763880610466, 0.3641493022441864, 0.0125868059694767, 0.0481770820915699, 0.01888020895421505, 0.0390625, 0.0032552082557231188, 0.0184461809694767, 0.01996527798473835, 0.0492621548473835, 0.004123263992369175, 0.0069444444961845875, 0.0694444477558136, 0.0, 0.0, 0.076171875, 0.0418836809694767, 0.009982638992369175, 0.008463541977107525, 0.0822482630610466, 0.0525173619389534, 0.201171875, 0.087890625, 0.0321180559694767, 0.02083333395421505, 0.2289496511220932, 0.0212673619389534, 0.0, 0.0490451380610466, 0.02365451492369175, 0.0078125, 0.01996527798473835, 0.01584201492369175, 0.0282118059694767, 0.0466579869389534, 0.067274309694767, 0.010850694961845875, 0.2677951455116272, 0.01822916604578495, 0.005425347480922937, 0.009114583022892475, 0.0, 0.0629340261220932, 0.011067708022892475, 0.011501736007630825, 0.011501736007630825, 0.0, 0.0203993059694767, 0.0902777761220932, 0.02582465298473835, 0.0572916679084301, 0.5240885615348816, 0.0438368059694767, 0.002170138992369175, 0.02105034701526165, 0.0, 0.0, 0.0394965298473835, 0.01605902798473835, 0.0010850694961845875, 0.0844184011220932, 0.0321180559694767, 0.02973090298473835, 0.01801215298473835, 0.0, 0.0375434048473835, 0.1282552033662796, 0.0342881940305233, 0.3489583432674408, 0.0028211805038154125, 0.0303819440305233, 0.0399305559694767, 0.046875, 0.3884548544883728, 0.0457899309694767, 0.01584201492369175, 0.013671875, 0.0759548619389534, 0.01822916604578495, 0.01410590298473835, 0.1050347238779068, 0.0588107630610466, 0.0583767369389534, 0.01215277798473835, 0.0106336809694767, 0.7619357705116272, 0.069227434694767, 0.4201388955116272, 0.015625, 0.1030815988779068, 0.01323784701526165, 0.0223524309694767, 0.0581597238779068, 0.0536024309694767, 0.0225694440305233, 0.0, 0.02886284701526165, 0.02191840298473835, 0.0466579869389534, 0.0355902798473835, 0.02734375, 0.0399305559694767, 0.0768229141831398, 0.02582465298473835, 0.1770833283662796, 0.00933159701526165, 0.3018663227558136, 0.0078125, 0.0529513880610466, 0.014322916977107525, 0.0032552082557231188, 0.015625, 0.0366753488779068, 0.0078125, 0.017578125, 0.01019965298473835, 0.01909722201526165, 0.8938801884651184, 0.01953125, 0.1011284738779068, 0.0366753488779068, 0.02083333395421505]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03059895895421505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0373263880610466, 0.0, 0.9995659589767456, 0.0232204869389534, 0.0, 0.0034722222480922937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011067708022892475, 0.0, 0.0, 0.0, 0.04296875, 0.0, 0.0069444444961845875, 0.3444010317325592, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.2840711772441864, 0.5180121660232544, 0.0, 0.0, 0.0212673619389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0052083334885537624, 0.0, 0.0, 0.0, 0.0028211805038154125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006510416860692203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6284722089767456, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006076388992369175, 0.0, 0.0, 0.01410590298473835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006076388992369175, 0.4142795205116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6814236044883728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007595486007630825, 0.0, 0.009982638992369175, 0.0047743055038154125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0049913194961845875, 0.0067274305038154125, 0.0, 0.0013020833721384406, 0.0, 0.0, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0106336809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32421875, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0360243059694767, 0.00434027798473835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.0, 0.0184461809694767, 0.0, 0.02495659701526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0034722222480922937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0045572915114462376, 0.0, 0.0, 0.0, 0.0, 0.6015625, 0.0, 0.0, 0.0, 0.0, 0.004123263992369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002170138992369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02669270895421505, 0.0167100690305233, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004123263992369175, 0.0, 0.0, 0.96875, 0.0004340277810115367, 0.0006510416860692203, 0.0, 0.0, 0.0, 0.01128472201526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0036892362404614687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008897569961845875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0232204869389534, 0.0184461809694767, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.00629340298473835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01410590298473835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0047743055038154125, 0.0, 0.0, 0.0, 0.0, 0.0049913194961845875]

 sparsity of   [0.0, 0.0036892362404614687, 0.0, 0.0, 0.0017361111240461469, 0.0, 0.0, 0.0203993059694767, 0.0, 0.0, 0.029296875, 0.01519097201526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032552082557231188, 0.008463541977107525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0006510416860692203, 0.0069444444961845875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.177517369389534, 0.0004340277810115367, 0.0, 0.0, 0.2083333283662796, 0.0, 0.0052083334885537624, 0.0, 0.0, 0.0, 0.009548611007630825, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.9993489384651184, 0.0, 0.02690972201526165, 0.0, 0.0, 0.0, 0.010416666977107525, 0.0184461809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1961805522441864, 0.1026475727558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005425347480922937, 0.0, 0.9997829794883728, 0.2350260466337204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006510416860692203, 0.0, 0.989366352558136, 0.00021701389050576836, 0.0, 0.02756076492369175, 0.01519097201526165, 0.0, 0.0071614584885537624, 0.0, 0.0, 0.0, 0.0, 0.0316840298473835, 0.0, 0.01215277798473835, 0.0004340277810115367, 0.01410590298473835, 0.0340711809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0325520820915699, 0.0481770820915699, 0.0, 0.0, 0.0, 0.009114583022892475, 0.0, 0.00021701389050576836, 0.0301649309694767, 0.0, 0.8550347089767456, 0.9993489384651184, 0.0, 0.0, 0.0, 0.029296875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.071180559694767, 0.0015190972480922937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.005642361007630825, 0.0, 0.0, 0.0, 0.0, 0.0496961809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9787326455116272, 0.0, 0.0, 0.0, 0.6512587070465088, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.01519097201526165, 0.01410590298473835, 0.02799479104578495, 0.02213541604578495, 0.0, 0.0028211805038154125, 0.0262586809694767, 0.0, 0.1963975727558136, 0.00390625, 0.0, 0.0, 0.00390625, 0.0225694440305233, 0.0067274305038154125, 0.0427517369389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0164930559694767, 0.0, 0.02083333395421505, 0.0, 0.0078125, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02408854104578495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02690972201526165, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.009765625, 0.0212673619389534, 0.0, 0.0026041667442768812, 0.0373263880610466, 0.0, 0.0028211805038154125, 0.0186631940305233, 0.0, 0.0, 0.0034722222480922937, 0.0, 0.0234375, 0.0, 0.0729166641831398, 0.0334201380610466, 0.0, 0.0, 0.0, 0.02018229104578495, 0.0, 0.0, 0.0, 0.0, 0.02777777798473835, 0.0, 0.0, 0.02473958395421505, 0.0, 0.0, 0.2250434011220932, 0.0, 0.0010850694961845875, 0.0, 0.0, 0.0546875, 0.0, 0.0303819440305233, 0.0, 0.4457465410232544, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.021484375, 0.0, 0.0, 0.02495659701526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.0, 0.9993489384651184, 0.0, 0.01128472201526165, 0.0, 0.0384114570915699, 0.0327690988779068, 0.0, 0.0421006940305233, 0.0368923619389534, 0.044921875, 0.0, 0.0, 0.421440988779068, 0.01779513992369175, 0.0, 0.0, 0.0, 0.0, 0.0514322929084301, 0.0, 0.0004340277810115367, 0.0, 0.02387152798473835, 0.01779513992369175, 0.0106336809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9993489384651184, 0.7541232705116272, 0.0, 0.0, 0.0004340277810115367, 0.0442708320915699, 0.00629340298473835, 0.007378472480922937, 0.0006510416860692203, 0.0, 0.0, 0.02799479104578495, 0.0, 0.0, 0.0, 0.014756944961845875, 0.0, 0.0753038227558136, 0.02886284701526165, 0.0753038227558136, 0.0, 0.0, 0.0030381944961845875, 0.0, 0.0, 0.0, 0.259331613779068, 0.0, 0.0013020833721384406, 0.005425347480922937, 0.0, 0.0, 0.0, 0.9997829794883728, 0.0, 0.0, 0.0, 0.0015190972480922937, 0.0004340277810115367, 0.0, 0.0004340277810115367, 0.2745225727558136, 0.0, 0.0015190972480922937, 0.0, 0.0, 0.0, 0.0, 0.009114583022892475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0145399309694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0167100690305233, 0.0, 0.0008680555620230734, 0.0, 0.0325520820915699, 0.0, 0.0, 0.0, 0.01974826492369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032552082557231188, 0.0, 0.0, 0.0, 0.0, 0.009982638992369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.033203125, 0.0529513880610466, 0.01171875, 0.0, 0.0, 0.0, 0.0, 0.9993489384651184, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.01410590298473835, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0518663190305233, 0.011935763992369175, 0.00434027798473835, 0.0, 0.0, 0.0, 0.0301649309694767, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0392795130610466, 0.0, 0.0, 0.0, 0.0290798619389534, 0.0032552082557231188, 0.0026041667442768812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01605902798473835, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.009982638992369175, 0.0, 0.0, 0.0325520820915699, 0.0, 0.00434027798473835, 0.0, 0.9845920205116272, 0.0, 0.698350727558136, 0.0, 0.0284288190305233, 0.0466579869389534, 0.0, 0.02191840298473835, 0.0, 0.0, 0.067274309694767, 0.9997829794883728, 0.0, 0.0, 0.006076388992369175, 0.0, 0.02560763992369175, 0.0, 0.0, 0.0494791679084301, 0.005425347480922937, 0.7914496660232544, 0.0023871527519077063, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 2104517.0285429433 (unstructured) 0 (structured)

max weight is  tensor([9.8234e-02, 1.2456e-01, 5.6228e-02, 4.1681e-10, 8.0518e-01, 7.9360e-10,
        2.2343e-01, 2.0604e-10, 2.1853e-01, 4.6268e-10, 2.2243e-01, 8.3814e-04,
        1.6536e-01, 7.1468e-10, 4.0791e-03, 2.7387e-10, 5.2370e-02, 2.9343e-10,
        3.6073e-01, 2.6110e-01, 1.6845e-10, 7.1468e-10, 1.0375e-10, 7.1468e-10,
        5.6741e-02, 2.0031e-01, 4.6268e-10, 1.2343e-02, 2.1473e-01, 4.3360e-10,
        7.0173e-03, 3.3080e-01, 8.4321e-01, 2.7034e-01, 1.5728e-10, 7.1468e-10,
        1.1416e-09, 5.6053e-02, 3.4449e-01, 1.6557e-02, 5.0527e-10, 8.3909e-02,
        4.9215e-10, 7.1468e-10, 2.9535e-01, 2.3837e-01, 4.1301e-01, 1.2090e-01,
        3.1871e-01, 1.1834e-09, 6.7824e-01, 6.0201e-01, 1.5064e-01, 1.0007e-10,
        6.4336e-01, 4.9109e-11, 4.8169e-02, 5.2602e-10, 7.5783e-04, 2.4840e-10,
        3.0808e-10, 2.2855e-02, 4.5030e-10, 8.9287e-10], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1958, 0.1108, 0.1316, 0.1767, 0.1904, 0.2304, 0.2190, 0.1584, 0.2011,
        0.1487, 0.1757, 0.1769, 0.1574, 0.2334, 0.1515, 0.1097, 0.3576, 0.1954,
        0.1736, 0.1525, 0.1453, 0.2085, 0.1653, 0.0791, 0.1196, 0.1352, 0.1410,
        0.1748, 0.1709, 0.1702, 0.1887, 0.1912, 0.1296, 0.1869, 0.1888, 0.1407,
        0.1178, 0.1796, 0.1870, 0.1543, 0.1873, 0.1470, 0.2298, 0.2741, 0.1349,
        0.1884, 0.1315, 0.1806, 0.1685, 0.1568, 0.1602, 0.1732, 0.1925, 0.1533,
        0.1732, 0.1901, 0.1837, 0.2043, 0.1402, 0.1146, 0.1266, 0.1186, 0.1040,
        0.1603], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.1397e-02, 8.1067e-02, 8.1630e-02, 8.9304e-02, 8.1860e-02, 1.0220e-01,
        8.2023e-02, 7.1249e-02, 1.0335e-08, 9.4951e-02, 7.9878e-02, 9.7149e-02,
        1.0062e-01, 9.7608e-02, 9.6504e-02, 9.1822e-02, 8.5831e-02, 1.1219e-01,
        6.6738e-02, 6.4168e-02, 5.2212e-02, 1.0238e-01, 1.0284e-01, 9.1308e-02,
        1.0480e-01, 5.8953e-02, 1.1414e-01, 7.5119e-02, 6.9906e-02, 1.5258e-01,
        8.5333e-02, 1.4725e-01, 1.0129e-01, 9.6092e-02, 1.1932e-01, 9.9346e-02,
        9.9141e-02, 9.6285e-02, 8.8236e-02, 1.4646e-01, 1.3175e-01, 9.8509e-02,
        8.0008e-02, 9.0190e-02, 8.5904e-02, 9.6124e-02, 1.0803e-01, 7.3378e-02,
        9.3589e-02, 7.9357e-02, 8.3853e-02, 8.0996e-02, 9.6199e-02, 1.1432e-01,
        8.0587e-02, 7.4427e-02, 9.5508e-02, 8.3346e-02, 9.8622e-02, 1.0852e-01,
        1.4151e-01, 1.2118e-01, 8.7077e-02, 1.0730e-01, 1.0001e-01, 9.5610e-02,
        8.6433e-02, 9.8476e-02, 9.8817e-02, 1.1364e-01, 9.5374e-02, 1.0768e-01,
        6.6843e-02, 1.2038e-01, 9.1557e-02, 1.1275e-01, 8.6946e-02, 7.4196e-02,
        8.7726e-02, 1.1087e-01, 9.3803e-02, 9.1962e-02, 7.1304e-02, 9.9173e-02,
        8.7192e-02, 1.0880e-01, 1.2590e-01, 7.1690e-02, 1.1331e-01, 7.7684e-02,
        8.4173e-02, 1.0346e-01, 1.1725e-01, 8.6897e-02, 7.4487e-02, 1.0858e-01,
        9.0752e-02, 9.7283e-02, 7.4252e-02, 8.1384e-02, 7.9014e-02, 1.4927e-01,
        9.2733e-02, 1.6069e-01, 7.4150e-02, 8.2246e-02, 9.7026e-02, 8.4253e-02,
        7.6254e-02, 8.5450e-02, 8.6631e-02, 9.1575e-02, 8.5103e-02, 1.6322e-01,
        9.3295e-02, 6.5658e-02, 9.8585e-02, 6.9196e-02, 7.5387e-02, 9.1881e-02,
        8.9894e-02, 8.6286e-02, 1.0165e-01, 6.9567e-02, 8.9511e-02, 7.3122e-02,
        7.3802e-02, 9.0549e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.3966e-02, 1.0648e-01, 8.4803e-02, 9.3990e-02, 9.4253e-02, 8.8661e-02,
        9.5521e-02, 9.0152e-02, 1.0101e-01, 1.0494e-01, 1.1115e-01, 8.5814e-02,
        1.2406e-01, 9.7709e-02, 1.0542e-01, 1.0164e-01, 1.0246e-01, 7.9926e-02,
        9.9753e-02, 9.3866e-02, 1.0567e-01, 8.6071e-02, 8.8472e-02, 9.8449e-02,
        1.0427e-01, 9.2570e-02, 9.0827e-02, 8.6773e-02, 8.4730e-02, 9.6690e-02,
        9.3344e-02, 8.9832e-02, 1.0337e-01, 1.0735e-01, 9.3689e-02, 9.7011e-02,
        8.7430e-02, 9.3819e-02, 9.6035e-02, 9.2862e-02, 1.0049e-01, 8.6866e-02,
        9.4622e-02, 9.8091e-02, 9.4618e-02, 8.4324e-02, 1.0974e-01, 6.9709e-02,
        9.4019e-02, 8.9658e-02, 9.4721e-02, 9.6988e-02, 9.8352e-02, 1.0646e-01,
        8.9497e-02, 9.4082e-02, 1.0230e-01, 1.0898e-01, 9.6321e-02, 9.2923e-02,
        8.2545e-02, 1.0525e-01, 1.0409e-01, 9.9374e-02, 9.3577e-02, 8.1505e-02,
        1.4540e-01, 1.0073e-01, 8.9760e-02, 8.6098e-02, 9.2472e-02, 9.5454e-02,
        8.8470e-02, 8.6245e-02, 9.6638e-02, 1.0293e-01, 1.0298e-01, 1.0050e-01,
        9.6868e-02, 9.9953e-02, 9.2221e-02, 8.9721e-02, 1.1105e-01, 9.7958e-02,
        1.0438e-01, 7.6688e-02, 9.7062e-02, 8.1790e-02, 9.2414e-02, 1.0144e-01,
        9.7041e-02, 1.0941e-01, 8.6351e-02, 9.1045e-02, 1.0052e-01, 1.0238e-01,
        1.0076e-01, 9.3753e-02, 1.0016e-01, 9.5773e-02, 9.9986e-02, 1.0672e-01,
        1.0239e-01, 1.1072e-01, 9.0731e-02, 8.4823e-02, 8.2987e-02, 9.4823e-02,
        9.8778e-02, 1.0313e-01, 9.1667e-02, 1.0289e-01, 9.3053e-02, 9.9312e-02,
        9.7004e-02, 9.3684e-02, 9.5293e-02, 1.0829e-01, 8.5676e-02, 2.3505e-08,
        9.7281e-02, 9.5807e-02, 1.1002e-01, 9.7496e-02, 9.7535e-02, 1.0173e-01,
        9.1697e-02, 9.8100e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.9679e-08, 1.0537e-01, 1.7427e-01, 1.2565e-01, 1.5277e-01, 1.1137e-01,
        1.2319e-01, 1.1627e-01, 1.3277e-01, 1.1839e-01, 1.3491e-01, 1.2953e-01,
        9.2601e-02, 1.2219e-01, 1.2402e-01, 1.2016e-01, 1.2027e-01, 1.2356e-01,
        1.3499e-01, 1.2198e-01, 5.5497e-08, 1.4229e-01, 1.5347e-01, 1.1106e-01,
        1.3072e-01, 1.3122e-01, 1.2738e-01, 1.3567e-01, 5.9822e-08, 1.4420e-01,
        1.3865e-01, 5.2671e-08, 1.4544e-01, 2.0431e-08, 1.2493e-01, 1.3603e-01,
        1.2232e-01, 1.1218e-01, 9.4356e-08, 1.3184e-01, 1.1532e-01, 1.4383e-01,
        1.2630e-01, 1.0468e-01, 1.0969e-01, 1.1949e-01, 1.1603e-01, 1.3364e-01,
        1.4572e-01, 1.3073e-01, 1.4454e-01, 1.4754e-01, 1.2523e-01, 1.0011e-01,
        1.1994e-01, 1.3658e-01, 1.1207e-01, 1.3572e-01, 1.3386e-01, 1.9679e-08,
        1.1647e-01, 7.4327e-02, 1.6225e-08, 1.3462e-01, 1.0929e-01, 1.3782e-01,
        1.1818e-01, 1.2386e-01, 1.3633e-01, 1.0860e-01, 1.2413e-01, 1.1932e-01,
        1.1763e-01, 1.1719e-01, 1.1509e-01, 1.2663e-01, 1.1884e-01, 1.5891e-01,
        1.4636e-01, 1.2967e-01, 1.0969e-01, 1.3490e-01, 1.2899e-01, 1.4112e-01,
        1.2876e-01, 1.1023e-01, 1.2526e-01, 9.3775e-02, 1.3483e-01, 1.2224e-01,
        1.1760e-01, 1.2802e-01, 1.0882e-01, 1.1549e-01, 1.2451e-01, 1.1995e-01,
        8.7325e-08, 1.3265e-01, 1.1917e-01, 1.3347e-01, 1.2394e-01, 1.2406e-01,
        1.1124e-01, 1.2861e-01, 1.3068e-01, 1.3181e-01, 4.9222e-08, 1.2958e-01,
        1.2521e-01, 1.1611e-01, 1.3901e-01, 2.4913e-08, 1.1987e-01, 1.1825e-01,
        1.2432e-01, 1.2160e-01, 1.0575e-01, 1.3949e-01, 2.8158e-08, 1.4178e-01,
        1.3899e-01, 1.5137e-01, 1.2574e-01, 1.2620e-01, 1.3877e-01, 1.1970e-01,
        9.8642e-02, 3.9326e-08, 1.7406e-01, 1.3868e-01, 1.1403e-01, 1.3186e-01,
        7.3522e-03, 1.0990e-01, 1.1761e-01, 1.2283e-01, 1.2541e-01, 1.2681e-01,
        1.2997e-01, 1.2381e-01, 1.2204e-01, 1.3776e-01, 1.3736e-01, 1.3291e-01,
        1.5050e-01, 2.3428e-08, 1.3070e-01, 1.0909e-01, 1.0673e-01, 1.3178e-01,
        1.2840e-01, 1.2515e-01, 1.3782e-01, 2.1033e-08, 1.1542e-01, 1.4993e-01,
        1.3538e-01, 1.6358e-01, 1.2785e-01, 1.3146e-01, 1.2057e-01, 1.0436e-01,
        1.2596e-01, 1.5179e-01, 1.2727e-01, 1.1901e-01, 1.3561e-01, 1.2612e-01,
        1.1192e-01, 1.3162e-01, 1.3873e-01, 1.4520e-01, 1.2480e-01, 1.1659e-01,
        1.2118e-01, 1.1875e-01, 1.1594e-01, 1.5538e-01, 1.3037e-01, 1.1645e-01,
        1.1692e-01, 1.1118e-01, 1.3050e-01, 1.2339e-01, 1.0810e-01, 1.2162e-01,
        1.1145e-01, 1.2393e-01, 1.1813e-01, 1.2801e-01, 1.1468e-01, 1.3483e-01,
        7.3898e-02, 1.2232e-01, 1.0798e-01, 1.1006e-01, 1.2453e-01, 1.1044e-01,
        1.3949e-01, 1.3515e-01, 1.1277e-01, 1.1608e-01, 1.3309e-01, 1.2119e-01,
        1.0573e-01, 1.1775e-01, 1.3342e-01, 1.1969e-01, 3.9710e-08, 9.9991e-02,
        1.2479e-01, 1.2966e-01, 1.4042e-01, 1.1813e-01, 1.2755e-01, 2.8158e-08,
        1.3062e-01, 1.3172e-01, 1.2838e-01, 1.5864e-01, 1.6223e-01, 1.3291e-01,
        1.3941e-01, 1.2211e-01, 1.2127e-01, 1.1784e-01, 1.3734e-01, 1.2934e-01,
        2.4913e-08, 9.5341e-02, 6.3234e-08, 1.3574e-01, 1.2751e-01, 1.3349e-01,
        1.2803e-01, 1.1124e-01, 1.0543e-01, 1.0984e-01, 1.0958e-01, 2.0431e-08,
        1.4560e-01, 1.2231e-01, 1.0794e-01, 1.3120e-01, 9.6727e-02, 1.3854e-01,
        9.9237e-02, 1.1862e-01, 1.1838e-01, 2.8158e-08, 1.4057e-01, 1.2153e-01,
        1.3411e-01, 1.2986e-01, 1.4905e-01, 1.4341e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.1966e-07, 7.6014e-02, 1.5608e-07, 1.9930e-07, 7.9369e-02, 2.8767e-07,
        1.1185e-07, 6.9892e-08, 5.5720e-08, 8.5450e-08, 5.7157e-02, 6.6818e-02,
        7.6138e-02, 2.4124e-07, 1.6679e-07, 4.9748e-02, 6.6558e-02, 9.9340e-08,
        5.8398e-02, 8.2766e-02, 7.0498e-08, 5.9655e-02, 6.3105e-02, 1.6023e-07,
        2.5468e-07, 8.4499e-02, 6.7361e-02, 8.3976e-08, 2.6384e-07, 2.3577e-07,
        8.3550e-08, 6.5235e-02, 1.1656e-07, 5.7115e-02, 4.6937e-02, 6.9892e-08,
        1.6972e-07, 7.3929e-02, 2.3577e-07, 9.5552e-08, 2.4124e-07, 2.4231e-07,
        9.6230e-08, 5.3811e-02, 1.7490e-07, 7.2849e-02, 8.4685e-02, 5.8397e-02,
        3.5659e-02, 1.1185e-07, 1.0826e-07, 2.2863e-07, 2.6384e-07, 1.6485e-07,
        2.6384e-07, 7.7108e-02, 4.6264e-08, 1.1185e-07, 2.8175e-07, 1.7891e-07,
        9.6230e-08, 1.5877e-07, 1.4716e-07, 9.8377e-08, 6.8767e-02, 5.3002e-02,
        5.6261e-02, 6.6060e-02, 7.6917e-02, 1.5535e-07, 7.8245e-08, 3.3357e-02,
        8.1470e-02, 1.4016e-07, 4.5550e-02, 4.6264e-08, 3.0654e-07, 6.7882e-02,
        7.5250e-02, 1.8135e-07, 9.3645e-08, 1.6485e-07, 2.8181e-07, 7.8245e-08,
        7.8245e-08, 2.8181e-07, 1.1656e-07, 6.0683e-02, 6.7494e-02, 1.8613e-07,
        7.8245e-08, 2.6384e-07, 5.9455e-02, 5.8824e-02, 7.4715e-02, 4.3996e-02,
        5.7954e-02, 3.4656e-02, 3.7778e-07, 7.0267e-02, 7.8751e-02, 2.6459e-07,
        6.7434e-02, 5.9821e-02, 7.6246e-02, 7.3545e-02, 4.3000e-02, 1.2890e-07,
        2.6231e-07, 6.0047e-02, 2.4124e-07, 2.1379e-07, 1.4016e-07, 9.5612e-02,
        7.3653e-02, 1.0961e-01, 8.3549e-08, 5.5974e-02, 1.0146e-07, 9.6230e-08,
        5.9473e-02, 1.1723e-07, 1.4016e-07, 7.2225e-02, 8.2250e-02, 7.4182e-02,
        2.8175e-07, 2.9638e-07, 5.8393e-02, 1.6485e-07, 2.0629e-07, 2.1985e-07,
        4.8383e-02, 5.6496e-07, 5.6939e-02, 6.3616e-02, 2.8181e-07, 7.6050e-02,
        1.7363e-07, 7.8557e-02, 4.0366e-02, 6.5043e-02, 3.2220e-07, 3.5863e-02,
        7.3721e-02, 7.8245e-08, 6.7686e-02, 5.1829e-02, 1.8135e-07, 1.9127e-03,
        2.3577e-07, 6.1632e-02, 2.8181e-07, 4.2442e-02, 8.4468e-08, 1.2266e-07,
        5.1490e-02, 8.3315e-02, 1.1656e-07, 1.6557e-07, 2.8181e-07, 1.9767e-07,
        2.6384e-07, 1.6023e-07, 2.7263e-07, 2.4153e-07, 4.6097e-02, 5.5590e-02,
        8.5674e-02, 3.6355e-02, 6.5821e-02, 1.4128e-07, 1.2324e-02, 5.9462e-02,
        1.6750e-07, 5.8371e-02, 5.0450e-02, 6.4828e-02, 8.3549e-08, 7.3354e-02,
        7.9394e-02, 7.4050e-02, 6.1317e-02, 7.2199e-02, 4.5972e-02, 6.1289e-02,
        6.5137e-02, 5.6959e-02, 1.0308e-07, 2.3577e-07, 8.6142e-02, 3.6878e-07,
        3.2220e-07, 2.4124e-07, 1.4488e-07, 6.7663e-02, 7.0325e-02, 7.6988e-02,
        2.4444e-07, 7.7828e-02, 5.1598e-08, 6.7007e-02, 8.1318e-02, 3.2919e-07,
        3.7778e-07, 5.2118e-02, 5.4604e-02, 2.6384e-07, 6.2407e-02, 7.7676e-02,
        3.1966e-07, 2.7263e-07, 9.8014e-02, 6.4142e-02, 1.9930e-07, 6.8546e-02,
        1.1723e-07, 1.4128e-07, 4.1286e-07, 2.9638e-07, 4.6472e-02, 2.3577e-07,
        7.6393e-02, 4.3938e-02, 1.0826e-07, 1.6349e-07, 6.1748e-09, 2.9021e-07,
        7.0754e-08, 2.1287e-07, 1.4128e-07, 6.3611e-02, 3.6522e-08, 7.1539e-02,
        5.1656e-02, 5.7282e-02, 1.8254e-07, 1.2636e-01, 1.4555e-07, 7.6477e-02,
        2.4124e-07, 3.9489e-07, 5.1663e-02, 8.0464e-02, 7.4882e-02, 2.4124e-07,
        1.6485e-07, 7.1762e-02, 2.7263e-07, 1.9767e-07, 6.3110e-02, 2.5468e-07,
        3.0654e-07, 2.6384e-07, 2.7263e-07, 5.3240e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.6022e-02, 4.6925e-02, 1.3547e-02, 3.1257e-07, 3.5875e-02, 1.7310e-07,
        3.8721e-02, 2.3980e-07, 2.1391e-07, 5.3079e-02, 1.0067e-07, 1.2329e-02,
        8.2314e-08, 1.3662e-07, 1.1049e-07, 3.5512e-02, 1.1049e-07, 8.7968e-08,
        1.0604e-07, 1.0686e-07, 3.9515e-02, 1.8813e-07, 2.2630e-07, 1.9759e-07,
        4.6774e-02, 2.0680e-07, 2.6343e-08, 2.2534e-07, 1.3777e-07, 1.3846e-07,
        1.8813e-07, 1.0959e-07, 4.6885e-02, 2.2340e-07, 1.5545e-08, 1.0067e-07,
        1.4353e-07, 8.9555e-08, 2.2340e-07, 4.5709e-02, 3.8571e-02, 6.9016e-08,
        1.6787e-07, 3.2366e-02, 5.0673e-02, 1.0007e-07, 2.1386e-07, 1.3055e-02,
        3.2982e-08, 7.9776e-08, 3.8074e-02, 4.9734e-02, 1.1974e-07, 2.0680e-07,
        3.3600e-07, 5.9636e-07, 1.0885e-07, 1.7310e-07, 4.8053e-08, 5.4012e-02,
        4.2750e-02, 1.1974e-07, 1.2161e-07, 1.2223e-07, 2.0699e-07, 1.0959e-07,
        3.3600e-07, 1.6787e-07, 4.7338e-02, 1.2969e-07, 5.2913e-08, 3.0798e-02,
        2.9268e-07, 4.5266e-02, 1.2161e-07, 1.1687e-07, 1.4353e-07, 2.9014e-02,
        1.5871e-07, 4.4179e-02, 2.9295e-07, 1.5472e-07, 2.5360e-07, 1.9584e-02,
        1.8813e-07, 4.5436e-02, 1.5871e-07, 1.0885e-07, 2.5360e-07, 4.6236e-02,
        2.9492e-02, 3.6572e-02, 1.0784e-07, 2.0680e-07, 4.3331e-02, 2.1386e-07,
        2.3620e-07, 1.0829e-07, 4.0802e-07, 2.2630e-07, 2.4834e-07, 1.4353e-07,
        1.6389e-07, 1.1049e-07, 1.5272e-07, 6.9016e-08, 2.3071e-07, 2.9160e-07,
        3.1457e-02, 8.7646e-08, 4.2769e-07, 2.4614e-02, 2.9268e-07, 2.3071e-07,
        4.0414e-02, 1.2223e-07, 5.2628e-02, 1.5959e-07, 2.2081e-07, 3.2503e-02,
        4.8053e-08, 1.8813e-07, 1.5871e-07, 8.1189e-08, 2.9956e-07, 5.2913e-08,
        4.1298e-02, 6.9016e-08, 1.0067e-07, 5.1719e-08, 3.3284e-07, 5.2985e-02,
        3.9546e-02, 5.5744e-02, 1.7407e-07, 1.1974e-07, 3.5243e-02, 2.1386e-07,
        4.7294e-02, 2.9160e-07, 1.7310e-07, 2.9268e-07, 2.9268e-07, 2.9268e-07,
        4.1420e-02, 2.4639e-07, 4.1983e-02, 3.9068e-02, 3.3356e-07, 1.1687e-07,
        5.1876e-02, 8.4979e-08, 1.1357e-07, 4.1429e-02, 2.6502e-07, 1.2602e-07,
        2.2340e-07, 1.0936e-07, 2.2630e-07, 1.4526e-07, 1.2161e-07, 3.9196e-07,
        2.1391e-07, 2.1386e-07, 5.2973e-08, 1.5871e-07, 1.1261e-07, 3.6995e-02,
        2.2454e-07, 1.8813e-07, 8.6680e-08, 2.3841e-03, 4.0219e-02, 1.4232e-07,
        3.2003e-07, 2.2340e-07, 2.4430e-08, 4.7527e-02, 8.3717e-08, 4.6795e-02,
        2.0680e-07, 2.2454e-07, 2.0680e-07, 6.9016e-08, 5.5035e-02, 2.5360e-07,
        1.2161e-07, 1.2602e-07, 4.6673e-07, 2.9268e-07, 3.1542e-02, 1.2711e-07,
        6.6804e-08, 4.1023e-07, 4.0847e-02, 2.9268e-07, 1.5871e-07, 2.9268e-07,
        4.6063e-08, 4.0958e-02, 1.0007e-07, 5.3223e-02, 4.6310e-08, 1.0067e-07,
        4.7323e-02, 7.9812e-09, 1.1215e-07, 2.2534e-07, 1.1215e-07, 4.0495e-02,
        4.2770e-07, 4.1644e-02, 1.7310e-07, 5.0861e-02, 3.9431e-02, 1.0885e-07,
        2.3583e-07, 3.2247e-02, 3.4697e-02, 2.4447e-02, 5.9084e-08, 4.6750e-02,
        1.1357e-07, 1.1184e-07, 5.1719e-08, 9.0800e-08, 1.0885e-07, 3.2519e-07,
        5.1719e-08, 1.1215e-07, 4.7946e-02, 3.3356e-07, 2.2081e-07, 4.5429e-02,
        8.6239e-08, 4.8053e-08, 4.4909e-02, 2.2340e-07, 8.7968e-08, 5.7296e-02,
        8.9555e-08, 8.2314e-08, 1.5828e-07, 6.4142e-08, 4.6879e-02, 4.6662e-02,
        5.1298e-02, 2.2534e-07, 2.1431e-02, 2.3071e-07, 1.5871e-07, 2.9295e-07,
        2.1386e-07, 1.1215e-07, 4.6673e-07, 5.1194e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.3728e-07, 3.2249e-07, 1.3782e-07, 1.6054e-07, 2.1797e-07, 3.1050e-07,
        1.8424e-07, 4.6918e-02, 4.5533e-07, 1.8087e-07, 1.0708e-07, 3.5591e-07,
        5.1206e-08, 2.3778e-07, 1.8000e-07, 5.8892e-08, 1.5085e-07, 2.3778e-07,
        1.8000e-07, 1.4056e-07, 1.2934e-07, 9.6977e-08, 3.5015e-08, 1.1106e-07,
        1.7488e-07, 2.6219e-07, 4.0161e-07, 2.6164e-07, 2.4411e-08, 1.8087e-07,
        3.5591e-07, 4.5533e-07, 3.2100e-07, 1.4186e-07, 1.6054e-07, 3.2249e-07,
        1.1106e-07, 4.8179e-08, 1.4605e-07, 1.6252e-07, 1.3538e-07, 2.2230e-07,
        1.8467e-07, 2.2190e-07, 2.4381e-02, 1.3498e-07, 7.8378e-08, 1.4908e-07,
        2.3778e-07, 4.3015e-08, 3.3523e-08, 2.6164e-07, 2.0878e-07, 8.2973e-08,
        3.5376e-07, 6.6247e-08, 2.1069e-07, 2.1652e-07, 6.4235e-08, 3.5591e-07,
        3.5753e-07, 3.3254e-07, 9.3254e-08, 1.4186e-07, 1.3782e-07, 1.4186e-07,
        3.5591e-07, 5.9708e-02, 4.9344e-07, 7.8378e-08, 1.4908e-07, 4.0508e-07,
        3.0983e-07, 2.4070e-07, 2.9576e-02, 9.1742e-08, 2.3778e-07, 2.3778e-07,
        2.3927e-07, 2.6219e-07, 7.9297e-08, 5.8892e-08, 7.3097e-08, 1.1331e-07,
        5.1206e-08, 5.7513e-08, 1.2037e-07, 2.6219e-07, 2.9785e-07, 1.2934e-07,
        8.0843e-08, 9.3254e-08, 2.4672e-07, 1.2162e-07, 3.5753e-07, 2.0569e-07,
        2.3927e-07, 5.7513e-08, 1.6054e-07, 1.2037e-07, 1.3140e-07, 2.0569e-07,
        1.4266e-07, 9.2204e-08, 5.5012e-08, 2.0569e-07, 1.6252e-07, 1.3538e-07,
        5.7513e-08, 1.9623e-07, 2.3728e-07, 3.5376e-07, 1.4446e-07, 1.4908e-07,
        2.0569e-07, 2.2230e-07, 9.2204e-08, 2.5052e-07, 1.3782e-07, 2.9840e-07,
        1.4186e-07, 2.3778e-07, 7.1541e-08, 8.1006e-02, 1.6091e-07, 3.5376e-07,
        5.0413e-03, 3.5015e-08, 1.1574e-07, 2.2230e-07, 2.0878e-07, 1.1574e-07,
        5.7513e-08, 3.1464e-07, 1.3498e-07, 3.0606e-07, 9.1742e-08, 1.9572e-02,
        2.1652e-07, 2.6164e-07, 2.2004e-07, 1.4186e-07, 3.8202e-07, 1.2037e-07,
        6.8971e-08, 3.4990e-07, 2.3927e-07, 2.1078e-07, 5.6280e-08, 5.8892e-08,
        1.3498e-07, 1.9676e-07, 1.8094e-07, 1.4908e-07, 5.1141e-08, 2.3778e-07,
        7.3097e-08, 7.1541e-08, 1.2037e-07, 3.5376e-07, 2.1069e-07, 1.5085e-07,
        2.0333e-03, 5.7513e-08, 3.3254e-07, 2.4296e-02, 1.5085e-07, 2.8746e-07,
        1.3075e-07, 4.0508e-07, 1.5505e-08, 3.2503e-07, 5.7513e-08, 1.8208e-07,
        1.1996e-07, 6.6247e-08, 5.6280e-08, 5.7513e-08, 2.2725e-07, 1.1598e-07,
        3.4990e-07, 9.3254e-08, 1.4105e-07, 6.5397e-08, 2.5824e-08, 2.6164e-07,
        5.1206e-08, 3.4990e-07, 1.2020e-07, 2.5052e-07, 3.2799e-07, 5.8892e-08,
        9.5175e-08, 1.8000e-07, 2.7743e-07, 2.3155e-02, 2.7257e-07, 1.7697e-07,
        1.1394e-07, 5.8892e-08, 1.3782e-07, 5.1206e-08, 1.1355e-07, 9.2204e-08,
        1.8739e-07, 2.1797e-07, 3.7099e-02, 3.5591e-07, 3.5591e-07, 3.3254e-07,
        8.1958e-08, 1.9623e-07, 1.2709e-07, 1.0523e-07, 4.5533e-07, 3.1193e-07,
        2.3001e-07, 9.3254e-08, 1.4605e-07, 2.3649e-02, 9.2204e-08, 9.3254e-08,
        2.5623e-07, 9.3254e-08, 1.5085e-07, 1.1331e-07, 2.9289e-02, 1.1869e-07,
        9.6977e-08, 2.2230e-07, 1.9623e-07, 2.4070e-07, 1.8000e-07, 5.1206e-08,
        3.5753e-07, 1.4186e-07, 5.8771e-02, 1.4217e-02, 3.6253e-08, 1.0708e-07,
        3.1050e-07, 1.6091e-07, 5.1206e-08, 3.5753e-07, 9.3254e-08, 4.3484e-07,
        3.5015e-08, 2.3001e-07, 1.5085e-07, 4.3484e-07, 1.9623e-07, 4.5681e-07,
        3.0983e-07, 2.0048e-07, 5.1141e-08, 1.3498e-07, 1.6252e-07, 3.5376e-07,
        2.6164e-07, 7.1541e-08, 2.3728e-07, 9.3254e-08, 1.9676e-07, 2.0353e-02,
        2.5052e-07, 1.2037e-07, 3.5376e-07, 2.9840e-07, 2.8807e-02, 3.5376e-07,
        1.4186e-07, 2.2230e-07, 7.1541e-08, 3.5753e-07, 9.6977e-08, 2.3927e-07,
        3.5591e-07, 1.3782e-07, 7.8378e-08, 3.5591e-07, 2.6028e-02, 5.8970e-08,
        1.4186e-07, 9.7191e-08, 2.0234e-08, 1.0708e-07, 5.8892e-08, 2.3927e-07,
        3.5015e-08, 3.0983e-07, 2.1374e-08, 9.1742e-08, 7.9297e-08, 1.9623e-07,
        5.8970e-08, 7.3097e-08, 5.1206e-08, 3.0606e-07, 1.5085e-07, 2.6164e-07,
        5.8892e-08, 1.1106e-07, 3.9860e-02, 5.8892e-08, 1.4186e-07, 2.3927e-07,
        5.7513e-08, 2.2004e-07, 1.1574e-07, 1.3498e-07, 9.5175e-08, 1.0708e-07,
        1.2934e-07, 2.9328e-07, 2.5383e-02, 1.6063e-07, 5.7513e-08, 1.4868e-07,
        7.1541e-08, 5.7513e-08, 1.1355e-07, 2.1069e-07, 2.3927e-07, 9.8211e-02,
        4.5681e-07, 1.0838e-07, 1.5701e-07, 9.5536e-08, 3.9225e-07, 1.1841e-07,
        1.4510e-07, 1.6551e-07, 1.8000e-07, 1.2194e-07, 1.5085e-07, 1.3498e-07,
        1.7180e-07, 2.3778e-07, 3.3254e-07, 1.8094e-07, 3.7949e-02, 9.9232e-02,
        4.0508e-07, 2.6164e-07, 1.4605e-07, 3.2811e-07, 9.3254e-08, 1.0708e-07,
        1.1895e-01, 2.0878e-07, 1.2891e-02, 4.3484e-07, 1.9676e-07, 3.2967e-02,
        1.6897e-07, 3.2811e-07, 1.3265e-07, 9.1742e-08, 1.8000e-07, 4.3484e-07,
        1.8000e-07, 1.4266e-02, 3.8202e-07, 9.1742e-08, 1.8043e-07, 2.3001e-07,
        1.1106e-07, 4.3484e-07, 1.9676e-07, 1.2037e-07, 2.0569e-07, 1.6252e-07,
        1.7814e-07, 1.4908e-07, 5.5012e-08, 1.1106e-07, 2.3778e-07, 2.5052e-07,
        1.2020e-07, 3.9403e-07, 2.9067e-02, 3.2503e-07, 2.0878e-07, 7.3589e-08,
        1.9623e-07, 3.0983e-07, 2.0569e-07, 4.3484e-07, 1.1331e-07, 2.2230e-07,
        1.6476e-07, 9.3254e-08, 4.0508e-07, 3.5591e-07, 5.1141e-08, 1.6252e-07,
        2.8224e-07, 5.5012e-08, 1.6252e-07, 1.6897e-07, 1.1996e-07, 1.7814e-07,
        5.8892e-08, 3.1193e-07, 1.1355e-07, 5.8970e-08, 5.3970e-07, 2.2004e-07,
        5.1206e-08, 3.5376e-07, 2.3927e-07, 2.3209e-07, 2.8911e-02, 1.4186e-07,
        1.0708e-07, 5.7513e-08, 1.5272e-07, 2.6164e-07, 1.0708e-07, 2.1069e-07,
        2.3001e-07, 9.2204e-08, 2.1797e-07, 1.8000e-07, 9.3254e-08, 1.7180e-07,
        2.0569e-07, 3.2100e-07, 9.2204e-08, 1.2709e-07, 2.3927e-07, 3.1050e-07,
        5.1206e-08, 1.3538e-07, 1.1106e-07, 2.2230e-07, 2.6164e-07, 2.1069e-07,
        5.1385e-08, 2.5623e-07, 3.3279e-07, 9.2204e-08, 1.1138e-07, 9.3254e-08,
        1.3782e-07, 9.1742e-08, 4.5032e-07, 2.1069e-07, 9.5536e-08, 2.2230e-07,
        2.3778e-07, 1.4605e-07, 3.5376e-07, 1.4868e-07, 1.6252e-07, 7.1541e-08,
        3.1753e-08, 1.7488e-07, 4.3484e-07, 1.4908e-07, 1.5136e-07, 2.0639e-07,
        2.8707e-02, 3.5591e-07, 1.3695e-07, 3.0606e-07, 1.6941e-07, 5.6280e-08,
        1.6897e-07, 9.2204e-08, 1.0708e-07, 3.0760e-02, 4.3484e-07, 3.1240e-07,
        1.3538e-07, 1.5879e-07, 1.6054e-07, 2.1797e-07, 1.3498e-07, 3.8202e-07,
        6.3519e-08, 1.9676e-07, 4.1651e-02, 3.0983e-07, 2.8201e-07, 7.3589e-08,
        1.4868e-07, 1.8000e-07, 2.3728e-07, 2.6032e-07, 1.5997e-07, 6.6247e-08,
        1.9623e-07, 1.2934e-07, 1.1355e-07, 1.9580e-02, 7.3589e-08, 1.4605e-07,
        3.5376e-07, 1.1996e-07, 3.5376e-07, 9.3254e-08, 1.2715e-02, 4.5681e-07,
        2.1797e-07, 1.4605e-07, 1.4908e-07, 1.8096e-07, 2.3778e-07, 2.0569e-07,
        1.4186e-07, 3.0606e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0059e-06, 1.2986e-06, 5.6218e-07, 3.4244e-07, 5.6737e-07, 4.0835e-07,
        8.1224e-07, 4.1831e-07, 7.3938e-07, 2.3887e-07, 2.3530e-07, 1.0059e-06,
        3.8852e-07, 1.2415e-06, 9.1538e-07, 9.6833e-07, 6.0118e-07, 4.5892e-07,
        7.3938e-07, 1.0059e-06, 2.7581e-07, 9.2930e-07, 1.0615e-06, 6.6162e-07,
        2.5670e-07, 5.9281e-07, 3.6190e-07, 2.4423e-07, 3.1920e-07, 6.8434e-07,
        6.9609e-07, 7.3938e-07, 6.5279e-07, 9.5341e-07, 6.2728e-07, 7.5916e-07,
        9.7527e-07, 8.1824e-07, 9.4707e-07, 1.1961e-06, 8.1961e-07, 7.5008e-08,
        3.1241e-07, 4.8219e-07, 4.1291e-07, 7.3938e-07, 1.0101e-06, 4.7690e-07,
        5.3439e-07, 1.4175e-07, 3.5193e-07, 3.2778e-07, 8.6080e-07, 6.5894e-07,
        3.3448e-07, 3.8852e-07, 7.1485e-07, 3.2778e-07, 2.9022e-07, 1.0570e-06,
        7.6666e-07, 9.2395e-07, 5.0943e-07, 4.2052e-07, 8.6341e-07, 4.9179e-07,
        7.6196e-07, 6.6009e-07, 8.1060e-07, 5.6506e-07, 3.6190e-07, 4.2414e-07,
        1.0101e-06, 8.3728e-07, 3.7320e-07, 3.1241e-07, 3.8968e-07, 1.4633e-06,
        3.2614e-08, 8.9237e-07, 2.9022e-07, 4.2840e-07, 3.1241e-07, 6.2185e-07,
        3.7320e-07, 7.7816e-07, 1.0749e-06, 3.8852e-07, 4.2682e-07, 4.1291e-07,
        1.0059e-06, 3.8852e-07, 2.0785e-07, 8.5193e-07, 9.1736e-07, 6.4036e-07,
        1.0464e-06, 2.1241e-07, 5.1592e-07, 3.1241e-07, 2.3530e-07, 4.5892e-07,
        4.9179e-07, 8.7001e-07, 5.0187e-07, 2.5721e-07, 1.4581e-06, 6.1730e-07,
        7.7284e-07, 4.9179e-07, 8.7312e-07, 5.8411e-07, 7.6719e-07, 7.1555e-07,
        2.7722e-07, 4.5892e-07, 1.3366e-06, 4.0996e-07, 5.3996e-07, 1.1043e-06,
        6.9697e-07, 1.0387e-06, 3.1241e-07, 3.6959e-07, 3.1786e-07, 9.5341e-07,
        4.6209e-07, 2.1262e-07, 5.3996e-07, 1.0153e-06, 1.0727e-06, 8.1224e-07,
        9.7234e-07, 3.8623e-07, 3.9665e-07, 8.1961e-09, 8.2877e-07, 3.9352e-07,
        5.7196e-07, 3.9345e-07, 2.6502e-07, 6.7102e-07, 3.6190e-07, 4.5385e-07,
        7.1323e-07, 4.2414e-07, 3.8623e-07, 4.7461e-07, 3.7577e-07, 5.6506e-07,
        8.6341e-07, 5.2697e-07, 1.3815e-06, 5.3655e-07, 3.9029e-07, 3.7320e-07,
        3.0339e-07, 5.7032e-07, 5.7196e-07, 2.7295e-07, 1.3864e-06, 1.7561e-06,
        6.2891e-02, 9.1582e-07, 5.3017e-07, 8.4468e-07, 2.8994e-07, 3.2657e-07,
        7.6877e-07, 1.9835e-06, 6.7393e-07, 1.8246e-06, 1.0059e-06, 3.7079e-07,
        4.9179e-07, 6.7671e-07, 9.8875e-07, 1.7561e-06, 3.1241e-07, 1.2400e-06,
        8.4081e-07, 3.6190e-07, 5.0187e-07, 3.0347e-07, 3.9029e-07, 1.4184e-06,
        3.8852e-07, 9.3300e-07, 9.2376e-07, 2.3896e-02, 5.1703e-07, 1.0953e-01,
        5.7195e-07, 1.4486e-06, 5.8046e-07, 4.9179e-07, 5.7233e-07, 6.7977e-07,
        2.9022e-07, 3.1241e-07, 5.1446e-07, 4.9550e-07, 5.2951e-07, 3.2241e-07,
        1.7964e-08, 6.5279e-07, 9.8875e-07, 2.1875e-07, 2.6502e-07, 3.8852e-07,
        1.0282e-06, 4.6362e-07, 1.0727e-06, 5.7471e-07, 8.2877e-07, 9.1538e-07,
        2.1262e-07, 1.0793e-06, 1.1961e-06, 5.3712e-07, 1.1961e-06, 1.4581e-06,
        6.0938e-07, 3.9345e-07, 7.0369e-07, 4.1640e-07, 6.2728e-07, 1.0664e-06,
        9.4048e-07, 1.0282e-06, 3.4754e-07, 2.1920e-06, 6.2286e-07, 1.0247e-06,
        8.5193e-07, 7.8524e-07, 7.4386e-07, 1.7713e-07, 8.2877e-07, 3.6959e-07,
        8.8505e-07, 8.4366e-02, 3.8852e-07, 1.5409e-06, 7.4386e-07, 1.0059e-06,
        8.0000e-07, 3.2376e-07, 4.9312e-07, 6.0259e-07, 3.6190e-07, 8.8021e-07,
        9.2395e-07, 3.6190e-07, 7.7633e-07, 7.3938e-07, 5.0597e-07, 2.3530e-07,
        1.1014e-06, 6.0973e-07, 1.4633e-06, 4.8624e-07, 2.4423e-07, 8.7312e-07,
        6.3573e-07, 9.8875e-07, 8.9072e-07, 3.8852e-07, 7.1323e-07, 3.0347e-07,
        8.5734e-07, 7.3938e-07, 1.0664e-06, 4.5892e-07, 4.5494e-07, 7.1555e-07,
        1.6103e-07, 3.4244e-07, 4.0268e-07, 2.1875e-07, 5.3086e-07, 5.9665e-07,
        4.8219e-07, 2.5721e-07, 6.3583e-07, 1.8246e-06, 3.4754e-07, 5.3439e-07,
        7.3938e-07, 3.0339e-07, 1.5409e-06, 5.1592e-07, 1.3864e-06, 1.1482e-06,
        2.8217e-07, 6.0973e-07, 2.4177e-07, 2.4423e-07, 5.9626e-07, 2.6986e-07,
        5.9951e-07, 1.6946e-07, 1.1961e-06, 1.1979e-06, 5.0274e-07, 6.1288e-07,
        1.0546e-06, 3.8852e-07, 5.1439e-07, 6.1822e-07, 3.5516e-07, 2.1451e-06,
        1.1961e-06, 8.6341e-07, 4.8795e-07, 3.8426e-07, 2.3921e-07, 2.9954e-07,
        9.4214e-02, 7.7226e-07, 8.2154e-09, 4.6584e-07, 1.1235e-06, 5.6737e-07,
        4.7008e-07, 8.5734e-07, 2.0785e-07, 4.6209e-07, 8.2877e-07, 7.3938e-07,
        8.6341e-07, 8.1224e-07, 4.6209e-07, 4.6981e-07, 4.4968e-07, 6.3583e-07,
        2.7722e-07, 4.2411e-07, 5.3439e-07, 5.7233e-07, 1.0507e-06, 9.6425e-07,
        4.3185e-07, 1.4116e-07, 1.2804e-06, 2.9125e-07, 1.5271e-07, 9.8391e-02,
        8.6728e-07, 1.1961e-06, 9.4048e-07, 3.9352e-07, 6.1822e-07, 5.7032e-07,
        8.4672e-07, 3.8623e-07, 1.9140e-07, 7.5535e-07, 6.8174e-02, 1.1979e-06,
        1.9972e-07, 1.0282e-06, 6.4878e-07, 8.3604e-07, 3.7320e-07, 3.7320e-07,
        3.6190e-07, 1.3864e-06, 9.1538e-07, 3.8426e-07, 9.0736e-07, 4.8795e-07,
        1.0361e-06, 4.7992e-07, 9.8875e-07, 4.8795e-07, 7.3938e-07, 1.1912e-06,
        6.6493e-07, 1.2195e-06, 3.9665e-07, 7.8038e-07, 3.2778e-07, 1.1482e-06,
        1.1961e-06, 1.6889e-07, 1.9972e-07, 6.4510e-07, 1.1961e-06, 1.0059e-06,
        3.2614e-08, 3.8852e-07, 2.0657e-07, 3.3448e-07, 2.0657e-07, 1.2235e-06,
        1.1979e-06, 5.6816e-07, 6.1822e-07, 3.6116e-07, 5.1592e-07, 9.6215e-07,
        6.8699e-07, 3.8128e-07, 1.4633e-06, 9.5674e-07, 9.6508e-07, 3.8302e-07,
        1.4633e-06, 8.5734e-07, 5.0187e-07, 1.0247e-06, 6.2328e-07, 6.0348e-02,
        1.0700e-06, 4.6981e-07, 5.3655e-07, 4.5385e-07, 6.8841e-07, 9.9055e-07,
        5.7050e-07, 9.8875e-07, 7.4386e-07, 6.0938e-07, 1.0001e-06, 3.7263e-07,
        4.2682e-07, 1.1843e-07, 3.8426e-07, 2.7295e-07, 7.8212e-07, 4.5892e-07,
        2.5721e-07, 1.0119e-06, 3.9029e-07, 2.5721e-07, 1.3815e-06, 7.5445e-07,
        2.5790e-07, 3.6190e-07, 2.5721e-07, 8.3529e-07, 1.8597e-06, 8.3604e-07,
        5.7196e-07, 3.2104e-07, 9.1538e-07, 8.5734e-07, 9.3722e-07, 1.2656e-06,
        1.0303e-06, 9.9194e-02, 9.9879e-08, 2.4423e-07, 9.9211e-08, 2.9865e-07,
        7.8938e-07, 1.0282e-06, 5.3086e-07, 5.8046e-07, 7.9399e-07, 8.2688e-07,
        4.5385e-07, 6.5279e-07, 7.0108e-07, 4.7731e-07, 1.3864e-06, 1.7086e-06,
        6.4036e-07, 4.5385e-07, 1.8948e-06, 1.5396e-06, 4.9179e-07, 3.8852e-07,
        7.6196e-07, 2.6502e-07, 1.4624e-06, 7.6877e-07, 2.9279e-07, 1.1961e-06,
        5.7785e-07, 3.2339e-07, 9.6508e-07, 1.0282e-06, 2.4005e-07, 1.3957e-06,
        1.6848e-06, 3.4754e-07, 2.4177e-07, 1.0282e-06, 2.3530e-07, 1.1118e-06,
        1.1244e-07, 6.8200e-07, 8.5271e-07, 9.7695e-07, 1.0059e-06, 2.0657e-07,
        1.7561e-06, 6.0973e-07, 1.3460e-06, 2.9781e-07, 5.3086e-07, 6.8841e-07,
        3.9352e-07, 2.1451e-06, 1.0282e-06, 4.7499e-07, 5.6737e-07, 5.9383e-07,
        5.7195e-07, 9.5341e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.6128e-06, 6.8980e-07, 7.8541e-07, 1.0914e-06, 1.5558e-06, 5.2555e-07,
        9.8207e-07, 1.0935e-07, 1.1243e-06, 5.5947e-07, 7.9905e-07, 7.2432e-07,
        2.2719e-07, 4.6708e-07, 6.4810e-07, 7.7952e-07, 3.8283e-07, 1.0928e-06,
        4.0482e-07, 4.8010e-07, 1.0175e-06, 1.0851e-06, 4.4459e-07, 1.0251e-06,
        9.3786e-07, 4.8252e-07, 8.0149e-07, 8.6652e-07, 5.8469e-07, 9.4108e-07,
        5.8304e-07, 6.7748e-07, 6.4285e-07, 5.7214e-07, 3.6229e-07, 4.2271e-07,
        4.9963e-07, 6.6668e-07, 1.0044e-06, 1.2403e-06, 5.4257e-07, 1.0289e-06,
        6.7607e-07, 4.1318e-07, 7.4779e-07, 9.5526e-07, 5.3894e-07, 7.6272e-07,
        9.9544e-07, 3.2585e-07, 9.8207e-07, 2.1750e-06, 2.5881e-07, 7.3188e-07,
        4.8814e-07, 9.8864e-07, 2.9734e-07, 4.5111e-07, 2.3683e-07, 2.4378e-07,
        1.7143e-06, 1.0368e-06, 2.5394e-07, 1.5031e-01, 2.2241e-07, 1.0251e-06,
        1.2278e-06, 7.4032e-07, 5.3561e-07, 1.0767e-06, 1.0740e-06, 9.8207e-07,
        9.4644e-07, 5.2248e-07, 1.2303e-06, 9.6675e-07, 6.4731e-07, 6.6367e-07,
        3.2746e-07, 1.0162e-06, 1.0265e-06, 7.4506e-07, 1.0815e-06, 8.6892e-07,
        3.2808e-07, 5.3878e-07, 8.8258e-07, 3.4857e-07, 1.0935e-07, 4.2375e-07,
        1.2633e-06, 8.7228e-07, 7.4779e-07, 8.6653e-07, 1.3594e-06, 1.5558e-06,
        3.1510e-07, 5.9972e-07, 5.4017e-07, 6.7748e-07, 5.6731e-07, 9.9544e-07,
        1.3039e-06, 4.8362e-07, 4.6708e-07, 1.0597e-06, 5.3894e-07, 4.7840e-07,
        1.0815e-06, 4.2968e-07, 2.0786e-06, 1.9892e-07, 8.2303e-07, 1.6368e-06,
        6.6232e-07, 5.3894e-07, 3.8283e-07, 1.3677e-01, 1.6846e-06, 1.9843e-06,
        4.2968e-07, 7.1572e-07, 5.7868e-07, 8.2111e-07, 8.7228e-07, 1.0757e-07,
        1.3039e-06, 4.5111e-07, 5.5037e-07, 6.3675e-07, 5.1282e-07, 5.3434e-07,
        3.1034e-07, 6.7607e-07, 6.5690e-07, 3.8283e-07, 6.0819e-07, 1.3046e-06,
        8.6653e-07, 7.6077e-07, 7.5463e-07, 1.3594e-06, 1.9175e-06, 4.5902e-07,
        5.5592e-07, 9.3955e-07, 7.9361e-07, 6.3383e-07, 1.1251e-06, 6.8325e-07,
        1.9844e-06, 2.5084e-07, 5.0665e-07, 8.6892e-07, 4.9963e-07, 6.3704e-07,
        1.3785e-06, 1.1182e-06, 1.1743e-06, 1.7841e-06, 4.9531e-07, 1.1269e-06,
        2.7878e-07, 3.0324e-07, 5.7411e-07, 1.3342e-06, 4.8362e-07, 5.8327e-07,
        5.3433e-07, 1.5556e-06, 7.3549e-07, 1.1176e-06, 4.2089e-07, 5.5655e-07,
        1.2303e-06, 1.4964e-06, 1.4964e-06, 4.1094e-07, 6.3383e-07, 5.0501e-07,
        3.7580e-07, 3.0948e-07, 1.1176e-06, 6.9422e-07, 2.5271e-07, 5.9974e-07,
        4.4249e-07, 1.0851e-06, 9.6675e-07, 5.9974e-07, 4.2968e-07, 9.7335e-07,
        7.1326e-07, 9.8207e-07, 6.7349e-07, 1.1570e-01, 2.9294e-07, 7.4681e-07,
        6.4810e-07, 6.7135e-07, 6.6072e-07, 9.6023e-07, 1.0078e-06, 4.8814e-07,
        9.0568e-08, 8.2299e-07, 1.4964e-06, 7.3549e-07, 5.6439e-07, 6.4810e-07,
        1.6889e-01, 1.1338e-06, 5.8469e-07, 5.3894e-07, 1.1176e-06, 6.3646e-07,
        2.9465e-07, 1.4964e-06, 8.7387e-07, 1.9843e-06, 1.0862e-06, 5.3434e-07,
        4.3501e-07, 3.0948e-07, 2.6987e-07, 4.8252e-07, 1.4490e-01, 1.5556e-06,
        1.0851e-06, 1.3031e-06, 2.5039e-07, 4.2545e-07, 1.0935e-07, 5.7077e-07,
        5.2555e-07, 2.4378e-07, 4.9963e-07, 6.8980e-07, 4.4459e-07, 2.5524e-07,
        6.6516e-07, 9.7501e-07, 3.1510e-07, 9.6675e-07, 7.3188e-07, 1.1176e-06,
        7.6197e-07, 2.3283e-07, 4.6708e-07, 8.0440e-07, 6.2638e-07, 1.0592e-06,
        7.1332e-07, 8.9806e-07, 2.5103e-07, 1.4023e-06, 6.6681e-07, 4.2143e-07,
        9.7501e-07, 8.0500e-07, 5.4935e-07, 6.7607e-07, 7.4779e-07, 1.9175e-06,
        1.0928e-06, 6.7608e-07, 1.1477e-06, 5.5592e-07, 1.7143e-06, 2.8435e-07,
        1.5558e-06, 5.9974e-07, 8.6239e-07, 4.9765e-08, 9.8520e-02, 1.0141e-06,
        4.0482e-07, 1.3362e-06, 6.5690e-07, 7.1680e-07, 6.3383e-07, 6.9005e-07,
        1.2633e-06, 7.4666e-07, 7.0117e-07, 9.1391e-07, 1.0851e-06, 4.9963e-07,
        9.3786e-07, 4.8252e-07, 1.0786e-06, 2.9228e-07, 6.0819e-07, 1.8950e-07,
        4.3967e-07, 5.2248e-07, 1.3605e-06, 7.1680e-07, 1.1743e-06, 1.3247e-06,
        3.5345e-07, 4.8814e-07, 8.6652e-07, 6.6516e-07, 5.0665e-07, 5.3878e-07,
        9.3698e-02, 9.8207e-07, 9.7123e-08, 1.3039e-06, 8.8258e-07, 5.5947e-07,
        2.1706e-07, 4.3499e-07, 1.4201e-06, 3.2585e-07, 8.0149e-07, 4.8831e-07,
        9.7532e-07, 7.2548e-07, 5.7302e-07, 5.4672e-07, 8.0440e-07, 4.9755e-07,
        4.4142e-07, 8.1511e-07, 2.7878e-07, 2.2719e-07, 1.1743e-06, 2.8092e-07,
        4.8500e-07, 1.0815e-06, 1.3087e-06, 1.2278e-06, 5.0501e-07, 3.7397e-07,
        4.8362e-07, 4.2968e-07, 5.0665e-07, 1.1210e-06, 2.2719e-07, 1.5137e-01,
        4.1854e-07, 7.6114e-07, 4.4506e-07, 4.1350e-07, 1.0094e-06, 4.6708e-07,
        7.9263e-07, 6.3383e-07, 3.5638e-07, 6.4285e-07, 4.2089e-07, 8.6653e-07,
        5.5947e-07, 8.8258e-07, 9.2854e-07, 1.4964e-06, 3.0037e-07, 4.8831e-07,
        1.7691e-07, 9.9056e-07, 6.9387e-07, 7.1639e-07, 9.4644e-07, 6.8434e-07,
        9.9206e-07, 1.0162e-06, 6.7748e-07, 7.6695e-07, 6.8980e-07, 6.8147e-07,
        9.4644e-07, 5.0685e-07, 4.8647e-08, 4.2968e-07, 8.7228e-07, 8.1123e-07,
        7.1326e-07, 7.8614e-07, 3.7397e-07, 2.7878e-07, 1.1484e-06, 3.3699e-07,
        6.4285e-07, 7.0164e-07, 7.1605e-07, 9.1476e-07, 6.2779e-07, 4.8814e-07,
        2.2241e-07, 7.7056e-07, 7.6114e-07, 4.2089e-07, 1.7352e-06, 2.1845e-07,
        1.5146e-06, 1.0251e-06, 5.6584e-07, 9.6277e-07, 1.0935e-07, 6.7607e-07,
        5.3878e-07, 5.4935e-07, 1.9843e-06, 2.4875e-07, 6.1714e-08, 7.3027e-07,
        3.7397e-07, 1.0928e-06, 1.2124e-01, 2.1535e-06, 1.2888e-01, 6.7607e-07,
        5.0665e-07, 9.6675e-07, 3.9247e-07, 6.0702e-07, 5.2248e-07, 7.5359e-07,
        4.2968e-07, 4.3967e-07, 6.4731e-07, 5.5592e-07, 6.6001e-07, 1.5558e-06,
        1.5420e-06, 1.1757e-06, 5.5592e-07, 1.2737e-06, 9.4034e-07, 5.9740e-07,
        4.1350e-07, 4.8362e-07, 1.0044e-06, 2.7235e-07, 5.3894e-07, 2.7878e-07,
        3.0948e-07, 4.6756e-07, 9.7335e-07, 4.3096e-07, 2.7878e-07, 3.8454e-07,
        3.9945e-07, 2.2719e-07, 1.1176e-06, 8.3790e-07, 6.2068e-07, 1.0355e-06,
        3.2585e-07, 1.0208e-06, 3.3837e-07, 8.6661e-07, 2.7219e-07, 2.7555e-07,
        8.3821e-07, 9.0124e-07, 7.1316e-07, 1.2023e-07, 3.7809e-07, 5.5592e-07,
        1.0757e-07, 5.5592e-07, 1.1041e-06, 6.7603e-07, 9.4108e-07, 7.3549e-07,
        1.0949e-06, 1.0545e-06, 1.1067e-06, 1.2206e-06, 1.0851e-06, 1.0935e-07,
        1.3342e-06, 1.0044e-06, 3.4423e-07, 5.3021e-07, 1.0251e-06, 4.0482e-07,
        9.5161e-07, 9.3389e-07, 7.5343e-07, 9.1476e-07, 3.8454e-07, 6.6367e-07,
        5.3894e-07, 5.2248e-07, 4.1350e-07, 1.3605e-06, 1.2393e-06, 7.1332e-07,
        2.2401e-07, 6.3646e-07, 8.8818e-07, 1.0251e-06, 1.1743e-06, 9.8207e-07,
        2.3008e-07, 1.6907e-07, 1.9076e-06, 2.9734e-07, 8.2299e-07, 9.1150e-07,
        1.3342e-06, 4.0715e-07, 5.9972e-07, 4.0482e-07, 7.1732e-07, 4.4719e-07,
        4.6935e-07, 6.0819e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.3217e-07, 1.3632e-06, 7.1818e-07, 2.9702e-07, 1.1136e-06, 1.2685e-06,
        8.7933e-07, 6.0902e-07, 1.1829e-06, 1.1829e-06, 2.0024e-06, 7.4234e-07,
        1.0662e-06, 3.1390e-07, 9.0570e-07, 1.1780e-06, 1.9086e-06, 9.0570e-07,
        8.9815e-07, 6.6715e-07, 1.1829e-06, 6.0902e-07, 1.6730e-06, 1.5546e-06,
        9.3045e-07, 1.8864e-06, 1.2409e-06, 1.4334e-06, 1.0291e-06, 3.9667e-07,
        8.3932e-07, 8.3659e-07, 6.1840e-07, 1.8229e-06, 6.1808e-07, 3.4682e-07,
        5.6805e-07, 1.3571e-06, 1.3075e-06, 1.9086e-06, 1.4982e-07, 1.1264e-06,
        1.2685e-06, 1.8162e-06, 1.5432e-06, 1.0718e-06, 1.9783e-06, 8.5424e-07,
        5.8871e-07, 8.2874e-07, 6.9688e-07, 6.5393e-07, 8.1943e-07, 7.6697e-07,
        1.9066e-06, 3.1390e-07, 9.8828e-07, 1.5179e-06, 6.9352e-07, 4.0328e-06,
        2.6089e-07, 6.8630e-08, 5.1101e-07, 1.3940e-06, 1.3688e-06, 1.1141e-06,
        1.6561e-06, 5.9080e-07, 1.9095e-06, 1.0824e-06, 1.0171e-06, 1.1541e-06,
        4.0924e-07, 7.2844e-07, 4.1169e-07, 1.4935e-06, 6.2734e-07, 8.5120e-07,
        1.2809e-06, 8.2965e-07, 9.4712e-07, 9.4155e-07, 9.1610e-07, 3.0585e-07,
        6.8976e-07, 6.8976e-07, 3.5889e-07, 2.2102e-06, 1.2152e-07, 1.2157e-06,
        1.0171e-06, 1.9453e-01, 4.3054e-07, 2.1808e-06, 2.0534e-06, 8.7074e-07,
        4.4408e-07, 9.6609e-07, 9.6575e-07, 3.5889e-07, 5.8464e-07, 9.9118e-07,
        1.9881e-06, 4.8117e-07, 5.1410e-07, 3.2724e-07, 8.5891e-07, 1.4003e-06,
        1.6304e-06, 9.2981e-07, 4.2441e-07, 6.1808e-07, 1.3498e-06, 6.8052e-07,
        1.2685e-06, 1.0662e-06, 8.9289e-07, 1.2969e-06, 1.0718e-06, 7.9140e-07,
        8.6626e-07, 1.7020e-06, 1.2409e-06, 5.1601e-07, 1.1277e-06, 1.6934e-06,
        1.8864e-06, 3.0995e-07, 2.6029e-07, 6.4051e-07, 4.2539e-07, 1.3692e-06,
        4.3701e-07, 8.9052e-07, 7.8877e-07, 9.1610e-07, 1.9677e-01, 1.0343e-06,
        1.0119e-06, 1.0662e-06, 3.6871e-07, 6.6200e-07, 1.3688e-06, 1.4337e-06,
        6.7665e-07, 6.5951e-07, 4.9654e-07, 1.6396e-06, 6.3300e-07, 5.7033e-07,
        4.4408e-07, 1.0811e-06, 7.9140e-07, 1.0973e-06, 1.2528e-06, 9.2981e-07,
        8.2507e-07, 1.1829e-06, 1.0824e-06, 4.0108e-07, 5.6550e-07, 8.6632e-07,
        1.4101e-06, 9.8988e-07, 2.4305e-06, 5.9942e-07, 1.0868e-06, 2.1681e-06,
        3.5889e-07, 7.2374e-07, 9.8869e-07, 9.6609e-07, 1.3751e-01, 6.5882e-07,
        1.3228e-06, 1.1829e-06, 5.8871e-07, 7.7510e-07, 1.4227e-06, 4.9685e-07,
        1.2647e-06, 1.3479e-06, 9.9538e-07, 1.5432e-06, 9.3246e-07, 1.3479e-06,
        1.6730e-06, 1.5899e-06, 1.7152e-06, 1.0341e-06, 1.1829e-06, 1.2632e-06,
        1.2108e-06, 1.1766e-06, 3.5889e-07, 1.7020e-06, 1.0662e-06, 2.2113e-07,
        8.1943e-07, 7.6865e-07, 9.2981e-07, 5.1410e-07, 1.2809e-06, 1.0944e-06,
        2.0024e-06, 2.9593e-07, 2.0024e-06, 5.1410e-07, 9.8479e-07, 4.8734e-07,
        1.1277e-06, 2.2788e-01, 8.5120e-07, 6.2234e-08, 3.7133e-07, 4.2477e-08,
        1.2732e-06, 7.6983e-07, 1.1829e-06, 5.5126e-07, 1.0662e-06, 9.6623e-07,
        1.3158e-06, 1.7717e-06, 3.4800e-07, 4.1495e-07, 6.9688e-07, 9.1523e-07,
        6.5393e-07, 1.0872e-06, 8.7933e-07, 2.4819e-07, 1.4334e-06, 1.0316e-06,
        5.1410e-07, 1.1528e-06, 1.1829e-06, 3.1390e-07, 1.1976e-06, 1.4334e-06,
        1.7703e-06, 1.1277e-06, 1.1433e-06, 9.8479e-07, 1.8064e-06, 2.9029e-07,
        4.8734e-07, 8.7767e-07, 2.6650e-07, 2.6650e-07, 4.6681e-07, 1.3828e-06,
        1.3479e-06, 1.3228e-06, 5.3017e-07, 6.3229e-07, 1.8145e-06, 1.2632e-06,
        5.5171e-07, 1.8324e-06, 7.1399e-07, 5.1581e-07, 3.8102e-07, 8.3747e-07,
        8.6926e-07, 3.9102e-07, 4.0165e-07, 7.3941e-07, 8.6916e-07, 1.5582e-06,
        4.6958e-07, 9.1235e-07, 9.2981e-07, 8.7074e-07, 6.9688e-07, 4.9654e-07,
        1.0470e-06, 3.8102e-07, 6.3229e-07, 9.4084e-07, 1.0720e-06, 1.2528e-06,
        1.2646e-06, 1.5432e-06, 3.1125e-07, 1.1764e-06, 8.4238e-07, 1.0957e-06,
        1.2452e-06, 2.0024e-06, 1.0006e-06, 5.0582e-07, 1.3310e-06, 1.0494e-07,
        1.6770e-06, 1.6730e-06, 7.5221e-07, 1.1175e-06, 6.1058e-07, 3.5633e-07,
        1.1168e-06, 3.1218e-07, 1.6726e-06, 9.4704e-07, 5.8464e-07, 7.3307e-07,
        1.6166e-06, 4.8427e-07, 1.0992e-07, 1.1437e-06, 3.7132e-07, 1.6794e-06,
        4.4959e-07, 9.4847e-07, 6.5393e-07, 7.3303e-07, 2.6096e-07, 1.0548e-06,
        5.0582e-07, 9.9824e-07, 3.0656e-07, 5.3301e-07, 7.0889e-07, 1.1433e-06,
        6.3980e-07, 9.6609e-07, 5.0229e-07, 9.4160e-07, 2.0908e-06, 8.9815e-07,
        5.8871e-07, 6.9997e-07, 1.4334e-06, 6.1058e-07, 1.4337e-06, 1.9225e-06,
        1.8864e-06, 1.6354e-06, 5.1410e-07, 1.2505e-06, 1.1976e-06, 8.5282e-07,
        2.4874e-07, 6.9603e-07, 7.8114e-07, 6.5393e-07, 3.5016e-07, 3.7590e-07,
        1.1061e-06, 2.0144e-06, 1.0720e-06, 1.2685e-06, 1.8982e-06, 4.7916e-07,
        1.0720e-06, 1.2532e-06, 5.9854e-08, 3.1390e-07, 8.9289e-07, 1.0343e-06,
        5.2021e-07, 9.9044e-07, 3.8102e-07, 8.5286e-07, 1.2452e-06, 8.5424e-07,
        3.9667e-07, 5.1601e-07, 1.5045e-06, 4.5621e-07, 1.6762e-06, 6.0903e-07,
        6.9688e-07, 5.0582e-07, 1.7020e-06, 6.9352e-07, 4.4844e-07, 1.0343e-06,
        4.1169e-07, 5.5126e-07, 2.1106e-06, 8.0825e-07, 5.0582e-07, 7.9140e-07,
        4.9908e-07, 6.6210e-07, 5.5126e-07, 9.7641e-07, 9.2981e-07, 9.2981e-07,
        9.6667e-07, 1.8162e-06, 1.4334e-06, 4.9079e-07, 1.8031e-06, 1.1056e-06,
        5.2225e-07, 1.3940e-06, 1.6913e-07, 2.4819e-07, 1.4285e-06, 3.4778e-07,
        6.1808e-07, 2.2685e-01, 1.7673e-01, 2.5510e-07, 1.0291e-06, 2.0534e-06,
        3.1870e-07, 1.7922e-06, 5.0582e-07, 1.1264e-06, 1.7152e-06, 1.9135e-06,
        8.3761e-07, 6.7411e-07, 1.3228e-06, 7.0889e-07, 9.6575e-07, 1.2677e-06,
        2.6909e-07, 1.2290e-06, 6.6210e-07, 1.4865e-06, 5.0582e-07, 1.0957e-06,
        4.9999e-07, 3.1146e-07, 3.5358e-07, 3.4849e-07, 9.4482e-07, 5.0251e-07,
        1.4972e-06, 9.3225e-08, 5.8025e-07, 8.2874e-07, 1.1433e-06, 1.0662e-06,
        2.7005e-07, 1.4699e-06, 5.0582e-07, 4.2441e-07, 6.1058e-07, 6.6796e-07,
        6.8052e-07, 1.0664e-06, 5.1583e-07, 3.7434e-07, 3.1390e-07, 3.5534e-07,
        5.8464e-07, 1.1829e-06, 6.9352e-07, 2.0024e-06, 6.9688e-07, 5.4742e-07,
        7.7411e-07, 8.0842e-07, 1.0720e-06, 1.1433e-06, 3.5633e-07, 1.0868e-06,
        1.0045e-06, 1.0720e-06, 1.2647e-06, 1.9969e-06, 6.6796e-07, 1.0537e-06,
        1.4611e-06, 1.6628e-06, 1.8763e-06, 4.5524e-07, 5.5093e-07, 5.5171e-07,
        5.0582e-07, 1.0720e-06, 8.2613e-07, 1.1264e-06, 7.9933e-07, 1.3692e-06,
        1.2050e-06, 1.3940e-06, 4.8734e-07, 3.1125e-07, 3.7590e-07, 1.9086e-06,
        1.5179e-06, 7.7945e-07, 1.3834e-06, 1.2137e-06, 2.4543e-06, 1.3782e-06,
        9.0570e-07, 6.3294e-08, 5.1581e-07, 4.9965e-07, 9.1235e-07, 1.3688e-06,
        7.7050e-07, 5.1410e-07, 4.3041e-07, 6.5117e-07, 1.8864e-06, 5.9695e-07,
        1.0338e-06, 3.9667e-07, 7.8007e-07, 8.7073e-07, 3.6142e-07, 1.5179e-06,
        3.6576e-07, 1.7262e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.7707e-06, 1.8743e-06, 2.2740e-06, 3.1261e-07, 9.3792e-07, 2.2903e-06,
        1.5182e-06, 6.6744e-07, 6.1300e-07, 1.4486e-06, 7.0324e-07, 5.9102e-07,
        1.7183e-06, 2.1056e-06, 5.5485e-07, 1.8410e-06, 6.1487e-07, 3.5300e-06,
        1.0961e-06, 1.9455e-06, 1.0815e-06, 1.2361e-06, 2.2499e-01, 1.4815e-06,
        1.1025e-06, 6.1222e-07, 1.7965e-06, 1.3601e-06, 4.5283e-07, 5.7486e-07,
        1.4899e-06, 1.1661e-06, 1.2341e-06, 1.1394e-06, 9.5275e-07, 5.2313e-07,
        1.1634e-06, 1.0978e-06, 1.4068e-06, 9.4524e-07, 2.2489e-01, 3.5443e-06,
        2.6610e-07, 5.7486e-07, 1.5362e-06, 1.9476e-06, 3.8896e-06, 7.9057e-07,
        1.2021e-06, 8.8190e-07, 2.2664e-06, 2.3685e-06, 9.8936e-07, 3.1740e-06,
        2.1834e-06, 8.1272e-07, 1.2531e-06, 8.8253e-07, 4.8976e-07, 1.5336e-06,
        4.5407e-06, 7.5984e-07, 1.2642e-06, 1.5182e-06, 7.4173e-07, 5.1719e-06,
        8.1106e-07, 1.0938e-06, 4.6817e-07, 8.1473e-07, 1.2531e-06, 4.2509e-07,
        1.2341e-06, 1.4917e-06, 3.1324e-06, 9.4967e-07, 1.3730e-06, 3.1845e-07,
        1.7281e-06, 1.8400e-06, 1.5666e-06, 8.6378e-07, 7.6244e-07, 8.2609e-07,
        1.1296e-06, 8.6245e-07, 1.9895e-07, 2.4812e-06, 6.7608e-07, 1.2518e-06,
        6.3376e-07, 6.0785e-07, 5.2063e-07, 1.3418e-06, 1.4543e-06, 2.5538e-06,
        2.2470e-06, 2.3281e-06, 1.1940e-06, 1.2366e-06, 1.0166e-06, 1.6978e-06,
        1.1371e-06, 1.6577e-06, 1.5388e-06, 1.2021e-06, 6.9432e-07, 8.4796e-07,
        1.8708e-06, 2.0496e-06, 2.1476e-06, 9.5519e-07, 1.1940e-06, 1.2335e-06,
        2.1854e-06, 1.0019e-06, 3.4463e-07, 2.2492e-06, 1.6710e-06, 2.3292e-06,
        1.4158e-06, 1.7062e-06, 6.4573e-07, 6.0120e-07, 9.7709e-07, 9.1124e-07,
        1.2531e-06, 7.6007e-07, 4.7585e-07, 6.3196e-07, 8.2108e-07, 1.8337e-06,
        3.0618e-06, 1.0497e-06, 1.5903e-06, 1.8708e-06, 2.2689e-06, 3.6366e-07,
        1.4597e-06, 1.5328e-06, 1.6937e-01, 2.2689e-06, 5.5696e-07, 1.1394e-06,
        1.2014e-06, 9.1908e-02, 1.4815e-06, 8.1856e-07, 1.0166e-06, 4.8314e-07,
        2.6736e-06, 6.3196e-07, 8.6245e-07, 6.3196e-07, 1.8152e-06, 1.5301e-06,
        1.1119e-06, 1.6855e-06, 2.0305e-06, 1.1766e-06, 1.5182e-06, 1.4115e-01,
        1.4185e-06, 4.1837e-07, 1.1178e-06, 7.0323e-07, 1.3502e-01, 1.5259e-06,
        2.9389e-07, 5.9879e-07, 1.9093e-06, 9.8800e-07, 1.4547e-06, 8.2608e-07,
        1.6656e-06, 1.7284e-07, 1.1574e-06, 1.8245e-06, 2.0268e-06, 8.1106e-07,
        2.1831e-06, 1.5336e-06, 1.0350e-06, 9.4728e-07, 3.7300e-06, 1.2943e-06,
        1.5388e-06, 1.8245e-06, 2.0117e-06, 2.7936e-06, 1.8400e-06, 2.6381e-06,
        1.3015e-06, 1.9368e-06, 1.3535e-06, 1.1905e-06, 5.2853e-07, 9.7970e-07,
        2.1203e-06, 7.2567e-07, 2.6381e-06, 1.2624e-06, 3.5443e-06, 2.0386e-06,
        3.9436e-06, 1.4393e-06, 8.8395e-07, 1.1827e-06, 8.1880e-07, 9.7308e-07,
        2.4190e-06, 1.1413e-06, 1.4393e-06, 5.1393e-07, 3.6415e-06, 2.4795e-06,
        1.3153e-06, 1.6153e-06, 1.8005e-06, 8.8174e-07, 1.1531e-06, 1.2017e-06,
        9.2654e-07, 7.9057e-07, 1.6580e-06, 2.2664e-06, 1.4257e-06, 1.5189e-06,
        2.5290e-06, 1.7018e-06, 3.0034e-06, 1.3232e-06, 5.7027e-07, 7.6950e-07,
        1.4101e-06, 1.1371e-06, 1.3374e-06, 9.0667e-07, 8.8175e-07, 1.3572e-06,
        1.7281e-06, 6.0120e-07, 2.1476e-06, 7.2988e-07, 1.0962e-06, 1.3561e-06,
        1.1536e-06, 2.0947e-06, 1.2624e-06, 1.8731e-06, 1.0800e-06, 6.5895e-07,
        1.0175e-06, 1.2809e-06, 8.5359e-07, 3.5075e-07, 7.2988e-07, 4.8315e-07,
        2.1476e-06, 8.5359e-07, 1.2981e-06, 1.0312e-06, 1.8409e-06, 1.2972e-06,
        8.2086e-07, 2.2293e-06, 1.3049e-06, 1.0615e-06, 3.6789e-06, 4.2442e-06,
        1.9454e-06, 1.4098e-06, 7.0258e-07, 2.3301e-06, 1.4057e-06, 4.6860e-07,
        5.5548e-07, 1.4367e-06, 1.2366e-06, 5.1504e-07, 1.4676e-06, 1.6177e-06,
        1.9584e-06, 7.1016e-07, 1.2664e-06, 1.5405e-06, 5.8280e-07, 9.7308e-07,
        2.5760e-06, 7.6950e-07, 1.2433e-06, 2.2256e-06, 1.1600e-06, 4.2175e-07,
        2.0837e-06, 2.2293e-06, 1.2518e-06, 1.5778e-06, 9.7821e-07, 1.6656e-06,
        1.8344e-06, 9.8638e-07, 5.3668e-07, 1.4469e-06, 1.5280e-06, 2.1476e-06,
        1.0312e-06, 1.0595e-06, 1.4638e-06, 1.8337e-06, 1.0446e-06, 2.1407e-06,
        3.0358e-07, 1.3946e-06, 8.7305e-07, 1.4461e-07, 2.7831e-06, 1.3946e-06,
        1.5280e-06, 1.3317e-06, 9.5115e-07, 1.1230e-06, 2.9012e-06, 1.8207e-06,
        5.0085e-07, 4.4091e-07, 7.7317e-07, 1.1556e-06, 9.8585e-07, 9.8447e-07,
        8.1106e-07, 2.9527e-07, 1.6977e-06, 1.9455e-06, 1.1958e-06, 1.8245e-06,
        2.2689e-06, 4.9872e-07, 1.0175e-06, 9.8800e-07, 1.5405e-06, 1.3946e-06,
        1.1036e-06, 7.3317e-07, 2.0929e-06, 5.9102e-07, 8.1379e-07, 2.1186e-06,
        4.6860e-07, 6.3196e-07, 1.3946e-06, 1.9760e-06, 1.3946e-06, 5.3609e-07,
        1.6012e-06, 1.1556e-06, 7.0323e-07, 1.2965e-06, 5.8280e-07, 1.1906e-06,
        1.0756e-06, 1.0756e-06, 7.5984e-07, 8.2424e-07, 1.0446e-06, 7.6950e-07,
        6.6235e-07, 7.0257e-07, 3.0034e-06, 8.7305e-07, 2.1422e-06, 1.2807e-06,
        1.5538e-06, 1.1492e-06, 1.0438e-06, 2.2470e-06, 2.7831e-06, 2.9218e-06,
        1.5405e-06, 2.8141e-07, 1.5541e-06, 5.5485e-07, 1.5538e-06, 1.3730e-06,
        4.2174e-07, 1.3946e-06, 1.8496e-06, 1.6071e-06, 1.5666e-06, 1.2951e-06,
        6.3166e-07, 6.4108e-08, 1.5405e-06, 6.3196e-07, 1.1436e-06, 1.5255e-06,
        1.3257e-01, 2.1993e-06, 7.1236e-07, 7.6553e-07, 1.8409e-06, 7.6010e-07,
        1.8470e-06, 7.5984e-07, 2.0436e-06, 2.1476e-06, 2.9012e-06, 1.6549e-01,
        5.3609e-07, 1.6124e-06, 1.4834e-06, 5.2313e-07, 7.5984e-07, 1.1668e-06,
        2.0590e-06, 2.0284e-01, 1.3365e-06, 8.8395e-07, 9.2656e-07, 9.1124e-07,
        1.7889e-06, 1.5808e-06, 7.0323e-07, 1.1536e-06, 2.2288e-06, 1.8731e-06,
        6.3734e-07, 6.6047e-07, 7.9057e-07, 4.8700e-07, 1.1492e-06, 3.2593e-07,
        6.2831e-07, 5.4507e-07, 1.6286e-06, 8.2608e-07, 2.8832e-06, 1.5154e-01,
        2.4352e-06, 2.3781e-06, 1.2951e-06, 6.4583e-07, 1.3572e-06, 1.6653e-06,
        1.6411e-06, 2.0665e-06, 3.4463e-07, 7.5757e-07, 1.6580e-06, 1.2366e-06,
        9.4267e-07, 2.1719e-06, 1.2403e-06, 1.8412e-06, 9.7325e-07, 2.2103e-06,
        6.4326e-07, 1.1296e-06, 6.7108e-07, 1.0961e-06, 1.6503e-06, 2.2689e-06,
        9.8781e-07, 2.2292e-06, 2.0591e-06, 1.2766e-06, 1.5118e-01, 1.2766e-06,
        9.4488e-07, 2.4580e-06, 1.1071e-06, 9.8585e-07, 1.2165e-06, 8.4259e-07,
        1.9584e-06, 8.1272e-07, 6.4346e-07, 2.7831e-06, 1.9716e-06, 1.3946e-06,
        1.0312e-06, 6.9546e-07, 3.1740e-06, 2.0690e-06, 1.1300e-06, 1.5301e-06,
        7.7537e-07, 3.9705e-07, 2.9012e-06, 1.0614e-06, 1.4190e-06, 1.2837e-06,
        3.3420e-06, 3.7300e-06, 2.1476e-06, 1.0025e-06, 1.0137e-01, 1.4498e-06,
        9.5275e-07, 1.3023e-06, 1.2972e-06, 1.1684e-06, 9.2413e-07, 8.8191e-07,
        2.4185e-06, 1.3730e-06, 7.4737e-07, 7.4456e-07, 1.3719e-06, 1.0120e-06,
        1.2448e-06, 1.5162e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7377e-06, 1.3716e-06, 2.8030e-07, 7.8909e-07, 3.5082e-07, 7.5538e-07,
        7.0872e-07, 7.3730e-07, 9.1664e-07, 1.1248e-06, 6.6922e-07, 1.9483e-06,
        1.7485e-06, 1.4613e-06, 2.7267e-06, 1.0258e-06, 1.0745e-06, 1.3590e-06,
        5.1770e-07, 1.0573e-06, 7.0559e-07, 3.0691e-06, 2.3859e-06, 6.2582e-07,
        9.6031e-07, 1.7691e-06, 7.0484e-07, 2.0324e-01, 1.0823e-06, 6.6249e-07,
        3.0855e-07, 4.8812e-07, 5.9691e-07, 8.7302e-07, 2.2342e-06, 5.0050e-07,
        1.5296e-06, 1.0833e-06, 1.7213e-06, 3.5262e-07, 8.8502e-07, 1.2482e-06,
        1.3915e-01, 8.4225e-07, 1.6480e-06, 5.7929e-07, 7.2896e-07, 2.1416e-06,
        1.7698e-06, 1.0987e-06, 1.1404e-06, 2.6879e-06, 7.1874e-07, 1.7449e-06,
        8.9269e-07, 1.0750e-06, 5.4489e-07, 1.7674e-06, 1.5361e-06, 7.5595e-07,
        1.0987e-06, 1.9463e-06, 1.6440e-06, 2.3299e-06, 1.2831e-06, 5.4489e-07,
        2.2633e-06, 2.1797e-06, 1.0987e-06, 1.0406e-06, 8.3589e-07, 2.8030e-07,
        1.1655e-06, 6.7334e-07, 1.3955e-06, 1.2247e-06, 6.5181e-07, 6.6249e-07,
        1.0038e-06, 1.8667e-06, 5.1346e-07, 9.9641e-07, 2.2049e-07, 5.5599e-07,
        2.1489e-06, 4.1154e-07, 9.6748e-07, 1.7340e-01, 1.3216e-06, 1.1942e-06,
        1.7522e-06, 1.5982e-06, 2.4669e-06, 9.3889e-07, 1.2362e-06, 1.1366e-06,
        1.2369e-06, 1.9483e-06, 4.7879e-07, 2.1822e-06, 1.3345e-06, 1.7782e-06,
        7.8823e-07, 2.2250e-06, 1.1942e-06, 1.0570e-06, 1.8727e-06, 1.0042e-06,
        1.1126e-06, 5.5425e-07, 1.2494e-06, 3.3620e-07, 7.3730e-07, 8.8502e-07,
        9.6866e-07, 8.8018e-07, 1.0349e-06, 4.2903e-07, 2.6118e-07, 1.9130e-06,
        8.3149e-07, 1.8958e-06, 8.8502e-07, 6.0446e-07, 6.6249e-07, 1.2369e-06,
        2.0418e-06, 1.7698e-06, 3.9610e-07, 9.0638e-07, 4.5217e-07, 1.7379e-06,
        9.1403e-07, 1.5721e-01, 1.0133e-06, 2.6977e-06, 3.5878e-07, 1.9130e-06,
        2.4370e-06, 6.0354e-07, 1.2062e-06, 1.6702e-06, 7.2909e-07, 6.6472e-07,
        1.4084e-06, 4.6577e-07, 1.6237e-06, 1.7563e-06, 2.0418e-06, 2.0418e-06,
        8.5721e-07, 2.0418e-06, 9.7040e-07, 1.1610e-06, 1.1982e-06, 5.6867e-07,
        6.7550e-07, 1.9131e-06, 5.5425e-07, 1.1963e-06, 7.4693e-07, 4.2979e-07,
        4.5459e-07, 9.0674e-07, 1.6602e-06, 3.9627e-06, 1.5417e-06, 1.1942e-06,
        6.6472e-07, 3.9271e-07, 6.5155e-07, 1.6061e-06, 1.1307e-06, 1.6391e-06,
        1.9720e-01, 7.0894e-07, 4.0939e-07, 3.7176e-07, 1.7316e-06, 7.2896e-07,
        6.4003e-07, 8.4375e-07, 1.0787e-06, 1.0597e-06, 2.5765e-06, 8.4440e-07,
        7.7743e-07, 2.1416e-06, 5.3944e-07, 2.6879e-06, 1.8410e-06, 6.8429e-07,
        1.7698e-06, 2.6118e-07, 2.3299e-06, 2.2899e-01, 7.4238e-07, 4.1748e-07,
        7.9333e-07, 7.1915e-07, 9.1664e-07, 1.3974e-06, 2.9627e-06, 9.2123e-07,
        1.9265e-07, 3.4341e-07, 6.2119e-07, 1.6338e-07, 9.6031e-07, 1.2491e-06,
        7.4238e-07, 1.4282e-06, 1.1387e-06, 7.2370e-07, 7.6165e-07, 7.4238e-07,
        4.8078e-07, 2.0488e-06, 2.8030e-07, 8.4225e-07, 6.3662e-07, 1.1404e-06,
        7.2909e-07, 7.5757e-07, 2.7369e-06, 1.3891e-06, 1.1242e-06, 1.3939e-06,
        1.1490e-06, 1.7923e-06, 7.5738e-07, 7.0430e-07, 1.6603e-06, 6.0354e-07,
        1.1727e-06, 4.2951e-07, 6.0698e-07, 6.6249e-07, 2.3717e-07, 1.7212e-06,
        5.6434e-07, 2.0863e-06, 8.3003e-07, 8.8727e-07, 1.9149e-06, 2.0351e-06,
        1.3567e-06, 1.3939e-06, 1.0787e-06, 1.7902e-06, 1.7737e-06, 1.1727e-06,
        1.0607e-06, 1.3029e-06, 1.9265e-07, 1.2550e-06, 7.6165e-07, 1.8017e-07,
        1.3346e-06, 7.2909e-07, 3.1003e-07, 1.1307e-06, 1.1049e-06, 1.9483e-06,
        1.2112e-06, 1.7316e-06, 4.9451e-07, 6.5155e-07, 2.2175e-06, 4.8281e-07,
        2.9705e-07, 1.4115e-06, 1.4586e-01, 4.0216e-07, 6.0354e-07, 1.0832e-06,
        7.6165e-07, 1.2369e-06, 1.5563e-06, 7.5663e-07, 5.7372e-07, 3.3642e-07,
        8.4440e-07, 1.1041e-07, 7.9333e-07, 2.3223e-06, 1.3197e-06, 6.7213e-07,
        1.4084e-06, 2.6879e-06, 5.4489e-07, 7.6257e-07, 6.0413e-07, 1.6068e-06,
        8.8502e-07, 5.6867e-07, 1.1221e-06, 1.0945e-06, 2.2342e-06, 9.6031e-07,
        2.1416e-06, 8.6611e-07, 2.1922e-06, 2.5883e-06, 1.2702e-06, 1.1832e-06,
        7.5315e-07, 1.0760e-06, 2.8030e-07, 8.3149e-07, 6.5155e-07, 1.5554e-06,
        6.5001e-07, 1.8351e-07, 1.1010e-06, 9.7714e-07, 6.6051e-07, 9.6031e-07,
        1.9987e-06, 7.5671e-07, 1.8638e-06, 2.0449e-06, 9.4434e-07, 8.4472e-07,
        1.2831e-06, 8.4959e-07, 1.1414e-06, 6.8882e-07, 1.5799e-06, 7.8823e-07,
        2.0248e-06, 3.2770e-06, 6.5155e-07, 5.4489e-07, 9.8222e-07, 9.0638e-07,
        1.0224e-01, 1.5973e-06, 2.2046e-07, 2.5673e-06, 6.3706e-07, 6.3829e-07,
        7.7695e-07, 6.4030e-07, 7.2896e-07, 6.6472e-07, 5.4769e-07, 1.4597e-07,
        7.5671e-07, 9.5450e-07, 6.4619e-07, 1.5269e-01, 1.7922e-06, 2.1922e-06,
        2.2798e-06, 1.1010e-06, 1.5296e-06, 1.4921e-06, 1.3093e-01, 4.8812e-07,
        2.6118e-07, 6.6030e-07, 4.5922e-07, 3.6504e-07, 9.8139e-07, 1.0833e-06,
        1.1149e-06, 9.9746e-02, 1.9131e-06, 2.2439e-06, 6.5001e-07, 5.5425e-07,
        1.8088e-06, 1.0882e-06, 8.0227e-07, 2.6118e-07, 2.7792e-07, 2.2342e-06,
        9.6748e-07, 9.4486e-07, 1.0597e-06, 2.3945e-06, 3.7176e-07, 1.7620e-06,
        6.6922e-07, 1.6076e-06, 2.6698e-07, 2.6118e-07, 2.9958e-06, 1.0082e-01,
        7.8020e-07, 9.8222e-07, 6.6051e-07, 3.3620e-07, 2.3299e-06, 4.3045e-07,
        9.6748e-07, 1.4957e-06, 1.0608e-06, 2.2902e-06, 8.8502e-07, 9.1412e-07,
        1.6707e-07, 6.0446e-07, 1.4835e-06, 1.1969e-01, 3.8887e-07, 6.6472e-07,
        3.7638e-07, 8.9269e-07, 7.4168e-07, 6.4658e-07, 2.3223e-06, 1.5973e-06,
        1.1955e-06, 1.9130e-06, 3.5329e-07, 1.0570e-06, 1.2783e-06, 1.3448e-06,
        2.4659e-06, 3.3620e-07, 9.6866e-07, 7.5778e-07, 1.6409e-06, 2.0293e-06,
        1.3864e-07, 1.5296e-06, 5.4280e-07, 5.5852e-07, 8.9269e-07, 1.9130e-06,
        1.2014e-06, 4.0039e-07, 9.9206e-07, 1.5702e-06, 9.5449e-07, 1.9267e-06,
        2.3171e-06, 1.0129e-06, 1.7379e-06, 4.8280e-07, 3.8193e-07, 1.5174e-06,
        6.2638e-07, 1.1830e-06, 9.6866e-07, 2.0449e-06, 1.0787e-06, 1.7808e-06,
        1.1382e-06, 1.8553e-06, 1.4906e-06, 1.1427e-01, 1.1832e-06, 8.4924e-07,
        1.8713e-01, 1.8767e-06, 7.4238e-07, 1.3925e-06, 1.4282e-06, 6.7810e-07,
        1.3160e-06, 2.0418e-06, 9.4096e-07, 8.8480e-07, 3.3620e-07, 7.3527e-07,
        2.5920e-06, 2.5669e-06, 2.8669e-06, 8.5228e-07, 1.2462e-06, 1.1010e-06,
        8.6491e-07, 6.3092e-07, 1.3298e-06, 1.1942e-06, 4.5217e-07, 1.8785e-06,
        9.5736e-07, 6.5155e-07, 1.0945e-06, 3.9511e-07, 1.4084e-06, 7.8372e-07,
        1.5275e-06, 6.5768e-07, 1.3159e-06, 6.6922e-07, 7.9187e-07, 6.2638e-07,
        1.5526e-01, 3.7708e-07, 5.5238e-07, 1.1332e-07, 1.0749e-06, 1.6314e-06,
        1.2780e-06, 9.7935e-07, 1.2764e-06, 7.0559e-07, 1.3874e-06, 7.0872e-07,
        7.5672e-07, 7.5315e-07, 8.3884e-07, 1.8460e-06, 1.1342e-06, 5.6308e-07,
        7.2182e-07, 8.8727e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.9834, 0.9878, 0.8866, 0.8743, 0.8926, 1.0310, 0.9419, 0.9678, 1.1526,
        0.9777], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.48148149251937866, 0.0, 1.0, 0.0, 1.0, 0.03703703731298447, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.03703703731298447, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.37037038803100586, 1.0, 1.0, 0.03703703731298447, 1.0, 1.0]

 sparsity of   [0.4322916567325592, 0.4479166567325592, 0.4392361044883728, 0.4375, 0.4305555522441864, 0.453125, 0.4375, 0.421875, 0.4392361044883728, 0.4305555522441864, 0.4375, 0.4340277910232544, 0.4322916567325592, 0.4305555522441864, 0.456597238779068, 0.4739583432674408, 0.4305555522441864, 0.4513888955116272, 0.4236111044883728, 0.4479166567325592, 0.4496527910232544, 0.4392361044883728, 0.4427083432674408, 0.4809027910232544, 0.4461805522441864, 0.3958333432674408, 0.4392361044883728, 0.440972238779068, 0.4461805522441864, 0.4479166567325592, 0.4496527910232544, 0.456597238779068, 0.440972238779068, 0.4375, 0.4461805522441864, 0.4375, 0.4444444477558136, 0.4583333432674408, 0.4340277910232544, 0.4479166567325592, 0.4357638955116272, 0.4340277910232544, 0.440972238779068, 0.4444444477558136, 0.4305555522441864, 0.4444444477558136, 0.4375, 0.4375, 0.4010416567325592, 0.4079861044883728, 0.4444444477558136, 0.4461805522441864, 0.453125, 0.4461805522441864, 0.4461805522441864, 0.4357638955116272, 0.4496527910232544, 0.4427083432674408, 0.4357638955116272, 0.4652777910232544, 0.4618055522441864, 0.440972238779068, 0.4600694477558136, 0.440972238779068]

 sparsity of   [0.0, 0.010416666977107525, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 1.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0069444444961845875, 0.0052083334885537624, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0086805559694767, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0086805559694767, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0069444444961845875, 0.0]

 sparsity of   [0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.009548611007630825, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.009548611007630825, 0.013888888992369175, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013888888992369175, 0.01128472201526165, 0.010416666977107525, 0.0078125, 0.010416666977107525, 0.013888888992369175, 0.014756944961845875, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.01215277798473835, 0.009548611007630825, 0.010416666977107525, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.013020833022892475, 0.01128472201526165, 0.014756944961845875, 0.01128472201526165, 0.014756944961845875, 0.0086805559694767, 0.013888888992369175, 0.0078125, 0.015625, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.015625, 0.014756944961845875, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.014756944961845875, 0.015625, 0.010416666977107525, 0.009548611007630825, 0.0078125, 0.010416666977107525, 0.0173611119389534, 0.015625, 0.01128472201526165, 0.010416666977107525, 0.01128472201526165, 0.015625, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.01215277798473835, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.0086805559694767, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.013888888992369175, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.010416666977107525, 0.014756944961845875, 0.0173611119389534, 0.010416666977107525, 0.01128472201526165, 0.015625, 0.013888888992369175, 0.0086805559694767, 0.01128472201526165, 0.0086805559694767, 0.010416666977107525, 0.013888888992369175, 0.01128472201526165, 0.013020833022892475, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.014756944961845875, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.0086805559694767, 0.010416666977107525, 0.014756944961845875, 0.01215277798473835]

 sparsity of   [1.0, 0.014756944961845875, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01128472201526165, 0.0086805559694767, 0.015625, 0.010416666977107525, 0.010416666977107525, 0.01128472201526165, 0.013888888992369175, 0.014756944961845875, 0.013888888992369175, 0.01215277798473835, 0.013020833022892475, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.009548611007630825, 1.0, 0.013888888992369175, 0.010416666977107525, 0.014756944961845875, 0.014756944961845875, 0.01128472201526165, 0.01128472201526165, 0.013020833022892475, 1.0, 0.010416666977107525, 0.013888888992369175, 1.0, 0.01215277798473835, 1.0, 0.01128472201526165, 0.01128472201526165, 0.015625, 0.013888888992369175, 1.0, 0.01215277798473835, 0.01128472201526165, 0.01215277798473835, 0.013020833022892475, 0.009548611007630825, 0.015625, 0.014756944961845875, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.01128472201526165, 0.0164930559694767, 0.013020833022892475, 0.01128472201526165, 0.01128472201526165, 0.014756944961845875, 0.01128472201526165, 0.013020833022892475, 1.0, 0.015625, 0.02170138992369175, 1.0, 0.01215277798473835, 0.01215277798473835, 0.015625, 0.01215277798473835, 0.010416666977107525, 0.01128472201526165, 0.014756944961845875, 0.010416666977107525, 0.014756944961845875, 0.015625, 0.013020833022892475, 0.014756944961845875, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.009548611007630825, 0.01128472201526165, 0.014756944961845875, 0.0086805559694767, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.009548611007630825, 0.013020833022892475, 0.01215277798473835, 0.010416666977107525, 1.0, 0.01128472201526165, 0.01822916604578495, 0.010416666977107525, 0.009548611007630825, 0.01215277798473835, 0.015625, 0.0086805559694767, 0.01215277798473835, 0.009548611007630825, 1.0, 0.014756944961845875, 0.013020833022892475, 0.014756944961845875, 0.013020833022892475, 1.0, 0.0164930559694767, 0.01215277798473835, 0.013888888992369175, 0.010416666977107525, 0.0164930559694767, 0.01215277798473835, 1.0, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.01128472201526165, 0.01215277798473835, 0.01215277798473835, 0.01128472201526165, 0.013020833022892475, 1.0, 0.009548611007630825, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.0755208358168602, 0.01128472201526165, 0.01128472201526165, 0.014756944961845875, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 0.015625, 0.01215277798473835, 0.0164930559694767, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 1.0, 0.01215277798473835, 0.013888888992369175, 0.01215277798473835, 0.01215277798473835, 0.0086805559694767, 0.01128472201526165, 0.014756944961845875, 1.0, 0.01215277798473835, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.01215277798473835, 0.0164930559694767, 0.010416666977107525, 0.01215277798473835, 0.015625, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.013888888992369175, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.009548611007630825, 0.0164930559694767, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.014756944961845875, 0.009548611007630825, 0.013888888992369175, 0.010416666977107525, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.013888888992369175, 0.01215277798473835, 0.01215277798473835, 0.014756944961845875, 0.009548611007630825, 0.013020833022892475, 0.013020833022892475, 0.009548611007630825, 0.01215277798473835, 0.009548611007630825, 0.01215277798473835, 0.014756944961845875, 0.013020833022892475, 0.01128472201526165, 0.0164930559694767, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.009548611007630825, 1.0, 0.01822916604578495, 0.01215277798473835, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.013020833022892475, 0.013888888992369175, 0.013020833022892475, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.01128472201526165, 0.009548611007630825, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.014756944961845875, 1.0, 0.014756944961845875, 1.0, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 1.0, 0.010416666977107525, 0.014756944961845875, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.0173611119389534, 1.0, 0.01215277798473835, 0.009548611007630825, 0.0086805559694767, 0.009548611007630825, 0.013020833022892475, 0.010416666977107525]

 sparsity of   [1.0, 0.0889756977558136, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0863715261220932, 0.0876736119389534, 0.0859375, 1.0, 1.0, 0.0902777761220932, 0.0881076380610466, 1.0, 0.0894097238779068, 0.0872395858168602, 1.0, 0.0894097238779068, 0.0889756977558136, 1.0, 1.0, 0.0889756977558136, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 0.0850694477558136, 1.0, 0.0885416641831398, 0.0924479141831398, 1.0, 1.0, 0.0850694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08984375, 1.0, 0.0863715261220932, 0.0859375, 0.0902777761220932, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0846354141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0876736119389534, 0.0911458358168602, 0.0872395858168602, 0.086805559694767, 0.086805559694767, 1.0, 1.0, 0.0928819477558136, 0.0859375, 1.0, 0.08984375, 1.0, 1.0, 0.0894097238779068, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 0.0876736119389534, 1.0, 1.0, 1.0, 0.0876736119389534, 0.0876736119389534, 0.0889756977558136, 0.0894097238779068, 0.0872395858168602, 0.0963541641831398, 1.0, 0.0863715261220932, 0.0846354141831398, 1.0, 0.0894097238779068, 0.0889756977558136, 0.0885416641831398, 0.08984375, 0.0915798619389534, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 0.0872395858168602, 0.0872395858168602, 0.086805559694767, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 0.0872395858168602, 0.0876736119389534, 0.0855034738779068, 1.0, 1.0, 0.08984375, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 0.08984375, 0.0850694477558136, 1.0, 0.0859375, 1.0, 0.0872395858168602, 0.08984375, 0.0915798619389534, 1.0, 0.094618059694767, 0.0876736119389534, 1.0, 0.086805559694767, 0.0885416641831398, 1.0, 0.2782118022441864, 1.0, 0.0881076380610466, 1.0, 0.090711809694767, 1.0, 1.0, 0.0876736119389534, 0.0837673619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 0.0902777761220932, 0.0863715261220932, 0.0924479141831398, 0.0915798619389534, 1.0, 0.1137152761220932, 0.0872395858168602, 1.0, 0.0894097238779068, 0.0872395858168602, 0.0894097238779068, 1.0, 0.086805559694767, 0.0872395858168602, 0.0894097238779068, 0.0889756977558136, 0.08984375, 0.0911458358168602, 0.0876736119389534, 0.0872395858168602, 0.0894097238779068, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 0.0881076380610466, 0.0885416641831398, 1.0, 0.086805559694767, 1.0, 0.0894097238779068, 0.0855034738779068, 1.0, 1.0, 0.0881076380610466, 0.0885416641831398, 1.0, 0.0894097238779068, 0.0850694477558136, 1.0, 1.0, 0.0872395858168602, 0.0881076380610466, 1.0, 0.0872395858168602, 1.0, 1.0, 1.0, 1.0, 0.0920138880610466, 1.0, 0.08984375, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 1.0, 0.0894097238779068, 0.0911458358168602, 0.0872395858168602, 1.0, 0.0859375, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0894097238779068, 0.090711809694767, 0.086805559694767, 1.0, 1.0, 0.0876736119389534, 1.0, 1.0, 0.0855034738779068, 1.0, 1.0, 1.0, 1.0, 0.086805559694767]

 sparsity of   [0.5260416865348816, 0.5230034589767456, 0.534288227558136, 1.0, 0.5277777910232544, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5217013955116272, 1.0, 0.5360243320465088, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5260416865348816, 1.0, 1.0, 1.0, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5243055820465088, 0.522569477558136, 1.0, 1.0, 0.5247395634651184, 0.522569477558136, 1.0, 1.0, 0.534288227558136, 1.0, 1.0, 0.5234375, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 0.526475727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.52734375, 1.0, 0.5212673544883728, 1.0, 1.0, 1.0, 0.52734375, 1.0, 0.5238715410232544, 1.0, 1.0, 1.0, 0.5295138955116272, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 0.5234375, 0.5295138955116272, 0.5247395634651184, 1.0, 1.0, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5290798544883728, 1.0, 1.0, 0.5230034589767456, 1.0, 0.5234375, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 0.5234375, 0.5217013955116272, 1.0, 1.0, 0.5243055820465088, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5256076455116272, 1.0, 0.5217013955116272, 0.5251736044883728, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5269097089767456, 1.0, 1.0, 1.0, 0.5924479365348816, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5230034589767456, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.5221354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5269097089767456, 1.0, 0.5234375, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.526475727558136, 1.0, 0.5243055820465088, 1.0, 0.5234375, 0.5230034589767456, 1.0, 1.0, 0.52734375, 0.5269097089767456, 0.5277777910232544, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5238715410232544, 1.0, 1.0, 0.5212673544883728, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.51953125, 0.5256076455116272, 0.522569477558136, 1.0, 0.5316840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5238715410232544]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000868320465088, 1.0, 1.0, 0.7209201455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7078993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7443576455116272, 1.0, 1.0, 0.7052951455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 0.7087673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.706163227558136, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7057291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7044270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 0.7113715410232544, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7096354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7039930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7087673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7096354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9231770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9233940839767456, 1.0, 0.9223090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.922960102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9225260615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9216579794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9233940839767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9236111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.922960102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9772135615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875]

Total parameter pruned: 13841480.002446651 (unstructured) 13405977 (structured)

Test: [0/79]	Time 0.176 (0.176)	Loss 0.2673 (0.2673) ([0.169]+[0.098])	Prec@1 94.531 (94.531)
 * Prec@1 93.390

 Total elapsed time  0:54:13.481911 
 FINETUNING


 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.48148149251937866, 0.0, 1.0, 0.0, 1.0, 0.03703703731298447, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.03703703731298447, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.37037038803100586, 1.0, 1.0, 0.03703703731298447, 1.0, 1.0]

 sparsity of   [0.4322916567325592, 0.4479166567325592, 0.4392361044883728, 0.4375, 0.4305555522441864, 0.453125, 0.4375, 0.421875, 0.4392361044883728, 0.4305555522441864, 0.4375, 0.4340277910232544, 0.4322916567325592, 0.4305555522441864, 0.456597238779068, 0.4739583432674408, 0.4305555522441864, 0.4513888955116272, 0.4236111044883728, 0.4479166567325592, 0.4496527910232544, 0.4392361044883728, 0.4427083432674408, 0.4809027910232544, 0.4461805522441864, 0.3958333432674408, 0.4392361044883728, 0.440972238779068, 0.4461805522441864, 0.4479166567325592, 0.4496527910232544, 0.456597238779068, 0.440972238779068, 0.4375, 0.4461805522441864, 0.4375, 0.4444444477558136, 0.4583333432674408, 0.4340277910232544, 0.4479166567325592, 0.4357638955116272, 0.4340277910232544, 0.440972238779068, 0.4444444477558136, 0.4305555522441864, 0.4444444477558136, 0.4375, 0.4375, 0.4010416567325592, 0.4079861044883728, 0.4444444477558136, 0.4461805522441864, 0.453125, 0.4461805522441864, 0.4461805522441864, 0.4357638955116272, 0.4496527910232544, 0.4427083432674408, 0.4357638955116272, 0.4652777910232544, 0.4618055522441864, 0.440972238779068, 0.4600694477558136, 0.440972238779068]

 sparsity of   [0.0, 0.010416666977107525, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 1.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0069444444961845875, 0.0052083334885537624, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0086805559694767, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0086805559694767, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0086805559694767, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0069444444961845875, 0.0]

 sparsity of   [0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.009548611007630825, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.009548611007630825, 0.013888888992369175, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013888888992369175, 0.01128472201526165, 0.010416666977107525, 0.0078125, 0.010416666977107525, 0.013888888992369175, 0.014756944961845875, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.01215277798473835, 0.009548611007630825, 0.010416666977107525, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.013020833022892475, 0.01128472201526165, 0.014756944961845875, 0.01128472201526165, 0.014756944961845875, 0.0086805559694767, 0.013888888992369175, 0.0078125, 0.015625, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.015625, 0.014756944961845875, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.014756944961845875, 0.015625, 0.010416666977107525, 0.009548611007630825, 0.0078125, 0.010416666977107525, 0.0173611119389534, 0.015625, 0.01128472201526165, 0.010416666977107525, 0.01128472201526165, 0.015625, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.01215277798473835, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.0086805559694767, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.013888888992369175, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.010416666977107525, 0.014756944961845875, 0.0173611119389534, 0.010416666977107525, 0.01128472201526165, 0.015625, 0.013888888992369175, 0.0086805559694767, 0.01128472201526165, 0.0086805559694767, 0.010416666977107525, 0.013888888992369175, 0.01128472201526165, 0.013020833022892475, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.014756944961845875, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.0086805559694767, 0.010416666977107525, 0.014756944961845875, 0.01215277798473835]

 sparsity of   [1.0, 0.014756944961845875, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01128472201526165, 0.0086805559694767, 0.015625, 0.010416666977107525, 0.010416666977107525, 0.01128472201526165, 0.013888888992369175, 0.014756944961845875, 0.013888888992369175, 0.01215277798473835, 0.013020833022892475, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.009548611007630825, 1.0, 0.013888888992369175, 0.010416666977107525, 0.014756944961845875, 0.014756944961845875, 0.01128472201526165, 0.01128472201526165, 0.013020833022892475, 1.0, 0.010416666977107525, 0.013888888992369175, 1.0, 0.01215277798473835, 1.0, 0.01128472201526165, 0.01128472201526165, 0.015625, 0.013888888992369175, 1.0, 0.01215277798473835, 0.01128472201526165, 0.01215277798473835, 0.013020833022892475, 0.009548611007630825, 0.015625, 0.014756944961845875, 0.013888888992369175, 0.010416666977107525, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.01128472201526165, 0.0164930559694767, 0.013020833022892475, 0.01128472201526165, 0.01128472201526165, 0.014756944961845875, 0.01128472201526165, 0.013020833022892475, 1.0, 0.015625, 0.02170138992369175, 1.0, 0.01215277798473835, 0.01215277798473835, 0.015625, 0.01215277798473835, 0.010416666977107525, 0.01128472201526165, 0.014756944961845875, 0.010416666977107525, 0.014756944961845875, 0.015625, 0.013020833022892475, 0.014756944961845875, 0.013020833022892475, 0.014756944961845875, 0.010416666977107525, 0.009548611007630825, 0.01128472201526165, 0.014756944961845875, 0.0086805559694767, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.009548611007630825, 0.013020833022892475, 0.01215277798473835, 0.010416666977107525, 1.0, 0.01128472201526165, 0.01822916604578495, 0.010416666977107525, 0.009548611007630825, 0.01215277798473835, 0.015625, 0.0086805559694767, 0.01215277798473835, 0.009548611007630825, 1.0, 0.014756944961845875, 0.013020833022892475, 0.014756944961845875, 0.013020833022892475, 1.0, 0.0164930559694767, 0.01215277798473835, 0.013888888992369175, 0.010416666977107525, 0.0164930559694767, 0.01215277798473835, 1.0, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.01128472201526165, 0.01215277798473835, 0.01215277798473835, 0.01128472201526165, 0.013020833022892475, 1.0, 0.009548611007630825, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.0755208358168602, 0.01128472201526165, 0.01128472201526165, 0.014756944961845875, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 0.015625, 0.01215277798473835, 0.0164930559694767, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 1.0, 0.01215277798473835, 0.013888888992369175, 0.01215277798473835, 0.01215277798473835, 0.0086805559694767, 0.01128472201526165, 0.014756944961845875, 1.0, 0.01215277798473835, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.01215277798473835, 0.0164930559694767, 0.010416666977107525, 0.01215277798473835, 0.015625, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 0.013888888992369175, 0.010416666977107525, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.009548611007630825, 0.0164930559694767, 0.010416666977107525, 0.01215277798473835, 0.01128472201526165, 0.014756944961845875, 0.009548611007630825, 0.013888888992369175, 0.010416666977107525, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.013888888992369175, 0.01215277798473835, 0.01215277798473835, 0.014756944961845875, 0.009548611007630825, 0.013020833022892475, 0.013020833022892475, 0.009548611007630825, 0.01215277798473835, 0.009548611007630825, 0.01215277798473835, 0.014756944961845875, 0.013020833022892475, 0.01128472201526165, 0.0164930559694767, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.009548611007630825, 1.0, 0.01822916604578495, 0.01215277798473835, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.013020833022892475, 0.013888888992369175, 0.013020833022892475, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.01128472201526165, 0.009548611007630825, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.014756944961845875, 1.0, 0.014756944961845875, 1.0, 0.013020833022892475, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 0.013888888992369175, 1.0, 0.010416666977107525, 0.014756944961845875, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.0173611119389534, 1.0, 0.01215277798473835, 0.009548611007630825, 0.0086805559694767, 0.009548611007630825, 0.013020833022892475, 0.010416666977107525]

 sparsity of   [1.0, 0.0889756977558136, 1.0, 1.0, 0.086805559694767, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0863715261220932, 0.0876736119389534, 0.0859375, 1.0, 1.0, 0.0902777761220932, 0.0881076380610466, 1.0, 0.0894097238779068, 0.0872395858168602, 1.0, 0.0894097238779068, 0.0889756977558136, 1.0, 1.0, 0.0889756977558136, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 0.0850694477558136, 1.0, 0.0885416641831398, 0.0924479141831398, 1.0, 1.0, 0.0850694477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08984375, 1.0, 0.0863715261220932, 0.0859375, 0.0902777761220932, 0.09375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0846354141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0876736119389534, 0.0911458358168602, 0.0872395858168602, 0.086805559694767, 0.086805559694767, 1.0, 1.0, 0.0928819477558136, 0.0859375, 1.0, 0.08984375, 1.0, 1.0, 0.0894097238779068, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0894097238779068, 0.0876736119389534, 1.0, 1.0, 1.0, 0.0876736119389534, 0.0876736119389534, 0.0889756977558136, 0.0894097238779068, 0.0872395858168602, 0.0963541641831398, 1.0, 0.0863715261220932, 0.0846354141831398, 1.0, 0.0894097238779068, 0.0889756977558136, 0.0885416641831398, 0.08984375, 0.0915798619389534, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 0.0872395858168602, 0.0872395858168602, 0.086805559694767, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0894097238779068, 1.0, 1.0, 0.0872395858168602, 0.0876736119389534, 0.0855034738779068, 1.0, 1.0, 0.08984375, 1.0, 1.0, 1.0, 0.0894097238779068, 1.0, 0.08984375, 0.0850694477558136, 1.0, 0.0859375, 1.0, 0.0872395858168602, 0.08984375, 0.0915798619389534, 1.0, 0.094618059694767, 0.0876736119389534, 1.0, 0.086805559694767, 0.0885416641831398, 1.0, 0.2782118022441864, 1.0, 0.0881076380610466, 1.0, 0.090711809694767, 1.0, 1.0, 0.0876736119389534, 0.0837673619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0889756977558136, 0.0902777761220932, 0.0863715261220932, 0.0924479141831398, 0.0915798619389534, 1.0, 0.1137152761220932, 0.0872395858168602, 1.0, 0.0894097238779068, 0.0872395858168602, 0.0894097238779068, 1.0, 0.086805559694767, 0.0872395858168602, 0.0894097238779068, 0.0889756977558136, 0.08984375, 0.0911458358168602, 0.0876736119389534, 0.0872395858168602, 0.0894097238779068, 1.0, 1.0, 0.0885416641831398, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 0.0881076380610466, 0.0885416641831398, 1.0, 0.086805559694767, 1.0, 0.0894097238779068, 0.0855034738779068, 1.0, 1.0, 0.0881076380610466, 0.0885416641831398, 1.0, 0.0894097238779068, 0.0850694477558136, 1.0, 1.0, 0.0872395858168602, 0.0881076380610466, 1.0, 0.0872395858168602, 1.0, 1.0, 1.0, 1.0, 0.0920138880610466, 1.0, 0.08984375, 0.0889756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.090711809694767, 1.0, 0.0894097238779068, 0.0911458358168602, 0.0872395858168602, 1.0, 0.0859375, 1.0, 0.0872395858168602, 1.0, 1.0, 0.0894097238779068, 0.090711809694767, 0.086805559694767, 1.0, 1.0, 0.0876736119389534, 1.0, 1.0, 0.0855034738779068, 1.0, 1.0, 1.0, 1.0, 0.086805559694767]

 sparsity of   [0.5260416865348816, 0.5230034589767456, 0.534288227558136, 1.0, 0.5277777910232544, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5217013955116272, 1.0, 0.5360243320465088, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5260416865348816, 1.0, 1.0, 1.0, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5243055820465088, 0.522569477558136, 1.0, 1.0, 0.5247395634651184, 0.522569477558136, 1.0, 1.0, 0.534288227558136, 1.0, 1.0, 0.5234375, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 0.526475727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.52734375, 1.0, 0.5212673544883728, 1.0, 1.0, 1.0, 0.52734375, 1.0, 0.5238715410232544, 1.0, 1.0, 1.0, 0.5295138955116272, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 0.5234375, 0.5295138955116272, 0.5247395634651184, 1.0, 1.0, 0.5217013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5290798544883728, 1.0, 1.0, 0.5230034589767456, 1.0, 0.5234375, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 0.5234375, 0.5217013955116272, 1.0, 1.0, 0.5243055820465088, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5256076455116272, 1.0, 0.5217013955116272, 0.5251736044883728, 1.0, 1.0, 0.522569477558136, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5269097089767456, 1.0, 1.0, 1.0, 0.5924479365348816, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5230034589767456, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.5221354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 1.0, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 0.5269097089767456, 1.0, 0.5234375, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.526475727558136, 1.0, 0.5243055820465088, 1.0, 0.5234375, 0.5230034589767456, 1.0, 1.0, 0.52734375, 0.5269097089767456, 0.5277777910232544, 1.0, 0.5230034589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5238715410232544, 1.0, 1.0, 0.5212673544883728, 1.0, 1.0, 0.5247395634651184, 1.0, 1.0, 0.5234375, 1.0, 1.0, 1.0, 1.0, 0.51953125, 0.5256076455116272, 0.522569477558136, 1.0, 0.5316840410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5238715410232544]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7048611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7013888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000868320465088, 1.0, 1.0, 0.7209201455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7078993320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7443576455116272, 1.0, 1.0, 0.7052951455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.70703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 0.7087673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.706163227558136, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7057291865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7044270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7035590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 0.7005208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 0.7113715410232544, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7096354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.702256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7039930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7018229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7026909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7087673544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7096354365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9231770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9233940839767456, 1.0, 0.9223090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.922960102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9225260615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9216579794883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9233940839767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9236111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.922960102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9826388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.982421875, 1.0, 0.982421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9884982705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9772135615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875]

Total parameter pruned: 13841480.002446651 (unstructured) 13405977 (structured)

Test: [0/79]	Time 0.168 (0.168)	Loss 2.5120 (2.5120) ([2.428]+[0.084])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0186, device='cuda:0')
Epoch: [300][0/391]	Time 0.193 (0.193)	Data 0.175 (0.175)	Loss 2.3116 (2.3116) ([2.312]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [300][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3055 (2.3065) ([2.306]+[0.000])	Prec@1 10.156 (10.102)
Epoch: [300][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3048 (2.3060) ([2.305]+[0.000])	Prec@1 10.156 (10.086)
Epoch: [300][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3009 (2.3060) ([2.301]+[0.000])	Prec@1 13.281 (9.946)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3065 (2.3065) ([2.306]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0890, device='cuda:0')
Epoch: [301][0/391]	Time 0.220 (0.220)	Data 0.200 (0.200)	Loss 2.3217 (2.3217) ([2.322]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [301][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3045 (2.3047) ([2.304]+[0.000])	Prec@1 16.406 (10.172)
Epoch: [301][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3053 (2.3050) ([2.305]+[0.000])	Prec@1 8.594 (10.001)
Epoch: [301][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3051 (2.3049) ([2.305]+[0.000])	Prec@1 13.281 (9.975)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3058 (2.3058) ([2.306]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0330, device='cuda:0')
Epoch: [302][0/391]	Time 0.205 (0.205)	Data 0.185 (0.185)	Loss 2.3151 (2.3151) ([2.315]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [302][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3070 (2.3041) ([2.307]+[0.000])	Prec@1 8.594 (10.133)
Epoch: [302][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3103 (2.3042) ([2.310]+[0.000])	Prec@1 10.156 (10.117)
Epoch: [302][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3035 (2.3041) ([2.303]+[0.000])	Prec@1 10.938 (10.052)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3051 (2.3051) ([2.305]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0296, device='cuda:0')
Epoch: [303][0/391]	Time 0.194 (0.194)	Data 0.175 (0.175)	Loss 2.3048 (2.3048) ([2.305]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [303][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3071 (2.3034) ([2.307]+[0.000])	Prec@1 9.375 (9.916)
Epoch: [303][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3050 (2.3038) ([2.305]+[0.000])	Prec@1 10.156 (9.989)
Epoch: [303][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.2950 (2.3037) ([2.295]+[0.000])	Prec@1 12.500 (9.972)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3047 (2.3047) ([2.305]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0146, device='cuda:0')
Epoch: [304][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3059 (2.3059) ([2.306]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [304][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3002 (2.3038) ([2.300]+[0.000])	Prec@1 11.719 (9.754)
Epoch: [304][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3099 (2.3036) ([2.310]+[0.000])	Prec@1 7.031 (9.935)
Epoch: [304][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3113 (2.3035) ([2.311]+[0.000])	Prec@1 10.938 (9.949)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3043 (2.3043) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0408, device='cuda:0')
Epoch: [305][0/391]	Time 0.206 (0.206)	Data 0.184 (0.184)	Loss 2.3086 (2.3086) ([2.309]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [305][100/391]	Time 0.016 (0.016)	Data 0.000 (0.002)	Loss 2.3054 (2.3032) ([2.305]+[0.000])	Prec@1 9.375 (10.118)
Epoch: [305][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.2998 (2.3034) ([2.300]+[0.000])	Prec@1 14.844 (9.939)
Epoch: [305][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3062 (2.3033) ([2.306]+[0.000])	Prec@1 8.594 (9.933)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3040 (2.3040) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0310, device='cuda:0')
Epoch: [306][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3016 (2.3016) ([2.302]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [306][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3034) ([2.303]+[0.000])	Prec@1 10.938 (10.048)
Epoch: [306][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3046 (2.3033) ([2.305]+[0.000])	Prec@1 8.594 (9.907)
Epoch: [306][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3015 (2.3032) ([2.302]+[0.000])	Prec@1 13.281 (9.884)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3038 (2.3038) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0216, device='cuda:0')
Epoch: [307][0/391]	Time 0.201 (0.201)	Data 0.181 (0.181)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [307][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.2996 (2.3030) ([2.300]+[0.000])	Prec@1 11.719 (10.164)
Epoch: [307][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3050 (2.3032) ([2.305]+[0.000])	Prec@1 5.469 (9.946)
Epoch: [307][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.2998 (2.3030) ([2.300]+[0.000])	Prec@1 12.500 (10.045)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3036 (2.3036) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0277, device='cuda:0')
Epoch: [308][0/391]	Time 0.205 (0.205)	Data 0.185 (0.185)	Loss 2.3021 (2.3021) ([2.302]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [308][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3030) ([2.302]+[0.000])	Prec@1 7.031 (9.955)
Epoch: [308][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3037 (2.3028) ([2.304]+[0.000])	Prec@1 9.375 (10.051)
Epoch: [308][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3031 (2.3029) ([2.303]+[0.000])	Prec@1 7.812 (10.034)
Test: [0/79]	Time 0.200 (0.200)	Loss 2.3034 (2.3034) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0150, device='cuda:0')
Epoch: [309][0/391]	Time 0.208 (0.208)	Data 0.189 (0.189)	Loss 2.3010 (2.3010) ([2.301]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [309][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.2989 (2.3025) ([2.299]+[0.000])	Prec@1 14.844 (10.009)
Epoch: [309][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3039 (2.3026) ([2.304]+[0.000])	Prec@1 11.719 (10.071)
Epoch: [309][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3064 (2.3027) ([2.306]+[0.000])	Prec@1 6.250 (10.107)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3033 (2.3033) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [310][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3053 (2.3053) ([2.305]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [310][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3023 (2.3029) ([2.302]+[0.000])	Prec@1 10.156 (9.754)
Epoch: [310][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3016 (2.3028) ([2.302]+[0.000])	Prec@1 10.156 (9.884)
Epoch: [310][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3021 (2.3028) ([2.302]+[0.000])	Prec@1 9.375 (9.977)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3032 (2.3032) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0270, device='cuda:0')
Epoch: [311][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3015 (2.3015) ([2.302]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [311][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.2997 (2.3026) ([2.300]+[0.000])	Prec@1 17.969 (10.373)
Epoch: [311][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3053 (2.3027) ([2.305]+[0.000])	Prec@1 8.594 (10.113)
Epoch: [311][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3051 (2.3027) ([2.305]+[0.000])	Prec@1 7.812 (10.052)
Test: [0/79]	Time 0.169 (0.169)	Loss 2.3031 (2.3031) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [312][0/391]	Time 0.201 (0.201)	Data 0.181 (0.181)	Loss 2.3043 (2.3043) ([2.304]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [312][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.210)
Epoch: [312][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3032 (2.3027) ([2.303]+[0.000])	Prec@1 10.156 (9.981)
Epoch: [312][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3014 (2.3027) ([2.301]+[0.000])	Prec@1 11.719 (10.063)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3030 (2.3030) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0195, device='cuda:0')
Epoch: [313][0/391]	Time 0.204 (0.204)	Data 0.184 (0.184)	Loss 2.3033 (2.3033) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [313][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3031 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.924)
Epoch: [313][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3027) ([2.303]+[0.000])	Prec@1 9.375 (9.946)
Epoch: [313][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3046 (2.3027) ([2.305]+[0.000])	Prec@1 6.250 (10.006)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3030 (2.3030) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0227, device='cuda:0')
Epoch: [314][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3016 (2.3016) ([2.302]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [314][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3038 (2.3028) ([2.304]+[0.000])	Prec@1 7.031 (9.847)
Epoch: [314][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3041 (2.3026) ([2.304]+[0.000])	Prec@1 8.594 (10.044)
Epoch: [314][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3022 (2.3027) ([2.302]+[0.000])	Prec@1 9.375 (9.959)
Test: [0/79]	Time 0.167 (0.167)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0209, device='cuda:0')
Epoch: [315][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [315][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3041 (2.3027) ([2.304]+[0.000])	Prec@1 7.031 (9.862)
Epoch: [315][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.861)
Epoch: [315][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3020 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.930)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0356, device='cuda:0')
Epoch: [316][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3030 (2.3030) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [316][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3022 (2.3027) ([2.302]+[0.000])	Prec@1 10.156 (9.816)
Epoch: [316][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 12.500 (9.888)
Epoch: [316][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.990)
Test: [0/79]	Time 0.210 (0.210)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0237, device='cuda:0')
Epoch: [317][0/391]	Time 0.232 (0.232)	Data 0.212 (0.212)	Loss 2.3032 (2.3032) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [317][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.195)
Epoch: [317][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.861)
Epoch: [317][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3031 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.969)
Test: [0/79]	Time 0.201 (0.201)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0081, device='cuda:0')
Epoch: [318][0/391]	Time 0.209 (0.209)	Data 0.190 (0.190)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [318][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.932)
Epoch: [318][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.051)
Epoch: [318][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.032)
Test: [0/79]	Time 0.202 (0.202)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0202, device='cuda:0')
Epoch: [319][0/391]	Time 0.225 (0.225)	Data 0.201 (0.201)	Loss 2.3023 (2.3023) ([2.302]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [319][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (10.149)
Epoch: [319][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 7.812 (10.203)
Epoch: [319][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.006)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0295, device='cuda:0')
Epoch: [320][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [320][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3022 (2.3025) ([2.302]+[0.000])	Prec@1 12.500 (10.288)
Epoch: [320][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3030 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.160)
Epoch: [320][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.039)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0317, device='cuda:0')
Epoch: [321][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3032 (2.3032) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [321][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.893)
Epoch: [321][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 7.031 (10.199)
Epoch: [321][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.045)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0701, device='cuda:0')
Epoch: [322][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3022 (2.3022) ([2.302]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [322][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3030 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.855)
Epoch: [322][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3032 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.981)
Epoch: [322][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.860)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0500, device='cuda:0')
Epoch: [323][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3023 (2.3023) ([2.302]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [323][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.893)
Epoch: [323][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.102)
Epoch: [323][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.177)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0517, device='cuda:0')
Epoch: [324][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [324][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.816)
Epoch: [324][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.218)
Epoch: [324][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.110)
Test: [0/79]	Time 0.204 (0.204)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0374, device='cuda:0')
Epoch: [325][0/391]	Time 0.210 (0.210)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [325][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.909)
Epoch: [325][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 17.188 (9.849)
Epoch: [325][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.894)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0186, device='cuda:0')
Epoch: [326][0/391]	Time 0.208 (0.208)	Data 0.191 (0.191)	Loss 2.3023 (2.3023) ([2.302]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [326][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.932)
Epoch: [326][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (9.888)
Epoch: [326][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3021 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.899)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0191, device='cuda:0')
Epoch: [327][0/391]	Time 0.203 (0.203)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [327][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.537)
Epoch: [327][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.853)
Epoch: [327][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.775)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0181, device='cuda:0')
Epoch: [328][0/391]	Time 0.198 (0.198)	Data 0.178 (0.178)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [328][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (9.886)
Epoch: [328][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.946)
Epoch: [328][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.013)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [329][0/391]	Time 0.235 (0.235)	Data 0.216 (0.216)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [329][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.406)
Epoch: [329][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.562)
Epoch: [329][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.689)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0185, device='cuda:0')
Epoch: [330][0/391]	Time 0.206 (0.206)	Data 0.188 (0.188)	Loss 2.3025 (2.3025) ([2.302]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [330][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.210)
Epoch: [330][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (10.242)
Epoch: [330][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.161)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0217, device='cuda:0')
Epoch: [331][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [331][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.971)
Epoch: [331][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.841)
Epoch: [331][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.808)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0191, device='cuda:0')
Epoch: [332][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [332][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.226)
Epoch: [332][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.904)
Epoch: [332][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.821)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0299, device='cuda:0')
Epoch: [333][0/391]	Time 0.196 (0.196)	Data 0.178 (0.178)	Loss 2.3025 (2.3025) ([2.302]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [333][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.592)
Epoch: [333][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.756)
Epoch: [333][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (9.777)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0182, device='cuda:0')
Epoch: [334][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [334][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.056)
Epoch: [334][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.020)
Epoch: [334][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.967)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0071, device='cuda:0')
Epoch: [335][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [335][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (10.141)
Epoch: [335][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.016)
Epoch: [335][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.982)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [336][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [336][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.589)
Epoch: [336][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.187)
Epoch: [336][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (10.154)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0365, device='cuda:0')
Epoch: [337][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (3.906)
Epoch: [337][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.473)
Epoch: [337][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.230)
Epoch: [337][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.135)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0370, device='cuda:0')
Epoch: [338][0/391]	Time 0.230 (0.230)	Data 0.212 (0.212)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [338][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.063)
Epoch: [338][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.047)
Epoch: [338][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.868)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [339][0/391]	Time 0.206 (0.206)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [339][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.847)
Epoch: [339][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.892)
Epoch: [339][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.723)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0391, device='cuda:0')
Epoch: [340][0/391]	Time 0.245 (0.245)	Data 0.225 (0.225)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [340][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.684)
Epoch: [340][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.962)
Epoch: [340][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.847)
Test: [0/79]	Time 0.206 (0.206)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0232, device='cuda:0')
Epoch: [341][0/391]	Time 0.222 (0.222)	Data 0.203 (0.203)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [341][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.017)
Epoch: [341][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.950)
Epoch: [341][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.892)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0377, device='cuda:0')
Epoch: [342][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [342][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.295)
Epoch: [342][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.036)
Epoch: [342][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.840)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [343][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [343][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.210)
Epoch: [343][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.110)
Epoch: [343][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.071)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0266, device='cuda:0')
Epoch: [344][0/391]	Time 0.210 (0.210)	Data 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [344][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.381)
Epoch: [344][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.187)
Epoch: [344][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.146)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0122, device='cuda:0')
Epoch: [345][0/391]	Time 0.191 (0.191)	Data 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [345][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.862)
Epoch: [345][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.012)
Epoch: [345][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.938)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0365, device='cuda:0')
Epoch: [346][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [346][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.025)
Epoch: [346][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.935)
Epoch: [346][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.821)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0312, device='cuda:0')
Epoch: [347][0/391]	Time 0.201 (0.201)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [347][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.831)
Epoch: [347][200/391]	Time 0.016 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.849)
Epoch: [347][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (10.013)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0314, device='cuda:0')
Epoch: [348][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [348][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (10.288)
Epoch: [348][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.059)
Epoch: [348][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.889)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0433, device='cuda:0')
Epoch: [349][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [349][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.025)
Epoch: [349][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (10.040)
Epoch: [349][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.889)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0235, device='cuda:0')
Epoch: [350][0/391]	Time 0.197 (0.197)	Data 0.177 (0.177)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [350][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (10.203)
Epoch: [350][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.919)
Epoch: [350][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.897)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0642, device='cuda:0')
Epoch: [351][0/391]	Time 0.196 (0.196)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [351][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.295)
Epoch: [351][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (10.020)
Epoch: [351][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.936)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0175, device='cuda:0')
Epoch: [352][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [352][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.978)
Epoch: [352][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.028)
Epoch: [352][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.894)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0237, device='cuda:0')
Epoch: [353][0/391]	Time 0.195 (0.195)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [353][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.584)
Epoch: [353][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.632)
Epoch: [353][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.658)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [354][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [354][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.319)
Epoch: [354][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.032)
Epoch: [354][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.029)
Test: [0/79]	Time 0.204 (0.204)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0327, device='cuda:0')
Epoch: [355][0/391]	Time 0.242 (0.242)	Data 0.221 (0.221)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [355][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.630)
Epoch: [355][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.705)
Epoch: [355][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.819)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0169, device='cuda:0')
Epoch: [356][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [356][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.522)
Epoch: [356][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.565)
Epoch: [356][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.705)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0190, device='cuda:0')
Epoch: [357][0/391]	Time 0.221 (0.221)	Data 0.202 (0.202)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [357][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.677)
Epoch: [357][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.760)
Epoch: [357][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.702)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0142, device='cuda:0')
Epoch: [358][0/391]	Time 0.194 (0.194)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [358][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.561)
Epoch: [358][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.783)
Epoch: [358][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.772)
Test: [0/79]	Time 0.210 (0.210)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0100, device='cuda:0')
Epoch: [359][0/391]	Time 0.232 (0.232)	Data 0.212 (0.212)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [359][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.630)
Epoch: [359][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.752)
Epoch: [359][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.572)
Test: [0/79]	Time 0.200 (0.200)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0245, device='cuda:0')
Epoch: [360][0/391]	Time 0.208 (0.208)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [360][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (10.056)
Epoch: [360][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.876)
Epoch: [360][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.840)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0288, device='cuda:0')
Epoch: [361][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [361][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.025)
Epoch: [361][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.869)
Epoch: [361][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.793)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0158, device='cuda:0')
Epoch: [362][0/391]	Time 0.239 (0.239)	Data 0.218 (0.218)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [362][100/391]	Time 0.014 (0.016)	Data 0.000 (0.003)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.592)
Epoch: [362][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.760)
Epoch: [362][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.710)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0351, device='cuda:0')
Epoch: [363][0/391]	Time 0.203 (0.203)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [363][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.839)
Epoch: [363][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.989)
Epoch: [363][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.941)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0142, device='cuda:0')
Epoch: [364][0/391]	Time 0.236 (0.236)	Data 0.216 (0.216)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [364][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.141)
Epoch: [364][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.117)
Epoch: [364][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.034)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0285, device='cuda:0')
Epoch: [365][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [365][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.816)
Epoch: [365][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.880)
Epoch: [365][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.821)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0133, device='cuda:0')
Epoch: [366][0/391]	Time 0.255 (0.255)	Data 0.236 (0.236)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [366][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.257)
Epoch: [366][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.075)
Epoch: [366][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.972)
Test: [0/79]	Time 0.270 (0.270)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0304, device='cuda:0')
Epoch: [367][0/391]	Time 0.262 (0.262)	Data 0.244 (0.244)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [367][100/391]	Time 0.013 (0.016)	Data 0.000 (0.003)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.032)
Epoch: [367][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.888)
Epoch: [367][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.907)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0381, device='cuda:0')
Epoch: [368][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [368][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.909)
Epoch: [368][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.075)
Epoch: [368][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (9.868)
Test: [0/79]	Time 0.200 (0.200)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0103, device='cuda:0')
Epoch: [369][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [369][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.955)
Epoch: [369][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.635)
Epoch: [369][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.653)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0342, device='cuda:0')
Epoch: [370][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [370][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (9.870)
Epoch: [370][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.869)
Epoch: [370][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.782)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0204, device='cuda:0')
Epoch: [371][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [371][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.288)
Epoch: [371][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.238)
Epoch: [371][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.154)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0189, device='cuda:0')
Epoch: [372][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [372][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.056)
Epoch: [372][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 17.188 (10.102)
Epoch: [372][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.904)
Test: [0/79]	Time 0.168 (0.168)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0248, device='cuda:0')
Epoch: [373][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [373][100/391]	Time 0.016 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.692)
Epoch: [373][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 17.188 (9.876)
Epoch: [373][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.803)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0377, device='cuda:0')
Epoch: [374][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [374][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.646)
Epoch: [374][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.585)
Epoch: [374][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 17.969 (9.598)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0201, device='cuda:0')
Epoch: [375][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [375][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.195)
Epoch: [375][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.958)
Epoch: [375][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.884)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0234, device='cuda:0')
Epoch: [376][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [376][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.839)
Epoch: [376][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.958)
Epoch: [376][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.886)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0366, device='cuda:0')
Epoch: [377][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [377][100/391]	Time 0.013 (0.014)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.133)
Epoch: [377][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.032)
Epoch: [377][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.988)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0235, device='cuda:0')
Epoch: [378][0/391]	Time 0.216 (0.216)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [378][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.723)
Epoch: [378][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.838)
Epoch: [378][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.853)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0365, device='cuda:0')
Epoch: [379][0/391]	Time 0.204 (0.204)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [379][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.203)
Epoch: [379][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.838)
Epoch: [379][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.751)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0231, device='cuda:0')
Epoch: [380][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [380][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.909)
Epoch: [380][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.997)
Epoch: [380][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.037)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0318, device='cuda:0')
Epoch: [381][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [381][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.522)
Epoch: [381][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.659)
Epoch: [381][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.666)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0239, device='cuda:0')
Epoch: [382][0/391]	Time 0.210 (0.210)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [382][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.210)
Epoch: [382][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.079)
Epoch: [382][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.065)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0205, device='cuda:0')
Epoch: [383][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [383][100/391]	Time 0.015 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.265)
Epoch: [383][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.125 (9.935)
Epoch: [383][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.962)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0194, device='cuda:0')
Epoch: [384][0/391]	Time 0.201 (0.201)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [384][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.785)
Epoch: [384][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.841)
Epoch: [384][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.764)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [385][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [385][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.017)
Epoch: [385][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.950)
Epoch: [385][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.785)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0264, device='cuda:0')
Epoch: [386][0/391]	Time 0.240 (0.240)	Data 0.220 (0.220)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [386][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.087)
Epoch: [386][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.977)
Epoch: [386][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.959)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0353, device='cuda:0')
Epoch: [387][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [387][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.947)
Epoch: [387][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.907)
Epoch: [387][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.863)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0114, device='cuda:0')
Epoch: [388][0/391]	Time 0.246 (0.246)	Data 0.225 (0.225)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [388][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.777)
Epoch: [388][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.752)
Epoch: [388][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.850)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0309, device='cuda:0')
Epoch: [389][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [389][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.032)
Epoch: [389][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.849)
Epoch: [389][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.917)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0234, device='cuda:0')
Epoch: [390][0/391]	Time 0.199 (0.199)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [390][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.932)
Epoch: [390][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.024)
Epoch: [390][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (10.003)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0182, device='cuda:0')
Epoch: [391][0/391]	Time 0.193 (0.193)	Data 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [391][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.862)
Epoch: [391][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.020)
Epoch: [391][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.881)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0194, device='cuda:0')
Epoch: [392][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [392][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.118)
Epoch: [392][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.822)
Epoch: [392][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.806)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0174, device='cuda:0')
Epoch: [393][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [393][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.661)
Epoch: [393][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.904)
Epoch: [393][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.021)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [394][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [394][100/391]	Time 0.015 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.870)
Epoch: [394][200/391]	Time 0.012 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.795)
Epoch: [394][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.749)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0236, device='cuda:0')
Epoch: [395][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [395][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.986)
Epoch: [395][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.106)
Epoch: [395][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.930)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0486, device='cuda:0')
Epoch: [396][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (3.906)
Epoch: [396][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.994)
Epoch: [396][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.841)
Epoch: [396][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.754)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0321, device='cuda:0')
Epoch: [397][0/391]	Time 0.208 (0.208)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [397][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.847)
Epoch: [397][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.028)
Epoch: [397][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.972)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0246, device='cuda:0')
Epoch: [398][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [398][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.893)
Epoch: [398][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.783)
Epoch: [398][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.567)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0114, device='cuda:0')
Epoch: [399][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [399][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.800)
Epoch: [399][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.838)
Epoch: [399][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 7.031 (9.741)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0304, device='cuda:0')
Epoch: [400][0/391]	Time 0.214 (0.214)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [400][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.009)
Epoch: [400][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.172)
Epoch: [400][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.013)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0169, device='cuda:0')
Epoch: [401][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [401][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.048)
Epoch: [401][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.020)
Epoch: [401][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.928)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0161, device='cuda:0')
Epoch: [402][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [402][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.367)
Epoch: [402][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.632)
Epoch: [402][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.705)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0168, device='cuda:0')
Epoch: [403][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [403][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.638)
Epoch: [403][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.841)
Epoch: [403][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.780)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0123, device='cuda:0')
Epoch: [404][0/391]	Time 0.186 (0.186)	Data 0.166 (0.166)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [404][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.195)
Epoch: [404][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.051)
Epoch: [404][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.920)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0359, device='cuda:0')
Epoch: [405][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [405][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.164)
Epoch: [405][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.713)
Epoch: [405][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.692)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0257, device='cuda:0')
Epoch: [406][0/391]	Time 0.218 (0.218)	Data 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [406][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.295)
Epoch: [406][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.047)
Epoch: [406][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.897)
Test: [0/79]	Time 0.218 (0.218)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [407][0/391]	Time 0.244 (0.244)	Data 0.225 (0.225)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [407][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.638)
Epoch: [407][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.904)
Epoch: [407][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.795)
Test: [0/79]	Time 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [408][0/391]	Time 0.216 (0.216)	Data 0.196 (0.196)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [408][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.589)
Epoch: [408][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.211)
Epoch: [408][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.193)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0308, device='cuda:0')
Epoch: [409][0/391]	Time 0.212 (0.212)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [409][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.785)
Epoch: [409][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.869)
Epoch: [409][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.886)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0424, device='cuda:0')
Epoch: [410][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [410][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.599)
Epoch: [410][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.768)
Epoch: [410][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.780)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0446, device='cuda:0')
Epoch: [411][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [411][100/391]	Time 0.013 (0.014)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (9.839)
Epoch: [411][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.055)
Epoch: [411][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.946)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0207, device='cuda:0')
Epoch: [412][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [412][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.241)
Epoch: [412][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.001)
Epoch: [412][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.772)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0482, device='cuda:0')
Epoch: [413][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [413][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.592)
Epoch: [413][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.705)
Epoch: [413][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.668)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0305, device='cuda:0')
Epoch: [414][0/391]	Time 0.201 (0.201)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [414][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.553)
Epoch: [414][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.612)
Epoch: [414][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.640)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0275, device='cuda:0')
Epoch: [415][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [415][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.909)
Epoch: [415][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.005)
Epoch: [415][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.897)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0255, device='cuda:0')
Epoch: [416][0/391]	Time 0.192 (0.192)	Data 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [416][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.543)
Epoch: [416][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.152)
Epoch: [416][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.029)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0203, device='cuda:0')
Epoch: [417][0/391]	Time 0.193 (0.193)	Data 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [417][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.118)
Epoch: [417][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.927)
Epoch: [417][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.001)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0414, device='cuda:0')
Epoch: [418][0/391]	Time 0.204 (0.204)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [418][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.265)
Epoch: [418][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.067)
Epoch: [418][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.860)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0281, device='cuda:0')
Epoch: [419][0/391]	Time 0.204 (0.204)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 16.406 (16.406)
Epoch: [419][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [419][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.791)
Epoch: [419][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.751)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0250, device='cuda:0')
Epoch: [420][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [420][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.017)
Epoch: [420][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.958)
Epoch: [420][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.928)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0099, device='cuda:0')
Epoch: [421][0/391]	Time 0.215 (0.215)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [421][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (10.241)
Epoch: [421][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.156)
Epoch: [421][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.899)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0313, device='cuda:0')
Epoch: [422][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [422][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.940)
Epoch: [422][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.822)
Epoch: [422][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.741)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0131, device='cuda:0')
Epoch: [423][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [423][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.824)
Epoch: [423][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.655)
Epoch: [423][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.702)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0257, device='cuda:0')
Epoch: [424][0/391]	Time 0.222 (0.222)	Data 0.202 (0.202)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [424][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.638)
Epoch: [424][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.464)
Epoch: [424][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.609)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0248, device='cuda:0')
Epoch: [425][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [425][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.094)
Epoch: [425][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.989)
Epoch: [425][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.969)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0194, device='cuda:0')
Epoch: [426][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [426][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.886)
Epoch: [426][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 7.031 (9.806)
Epoch: [426][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.837)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0277, device='cuda:0')
Epoch: [427][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [427][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.684)
Epoch: [427][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.915)
Epoch: [427][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.899)
Test: [0/79]	Time 0.226 (0.226)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [428][0/391]	Time 0.213 (0.213)	Data 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [428][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.094)
Epoch: [428][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.016)
Epoch: [428][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.897)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0350, device='cuda:0')
Epoch: [429][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [429][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.102)
Epoch: [429][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.024)
Epoch: [429][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.744)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0531, device='cuda:0')
Epoch: [430][0/391]	Time 0.209 (0.209)	Data 0.188 (0.188)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [430][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.522)
Epoch: [430][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.775)
Epoch: [430][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.785)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [431][0/391]	Time 0.215 (0.215)	Data 0.197 (0.197)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [431][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.218)
Epoch: [431][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.896)
Epoch: [431][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.975)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0397, device='cuda:0')
Epoch: [432][0/391]	Time 0.244 (0.244)	Data 0.225 (0.225)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [432][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.661)
Epoch: [432][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.939)
Epoch: [432][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.853)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [433][0/391]	Time 0.222 (0.222)	Data 0.201 (0.201)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [433][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.715)
Epoch: [433][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.791)
Epoch: [433][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.746)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0251, device='cuda:0')
Epoch: [434][0/391]	Time 0.207 (0.207)	Data 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [434][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.512)
Epoch: [434][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.316)
Epoch: [434][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.180)
Test: [0/79]	Time 0.207 (0.207)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0485, device='cuda:0')
Epoch: [435][0/391]	Time 0.213 (0.213)	Data 0.194 (0.194)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [435][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.071)
Epoch: [435][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.005)
Epoch: [435][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.949)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0184, device='cuda:0')
Epoch: [436][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [436][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.133)
Epoch: [436][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.160)
Epoch: [436][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.995)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0261, device='cuda:0')
Epoch: [437][0/391]	Time 0.241 (0.241)	Data 0.222 (0.222)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [437][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.048)
Epoch: [437][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.845)
Epoch: [437][300/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.780)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0393, device='cuda:0')
Epoch: [438][0/391]	Time 0.198 (0.198)	Data 0.178 (0.178)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [438][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.156)
Epoch: [438][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.024)
Epoch: [438][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.975)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0118, device='cuda:0')
Epoch: [439][0/391]	Time 0.204 (0.204)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [439][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.971)
Epoch: [439][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.775)
Epoch: [439][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.741)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0193, device='cuda:0')
Epoch: [440][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [440][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.040)
Epoch: [440][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.059)
Epoch: [440][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.011)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0258, device='cuda:0')
Epoch: [441][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [441][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.971)
Epoch: [441][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.717)
Epoch: [441][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.731)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0355, device='cuda:0')
Epoch: [442][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [442][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.599)
Epoch: [442][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.935)
Epoch: [442][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.871)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0152, device='cuda:0')
Epoch: [443][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [443][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.319)
Epoch: [443][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.164)
Epoch: [443][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.951)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0317, device='cuda:0')
Epoch: [444][0/391]	Time 0.205 (0.205)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [444][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (10.179)
Epoch: [444][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.024)
Epoch: [444][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.026)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0375, device='cuda:0')
Epoch: [445][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [445][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.971)
Epoch: [445][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.059)
Epoch: [445][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 17.969 (9.964)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0505, device='cuda:0')
Epoch: [446][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [446][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.994)
Epoch: [446][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.830)
Epoch: [446][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.923)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [447][0/391]	Time 0.205 (0.205)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [447][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.365)
Epoch: [447][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.125)
Epoch: [447][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.925)
Test: [0/79]	Time 0.209 (0.209)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [448][0/391]	Time 0.207 (0.207)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [448][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.793)
Epoch: [448][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.713)
Epoch: [448][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.588)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0435, device='cuda:0')
Epoch: [449][0/391]	Time 0.198 (0.198)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [449][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.847)
Epoch: [449][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.779)
Epoch: [449][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.866)
Test: [0/79]	Time 0.231 (0.231)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [450][0/391]	Time 0.214 (0.214)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [450][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.669)
Epoch: [450][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.705)
Epoch: [450][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.572)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0413, device='cuda:0')
Epoch: [451][0/391]	Time 0.195 (0.195)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [451][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.615)
Epoch: [451][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.565)
Epoch: [451][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.577)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0366, device='cuda:0')
Epoch: [452][0/391]	Time 0.202 (0.202)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [452][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.017)
Epoch: [452][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.927)
Epoch: [452][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.884)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [453][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [453][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.746)
Epoch: [453][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.814)
Epoch: [453][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.777)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0162, device='cuda:0')
Epoch: [454][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [454][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.226)
Epoch: [454][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.001)
Epoch: [454][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.985)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0183, device='cuda:0')
Epoch: [455][0/391]	Time 0.195 (0.195)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [455][100/391]	Time 0.015 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.630)
Epoch: [455][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.857)
Epoch: [455][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.884)
Test: [0/79]	Time 0.168 (0.168)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0211, device='cuda:0')
Epoch: [456][0/391]	Time 0.194 (0.194)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [456][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.630)
Epoch: [456][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.744)
Epoch: [456][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.723)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0426, device='cuda:0')
Epoch: [457][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [457][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.831)
Epoch: [457][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.005)
Epoch: [457][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.795)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [458][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [458][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.800)
Epoch: [458][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.736)
Epoch: [458][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 17.969 (9.650)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0194, device='cuda:0')
Epoch: [459][0/391]	Time 0.211 (0.211)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [459][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.110)
Epoch: [459][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.838)
Epoch: [459][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.782)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0368, device='cuda:0')
Epoch: [460][0/391]	Time 0.204 (0.204)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [460][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.257)
Epoch: [460][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.036)
Epoch: [460][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.967)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0190, device='cuda:0')
Epoch: [461][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [461][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.971)
Epoch: [461][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.888)
Epoch: [461][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.821)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0200, device='cuda:0')
Epoch: [462][0/391]	Time 0.200 (0.200)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [462][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.723)
Epoch: [462][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.869)
Epoch: [462][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.699)
Test: [0/79]	Time 0.202 (0.202)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0074, device='cuda:0')
Epoch: [463][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [463][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.638)
Epoch: [463][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.880)
Epoch: [463][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.876)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0264, device='cuda:0')
Epoch: [464][0/391]	Time 0.241 (0.241)	Data 0.223 (0.223)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [464][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.179)
Epoch: [464][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.110)
Epoch: [464][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.029)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [465][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [465][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.241)
Epoch: [465][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.133)
Epoch: [465][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.112)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0294, device='cuda:0')
Epoch: [466][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [466][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.924)
Epoch: [466][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.775)
Epoch: [466][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.834)
Test: [0/79]	Time 0.207 (0.207)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0168, device='cuda:0')
Epoch: [467][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [467][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.800)
Epoch: [467][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.974)
Epoch: [467][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.873)
Test: [0/79]	Time 0.209 (0.209)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0251, device='cuda:0')
Epoch: [468][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [468][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 17.969 (9.715)
Epoch: [468][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.635)
Epoch: [468][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.705)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [469][0/391]	Time 0.205 (0.205)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [469][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.017)
Epoch: [469][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.857)
Epoch: [469][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.749)
Test: [0/79]	Time 0.196 (0.196)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0333, device='cuda:0')
Epoch: [470][0/391]	Time 0.206 (0.206)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [470][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.723)
Epoch: [470][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.849)
Epoch: [470][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.694)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0186, device='cuda:0')
Epoch: [471][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [471][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.839)
Epoch: [471][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.830)
Epoch: [471][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.772)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0181, device='cuda:0')
Epoch: [472][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [472][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.032)
Epoch: [472][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.869)
Epoch: [472][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.723)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0179, device='cuda:0')
Epoch: [473][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [473][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.893)
Epoch: [473][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.024)
Epoch: [473][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.962)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0288, device='cuda:0')
Epoch: [474][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [474][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.002)
Epoch: [474][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.822)
Epoch: [474][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.775)
Test: [0/79]	Time 0.208 (0.208)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0328, device='cuda:0')
Epoch: [475][0/391]	Time 0.211 (0.211)	Data 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [475][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.210)
Epoch: [475][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.966)
Epoch: [475][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.827)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0300, device='cuda:0')
Epoch: [476][0/391]	Time 0.202 (0.202)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [476][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.947)
Epoch: [476][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.110)
Epoch: [476][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.990)
Test: [0/79]	Time 0.167 (0.167)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0254, device='cuda:0')
Epoch: [477][0/391]	Time 0.198 (0.198)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [477][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.963)
Epoch: [477][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.783)
Epoch: [477][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.772)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0103, device='cuda:0')
Epoch: [478][0/391]	Time 0.217 (0.217)	Data 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [478][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.808)
Epoch: [478][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.791)
Epoch: [478][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.772)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0093, device='cuda:0')
Epoch: [479][0/391]	Time 0.204 (0.204)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [479][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.916)
Epoch: [479][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.736)
Epoch: [479][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.658)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0684, device='cuda:0')
Epoch: [480][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [480][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.398)
Epoch: [480][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.519)
Epoch: [480][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.663)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0377, device='cuda:0')
Epoch: [481][0/391]	Time 0.217 (0.217)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [481][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.087)
Epoch: [481][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (9.997)
Epoch: [481][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.912)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0205, device='cuda:0')
Epoch: [482][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [482][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (10.265)
Epoch: [482][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.962)
Epoch: [482][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.886)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0323, device='cuda:0')
Epoch: [483][0/391]	Time 0.215 (0.215)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [483][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (10.118)
Epoch: [483][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.051)
Epoch: [483][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.819)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0307, device='cuda:0')
Epoch: [484][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [484][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.855)
Epoch: [484][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.834)
Epoch: [484][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.832)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0353, device='cuda:0')
Epoch: [485][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 17.188 (17.188)
Epoch: [485][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (10.164)
Epoch: [485][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.803)
Epoch: [485][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.775)
Test: [0/79]	Time 0.206 (0.206)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0145, device='cuda:0')
Epoch: [486][0/391]	Time 0.234 (0.234)	Data 0.215 (0.215)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [486][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.971)
Epoch: [486][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.907)
Epoch: [486][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (10.003)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0611, device='cuda:0')
Epoch: [487][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [487][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.947)
Epoch: [487][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.884)
Epoch: [487][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.671)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0210, device='cuda:0')
Epoch: [488][0/391]	Time 0.234 (0.234)	Data 0.213 (0.213)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [488][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.847)
Epoch: [488][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.729)
Epoch: [488][300/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.884)
Test: [0/79]	Time 0.204 (0.204)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0238, device='cuda:0')
Epoch: [489][0/391]	Time 0.217 (0.217)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [489][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.638)
Epoch: [489][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.721)
Epoch: [489][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.850)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0390, device='cuda:0')
Epoch: [490][0/391]	Time 0.196 (0.196)	Data 0.176 (0.176)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [490][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.785)
Epoch: [490][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.063)
Epoch: [490][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.029)
Test: [0/79]	Time 0.210 (0.210)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0220, device='cuda:0')
Epoch: [491][0/391]	Time 0.203 (0.203)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [491][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.032)
Epoch: [491][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.942)
Epoch: [491][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.814)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0112, device='cuda:0')
Epoch: [492][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [492][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.156)
Epoch: [492][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.977)
Epoch: [492][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.829)
Test: [0/79]	Time 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [493][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [493][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.048)
Epoch: [493][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.915)
Epoch: [493][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 19.531 (9.777)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0317, device='cuda:0')
Epoch: [494][0/391]	Time 0.192 (0.192)	Data 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [494][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.342)
Epoch: [494][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.032)
Epoch: [494][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.026)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0218, device='cuda:0')
Epoch: [495][0/391]	Time 0.195 (0.195)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [495][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.241)
Epoch: [495][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.977)
Epoch: [495][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.910)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0222, device='cuda:0')
Epoch: [496][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [496][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.785)
Epoch: [496][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.024)
Epoch: [496][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.964)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0129, device='cuda:0')
Epoch: [497][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [497][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.762)
Epoch: [497][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.810)
Epoch: [497][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.629)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [498][0/391]	Time 0.216 (0.216)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [498][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.692)
Epoch: [498][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (9.838)
Epoch: [498][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.816)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0261, device='cuda:0')
Epoch: [499][0/391]	Time 0.218 (0.218)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [499][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.040)
Epoch: [499][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.888)
Epoch: [499][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.777)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000

 Elapsed time for training  1:14:54.715868

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875]
Total parameter pruned: 13613689.0 (unstructured) 13608729 (structured)
Test: [0/79]	Time 0.209 (0.209)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
Best accuracy:  10.0
