V0.0.1-_resnet18_Cifar10_lr0.1_l1.0_a0.1_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5487061738967896, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.3523094356060028, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21628862619400024, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.18689115345478058, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.14327552914619446, Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.12737567722797394, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11029236763715744, Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.2971794605255127, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09336047619581223, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11640232801437378, Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.1180206686258316, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.09721729904413223, Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.1253737211227417, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.11488018184900284, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08681126683950424, Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08893948793411255, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0640970766544342, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.08375772833824158, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06052006036043167, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.017058581113815308, Linear(in_features=512, out_features=100, bias=True): 0.39575180411338806}
current lr 1.00000e-01
Grad=  tensor(46.4286, device='cuda:0')
Epoch: [0][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 5.7286 (5.7286) ([4.526]+[1.202])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 3.0011 (3.5054) ([2.061]+[0.940])	Prec@1 18.750 (16.971)
Epoch: [0][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 2.6322 (3.1396) ([1.826]+[0.806])	Prec@1 28.125 (22.753)
Epoch: [0][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 2.3751 (2.9443) ([1.694]+[0.682])	Prec@1 31.250 (25.914)
Test: [0/79]	Time 0.124 (0.124)	Loss 2.1027 (2.1027) ([1.524]+[0.579])	Prec@1 50.000 (50.000)
 * Prec@1 39.760
current lr 1.00000e-01
Grad=  tensor(0.9593, device='cuda:0')
Epoch: [1][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 2.2373 (2.2373) ([1.659]+[0.579])	Prec@1 35.938 (35.938)
Epoch: [1][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 2.0817 (2.1698) ([1.593]+[0.488])	Prec@1 42.188 (38.923)
Epoch: [1][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.9748 (2.0928) ([1.552]+[0.423])	Prec@1 44.531 (40.501)
Epoch: [1][300/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 1.6965 (2.0232) ([1.309]+[0.387])	Prec@1 53.125 (42.598)
Test: [0/79]	Time 0.126 (0.126)	Loss 1.6292 (1.6292) ([1.262]+[0.368])	Prec@1 54.688 (54.688)
 * Prec@1 50.390
current lr 1.00000e-01
Grad=  tensor(1.2615, device='cuda:0')
Epoch: [2][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 1.6822 (1.6822) ([1.315]+[0.368])	Prec@1 46.094 (46.094)
Epoch: [2][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.6636 (1.7097) ([1.305]+[0.358])	Prec@1 55.469 (50.928)
Epoch: [2][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.6413 (1.6755) ([1.285]+[0.357])	Prec@1 53.125 (52.033)
Epoch: [2][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.4415 (1.6326) ([1.088]+[0.353])	Prec@1 62.500 (53.649)
Test: [0/79]	Time 0.130 (0.130)	Loss 1.4525 (1.4525) ([1.101]+[0.351])	Prec@1 58.594 (58.594)
 * Prec@1 59.470
current lr 1.00000e-01
Grad=  tensor(1.6055, device='cuda:0')
Epoch: [3][0/391]	Time 0.171 (0.171)	Data 0.128 (0.128)	Loss 1.3974 (1.3974) ([1.046]+[0.351])	Prec@1 64.062 (64.062)
Epoch: [3][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.2664 (1.4106) ([0.917]+[0.349])	Prec@1 67.969 (62.252)
Epoch: [3][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.3883 (1.3819) ([1.042]+[0.346])	Prec@1 60.938 (63.161)
Epoch: [3][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.2528 (1.3606) ([0.907]+[0.346])	Prec@1 68.750 (64.112)
Test: [0/79]	Time 0.129 (0.129)	Loss 1.1385 (1.1385) ([0.796]+[0.343])	Prec@1 73.438 (73.438)
 * Prec@1 66.080
current lr 1.00000e-01
Grad=  tensor(1.9733, device='cuda:0')
Epoch: [4][0/391]	Time 0.171 (0.171)	Data 0.128 (0.128)	Loss 1.3557 (1.3557) ([1.013]+[0.343])	Prec@1 59.375 (59.375)
Epoch: [4][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.2835 (1.2232) ([0.944]+[0.339])	Prec@1 66.406 (68.943)
Epoch: [4][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.2357 (1.2107) ([0.894]+[0.342])	Prec@1 69.531 (69.516)
Epoch: [4][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9717 (1.2006) ([0.630]+[0.342])	Prec@1 80.469 (69.972)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.3377 (1.3377) ([0.998]+[0.339])	Prec@1 68.750 (68.750)
 * Prec@1 66.630
current lr 1.00000e-01
Grad=  tensor(2.2411, device='cuda:0')
Epoch: [5][0/391]	Time 0.175 (0.175)	Data 0.132 (0.132)	Loss 1.1852 (1.1852) ([0.846]+[0.339])	Prec@1 65.625 (65.625)
Epoch: [5][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.0870 (1.1104) ([0.745]+[0.342])	Prec@1 72.656 (73.368)
Epoch: [5][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.3601 (1.0835) ([1.019]+[0.341])	Prec@1 66.406 (74.510)
Epoch: [5][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 1.0761 (1.0835) ([0.732]+[0.344])	Prec@1 77.344 (74.577)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.0368 (1.0368) ([0.698]+[0.339])	Prec@1 74.219 (74.219)
 * Prec@1 73.790
current lr 1.00000e-01
Grad=  tensor(1.8717, device='cuda:0')
Epoch: [6][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 1.0103 (1.0103) ([0.671]+[0.339])	Prec@1 77.344 (77.344)
Epoch: [6][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9019 (1.0087) ([0.562]+[0.340])	Prec@1 81.250 (76.864)
Epoch: [6][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.1105 (1.0081) ([0.770]+[0.341])	Prec@1 77.344 (76.870)
Epoch: [6][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8502 (1.0075) ([0.513]+[0.337])	Prec@1 83.594 (77.040)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.1385 (1.1385) ([0.801]+[0.338])	Prec@1 70.312 (70.312)
 * Prec@1 71.250
current lr 1.00000e-01
Grad=  tensor(1.3298, device='cuda:0')
Epoch: [7][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.8187 (0.8187) ([0.481]+[0.338])	Prec@1 84.375 (84.375)
Epoch: [7][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9544 (0.9593) ([0.617]+[0.338])	Prec@1 79.688 (78.620)
Epoch: [7][200/391]	Time 0.038 (0.037)	Data 0.000 (0.001)	Loss 0.8676 (0.9621) ([0.533]+[0.335])	Prec@1 82.031 (78.498)
Epoch: [7][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9093 (0.9648) ([0.573]+[0.336])	Prec@1 81.250 (78.398)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8781 (0.8781) ([0.544]+[0.334])	Prec@1 79.688 (79.688)
 * Prec@1 76.630
current lr 1.00000e-01
Grad=  tensor(1.7689, device='cuda:0')
Epoch: [8][0/391]	Time 0.173 (0.173)	Data 0.130 (0.130)	Loss 0.8931 (0.8931) ([0.559]+[0.334])	Prec@1 78.906 (78.906)
Epoch: [8][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.0026 (0.9327) ([0.669]+[0.334])	Prec@1 73.438 (79.649)
Epoch: [8][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8372 (0.9348) ([0.501]+[0.336])	Prec@1 85.156 (79.548)
Epoch: [8][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9020 (0.9330) ([0.568]+[0.334])	Prec@1 78.125 (79.560)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.0310 (1.0310) ([0.700]+[0.331])	Prec@1 75.000 (75.000)
 * Prec@1 74.870
current lr 1.00000e-01
Grad=  tensor(2.3682, device='cuda:0')
Epoch: [9][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.8816 (0.8816) ([0.550]+[0.331])	Prec@1 85.156 (85.156)
Epoch: [9][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8981 (0.9215) ([0.565]+[0.334])	Prec@1 77.344 (79.417)
Epoch: [9][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8183 (0.9135) ([0.486]+[0.333])	Prec@1 83.594 (80.053)
Epoch: [9][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9495 (0.9140) ([0.619]+[0.330])	Prec@1 78.906 (80.061)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.9205 (0.9205) ([0.591]+[0.329])	Prec@1 80.469 (80.469)
 * Prec@1 76.150
current lr 1.00000e-01
Grad=  tensor(2.5199, device='cuda:0')
Epoch: [10][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.9819 (0.9819) ([0.652]+[0.329])	Prec@1 75.781 (75.781)
Epoch: [10][100/391]	Time 0.037 (0.039)	Data 0.000 (0.001)	Loss 0.8264 (0.8949) ([0.497]+[0.330])	Prec@1 81.250 (80.941)
Epoch: [10][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 1.0276 (0.8861) ([0.701]+[0.326])	Prec@1 77.344 (81.028)
Epoch: [10][300/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9039 (0.8855) ([0.577]+[0.327])	Prec@1 81.250 (81.019)
Test: [0/79]	Time 0.126 (0.126)	Loss 1.0486 (1.0486) ([0.723]+[0.326])	Prec@1 77.344 (77.344)
 * Prec@1 71.640
current lr 1.00000e-01
Grad=  tensor(2.1049, device='cuda:0')
Epoch: [11][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.8712 (0.8712) ([0.545]+[0.326])	Prec@1 77.344 (77.344)
Epoch: [11][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7662 (0.8544) ([0.442]+[0.324])	Prec@1 85.156 (82.008)
Epoch: [11][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7690 (0.8630) ([0.446]+[0.323])	Prec@1 85.156 (81.767)
Epoch: [11][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9113 (0.8639) ([0.588]+[0.324])	Prec@1 81.250 (81.741)
Test: [0/79]	Time 0.129 (0.129)	Loss 1.0335 (1.0335) ([0.711]+[0.322])	Prec@1 75.781 (75.781)
 * Prec@1 73.260
current lr 1.00000e-01
Grad=  tensor(1.7502, device='cuda:0')
Epoch: [12][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.8331 (0.8331) ([0.511]+[0.322])	Prec@1 83.594 (83.594)
Epoch: [12][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7953 (0.8306) ([0.473]+[0.323])	Prec@1 84.375 (83.045)
Epoch: [12][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8582 (0.8435) ([0.535]+[0.323])	Prec@1 82.812 (82.416)
Epoch: [12][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7664 (0.8439) ([0.446]+[0.320])	Prec@1 87.500 (82.371)
Test: [0/79]	Time 0.130 (0.130)	Loss 1.1007 (1.1007) ([0.781]+[0.319])	Prec@1 72.656 (72.656)
 * Prec@1 73.340
current lr 1.00000e-01
Grad=  tensor(1.7062, device='cuda:0')
Epoch: [13][0/391]	Time 0.171 (0.171)	Data 0.127 (0.127)	Loss 0.7341 (0.7341) ([0.415]+[0.319])	Prec@1 89.062 (89.062)
Epoch: [13][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8594 (0.8250) ([0.538]+[0.321])	Prec@1 82.812 (82.426)
Epoch: [13][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8922 (0.8297) ([0.574]+[0.318])	Prec@1 78.906 (82.412)
Epoch: [13][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9051 (0.8340) ([0.587]+[0.318])	Prec@1 84.375 (82.304)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.8470 (0.8470) ([0.528]+[0.319])	Prec@1 81.250 (81.250)
 * Prec@1 79.180
current lr 1.00000e-01
Grad=  tensor(1.9836, device='cuda:0')
Epoch: [14][0/391]	Time 0.174 (0.174)	Data 0.132 (0.132)	Loss 0.7455 (0.7455) ([0.426]+[0.319])	Prec@1 87.500 (87.500)
Epoch: [14][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7968 (0.8186) ([0.478]+[0.319])	Prec@1 82.812 (82.929)
Epoch: [14][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8435 (0.8254) ([0.525]+[0.318])	Prec@1 82.812 (82.665)
Epoch: [14][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8777 (0.8251) ([0.561]+[0.317])	Prec@1 84.375 (82.592)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.0299 (1.0299) ([0.714]+[0.316])	Prec@1 75.000 (75.000)
 * Prec@1 77.660
current lr 1.00000e-01
Grad=  tensor(2.2664, device='cuda:0')
Epoch: [15][0/391]	Time 0.174 (0.174)	Data 0.125 (0.125)	Loss 0.9617 (0.9617) ([0.645]+[0.316])	Prec@1 78.906 (78.906)
Epoch: [15][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9061 (0.8215) ([0.590]+[0.316])	Prec@1 80.469 (82.604)
Epoch: [15][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7639 (0.8254) ([0.447]+[0.317])	Prec@1 85.156 (82.634)
Epoch: [15][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7569 (0.8242) ([0.440]+[0.317])	Prec@1 86.719 (82.787)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.1436 (1.1436) ([0.829]+[0.315])	Prec@1 75.000 (75.000)
 * Prec@1 73.710
current lr 1.00000e-01
Grad=  tensor(1.6829, device='cuda:0')
Epoch: [16][0/391]	Time 0.171 (0.171)	Data 0.127 (0.127)	Loss 0.6956 (0.6956) ([0.381]+[0.315])	Prec@1 85.156 (85.156)
Epoch: [16][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8249 (0.8045) ([0.509]+[0.316])	Prec@1 79.688 (83.323)
Epoch: [16][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9040 (0.8054) ([0.589]+[0.315])	Prec@1 82.031 (83.399)
Epoch: [16][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9766 (0.8066) ([0.661]+[0.315])	Prec@1 75.781 (83.282)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9936 (0.9936) ([0.680]+[0.314])	Prec@1 76.562 (76.562)
 * Prec@1 76.640
current lr 1.00000e-01
Grad=  tensor(2.9509, device='cuda:0')
Epoch: [17][0/391]	Time 0.170 (0.170)	Data 0.121 (0.121)	Loss 0.8825 (0.8825) ([0.568]+[0.314])	Prec@1 81.250 (81.250)
Epoch: [17][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8783 (0.7981) ([0.564]+[0.314])	Prec@1 80.469 (83.238)
Epoch: [17][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7541 (0.8054) ([0.439]+[0.315])	Prec@1 86.719 (83.213)
Epoch: [17][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7466 (0.8068) ([0.433]+[0.314])	Prec@1 84.375 (83.269)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.1667 (1.1667) ([0.855]+[0.312])	Prec@1 71.875 (71.875)
 * Prec@1 76.750
current lr 1.00000e-01
Grad=  tensor(2.5004, device='cuda:0')
Epoch: [18][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.7138 (0.7138) ([0.402]+[0.312])	Prec@1 83.594 (83.594)
Epoch: [18][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7989 (0.7840) ([0.486]+[0.313])	Prec@1 81.250 (83.718)
Epoch: [18][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7153 (0.7837) ([0.403]+[0.312])	Prec@1 85.156 (83.951)
Epoch: [18][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7681 (0.7882) ([0.456]+[0.312])	Prec@1 85.938 (83.814)
Test: [0/79]	Time 0.125 (0.125)	Loss 1.0344 (1.0344) ([0.722]+[0.313])	Prec@1 76.562 (76.562)
 * Prec@1 79.390
current lr 1.00000e-01
Grad=  tensor(1.6064, device='cuda:0')
Epoch: [19][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.7231 (0.7231) ([0.411]+[0.313])	Prec@1 87.500 (87.500)
Epoch: [19][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7976 (0.7678) ([0.485]+[0.313])	Prec@1 80.469 (84.414)
Epoch: [19][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8339 (0.7756) ([0.521]+[0.313])	Prec@1 82.812 (84.235)
Epoch: [19][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7814 (0.7831) ([0.469]+[0.313])	Prec@1 83.594 (84.022)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9818 (0.9818) ([0.669]+[0.313])	Prec@1 79.688 (79.688)
 * Prec@1 76.890
current lr 1.00000e-01
Grad=  tensor(1.7673, device='cuda:0')
Epoch: [20][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.6734 (0.6734) ([0.360]+[0.313])	Prec@1 90.625 (90.625)
Epoch: [20][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7474 (0.7727) ([0.435]+[0.313])	Prec@1 84.375 (84.011)
Epoch: [20][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8233 (0.7816) ([0.510]+[0.314])	Prec@1 81.250 (83.854)
Epoch: [20][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7135 (0.7841) ([0.400]+[0.314])	Prec@1 85.156 (83.762)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.9508 (0.9508) ([0.639]+[0.312])	Prec@1 79.688 (79.688)
 * Prec@1 77.590
current lr 1.00000e-01
Grad=  tensor(1.9127, device='cuda:0')
Epoch: [21][0/391]	Time 0.173 (0.173)	Data 0.130 (0.130)	Loss 0.7826 (0.7826) ([0.471]+[0.312])	Prec@1 87.500 (87.500)
Epoch: [21][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6423 (0.7634) ([0.331]+[0.312])	Prec@1 87.500 (84.677)
Epoch: [21][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7552 (0.7663) ([0.444]+[0.312])	Prec@1 81.250 (84.624)
Epoch: [21][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7548 (0.7754) ([0.445]+[0.310])	Prec@1 87.500 (84.313)
Test: [0/79]	Time 0.135 (0.135)	Loss 0.9164 (0.9164) ([0.606]+[0.310])	Prec@1 76.562 (76.562)
 * Prec@1 76.890
current lr 1.00000e-01
Grad=  tensor(2.2187, device='cuda:0')
Epoch: [22][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.8233 (0.8233) ([0.513]+[0.310])	Prec@1 82.812 (82.812)
Epoch: [22][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 1.0465 (0.7599) ([0.736]+[0.311])	Prec@1 76.562 (84.893)
Epoch: [22][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6865 (0.7593) ([0.379]+[0.308])	Prec@1 89.062 (84.861)
Epoch: [22][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7536 (0.7607) ([0.445]+[0.309])	Prec@1 85.156 (84.728)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.9760 (0.9760) ([0.667]+[0.309])	Prec@1 82.031 (82.031)
 * Prec@1 81.310
current lr 1.00000e-01
Grad=  tensor(1.8037, device='cuda:0')
Epoch: [23][0/391]	Time 0.170 (0.170)	Data 0.126 (0.126)	Loss 0.6916 (0.6916) ([0.383]+[0.309])	Prec@1 85.938 (85.938)
Epoch: [23][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7506 (0.7657) ([0.441]+[0.310])	Prec@1 85.938 (84.623)
Epoch: [23][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8763 (0.7558) ([0.569]+[0.307])	Prec@1 79.688 (84.771)
Epoch: [23][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7312 (0.7597) ([0.423]+[0.308])	Prec@1 88.281 (84.661)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8913 (0.8913) ([0.585]+[0.306])	Prec@1 82.031 (82.031)
 * Prec@1 80.180
current lr 1.00000e-01
Grad=  tensor(1.8040, device='cuda:0')
Epoch: [24][0/391]	Time 0.171 (0.171)	Data 0.128 (0.128)	Loss 0.6780 (0.6780) ([0.372]+[0.306])	Prec@1 85.938 (85.938)
Epoch: [24][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7764 (0.7519) ([0.470]+[0.307])	Prec@1 85.156 (84.932)
Epoch: [24][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7891 (0.7542) ([0.483]+[0.306])	Prec@1 81.250 (85.106)
Epoch: [24][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7613 (0.7500) ([0.457]+[0.304])	Prec@1 85.156 (85.190)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.8430 (0.8430) ([0.540]+[0.303])	Prec@1 81.250 (81.250)
 * Prec@1 80.770
current lr 1.00000e-01
Grad=  tensor(2.0788, device='cuda:0')
Epoch: [25][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.6980 (0.6980) ([0.395]+[0.303])	Prec@1 86.719 (86.719)
Epoch: [25][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7370 (0.7422) ([0.432]+[0.305])	Prec@1 85.156 (85.303)
Epoch: [25][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8384 (0.7531) ([0.534]+[0.305])	Prec@1 82.812 (84.861)
Epoch: [25][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7346 (0.7551) ([0.429]+[0.305])	Prec@1 85.156 (84.780)
Test: [0/79]	Time 0.125 (0.125)	Loss 1.1722 (1.1722) ([0.868]+[0.304])	Prec@1 69.531 (69.531)
 * Prec@1 70.850
current lr 1.00000e-01
Grad=  tensor(2.9674, device='cuda:0')
Epoch: [26][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.8080 (0.8080) ([0.504]+[0.304])	Prec@1 81.250 (81.250)
Epoch: [26][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7236 (0.7540) ([0.417]+[0.306])	Prec@1 88.281 (84.692)
Epoch: [26][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8218 (0.7559) ([0.517]+[0.305])	Prec@1 82.031 (84.810)
Epoch: [26][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6370 (0.7476) ([0.334]+[0.304])	Prec@1 88.281 (85.016)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9968 (0.9968) ([0.693]+[0.304])	Prec@1 76.562 (76.562)
 * Prec@1 76.590
current lr 1.00000e-01
Grad=  tensor(1.9776, device='cuda:0')
Epoch: [27][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6702 (0.6702) ([0.366]+[0.304])	Prec@1 87.500 (87.500)
Epoch: [27][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6576 (0.7466) ([0.354]+[0.304])	Prec@1 85.156 (84.986)
Epoch: [27][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8558 (0.7491) ([0.552]+[0.304])	Prec@1 76.562 (84.814)
Epoch: [27][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8874 (0.7518) ([0.584]+[0.304])	Prec@1 79.688 (84.868)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.9195 (0.9195) ([0.617]+[0.302])	Prec@1 78.125 (78.125)
 * Prec@1 79.540
current lr 1.00000e-01
Grad=  tensor(1.9372, device='cuda:0')
Epoch: [28][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.8163 (0.8163) ([0.514]+[0.302])	Prec@1 82.812 (82.812)
Epoch: [28][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7865 (0.7372) ([0.482]+[0.304])	Prec@1 84.375 (85.079)
Epoch: [28][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6970 (0.7449) ([0.395]+[0.302])	Prec@1 82.812 (84.915)
Epoch: [28][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7327 (0.7451) ([0.431]+[0.302])	Prec@1 87.500 (84.946)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8783 (0.8783) ([0.577]+[0.301])	Prec@1 80.469 (80.469)
 * Prec@1 77.550
current lr 1.00000e-01
Grad=  tensor(1.7770, device='cuda:0')
Epoch: [29][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.6300 (0.6300) ([0.329]+[0.301])	Prec@1 87.500 (87.500)
Epoch: [29][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7720 (0.7220) ([0.471]+[0.301])	Prec@1 83.594 (85.427)
Epoch: [29][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8380 (0.7383) ([0.535]+[0.303])	Prec@1 82.812 (84.946)
Epoch: [29][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9069 (0.7466) ([0.604]+[0.303])	Prec@1 79.688 (84.767)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.9094 (0.9094) ([0.608]+[0.302])	Prec@1 78.906 (78.906)
 * Prec@1 78.090
current lr 1.00000e-01
Grad=  tensor(2.3134, device='cuda:0')
Epoch: [30][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.7775 (0.7775) ([0.476]+[0.302])	Prec@1 85.938 (85.938)
Epoch: [30][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8161 (0.7407) ([0.514]+[0.302])	Prec@1 82.812 (85.156)
Epoch: [30][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6532 (0.7357) ([0.352]+[0.301])	Prec@1 88.281 (85.405)
Epoch: [30][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7081 (0.7469) ([0.406]+[0.303])	Prec@1 88.281 (84.915)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8699 (0.8699) ([0.568]+[0.302])	Prec@1 83.594 (83.594)
 * Prec@1 81.950
current lr 1.00000e-01
Grad=  tensor(2.2741, device='cuda:0')
Epoch: [31][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.7358 (0.7358) ([0.434]+[0.302])	Prec@1 86.719 (86.719)
Epoch: [31][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7242 (0.7224) ([0.424]+[0.300])	Prec@1 85.938 (85.922)
Epoch: [31][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7415 (0.7253) ([0.440]+[0.301])	Prec@1 83.594 (85.805)
Epoch: [31][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6445 (0.7283) ([0.342]+[0.302])	Prec@1 88.281 (85.543)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7458 (0.7458) ([0.444]+[0.302])	Prec@1 85.156 (85.156)
 * Prec@1 82.120
current lr 1.00000e-01
Grad=  tensor(1.9794, device='cuda:0')
Epoch: [32][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.7050 (0.7050) ([0.403]+[0.302])	Prec@1 82.812 (82.812)
Epoch: [32][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6359 (0.7430) ([0.334]+[0.302])	Prec@1 89.062 (84.916)
Epoch: [32][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8090 (0.7324) ([0.508]+[0.301])	Prec@1 83.594 (85.424)
Epoch: [32][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8240 (0.7411) ([0.523]+[0.301])	Prec@1 84.375 (85.117)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8627 (0.8627) ([0.563]+[0.300])	Prec@1 75.781 (75.781)
 * Prec@1 78.230
current lr 1.00000e-01
Grad=  tensor(2.6650, device='cuda:0')
Epoch: [33][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.7491 (0.7491) ([0.449]+[0.300])	Prec@1 85.938 (85.938)
Epoch: [33][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7899 (0.7297) ([0.490]+[0.300])	Prec@1 86.719 (85.551)
Epoch: [33][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7566 (0.7321) ([0.457]+[0.299])	Prec@1 85.938 (85.467)
Epoch: [33][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6474 (0.7292) ([0.349]+[0.299])	Prec@1 89.062 (85.460)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9154 (0.9154) ([0.616]+[0.299])	Prec@1 80.469 (80.469)
 * Prec@1 79.320
current lr 1.00000e-01
Grad=  tensor(2.1378, device='cuda:0')
Epoch: [34][0/391]	Time 0.168 (0.168)	Data 0.124 (0.124)	Loss 0.6984 (0.6984) ([0.399]+[0.299])	Prec@1 85.938 (85.938)
Epoch: [34][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6122 (0.7254) ([0.311]+[0.301])	Prec@1 89.062 (85.288)
Epoch: [34][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8157 (0.7306) ([0.516]+[0.300])	Prec@1 82.812 (85.199)
Epoch: [34][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6162 (0.7354) ([0.315]+[0.302])	Prec@1 87.500 (85.115)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.9189 (0.9189) ([0.619]+[0.299])	Prec@1 78.906 (78.906)
 * Prec@1 80.780
current lr 1.00000e-01
Grad=  tensor(2.9625, device='cuda:0')
Epoch: [35][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.7743 (0.7743) ([0.475]+[0.299])	Prec@1 81.250 (81.250)
Epoch: [35][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6863 (0.7248) ([0.386]+[0.301])	Prec@1 86.719 (85.442)
Epoch: [35][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8414 (0.7326) ([0.542]+[0.300])	Prec@1 81.250 (85.487)
Epoch: [35][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8265 (0.7307) ([0.527]+[0.299])	Prec@1 81.250 (85.530)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7825 (0.7825) ([0.484]+[0.299])	Prec@1 84.375 (84.375)
 * Prec@1 82.510
current lr 1.00000e-01
Grad=  tensor(2.0642, device='cuda:0')
Epoch: [36][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.6704 (0.6704) ([0.372]+[0.299])	Prec@1 85.938 (85.938)
Epoch: [36][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7239 (0.7135) ([0.426]+[0.298])	Prec@1 85.156 (85.922)
Epoch: [36][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7366 (0.7246) ([0.437]+[0.299])	Prec@1 83.594 (85.374)
Epoch: [36][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6523 (0.7261) ([0.354]+[0.298])	Prec@1 88.281 (85.488)
Test: [0/79]	Time 0.126 (0.126)	Loss 1.1036 (1.1036) ([0.805]+[0.298])	Prec@1 75.000 (75.000)
 * Prec@1 76.620
current lr 1.00000e-01
Grad=  tensor(3.6166, device='cuda:0')
Epoch: [37][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.9396 (0.9396) ([0.641]+[0.298])	Prec@1 78.906 (78.906)
Epoch: [37][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6287 (0.7344) ([0.328]+[0.301])	Prec@1 89.062 (85.311)
Epoch: [37][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6975 (0.7350) ([0.398]+[0.300])	Prec@1 85.156 (85.160)
Epoch: [37][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7703 (0.7307) ([0.471]+[0.299])	Prec@1 84.375 (85.289)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8887 (0.8887) ([0.591]+[0.298])	Prec@1 82.031 (82.031)
 * Prec@1 80.250
current lr 1.00000e-01
Grad=  tensor(4.1006, device='cuda:0')
Epoch: [38][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.8983 (0.8983) ([0.600]+[0.298])	Prec@1 79.688 (79.688)
Epoch: [38][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6929 (0.7176) ([0.395]+[0.298])	Prec@1 88.281 (85.651)
Epoch: [38][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6726 (0.7235) ([0.374]+[0.298])	Prec@1 87.500 (85.568)
Epoch: [38][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.5431 (0.7226) ([0.245]+[0.298])	Prec@1 90.625 (85.613)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.9423 (0.9423) ([0.644]+[0.298])	Prec@1 82.031 (82.031)
 * Prec@1 80.830
current lr 1.00000e-01
Grad=  tensor(1.9535, device='cuda:0')
Epoch: [39][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7103 (0.7103) ([0.412]+[0.298])	Prec@1 85.938 (85.938)
Epoch: [39][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6282 (0.7105) ([0.331]+[0.297])	Prec@1 89.844 (85.829)
Epoch: [39][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7187 (0.7152) ([0.421]+[0.297])	Prec@1 84.375 (85.700)
Epoch: [39][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7699 (0.7242) ([0.472]+[0.298])	Prec@1 83.594 (85.540)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.7305 (0.7305) ([0.434]+[0.296])	Prec@1 84.375 (84.375)
 * Prec@1 83.130
current lr 1.00000e-01
Grad=  tensor(2.1693, device='cuda:0')
Epoch: [40][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.6958 (0.6958) ([0.400]+[0.296])	Prec@1 85.938 (85.938)
Epoch: [40][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6998 (0.7235) ([0.405]+[0.295])	Prec@1 87.500 (85.667)
Epoch: [40][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6635 (0.7181) ([0.368]+[0.295])	Prec@1 88.281 (85.794)
Epoch: [40][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7717 (0.7170) ([0.476]+[0.296])	Prec@1 85.938 (85.810)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8661 (0.8661) ([0.569]+[0.297])	Prec@1 83.594 (83.594)
 * Prec@1 80.480
current lr 1.00000e-01
Grad=  tensor(2.6462, device='cuda:0')
Epoch: [41][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.8079 (0.8079) ([0.511]+[0.297])	Prec@1 81.250 (81.250)
Epoch: [41][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6956 (0.7126) ([0.401]+[0.295])	Prec@1 86.719 (85.999)
Epoch: [41][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6581 (0.7189) ([0.363]+[0.296])	Prec@1 85.156 (85.751)
Epoch: [41][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6527 (0.7193) ([0.356]+[0.296])	Prec@1 85.156 (85.779)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.6559 (0.6559) ([0.359]+[0.296])	Prec@1 89.062 (89.062)
 * Prec@1 82.740
current lr 1.00000e-01
Grad=  tensor(2.7166, device='cuda:0')
Epoch: [42][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.7081 (0.7081) ([0.412]+[0.296])	Prec@1 86.719 (86.719)
Epoch: [42][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6786 (0.7167) ([0.382]+[0.296])	Prec@1 85.938 (85.876)
Epoch: [42][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6430 (0.7144) ([0.348]+[0.295])	Prec@1 90.625 (85.875)
Epoch: [42][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.8255 (0.7234) ([0.529]+[0.297])	Prec@1 83.594 (85.533)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8438 (0.8438) ([0.547]+[0.297])	Prec@1 81.250 (81.250)
 * Prec@1 78.290
current lr 1.00000e-01
Grad=  tensor(3.4299, device='cuda:0')
Epoch: [43][0/391]	Time 0.162 (0.162)	Data 0.121 (0.121)	Loss 0.8412 (0.8412) ([0.545]+[0.297])	Prec@1 81.250 (81.250)
Epoch: [43][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8154 (0.7155) ([0.519]+[0.296])	Prec@1 78.906 (85.520)
Epoch: [43][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6815 (0.7220) ([0.384]+[0.297])	Prec@1 88.281 (85.370)
Epoch: [43][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6204 (0.7188) ([0.325]+[0.295])	Prec@1 91.406 (85.569)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9508 (0.9508) ([0.656]+[0.295])	Prec@1 77.344 (77.344)
 * Prec@1 77.780
current lr 1.00000e-01
Grad=  tensor(2.0505, device='cuda:0')
Epoch: [44][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6838 (0.6838) ([0.389]+[0.295])	Prec@1 85.938 (85.938)
Epoch: [44][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6655 (0.6912) ([0.370]+[0.295])	Prec@1 90.625 (86.580)
Epoch: [44][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7281 (0.7045) ([0.433]+[0.295])	Prec@1 88.281 (86.229)
Epoch: [44][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7196 (0.7133) ([0.424]+[0.296])	Prec@1 88.281 (85.925)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.7957 (0.7957) ([0.500]+[0.295])	Prec@1 81.250 (81.250)
 * Prec@1 81.930
current lr 1.00000e-01
Grad=  tensor(3.2862, device='cuda:0')
Epoch: [45][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.8342 (0.8342) ([0.539]+[0.295])	Prec@1 80.469 (80.469)
Epoch: [45][100/391]	Time 0.035 (0.037)	Data 0.000 (0.001)	Loss 0.8141 (0.7100) ([0.518]+[0.296])	Prec@1 85.156 (86.015)
Epoch: [45][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.5855 (0.7094) ([0.289]+[0.296])	Prec@1 86.719 (85.821)
Epoch: [45][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.8050 (0.7173) ([0.510]+[0.295])	Prec@1 84.375 (85.616)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8328 (0.8328) ([0.538]+[0.294])	Prec@1 84.375 (84.375)
 * Prec@1 80.560
current lr 1.00000e-01
Grad=  tensor(1.4042, device='cuda:0')
Epoch: [46][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.5641 (0.5641) ([0.270]+[0.294])	Prec@1 89.844 (89.844)
Epoch: [46][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7380 (0.7072) ([0.444]+[0.294])	Prec@1 82.031 (85.930)
Epoch: [46][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7892 (0.7129) ([0.494]+[0.295])	Prec@1 82.812 (85.798)
Epoch: [46][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6308 (0.7142) ([0.336]+[0.295])	Prec@1 88.281 (85.634)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.7368 (0.7368) ([0.443]+[0.294])	Prec@1 85.156 (85.156)
 * Prec@1 84.340
current lr 1.00000e-01
Grad=  tensor(2.1948, device='cuda:0')
Epoch: [47][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.6701 (0.6701) ([0.376]+[0.294])	Prec@1 86.719 (86.719)
Epoch: [47][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6469 (0.7005) ([0.353]+[0.294])	Prec@1 89.062 (85.968)
Epoch: [47][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.5748 (0.7091) ([0.279]+[0.296])	Prec@1 91.406 (85.840)
Epoch: [47][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6695 (0.7114) ([0.375]+[0.294])	Prec@1 85.156 (85.792)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7735 (0.7735) ([0.480]+[0.294])	Prec@1 88.281 (88.281)
 * Prec@1 82.020
current lr 1.00000e-01
Grad=  tensor(3.1283, device='cuda:0')
Epoch: [48][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.8305 (0.8305) ([0.537]+[0.294])	Prec@1 79.688 (79.688)
Epoch: [48][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9190 (0.7122) ([0.625]+[0.294])	Prec@1 76.562 (85.729)
Epoch: [48][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6890 (0.7133) ([0.395]+[0.294])	Prec@1 88.281 (85.763)
Epoch: [48][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.9022 (0.7167) ([0.606]+[0.297])	Prec@1 78.906 (85.647)
Test: [0/79]	Time 0.128 (0.128)	Loss 1.0570 (1.0570) ([0.762]+[0.296])	Prec@1 77.344 (77.344)
 * Prec@1 75.700
current lr 1.00000e-01
Grad=  tensor(1.8623, device='cuda:0')
Epoch: [49][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.6501 (0.6501) ([0.355]+[0.296])	Prec@1 88.281 (88.281)
Epoch: [49][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5898 (0.6876) ([0.296]+[0.294])	Prec@1 90.625 (86.378)
Epoch: [49][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6993 (0.7004) ([0.405]+[0.295])	Prec@1 85.156 (86.027)
Epoch: [49][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6254 (0.7017) ([0.330]+[0.295])	Prec@1 88.281 (86.039)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.9926 (0.9926) ([0.698]+[0.294])	Prec@1 75.000 (75.000)
 * Prec@1 80.240
current lr 1.00000e-01
Grad=  tensor(1.5690, device='cuda:0')
Epoch: [50][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.6027 (0.6027) ([0.308]+[0.294])	Prec@1 88.281 (88.281)
Epoch: [50][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7144 (0.6991) ([0.420]+[0.294])	Prec@1 85.156 (86.402)
Epoch: [50][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7511 (0.7081) ([0.455]+[0.296])	Prec@1 85.938 (86.058)
Epoch: [50][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6911 (0.7132) ([0.398]+[0.293])	Prec@1 85.938 (85.836)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8575 (0.8575) ([0.564]+[0.293])	Prec@1 81.250 (81.250)
 * Prec@1 82.550
current lr 1.00000e-01
Grad=  tensor(2.4740, device='cuda:0')
Epoch: [51][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7293 (0.7293) ([0.436]+[0.293])	Prec@1 86.719 (86.719)
Epoch: [51][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.5944 (0.6947) ([0.301]+[0.294])	Prec@1 89.062 (86.610)
Epoch: [51][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6847 (0.6979) ([0.393]+[0.292])	Prec@1 85.156 (86.350)
Epoch: [51][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6599 (0.7044) ([0.367]+[0.293])	Prec@1 88.281 (86.036)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8417 (0.8417) ([0.551]+[0.291])	Prec@1 82.031 (82.031)
 * Prec@1 82.160
current lr 1.00000e-01
Grad=  tensor(2.8633, device='cuda:0')
Epoch: [52][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7890 (0.7890) ([0.498]+[0.291])	Prec@1 79.688 (79.688)
Epoch: [52][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.6875 (0.6927) ([0.396]+[0.291])	Prec@1 89.062 (86.293)
Epoch: [52][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7030 (0.7032) ([0.412]+[0.291])	Prec@1 85.156 (85.922)
Epoch: [52][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7132 (0.7089) ([0.421]+[0.293])	Prec@1 86.719 (85.779)
Test: [0/79]	Time 0.123 (0.123)	Loss 1.0758 (1.0758) ([0.783]+[0.292])	Prec@1 75.000 (75.000)
 * Prec@1 78.130
current lr 1.00000e-01
Grad=  tensor(2.6523, device='cuda:0')
Epoch: [53][0/391]	Time 0.171 (0.171)	Data 0.123 (0.123)	Loss 0.8463 (0.8463) ([0.554]+[0.292])	Prec@1 81.250 (81.250)
Epoch: [53][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6258 (0.7075) ([0.334]+[0.292])	Prec@1 86.719 (85.713)
Epoch: [53][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7119 (0.7140) ([0.418]+[0.293])	Prec@1 86.719 (85.689)
Epoch: [53][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7655 (0.7159) ([0.472]+[0.293])	Prec@1 79.688 (85.636)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7549 (0.7549) ([0.464]+[0.291])	Prec@1 84.375 (84.375)
 * Prec@1 80.860
current lr 1.00000e-01
Grad=  tensor(2.5034, device='cuda:0')
Epoch: [54][0/391]	Time 0.174 (0.174)	Data 0.126 (0.126)	Loss 0.7542 (0.7542) ([0.463]+[0.291])	Prec@1 85.156 (85.156)
Epoch: [54][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6734 (0.6953) ([0.381]+[0.292])	Prec@1 85.156 (86.502)
Epoch: [54][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6515 (0.7047) ([0.359]+[0.292])	Prec@1 90.625 (86.124)
Epoch: [54][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8594 (0.7033) ([0.568]+[0.291])	Prec@1 82.812 (86.047)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.9223 (0.9223) ([0.630]+[0.292])	Prec@1 78.125 (78.125)
 * Prec@1 80.430
current lr 1.00000e-01
Grad=  tensor(1.6294, device='cuda:0')
Epoch: [55][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.5941 (0.5941) ([0.302]+[0.292])	Prec@1 89.062 (89.062)
Epoch: [55][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7283 (0.6916) ([0.436]+[0.292])	Prec@1 83.594 (86.355)
Epoch: [55][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6742 (0.7072) ([0.381]+[0.293])	Prec@1 84.375 (86.007)
Epoch: [55][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8354 (0.7065) ([0.544]+[0.291])	Prec@1 83.594 (86.158)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8978 (0.8978) ([0.607]+[0.290])	Prec@1 80.469 (80.469)
 * Prec@1 80.400
current lr 1.00000e-01
Grad=  tensor(2.2164, device='cuda:0')
Epoch: [56][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.7127 (0.7127) ([0.422]+[0.290])	Prec@1 84.375 (84.375)
Epoch: [56][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6762 (0.6976) ([0.385]+[0.291])	Prec@1 85.938 (86.208)
Epoch: [56][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8304 (0.6996) ([0.539]+[0.292])	Prec@1 81.250 (86.136)
Epoch: [56][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7823 (0.7071) ([0.490]+[0.292])	Prec@1 81.250 (85.899)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7599 (0.7599) ([0.468]+[0.292])	Prec@1 82.031 (82.031)
 * Prec@1 84.900
current lr 1.00000e-01
Grad=  tensor(1.7679, device='cuda:0')
Epoch: [57][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.5979 (0.5979) ([0.306]+[0.292])	Prec@1 85.156 (85.156)
Epoch: [57][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6128 (0.6946) ([0.320]+[0.292])	Prec@1 89.062 (86.247)
Epoch: [57][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6284 (0.7123) ([0.335]+[0.293])	Prec@1 89.844 (85.728)
Epoch: [57][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6799 (0.7074) ([0.388]+[0.291])	Prec@1 86.719 (85.873)
Test: [0/79]	Time 0.124 (0.124)	Loss 1.1331 (1.1331) ([0.841]+[0.292])	Prec@1 73.438 (73.438)
 * Prec@1 72.870
current lr 1.00000e-01
Grad=  tensor(2.0468, device='cuda:0')
Epoch: [58][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.7232 (0.7232) ([0.431]+[0.292])	Prec@1 85.938 (85.938)
Epoch: [58][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6667 (0.6922) ([0.375]+[0.292])	Prec@1 89.844 (86.193)
Epoch: [58][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6578 (0.7014) ([0.366]+[0.292])	Prec@1 87.500 (86.007)
Epoch: [58][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6616 (0.7093) ([0.369]+[0.293])	Prec@1 86.719 (85.865)
Test: [0/79]	Time 0.123 (0.123)	Loss 1.1481 (1.1481) ([0.856]+[0.292])	Prec@1 72.656 (72.656)
 * Prec@1 77.040
current lr 1.00000e-01
Grad=  tensor(2.5328, device='cuda:0')
Epoch: [59][0/391]	Time 0.179 (0.179)	Data 0.136 (0.136)	Loss 0.7273 (0.7273) ([0.435]+[0.292])	Prec@1 82.031 (82.031)
Epoch: [59][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6341 (0.7008) ([0.342]+[0.292])	Prec@1 89.062 (86.317)
Epoch: [59][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7113 (0.6933) ([0.421]+[0.290])	Prec@1 87.500 (86.346)
Epoch: [59][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6654 (0.7029) ([0.374]+[0.291])	Prec@1 86.719 (86.098)
Test: [0/79]	Time 0.126 (0.126)	Loss 1.0817 (1.0817) ([0.791]+[0.290])	Prec@1 76.562 (76.562)
 * Prec@1 76.340
current lr 1.00000e-01
Grad=  tensor(2.4368, device='cuda:0')
Epoch: [60][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7461 (0.7461) ([0.456]+[0.290])	Prec@1 83.594 (83.594)
Epoch: [60][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6837 (0.7121) ([0.392]+[0.291])	Prec@1 88.281 (85.814)
Epoch: [60][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6768 (0.7095) ([0.387]+[0.290])	Prec@1 86.719 (85.689)
Epoch: [60][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7617 (0.7107) ([0.471]+[0.290])	Prec@1 89.844 (85.696)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8949 (0.8949) ([0.604]+[0.290])	Prec@1 75.781 (75.781)
 * Prec@1 79.430
current lr 1.00000e-01
Grad=  tensor(2.2607, device='cuda:0')
Epoch: [61][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6745 (0.6745) ([0.384]+[0.290])	Prec@1 89.062 (89.062)
Epoch: [61][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6525 (0.6942) ([0.362]+[0.291])	Prec@1 85.938 (86.409)
Epoch: [61][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7787 (0.6985) ([0.488]+[0.290])	Prec@1 83.594 (86.210)
Epoch: [61][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6041 (0.7003) ([0.313]+[0.291])	Prec@1 87.500 (86.213)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8884 (0.8884) ([0.599]+[0.289])	Prec@1 80.469 (80.469)
 * Prec@1 83.150
current lr 1.00000e-01
Grad=  tensor(1.6322, device='cuda:0')
Epoch: [62][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.5967 (0.5967) ([0.308]+[0.289])	Prec@1 90.625 (90.625)
Epoch: [62][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7877 (0.6914) ([0.497]+[0.291])	Prec@1 84.375 (86.556)
Epoch: [62][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7908 (0.6970) ([0.501]+[0.290])	Prec@1 83.594 (86.167)
Epoch: [62][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6621 (0.7028) ([0.373]+[0.290])	Prec@1 85.938 (86.070)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8974 (0.8974) ([0.607]+[0.290])	Prec@1 79.688 (79.688)
 * Prec@1 79.500
current lr 1.00000e-01
Grad=  tensor(1.6434, device='cuda:0')
Epoch: [63][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.6024 (0.6024) ([0.312]+[0.290])	Prec@1 87.500 (87.500)
Epoch: [63][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6137 (0.6850) ([0.325]+[0.289])	Prec@1 90.625 (86.262)
Epoch: [63][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7246 (0.6982) ([0.435]+[0.290])	Prec@1 83.594 (86.070)
Epoch: [63][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6863 (0.7024) ([0.396]+[0.290])	Prec@1 85.938 (85.914)
Test: [0/79]	Time 0.132 (0.132)	Loss 1.1168 (1.1168) ([0.827]+[0.290])	Prec@1 78.906 (78.906)
 * Prec@1 75.480
current lr 1.00000e-01
Grad=  tensor(1.6601, device='cuda:0')
Epoch: [64][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.6083 (0.6083) ([0.319]+[0.290])	Prec@1 89.062 (89.062)
Epoch: [64][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6368 (0.6903) ([0.348]+[0.289])	Prec@1 87.500 (86.247)
Epoch: [64][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6452 (0.6969) ([0.357]+[0.288])	Prec@1 85.938 (85.957)
Epoch: [64][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5184 (0.6954) ([0.229]+[0.289])	Prec@1 93.750 (86.143)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.7440 (0.7440) ([0.455]+[0.289])	Prec@1 84.375 (84.375)
 * Prec@1 82.880
current lr 1.00000e-01
Grad=  tensor(1.6106, device='cuda:0')
Epoch: [65][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.6411 (0.6411) ([0.352]+[0.289])	Prec@1 90.625 (90.625)
Epoch: [65][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8140 (0.6902) ([0.525]+[0.289])	Prec@1 82.812 (86.463)
Epoch: [65][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6615 (0.6975) ([0.372]+[0.290])	Prec@1 87.500 (86.136)
Epoch: [65][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6923 (0.7032) ([0.402]+[0.290])	Prec@1 86.719 (85.930)
Test: [0/79]	Time 0.127 (0.127)	Loss 1.1278 (1.1278) ([0.839]+[0.289])	Prec@1 78.125 (78.125)
 * Prec@1 74.030
current lr 1.00000e-01
Grad=  tensor(2.4270, device='cuda:0')
Epoch: [66][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.7634 (0.7634) ([0.474]+[0.289])	Prec@1 84.375 (84.375)
Epoch: [66][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6560 (0.6897) ([0.367]+[0.289])	Prec@1 89.844 (86.270)
Epoch: [66][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6047 (0.6958) ([0.316]+[0.289])	Prec@1 89.062 (86.202)
Epoch: [66][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6462 (0.6979) ([0.358]+[0.289])	Prec@1 87.500 (86.231)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8489 (0.8489) ([0.561]+[0.288])	Prec@1 82.812 (82.812)
 * Prec@1 81.350
current lr 1.00000e-01
Grad=  tensor(1.6482, device='cuda:0')
Epoch: [67][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6030 (0.6030) ([0.315]+[0.288])	Prec@1 87.500 (87.500)
Epoch: [67][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8621 (0.6759) ([0.574]+[0.288])	Prec@1 84.375 (86.812)
Epoch: [67][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7990 (0.6892) ([0.511]+[0.288])	Prec@1 83.594 (86.295)
Epoch: [67][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8061 (0.6977) ([0.517]+[0.290])	Prec@1 80.469 (86.028)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.9479 (0.9479) ([0.659]+[0.289])	Prec@1 78.906 (78.906)
 * Prec@1 77.490
current lr 1.00000e-01
Grad=  tensor(3.0080, device='cuda:0')
Epoch: [68][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.7930 (0.7930) ([0.504]+[0.289])	Prec@1 83.594 (83.594)
Epoch: [68][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6963 (0.6900) ([0.406]+[0.290])	Prec@1 87.500 (86.564)
Epoch: [68][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7688 (0.6970) ([0.479]+[0.290])	Prec@1 80.469 (86.221)
Epoch: [68][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.8010 (0.6998) ([0.512]+[0.289])	Prec@1 83.594 (86.039)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8459 (0.8459) ([0.558]+[0.288])	Prec@1 83.594 (83.594)
 * Prec@1 82.020
current lr 1.00000e-01
Grad=  tensor(3.2400, device='cuda:0')
Epoch: [69][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.8744 (0.8744) ([0.586]+[0.288])	Prec@1 82.031 (82.031)
Epoch: [69][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6799 (0.7013) ([0.392]+[0.287])	Prec@1 84.375 (86.069)
Epoch: [69][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6948 (0.6942) ([0.407]+[0.288])	Prec@1 86.719 (86.280)
Epoch: [69][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7248 (0.6990) ([0.437]+[0.288])	Prec@1 87.500 (86.078)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.9863 (0.9863) ([0.697]+[0.290])	Prec@1 78.125 (78.125)
 * Prec@1 79.770
current lr 1.00000e-01
Grad=  tensor(2.0849, device='cuda:0')
Epoch: [70][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.6475 (0.6475) ([0.358]+[0.290])	Prec@1 86.719 (86.719)
Epoch: [70][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7748 (0.6936) ([0.487]+[0.288])	Prec@1 81.250 (86.394)
Epoch: [70][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6442 (0.6934) ([0.357]+[0.287])	Prec@1 88.281 (86.482)
Epoch: [70][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5229 (0.6910) ([0.236]+[0.287])	Prec@1 90.625 (86.493)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.7529 (0.7529) ([0.466]+[0.287])	Prec@1 82.812 (82.812)
 * Prec@1 83.400
current lr 1.00000e-01
Grad=  tensor(2.4956, device='cuda:0')
Epoch: [71][0/391]	Time 0.169 (0.169)	Data 0.125 (0.125)	Loss 0.7442 (0.7442) ([0.457]+[0.287])	Prec@1 86.719 (86.719)
Epoch: [71][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6500 (0.6858) ([0.363]+[0.287])	Prec@1 85.938 (86.634)
Epoch: [71][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7270 (0.7001) ([0.439]+[0.288])	Prec@1 82.812 (86.116)
Epoch: [71][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7337 (0.6999) ([0.447]+[0.287])	Prec@1 82.031 (86.145)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8432 (0.8432) ([0.556]+[0.287])	Prec@1 82.031 (82.031)
 * Prec@1 80.480
current lr 1.00000e-01
Grad=  tensor(2.3179, device='cuda:0')
Epoch: [72][0/391]	Time 0.166 (0.166)	Data 0.122 (0.122)	Loss 0.6568 (0.6568) ([0.370]+[0.287])	Prec@1 87.500 (87.500)
Epoch: [72][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6832 (0.6875) ([0.396]+[0.287])	Prec@1 85.938 (86.595)
Epoch: [72][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6632 (0.6926) ([0.376]+[0.287])	Prec@1 86.719 (86.283)
Epoch: [72][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7235 (0.6971) ([0.437]+[0.286])	Prec@1 82.812 (86.143)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.8073 (0.8073) ([0.520]+[0.287])	Prec@1 84.375 (84.375)
 * Prec@1 83.960
current lr 1.00000e-01
Grad=  tensor(2.0490, device='cuda:0')
Epoch: [73][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6796 (0.6796) ([0.393]+[0.287])	Prec@1 86.719 (86.719)
Epoch: [73][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5155 (0.6744) ([0.230]+[0.285])	Prec@1 92.969 (87.044)
Epoch: [73][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7199 (0.6856) ([0.435]+[0.285])	Prec@1 83.594 (86.598)
Epoch: [73][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6351 (0.6875) ([0.350]+[0.286])	Prec@1 89.844 (86.540)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.7240 (0.7240) ([0.438]+[0.286])	Prec@1 81.250 (81.250)
 * Prec@1 81.940
current lr 1.00000e-01
Grad=  tensor(1.8476, device='cuda:0')
Epoch: [74][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.6070 (0.6070) ([0.321]+[0.286])	Prec@1 89.062 (89.062)
Epoch: [74][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6729 (0.6998) ([0.387]+[0.286])	Prec@1 89.062 (85.628)
Epoch: [74][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6187 (0.6987) ([0.332]+[0.287])	Prec@1 89.062 (85.840)
Epoch: [74][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6759 (0.6959) ([0.390]+[0.286])	Prec@1 86.719 (86.023)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.7653 (0.7653) ([0.479]+[0.287])	Prec@1 85.156 (85.156)
 * Prec@1 83.950
current lr 1.00000e-01
Grad=  tensor(1.4910, device='cuda:0')
Epoch: [75][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.5930 (0.5930) ([0.306]+[0.287])	Prec@1 90.625 (90.625)
Epoch: [75][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5563 (0.6802) ([0.271]+[0.286])	Prec@1 93.750 (86.904)
Epoch: [75][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5529 (0.6880) ([0.267]+[0.286])	Prec@1 90.625 (86.552)
Epoch: [75][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7339 (0.6927) ([0.447]+[0.286])	Prec@1 85.156 (86.316)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.7106 (0.7106) ([0.425]+[0.286])	Prec@1 86.719 (86.719)
 * Prec@1 83.130
current lr 1.00000e-01
Grad=  tensor(2.0492, device='cuda:0')
Epoch: [76][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.6679 (0.6679) ([0.382]+[0.286])	Prec@1 88.281 (88.281)
Epoch: [76][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7340 (0.6768) ([0.448]+[0.286])	Prec@1 83.594 (86.858)
Epoch: [76][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6811 (0.6896) ([0.396]+[0.285])	Prec@1 83.594 (86.431)
Epoch: [76][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6738 (0.6939) ([0.388]+[0.286])	Prec@1 87.500 (86.265)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8866 (0.8866) ([0.600]+[0.286])	Prec@1 79.688 (79.688)
 * Prec@1 80.860
current lr 1.00000e-01
Grad=  tensor(1.9916, device='cuda:0')
Epoch: [77][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.6238 (0.6238) ([0.338]+[0.286])	Prec@1 88.281 (88.281)
Epoch: [77][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8264 (0.6947) ([0.539]+[0.288])	Prec@1 83.594 (85.876)
Epoch: [77][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7131 (0.6927) ([0.426]+[0.287])	Prec@1 85.938 (86.062)
Epoch: [77][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6925 (0.6970) ([0.405]+[0.287])	Prec@1 87.500 (85.950)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8906 (0.8906) ([0.603]+[0.287])	Prec@1 83.594 (83.594)
 * Prec@1 81.370
current lr 1.00000e-01
Grad=  tensor(2.1175, device='cuda:0')
Epoch: [78][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.6867 (0.6867) ([0.399]+[0.287])	Prec@1 85.938 (85.938)
Epoch: [78][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6187 (0.6870) ([0.333]+[0.286])	Prec@1 88.281 (86.347)
Epoch: [78][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6894 (0.6930) ([0.404]+[0.286])	Prec@1 86.719 (86.330)
Epoch: [78][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7516 (0.6887) ([0.466]+[0.285])	Prec@1 83.594 (86.379)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.6523 (0.6523) ([0.366]+[0.286])	Prec@1 87.500 (87.500)
 * Prec@1 80.690
current lr 1.00000e-01
Grad=  tensor(2.1290, device='cuda:0')
Epoch: [79][0/391]	Time 0.173 (0.173)	Data 0.124 (0.124)	Loss 0.7244 (0.7244) ([0.438]+[0.286])	Prec@1 84.375 (84.375)
Epoch: [79][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7363 (0.6780) ([0.451]+[0.285])	Prec@1 82.812 (86.989)
Epoch: [79][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6944 (0.6825) ([0.409]+[0.285])	Prec@1 86.719 (86.839)
Epoch: [79][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7953 (0.6898) ([0.511]+[0.284])	Prec@1 79.688 (86.381)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7734 (0.7734) ([0.488]+[0.285])	Prec@1 82.031 (82.031)
 * Prec@1 83.320
current lr 1.00000e-01
Grad=  tensor(2.1954, device='cuda:0')
Epoch: [80][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6617 (0.6617) ([0.377]+[0.285])	Prec@1 88.281 (88.281)
Epoch: [80][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6867 (0.6795) ([0.402]+[0.285])	Prec@1 89.062 (86.866)
Epoch: [80][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5776 (0.7035) ([0.290]+[0.287])	Prec@1 92.188 (86.206)
Epoch: [80][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7302 (0.6936) ([0.446]+[0.284])	Prec@1 86.719 (86.428)
Test: [0/79]	Time 0.130 (0.130)	Loss 1.0976 (1.0976) ([0.812]+[0.285])	Prec@1 74.219 (74.219)
 * Prec@1 77.700
current lr 1.00000e-01
Grad=  tensor(2.2164, device='cuda:0')
Epoch: [81][0/391]	Time 0.173 (0.173)	Data 0.130 (0.130)	Loss 0.7353 (0.7353) ([0.450]+[0.285])	Prec@1 85.938 (85.938)
Epoch: [81][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6887 (0.6887) ([0.402]+[0.287])	Prec@1 87.500 (86.378)
Epoch: [81][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6000 (0.6852) ([0.315]+[0.285])	Prec@1 87.500 (86.606)
Epoch: [81][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7859 (0.6883) ([0.501]+[0.285])	Prec@1 85.156 (86.368)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.9399 (0.9399) ([0.654]+[0.286])	Prec@1 80.469 (80.469)
 * Prec@1 82.220
current lr 1.00000e-01
Grad=  tensor(2.6708, device='cuda:0')
Epoch: [82][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.6610 (0.6610) ([0.375]+[0.286])	Prec@1 86.719 (86.719)
Epoch: [82][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6435 (0.6772) ([0.359]+[0.285])	Prec@1 90.625 (86.974)
Epoch: [82][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.7076 (0.6837) ([0.422]+[0.285])	Prec@1 86.719 (86.657)
Epoch: [82][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7520 (0.6938) ([0.465]+[0.287])	Prec@1 84.375 (86.303)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8386 (0.8386) ([0.553]+[0.285])	Prec@1 85.156 (85.156)
 * Prec@1 83.430
current lr 1.00000e-01
Grad=  tensor(1.4043, device='cuda:0')
Epoch: [83][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.6174 (0.6174) ([0.332]+[0.285])	Prec@1 89.062 (89.062)
Epoch: [83][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7076 (0.6903) ([0.421]+[0.287])	Prec@1 88.281 (86.262)
Epoch: [83][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8270 (0.6947) ([0.540]+[0.287])	Prec@1 81.250 (86.206)
Epoch: [83][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7812 (0.6988) ([0.495]+[0.286])	Prec@1 84.375 (86.104)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.9419 (0.9419) ([0.657]+[0.285])	Prec@1 78.125 (78.125)
 * Prec@1 78.480
current lr 1.00000e-01
Grad=  tensor(2.8995, device='cuda:0')
Epoch: [84][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.8085 (0.8085) ([0.523]+[0.285])	Prec@1 82.812 (82.812)
Epoch: [84][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7113 (0.6778) ([0.426]+[0.285])	Prec@1 84.375 (86.518)
Epoch: [84][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6994 (0.6829) ([0.415]+[0.284])	Prec@1 82.812 (86.252)
Epoch: [84][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6204 (0.6898) ([0.335]+[0.285])	Prec@1 87.500 (86.062)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.9192 (0.9192) ([0.634]+[0.285])	Prec@1 82.812 (82.812)
 * Prec@1 79.390
current lr 1.00000e-01
Grad=  tensor(2.2715, device='cuda:0')
Epoch: [85][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.6497 (0.6497) ([0.365]+[0.285])	Prec@1 89.062 (89.062)
Epoch: [85][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5482 (0.6863) ([0.262]+[0.286])	Prec@1 89.062 (86.092)
Epoch: [85][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7679 (0.6950) ([0.481]+[0.287])	Prec@1 84.375 (85.953)
Epoch: [85][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6288 (0.6946) ([0.344]+[0.285])	Prec@1 90.625 (85.989)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8990 (0.8990) ([0.615]+[0.284])	Prec@1 78.125 (78.125)
 * Prec@1 80.370
current lr 1.00000e-01
Grad=  tensor(2.4241, device='cuda:0')
Epoch: [86][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.7185 (0.7185) ([0.434]+[0.284])	Prec@1 84.375 (84.375)
Epoch: [86][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7320 (0.6546) ([0.449]+[0.283])	Prec@1 82.031 (87.531)
Epoch: [86][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6690 (0.6758) ([0.384]+[0.285])	Prec@1 86.719 (86.602)
Epoch: [86][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.5742 (0.6805) ([0.290]+[0.284])	Prec@1 89.844 (86.436)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7158 (0.7158) ([0.431]+[0.285])	Prec@1 80.469 (80.469)
 * Prec@1 83.400
current lr 1.00000e-01
Grad=  tensor(1.4893, device='cuda:0')
Epoch: [87][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.5453 (0.5453) ([0.260]+[0.285])	Prec@1 91.406 (91.406)
Epoch: [87][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.7960 (0.6760) ([0.511]+[0.285])	Prec@1 86.719 (86.889)
Epoch: [87][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5967 (0.6828) ([0.313]+[0.284])	Prec@1 89.062 (86.699)
Epoch: [87][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6426 (0.6841) ([0.358]+[0.284])	Prec@1 89.062 (86.516)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.6732 (0.6732) ([0.388]+[0.285])	Prec@1 88.281 (88.281)
 * Prec@1 83.490
current lr 1.00000e-01
Grad=  tensor(1.5489, device='cuda:0')
Epoch: [88][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.5566 (0.5566) ([0.271]+[0.285])	Prec@1 91.406 (91.406)
Epoch: [88][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6518 (0.6912) ([0.366]+[0.285])	Prec@1 84.375 (86.100)
Epoch: [88][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7502 (0.6869) ([0.466]+[0.285])	Prec@1 84.375 (86.466)
Epoch: [88][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7142 (0.6905) ([0.430]+[0.284])	Prec@1 87.500 (86.316)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.7864 (0.7864) ([0.501]+[0.285])	Prec@1 82.812 (82.812)
 * Prec@1 83.120
current lr 1.00000e-01
Grad=  tensor(2.1423, device='cuda:0')
Epoch: [89][0/391]	Time 0.170 (0.170)	Data 0.128 (0.128)	Loss 0.6773 (0.6773) ([0.392]+[0.285])	Prec@1 87.500 (87.500)
Epoch: [89][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.7293 (0.6756) ([0.446]+[0.284])	Prec@1 86.719 (86.935)
Epoch: [89][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6074 (0.6824) ([0.325]+[0.283])	Prec@1 89.062 (86.594)
Epoch: [89][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7597 (0.6767) ([0.479]+[0.281])	Prec@1 85.938 (86.750)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8838 (0.8838) ([0.603]+[0.281])	Prec@1 81.250 (81.250)
 * Prec@1 81.590
current lr 1.00000e-01
Grad=  tensor(2.5560, device='cuda:0')
Epoch: [90][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.7339 (0.7339) ([0.453]+[0.281])	Prec@1 85.938 (85.938)
Epoch: [90][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.8508 (0.6774) ([0.568]+[0.283])	Prec@1 80.469 (86.912)
Epoch: [90][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6346 (0.6809) ([0.353]+[0.281])	Prec@1 89.844 (86.618)
Epoch: [90][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7544 (0.6901) ([0.471]+[0.283])	Prec@1 84.375 (86.171)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.8831 (0.8831) ([0.601]+[0.282])	Prec@1 78.906 (78.906)
 * Prec@1 79.340
current lr 1.00000e-01
Grad=  tensor(2.2613, device='cuda:0')
Epoch: [91][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.6057 (0.6057) ([0.323]+[0.282])	Prec@1 89.062 (89.062)
Epoch: [91][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6681 (0.6748) ([0.385]+[0.284])	Prec@1 85.156 (86.533)
Epoch: [91][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7508 (0.6797) ([0.467]+[0.283])	Prec@1 80.469 (86.307)
Epoch: [91][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6411 (0.6886) ([0.358]+[0.284])	Prec@1 85.938 (86.140)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.6455 (0.6455) ([0.363]+[0.282])	Prec@1 85.938 (85.938)
 * Prec@1 85.360
current lr 1.00000e-01
Grad=  tensor(2.2294, device='cuda:0')
Epoch: [92][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.6086 (0.6086) ([0.326]+[0.282])	Prec@1 92.188 (92.188)
Epoch: [92][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.9095 (0.6997) ([0.625]+[0.285])	Prec@1 82.031 (85.876)
Epoch: [92][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6974 (0.6963) ([0.413]+[0.284])	Prec@1 85.938 (86.035)
Epoch: [92][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6403 (0.6913) ([0.357]+[0.283])	Prec@1 83.594 (86.187)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.8197 (0.8197) ([0.537]+[0.283])	Prec@1 80.469 (80.469)
 * Prec@1 78.840
current lr 1.00000e-01
Grad=  tensor(2.0305, device='cuda:0')
Epoch: [93][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.6014 (0.6014) ([0.318]+[0.283])	Prec@1 88.281 (88.281)
Epoch: [93][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5619 (0.6800) ([0.278]+[0.283])	Prec@1 89.062 (86.564)
Epoch: [93][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7052 (0.6778) ([0.422]+[0.283])	Prec@1 89.844 (86.637)
Epoch: [93][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.5982 (0.6853) ([0.315]+[0.283])	Prec@1 88.281 (86.415)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.7434 (0.7434) ([0.459]+[0.284])	Prec@1 80.469 (80.469)
 * Prec@1 83.440
current lr 1.00000e-01
Grad=  tensor(1.2472, device='cuda:0')
Epoch: [94][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.5919 (0.5919) ([0.308]+[0.284])	Prec@1 92.188 (92.188)
Epoch: [94][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6498 (0.6708) ([0.367]+[0.283])	Prec@1 89.062 (86.719)
Epoch: [94][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7029 (0.6923) ([0.418]+[0.284])	Prec@1 85.156 (86.042)
Epoch: [94][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6823 (0.6897) ([0.398]+[0.285])	Prec@1 85.938 (86.169)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.8694 (0.8694) ([0.586]+[0.283])	Prec@1 82.031 (82.031)
 * Prec@1 82.770
current lr 1.00000e-01
Grad=  tensor(2.1477, device='cuda:0')
Epoch: [95][0/391]	Time 0.176 (0.176)	Data 0.134 (0.134)	Loss 0.7173 (0.7173) ([0.434]+[0.283])	Prec@1 84.375 (84.375)
Epoch: [95][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6310 (0.6643) ([0.348]+[0.283])	Prec@1 87.500 (86.966)
Epoch: [95][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6505 (0.6863) ([0.366]+[0.285])	Prec@1 89.062 (86.291)
Epoch: [95][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7944 (0.6869) ([0.511]+[0.284])	Prec@1 83.594 (86.340)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.7955 (0.7955) ([0.512]+[0.283])	Prec@1 82.812 (82.812)
 * Prec@1 82.410
current lr 1.00000e-01
Grad=  tensor(2.1874, device='cuda:0')
Epoch: [96][0/391]	Time 0.171 (0.171)	Data 0.129 (0.129)	Loss 0.6865 (0.6865) ([0.403]+[0.283])	Prec@1 84.375 (84.375)
Epoch: [96][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6490 (0.6833) ([0.366]+[0.283])	Prec@1 85.938 (86.224)
Epoch: [96][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6437 (0.6846) ([0.361]+[0.283])	Prec@1 87.500 (86.318)
Epoch: [96][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.7662 (0.6871) ([0.483]+[0.283])	Prec@1 84.375 (86.207)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.7042 (0.7042) ([0.422]+[0.282])	Prec@1 87.500 (87.500)
 * Prec@1 86.080
current lr 1.00000e-01
Grad=  tensor(1.9908, device='cuda:0')
Epoch: [97][0/391]	Time 0.176 (0.176)	Data 0.132 (0.132)	Loss 0.5950 (0.5950) ([0.313]+[0.282])	Prec@1 87.500 (87.500)
Epoch: [97][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.6711 (0.6829) ([0.389]+[0.282])	Prec@1 89.844 (86.255)
Epoch: [97][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.8339 (0.6890) ([0.552]+[0.282])	Prec@1 83.594 (86.233)
Epoch: [97][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.6271 (0.6877) ([0.345]+[0.282])	Prec@1 87.500 (86.143)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.9723 (0.9723) ([0.690]+[0.282])	Prec@1 79.688 (79.688)
 * Prec@1 78.350
current lr 1.00000e-01
Grad=  tensor(1.6887, device='cuda:0')
Epoch: [98][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.6262 (0.6262) ([0.344]+[0.282])	Prec@1 86.719 (86.719)
Epoch: [98][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.5661 (0.6705) ([0.284]+[0.282])	Prec@1 89.062 (86.742)
Epoch: [98][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6856 (0.6780) ([0.404]+[0.282])	Prec@1 84.375 (86.392)
Epoch: [98][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.6982 (0.6801) ([0.416]+[0.282])	Prec@1 85.938 (86.355)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.8138 (0.8138) ([0.530]+[0.284])	Prec@1 79.688 (79.688)
 * Prec@1 80.580
current lr 1.00000e-01
Grad=  tensor(1.3900, device='cuda:0')
Epoch: [99][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.5851 (0.5851) ([0.301]+[0.284])	Prec@1 89.844 (89.844)
Epoch: [99][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5571 (0.6674) ([0.274]+[0.283])	Prec@1 89.844 (87.051)
Epoch: [99][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.7236 (0.6711) ([0.441]+[0.282])	Prec@1 85.938 (86.948)
Epoch: [99][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.6010 (0.6830) ([0.317]+[0.284])	Prec@1 89.844 (86.459)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.8457 (0.8457) ([0.563]+[0.283])	Prec@1 84.375 (84.375)
 * Prec@1 82.010
current lr 1.00000e-02
Grad=  tensor(4.3463, device='cuda:0')
Epoch: [100][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.9338 (0.9338) ([0.651]+[0.283])	Prec@1 75.000 (75.000)
Epoch: [100][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5113 (0.5443) ([0.266]+[0.245])	Prec@1 92.188 (89.944)
Epoch: [100][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.4524 (0.5088) ([0.210]+[0.243])	Prec@1 92.969 (91.115)
Epoch: [100][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.4322 (0.4888) ([0.192]+[0.240])	Prec@1 95.312 (91.733)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4278 (0.4278) ([0.189]+[0.239])	Prec@1 95.312 (95.312)
 * Prec@1 91.930
current lr 1.00000e-02
Grad=  tensor(1.1927, device='cuda:0')
Epoch: [101][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.4260 (0.4260) ([0.188]+[0.239])	Prec@1 93.750 (93.750)
Epoch: [101][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.5072 (0.4185) ([0.271]+[0.236])	Prec@1 92.969 (93.959)
Epoch: [101][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3285 (0.4153) ([0.094]+[0.234])	Prec@1 96.875 (93.867)
Epoch: [101][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.4377 (0.4127) ([0.206]+[0.232])	Prec@1 91.406 (93.965)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4412 (0.4412) ([0.211]+[0.230])	Prec@1 93.750 (93.750)
 * Prec@1 92.440
current lr 1.00000e-02
Grad=  tensor(1.9706, device='cuda:0')
Epoch: [102][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.3759 (0.3759) ([0.146]+[0.230])	Prec@1 96.094 (96.094)
Epoch: [102][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.4373 (0.3854) ([0.209]+[0.228])	Prec@1 89.844 (94.694)
Epoch: [102][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3879 (0.3859) ([0.162]+[0.226])	Prec@1 94.531 (94.632)
Epoch: [102][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3809 (0.3855) ([0.157]+[0.224])	Prec@1 93.750 (94.599)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4515 (0.4515) ([0.229]+[0.223])	Prec@1 93.750 (93.750)
 * Prec@1 92.150
current lr 1.00000e-02
Grad=  tensor(2.3560, device='cuda:0')
Epoch: [103][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.4216 (0.4216) ([0.199]+[0.223])	Prec@1 92.969 (92.969)
Epoch: [103][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3842 (0.3579) ([0.163]+[0.221])	Prec@1 96.094 (95.312)
Epoch: [103][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss 0.3398 (0.3619) ([0.121]+[0.219])	Prec@1 96.875 (95.200)
Epoch: [103][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3629 (0.3619) ([0.145]+[0.218])	Prec@1 94.531 (95.162)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4185 (0.4185) ([0.202]+[0.216])	Prec@1 93.750 (93.750)
 * Prec@1 92.690
current lr 1.00000e-02
Grad=  tensor(2.0751, device='cuda:0')
Epoch: [104][0/391]	Time 0.170 (0.170)	Data 0.126 (0.126)	Loss 0.3576 (0.3576) ([0.142]+[0.216])	Prec@1 92.969 (92.969)
Epoch: [104][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.4831 (0.3461) ([0.269]+[0.214])	Prec@1 91.406 (95.653)
Epoch: [104][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3133 (0.3450) ([0.100]+[0.213])	Prec@1 98.438 (95.608)
Epoch: [104][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2680 (0.3430) ([0.057]+[0.211])	Prec@1 99.219 (95.640)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4234 (0.4234) ([0.214]+[0.210])	Prec@1 93.750 (93.750)
 * Prec@1 92.920
current lr 1.00000e-02
Grad=  tensor(2.4333, device='cuda:0')
Epoch: [105][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.3865 (0.3865) ([0.177]+[0.210])	Prec@1 94.531 (94.531)
Epoch: [105][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3028 (0.3320) ([0.095]+[0.208])	Prec@1 96.094 (95.792)
Epoch: [105][200/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss 0.3696 (0.3296) ([0.163]+[0.207])	Prec@1 94.531 (95.938)
Epoch: [105][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2509 (0.3287) ([0.046]+[0.205])	Prec@1 98.438 (95.902)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4165 (0.4165) ([0.212]+[0.204])	Prec@1 93.750 (93.750)
 * Prec@1 92.490
current lr 1.00000e-02
Grad=  tensor(2.7125, device='cuda:0')
Epoch: [106][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.3198 (0.3198) ([0.116]+[0.204])	Prec@1 96.094 (96.094)
Epoch: [106][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3593 (0.3125) ([0.157]+[0.203])	Prec@1 93.750 (96.388)
Epoch: [106][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3038 (0.3139) ([0.103]+[0.201])	Prec@1 96.094 (96.288)
Epoch: [106][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2813 (0.3155) ([0.082]+[0.200])	Prec@1 97.656 (96.161)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4198 (0.4198) ([0.221]+[0.199])	Prec@1 92.969 (92.969)
 * Prec@1 93.010
current lr 1.00000e-02
Grad=  tensor(1.0794, device='cuda:0')
Epoch: [107][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.2519 (0.2519) ([0.053]+[0.199])	Prec@1 99.219 (99.219)
Epoch: [107][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2989 (0.3054) ([0.102]+[0.197])	Prec@1 97.656 (96.457)
Epoch: [107][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3538 (0.3057) ([0.158]+[0.196])	Prec@1 93.750 (96.385)
Epoch: [107][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2665 (0.3028) ([0.072]+[0.195])	Prec@1 97.656 (96.468)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4194 (0.4194) ([0.226]+[0.194])	Prec@1 94.531 (94.531)
 * Prec@1 93.210
current lr 1.00000e-02
Grad=  tensor(2.0502, device='cuda:0')
Epoch: [108][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2528 (0.2528) ([0.059]+[0.194])	Prec@1 97.656 (97.656)
Epoch: [108][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2904 (0.2925) ([0.098]+[0.192])	Prec@1 97.656 (96.527)
Epoch: [108][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3328 (0.2926) ([0.142]+[0.191])	Prec@1 95.312 (96.618)
Epoch: [108][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2637 (0.2925) ([0.074]+[0.190])	Prec@1 96.875 (96.621)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4125 (0.4125) ([0.224]+[0.189])	Prec@1 94.531 (94.531)
 * Prec@1 92.600
current lr 1.00000e-02
Grad=  tensor(2.4367, device='cuda:0')
Epoch: [109][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2677 (0.2677) ([0.079]+[0.189])	Prec@1 96.094 (96.094)
Epoch: [109][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3342 (0.2759) ([0.147]+[0.188])	Prec@1 93.750 (97.092)
Epoch: [109][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3209 (0.2754) ([0.134]+[0.186])	Prec@1 94.531 (97.089)
Epoch: [109][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2158 (0.2778) ([0.030]+[0.185])	Prec@1 100.000 (96.994)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4646 (0.4646) ([0.280]+[0.184])	Prec@1 93.750 (93.750)
 * Prec@1 92.610
current lr 1.00000e-02
Grad=  tensor(1.3019, device='cuda:0')
Epoch: [110][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2352 (0.2352) ([0.051]+[0.184])	Prec@1 99.219 (99.219)
Epoch: [110][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2609 (0.2748) ([0.077]+[0.183])	Prec@1 98.438 (96.937)
Epoch: [110][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3189 (0.2751) ([0.136]+[0.183])	Prec@1 96.094 (96.945)
Epoch: [110][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2494 (0.2743) ([0.068]+[0.182])	Prec@1 98.438 (96.940)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3312 (0.3312) ([0.151]+[0.181])	Prec@1 96.094 (96.094)
 * Prec@1 92.820
current lr 1.00000e-02
Grad=  tensor(4.5510, device='cuda:0')
Epoch: [111][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.3057 (0.3057) ([0.125]+[0.181])	Prec@1 95.312 (95.312)
Epoch: [111][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3023 (0.2694) ([0.123]+[0.180])	Prec@1 96.875 (97.030)
Epoch: [111][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2309 (0.2672) ([0.052]+[0.178])	Prec@1 97.656 (96.945)
Epoch: [111][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3513 (0.2689) ([0.174]+[0.178])	Prec@1 91.406 (96.870)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4142 (0.4142) ([0.237]+[0.177])	Prec@1 93.750 (93.750)
 * Prec@1 92.290
current lr 1.00000e-02
Grad=  tensor(4.7184, device='cuda:0')
Epoch: [112][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2604 (0.2604) ([0.084]+[0.177])	Prec@1 96.094 (96.094)
Epoch: [112][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2863 (0.2577) ([0.111]+[0.176])	Prec@1 96.094 (97.223)
Epoch: [112][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2259 (0.2579) ([0.051]+[0.175])	Prec@1 98.438 (97.201)
Epoch: [112][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2318 (0.2582) ([0.058]+[0.174])	Prec@1 98.438 (97.194)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.4657 (0.4657) ([0.292]+[0.173])	Prec@1 90.625 (90.625)
 * Prec@1 92.300
current lr 1.00000e-02
Grad=  tensor(5.3107, device='cuda:0')
Epoch: [113][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.3208 (0.3208) ([0.148]+[0.173])	Prec@1 94.531 (94.531)
Epoch: [113][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2135 (0.2487) ([0.041]+[0.172])	Prec@1 98.438 (97.633)
Epoch: [113][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2025 (0.2576) ([0.031]+[0.172])	Prec@1 100.000 (97.205)
Epoch: [113][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2911 (0.2584) ([0.120]+[0.171])	Prec@1 95.312 (97.124)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3904 (0.3904) ([0.220]+[0.170])	Prec@1 91.406 (91.406)
 * Prec@1 92.760
current lr 1.00000e-02
Grad=  tensor(0.9166, device='cuda:0')
Epoch: [114][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1942 (0.1942) ([0.024]+[0.170])	Prec@1 100.000 (100.000)
Epoch: [114][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2416 (0.2500) ([0.072]+[0.170])	Prec@1 97.656 (97.231)
Epoch: [114][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2504 (0.2491) ([0.082]+[0.169])	Prec@1 97.656 (97.248)
Epoch: [114][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2667 (0.2509) ([0.099]+[0.168])	Prec@1 96.094 (97.173)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3330 (0.3330) ([0.166]+[0.167])	Prec@1 94.531 (94.531)
 * Prec@1 92.910
current lr 1.00000e-02
Grad=  tensor(1.2717, device='cuda:0')
Epoch: [115][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2083 (0.2083) ([0.041]+[0.167])	Prec@1 99.219 (99.219)
Epoch: [115][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2209 (0.2445) ([0.055]+[0.166])	Prec@1 98.438 (97.347)
Epoch: [115][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2172 (0.2489) ([0.051]+[0.166])	Prec@1 97.656 (97.143)
Epoch: [115][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2649 (0.2514) ([0.100]+[0.165])	Prec@1 97.656 (97.090)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3503 (0.3503) ([0.186]+[0.165])	Prec@1 96.094 (96.094)
 * Prec@1 92.510
current lr 1.00000e-02
Grad=  tensor(4.9096, device='cuda:0')
Epoch: [116][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.2536 (0.2536) ([0.089]+[0.165])	Prec@1 94.531 (94.531)
Epoch: [116][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2153 (0.2378) ([0.051]+[0.164])	Prec@1 98.438 (97.409)
Epoch: [116][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2913 (0.2421) ([0.128]+[0.163])	Prec@1 95.312 (97.252)
Epoch: [116][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2353 (0.2462) ([0.073]+[0.163])	Prec@1 96.875 (97.096)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4051 (0.4051) ([0.243]+[0.162])	Prec@1 94.531 (94.531)
 * Prec@1 91.460
current lr 1.00000e-02
Grad=  tensor(2.6826, device='cuda:0')
Epoch: [117][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2019 (0.2019) ([0.040]+[0.162])	Prec@1 98.438 (98.438)
Epoch: [117][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2704 (0.2431) ([0.109]+[0.162])	Prec@1 95.312 (97.231)
Epoch: [117][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2644 (0.2498) ([0.103]+[0.161])	Prec@1 95.312 (96.933)
Epoch: [117][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2693 (0.2480) ([0.108]+[0.161])	Prec@1 96.875 (97.007)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3925 (0.3925) ([0.232]+[0.160])	Prec@1 91.406 (91.406)
 * Prec@1 92.400
current lr 1.00000e-02
Grad=  tensor(3.8069, device='cuda:0')
Epoch: [118][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2105 (0.2105) ([0.050]+[0.160])	Prec@1 97.656 (97.656)
Epoch: [118][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2668 (0.2397) ([0.107]+[0.160])	Prec@1 95.312 (97.262)
Epoch: [118][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2626 (0.2383) ([0.103]+[0.159])	Prec@1 96.094 (97.306)
Epoch: [118][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2395 (0.2409) ([0.081]+[0.159])	Prec@1 98.438 (97.199)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4704 (0.4704) ([0.312]+[0.158])	Prec@1 91.406 (91.406)
 * Prec@1 91.780
current lr 1.00000e-02
Grad=  tensor(1.9857, device='cuda:0')
Epoch: [119][0/391]	Time 0.168 (0.168)	Data 0.127 (0.127)	Loss 0.1926 (0.1926) ([0.034]+[0.158])	Prec@1 100.000 (100.000)
Epoch: [119][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2340 (0.2407) ([0.076]+[0.158])	Prec@1 96.875 (97.084)
Epoch: [119][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2501 (0.2448) ([0.092]+[0.158])	Prec@1 96.875 (96.992)
Epoch: [119][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2366 (0.2461) ([0.079]+[0.157])	Prec@1 96.875 (96.932)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3740 (0.3740) ([0.217]+[0.157])	Prec@1 94.531 (94.531)
 * Prec@1 91.640
current lr 1.00000e-02
Grad=  tensor(5.2356, device='cuda:0')
Epoch: [120][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.2297 (0.2297) ([0.073]+[0.157])	Prec@1 96.875 (96.875)
Epoch: [120][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2478 (0.2398) ([0.091]+[0.157])	Prec@1 96.875 (97.161)
Epoch: [120][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2151 (0.2435) ([0.059]+[0.156])	Prec@1 98.438 (96.992)
Epoch: [120][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3101 (0.2448) ([0.154]+[0.156])	Prec@1 92.969 (96.942)
Test: [0/79]	Time 0.132 (0.132)	Loss 0.4386 (0.4386) ([0.283]+[0.156])	Prec@1 91.406 (91.406)
 * Prec@1 92.220
current lr 1.00000e-02
Grad=  tensor(1.6420, device='cuda:0')
Epoch: [121][0/391]	Time 0.171 (0.171)	Data 0.130 (0.130)	Loss 0.1809 (0.1809) ([0.025]+[0.156])	Prec@1 100.000 (100.000)
Epoch: [121][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2000 (0.2340) ([0.045]+[0.155])	Prec@1 99.219 (97.262)
Epoch: [121][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2500 (0.2359) ([0.095]+[0.155])	Prec@1 96.094 (97.182)
Epoch: [121][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1874 (0.2384) ([0.033]+[0.154])	Prec@1 100.000 (97.103)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4621 (0.4621) ([0.308]+[0.154])	Prec@1 92.188 (92.188)
 * Prec@1 91.070
current lr 1.00000e-02
Grad=  tensor(7.8763, device='cuda:0')
Epoch: [122][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2471 (0.2471) ([0.093]+[0.154])	Prec@1 96.094 (96.094)
Epoch: [122][100/391]	Time 0.035 (0.037)	Data 0.000 (0.001)	Loss 0.2186 (0.2404) ([0.065]+[0.154])	Prec@1 98.438 (96.991)
Epoch: [122][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2167 (0.2422) ([0.063]+[0.154])	Prec@1 97.656 (96.918)
Epoch: [122][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2422 (0.2414) ([0.089]+[0.153])	Prec@1 97.656 (96.963)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4097 (0.4097) ([0.257]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 91.870
current lr 1.00000e-02
Grad=  tensor(2.0262, device='cuda:0')
Epoch: [123][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.1992 (0.1992) ([0.046]+[0.153])	Prec@1 99.219 (99.219)
Epoch: [123][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3164 (0.2404) ([0.164]+[0.153])	Prec@1 92.969 (97.045)
Epoch: [123][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2073 (0.2433) ([0.055]+[0.153])	Prec@1 99.219 (96.961)
Epoch: [123][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2537 (0.2445) ([0.101]+[0.152])	Prec@1 96.875 (96.870)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3842 (0.3842) ([0.232]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 91.800
current lr 1.00000e-02
Grad=  tensor(5.3907, device='cuda:0')
Epoch: [124][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2518 (0.2518) ([0.099]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [124][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2212 (0.2359) ([0.069]+[0.152])	Prec@1 96.094 (97.099)
Epoch: [124][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3049 (0.2435) ([0.153]+[0.152])	Prec@1 93.750 (96.789)
Epoch: [124][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2176 (0.2442) ([0.066]+[0.152])	Prec@1 96.875 (96.782)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3334 (0.3334) ([0.182]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 92.040
current lr 1.00000e-02
Grad=  tensor(8.0411, device='cuda:0')
Epoch: [125][0/391]	Time 0.162 (0.162)	Data 0.121 (0.121)	Loss 0.2559 (0.2559) ([0.104]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [125][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2112 (0.2346) ([0.060]+[0.151])	Prec@1 97.656 (97.262)
Epoch: [125][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1979 (0.2354) ([0.047]+[0.151])	Prec@1 99.219 (97.178)
Epoch: [125][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2307 (0.2397) ([0.080]+[0.151])	Prec@1 96.875 (96.961)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.5903 (0.5903) ([0.439]+[0.151])	Prec@1 87.500 (87.500)
 * Prec@1 90.200
current lr 1.00000e-02
Grad=  tensor(9.0667, device='cuda:0')
Epoch: [126][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2595 (0.2595) ([0.109]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [126][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1921 (0.2393) ([0.041]+[0.151])	Prec@1 99.219 (97.123)
Epoch: [126][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2044 (0.2389) ([0.054]+[0.150])	Prec@1 98.438 (97.058)
Epoch: [126][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2404 (0.2439) ([0.090]+[0.150])	Prec@1 97.656 (96.841)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5158 (0.5158) ([0.365]+[0.150])	Prec@1 89.844 (89.844)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(2.0962, device='cuda:0')
Epoch: [127][0/391]	Time 0.166 (0.166)	Data 0.122 (0.122)	Loss 0.1824 (0.1824) ([0.032]+[0.150])	Prec@1 99.219 (99.219)
Epoch: [127][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1775 (0.2384) ([0.027]+[0.150])	Prec@1 99.219 (97.061)
Epoch: [127][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2494 (0.2376) ([0.099]+[0.150])	Prec@1 96.875 (97.151)
Epoch: [127][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2512 (0.2408) ([0.101]+[0.150])	Prec@1 96.094 (96.940)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3910 (0.3910) ([0.241]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 91.130
current lr 1.00000e-02
Grad=  tensor(6.7837, device='cuda:0')
Epoch: [128][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2531 (0.2531) ([0.103]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [128][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2707 (0.2451) ([0.121]+[0.150])	Prec@1 95.312 (96.790)
Epoch: [128][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2422 (0.2477) ([0.092]+[0.150])	Prec@1 96.094 (96.653)
Epoch: [128][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2425 (0.2486) ([0.093]+[0.150])	Prec@1 95.312 (96.639)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4180 (0.4180) ([0.268]+[0.150])	Prec@1 93.750 (93.750)
 * Prec@1 91.210
current lr 1.00000e-02
Grad=  tensor(7.5002, device='cuda:0')
Epoch: [129][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.2869 (0.2869) ([0.137]+[0.150])	Prec@1 95.312 (95.312)
Epoch: [129][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3556 (0.2367) ([0.206]+[0.149])	Prec@1 91.406 (97.030)
Epoch: [129][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2394 (0.2389) ([0.090]+[0.149])	Prec@1 96.094 (96.898)
Epoch: [129][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2951 (0.2430) ([0.146]+[0.149])	Prec@1 95.312 (96.761)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4597 (0.4597) ([0.310]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 91.080
current lr 1.00000e-02
Grad=  tensor(4.7645, device='cuda:0')
Epoch: [130][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2308 (0.2308) ([0.081]+[0.150])	Prec@1 96.875 (96.875)
Epoch: [130][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2498 (0.2310) ([0.101]+[0.149])	Prec@1 96.875 (97.277)
Epoch: [130][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2349 (0.2359) ([0.086]+[0.149])	Prec@1 96.094 (96.995)
Epoch: [130][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3057 (0.2413) ([0.156]+[0.149])	Prec@1 94.531 (96.800)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5360 (0.5360) ([0.387]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 91.420
current lr 1.00000e-02
Grad=  tensor(13.3985, device='cuda:0')
Epoch: [131][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.3451 (0.3451) ([0.196]+[0.149])	Prec@1 92.188 (92.188)
Epoch: [131][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2397 (0.2355) ([0.091]+[0.149])	Prec@1 98.438 (97.037)
Epoch: [131][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2654 (0.2339) ([0.117]+[0.149])	Prec@1 95.312 (97.108)
Epoch: [131][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2438 (0.2393) ([0.095]+[0.149])	Prec@1 97.656 (96.930)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3919 (0.3919) ([0.243]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.570
current lr 1.00000e-02
Grad=  tensor(10.2809, device='cuda:0')
Epoch: [132][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2570 (0.2570) ([0.108]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [132][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2723 (0.2263) ([0.124]+[0.149])	Prec@1 95.312 (97.378)
Epoch: [132][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2330 (0.2305) ([0.084]+[0.149])	Prec@1 96.875 (97.190)
Epoch: [132][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2994 (0.2368) ([0.151]+[0.149])	Prec@1 94.531 (96.937)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3969 (0.3969) ([0.248]+[0.149])	Prec@1 93.750 (93.750)
 * Prec@1 91.700
current lr 1.00000e-02
Grad=  tensor(7.1832, device='cuda:0')
Epoch: [133][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.2351 (0.2351) ([0.086]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [133][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2626 (0.2353) ([0.114]+[0.149])	Prec@1 96.094 (97.076)
Epoch: [133][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2993 (0.2410) ([0.151]+[0.149])	Prec@1 92.969 (96.821)
Epoch: [133][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2118 (0.2442) ([0.063]+[0.149])	Prec@1 98.438 (96.704)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5601 (0.5601) ([0.411]+[0.149])	Prec@1 89.062 (89.062)
 * Prec@1 90.670
current lr 1.00000e-02
Grad=  tensor(9.1465, device='cuda:0')
Epoch: [134][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2501 (0.2501) ([0.101]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [134][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2100 (0.2344) ([0.061]+[0.149])	Prec@1 97.656 (97.115)
Epoch: [134][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2703 (0.2401) ([0.121]+[0.149])	Prec@1 96.094 (96.801)
Epoch: [134][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2474 (0.2430) ([0.099]+[0.149])	Prec@1 95.312 (96.675)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4685 (0.4685) ([0.319]+[0.149])	Prec@1 93.750 (93.750)
 * Prec@1 91.740
current lr 1.00000e-02
Grad=  tensor(4.2721, device='cuda:0')
Epoch: [135][0/391]	Time 0.171 (0.171)	Data 0.127 (0.127)	Loss 0.2219 (0.2219) ([0.073]+[0.149])	Prec@1 98.438 (98.438)
Epoch: [135][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1987 (0.2428) ([0.050]+[0.149])	Prec@1 98.438 (96.774)
Epoch: [135][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2716 (0.2463) ([0.123]+[0.149])	Prec@1 96.094 (96.685)
Epoch: [135][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3228 (0.2502) ([0.174]+[0.149])	Prec@1 92.969 (96.519)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3758 (0.3758) ([0.227]+[0.149])	Prec@1 92.188 (92.188)
 * Prec@1 90.980
current lr 1.00000e-02
Grad=  tensor(7.0557, device='cuda:0')
Epoch: [136][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2510 (0.2510) ([0.102]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [136][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2204 (0.2403) ([0.072]+[0.149])	Prec@1 98.438 (96.790)
Epoch: [136][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2729 (0.2449) ([0.124]+[0.149])	Prec@1 96.875 (96.704)
Epoch: [136][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2083 (0.2440) ([0.059]+[0.149])	Prec@1 97.656 (96.758)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3858 (0.3858) ([0.237]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.760
current lr 1.00000e-02
Grad=  tensor(5.4966, device='cuda:0')
Epoch: [137][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2019 (0.2019) ([0.053]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [137][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2693 (0.2378) ([0.121]+[0.149])	Prec@1 96.094 (96.983)
Epoch: [137][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1837 (0.2364) ([0.035]+[0.149])	Prec@1 98.438 (97.011)
Epoch: [137][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2395 (0.2407) ([0.091]+[0.148])	Prec@1 96.094 (96.831)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3810 (0.3810) ([0.232]+[0.149])	Prec@1 92.188 (92.188)
 * Prec@1 91.760
current lr 1.00000e-02
Grad=  tensor(16.6842, device='cuda:0')
Epoch: [138][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.3255 (0.3255) ([0.177]+[0.149])	Prec@1 92.969 (92.969)
Epoch: [138][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2179 (0.2352) ([0.069]+[0.148])	Prec@1 98.438 (97.115)
Epoch: [138][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3011 (0.2366) ([0.153]+[0.148])	Prec@1 94.531 (96.992)
Epoch: [138][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3099 (0.2382) ([0.162]+[0.148])	Prec@1 93.750 (96.904)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4082 (0.4082) ([0.260]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 91.830
current lr 1.00000e-02
Grad=  tensor(4.6410, device='cuda:0')
Epoch: [139][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2377 (0.2377) ([0.089]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [139][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1698 (0.2253) ([0.022]+[0.148])	Prec@1 100.000 (97.393)
Epoch: [139][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1917 (0.2348) ([0.043]+[0.148])	Prec@1 99.219 (97.023)
Epoch: [139][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2191 (0.2392) ([0.071]+[0.148])	Prec@1 97.656 (96.917)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4029 (0.4029) ([0.255]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 91.690
current lr 1.00000e-02
Grad=  tensor(7.6770, device='cuda:0')
Epoch: [140][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2504 (0.2504) ([0.102]+[0.148])	Prec@1 96.094 (96.094)
Epoch: [140][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2211 (0.2313) ([0.073]+[0.148])	Prec@1 99.219 (97.123)
Epoch: [140][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2596 (0.2371) ([0.112]+[0.148])	Prec@1 95.312 (97.023)
Epoch: [140][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2717 (0.2445) ([0.124]+[0.148])	Prec@1 96.875 (96.758)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4581 (0.4581) ([0.310]+[0.148])	Prec@1 92.969 (92.969)
 * Prec@1 89.680
current lr 1.00000e-02
Grad=  tensor(5.2797, device='cuda:0')
Epoch: [141][0/391]	Time 0.170 (0.170)	Data 0.127 (0.127)	Loss 0.2106 (0.2106) ([0.062]+[0.148])	Prec@1 97.656 (97.656)
Epoch: [141][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2252 (0.2400) ([0.077]+[0.148])	Prec@1 96.875 (96.736)
Epoch: [141][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1968 (0.2405) ([0.048]+[0.148])	Prec@1 99.219 (96.762)
Epoch: [141][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2609 (0.2389) ([0.113]+[0.148])	Prec@1 95.312 (96.828)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.5088 (0.5088) ([0.360]+[0.148])	Prec@1 91.406 (91.406)
 * Prec@1 91.060
current lr 1.00000e-02
Grad=  tensor(6.4825, device='cuda:0')
Epoch: [142][0/391]	Time 0.172 (0.172)	Data 0.128 (0.128)	Loss 0.2420 (0.2420) ([0.094]+[0.148])	Prec@1 96.875 (96.875)
Epoch: [142][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2136 (0.2315) ([0.065]+[0.148])	Prec@1 98.438 (97.215)
Epoch: [142][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2416 (0.2394) ([0.093]+[0.148])	Prec@1 96.875 (96.972)
Epoch: [142][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2538 (0.2390) ([0.106]+[0.148])	Prec@1 96.094 (96.942)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3684 (0.3684) ([0.220]+[0.148])	Prec@1 92.188 (92.188)
 * Prec@1 90.270
current lr 1.00000e-02
Grad=  tensor(4.3252, device='cuda:0')
Epoch: [143][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2065 (0.2065) ([0.058]+[0.148])	Prec@1 99.219 (99.219)
Epoch: [143][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2408 (0.2400) ([0.092]+[0.148])	Prec@1 96.094 (96.921)
Epoch: [143][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2061 (0.2426) ([0.057]+[0.149])	Prec@1 98.438 (96.828)
Epoch: [143][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2738 (0.2449) ([0.125]+[0.149])	Prec@1 96.094 (96.717)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4285 (0.4285) ([0.280]+[0.149])	Prec@1 92.188 (92.188)
 * Prec@1 90.760
current lr 1.00000e-02
Grad=  tensor(4.2867, device='cuda:0')
Epoch: [144][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2003 (0.2003) ([0.052]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [144][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1829 (0.2336) ([0.034]+[0.149])	Prec@1 99.219 (97.223)
Epoch: [144][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2758 (0.2366) ([0.127]+[0.149])	Prec@1 96.875 (97.069)
Epoch: [144][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2156 (0.2399) ([0.067]+[0.149])	Prec@1 99.219 (96.961)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.4038 (0.4038) ([0.255]+[0.148])	Prec@1 92.969 (92.969)
 * Prec@1 91.640
current lr 1.00000e-02
Grad=  tensor(10.4028, device='cuda:0')
Epoch: [145][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2527 (0.2527) ([0.104]+[0.148])	Prec@1 94.531 (94.531)
Epoch: [145][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2343 (0.2376) ([0.086]+[0.149])	Prec@1 94.531 (97.037)
Epoch: [145][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2297 (0.2436) ([0.081]+[0.149])	Prec@1 95.312 (96.789)
Epoch: [145][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2512 (0.2435) ([0.102]+[0.149])	Prec@1 96.875 (96.763)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4305 (0.4305) ([0.282]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 90.520
current lr 1.00000e-02
Grad=  tensor(9.2998, device='cuda:0')
Epoch: [146][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2547 (0.2547) ([0.106]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [146][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1960 (0.2437) ([0.047]+[0.149])	Prec@1 99.219 (96.860)
Epoch: [146][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2249 (0.2393) ([0.076]+[0.149])	Prec@1 96.875 (96.906)
Epoch: [146][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3333 (0.2455) ([0.184]+[0.149])	Prec@1 95.312 (96.675)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4338 (0.4338) ([0.285]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.960
current lr 1.00000e-02
Grad=  tensor(18.9257, device='cuda:0')
Epoch: [147][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.3410 (0.3410) ([0.192]+[0.149])	Prec@1 95.312 (95.312)
Epoch: [147][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2477 (0.2337) ([0.099]+[0.149])	Prec@1 96.875 (97.324)
Epoch: [147][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1700 (0.2353) ([0.021]+[0.149])	Prec@1 100.000 (97.143)
Epoch: [147][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2178 (0.2430) ([0.069]+[0.149])	Prec@1 97.656 (96.867)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2989 (0.2989) ([0.150]+[0.149])	Prec@1 95.312 (95.312)
 * Prec@1 91.270
current lr 1.00000e-02
Grad=  tensor(4.2928, device='cuda:0')
Epoch: [148][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.2108 (0.2108) ([0.062]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [148][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2218 (0.2401) ([0.073]+[0.149])	Prec@1 96.875 (96.890)
Epoch: [148][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2524 (0.2387) ([0.103]+[0.149])	Prec@1 96.875 (96.914)
Epoch: [148][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2661 (0.2400) ([0.117]+[0.149])	Prec@1 95.312 (96.901)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5468 (0.5468) ([0.398]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 90.840
current lr 1.00000e-02
Grad=  tensor(3.6742, device='cuda:0')
Epoch: [149][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1993 (0.1993) ([0.050]+[0.149])	Prec@1 97.656 (97.656)
Epoch: [149][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2153 (0.2367) ([0.066]+[0.149])	Prec@1 96.875 (97.045)
Epoch: [149][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2433 (0.2416) ([0.094]+[0.149])	Prec@1 96.094 (96.879)
Epoch: [149][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2560 (0.2448) ([0.107]+[0.149])	Prec@1 96.875 (96.779)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4756 (0.4756) ([0.326]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.130
current lr 1.00000e-02
Grad=  tensor(7.7391, device='cuda:0')
Epoch: [150][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2401 (0.2401) ([0.091]+[0.149])	Prec@1 96.094 (96.094)
Epoch: [150][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2404 (0.2353) ([0.091]+[0.149])	Prec@1 95.312 (97.014)
Epoch: [150][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1882 (0.2361) ([0.039]+[0.149])	Prec@1 99.219 (97.108)
Epoch: [150][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2621 (0.2383) ([0.113]+[0.149])	Prec@1 97.656 (96.979)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3727 (0.3727) ([0.223]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.860
current lr 1.00000e-02
Grad=  tensor(7.4180, device='cuda:0')
Epoch: [151][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2629 (0.2629) ([0.113]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [151][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2467 (0.2397) ([0.097]+[0.149])	Prec@1 95.312 (97.014)
Epoch: [151][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2368 (0.2372) ([0.088]+[0.149])	Prec@1 96.875 (97.023)
Epoch: [151][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2487 (0.2410) ([0.099]+[0.149])	Prec@1 95.312 (96.846)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4565 (0.4565) ([0.307]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 91.810
current lr 1.00000e-02
Grad=  tensor(2.0505, device='cuda:0')
Epoch: [152][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1786 (0.1786) ([0.029]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [152][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2419 (0.2337) ([0.093]+[0.149])	Prec@1 96.094 (97.169)
Epoch: [152][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2164 (0.2344) ([0.067]+[0.149])	Prec@1 96.875 (97.186)
Epoch: [152][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1977 (0.2349) ([0.048]+[0.149])	Prec@1 98.438 (97.145)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4682 (0.4682) ([0.319]+[0.149])	Prec@1 91.406 (91.406)
 * Prec@1 92.050
current lr 1.00000e-02
Grad=  tensor(4.6990, device='cuda:0')
Epoch: [153][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2151 (0.2151) ([0.066]+[0.149])	Prec@1 96.875 (96.875)
Epoch: [153][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2144 (0.2299) ([0.065]+[0.149])	Prec@1 97.656 (97.184)
Epoch: [153][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2922 (0.2379) ([0.143]+[0.149])	Prec@1 95.312 (96.906)
Epoch: [153][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2412 (0.2400) ([0.092]+[0.149])	Prec@1 98.438 (96.937)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3958 (0.3958) ([0.246]+[0.149])	Prec@1 92.969 (92.969)
 * Prec@1 91.270
current lr 1.00000e-02
Grad=  tensor(4.2441, device='cuda:0')
Epoch: [154][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.2025 (0.2025) ([0.053]+[0.149])	Prec@1 98.438 (98.438)
Epoch: [154][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2161 (0.2387) ([0.067]+[0.149])	Prec@1 97.656 (97.123)
Epoch: [154][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2437 (0.2384) ([0.094]+[0.149])	Prec@1 96.875 (97.073)
Epoch: [154][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2294 (0.2379) ([0.080]+[0.149])	Prec@1 96.094 (97.067)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4446 (0.4446) ([0.295]+[0.149])	Prec@1 90.625 (90.625)
 * Prec@1 91.680
current lr 1.00000e-02
Grad=  tensor(3.0321, device='cuda:0')
Epoch: [155][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1849 (0.1849) ([0.035]+[0.149])	Prec@1 99.219 (99.219)
Epoch: [155][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2361 (0.2413) ([0.087]+[0.150])	Prec@1 96.094 (96.782)
Epoch: [155][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2314 (0.2421) ([0.082]+[0.149])	Prec@1 96.094 (96.801)
Epoch: [155][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2890 (0.2398) ([0.140]+[0.149])	Prec@1 96.875 (96.927)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4183 (0.4183) ([0.269]+[0.149])	Prec@1 92.188 (92.188)
 * Prec@1 90.770
current lr 1.00000e-02
Grad=  tensor(2.6577, device='cuda:0')
Epoch: [156][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.1886 (0.1886) ([0.039]+[0.149])	Prec@1 98.438 (98.438)
Epoch: [156][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2366 (0.2346) ([0.087]+[0.149])	Prec@1 96.875 (97.006)
Epoch: [156][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2664 (0.2353) ([0.117]+[0.149])	Prec@1 94.531 (96.984)
Epoch: [156][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2824 (0.2377) ([0.133]+[0.149])	Prec@1 94.531 (96.955)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4859 (0.4859) ([0.336]+[0.150])	Prec@1 89.844 (89.844)
 * Prec@1 90.740
current lr 1.00000e-02
Grad=  tensor(7.4183, device='cuda:0')
Epoch: [157][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2147 (0.2147) ([0.065]+[0.150])	Prec@1 96.875 (96.875)
Epoch: [157][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2481 (0.2398) ([0.099]+[0.150])	Prec@1 96.875 (96.798)
Epoch: [157][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2077 (0.2386) ([0.058]+[0.149])	Prec@1 97.656 (96.929)
Epoch: [157][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2433 (0.2425) ([0.094]+[0.150])	Prec@1 97.656 (96.787)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4489 (0.4489) ([0.299]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 91.380
current lr 1.00000e-02
Grad=  tensor(2.2258, device='cuda:0')
Epoch: [158][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.1841 (0.1841) ([0.034]+[0.150])	Prec@1 100.000 (100.000)
Epoch: [158][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2156 (0.2268) ([0.066]+[0.149])	Prec@1 97.656 (97.447)
Epoch: [158][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2179 (0.2303) ([0.069]+[0.149])	Prec@1 96.094 (97.295)
Epoch: [158][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2293 (0.2378) ([0.080]+[0.150])	Prec@1 96.875 (97.051)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4988 (0.4988) ([0.349]+[0.150])	Prec@1 93.750 (93.750)
 * Prec@1 90.980
current lr 1.00000e-02
Grad=  tensor(8.5370, device='cuda:0')
Epoch: [159][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2244 (0.2244) ([0.075]+[0.150])	Prec@1 96.094 (96.094)
Epoch: [159][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2147 (0.2329) ([0.065]+[0.150])	Prec@1 98.438 (97.146)
Epoch: [159][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2619 (0.2331) ([0.112]+[0.149])	Prec@1 96.875 (97.132)
Epoch: [159][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2542 (0.2393) ([0.104]+[0.150])	Prec@1 96.094 (96.906)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4754 (0.4754) ([0.326]+[0.150])	Prec@1 89.844 (89.844)
 * Prec@1 91.100
current lr 1.00000e-02
Grad=  tensor(7.3815, device='cuda:0')
Epoch: [160][0/391]	Time 0.171 (0.171)	Data 0.121 (0.121)	Loss 0.2192 (0.2192) ([0.069]+[0.150])	Prec@1 97.656 (97.656)
Epoch: [160][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2127 (0.2323) ([0.063]+[0.150])	Prec@1 96.094 (97.215)
Epoch: [160][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2128 (0.2375) ([0.063]+[0.150])	Prec@1 98.438 (97.054)
Epoch: [160][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2401 (0.2393) ([0.090]+[0.150])	Prec@1 96.875 (96.981)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5250 (0.5250) ([0.375]+[0.150])	Prec@1 91.406 (91.406)
 * Prec@1 91.500
current lr 1.00000e-02
Grad=  tensor(2.0288, device='cuda:0')
Epoch: [161][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.1821 (0.1821) ([0.032]+[0.150])	Prec@1 100.000 (100.000)
Epoch: [161][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1877 (0.2359) ([0.038]+[0.150])	Prec@1 99.219 (97.076)
Epoch: [161][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2464 (0.2390) ([0.096]+[0.150])	Prec@1 95.312 (96.957)
Epoch: [161][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2133 (0.2459) ([0.063]+[0.150])	Prec@1 97.656 (96.696)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4711 (0.4711) ([0.321]+[0.150])	Prec@1 89.062 (89.062)
 * Prec@1 90.120
current lr 1.00000e-02
Grad=  tensor(19.1867, device='cuda:0')
Epoch: [162][0/391]	Time 0.162 (0.162)	Data 0.120 (0.120)	Loss 0.3238 (0.3238) ([0.173]+[0.150])	Prec@1 92.969 (92.969)
Epoch: [162][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1999 (0.2382) ([0.049]+[0.150])	Prec@1 99.219 (96.999)
Epoch: [162][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2842 (0.2392) ([0.134]+[0.150])	Prec@1 96.094 (96.972)
Epoch: [162][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2531 (0.2397) ([0.103]+[0.150])	Prec@1 96.875 (96.917)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4421 (0.4421) ([0.292]+[0.150])	Prec@1 90.625 (90.625)
 * Prec@1 91.390
current lr 1.00000e-02
Grad=  tensor(5.1994, device='cuda:0')
Epoch: [163][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2194 (0.2194) ([0.069]+[0.150])	Prec@1 98.438 (98.438)
Epoch: [163][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2404 (0.2379) ([0.090]+[0.150])	Prec@1 96.094 (97.076)
Epoch: [163][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3143 (0.2372) ([0.164]+[0.150])	Prec@1 94.531 (97.112)
Epoch: [163][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1956 (0.2378) ([0.045]+[0.150])	Prec@1 99.219 (97.072)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4473 (0.4473) ([0.297]+[0.150])	Prec@1 93.750 (93.750)
 * Prec@1 91.420
current lr 1.00000e-02
Grad=  tensor(10.9966, device='cuda:0')
Epoch: [164][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2526 (0.2526) ([0.102]+[0.150])	Prec@1 95.312 (95.312)
Epoch: [164][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2098 (0.2397) ([0.059]+[0.150])	Prec@1 96.875 (96.774)
Epoch: [164][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2324 (0.2405) ([0.082]+[0.150])	Prec@1 96.875 (96.782)
Epoch: [164][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1951 (0.2459) ([0.045]+[0.151])	Prec@1 98.438 (96.589)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4611 (0.4611) ([0.310]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 92.160
current lr 1.00000e-02
Grad=  tensor(9.7148, device='cuda:0')
Epoch: [165][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2616 (0.2616) ([0.111]+[0.151])	Prec@1 95.312 (95.312)
Epoch: [165][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2131 (0.2354) ([0.063]+[0.151])	Prec@1 98.438 (97.146)
Epoch: [165][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2109 (0.2358) ([0.060]+[0.151])	Prec@1 97.656 (97.097)
Epoch: [165][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2307 (0.2391) ([0.080]+[0.151])	Prec@1 96.875 (96.924)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4245 (0.4245) ([0.274]+[0.151])	Prec@1 92.969 (92.969)
 * Prec@1 91.720
current lr 1.00000e-02
Grad=  tensor(6.5100, device='cuda:0')
Epoch: [166][0/391]	Time 0.170 (0.170)	Data 0.128 (0.128)	Loss 0.2436 (0.2436) ([0.093]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [166][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2783 (0.2317) ([0.128]+[0.150])	Prec@1 96.094 (97.277)
Epoch: [166][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2120 (0.2345) ([0.062]+[0.150])	Prec@1 97.656 (97.174)
Epoch: [166][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2442 (0.2381) ([0.094]+[0.151])	Prec@1 96.875 (97.041)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4496 (0.4496) ([0.299]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 91.100
current lr 1.00000e-02
Grad=  tensor(7.5508, device='cuda:0')
Epoch: [167][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.2113 (0.2113) ([0.061]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [167][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2373 (0.2383) ([0.087]+[0.151])	Prec@1 96.875 (97.161)
Epoch: [167][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2530 (0.2399) ([0.103]+[0.150])	Prec@1 96.094 (97.019)
Epoch: [167][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2949 (0.2396) ([0.144]+[0.150])	Prec@1 96.875 (97.051)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4110 (0.4110) ([0.260]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 91.860
current lr 1.00000e-02
Grad=  tensor(6.2183, device='cuda:0')
Epoch: [168][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2174 (0.2174) ([0.067]+[0.150])	Prec@1 99.219 (99.219)
Epoch: [168][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2211 (0.2352) ([0.071]+[0.151])	Prec@1 96.875 (97.184)
Epoch: [168][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1886 (0.2356) ([0.038]+[0.150])	Prec@1 99.219 (97.104)
Epoch: [168][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2591 (0.2399) ([0.109]+[0.150])	Prec@1 96.094 (96.942)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5516 (0.5516) ([0.401]+[0.151])	Prec@1 86.719 (86.719)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(5.5719, device='cuda:0')
Epoch: [169][0/391]	Time 0.164 (0.164)	Data 0.121 (0.121)	Loss 0.1971 (0.1971) ([0.046]+[0.151])	Prec@1 97.656 (97.656)
Epoch: [169][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2505 (0.2338) ([0.100]+[0.151])	Prec@1 95.312 (97.138)
Epoch: [169][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2943 (0.2349) ([0.144]+[0.151])	Prec@1 94.531 (97.163)
Epoch: [169][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2248 (0.2402) ([0.074]+[0.151])	Prec@1 98.438 (96.981)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3208 (0.3208) ([0.170]+[0.151])	Prec@1 95.312 (95.312)
 * Prec@1 91.840
current lr 1.00000e-02
Grad=  tensor(2.8843, device='cuda:0')
Epoch: [170][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1944 (0.1944) ([0.044]+[0.151])	Prec@1 98.438 (98.438)
Epoch: [170][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2489 (0.2264) ([0.098]+[0.151])	Prec@1 96.875 (97.347)
Epoch: [170][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2347 (0.2353) ([0.084]+[0.151])	Prec@1 96.094 (97.089)
Epoch: [170][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2379 (0.2398) ([0.087]+[0.151])	Prec@1 96.094 (96.942)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4639 (0.4639) ([0.313]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 90.220
current lr 1.00000e-02
Grad=  tensor(5.5962, device='cuda:0')
Epoch: [171][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2102 (0.2102) ([0.059]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [171][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2061 (0.2346) ([0.055]+[0.151])	Prec@1 96.875 (97.146)
Epoch: [171][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2466 (0.2373) ([0.096]+[0.151])	Prec@1 97.656 (97.065)
Epoch: [171][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2093 (0.2414) ([0.058]+[0.151])	Prec@1 98.438 (96.945)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4281 (0.4281) ([0.277]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 91.310
current lr 1.00000e-02
Grad=  tensor(8.6425, device='cuda:0')
Epoch: [172][0/391]	Time 0.163 (0.163)	Data 0.120 (0.120)	Loss 0.2227 (0.2227) ([0.072]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [172][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2259 (0.2232) ([0.075]+[0.150])	Prec@1 97.656 (97.772)
Epoch: [172][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1966 (0.2280) ([0.046]+[0.150])	Prec@1 98.438 (97.505)
Epoch: [172][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3702 (0.2297) ([0.220]+[0.150])	Prec@1 92.969 (97.360)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4656 (0.4656) ([0.315]+[0.150])	Prec@1 92.188 (92.188)
 * Prec@1 90.880
current lr 1.00000e-02
Grad=  tensor(6.0463, device='cuda:0')
Epoch: [173][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2347 (0.2347) ([0.084]+[0.150])	Prec@1 98.438 (98.438)
Epoch: [173][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1805 (0.2293) ([0.030]+[0.150])	Prec@1 100.000 (97.409)
Epoch: [173][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2278 (0.2330) ([0.077]+[0.150])	Prec@1 96.094 (97.299)
Epoch: [173][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2353 (0.2345) ([0.085]+[0.150])	Prec@1 96.094 (97.233)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.4052 (0.4052) ([0.255]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 90.900
current lr 1.00000e-02
Grad=  tensor(4.5165, device='cuda:0')
Epoch: [174][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2025 (0.2025) ([0.052]+[0.151])	Prec@1 98.438 (98.438)
Epoch: [174][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3279 (0.2284) ([0.178]+[0.150])	Prec@1 95.312 (97.355)
Epoch: [174][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2404 (0.2298) ([0.090]+[0.150])	Prec@1 97.656 (97.275)
Epoch: [174][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2665 (0.2345) ([0.116]+[0.150])	Prec@1 95.312 (97.096)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4936 (0.4936) ([0.343]+[0.150])	Prec@1 92.969 (92.969)
 * Prec@1 91.210
current lr 1.00000e-02
Grad=  tensor(6.8822, device='cuda:0')
Epoch: [175][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2224 (0.2224) ([0.072]+[0.150])	Prec@1 97.656 (97.656)
Epoch: [175][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3087 (0.2284) ([0.158]+[0.150])	Prec@1 94.531 (97.347)
Epoch: [175][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3570 (0.2375) ([0.206]+[0.151])	Prec@1 91.406 (97.062)
Epoch: [175][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2336 (0.2379) ([0.083]+[0.151])	Prec@1 98.438 (97.072)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4824 (0.4824) ([0.332]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 90.900
current lr 1.00000e-02
Grad=  tensor(5.0572, device='cuda:0')
Epoch: [176][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2025 (0.2025) ([0.052]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [176][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2002 (0.2333) ([0.050]+[0.151])	Prec@1 98.438 (97.239)
Epoch: [176][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2455 (0.2331) ([0.095]+[0.151])	Prec@1 97.656 (97.221)
Epoch: [176][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2613 (0.2379) ([0.111]+[0.151])	Prec@1 96.094 (97.083)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4934 (0.4934) ([0.343]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 91.530
current lr 1.00000e-02
Grad=  tensor(11.0297, device='cuda:0')
Epoch: [177][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2459 (0.2459) ([0.095]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [177][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3302 (0.2280) ([0.180]+[0.151])	Prec@1 94.531 (97.494)
Epoch: [177][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2391 (0.2289) ([0.089]+[0.150])	Prec@1 96.875 (97.396)
Epoch: [177][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2842 (0.2367) ([0.133]+[0.151])	Prec@1 96.094 (97.098)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3909 (0.3909) ([0.240]+[0.151])	Prec@1 93.750 (93.750)
 * Prec@1 91.340
current lr 1.00000e-02
Grad=  tensor(4.9521, device='cuda:0')
Epoch: [178][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2235 (0.2235) ([0.073]+[0.151])	Prec@1 98.438 (98.438)
Epoch: [178][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2120 (0.2377) ([0.061]+[0.151])	Prec@1 98.438 (97.107)
Epoch: [178][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1787 (0.2355) ([0.028]+[0.151])	Prec@1 100.000 (97.194)
Epoch: [178][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2896 (0.2401) ([0.139]+[0.151])	Prec@1 94.531 (96.984)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4219 (0.4219) ([0.271]+[0.151])	Prec@1 89.062 (89.062)
 * Prec@1 91.550
current lr 1.00000e-02
Grad=  tensor(10.0705, device='cuda:0')
Epoch: [179][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2809 (0.2809) ([0.130]+[0.151])	Prec@1 95.312 (95.312)
Epoch: [179][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2194 (0.2288) ([0.068]+[0.151])	Prec@1 97.656 (97.401)
Epoch: [179][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3140 (0.2337) ([0.163]+[0.151])	Prec@1 93.750 (97.112)
Epoch: [179][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2636 (0.2378) ([0.112]+[0.151])	Prec@1 96.875 (96.961)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.5825 (0.5825) ([0.431]+[0.151])	Prec@1 89.062 (89.062)
 * Prec@1 90.960
current lr 1.00000e-02
Grad=  tensor(5.7093, device='cuda:0')
Epoch: [180][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2248 (0.2248) ([0.074]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [180][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2299 (0.2281) ([0.079]+[0.151])	Prec@1 96.094 (97.208)
Epoch: [180][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2648 (0.2325) ([0.114]+[0.151])	Prec@1 96.875 (97.155)
Epoch: [180][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3449 (0.2360) ([0.194]+[0.151])	Prec@1 92.188 (97.088)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4749 (0.4749) ([0.324]+[0.151])	Prec@1 91.406 (91.406)
 * Prec@1 90.470
current lr 1.00000e-02
Grad=  tensor(2.2235, device='cuda:0')
Epoch: [181][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.1825 (0.1825) ([0.031]+[0.151])	Prec@1 99.219 (99.219)
Epoch: [181][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2203 (0.2360) ([0.069]+[0.151])	Prec@1 97.656 (97.076)
Epoch: [181][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2347 (0.2361) ([0.084]+[0.151])	Prec@1 96.875 (97.139)
Epoch: [181][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2142 (0.2367) ([0.063]+[0.151])	Prec@1 96.094 (97.122)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4702 (0.4702) ([0.319]+[0.151])	Prec@1 91.406 (91.406)
 * Prec@1 91.800
current lr 1.00000e-02
Grad=  tensor(5.9787, device='cuda:0')
Epoch: [182][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2373 (0.2373) ([0.086]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [182][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2301 (0.2343) ([0.079]+[0.151])	Prec@1 98.438 (97.277)
Epoch: [182][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2494 (0.2349) ([0.098]+[0.151])	Prec@1 97.656 (97.217)
Epoch: [182][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2510 (0.2373) ([0.100]+[0.151])	Prec@1 95.312 (97.093)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4948 (0.4948) ([0.343]+[0.151])	Prec@1 90.625 (90.625)
 * Prec@1 91.880
current lr 1.00000e-02
Grad=  tensor(14.1588, device='cuda:0')
Epoch: [183][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2710 (0.2710) ([0.120]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [183][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1822 (0.2259) ([0.031]+[0.151])	Prec@1 99.219 (97.471)
Epoch: [183][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2310 (0.2288) ([0.080]+[0.151])	Prec@1 96.875 (97.345)
Epoch: [183][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2803 (0.2321) ([0.129]+[0.151])	Prec@1 93.750 (97.257)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3718 (0.3718) ([0.220]+[0.151])	Prec@1 92.188 (92.188)
 * Prec@1 91.990
current lr 1.00000e-02
Grad=  tensor(7.5035, device='cuda:0')
Epoch: [184][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2407 (0.2407) ([0.089]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [184][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2327 (0.2260) ([0.081]+[0.151])	Prec@1 96.875 (97.610)
Epoch: [184][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3009 (0.2354) ([0.150]+[0.151])	Prec@1 96.875 (97.279)
Epoch: [184][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2894 (0.2356) ([0.138]+[0.151])	Prec@1 94.531 (97.140)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4329 (0.4329) ([0.281]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.550
current lr 1.00000e-02
Grad=  tensor(6.3186, device='cuda:0')
Epoch: [185][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.2005 (0.2005) ([0.049]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [185][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2317 (0.2283) ([0.080]+[0.152])	Prec@1 97.656 (97.424)
Epoch: [185][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2990 (0.2319) ([0.148]+[0.151])	Prec@1 94.531 (97.275)
Epoch: [185][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3151 (0.2363) ([0.164]+[0.152])	Prec@1 95.312 (97.124)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4043 (0.4043) ([0.253]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 91.660
current lr 1.00000e-02
Grad=  tensor(2.7104, device='cuda:0')
Epoch: [186][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.1887 (0.1887) ([0.037]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [186][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2043 (0.2261) ([0.053]+[0.152])	Prec@1 98.438 (97.548)
Epoch: [186][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1847 (0.2320) ([0.033]+[0.152])	Prec@1 100.000 (97.334)
Epoch: [186][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2632 (0.2341) ([0.112]+[0.152])	Prec@1 96.875 (97.205)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4775 (0.4775) ([0.326]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 91.970
current lr 1.00000e-02
Grad=  tensor(7.0755, device='cuda:0')
Epoch: [187][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2218 (0.2218) ([0.070]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [187][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2273 (0.2337) ([0.076]+[0.151])	Prec@1 97.656 (97.246)
Epoch: [187][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1943 (0.2367) ([0.043]+[0.151])	Prec@1 98.438 (97.159)
Epoch: [187][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3314 (0.2372) ([0.180]+[0.151])	Prec@1 93.750 (97.129)
Test: [0/79]	Time 0.132 (0.132)	Loss 0.5740 (0.5740) ([0.423]+[0.152])	Prec@1 89.844 (89.844)
 * Prec@1 90.630
current lr 1.00000e-02
Grad=  tensor(3.7425, device='cuda:0')
Epoch: [188][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.2010 (0.2010) ([0.050]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [188][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3026 (0.2350) ([0.151]+[0.151])	Prec@1 92.188 (97.192)
Epoch: [188][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2011 (0.2401) ([0.049]+[0.152])	Prec@1 97.656 (96.937)
Epoch: [188][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3621 (0.2422) ([0.210]+[0.152])	Prec@1 92.188 (96.875)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4216 (0.4216) ([0.270]+[0.152])	Prec@1 90.625 (90.625)
 * Prec@1 91.180
current lr 1.00000e-02
Grad=  tensor(10.5918, device='cuda:0')
Epoch: [189][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2761 (0.2761) ([0.124]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [189][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2161 (0.2354) ([0.064]+[0.152])	Prec@1 97.656 (97.130)
Epoch: [189][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2380 (0.2355) ([0.086]+[0.152])	Prec@1 97.656 (97.132)
Epoch: [189][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2061 (0.2374) ([0.054]+[0.152])	Prec@1 99.219 (97.080)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4718 (0.4718) ([0.320]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(7.5719, device='cuda:0')
Epoch: [190][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2490 (0.2490) ([0.097]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [190][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1891 (0.2262) ([0.037]+[0.152])	Prec@1 98.438 (97.432)
Epoch: [190][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2131 (0.2251) ([0.062]+[0.151])	Prec@1 98.438 (97.466)
Epoch: [190][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3290 (0.2306) ([0.177]+[0.152])	Prec@1 92.969 (97.277)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.3817 (0.3817) ([0.230]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 91.690
current lr 1.00000e-02
Grad=  tensor(3.5210, device='cuda:0')
Epoch: [191][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.1889 (0.1889) ([0.037]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [191][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1865 (0.2330) ([0.035]+[0.152])	Prec@1 98.438 (97.316)
Epoch: [191][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2970 (0.2374) ([0.145]+[0.152])	Prec@1 96.875 (97.135)
Epoch: [191][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2826 (0.2404) ([0.131]+[0.152])	Prec@1 96.875 (97.013)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3854 (0.3854) ([0.234]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 92.290
current lr 1.00000e-02
Grad=  tensor(6.2752, device='cuda:0')
Epoch: [192][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.2063 (0.2063) ([0.055]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [192][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2512 (0.2389) ([0.099]+[0.152])	Prec@1 96.875 (97.022)
Epoch: [192][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2520 (0.2399) ([0.100]+[0.152])	Prec@1 96.094 (97.019)
Epoch: [192][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2058 (0.2367) ([0.054]+[0.151])	Prec@1 98.438 (97.166)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.4665 (0.4665) ([0.315]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 91.630
current lr 1.00000e-02
Grad=  tensor(6.9624, device='cuda:0')
Epoch: [193][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.2393 (0.2393) ([0.088]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [193][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2569 (0.2381) ([0.105]+[0.152])	Prec@1 96.875 (97.045)
Epoch: [193][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1776 (0.2342) ([0.026]+[0.152])	Prec@1 99.219 (97.124)
Epoch: [193][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2392 (0.2339) ([0.088]+[0.152])	Prec@1 97.656 (97.171)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3990 (0.3990) ([0.247]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 91.150
current lr 1.00000e-02
Grad=  tensor(1.3132, device='cuda:0')
Epoch: [194][0/391]	Time 0.168 (0.168)	Data 0.127 (0.127)	Loss 0.1791 (0.1791) ([0.027]+[0.152])	Prec@1 100.000 (100.000)
Epoch: [194][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2566 (0.2340) ([0.105]+[0.152])	Prec@1 96.875 (97.184)
Epoch: [194][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2207 (0.2357) ([0.069]+[0.152])	Prec@1 98.438 (97.178)
Epoch: [194][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2401 (0.2355) ([0.088]+[0.152])	Prec@1 98.438 (97.199)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4140 (0.4140) ([0.262]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(3.3524, device='cuda:0')
Epoch: [195][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.2059 (0.2059) ([0.054]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [195][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2002 (0.2241) ([0.048]+[0.152])	Prec@1 98.438 (97.679)
Epoch: [195][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2278 (0.2284) ([0.076]+[0.152])	Prec@1 96.875 (97.489)
Epoch: [195][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2018 (0.2327) ([0.050]+[0.152])	Prec@1 99.219 (97.334)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4040 (0.4040) ([0.252]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 91.870
current lr 1.00000e-02
Grad=  tensor(3.6272, device='cuda:0')
Epoch: [196][0/391]	Time 0.171 (0.171)	Data 0.130 (0.130)	Loss 0.2254 (0.2254) ([0.074]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [196][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1783 (0.2212) ([0.027]+[0.152])	Prec@1 98.438 (97.718)
Epoch: [196][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2192 (0.2216) ([0.068]+[0.151])	Prec@1 98.438 (97.648)
Epoch: [196][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2902 (0.2303) ([0.139]+[0.152])	Prec@1 92.969 (97.342)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4832 (0.4832) ([0.332]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 92.100
current lr 1.00000e-02
Grad=  tensor(8.4378, device='cuda:0')
Epoch: [197][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2542 (0.2542) ([0.103]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [197][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1788 (0.2354) ([0.027]+[0.152])	Prec@1 100.000 (97.200)
Epoch: [197][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2253 (0.2354) ([0.074]+[0.152])	Prec@1 97.656 (97.116)
Epoch: [197][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2182 (0.2340) ([0.067]+[0.151])	Prec@1 96.875 (97.184)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4066 (0.4066) ([0.255]+[0.151])	Prec@1 89.062 (89.062)
 * Prec@1 90.250
current lr 1.00000e-02
Grad=  tensor(8.0956, device='cuda:0')
Epoch: [198][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2398 (0.2398) ([0.088]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [198][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2521 (0.2395) ([0.101]+[0.152])	Prec@1 97.656 (97.092)
Epoch: [198][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2323 (0.2376) ([0.081]+[0.151])	Prec@1 96.094 (97.182)
Epoch: [198][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2221 (0.2394) ([0.070]+[0.152])	Prec@1 97.656 (97.075)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4709 (0.4709) ([0.319]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 90.810
current lr 1.00000e-02
Grad=  tensor(11.8256, device='cuda:0')
Epoch: [199][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2674 (0.2674) ([0.116]+[0.152])	Prec@1 96.875 (96.875)
Epoch: [199][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1864 (0.2353) ([0.035]+[0.152])	Prec@1 98.438 (97.153)
Epoch: [199][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2290 (0.2367) ([0.077]+[0.152])	Prec@1 97.656 (97.077)
Epoch: [199][300/391]	Time 0.037 (0.036)	Data 0.000 (0.001)	Loss 0.2846 (0.2388) ([0.133]+[0.152])	Prec@1 96.094 (97.031)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3946 (0.3946) ([0.243]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 90.870
current lr 1.00000e-02
Grad=  tensor(5.0186, device='cuda:0')
Epoch: [200][0/391]	Time 0.172 (0.172)	Data 0.122 (0.122)	Loss 0.2171 (0.2171) ([0.065]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [200][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2427 (0.2314) ([0.091]+[0.152])	Prec@1 96.094 (97.424)
Epoch: [200][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3194 (0.2333) ([0.168]+[0.152])	Prec@1 94.531 (97.248)
Epoch: [200][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2638 (0.2335) ([0.112]+[0.152])	Prec@1 96.875 (97.285)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.7320 (0.7320) ([0.580]+[0.152])	Prec@1 86.719 (86.719)
 * Prec@1 90.200
current lr 1.00000e-02
Grad=  tensor(13.4263, device='cuda:0')
Epoch: [201][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2698 (0.2698) ([0.118]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [201][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2289 (0.2272) ([0.077]+[0.152])	Prec@1 97.656 (97.316)
Epoch: [201][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2061 (0.2299) ([0.055]+[0.151])	Prec@1 97.656 (97.384)
Epoch: [201][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2574 (0.2311) ([0.106]+[0.152])	Prec@1 95.312 (97.373)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4432 (0.4432) ([0.291]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 91.980
current lr 1.00000e-02
Grad=  tensor(2.6424, device='cuda:0')
Epoch: [202][0/391]	Time 0.173 (0.173)	Data 0.123 (0.123)	Loss 0.1984 (0.1984) ([0.046]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [202][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2602 (0.2320) ([0.108]+[0.152])	Prec@1 95.312 (97.153)
Epoch: [202][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2427 (0.2317) ([0.091]+[0.152])	Prec@1 96.094 (97.283)
Epoch: [202][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2946 (0.2320) ([0.143]+[0.152])	Prec@1 94.531 (97.272)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4395 (0.4395) ([0.288]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 91.700
current lr 1.00000e-02
Grad=  tensor(7.5860, device='cuda:0')
Epoch: [203][0/391]	Time 0.172 (0.172)	Data 0.123 (0.123)	Loss 0.2222 (0.2222) ([0.071]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [203][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2369 (0.2240) ([0.085]+[0.152])	Prec@1 97.656 (97.571)
Epoch: [203][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2353 (0.2300) ([0.084]+[0.152])	Prec@1 96.875 (97.369)
Epoch: [203][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.3150 (0.2340) ([0.163]+[0.152])	Prec@1 93.750 (97.244)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.4335 (0.4335) ([0.282]+[0.152])	Prec@1 95.312 (95.312)
 * Prec@1 91.590
current lr 1.00000e-02
Grad=  tensor(7.4145, device='cuda:0')
Epoch: [204][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2593 (0.2593) ([0.107]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [204][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2417 (0.2311) ([0.090]+[0.152])	Prec@1 96.094 (97.455)
Epoch: [204][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2452 (0.2326) ([0.094]+[0.152])	Prec@1 96.875 (97.334)
Epoch: [204][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2248 (0.2327) ([0.073]+[0.152])	Prec@1 97.656 (97.368)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4993 (0.4993) ([0.347]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 90.790
current lr 1.00000e-02
Grad=  tensor(12.5259, device='cuda:0')
Epoch: [205][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2452 (0.2452) ([0.093]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [205][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2222 (0.2338) ([0.070]+[0.152])	Prec@1 98.438 (97.386)
Epoch: [205][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2380 (0.2314) ([0.086]+[0.152])	Prec@1 96.094 (97.330)
Epoch: [205][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2713 (0.2359) ([0.119]+[0.152])	Prec@1 94.531 (97.199)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4615 (0.4615) ([0.310]+[0.152])	Prec@1 93.750 (93.750)
 * Prec@1 91.590
current lr 1.00000e-02
Grad=  tensor(4.3697, device='cuda:0')
Epoch: [206][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2177 (0.2177) ([0.066]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [206][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2559 (0.2299) ([0.104]+[0.152])	Prec@1 96.094 (97.347)
Epoch: [206][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2869 (0.2326) ([0.135]+[0.152])	Prec@1 94.531 (97.275)
Epoch: [206][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2419 (0.2318) ([0.090]+[0.152])	Prec@1 96.094 (97.308)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5386 (0.5386) ([0.387]+[0.152])	Prec@1 89.844 (89.844)
 * Prec@1 91.480
current lr 1.00000e-02
Grad=  tensor(14.7992, device='cuda:0')
Epoch: [207][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2827 (0.2827) ([0.131]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [207][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1896 (0.2319) ([0.038]+[0.152])	Prec@1 97.656 (97.362)
Epoch: [207][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1970 (0.2280) ([0.045]+[0.152])	Prec@1 99.219 (97.338)
Epoch: [207][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2204 (0.2298) ([0.069]+[0.152])	Prec@1 97.656 (97.334)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5441 (0.5441) ([0.392]+[0.152])	Prec@1 90.625 (90.625)
 * Prec@1 91.070
current lr 1.00000e-02
Grad=  tensor(3.4489, device='cuda:0')
Epoch: [208][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1894 (0.1894) ([0.038]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [208][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2570 (0.2231) ([0.105]+[0.152])	Prec@1 96.094 (97.594)
Epoch: [208][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2271 (0.2272) ([0.075]+[0.152])	Prec@1 97.656 (97.365)
Epoch: [208][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1996 (0.2303) ([0.048]+[0.152])	Prec@1 98.438 (97.301)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5998 (0.5998) ([0.448]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 90.480
current lr 1.00000e-02
Grad=  tensor(6.5594, device='cuda:0')
Epoch: [209][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.2120 (0.2120) ([0.060]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [209][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1931 (0.2383) ([0.041]+[0.152])	Prec@1 98.438 (97.130)
Epoch: [209][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1980 (0.2402) ([0.046]+[0.152])	Prec@1 97.656 (96.999)
Epoch: [209][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2131 (0.2405) ([0.061]+[0.152])	Prec@1 97.656 (96.963)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3924 (0.3924) ([0.240]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.310
current lr 1.00000e-02
Grad=  tensor(7.9582, device='cuda:0')
Epoch: [210][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.2411 (0.2411) ([0.089]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [210][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2148 (0.2404) ([0.062]+[0.152])	Prec@1 97.656 (97.076)
Epoch: [210][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2492 (0.2419) ([0.097]+[0.152])	Prec@1 96.094 (96.964)
Epoch: [210][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2356 (0.2408) ([0.083]+[0.152])	Prec@1 97.656 (97.013)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4422 (0.4422) ([0.290]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 91.740
current lr 1.00000e-02
Grad=  tensor(15.1280, device='cuda:0')
Epoch: [211][0/391]	Time 0.181 (0.181)	Data 0.139 (0.139)	Loss 0.3059 (0.3059) ([0.153]+[0.153])	Prec@1 95.312 (95.312)
Epoch: [211][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1953 (0.2347) ([0.043]+[0.153])	Prec@1 98.438 (97.269)
Epoch: [211][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1830 (0.2314) ([0.031]+[0.152])	Prec@1 99.219 (97.376)
Epoch: [211][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1874 (0.2303) ([0.035]+[0.152])	Prec@1 99.219 (97.420)
Test: [0/79]	Time 0.133 (0.133)	Loss 0.5044 (0.5044) ([0.352]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 91.240
current lr 1.00000e-02
Grad=  tensor(4.4623, device='cuda:0')
Epoch: [212][0/391]	Time 0.179 (0.179)	Data 0.130 (0.130)	Loss 0.1955 (0.1955) ([0.043]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [212][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2316 (0.2242) ([0.080]+[0.152])	Prec@1 96.094 (97.610)
Epoch: [212][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2424 (0.2224) ([0.091]+[0.152])	Prec@1 96.875 (97.641)
Epoch: [212][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2177 (0.2300) ([0.066]+[0.152])	Prec@1 98.438 (97.353)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.5016 (0.5016) ([0.349]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 91.140
current lr 1.00000e-02
Grad=  tensor(7.4017, device='cuda:0')
Epoch: [213][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2007 (0.2007) ([0.049]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [213][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1985 (0.2239) ([0.047]+[0.152])	Prec@1 98.438 (97.687)
Epoch: [213][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2074 (0.2300) ([0.055]+[0.152])	Prec@1 99.219 (97.439)
Epoch: [213][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1958 (0.2321) ([0.044]+[0.152])	Prec@1 97.656 (97.379)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4423 (0.4423) ([0.290]+[0.152])	Prec@1 90.625 (90.625)
 * Prec@1 91.290
current lr 1.00000e-02
Grad=  tensor(4.6185, device='cuda:0')
Epoch: [214][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2191 (0.2191) ([0.067]+[0.152])	Prec@1 99.219 (99.219)
Epoch: [214][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2114 (0.2245) ([0.059]+[0.152])	Prec@1 97.656 (97.741)
Epoch: [214][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2179 (0.2270) ([0.066]+[0.152])	Prec@1 98.438 (97.617)
Epoch: [214][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2667 (0.2291) ([0.115]+[0.152])	Prec@1 96.875 (97.498)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.5089 (0.5089) ([0.357]+[0.152])	Prec@1 89.062 (89.062)
 * Prec@1 91.830
current lr 1.00000e-02
Grad=  tensor(7.0773, device='cuda:0')
Epoch: [215][0/391]	Time 0.165 (0.165)	Data 0.121 (0.121)	Loss 0.2211 (0.2211) ([0.069]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [215][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2409 (0.2319) ([0.089]+[0.152])	Prec@1 96.094 (97.339)
Epoch: [215][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2491 (0.2383) ([0.097]+[0.152])	Prec@1 96.094 (97.030)
Epoch: [215][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2434 (0.2410) ([0.091]+[0.153])	Prec@1 97.656 (96.948)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4224 (0.4224) ([0.270]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.450
current lr 1.00000e-02
Grad=  tensor(9.9281, device='cuda:0')
Epoch: [216][0/391]	Time 0.172 (0.172)	Data 0.123 (0.123)	Loss 0.2580 (0.2580) ([0.105]+[0.153])	Prec@1 94.531 (94.531)
Epoch: [216][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2249 (0.2355) ([0.072]+[0.153])	Prec@1 96.875 (97.184)
Epoch: [216][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.3308 (0.2356) ([0.178]+[0.153])	Prec@1 92.969 (97.233)
Epoch: [216][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2490 (0.2388) ([0.096]+[0.153])	Prec@1 96.875 (97.135)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4035 (0.4035) ([0.251]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 91.820
current lr 1.00000e-02
Grad=  tensor(4.9172, device='cuda:0')
Epoch: [217][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2322 (0.2322) ([0.079]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [217][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2300 (0.2136) ([0.078]+[0.152])	Prec@1 97.656 (98.097)
Epoch: [217][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2632 (0.2223) ([0.111]+[0.152])	Prec@1 95.312 (97.730)
Epoch: [217][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2306 (0.2257) ([0.078]+[0.152])	Prec@1 96.875 (97.586)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3676 (0.3676) ([0.215]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.690
current lr 1.00000e-02
Grad=  tensor(14.7236, device='cuda:0')
Epoch: [218][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2962 (0.2962) ([0.144]+[0.153])	Prec@1 94.531 (94.531)
Epoch: [218][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.2631 (0.2357) ([0.111]+[0.152])	Prec@1 95.312 (97.130)
Epoch: [218][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2490 (0.2359) ([0.097]+[0.152])	Prec@1 96.094 (97.155)
Epoch: [218][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2016 (0.2339) ([0.049]+[0.152])	Prec@1 98.438 (97.228)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.6564 (0.6564) ([0.504]+[0.152])	Prec@1 89.844 (89.844)
 * Prec@1 92.050
current lr 1.00000e-02
Grad=  tensor(7.3584, device='cuda:0')
Epoch: [219][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.2249 (0.2249) ([0.072]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [219][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2661 (0.2252) ([0.114]+[0.152])	Prec@1 95.312 (97.502)
Epoch: [219][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2399 (0.2315) ([0.088]+[0.152])	Prec@1 98.438 (97.268)
Epoch: [219][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1943 (0.2352) ([0.042]+[0.152])	Prec@1 98.438 (97.116)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4595 (0.4595) ([0.307]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 92.010
current lr 1.00000e-02
Grad=  tensor(3.2734, device='cuda:0')
Epoch: [220][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1923 (0.1923) ([0.040]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [220][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1918 (0.2263) ([0.039]+[0.152])	Prec@1 97.656 (97.679)
Epoch: [220][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2275 (0.2275) ([0.075]+[0.152])	Prec@1 96.094 (97.621)
Epoch: [220][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2496 (0.2295) ([0.097]+[0.152])	Prec@1 97.656 (97.519)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3963 (0.3963) ([0.244]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 91.280
current lr 1.00000e-02
Grad=  tensor(9.3170, device='cuda:0')
Epoch: [221][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.2418 (0.2418) ([0.089]+[0.152])	Prec@1 96.094 (96.094)
Epoch: [221][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2161 (0.2342) ([0.064]+[0.152])	Prec@1 96.875 (97.184)
Epoch: [221][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2003 (0.2299) ([0.048]+[0.152])	Prec@1 99.219 (97.392)
Epoch: [221][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2179 (0.2324) ([0.066]+[0.152])	Prec@1 98.438 (97.345)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4349 (0.4349) ([0.283]+[0.152])	Prec@1 91.406 (91.406)
 * Prec@1 91.240
current lr 1.00000e-02
Grad=  tensor(3.6840, device='cuda:0')
Epoch: [222][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2058 (0.2058) ([0.053]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [222][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.2130 (0.2250) ([0.061]+[0.152])	Prec@1 97.656 (97.664)
Epoch: [222][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.2306 (0.2313) ([0.078]+[0.152])	Prec@1 96.094 (97.338)
Epoch: [222][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2218 (0.2330) ([0.069]+[0.152])	Prec@1 98.438 (97.295)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4109 (0.4109) ([0.258]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 91.550
current lr 1.00000e-02
Grad=  tensor(11.6198, device='cuda:0')
Epoch: [223][0/391]	Time 0.161 (0.161)	Data 0.120 (0.120)	Loss 0.2946 (0.2946) ([0.142]+[0.153])	Prec@1 95.312 (95.312)
Epoch: [223][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2151 (0.2342) ([0.063]+[0.152])	Prec@1 99.219 (97.254)
Epoch: [223][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2214 (0.2303) ([0.069]+[0.152])	Prec@1 97.656 (97.318)
Epoch: [223][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2324 (0.2329) ([0.080]+[0.152])	Prec@1 96.875 (97.225)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3886 (0.3886) ([0.236]+[0.152])	Prec@1 92.188 (92.188)
 * Prec@1 91.490
current lr 1.00000e-02
Grad=  tensor(4.3868, device='cuda:0')
Epoch: [224][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1991 (0.1991) ([0.047]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [224][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2311 (0.2256) ([0.079]+[0.152])	Prec@1 96.875 (97.432)
Epoch: [224][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2857 (0.2308) ([0.133]+[0.152])	Prec@1 96.094 (97.322)
Epoch: [224][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2472 (0.2320) ([0.095]+[0.152])	Prec@1 96.875 (97.295)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4620 (0.4620) ([0.309]+[0.153])	Prec@1 90.625 (90.625)
 * Prec@1 90.650
current lr 1.00000e-02
Grad=  tensor(5.3727, device='cuda:0')
Epoch: [225][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2417 (0.2417) ([0.089]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [225][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1953 (0.2462) ([0.042]+[0.153])	Prec@1 98.438 (96.798)
Epoch: [225][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2330 (0.2444) ([0.080]+[0.153])	Prec@1 97.656 (96.836)
Epoch: [225][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2816 (0.2453) ([0.129]+[0.153])	Prec@1 93.750 (96.846)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3890 (0.3890) ([0.236]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.490
current lr 1.00000e-02
Grad=  tensor(13.9649, device='cuda:0')
Epoch: [226][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2774 (0.2774) ([0.124]+[0.153])	Prec@1 93.750 (93.750)
Epoch: [226][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2665 (0.2328) ([0.113]+[0.153])	Prec@1 96.094 (97.293)
Epoch: [226][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1965 (0.2398) ([0.043]+[0.153])	Prec@1 98.438 (97.046)
Epoch: [226][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2127 (0.2399) ([0.060]+[0.153])	Prec@1 96.875 (97.072)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3042 (0.3042) ([0.151]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.570
current lr 1.00000e-02
Grad=  tensor(5.1053, device='cuda:0')
Epoch: [227][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2006 (0.2006) ([0.048]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [227][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2406 (0.2154) ([0.088]+[0.153])	Prec@1 96.875 (97.904)
Epoch: [227][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3300 (0.2206) ([0.177]+[0.153])	Prec@1 91.406 (97.676)
Epoch: [227][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2435 (0.2239) ([0.091]+[0.152])	Prec@1 95.312 (97.519)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4096 (0.4096) ([0.257]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 91.600
current lr 1.00000e-02
Grad=  tensor(4.0328, device='cuda:0')
Epoch: [228][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2006 (0.2006) ([0.048]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [228][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2185 (0.2284) ([0.066]+[0.153])	Prec@1 96.875 (97.471)
Epoch: [228][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2421 (0.2317) ([0.090]+[0.152])	Prec@1 96.875 (97.404)
Epoch: [228][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1840 (0.2333) ([0.032]+[0.152])	Prec@1 99.219 (97.340)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4479 (0.4479) ([0.295]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 91.210
current lr 1.00000e-02
Grad=  tensor(9.8539, device='cuda:0')
Epoch: [229][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2657 (0.2657) ([0.113]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [229][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2385 (0.2238) ([0.086]+[0.152])	Prec@1 96.875 (97.695)
Epoch: [229][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1895 (0.2239) ([0.037]+[0.152])	Prec@1 98.438 (97.683)
Epoch: [229][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1936 (0.2295) ([0.041]+[0.152])	Prec@1 98.438 (97.433)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4924 (0.4924) ([0.340]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 91.600
current lr 1.00000e-02
Grad=  tensor(5.7048, device='cuda:0')
Epoch: [230][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1985 (0.1985) ([0.046]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [230][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2596 (0.2272) ([0.107]+[0.152])	Prec@1 96.094 (97.649)
Epoch: [230][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2130 (0.2303) ([0.060]+[0.153])	Prec@1 98.438 (97.485)
Epoch: [230][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2866 (0.2316) ([0.134]+[0.152])	Prec@1 93.750 (97.379)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4593 (0.4593) ([0.307]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(9.4722, device='cuda:0')
Epoch: [231][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2441 (0.2441) ([0.092]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [231][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2786 (0.2237) ([0.126]+[0.152])	Prec@1 95.312 (97.679)
Epoch: [231][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2366 (0.2302) ([0.084]+[0.152])	Prec@1 98.438 (97.415)
Epoch: [231][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2743 (0.2318) ([0.122]+[0.152])	Prec@1 94.531 (97.319)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3407 (0.3407) ([0.188]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 91.220
current lr 1.00000e-02
Grad=  tensor(3.8787, device='cuda:0')
Epoch: [232][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.1852 (0.1852) ([0.033]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [232][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2126 (0.2282) ([0.060]+[0.153])	Prec@1 98.438 (97.579)
Epoch: [232][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1949 (0.2305) ([0.042]+[0.152])	Prec@1 99.219 (97.415)
Epoch: [232][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2146 (0.2355) ([0.062]+[0.153])	Prec@1 97.656 (97.212)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4635 (0.4635) ([0.311]+[0.152])	Prec@1 94.531 (94.531)
 * Prec@1 91.840
current lr 1.00000e-02
Grad=  tensor(5.6751, device='cuda:0')
Epoch: [233][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2160 (0.2160) ([0.064]+[0.152])	Prec@1 97.656 (97.656)
Epoch: [233][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2127 (0.2203) ([0.061]+[0.152])	Prec@1 97.656 (97.718)
Epoch: [233][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2322 (0.2259) ([0.080]+[0.152])	Prec@1 97.656 (97.458)
Epoch: [233][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2382 (0.2303) ([0.086]+[0.152])	Prec@1 96.094 (97.368)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3783 (0.3783) ([0.226]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.430
current lr 1.00000e-02
Grad=  tensor(12.3116, device='cuda:0')
Epoch: [234][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.2666 (0.2666) ([0.114]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [234][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2646 (0.2314) ([0.112]+[0.153])	Prec@1 96.094 (97.370)
Epoch: [234][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2170 (0.2338) ([0.064]+[0.153])	Prec@1 98.438 (97.236)
Epoch: [234][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2078 (0.2389) ([0.055]+[0.153])	Prec@1 98.438 (97.075)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5168 (0.5168) ([0.364]+[0.153])	Prec@1 92.969 (92.969)
 * Prec@1 90.930
current lr 1.00000e-02
Grad=  tensor(2.8155, device='cuda:0')
Epoch: [235][0/391]	Time 0.166 (0.166)	Data 0.125 (0.125)	Loss 0.1881 (0.1881) ([0.035]+[0.153])	Prec@1 100.000 (100.000)
Epoch: [235][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2583 (0.2265) ([0.106]+[0.153])	Prec@1 96.875 (97.602)
Epoch: [235][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2108 (0.2278) ([0.058]+[0.153])	Prec@1 97.656 (97.505)
Epoch: [235][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3086 (0.2319) ([0.156]+[0.153])	Prec@1 94.531 (97.358)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3371 (0.3371) ([0.184]+[0.153])	Prec@1 94.531 (94.531)
 * Prec@1 91.440
current lr 1.00000e-02
Grad=  tensor(7.7073, device='cuda:0')
Epoch: [236][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2633 (0.2633) ([0.111]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [236][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2383 (0.2327) ([0.086]+[0.153])	Prec@1 98.438 (97.362)
Epoch: [236][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2069 (0.2307) ([0.054]+[0.153])	Prec@1 98.438 (97.388)
Epoch: [236][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2305 (0.2352) ([0.078]+[0.153])	Prec@1 97.656 (97.199)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.5786 (0.5786) ([0.426]+[0.153])	Prec@1 88.281 (88.281)
 * Prec@1 90.270
current lr 1.00000e-02
Grad=  tensor(10.9698, device='cuda:0')
Epoch: [237][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.2522 (0.2522) ([0.099]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [237][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1949 (0.2173) ([0.042]+[0.152])	Prec@1 99.219 (97.919)
Epoch: [237][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2446 (0.2263) ([0.092]+[0.152])	Prec@1 95.312 (97.540)
Epoch: [237][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2384 (0.2280) ([0.086]+[0.152])	Prec@1 96.875 (97.480)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3244 (0.3244) ([0.172]+[0.152])	Prec@1 95.312 (95.312)
 * Prec@1 91.870
current lr 1.00000e-02
Grad=  tensor(10.5329, device='cuda:0')
Epoch: [238][0/391]	Time 0.164 (0.164)	Data 0.123 (0.123)	Loss 0.2408 (0.2408) ([0.088]+[0.152])	Prec@1 95.312 (95.312)
Epoch: [238][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2201 (0.2275) ([0.068]+[0.152])	Prec@1 97.656 (97.532)
Epoch: [238][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3006 (0.2296) ([0.148]+[0.152])	Prec@1 96.094 (97.439)
Epoch: [238][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2018 (0.2330) ([0.050]+[0.152])	Prec@1 97.656 (97.285)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3532 (0.3532) ([0.201]+[0.152])	Prec@1 92.969 (92.969)
 * Prec@1 90.760
current lr 1.00000e-02
Grad=  tensor(4.7640, device='cuda:0')
Epoch: [239][0/391]	Time 0.163 (0.163)	Data 0.122 (0.122)	Loss 0.2502 (0.2502) ([0.098]+[0.152])	Prec@1 98.438 (98.438)
Epoch: [239][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2093 (0.2315) ([0.057]+[0.152])	Prec@1 98.438 (97.378)
Epoch: [239][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2355 (0.2320) ([0.083]+[0.152])	Prec@1 96.875 (97.322)
Epoch: [239][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2506 (0.2363) ([0.098]+[0.153])	Prec@1 98.438 (97.176)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4590 (0.4590) ([0.307]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 91.220
current lr 1.00000e-02
Grad=  tensor(8.5094, device='cuda:0')
Epoch: [240][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2085 (0.2085) ([0.056]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [240][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.3002 (0.2253) ([0.148]+[0.152])	Prec@1 94.531 (97.556)
Epoch: [240][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1844 (0.2282) ([0.032]+[0.152])	Prec@1 100.000 (97.470)
Epoch: [240][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2469 (0.2332) ([0.095]+[0.152])	Prec@1 95.312 (97.275)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3423 (0.3423) ([0.190]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.310
current lr 1.00000e-02
Grad=  tensor(7.7609, device='cuda:0')
Epoch: [241][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.2168 (0.2168) ([0.064]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [241][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2642 (0.2325) ([0.112]+[0.153])	Prec@1 96.875 (97.331)
Epoch: [241][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2619 (0.2371) ([0.109]+[0.153])	Prec@1 94.531 (97.135)
Epoch: [241][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3516 (0.2372) ([0.199]+[0.153])	Prec@1 92.969 (97.140)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3145 (0.3145) ([0.162]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.370
current lr 1.00000e-02
Grad=  tensor(11.6926, device='cuda:0')
Epoch: [242][0/391]	Time 0.175 (0.175)	Data 0.134 (0.134)	Loss 0.2534 (0.2534) ([0.101]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [242][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1776 (0.2238) ([0.025]+[0.153])	Prec@1 99.219 (97.602)
Epoch: [242][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2390 (0.2262) ([0.086]+[0.153])	Prec@1 96.875 (97.474)
Epoch: [242][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.3221 (0.2303) ([0.169]+[0.153])	Prec@1 95.312 (97.360)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5222 (0.5222) ([0.369]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(9.5477, device='cuda:0')
Epoch: [243][0/391]	Time 0.170 (0.170)	Data 0.128 (0.128)	Loss 0.2373 (0.2373) ([0.084]+[0.153])	Prec@1 96.094 (96.094)
Epoch: [243][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2089 (0.2359) ([0.056]+[0.153])	Prec@1 97.656 (97.092)
Epoch: [243][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2324 (0.2336) ([0.080]+[0.153])	Prec@1 97.656 (97.283)
Epoch: [243][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2173 (0.2358) ([0.065]+[0.153])	Prec@1 97.656 (97.246)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3716 (0.3716) ([0.219]+[0.153])	Prec@1 93.750 (93.750)
 * Prec@1 91.910
current lr 1.00000e-02
Grad=  tensor(5.8658, device='cuda:0')
Epoch: [244][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.2044 (0.2044) ([0.052]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [244][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2672 (0.2221) ([0.115]+[0.152])	Prec@1 94.531 (97.664)
Epoch: [244][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2131 (0.2233) ([0.061]+[0.152])	Prec@1 96.875 (97.645)
Epoch: [244][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2120 (0.2271) ([0.060]+[0.152])	Prec@1 98.438 (97.513)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4843 (0.4843) ([0.331]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 91.400
current lr 1.00000e-02
Grad=  tensor(4.0813, device='cuda:0')
Epoch: [245][0/391]	Time 0.168 (0.168)	Data 0.127 (0.127)	Loss 0.1921 (0.1921) ([0.039]+[0.153])	Prec@1 98.438 (98.438)
Epoch: [245][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2159 (0.2309) ([0.063]+[0.153])	Prec@1 99.219 (97.416)
Epoch: [245][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2226 (0.2319) ([0.070]+[0.153])	Prec@1 98.438 (97.310)
Epoch: [245][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2495 (0.2372) ([0.097]+[0.153])	Prec@1 96.094 (97.124)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4190 (0.4190) ([0.266]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 91.670
current lr 1.00000e-02
Grad=  tensor(4.1734, device='cuda:0')
Epoch: [246][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.2080 (0.2080) ([0.055]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [246][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2614 (0.2264) ([0.109]+[0.153])	Prec@1 94.531 (97.432)
Epoch: [246][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2185 (0.2282) ([0.066]+[0.153])	Prec@1 99.219 (97.439)
Epoch: [246][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2706 (0.2331) ([0.118]+[0.153])	Prec@1 95.312 (97.270)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.4803 (0.4803) ([0.327]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 90.960
current lr 1.00000e-02
Grad=  tensor(5.5756, device='cuda:0')
Epoch: [247][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2351 (0.2351) ([0.082]+[0.153])	Prec@1 97.656 (97.656)
Epoch: [247][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2517 (0.2227) ([0.099]+[0.153])	Prec@1 97.656 (97.757)
Epoch: [247][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2194 (0.2264) ([0.067]+[0.153])	Prec@1 97.656 (97.563)
Epoch: [247][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss 0.2797 (0.2281) ([0.127]+[0.153])	Prec@1 96.875 (97.454)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.5050 (0.5050) ([0.352]+[0.153])	Prec@1 89.844 (89.844)
 * Prec@1 91.550
current lr 1.00000e-02
Grad=  tensor(3.4427, device='cuda:0')
Epoch: [248][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.1828 (0.1828) ([0.030]+[0.153])	Prec@1 99.219 (99.219)
Epoch: [248][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.2333 (0.2298) ([0.081]+[0.153])	Prec@1 96.094 (97.347)
Epoch: [248][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2199 (0.2337) ([0.067]+[0.153])	Prec@1 96.875 (97.201)
Epoch: [248][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2533 (0.2333) ([0.101]+[0.153])	Prec@1 96.094 (97.212)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.4989 (0.4989) ([0.346]+[0.153])	Prec@1 91.406 (91.406)
 * Prec@1 91.580
current lr 1.00000e-02
Grad=  tensor(7.2967, device='cuda:0')
Epoch: [249][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.2267 (0.2267) ([0.074]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [249][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1905 (0.2276) ([0.038]+[0.153])	Prec@1 100.000 (97.455)
Epoch: [249][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2066 (0.2314) ([0.054]+[0.153])	Prec@1 98.438 (97.341)
Epoch: [249][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2340 (0.2282) ([0.082]+[0.152])	Prec@1 96.094 (97.464)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.3966 (0.3966) ([0.244]+[0.153])	Prec@1 92.188 (92.188)
 * Prec@1 91.560
current lr 1.00000e-03
Grad=  tensor(7.1923, device='cuda:0')
Epoch: [250][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.2055 (0.2055) ([0.053]+[0.153])	Prec@1 96.875 (96.875)
Epoch: [250][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1798 (0.1994) ([0.031]+[0.149])	Prec@1 99.219 (98.414)
Epoch: [250][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.2201 (0.1920) ([0.071]+[0.149])	Prec@1 96.875 (98.667)
Epoch: [250][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1758 (0.1882) ([0.027]+[0.149])	Prec@1 98.438 (98.801)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3454 (0.3454) ([0.197]+[0.149])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-03
Grad=  tensor(1.6158, device='cuda:0')
Epoch: [251][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.1694 (0.1694) ([0.021]+[0.149])	Prec@1 100.000 (100.000)
Epoch: [251][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1734 (0.1730) ([0.025]+[0.148])	Prec@1 99.219 (99.327)
Epoch: [251][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1602 (0.1730) ([0.012]+[0.148])	Prec@1 100.000 (99.296)
Epoch: [251][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1970 (0.1724) ([0.049]+[0.148])	Prec@1 97.656 (99.307)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3503 (0.3503) ([0.202]+[0.148])	Prec@1 96.094 (96.094)
 * Prec@1 94.290
current lr 1.00000e-03
Grad=  tensor(1.4813, device='cuda:0')
Epoch: [252][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.1660 (0.1660) ([0.018]+[0.148])	Prec@1 100.000 (100.000)
Epoch: [252][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1587 (0.1651) ([0.011]+[0.148])	Prec@1 100.000 (99.567)
Epoch: [252][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1603 (0.1646) ([0.013]+[0.148])	Prec@1 100.000 (99.596)
Epoch: [252][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss 0.1561 (0.1645) ([0.009]+[0.147])	Prec@1 100.000 (99.593)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3493 (0.3493) ([0.202]+[0.147])	Prec@1 95.312 (95.312)
 * Prec@1 94.390
current lr 1.00000e-03
Grad=  tensor(1.4848, device='cuda:0')
Epoch: [253][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.1653 (0.1653) ([0.018]+[0.147])	Prec@1 99.219 (99.219)
Epoch: [253][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1549 (0.1631) ([0.008]+[0.147])	Prec@1 100.000 (99.606)
Epoch: [253][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1554 (0.1628) ([0.008]+[0.147])	Prec@1 100.000 (99.607)
Epoch: [253][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1706 (0.1621) ([0.024]+[0.147])	Prec@1 99.219 (99.639)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3648 (0.3648) ([0.218]+[0.147])	Prec@1 94.531 (94.531)
 * Prec@1 94.550
current lr 1.00000e-03
Grad=  tensor(0.5551, device='cuda:0')
Epoch: [254][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1551 (0.1551) ([0.008]+[0.147])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1613 (0.1592) ([0.015]+[0.147])	Prec@1 100.000 (99.698)
Epoch: [254][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1553 (0.1594) ([0.009]+[0.146])	Prec@1 100.000 (99.705)
Epoch: [254][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1568 (0.1601) ([0.011]+[0.146])	Prec@1 100.000 (99.676)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3583 (0.3583) ([0.212]+[0.146])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-03
Grad=  tensor(0.5574, device='cuda:0')
Epoch: [255][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1531 (0.1531) ([0.007]+[0.146])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1518 (0.1572) ([0.006]+[0.146])	Prec@1 100.000 (99.791)
Epoch: [255][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1582 (0.1574) ([0.012]+[0.146])	Prec@1 100.000 (99.767)
Epoch: [255][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1511 (0.1577) ([0.005]+[0.146])	Prec@1 100.000 (99.759)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3445 (0.3445) ([0.199]+[0.146])	Prec@1 96.094 (96.094)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.4633, device='cuda:0')
Epoch: [256][0/391]	Time 0.167 (0.167)	Data 0.125 (0.125)	Loss 0.1539 (0.1539) ([0.008]+[0.146])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1631 (0.1564) ([0.018]+[0.145])	Prec@1 99.219 (99.760)
Epoch: [256][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1511 (0.1556) ([0.006]+[0.145])	Prec@1 100.000 (99.794)
Epoch: [256][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1507 (0.1557) ([0.006]+[0.145])	Prec@1 100.000 (99.785)
Test: [0/79]	Time 0.130 (0.130)	Loss 0.3534 (0.3534) ([0.208]+[0.145])	Prec@1 94.531 (94.531)
 * Prec@1 94.500
current lr 1.00000e-03
Grad=  tensor(0.4463, device='cuda:0')
Epoch: [257][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.1495 (0.1495) ([0.004]+[0.145])	Prec@1 100.000 (100.000)
Epoch: [257][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1573 (0.1538) ([0.012]+[0.145])	Prec@1 100.000 (99.845)
Epoch: [257][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1482 (0.1542) ([0.003]+[0.145])	Prec@1 100.000 (99.821)
Epoch: [257][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1568 (0.1539) ([0.012]+[0.145])	Prec@1 100.000 (99.834)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.3566 (0.3566) ([0.212]+[0.144])	Prec@1 94.531 (94.531)
 * Prec@1 94.550
current lr 1.00000e-03
Grad=  tensor(0.3618, device='cuda:0')
Epoch: [258][0/391]	Time 0.163 (0.163)	Data 0.121 (0.121)	Loss 0.1487 (0.1487) ([0.004]+[0.144])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1536 (0.1526) ([0.009]+[0.144])	Prec@1 99.219 (99.892)
Epoch: [258][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1487 (0.1534) ([0.005]+[0.144])	Prec@1 100.000 (99.856)
Epoch: [258][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1578 (0.1534) ([0.014]+[0.144])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3707 (0.3707) ([0.227]+[0.144])	Prec@1 93.750 (93.750)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(4.0871, device='cuda:0')
Epoch: [259][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1661 (0.1661) ([0.022]+[0.144])	Prec@1 99.219 (99.219)
Epoch: [259][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1548 (0.1537) ([0.011]+[0.144])	Prec@1 100.000 (99.752)
Epoch: [259][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1753 (0.1534) ([0.032]+[0.144])	Prec@1 99.219 (99.790)
Epoch: [259][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1604 (0.1531) ([0.017]+[0.143])	Prec@1 99.219 (99.798)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3624 (0.3624) ([0.219]+[0.143])	Prec@1 93.750 (93.750)
 * Prec@1 94.490
current lr 1.00000e-03
Grad=  tensor(3.8974, device='cuda:0')
Epoch: [260][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1579 (0.1579) ([0.015]+[0.143])	Prec@1 99.219 (99.219)
Epoch: [260][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1601 (0.1506) ([0.017]+[0.143])	Prec@1 99.219 (99.884)
Epoch: [260][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1605 (0.1515) ([0.017]+[0.143])	Prec@1 99.219 (99.825)
Epoch: [260][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1587 (0.1518) ([0.016]+[0.143])	Prec@1 99.219 (99.824)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3832 (0.3832) ([0.240]+[0.143])	Prec@1 93.750 (93.750)
 * Prec@1 94.630
current lr 1.00000e-03
Grad=  tensor(0.3816, device='cuda:0')
Epoch: [261][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1466 (0.1466) ([0.004]+[0.143])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1487 (0.1499) ([0.006]+[0.143])	Prec@1 100.000 (99.923)
Epoch: [261][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1566 (0.1498) ([0.014]+[0.142])	Prec@1 99.219 (99.903)
Epoch: [261][300/391]	Time 0.035 (0.036)	Data 0.000 (0.001)	Loss 0.1476 (0.1502) ([0.005]+[0.142])	Prec@1 100.000 (99.875)
Test: [0/79]	Time 0.131 (0.131)	Loss 0.3915 (0.3915) ([0.249]+[0.142])	Prec@1 93.750 (93.750)
 * Prec@1 94.580
current lr 1.00000e-03
Grad=  tensor(2.2872, device='cuda:0')
Epoch: [262][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.1591 (0.1591) ([0.017]+[0.142])	Prec@1 99.219 (99.219)
Epoch: [262][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1667 (0.1496) ([0.025]+[0.142])	Prec@1 99.219 (99.861)
Epoch: [262][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1511 (0.1501) ([0.009]+[0.142])	Prec@1 99.219 (99.841)
Epoch: [262][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1449 (0.1497) ([0.003]+[0.142])	Prec@1 100.000 (99.855)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.3542 (0.3542) ([0.213]+[0.142])	Prec@1 95.312 (95.312)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.3254, device='cuda:0')
Epoch: [263][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1446 (0.1446) ([0.003]+[0.142])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1472 (0.1492) ([0.006]+[0.141])	Prec@1 100.000 (99.853)
Epoch: [263][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1456 (0.1488) ([0.004]+[0.141])	Prec@1 100.000 (99.864)
Epoch: [263][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1447 (0.1484) ([0.003]+[0.141])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3628 (0.3628) ([0.222]+[0.141])	Prec@1 95.312 (95.312)
 * Prec@1 94.790
current lr 1.00000e-03
Grad=  tensor(0.7211, device='cuda:0')
Epoch: [264][0/391]	Time 0.210 (0.210)	Data 0.170 (0.170)	Loss 0.1479 (0.1479) ([0.007]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.036 (0.038)	Data 0.000 (0.002)	Loss 0.1448 (0.1476) ([0.004]+[0.141])	Prec@1 100.000 (99.923)
Epoch: [264][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1502 (0.1475) ([0.009]+[0.141])	Prec@1 100.000 (99.907)
Epoch: [264][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1484 (0.1476) ([0.008]+[0.141])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3814 (0.3814) ([0.241]+[0.141])	Prec@1 93.750 (93.750)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.9085, device='cuda:0')
Epoch: [265][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1453 (0.1453) ([0.005]+[0.141])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1474 (0.1463) ([0.007]+[0.140])	Prec@1 100.000 (99.938)
Epoch: [265][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1465 (0.1463) ([0.006]+[0.140])	Prec@1 100.000 (99.930)
Epoch: [265][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1439 (0.1463) ([0.004]+[0.140])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.4018 (0.4018) ([0.262]+[0.140])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(2.4968, device='cuda:0')
Epoch: [266][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1522 (0.1522) ([0.012]+[0.140])	Prec@1 99.219 (99.219)
Epoch: [266][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1466 (0.1463) ([0.007]+[0.140])	Prec@1 100.000 (99.907)
Epoch: [266][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1436 (0.1459) ([0.004]+[0.140])	Prec@1 100.000 (99.918)
Epoch: [266][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1436 (0.1461) ([0.004]+[0.140])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.4131 (0.4131) ([0.274]+[0.139])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.3157, device='cuda:0')
Epoch: [267][0/391]	Time 0.164 (0.164)	Data 0.120 (0.120)	Loss 0.1420 (0.1420) ([0.003]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1419 (0.1450) ([0.003]+[0.139])	Prec@1 100.000 (99.923)
Epoch: [267][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1405 (0.1451) ([0.001]+[0.139])	Prec@1 100.000 (99.926)
Epoch: [267][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1466 (0.1453) ([0.008]+[0.139])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3989 (0.3989) ([0.260]+[0.139])	Prec@1 92.969 (92.969)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.3002, device='cuda:0')
Epoch: [268][0/391]	Time 0.173 (0.173)	Data 0.123 (0.123)	Loss 0.1415 (0.1415) ([0.003]+[0.139])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1449 (0.1443) ([0.006]+[0.139])	Prec@1 100.000 (99.930)
Epoch: [268][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1481 (0.1447) ([0.009]+[0.139])	Prec@1 99.219 (99.891)
Epoch: [268][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1410 (0.1446) ([0.003]+[0.138])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3980 (0.3980) ([0.260]+[0.138])	Prec@1 93.750 (93.750)
 * Prec@1 94.660
current lr 1.00000e-03
Grad=  tensor(1.3064, device='cuda:0')
Epoch: [269][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1460 (0.1460) ([0.008]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1485 (0.1437) ([0.010]+[0.138])	Prec@1 100.000 (99.961)
Epoch: [269][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1436 (0.1438) ([0.006]+[0.138])	Prec@1 100.000 (99.938)
Epoch: [269][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1432 (0.1435) ([0.005]+[0.138])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3818 (0.3818) ([0.244]+[0.138])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.4223, device='cuda:0')
Epoch: [270][0/391]	Time 0.165 (0.165)	Data 0.123 (0.123)	Loss 0.1411 (0.1411) ([0.003]+[0.138])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1480 (0.1439) ([0.010]+[0.138])	Prec@1 99.219 (99.861)
Epoch: [270][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1401 (0.1435) ([0.003]+[0.138])	Prec@1 100.000 (99.887)
Epoch: [270][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1418 (0.1431) ([0.004]+[0.137])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3884 (0.3884) ([0.251]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 94.670
current lr 1.00000e-03
Grad=  tensor(0.5999, device='cuda:0')
Epoch: [271][0/391]	Time 0.165 (0.165)	Data 0.122 (0.122)	Loss 0.1418 (0.1418) ([0.004]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1480 (0.1426) ([0.011]+[0.137])	Prec@1 100.000 (99.954)
Epoch: [271][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1388 (0.1423) ([0.002]+[0.137])	Prec@1 100.000 (99.957)
Epoch: [271][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1612 (0.1423) ([0.024]+[0.137])	Prec@1 99.219 (99.938)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3793 (0.3793) ([0.243]+[0.137])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.4162, device='cuda:0')
Epoch: [272][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1410 (0.1410) ([0.004]+[0.137])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.036 (0.038)	Data 0.000 (0.001)	Loss 0.1397 (0.1427) ([0.003]+[0.137])	Prec@1 100.000 (99.907)
Epoch: [272][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1556 (0.1423) ([0.019]+[0.136])	Prec@1 99.219 (99.903)
Epoch: [272][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1386 (0.1422) ([0.002]+[0.136])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3989 (0.3989) ([0.263]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.5858, device='cuda:0')
Epoch: [273][0/391]	Time 0.166 (0.166)	Data 0.124 (0.124)	Loss 0.1412 (0.1412) ([0.005]+[0.136])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1413 (0.1417) ([0.005]+[0.136])	Prec@1 100.000 (99.923)
Epoch: [273][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1379 (0.1410) ([0.002]+[0.136])	Prec@1 100.000 (99.934)
Epoch: [273][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1391 (0.1410) ([0.003]+[0.136])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3994 (0.3994) ([0.264]+[0.136])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.3447, device='cuda:0')
Epoch: [274][0/391]	Time 0.167 (0.167)	Data 0.123 (0.123)	Loss 0.1385 (0.1385) ([0.003]+[0.136])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1380 (0.1400) ([0.002]+[0.136])	Prec@1 100.000 (99.961)
Epoch: [274][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1383 (0.1399) ([0.003]+[0.135])	Prec@1 100.000 (99.953)
Epoch: [274][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1397 (0.1399) ([0.004]+[0.135])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.135 (0.135)	Loss 0.4104 (0.4104) ([0.275]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.2898, device='cuda:0')
Epoch: [275][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1366 (0.1366) ([0.001]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1417 (0.1394) ([0.007]+[0.135])	Prec@1 100.000 (99.946)
Epoch: [275][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1366 (0.1396) ([0.002]+[0.135])	Prec@1 100.000 (99.930)
Epoch: [275][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1433 (0.1397) ([0.009]+[0.135])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3938 (0.3938) ([0.259]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.3751, device='cuda:0')
Epoch: [276][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.1376 (0.1376) ([0.003]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1410 (0.1389) ([0.006]+[0.134])	Prec@1 100.000 (99.977)
Epoch: [276][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1372 (0.1394) ([0.003]+[0.134])	Prec@1 100.000 (99.942)
Epoch: [276][300/391]	Time 0.039 (0.037)	Data 0.000 (0.001)	Loss 0.1364 (0.1392) ([0.002]+[0.134])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.4017 (0.4017) ([0.268]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 94.680
current lr 1.00000e-03
Grad=  tensor(0.6919, device='cuda:0')
Epoch: [277][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1389 (0.1389) ([0.005]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1365 (0.1381) ([0.003]+[0.134])	Prec@1 100.000 (99.954)
Epoch: [277][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1382 (0.1383) ([0.004]+[0.134])	Prec@1 100.000 (99.957)
Epoch: [277][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1361 (0.1383) ([0.002]+[0.134])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3877 (0.3877) ([0.254]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 94.820
current lr 1.00000e-03
Grad=  tensor(0.2950, device='cuda:0')
Epoch: [278][0/391]	Time 0.171 (0.171)	Data 0.127 (0.127)	Loss 0.1349 (0.1349) ([0.001]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1366 (0.1372) ([0.003]+[0.133])	Prec@1 100.000 (99.969)
Epoch: [278][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1375 (0.1375) ([0.004]+[0.133])	Prec@1 100.000 (99.949)
Epoch: [278][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1347 (0.1374) ([0.002]+[0.133])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3918 (0.3918) ([0.259]+[0.133])	Prec@1 93.750 (93.750)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(1.0298, device='cuda:0')
Epoch: [279][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1393 (0.1393) ([0.006]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1372 (0.1370) ([0.004]+[0.133])	Prec@1 100.000 (99.961)
Epoch: [279][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1361 (0.1369) ([0.003]+[0.133])	Prec@1 100.000 (99.957)
Epoch: [279][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1351 (0.1370) ([0.002]+[0.133])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.4027 (0.4027) ([0.270]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.7633, device='cuda:0')
Epoch: [280][0/391]	Time 0.165 (0.165)	Data 0.124 (0.124)	Loss 0.1374 (0.1374) ([0.005]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1350 (0.1362) ([0.003]+[0.132])	Prec@1 100.000 (99.969)
Epoch: [280][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1344 (0.1370) ([0.002]+[0.132])	Prec@1 100.000 (99.930)
Epoch: [280][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1350 (0.1367) ([0.003]+[0.132])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3479 (0.3479) ([0.216]+[0.132])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.4955, device='cuda:0')
Epoch: [281][0/391]	Time 0.167 (0.167)	Data 0.126 (0.126)	Loss 0.1363 (0.1363) ([0.004]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1344 (0.1360) ([0.003]+[0.132])	Prec@1 100.000 (99.930)
Epoch: [281][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1343 (0.1362) ([0.003]+[0.132])	Prec@1 100.000 (99.938)
Epoch: [281][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1350 (0.1360) ([0.004]+[0.132])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3845 (0.3845) ([0.253]+[0.131])	Prec@1 93.750 (93.750)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.3073, device='cuda:0')
Epoch: [282][0/391]	Time 0.164 (0.164)	Data 0.122 (0.122)	Loss 0.1329 (0.1329) ([0.001]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1427 (0.1355) ([0.011]+[0.131])	Prec@1 99.219 (99.938)
Epoch: [282][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1354 (0.1352) ([0.004]+[0.131])	Prec@1 100.000 (99.969)
Epoch: [282][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1350 (0.1352) ([0.004]+[0.131])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3711 (0.3711) ([0.240]+[0.131])	Prec@1 93.750 (93.750)
 * Prec@1 94.870
current lr 1.00000e-03
Grad=  tensor(2.5490, device='cuda:0')
Epoch: [283][0/391]	Time 0.169 (0.169)	Data 0.127 (0.127)	Loss 0.1409 (0.1409) ([0.010]+[0.131])	Prec@1 99.219 (99.219)
Epoch: [283][100/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1330 (0.1347) ([0.002]+[0.131])	Prec@1 100.000 (99.946)
Epoch: [283][200/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1321 (0.1346) ([0.002]+[0.131])	Prec@1 100.000 (99.946)
Epoch: [283][300/391]	Time 0.036 (0.036)	Data 0.000 (0.001)	Loss 0.1343 (0.1346) ([0.004]+[0.130])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3506 (0.3506) ([0.220]+[0.130])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(1.4755, device='cuda:0')
Epoch: [284][0/391]	Time 0.172 (0.172)	Data 0.130 (0.130)	Loss 0.1415 (0.1415) ([0.011]+[0.130])	Prec@1 99.219 (99.219)
Epoch: [284][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1355 (0.1338) ([0.005]+[0.130])	Prec@1 100.000 (99.969)
Epoch: [284][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1353 (0.1341) ([0.005]+[0.130])	Prec@1 100.000 (99.957)
Epoch: [284][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1324 (0.1339) ([0.002]+[0.130])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3385 (0.3385) ([0.209]+[0.130])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.2856, device='cuda:0')
Epoch: [285][0/391]	Time 0.168 (0.168)	Data 0.125 (0.125)	Loss 0.1308 (0.1308) ([0.001]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1326 (0.1333) ([0.003]+[0.130])	Prec@1 100.000 (99.969)
Epoch: [285][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1315 (0.1331) ([0.002]+[0.130])	Prec@1 100.000 (99.984)
Epoch: [285][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1354 (0.1332) ([0.006]+[0.129])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.128 (0.128)	Loss 0.3522 (0.3522) ([0.223]+[0.129])	Prec@1 94.531 (94.531)
 * Prec@1 94.780
current lr 1.00000e-03
Grad=  tensor(0.6385, device='cuda:0')
Epoch: [286][0/391]	Time 0.171 (0.171)	Data 0.129 (0.129)	Loss 0.1336 (0.1336) ([0.004]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1307 (0.1328) ([0.002]+[0.129])	Prec@1 100.000 (99.985)
Epoch: [286][200/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1326 (0.1329) ([0.004]+[0.129])	Prec@1 100.000 (99.973)
Epoch: [286][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1307 (0.1327) ([0.002]+[0.129])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3860 (0.3860) ([0.257]+[0.129])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.3915, device='cuda:0')
Epoch: [287][0/391]	Time 0.170 (0.170)	Data 0.128 (0.128)	Loss 0.1326 (0.1326) ([0.004]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1337 (0.1323) ([0.005]+[0.129])	Prec@1 100.000 (99.969)
Epoch: [287][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1359 (0.1322) ([0.007]+[0.128])	Prec@1 99.219 (99.961)
Epoch: [287][300/391]	Time 0.036 (0.037)	Data 0.000 (0.001)	Loss 0.1307 (0.1321) ([0.002]+[0.128])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3775 (0.3775) ([0.249]+[0.128])	Prec@1 93.750 (93.750)
 * Prec@1 94.870
current lr 1.00000e-03
Grad=  tensor(2.7493, device='cuda:0')
Epoch: [288][0/391]	Time 0.171 (0.171)	Data 0.129 (0.129)	Loss 0.1423 (0.1423) ([0.014]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1290 (0.1315) ([0.001]+[0.128])	Prec@1 100.000 (99.969)
Epoch: [288][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1332 (0.1317) ([0.005]+[0.128])	Prec@1 100.000 (99.949)
Epoch: [288][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1329 (0.1318) ([0.005]+[0.128])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3650 (0.3650) ([0.237]+[0.128])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.7620, device='cuda:0')
Epoch: [289][0/391]	Time 0.182 (0.182)	Data 0.139 (0.139)	Loss 0.1319 (0.1319) ([0.004]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.037 (0.038)	Data 0.000 (0.002)	Loss 0.1316 (0.1315) ([0.004]+[0.128])	Prec@1 100.000 (99.954)
Epoch: [289][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1353 (0.1316) ([0.008]+[0.127])	Prec@1 100.000 (99.953)
Epoch: [289][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1325 (0.1313) ([0.005]+[0.127])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3599 (0.3599) ([0.233]+[0.127])	Prec@1 94.531 (94.531)
 * Prec@1 94.860
current lr 1.00000e-03
Grad=  tensor(0.2929, device='cuda:0')
Epoch: [290][0/391]	Time 0.168 (0.168)	Data 0.126 (0.126)	Loss 0.1285 (0.1285) ([0.001]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1297 (0.1304) ([0.003]+[0.127])	Prec@1 100.000 (99.977)
Epoch: [290][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1291 (0.1307) ([0.002]+[0.127])	Prec@1 100.000 (99.953)
Epoch: [290][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1359 (0.1306) ([0.009]+[0.127])	Prec@1 99.219 (99.953)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.3657 (0.3657) ([0.239]+[0.127])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.2863, device='cuda:0')
Epoch: [291][0/391]	Time 0.174 (0.174)	Data 0.130 (0.130)	Loss 0.1275 (0.1275) ([0.001]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1292 (0.1299) ([0.003]+[0.126])	Prec@1 100.000 (99.954)
Epoch: [291][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1302 (0.1301) ([0.004]+[0.126])	Prec@1 100.000 (99.957)
Epoch: [291][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1280 (0.1302) ([0.002]+[0.126])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3622 (0.3622) ([0.236]+[0.126])	Prec@1 93.750 (93.750)
 * Prec@1 94.740
current lr 1.00000e-03
Grad=  tensor(0.3198, device='cuda:0')
Epoch: [292][0/391]	Time 0.171 (0.171)	Data 0.125 (0.125)	Loss 0.1276 (0.1276) ([0.002]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1278 (0.1295) ([0.002]+[0.126])	Prec@1 100.000 (99.961)
Epoch: [292][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1277 (0.1294) ([0.002]+[0.126])	Prec@1 100.000 (99.961)
Epoch: [292][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1274 (0.1294) ([0.002]+[0.126])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.129 (0.129)	Loss 0.3756 (0.3756) ([0.250]+[0.126])	Prec@1 93.750 (93.750)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.2937, device='cuda:0')
Epoch: [293][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1275 (0.1275) ([0.002]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1269 (0.1288) ([0.001]+[0.125])	Prec@1 100.000 (99.969)
Epoch: [293][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1300 (0.1291) ([0.005]+[0.125])	Prec@1 100.000 (99.953)
Epoch: [293][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1301 (0.1290) ([0.005]+[0.125])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3902 (0.3902) ([0.265]+[0.125])	Prec@1 93.750 (93.750)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(2.1734, device='cuda:0')
Epoch: [294][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1382 (0.1382) ([0.013]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1268 (0.1284) ([0.002]+[0.125])	Prec@1 100.000 (99.961)
Epoch: [294][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1338 (0.1284) ([0.009]+[0.125])	Prec@1 99.219 (99.969)
Epoch: [294][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1306 (0.1285) ([0.006]+[0.125])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3894 (0.3894) ([0.265]+[0.125])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-03
Grad=  tensor(0.3055, device='cuda:0')
Epoch: [295][0/391]	Time 0.166 (0.166)	Data 0.123 (0.123)	Loss 0.1265 (0.1265) ([0.002]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1259 (0.1279) ([0.001]+[0.124])	Prec@1 100.000 (99.985)
Epoch: [295][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1254 (0.1277) ([0.001]+[0.124])	Prec@1 100.000 (99.981)
Epoch: [295][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1266 (0.1277) ([0.002]+[0.124])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.3728 (0.3728) ([0.249]+[0.124])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.3276, device='cuda:0')
Epoch: [296][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.1266 (0.1266) ([0.003]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1305 (0.1278) ([0.007]+[0.124])	Prec@1 100.000 (99.946)
Epoch: [296][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1260 (0.1273) ([0.002]+[0.124])	Prec@1 100.000 (99.973)
Epoch: [296][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1260 (0.1271) ([0.002]+[0.124])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.3558 (0.3558) ([0.232]+[0.124])	Prec@1 93.750 (93.750)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.2993, device='cuda:0')
Epoch: [297][0/391]	Time 0.169 (0.169)	Data 0.126 (0.126)	Loss 0.1253 (0.1253) ([0.002]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1243 (0.1273) ([0.001]+[0.123])	Prec@1 100.000 (99.954)
Epoch: [297][200/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1357 (0.1270) ([0.012]+[0.123])	Prec@1 100.000 (99.973)
Epoch: [297][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1247 (0.1267) ([0.002]+[0.123])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3359 (0.3359) ([0.213]+[0.123])	Prec@1 95.312 (95.312)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.2957, device='cuda:0')
Epoch: [298][0/391]	Time 0.172 (0.172)	Data 0.129 (0.129)	Loss 0.1247 (0.1247) ([0.002]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1265 (0.1259) ([0.004]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [298][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1247 (0.1261) ([0.002]+[0.123])	Prec@1 100.000 (99.988)
Epoch: [298][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1242 (0.1261) ([0.002]+[0.123])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3487 (0.3487) ([0.226]+[0.123])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.3396, device='cuda:0')
Epoch: [299][0/391]	Time 0.167 (0.167)	Data 0.124 (0.124)	Loss 0.1246 (0.1246) ([0.002]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1244 (0.1255) ([0.002]+[0.122])	Prec@1 100.000 (99.977)
Epoch: [299][200/391]	Time 0.037 (0.038)	Data 0.000 (0.001)	Loss 0.1239 (0.1256) ([0.002]+[0.122])	Prec@1 100.000 (99.977)
Epoch: [299][300/391]	Time 0.037 (0.037)	Data 0.000 (0.001)	Loss 0.1243 (0.1255) ([0.002]+[0.122])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.3464 (0.3464) ([0.224]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 94.680

 Elapsed time for training  1:18:01.847056

 sparsity of   [0.9259259104728699, 0.9629629850387573, 0.9629629850387573, 0.4444444477558136, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9259259104728699, 0.3333333432674408, 0.9259259104728699, 0.0, 0.9629629850387573, 0.40740740299224854, 0.9629629850387573, 0.9259259104728699, 0.9259259104728699, 0.0, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.25925925374031067, 0.9259259104728699, 0.9259259104728699, 0.9259259104728699, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9259259104728699, 0.9629629850387573, 0.9629629850387573, 0.9259259104728699, 0.9629629850387573, 0.9629629850387573, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.5555555820465088, 0.0, 0.9629629850387573, 0.8888888955116272, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.7777777910232544, 0.9259259104728699, 0.9629629850387573, 0.9259259104728699, 0.7777777910232544]

 sparsity of   [0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9965277910232544, 0.9982638955116272, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0]

 sparsity of   [0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9982638955116272, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2673611044883728]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2074652761220932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1006944477558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.9982638955116272, 0.0, 0.0, 0.1614583283662796, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.7769097089767456, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0998263880610466, 0.0, 0.0, 0.0, 0.0, 0.063368059694767, 0.0, 0.0, 0.0, 0.0, 0.203125, 0.0303819440305233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7690972089767456, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1588541716337204, 0.0, 0.0, 0.8081597089767456, 0.0, 0.6041666865348816, 0.0, 0.0, 0.0694444477558136, 0.0173611119389534, 0.9982638955116272, 0.0954861119389534, 0.02604166604578495, 0.0, 0.8446180820465088, 0.0, 0.0, 0.0, 0.8133680820465088, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.7361111044883728, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0329861119389534, 0.0, 0.0, 0.0, 0.9973958134651184, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0321180559694767, 0.0, 0.4418402910232544, 0.0, 0.9982638955116272, 0.0251736119389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0581597238779068, 0.0, 0.0, 0.1111111119389534, 0.9973958134651184, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0572916679084301, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3324652910232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02951388992369175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0954861119389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2491319477558136, 0.4991319477558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9973958134651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.1241319477558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272]

 sparsity of   [0.0065104165114462376, 0.0, 0.0, 0.006076388992369175, 0.0, 0.1349826455116272, 0.9986979365348816, 0.0, 0.0, 0.0, 0.02387152798473835, 0.0234375, 0.0768229141831398, 0.13671875, 0.1341145783662796, 0.0690104141831398, 0.0, 0.1328125, 0.0, 0.1176215261220932, 0.0, 0.1323784738779068, 0.999131977558136, 0.0, 0.0, 0.0, 0.1284722238779068, 0.0, 0.13671875, 0.0, 0.0, 0.0, 0.0, 0.1328125, 0.0, 0.1662326455116272, 0.0, 0.0, 0.0, 0.1271701455116272, 0.010416666977107525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1315104216337204, 0.0, 0.0, 0.0, 0.0, 0.130642369389534, 0.0, 0.1358506977558136, 0.3250868022441864, 0.8489583134651184, 0.0, 0.13671875, 0.0, 0.0, 0.1297743022441864, 0.13671875, 0.0, 0.0, 0.1762152761220932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1302083283662796, 0.0607638880610466, 0.0, 0.0, 0.09375, 0.1310763955116272, 0.0, 0.1319444477558136, 0.1940104216337204, 0.1076388880610466, 0.8559027910232544, 0.0, 0.1297743022441864, 0.0, 0.0, 0.8806423544883728, 0.0, 0.0, 0.13671875, 0.0, 0.1280381977558136, 0.0, 0.0, 0.0572916679084301, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.130642369389534, 0.0, 0.13671875, 0.0, 0.1297743022441864, 0.1336805522441864, 0.0, 0.13671875, 0.999131977558136, 0.0, 0.4596354067325592, 0.0, 0.0, 0.0, 0.0069444444961845875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.1315104216337204, 0.0, 0.0047743055038154125, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.07421875, 0.0, 0.0, 0.0017361111240461469, 0.1319444477558136, 0.1245659738779068, 0.0, 0.1879340261220932, 0.0, 0.1302083283662796, 0.0920138880610466, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.1310763955116272, 0.0, 0.0, 0.1705729216337204, 0.106336809694767, 0.0, 0.0, 0.0, 0.1336805522441864, 0.999131977558136, 0.0, 0.02604166604578495, 0.999131977558136, 0.9986979365348816, 0.1302083283662796, 0.0186631940305233, 0.999131977558136, 0.0, 0.0, 0.1319444477558136, 0.009114583022892475, 0.1293402761220932, 0.0, 0.0, 0.0, 0.0460069440305233, 0.1215277761220932, 0.0, 0.1319444477558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1332465261220932, 0.1293402761220932, 0.0, 0.0, 0.0, 0.0, 0.4379340410232544, 0.13671875, 0.1336805522441864, 0.0, 0.1284722238779068, 0.0, 0.1254340261220932, 0.1323784738779068, 0.0564236119389534, 0.1336805522441864, 0.1297743022441864, 0.999131977558136, 0.0, 0.13671875, 0.0, 0.0, 0.013020833022892475, 0.125, 0.0, 0.1059027761220932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13671875, 0.0, 0.0, 0.1254340261220932, 0.1158854141831398, 0.3337673544883728, 0.13671875, 0.1006944477558136, 0.0, 0.0, 0.1336805522441864, 0.1328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3498263955116272, 0.0, 0.0, 0.1319444477558136, 0.0, 0.1328125, 0.1336805522441864, 0.0, 0.0, 0.13671875, 0.0, 0.0, 0.0, 0.0, 0.118055559694767, 0.1254340261220932, 0.0, 0.0, 0.0, 0.0, 0.13671875, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0703125, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0677083358168602, 0.999131977558136, 0.0, 0.45703125, 0.0, 0.9995659589767456, 0.0373263880610466, 0.9995659589767456, 0.999131977558136, 0.1966145783662796, 0.0, 0.999131977558136, 0.1167534738779068, 0.9995659589767456, 0.359375, 0.999131977558136, 0.999131977558136, 0.3953993022441864, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.1002604141831398, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8172743320465088, 0.999131977558136, 0.0642361119389534, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.4166666567325592, 0.9995659589767456, 0.9995659589767456, 0.3832465410232544, 0.4197048544883728, 0.0, 0.46875, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.3893229067325592, 0.4118923544883728, 0.999131977558136, 0.4301215410232544, 0.1263020783662796, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.4027777910232544, 0.17578125, 0.3962673544883728, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.3810763955116272, 0.999131977558136, 0.464409738779068, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1475694477558136, 0.999131977558136, 0.0, 0.1401909738779068, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.1119791641831398, 0.9995659589767456, 0.0, 0.9986979365348816, 0.0, 0.0, 0.1692708283662796, 0.09375, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1276041716337204, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.09765625, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.45703125, 0.0, 0.0, 0.0, 0.9986979365348816, 0.4118923544883728, 0.0, 0.999131977558136, 0.0473090298473835, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.4153645932674408, 0.999131977558136, 0.9986979365348816, 0.1028645858168602, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9986979365348816, 0.4171006977558136, 0.0, 0.0421006940305233, 0.09765625, 0.999131977558136, 0.405815988779068, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1089409738779068, 0.999131977558136, 0.0863715261220932, 0.394097238779068, 0.999131977558136, 0.999131977558136, 0.4704861044883728, 0.999131977558136, 0.999131977558136, 0.0559895820915699, 0.999131977558136, 0.9995659589767456, 0.1427951455116272, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.09375, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4001736044883728, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.4118923544883728, 0.2161458283662796, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0763888880610466, 0.2061631977558136, 0.0, 0.9986979365348816, 0.0681423619389534, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.44140625, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9986979365348816, 0.9986979365348816, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.138454869389534, 0.2096354216337204, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0763888880610466, 0.999131977558136, 0.999131977558136]

 sparsity of   [0.7894965410232544, 0.1744791716337204, 0.00434027798473835, 0.7339409589767456, 0.0008680555620230734, 0.7890625, 0.796875, 0.0203993059694767, 0.0, 0.0034722222480922937, 0.0321180559694767, 0.7573784589767456, 0.0646701380610466, 0.0004340277810115367, 0.7907986044883728, 0.7925347089767456, 0.0, 0.7769097089767456, 0.0, 0.0065104165114462376, 0.0030381944961845875, 0.7925347089767456, 0.999131977558136, 0.6024305820465088, 0.0013020833721384406, 0.0026041667442768812, 0.7925347089767456, 0.006076388992369175, 0.0030381944961845875, 0.0086805559694767, 0.7578125, 0.0, 0.6076388955116272, 0.7651909589767456, 0.4422743022441864, 0.999131977558136, 0.6306423544883728, 0.0026041667442768812, 0.005642361007630825, 0.792100727558136, 0.0338541679084301, 0.7873263955116272, 0.0, 0.0, 0.013888888992369175, 0.0, 0.7209201455116272, 0.0047743055038154125, 0.796875, 0.0052083334885537624, 0.796875, 0.7894965410232544, 0.0034722222480922937, 0.005642361007630825, 0.002170138992369175, 0.788194477558136, 0.7387152910232544, 0.796875, 0.7873263955116272, 0.999131977558136, 0.0234375, 0.0481770820915699, 0.0017361111240461469, 0.01953125, 0.0013020833721384406, 0.7855902910232544, 0.7873263955116272, 0.796875, 0.002170138992369175, 0.9748263955116272, 0.0026041667442768812, 0.00824652798473835, 0.013020833022892475, 0.0, 0.4618055522441864, 0.7786458134651184, 0.7717013955116272, 0.7799479365348816, 0.4001736044883728, 0.7912326455116272, 0.7816840410232544, 0.00824652798473835, 0.007378472480922937, 0.7747395634651184, 0.796875, 0.0, 0.009114583022892475, 0.7886284589767456, 0.780381977558136, 0.0052083334885537624, 0.8363715410232544, 0.0, 0.0368923619389534, 0.002170138992369175, 0.002170138992369175, 0.0026041667442768812, 0.0017361111240461469, 0.01953125, 0.6666666865348816, 0.0, 0.0434027798473835, 0.0, 0.0008680555620230734, 0.772569477558136, 0.00434027798473835, 0.7942708134651184, 0.7894965410232544, 0.7621527910232544, 0.7894965410232544, 0.7660590410232544, 0.7777777910232544, 0.4769965410232544, 0.0, 0.0, 0.1506076455116272, 0.11328125, 0.5243055820465088, 0.796875, 0.007378472480922937, 0.796875, 0.7873263955116272, 0.0013020833721384406, 0.7703993320465088, 0.7000868320465088, 0.0, 0.0512152798473835, 0.0034722222480922937, 0.0034722222480922937, 0.7934027910232544, 0.796875, 0.013454861007630825, 0.0364583320915699, 0.7721354365348816, 0.0069444444961845875, 0.0013020833721384406, 0.76171875, 0.002170138992369175, 0.7938368320465088, 0.7916666865348816, 0.007378472480922937, 0.0282118059694767, 0.6814236044883728, 0.796875, 0.7873263955116272, 0.0125868059694767, 0.0, 0.0026041667442768812, 0.002170138992369175, 0.7756076455116272, 0.0, 0.7899305820465088, 0.0026041667442768812, 0.0078125, 0.7947048544883728, 0.7543402910232544, 0.796875, 0.796875, 0.7899305820465088, 0.733506977558136, 0.0, 0.006076388992369175, 0.02213541604578495, 0.971788227558136, 0.8381076455116272, 0.7886284589767456, 0.7925347089767456, 0.7903645634651184, 0.0, 0.6705729365348816, 0.7873263955116272, 0.02473958395421505, 0.796875, 0.009114583022892475, 0.010850694961845875, 0.0017361111240461469, 0.0, 0.796875, 0.0, 0.7860243320465088, 0.7947048544883728, 0.796875, 0.7864583134651184, 0.0, 0.01171875, 0.0004340277810115367, 0.005642361007630825, 0.796006977558136, 0.7907986044883728, 0.00390625, 0.0, 0.0, 0.01171875, 0.850694477558136, 0.7899305820465088, 0.796875, 0.0125868059694767, 0.796875, 0.6657986044883728, 0.7916666865348816, 0.7942708134651184, 0.0030381944961845875, 0.796875, 0.7868923544883728, 0.999131977558136, 0.0030381944961845875, 0.796875, 0.0173611119389534, 0.796875, 0.002170138992369175, 0.7873263955116272, 0.0559895820915699, 0.0512152798473835, 0.0047743055038154125, 0.7786458134651184, 0.0017361111240461469, 0.0, 0.0, 0.780381977558136, 0.0212673619389534, 0.7877604365348816, 0.796875, 0.0338541679084301, 0.0, 0.710069477558136, 0.768663227558136, 0.010850694961845875, 0.796875, 0.0164930559694767, 0.796875, 0.0, 0.0125868059694767, 0.0, 0.01822916604578495, 0.0004340277810115367, 0.8059895634651184, 0.0069444444961845875, 0.0, 0.796875, 0.009982638992369175, 0.7907986044883728, 0.7612847089767456, 0.0, 0.796875, 0.788194477558136, 0.0542534738779068, 0.0, 0.0, 0.0078125, 0.721788227558136, 0.765625, 0.0, 0.0, 0.7912326455116272, 0.0377604179084301, 0.7899305820465088, 0.0]

 sparsity of   [0.909288227558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.1710069477558136, 0.0716145858168602, 0.0442708320915699, 0.0347222238779068, 0.1154513880610466, 0.046875, 0.9995659589767456, 0.9071180820465088, 0.0490451380610466, 0.999131977558136, 0.9986979365348816, 0.874131977558136, 0.999131977558136, 0.999131977558136, 0.0386284738779068, 0.0460069440305233, 0.9982638955116272, 0.999131977558136, 0.0516493059694767, 0.999131977558136, 0.0651041641831398, 0.8046875, 0.9986979365348816, 0.999131977558136, 0.0373263880610466, 0.296875, 0.9995659589767456, 0.03515625, 0.1488715261220932, 0.121961809694767, 0.897569477558136, 0.03515625, 0.1028645858168602, 0.1527777761220932, 0.999131977558136, 0.16796875, 0.0503472238779068, 0.03125, 0.8511284589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.91015625, 0.0316840298473835, 0.0963541641831398, 0.0, 0.0, 0.0546875, 0.999131977558136, 0.0455729179084301, 0.4288194477558136, 0.0989583358168602, 0.1037326380610466, 0.154079869389534, 0.9995659589767456, 0.1471354216337204, 0.063368059694767, 0.1935763955116272, 0.8680555820465088, 0.999131977558136, 0.8849826455116272, 0.0, 0.999131977558136, 0.999131977558136, 0.02994791604578495, 0.02951388992369175, 0.0, 0.075086809694767, 0.02473958395421505, 0.0494791679084301, 0.0802951380610466, 0.1046006977558136, 0.8515625, 0.999131977558136, 0.7873263955116272, 0.046875, 0.02777777798473835, 0.078993059694767, 0.8784722089767456, 0.1232638880610466, 0.1705729216337204, 0.0, 0.109375, 0.1258680522441864, 0.1284722238779068, 0.0, 0.1323784738779068, 0.05078125, 0.01692708395421505, 0.0933159738779068, 0.0325520820915699, 0.0394965298473835, 0.0729166641831398, 0.0342881940305233, 0.8745659589767456, 0.2756076455116272, 0.0, 0.1896701455116272, 0.9986979365348816, 0.0373263880610466, 0.0264756940305233, 0.9995659589767456, 0.0477430559694767, 0.0, 0.0481770820915699, 0.0442708320915699, 0.999131977558136, 0.0386284738779068, 0.140625, 0.02300347201526165, 0.1766493022441864, 0.1293402761220932, 0.999131977558136, 0.0390625, 0.9986979365348816, 0.0407986119389534, 0.0, 0.0390625, 0.0438368059694767, 0.0316840298473835, 0.999131977558136, 0.0846354141831398, 0.1380208283662796, 0.078993059694767, 0.9986979365348816, 0.8745659589767456, 0.9982638955116272, 0.6059027910232544, 0.0421006940305233, 0.8697916865348816, 0.12890625, 0.999131977558136, 0.999131977558136, 0.0638020858168602, 0.0438368059694767, 0.05078125, 0.0620659738779068, 0.0412326380610466, 0.9986979365348816, 0.999131977558136, 0.0611979179084301, 0.999131977558136, 0.433159738779068, 0.1432291716337204, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0993923619389534, 0.0464409738779068, 0.0546875, 0.1436631977558136, 0.999131977558136, 0.2018229216337204, 0.0394965298473835, 0.0, 0.999131977558136, 0.0538194440305233, 0.0342881940305233, 0.999131977558136, 0.1484375, 0.999131977558136, 0.02387152798473835, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.013020833022892475, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.063368059694767, 0.3250868022441864, 0.9986979365348816, 0.87890625, 0.0616319440305233, 0.999131977558136, 0.0733506977558136, 0.2131076455116272, 0.999131977558136, 0.1571180522441864, 0.1510416716337204, 0.0, 0.0234375, 0.1514756977558136, 0.6293402910232544, 0.999131977558136, 0.1475694477558136, 0.1883680522441864, 0.999131977558136, 0.0282118059694767, 0.9995659589767456, 0.999131977558136, 0.177517369389534, 0.999131977558136, 0.0807291641831398, 0.0525173619389534, 0.999131977558136, 0.1323784738779068, 0.0559895820915699, 0.999131977558136, 0.999131977558136, 0.0546875, 0.0768229141831398, 0.0464409738779068, 0.49609375, 0.999131977558136, 0.8480902910232544, 0.0329861119389534, 0.5208333134651184, 0.999131977558136, 0.0460069440305233, 0.1918402761220932, 0.9986979365348816, 0.0707465261220932, 0.0442708320915699, 0.2174479216337204, 0.02387152798473835, 0.0316840298473835, 0.0460069440305233, 0.0520833320915699, 0.0581597238779068, 0.0503472238779068, 0.9995659589767456, 0.0373263880610466, 0.0186631940305233, 0.8810763955116272, 0.0486111119389534, 0.8927951455116272, 0.1319444477558136, 0.0303819440305233, 0.0776909738779068, 0.999131977558136, 0.9986979365348816, 0.526475727558136, 0.10546875, 0.838975727558136, 0.8871527910232544, 0.8042534589767456, 0.999131977558136, 0.37890625, 0.0364583320915699, 0.0603298619389534, 0.0746527761220932, 0.0416666679084301, 0.0759548619389534, 0.0915798619389534, 0.1527777761220932, 0.0282118059694767, 0.2465277761220932, 0.0, 0.1714409738779068, 0.0733506977558136, 0.0, 0.1427951455116272, 0.1080729141831398, 0.1145833358168602, 0.1332465261220932, 0.0911458358168602, 0.999131977558136, 0.1809895783662796, 0.0325520820915699, 0.1453993022441864, 0.1098090261220932, 0.999131977558136, 0.0, 0.9201388955116272, 0.999131977558136, 0.9448784589767456, 0.999131977558136, 0.0364583320915699, 0.999131977558136, 0.0477430559694767, 0.9982638955116272, 0.0386284738779068, 0.0394965298473835, 0.999131977558136, 0.26953125, 0.0407986119389534, 0.02560763992369175, 0.999131977558136, 0.999131977558136, 0.08984375, 0.0, 0.0625, 0.4418402910232544, 0.1514756977558136, 0.9986979365348816, 0.0, 0.07421875, 0.4631076455116272, 0.0316840298473835, 0.999131977558136, 0.999131977558136, 0.177517369389534, 0.1319444477558136, 0.901475727558136, 0.0, 0.0659722238779068, 0.3489583432674408, 0.999131977558136, 0.0364583320915699, 0.0863715261220932, 0.009982638992369175, 0.1905381977558136, 0.067274309694767, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.382378488779068, 0.9978298544883728, 0.999131977558136, 0.1375868022441864, 0.0833333358168602, 0.9995659589767456, 0.999131977558136, 0.1553819477558136, 0.1519097238779068, 0.03125, 0.0616319440305233, 0.0494791679084301, 0.88671875, 0.2044270783662796, 0.0403645820915699, 0.9986979365348816, 0.02083333395421505, 0.075086809694767, 0.110243059694767, 0.2291666716337204, 0.0, 0.0325520820915699, 0.9691840410232544, 0.0290798619389534, 0.1002604141831398, 0.1241319477558136, 0.999131977558136, 0.0486111119389534, 0.0342881940305233, 0.999131977558136, 0.0651041641831398, 0.999131977558136, 0.9995659589767456, 0.1549479216337204, 0.02994791604578495, 0.0, 0.999131977558136, 0.999131977558136, 0.8572048544883728, 0.0460069440305233, 0.0373263880610466, 0.0998263880610466, 0.1371527761220932, 0.0490451380610466, 0.347222238779068, 0.854600727558136, 0.5021701455116272, 0.0611979179084301, 0.0512152798473835, 0.3129340410232544, 0.999131977558136, 0.0, 0.1302083283662796, 0.1397569477558136, 0.02994791604578495, 0.0, 0.999131977558136, 0.999131977558136, 0.0594618059694767, 0.866319477558136, 0.0, 0.9986979365348816, 0.0646701380610466, 0.999131977558136, 0.0425347238779068, 0.999131977558136, 0.0, 0.1254340261220932, 0.0373263880610466, 0.0373263880610466, 0.0, 0.1284722238779068, 0.9986979365348816, 0.1467013955116272, 0.999131977558136, 0.0473090298473835, 0.8953993320465088, 0.0963541641831398, 0.0707465261220932, 0.9440104365348816, 0.0859375, 0.9986979365348816, 0.134548619389534, 0.0902777761220932, 0.9353298544883728, 0.0737847238779068, 0.999131977558136, 0.0638020858168602, 0.0763888880610466, 0.0594618059694767, 0.9986979365348816, 0.0638020858168602, 0.2109375, 0.2447916716337204, 0.146267369389534, 0.1432291716337204, 0.02387152798473835, 0.7934027910232544, 0.0425347238779068, 0.0407986119389534, 0.1432291716337204, 0.0386284738779068, 0.0759548619389534, 0.999131977558136, 0.02170138992369175, 0.0442708320915699, 0.999131977558136, 0.0685763880610466, 0.02387152798473835, 0.1527777761220932, 0.0347222238779068, 0.0616319440305233, 0.0, 0.1519097238779068, 0.0724826380610466, 0.936631977558136, 0.11328125, 0.134548619389534, 0.999131977558136, 0.146267369389534, 0.05859375, 0.1401909738779068, 0.03515625, 0.8767361044883728, 0.0407986119389534, 0.999131977558136, 0.999131977558136, 0.0290798619389534, 0.999131977558136, 0.999131977558136, 0.885850727558136, 0.8901909589767456, 0.0334201380610466, 0.9986979365348816, 0.999131977558136, 0.0, 0.0451388880610466, 0.999131977558136, 0.0373263880610466, 0.1106770858168602, 0.0, 0.1371527761220932, 0.9986979365348816, 0.0, 0.9409722089767456, 0.0086805559694767, 0.1783854216337204, 0.9986979365348816, 0.0603298619389534, 0.0, 0.9986979365348816, 0.999131977558136, 0.0798611119389534, 0.03081597201526165, 0.9986979365348816, 0.0338541679084301, 0.0, 0.999131977558136, 0.1875, 0.2326388955116272, 0.0, 0.02734375, 0.999131977558136, 0.0399305559694767, 0.292534738779068, 0.0225694440305233, 0.999131977558136, 0.0290798619389534, 0.0386284738779068, 0.03081597201526165, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0390625, 0.0915798619389534, 0.0481770820915699, 0.999131977558136, 0.890625, 0.1536458283662796, 0.02213541604578495, 0.1935763955116272, 0.928819477558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.921875, 0.02387152798473835, 0.0590277798473835]

 sparsity of   [0.3177083432674408, 0.0377604179084301, 0.0716145858168602, 0.5212673544883728, 0.1536458283662796, 0.3285590410232544, 0.2217881977558136, 0.3604600727558136, 0.200954869389534, 0.0460069440305233, 0.0648871511220932, 0.2161458283662796, 0.6909722089767456, 0.7122395634651184, 0.0473090298473835, 0.063368059694767, 0.0418836809694767, 0.0824652761220932, 0.02951388992369175, 0.7172309160232544, 0.1440972238779068, 0.02365451492369175, 0.0737847238779068, 0.0562065988779068, 0.0366753488779068, 0.0529513880610466, 0.1197916641831398, 0.0748697891831398, 0.0, 0.0, 0.7591145634651184, 0.2178819477558136, 0.69140625, 0.46484375, 0.2013888955116272, 0.7020399570465088, 0.0666232630610466, 0.345703125, 0.0384114570915699, 0.5744357705116272, 0.6521267294883728, 0.2348090261220932, 0.5110676884651184, 0.115234375, 0.0625, 0.0223524309694767, 0.6588541865348816, 0.5123698115348816, 0.0223524309694767, 0.482421875, 0.2259114533662796, 0.45703125, 0.8255208134651184, 0.5991753339767456, 0.0601128488779068, 0.4364149272441864, 0.6575520634651184, 0.0290798619389534, 0.056640625, 0.1202256977558136, 0.0611979179084301, 0.0614149309694767, 0.0674913227558136, 0.0425347238779068, 0.046875, 0.0388454869389534, 0.05078125, 0.4828559160232544, 0.0, 0.0674913227558136, 0.7612847089767456, 0.0414496548473835, 0.3598090410232544, 0.7834201455116272, 0.1976996511220932, 0.821397602558136, 0.618272602558136, 0.0399305559694767, 0.0763888880610466, 0.0538194440305233, 0.25390625, 0.0616319440305233, 0.1888020783662796, 0.6490885615348816, 0.5991753339767456, 0.0, 0.0531684048473835, 0.0, 0.6271701455116272, 0.0620659738779068, 0.200954869389534, 0.052734375, 0.0700954869389534, 0.5099826455116272, 0.05859375, 0.02864583395421505, 0.6571180820465088, 0.5939670205116272, 0.2838541567325592, 0.0542534738779068, 0.6037326455116272, 0.1937934011220932, 0.916015625, 0.7274305820465088, 0.064453125, 0.6174045205116272, 0.0, 0.0434027798473835, 0.2721354067325592, 0.662109375, 0.0206163190305233, 0.1310763955116272, 0.6373698115348816, 0.02734375, 0.0334201380610466, 0.1189236119389534, 0.0802951380610466, 0.0536024309694767, 0.6673176884651184, 0.0, 0.2834201455116272, 0.0620659738779068, 0.0523003488779068, 0.1030815988779068, 0.2669270932674408, 0.02408854104578495, 0.1608072966337204, 0.0572916679084301, 0.7163628339767456, 0.0737847238779068, 0.1015625, 0.0846354141831398, 0.0514322929084301, 0.0, 0.3812934160232544, 0.0353732630610466, 0.3548177182674408, 0.0438368059694767, 0.0546875, 0.0542534738779068, 0.553819477558136, 0.653428852558136, 0.0340711809694767, 0.0529513880610466, 0.1362847238779068, 0.0531684048473835, 0.10546875, 0.6204426884651184, 0.0559895820915699, 0.1369357705116272, 0.6640625, 0.1963975727558136, 0.0477430559694767, 0.03515625, 0.6067708134651184, 0.067274309694767, 0.183376744389534, 0.3637152910232544, 0.7063801884651184, 0.029296875, 0.659288227558136, 0.0735677108168602, 0.2486979216337204, 0.7547743320465088, 0.2873263955116272, 0.5766059160232544, 0.0503472238779068, 0.0496961809694767, 0.0, 0.724609375, 0.0366753488779068, 0.5115017294883728, 0.0392795130610466, 0.23046875, 0.6532118320465088, 0.0447048619389534, 0.1779513955116272, 0.8198784589767456, 0.0709635391831398, 0.3461371660232544, 0.1877170205116272, 0.628038227558136, 0.0381944440305233, 0.0553385429084301, 0.7654079794883728, 0.5525173544883728, 0.0373263880610466, 0.653428852558136, 0.0486111119389534, 0.0607638880610466, 0.6753472089767456, 0.4329427182674408, 0.8671875, 0.0935329869389534, 0.7734375, 0.3819444477558136, 0.3721788227558136, 0.0462239570915699, 0.1471354216337204, 0.044921875, 0.02756076492369175, 0.0611979179084301, 0.214626744389534, 0.0, 0.25, 0.427300363779068, 0.0648871511220932, 0.0570746548473835, 0.116102434694767, 0.0759548619389534, 0.1467013955116272, 0.0362413190305233, 0.0590277798473835, 0.5933159589767456, 0.1547309011220932, 0.0518663190305233, 0.0753038227558136, 0.0670572891831398, 0.02669270895421505, 0.8344184160232544, 0.0481770820915699, 0.02604166604578495, 0.0, 0.0538194440305233, 0.0564236119389534, 0.0601128488779068, 0.2003038227558136, 0.1215277761220932, 0.0390625, 0.2039930522441864, 0.423394113779068, 0.818359375, 0.6282551884651184, 0.0525173619389534, 0.7840712070465088, 0.0397135429084301, 0.02777777798473835, 0.136501744389534, 0.0325520820915699, 0.6312934160232544, 0.6178385615348816, 0.3747829794883728, 0.1848958283662796, 0.7191840410232544, 0.8524305820465088, 0.704210102558136, 0.2005208283662796, 0.6536458134651184, 0.6812065839767456, 0.5813801884651184, 0.6575520634651184, 0.0271267369389534, 0.02278645895421505, 0.056640625, 0.064453125, 0.03515625, 0.0824652761220932, 0.0414496548473835, 0.6434462070465088, 0.8355034589767456, 0.916015625, 0.078993059694767, 0.0659722238779068, 0.04296875, 0.0418836809694767, 0.2003038227558136, 0.916015625, 0.0, 0.0607638880610466, 0.02495659701526165, 0.0516493059694767, 0.0342881940305233, 0.021484375, 0.0980902761220932, 0.6336805820465088, 0.6720920205116272, 0.916015625, 0.8515625, 0.0496961809694767, 0.0883246511220932, 0.1532118022441864, 0.13671875, 0.0473090298473835, 0.829210102558136, 0.5887587070465088, 0.0755208358168602, 0.0564236119389534, 0.0562065988779068, 0.1610243022441864, 0.1148003488779068, 0.03125, 0.0353732630610466, 0.185546875, 0.0989583358168602, 0.5490451455116272, 0.0590277798473835, 0.6673176884651184, 0.6783854365348816, 0.01519097201526165, 0.5837673544883728, 0.013671875, 0.0475260429084301, 0.0475260429084301, 0.02734375, 0.0193142369389534, 0.03059895895421505, 0.02473958395421505, 0.0475260429084301, 0.0360243059694767, 0.5564236044883728, 0.02669270895421505, 0.0455729179084301, 0.1104600727558136, 0.2024739533662796, 0.0505642369389534, 0.6354166865348816, 0.0455729179084301, 0.0562065988779068, 0.7795138955116272, 0.0657552108168602, 0.517578125, 0.0533854179084301, 0.0, 0.1779513955116272, 0.435112863779068, 0.3598090410232544, 0.2803819477558136, 0.916015625, 0.7947048544883728, 0.5442708134651184, 0.0, 0.2337239533662796, 0.02105034701526165, 0.014322916977107525, 0.02994791604578495, 0.1022135391831398, 0.0447048619389534, 0.0, 0.0748697891831398, 0.0405815988779068, 0.3962673544883728, 0.0384114570915699, 0.7749565839767456, 0.052734375, 0.8454861044883728, 0.0638020858168602, 0.0388454869389534, 0.177734375, 0.7076823115348816, 0.2061631977558136, 0.629991352558136, 0.5247395634651184, 0.6343315839767456, 0.0366753488779068, 0.177734375, 0.0349392369389534, 0.0538194440305233, 0.5149739384651184, 0.1245659738779068, 0.075086809694767, 0.0473090298473835, 0.0870225727558136, 0.7664930820465088, 0.1187065988779068, 0.6527777910232544, 0.6825087070465088, 0.0705295130610466, 0.0416666679084301, 0.0607638880610466, 0.5544704794883728, 0.2395833283662796, 0.2931857705116272, 0.7291666865348816, 0.1046006977558136, 0.01888020895421505, 0.609375, 0.7389323115348816, 0.1358506977558136, 0.6323784589767456, 0.0831163227558136, 0.0499131940305233, 0.7682291865348816, 0.520616352558136, 0.1319444477558136, 0.0442708320915699, 0.708116352558136, 0.0405815988779068, 0.152126744389534, 0.0670572891831398, 0.0421006940305233, 0.2970920205116272, 0.7039930820465088, 0.0366753488779068, 0.0232204869389534, 0.0453559048473835, 0.0564236119389534, 0.052734375, 0.0492621548473835, 0.0321180559694767, 0.7068142294883728, 0.0596788190305233, 0.0431857630610466, 0.0681423619389534, 0.0763888880610466, 0.02300347201526165, 0.0494791679084301, 0.0651041641831398, 0.0440538190305233, 0.0776909738779068, 0.7493489384651184, 0.116102434694767, 0.6666666865348816, 0.0499131940305233, 0.6872829794883728, 0.0651041641831398, 0.8528645634651184, 0.5609809160232544, 0.4782986044883728, 0.0460069440305233, 0.411675363779068, 0.805772602558136, 0.2291666716337204, 0.048828125, 0.7078993320465088, 0.0470920130610466, 0.04296875, 0.072265625, 0.0668402761220932, 0.7350260615348816, 0.0466579869389534, 0.9997829794883728, 0.0442708320915699, 0.0553385429084301, 0.210720494389534, 0.7358940839767456, 0.0661892369389534, 0.5182291865348816, 0.73046875, 0.02408854104578495, 0.086805559694767, 0.1098090261220932, 0.8257378339767456, 0.759765625, 0.7044270634651184, 0.555772602558136, 0.0271267369389534, 0.06640625, 0.0575086809694767, 0.0360243059694767, 0.0607638880610466, 0.6169704794883728, 0.3244357705116272, 0.0264756940305233, 0.0186631940305233, 0.0913628488779068, 0.0533854179084301, 0.6508246660232544, 0.0, 0.0301649309694767, 0.6599392294883728, 0.4915364682674408, 0.0314670130610466, 0.0559895820915699, 0.0455729179084301, 0.7612847089767456, 0.1610243022441864, 0.0514322929084301, 0.6755642294883728, 0.0421006940305233, 0.916015625, 0.0735677108168602, 0.7942708134651184, 0.2300347238779068, 0.779296875, 0.0509982630610466, 0.0944010391831398, 0.183376744389534, 0.0314670130610466, 0.0538194440305233, 0.0575086809694767, 0.0501302070915699, 0.2491319477558136, 0.0, 0.02018229104578495, 0.1321614533662796, 0.1124131977558136, 0.8532986044883728, 0.0544704869389534, 0.5983073115348816, 0.2196180522441864, 0.3585069477558136, 0.1098090261220932, 0.0551215298473835, 0.0, 0.0627170130610466, 0.0822482630610466, 0.0638020858168602, 0.0, 0.0833333358168602, 0.7282986044883728, 0.0, 0.0759548619389534, 0.0492621548473835, 0.0555555559694767, 0.3841145932674408, 0.66015625, 0.0444878488779068, 0.1866319477558136, 0.0559895820915699, 0.0492621548473835, 0.0555555559694767, 0.1985677033662796]

 sparsity of   [0.0546875, 0.0625, 0.0546875, 0.03515625, 0.046875, 0.0625, 0.05078125, 0.0546875, 0.0625, 0.02734375, 0.05078125, 0.015625, 0.0625, 0.0625, 0.0625, 0.05859375, 0.0078125, 0.0, 0.05859375, 0.0, 0.0, 0.0234375, 0.0390625, 0.0625, 0.00390625, 0.00390625, 0.015625, 0.0625, 0.0, 0.0, 0.0625, 0.0625, 0.0625, 0.0390625, 0.046875, 0.0625, 0.0625, 0.01171875, 0.01953125, 0.0546875, 0.015625, 0.01171875, 0.015625, 0.0078125, 0.0546875, 0.05078125, 0.0078125, 0.0078125, 0.1328125, 0.0625, 0.01953125, 0.0625, 0.0625, 0.03125, 0.015625, 0.0078125, 0.0625, 0.00390625, 0.01171875, 0.015625, 0.05859375, 0.01953125, 0.04296875, 0.0546875, 0.01171875, 0.0625, 0.046875, 0.01953125, 0.0, 0.0625, 0.015625, 0.04296875, 0.0078125, 0.0078125, 0.0546875, 0.0625, 0.015625, 0.01953125, 0.01953125, 0.9921875, 0.0234375, 0.01171875, 0.01171875, 0.01953125, 0.02734375, 0.0, 0.0625, 0.0, 0.0625, 0.03125, 0.04296875, 0.015625, 0.05078125, 0.00390625, 0.0234375, 0.02734375, 0.015625, 0.01171875, 0.0546875, 0.0390625, 0.0625, 0.01171875, 0.0625, 0.0625, 0.0234375, 0.0625, 0.0, 0.0625, 0.046875, 0.01171875, 0.01171875, 0.0234375, 0.03125, 0.03515625, 0.0, 0.046875, 0.0078125, 0.0625, 0.0546875, 0.0, 0.0625, 0.0625, 0.19921875, 0.0078125, 0.0078125, 0.04296875, 0.015625, 0.0625, 0.0, 0.015625, 0.0, 0.0625, 0.0625, 0.0, 0.0625, 0.0625, 0.0078125, 0.0625, 0.0625, 0.0078125, 0.05859375, 0.0625, 0.0625, 0.015625, 0.04296875, 0.0078125, 0.0546875, 0.0625, 0.00390625, 0.015625, 0.0625, 0.0625, 0.0078125, 0.01171875, 0.0546875, 0.04296875, 0.01171875, 0.0078125, 0.0625, 0.0078125, 0.00390625, 0.0625, 0.0625, 0.0, 0.05859375, 0.00390625, 0.01953125, 0.015625, 0.0, 0.0234375, 0.0546875, 0.0078125, 0.0078125, 0.00390625, 0.0390625, 0.0625, 0.0390625, 0.0, 0.16015625, 0.0625, 0.05078125, 0.0078125, 0.05078125, 0.12109375, 0.0, 0.02734375, 0.01953125, 0.046875, 0.01953125, 0.0625, 0.03125, 0.01171875, 0.0546875, 0.0234375, 0.01953125, 0.0078125, 0.0546875, 0.03125, 0.046875, 0.0625, 0.05859375, 0.05078125, 0.0078125, 0.0, 0.04296875, 0.0625, 0.01953125, 0.0625, 0.03515625, 0.02734375, 0.0546875, 0.0625, 0.01171875, 0.0546875, 0.015625, 0.0078125, 0.01953125, 0.015625, 0.0625, 0.0078125, 0.015625, 0.04296875, 0.0, 0.01953125, 0.0625, 0.0234375, 0.0546875, 0.01953125, 0.015625, 0.05078125, 0.01953125, 0.015625, 0.01953125, 0.03515625, 0.015625, 0.01953125, 0.05078125, 0.015625, 0.01171875, 0.0625, 0.046875, 0.0625, 0.0625, 0.0078125, 0.0625, 0.05078125, 0.01171875, 0.0625, 0.05859375, 0.0, 0.0, 0.0390625, 0.0625, 0.01171875, 0.01171875, 0.0625, 0.05078125, 0.046875, 0.0625, 0.9921875, 0.0546875, 0.01953125, 0.046875, 0.0625, 0.015625, 0.04296875, 0.0234375, 0.0, 0.0234375, 0.05078125, 0.01171875, 0.04296875, 0.0, 0.00390625, 0.03515625, 0.01171875, 0.03125, 0.0, 0.05078125, 0.0, 0.0078125, 0.0625, 0.01171875, 0.01171875, 0.0078125, 0.015625, 0.0625, 0.0625, 0.0078125, 0.01171875, 0.015625, 0.0546875, 0.0625, 0.01171875, 0.01953125, 0.0625, 0.03515625, 0.0, 0.0625, 0.0, 0.05078125, 0.015625, 0.03125, 0.0625, 0.0, 0.0078125, 0.05078125, 0.05078125, 0.015625, 0.0625, 0.0078125, 0.0390625, 0.00390625, 0.05078125, 0.0625, 0.00390625, 0.0625, 0.015625, 0.01171875, 0.01953125, 0.015625, 0.01171875, 0.0, 0.015625, 0.046875, 0.0546875, 0.015625, 0.01171875, 0.00390625, 0.0625, 0.0, 0.01171875, 0.0625, 0.0, 0.015625, 0.015625, 0.0546875, 0.0, 0.0078125, 0.0625, 0.0625, 0.01171875, 0.0078125, 0.0625, 0.0625, 0.03515625, 0.015625, 0.03515625, 0.0625, 0.015625, 0.05078125, 0.01171875, 0.0078125, 0.04296875, 0.01171875, 0.0, 0.04296875, 0.015625, 0.015625, 0.0078125, 0.04296875, 0.0625, 0.01171875, 0.05078125, 0.04296875, 0.0625, 0.0625, 0.0625, 0.02734375, 0.0625, 0.0625, 0.0546875, 0.01953125, 0.046875, 0.0234375, 0.0546875, 0.0625, 0.01171875, 0.01953125, 0.0234375, 0.01171875, 0.0625, 0.0078125, 0.046875, 0.015625, 0.0, 0.015625, 0.0546875, 0.0, 0.02734375, 0.02734375, 0.05078125, 0.01953125, 0.0, 0.01171875, 0.01953125, 0.05859375, 0.01171875, 0.01953125, 0.0625, 0.0078125, 0.0625, 0.01171875, 0.0625, 0.015625, 0.01171875, 0.03125, 0.0625, 0.0234375, 0.01953125, 0.05859375, 0.046875, 0.01171875, 0.04296875, 0.05078125, 0.0546875, 0.0078125, 0.05859375, 0.01171875, 0.015625, 0.01953125, 0.01171875, 0.0078125, 0.01171875, 0.01953125, 0.01953125, 0.0, 0.015625, 0.01171875, 0.0, 0.30078125, 0.01953125, 0.0625, 0.0078125, 0.0625, 0.0625, 0.0, 0.0, 0.0546875, 0.0234375, 0.01171875, 0.0625, 0.05859375, 0.0078125, 0.015625, 0.01171875, 0.0625, 0.01171875, 0.0078125, 0.328125, 0.0625, 0.0234375, 0.03515625, 0.01953125, 0.05078125, 0.0234375, 0.0078125, 0.0, 0.0625, 0.01171875, 0.015625, 0.0625, 0.015625, 0.0625, 0.0625, 0.01953125, 0.0625, 0.0, 0.0625, 0.04296875, 0.0078125, 0.05078125, 0.0234375, 0.01171875, 0.02734375, 0.0625, 0.015625, 0.0625, 0.0, 0.02734375, 0.05078125, 0.0546875, 0.0, 0.01171875, 0.015625, 0.0, 0.0625, 0.0625, 0.015625, 0.01171875, 0.0078125, 0.01171875, 0.01171875, 0.0, 0.05078125, 0.0234375, 0.11328125, 0.0, 0.0390625, 0.0, 0.0, 0.04296875, 0.0546875, 0.015625, 0.01171875, 0.046875, 0.0625, 0.015625, 0.046875, 0.05078125, 0.0078125, 0.01171875]

 sparsity of   [0.1319444477558136, 0.005642361007630825, 0.3524305522441864, 0.2936197817325592, 0.3834635317325592, 0.1150173619389534, 0.1673177033662796, 0.0915798619389534, 0.2610677182674408, 0.3318142294883728, 0.0904947891831398, 0.2745225727558136, 0.0948350727558136, 0.120008684694767, 0.183376744389534, 0.005425347480922937, 0.4166666567325592, 0.1994357705116272, 0.095703125, 0.1603732705116272, 0.1263020783662796, 0.4375, 0.3435329794883728, 0.099609375, 0.1790364533662796, 0.0902777761220932, 0.0462239570915699, 0.0928819477558136, 0.084852434694767, 0.1349826455116272, 0.0922309011220932, 0.0989583358168602, 0.3708767294883728, 0.0978732630610466, 0.4236111044883728, 0.1017795130610466, 0.4066840410232544, 0.2100694477558136, 0.161892369389534, 0.9997829794883728, 0.1304253488779068, 0.2191840261220932, 0.1341145783662796, 0.2085503488779068, 0.9997829794883728, 0.3912760317325592, 0.0555555559694767, 0.0779079869389534, 0.1330295205116272, 0.0885416641831398, 0.0967881977558136, 0.0748697891831398, 0.1555989533662796, 0.067274309694767, 0.2309027761220932, 0.0776909738779068, 0.2248263955116272, 0.0581597238779068, 0.1015625, 0.0785590261220932, 0.1890190988779068, 0.0640190988779068, 0.0518663190305233, 0.1013454869389534, 0.0568576380610466, 0.1675347238779068, 0.6187065839767456, 0.2743055522441864, 0.1983506977558136, 0.2643229067325592, 0.2688802182674408, 0.4084201455116272, 0.0518663190305233, 0.2298177033662796, 0.8138020634651184, 0.173611119389534, 0.1714409738779068, 0.9997829794883728, 0.1818576455116272, 0.0870225727558136, 0.3005642294883728, 0.1317274272441864, 0.2105034738779068, 0.2298177033662796, 0.1217447891831398, 0.0625, 0.092664934694767, 0.1295572966337204, 0.0967881977558136, 0.00390625, 0.590928852558136, 0.1330295205116272, 0.0596788190305233, 0.1555989533662796, 0.0865885391831398, 0.0802951380610466, 0.1944444477558136, 0.1087239608168602, 0.1195746511220932, 0.9995659589767456, 0.14453125, 0.1644965261220932, 0.2228732705116272, 0.0575086809694767, 0.7877604365348816, 0.2734375, 0.0783420130610466, 0.0416666679084301, 0.0980902761220932, 0.0657552108168602, 0.1477864533662796, 0.0822482630610466, 0.0915798619389534, 0.0694444477558136, 0.0681423619389534, 0.0709635391831398, 0.0661892369389534, 0.1174045130610466, 0.1959635466337204, 0.099609375, 0.1673177033662796, 0.0733506977558136, 0.0598958320915699, 0.1106770858168602, 0.173611119389534, 0.2855902910232544, 0.6278212070465088, 0.1519097238779068, 0.1640625, 0.0978732630610466, 0.1078559011220932, 0.0772569477558136, 0.060546875, 0.1516927033662796, 0.0865885391831398, 0.1888020783662796, 0.1595052033662796, 0.0627170130610466, 0.088758684694767, 0.090711809694767, 0.148220494389534, 0.1362847238779068, 0.1126302108168602, 0.3799913227558136, 0.1022135391831398, 0.02951388992369175, 0.0852864608168602, 0.073133684694767, 0.2131076455116272, 0.1848958283662796, 0.0787760391831398, 0.1373697966337204, 0.1623263955116272, 0.3177083432674408, 0.114149309694767, 0.189453125, 0.5338541865348816, 0.753038227558136, 0.0503472238779068, 0.2378472238779068, 0.1174045130610466, 0.5060763955116272, 0.078993059694767, 0.1085069477558136, 0.0703125, 0.4249131977558136, 0.3734809160232544, 0.2024739533662796, 0.1208767369389534, 0.2736545205116272, 0.1946614533662796, 0.0978732630610466, 0.1187065988779068, 0.3684895932674408, 0.1499565988779068, 0.0651041641831398, 0.1477864533662796, 0.1078559011220932, 0.1514756977558136, 0.1456163227558136, 0.0998263880610466, 0.162109375, 0.01888020895421505, 0.173611119389534, 0.273003488779068, 0.2137586772441864, 0.4678819477558136, 0.1241319477558136, 0.3891059160232544, 0.1341145783662796, 0.0991753488779068, 0.111328125, 0.100477434694767, 0.0944010391831398, 0.205078125, 0.2784288227558136, 0.327690988779068, 0.1243489608168602, 0.132595494389534, 0.5687934160232544, 0.1328125, 0.0792100727558136, 0.1786024272441864, 0.291015625, 0.0835503488779068, 0.142361119389534, 0.086805559694767, 0.0924479141831398, 0.087890625, 0.0833333358168602, 0.0034722222480922937, 0.3122829794883728, 0.1927083283662796, 0.1022135391831398, 0.1812065988779068, 0.1182725727558136, 0.1527777761220932, 0.1410590261220932, 0.3018663227558136, 0.0707465261220932, 0.1330295205116272, 0.1100260391831398, 0.0974392369389534, 0.1627604216337204, 0.2936197817325592, 0.2137586772441864, 0.1115451380610466, 0.1009114608168602, 0.1108940988779068, 0.1260850727558136, 0.1362847238779068, 0.0726996511220932, 0.2319878488779068, 0.212890625, 0.1052517369389534, 0.1588541716337204, 0.1449652761220932, 0.071180559694767, 0.108289934694767, 0.1820746511220932, 0.0492621548473835, 0.5416666865348816, 0.1929253488779068, 0.121961809694767, 0.220486119389534, 0.09765625, 0.1768663227558136, 0.1725260466337204, 0.0970052108168602, 0.28125, 0.090711809694767, 0.0998263880610466, 0.9997829794883728, 0.056640625, 0.0470920130610466, 0.1276041716337204, 0.083984375, 0.1414930522441864, 0.2990451455116272, 0.056640625, 0.0023871527519077063, 0.0013020833721384406, 0.0954861119389534, 0.4184027910232544, 0.321831613779068, 0.3498263955116272, 0.0911458358168602, 0.0614149309694767, 0.1002604141831398, 0.4854600727558136, 0.0707465261220932, 0.146484375, 0.2139756977558136, 0.0954861119389534, 0.3760850727558136, 0.1506076455116272, 0.3259548544883728, 0.1597222238779068, 0.2289496511220932, 0.064453125, 0.0974392369389534, 0.1315104216337204, 0.1940104216337204, 0.0959201380610466, 0.1215277761220932, 0.1163194477558136, 0.041015625, 0.1360677033662796, 0.1282552033662796, 0.2708333432674408, 0.1605902761220932, 0.0577256940305233, 0.0627170130610466, 0.2764756977558136, 0.103515625, 0.2973090410232544, 0.6477864384651184, 0.1901041716337204, 0.2825520932674408, 0.1126302108168602, 0.302300363779068, 0.4168836772441864, 0.3407118022441864, 0.1929253488779068, 0.0970052108168602, 0.1649305522441864, 0.1059027761220932, 0.1243489608168602, 0.1605902761220932, 0.0625, 0.308159738779068, 0.2296006977558136, 0.0657552108168602, 0.4322916567325592, 0.2133246511220932, 0.2109375, 0.1195746511220932, 0.1358506977558136, 0.1410590261220932, 0.9997829794883728, 0.0952690988779068, 0.0264756940305233, 0.9997829794883728, 0.1234809011220932, 0.0913628488779068, 0.0824652761220932, 0.0531684048473835, 0.16796875, 0.1302083283662796, 0.1590711772441864, 0.331597238779068, 0.0759548619389534, 0.0479600690305233, 0.220703125, 0.1065538227558136, 0.3274739682674408, 0.3279079794883728, 0.1586371511220932, 0.0010850694961845875, 0.1115451380610466, 0.1356336772441864, 0.0865885391831398, 0.092664934694767, 0.1625434011220932, 0.2200520783662796, 0.1223958358168602, 0.1078559011220932, 0.0010850694961845875, 0.0855034738779068, 0.1525607705116272, 0.1276041716337204, 0.005859375, 0.1050347238779068, 0.1636284738779068, 0.2940538227558136, 0.0991753488779068, 0.1150173619389534, 0.0967881977558136, 0.0679253488779068, 0.09765625, 0.2163628488779068, 0.0835503488779068, 0.2899305522441864, 0.2141927033662796, 0.0570746548473835, 0.6486545205116272, 0.218532994389534, 0.355034738779068, 0.1263020783662796, 0.1566840261220932, 0.437065988779068, 0.3376736044883728, 0.0381944440305233, 0.1373697966337204, 0.3012152910232544, 0.09765625, 0.0833333358168602, 0.1087239608168602, 0.02408854104578495, 0.1770833283662796, 0.3220486044883728, 0.1236979141831398, 0.02690972201526165, 0.140625, 0.0564236119389534, 0.0457899309694767, 0.398003488779068, 0.146484375, 0.065321184694767, 0.16015625, 0.9995659589767456, 0.0032552082557231188, 0.1901041716337204, 0.0481770820915699, 0.4793836772441864, 0.083984375, 0.3248697817325592, 0.3361545205116272, 0.1840277761220932, 0.16796875, 0.0961371511220932, 0.1684027761220932, 0.2018229216337204, 0.1976996511220932, 0.0243055559694767, 0.2020399272441864, 0.114149309694767, 0.4793836772441864, 0.107421875, 0.1087239608168602, 0.130859375, 0.11328125, 0.0620659738779068, 0.0915798619389534, 0.1664496511220932, 0.0651041641831398, 0.0933159738779068, 0.1059027761220932, 0.1590711772441864, 0.18359375, 0.0831163227558136, 0.0657552108168602, 0.1471354216337204, 0.095703125, 0.104383684694767, 0.1987847238779068, 0.0792100727558136, 0.0629340261220932, 0.1276041716337204, 0.4420572817325592, 0.0618489570915699, 0.1228298619389534, 0.563585102558136, 0.8101128339767456, 0.03081597201526165, 0.07421875, 0.1032986119389534, 0.0668402761220932, 0.0904947891831398, 0.088758684694767, 0.0861545130610466, 0.123914934694767, 0.1011284738779068, 0.095703125, 0.3038194477558136, 0.114149309694767, 0.2280815988779068, 0.2732204794883728, 0.2426215261220932, 0.201171875, 0.0831163227558136, 0.1150173619389534, 0.0766059011220932, 0.3111979067325592, 0.115234375, 0.0948350727558136, 0.1360677033662796, 0.1260850727558136, 0.075086809694767, 0.1312934011220932, 0.0694444477558136, 0.2163628488779068, 0.1178385391831398, 0.1069878488779068, 0.069227434694767, 0.1555989533662796, 0.4151475727558136, 0.090711809694767, 0.2996961772441864, 0.0842013880610466, 0.9997829794883728, 0.282769113779068, 0.0726996511220932, 0.1818576455116272, 0.1069878488779068, 0.096571184694767, 0.3135850727558136, 0.2921006977558136, 0.3721788227558136, 0.0627170130610466, 0.1412760466337204, 0.2007378488779068, 0.1271701455116272, 0.2630208432674408, 0.1456163227558136, 0.1000434011220932, 0.2411024272441864, 0.0998263880610466, 0.1395399272441864, 0.0796440988779068, 0.0783420130610466, 0.9997829794883728, 0.1996527761220932, 0.106336809694767, 0.4240451455116272, 0.0581597238779068, 0.0028211805038154125, 0.0974392369389534, 0.2669270932674408, 0.1187065988779068, 0.1282552033662796, 0.0462239570915699, 0.0013020833721384406, 0.5275607705116272, 0.0930989608168602, 0.1974826455116272, 0.0924479141831398, 0.22265625, 0.1299913227558136, 0.0870225727558136, 0.0232204869389534, 0.1907552033662796]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0052083334885537624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0047743055038154125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9761284589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8841145634651184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8904079794883728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6829426884651184, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011501736007630825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 2299728.0278581306 (unstructured) 0 (structured)

max weight is  tensor([2.7640e-09, 1.5121e-08, 4.5537e-09, 4.5074e-09, 1.1628e-09, 6.3740e-09,
        1.3325e-01, 3.6999e-01, 2.3280e-01, 2.7640e-09, 4.7434e-09, 2.7899e-09,
        7.2682e-01, 6.2339e-09, 1.7968e-09, 1.1628e-09, 2.7640e-09, 2.7832e-09,
        3.8574e-01, 1.5103e-08, 1.1628e-09, 4.6519e-09, 4.5635e-09, 2.7640e-09,
        2.7640e-09, 2.7640e-09, 4.7660e-09, 5.5215e-01, 1.0462e-01, 3.1644e-01,
        4.6639e-09, 4.3841e-09, 9.9471e-09, 2.7640e-09, 1.1628e-09, 1.1628e-09,
        1.1628e-09, 3.6904e-01, 1.1628e-09, 5.1866e-02, 5.2125e-01, 1.9786e-01,
        3.1166e-01, 4.0265e-09, 5.2069e-01, 2.6476e-09, 1.4315e-01, 4.6469e-09,
        1.5888e-01, 1.5298e-08, 3.4164e-09, 1.2183e-08, 6.5260e-01, 4.5537e-09,
        2.9677e-01, 1.1628e-09, 3.7789e-01, 2.2469e-01, 4.4476e-09, 2.5043e-09,
        2.7640e-09, 1.1628e-09, 2.7640e-09, 2.2910e-09], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.5071e-08, 9.7098e-02, 1.0106e-01, 2.0254e-07, 1.7420e-01, 1.6061e-07,
        1.4542e-07, 3.7263e-02, 1.8110e-07, 2.0029e-01, 8.9066e-02, 1.6061e-07,
        2.4011e-01, 1.0833e-01, 2.3420e-01, 9.8571e-02, 1.0936e-01, 5.4128e-02,
        2.3333e-01, 3.0485e-07, 7.9195e-02, 6.2628e-08, 9.2222e-02, 6.4968e-08,
        1.7564e-01, 2.5143e-01, 8.5071e-08, 1.4725e-01, 2.4312e-01, 9.4605e-02,
        1.1187e-01, 1.4397e-07, 1.1699e-01, 2.9594e-08, 3.4637e-02, 1.7435e-01,
        3.5284e-02, 2.2344e-01, 1.6407e-01, 2.4893e-02, 7.4185e-02, 6.4369e-02,
        6.4968e-08, 1.9815e-01, 3.0485e-07, 2.9594e-08, 9.6810e-02, 8.5071e-08,
        3.0485e-07, 1.8763e-01, 3.6624e-01, 2.2805e-01, 3.0485e-07, 4.7120e-02,
        1.0076e-01, 4.8235e-02, 5.4528e-02, 4.3456e-02, 3.0485e-07, 1.1680e-01,
        1.4397e-07, 9.0797e-02, 3.0485e-07, 1.0593e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.0519e-07, 9.6282e-08, 1.2832e-01, 1.4042e-01, 1.2705e-01, 6.1490e-02,
        3.1437e-02, 5.4947e-03, 7.4858e-02, 5.0100e-02, 1.0240e-01, 9.7964e-02,
        1.2789e-01, 1.1029e-01, 1.1494e-01, 7.5473e-02, 7.8054e-02, 5.6647e-02,
        1.2527e-01, 1.0724e-01, 1.0453e-01, 4.0083e-07, 4.0083e-07, 1.1431e-01,
        1.1092e-07, 1.3625e-01, 5.6860e-02, 1.6532e-01, 3.1878e-02, 5.4807e-02,
        4.9428e-02, 2.7311e-07, 7.0994e-02, 1.1953e-01, 9.9702e-02, 6.6707e-02,
        9.7007e-02, 3.1010e-02, 1.1974e-01, 3.4835e-02, 1.6242e-01, 2.6734e-02,
        1.0007e-01, 1.1110e-01, 2.4338e-01, 1.1022e-01, 6.2037e-02, 4.0083e-07,
        3.5848e-02, 1.3820e-01, 1.5152e-07, 1.1853e-07, 9.1246e-02, 6.3181e-02,
        2.3979e-02, 1.1724e-01, 4.4884e-02, 7.7955e-02, 9.8995e-02, 8.6666e-02,
        1.1428e-01, 1.5366e-01, 2.4546e-01, 1.1734e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.8043e-02, 1.6468e-07, 1.3548e-07, 9.6639e-02, 3.1736e-07, 2.1404e-01,
        7.3222e-07, 2.0331e-01, 4.6537e-07, 7.7377e-02, 8.7812e-02, 9.4081e-02,
        1.2212e-01, 4.7018e-02, 8.6427e-02, 1.1760e-01, 1.0753e-07, 1.3629e-02,
        1.3548e-07, 1.1180e-01, 1.6468e-07, 1.8058e-07, 4.6537e-07, 7.0416e-02,
        9.4259e-02, 1.1781e-01, 8.9177e-02, 2.5353e-07, 6.6100e-02, 1.8058e-07,
        7.8829e-02, 1.1318e-01, 1.3548e-07, 1.1410e-01, 9.4546e-02, 1.3913e-01,
        1.0406e-01, 2.9227e-07, 1.5491e-07, 5.4628e-07, 1.8051e-02, 3.1736e-07,
        1.5490e-07, 4.3860e-02, 4.6537e-07, 1.2434e-01, 1.2552e-01, 7.5510e-02,
        4.5023e-02, 5.4628e-07, 2.9214e-07, 8.5169e-02, 6.3900e-02, 9.7334e-02,
        9.3809e-02, 1.3460e-01, 1.4629e-07, 1.3548e-07, 1.2977e-01, 9.3551e-02,
        8.6139e-02, 7.3320e-07, 5.4541e-02, 8.7400e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0669e-01, 9.9968e-02, 6.1705e-02, 4.7545e-02, 9.6447e-03, 1.0576e-01,
        1.5632e-02, 4.1433e-03, 1.0498e-02, 1.0009e-01, 9.5051e-03, 7.7187e-02,
        1.4693e-02, 8.3404e-03, 9.4329e-03, 6.6672e-02, 8.0854e-02, 7.9455e-02,
        5.7567e-02, 4.0710e-02, 9.6456e-03, 7.3966e-02, 1.3997e-01, 8.9195e-03,
        9.1540e-02, 8.9051e-03, 5.3774e-02, 3.7976e-02, 3.4952e-03, 7.5136e-02,
        7.0935e-02, 7.7691e-02, 7.1495e-03, 4.8537e-02, 7.0076e-02, 8.9462e-02,
        9.1914e-02, 5.6732e-02, 1.5176e-01, 4.4505e-07, 3.3757e-02, 1.7832e-02,
        1.2448e-02, 1.4976e-02, 5.8998e-02, 8.1944e-02, 9.7656e-03, 1.3454e-01,
        9.8356e-03, 1.2094e-02, 8.3012e-02, 1.6742e-01, 1.6049e-02, 8.4627e-02,
        8.0837e-02, 9.5861e-03, 5.5853e-02, 5.2541e-02, 7.7083e-02, 2.5505e-02,
        8.9389e-02, 1.2094e-02, 4.4831e-02, 6.5756e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([7.0737e-02, 4.5375e-02, 7.3874e-02, 7.0251e-02, 5.1508e-02, 7.0814e-02,
        4.6805e-02, 4.9268e-02, 7.0824e-02, 6.2964e-02, 5.2350e-02, 1.0748e-06,
        7.3627e-02, 6.6679e-02, 6.1994e-02, 6.0821e-02, 6.7630e-02, 8.9814e-02,
        5.4290e-02, 4.1169e-02, 8.1043e-02, 6.0914e-02, 6.0356e-02, 1.3912e-01,
        7.3735e-02, 7.5178e-02, 6.4329e-02, 6.3250e-02, 5.8038e-02, 5.6615e-02,
        6.7446e-02, 5.0198e-02, 5.6938e-02, 6.6773e-02, 5.2979e-02, 8.3478e-02,
        4.9873e-02, 4.7257e-02, 7.0609e-02, 5.0822e-02, 6.5432e-02, 5.3205e-02,
        6.8347e-02, 7.1211e-02, 7.5834e-02, 2.6666e-02, 5.2058e-02, 6.2794e-02,
        5.0503e-02, 5.3886e-02, 6.4369e-02, 6.0694e-02, 4.6202e-02, 5.2642e-02,
        7.6624e-02, 5.7170e-02, 5.8343e-02, 6.0191e-02, 6.3409e-02, 7.4962e-02,
        4.2742e-02, 5.8144e-02, 5.2581e-02, 8.0229e-02, 5.9333e-02, 4.7554e-02,
        8.6280e-02, 6.4258e-02, 5.1287e-02, 4.0195e-02, 8.8304e-02, 5.9507e-02,
        8.7118e-02, 6.2606e-02, 4.7962e-02, 6.0244e-02, 6.4605e-02, 6.8982e-07,
        5.6297e-02, 4.6469e-02, 6.0685e-02, 6.5196e-02, 6.4213e-02, 5.9113e-02,
        5.4744e-02, 6.2191e-02, 3.0199e-07, 6.7448e-02, 6.9465e-02, 6.4414e-02,
        4.5170e-02, 9.9999e-08, 9.9999e-08, 6.9603e-02, 5.0243e-02, 5.4299e-02,
        4.4946e-02, 3.3029e-02, 3.8558e-02, 7.6665e-02, 5.2497e-02, 6.3357e-02,
        7.9142e-02, 7.7344e-02, 3.5892e-02, 5.2404e-02, 8.0517e-02, 6.2307e-02,
        6.4870e-02, 6.8736e-02, 6.9869e-02, 8.9390e-02, 4.6144e-02, 5.3297e-02,
        6.6596e-02, 4.1067e-02, 5.2539e-02, 8.5796e-02, 6.0372e-08, 6.7678e-02,
        5.5715e-02, 5.7070e-02, 5.2730e-02, 6.0174e-02, 6.5984e-02, 6.6424e-02,
        6.3518e-02, 8.4755e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.9282e-02, 2.9464e-02, 1.8208e-04, 6.1204e-02, 6.2641e-02, 5.6063e-02,
        6.8884e-02, 5.2854e-02, 7.3712e-02, 1.5593e-03, 7.1180e-02, 8.0318e-02,
        7.1001e-02, 9.7947e-02, 2.9283e-02, 1.9480e-03, 3.1931e-03, 6.9368e-02,
        2.1469e-02, 6.8434e-02, 5.6592e-02, 5.2259e-02, 6.1202e-02, 3.1199e-02,
        7.9237e-02, 1.5169e-01, 4.4802e-02, 4.5384e-07, 5.1008e-02, 6.8102e-02,
        5.5847e-02, 5.8548e-02, 1.3613e-01, 2.4413e-06, 6.7529e-02, 7.8470e-02,
        7.3051e-02, 7.6403e-02, 5.1339e-02, 5.0035e-02, 5.5533e-03, 1.4354e-03,
        6.4829e-02, 6.8964e-02, 5.1575e-02, 6.5123e-02, 7.0046e-02, 6.6611e-02,
        6.6929e-02, 6.6101e-02, 6.7429e-04, 6.9693e-02, 8.5784e-07, 1.1517e-01,
        7.5464e-02, 6.9870e-02, 7.4428e-02, 4.2858e-02, 7.8552e-02, 7.8871e-02,
        7.0973e-02, 7.2030e-02, 6.3856e-02, 1.2318e-03, 4.7523e-02, 6.2833e-02,
        1.3754e-05, 5.6442e-02, 6.4182e-02, 7.3360e-02, 5.6126e-02, 6.0237e-02,
        6.9021e-02, 7.7591e-02, 5.5701e-02, 7.1938e-02, 1.3115e-01, 7.2636e-02,
        6.6421e-02, 7.2733e-02, 9.2334e-03, 5.7498e-02, 5.4983e-02, 6.7455e-02,
        6.5410e-02, 1.9033e-02, 6.3131e-02, 7.3871e-02, 7.0197e-02, 4.1180e-03,
        6.4509e-02, 8.8802e-02, 2.0028e-03, 5.0195e-02, 6.8572e-02, 6.3903e-02,
        5.0533e-02, 6.4044e-02, 5.9287e-02, 6.2417e-02, 5.7941e-02, 6.9226e-02,
        7.8423e-02, 6.3317e-02, 8.2448e-02, 5.4699e-02, 7.1578e-02, 4.2879e-02,
        6.6346e-02, 1.9168e-02, 7.5162e-02, 7.1424e-02, 5.1802e-02, 9.5952e-02,
        3.5215e-02, 3.0545e-02, 5.4469e-02, 6.0588e-02, 8.2570e-02, 5.0353e-02,
        5.3108e-02, 5.5709e-02, 3.6970e-02, 7.1767e-02, 6.7887e-02, 5.5839e-02,
        7.5697e-02, 6.4744e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.1161e-02, 7.5508e-02, 1.3868e-01, 9.6410e-02, 5.5470e-02, 6.6910e-02,
        1.0615e-01, 1.3868e-01, 5.2924e-02, 3.2260e-01, 6.7347e-02, 5.6293e-02,
        9.2586e-02, 3.1706e-02, 6.5022e-02, 1.5404e-01, 1.4830e-01, 5.4581e-02,
        2.9332e-01, 5.0461e-02, 4.9990e-02, 8.7361e-02, 6.3691e-02, 1.2801e-01,
        6.4499e-02, 5.1817e-02, 1.9650e-01, 6.7009e-09, 6.8545e-02, 9.0059e-02,
        4.3148e-02, 7.5867e-02, 3.7836e-02, 1.8186e-08, 7.4651e-02, 4.4483e-02,
        6.6872e-02, 1.2379e-01, 5.0337e-02, 5.0697e-02, 1.7113e-01, 1.6203e-01,
        1.0924e-01, 6.8745e-02, 7.2503e-02, 8.4326e-02, 1.0512e-01, 7.2869e-02,
        7.7332e-02, 4.5028e-02, 2.3449e-01, 4.6910e-02, 5.3586e-08, 2.9411e-02,
        8.0321e-02, 7.7098e-02, 1.0142e-01, 2.8803e-01, 1.2511e-01, 4.2082e-02,
        1.0885e-01, 1.2219e-01, 6.1782e-02, 2.2658e-01, 7.5901e-02, 6.2207e-02,
        1.6062e-01, 8.3490e-02, 9.7213e-02, 5.9791e-02, 1.0880e-01, 6.3398e-02,
        6.3468e-02, 7.5089e-02, 7.5879e-02, 6.6861e-02, 3.6830e-02, 4.7956e-02,
        5.8209e-02, 1.4280e-01, 1.5485e-01, 6.0177e-02, 7.1810e-02, 7.0845e-02,
        8.9800e-02, 1.7204e-01, 1.2010e-01, 9.8174e-02, 1.0020e-01, 2.0103e-01,
        7.6390e-02, 4.4555e-02, 1.1940e-01, 9.3617e-02, 6.4060e-02, 7.7263e-02,
        7.0939e-02, 4.7490e-02, 7.0658e-02, 8.7987e-02, 1.2859e-01, 6.6236e-02,
        5.6065e-02, 6.3640e-02, 4.1342e-02, 1.0229e-01, 1.2376e-01, 1.8119e-01,
        4.4602e-02, 1.6377e-01, 4.6875e-02, 1.0806e-01, 8.6334e-02, 5.7976e-02,
        6.0552e-02, 5.2008e-02, 6.8238e-02, 7.6996e-02, 3.7250e-02, 1.2606e-01,
        8.6310e-02, 5.5738e-02, 4.8192e-02, 1.4462e-01, 1.5141e-01, 1.8800e-01,
        3.3961e-02, 1.2655e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7999e-06, 4.9005e-02, 4.9063e-02, 7.5509e-07, 4.6538e-02, 5.1226e-02,
        5.5737e-02, 1.0300e-06, 6.6176e-07, 4.6611e-07, 4.9254e-02, 5.8450e-02,
        1.3555e-01, 4.8199e-02, 6.4702e-07, 5.6447e-02, 5.9905e-02, 7.3286e-07,
        6.0380e-02, 4.7403e-02, 3.4964e-06, 5.3242e-02, 8.0818e-07, 6.9357e-07,
        5.9492e-02, 1.3433e-06, 5.6078e-02, 5.7080e-02, 4.4877e-02, 5.9621e-02,
        4.3013e-07, 6.8516e-02, 1.0678e-01, 5.3705e-02, 3.4861e-07, 3.4861e-07,
        1.4696e-06, 5.1475e-02, 5.7671e-02, 6.6250e-02, 5.5072e-02, 5.6627e-02,
        5.5856e-02, 5.6765e-02, 6.0654e-02, 1.0722e-01, 4.3502e-02, 1.2862e-01,
        9.9204e-07, 5.5969e-02, 5.2093e-02, 4.8224e-02, 5.7860e-02, 1.8902e-06,
        1.2902e-01, 5.9179e-02, 5.7372e-02, 5.9319e-02, 5.3146e-02, 9.7905e-07,
        6.0141e-02, 5.3758e-02, 7.0407e-07, 6.3402e-02, 1.4168e-06, 6.2716e-02,
        6.8637e-02, 1.6968e-06, 8.8331e-07, 2.5026e-06, 4.3013e-07, 2.3498e-06,
        6.0241e-02, 1.1042e-06, 1.1042e-06, 5.5537e-02, 5.1518e-02, 3.4964e-06,
        6.0050e-02, 9.6620e-02, 8.0701e-07, 5.4766e-02, 1.2537e-02, 6.3479e-02,
        3.9575e-02, 7.9946e-07, 5.8018e-02, 1.0687e-06, 6.2963e-02, 5.4120e-02,
        1.9580e-06, 6.4657e-02, 5.6033e-02, 5.5358e-02, 3.1144e-07, 4.4713e-02,
        6.0081e-02, 5.2067e-07, 5.1846e-02, 4.5571e-02, 5.0874e-02, 5.9824e-02,
        4.5679e-02, 2.7073e-02, 5.5815e-02, 1.2687e-06, 3.6111e-02, 2.1679e-06,
        4.7829e-02, 1.0286e-06, 2.6671e-06, 6.4304e-02, 5.0658e-02, 3.2404e-02,
        6.2564e-02, 5.2308e-02, 1.3877e-06, 6.2874e-02, 7.0478e-02, 1.4513e-06,
        1.3436e-06, 4.8622e-02, 6.0900e-02, 6.6090e-02, 5.2067e-07, 1.1412e-06,
        1.4261e-01, 6.0481e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.6941e-02, 5.5680e-02, 6.8311e-02, 6.7682e-03, 6.4745e-02, 1.0469e-02,
        5.0929e-02, 4.4946e-03, 5.0265e-02, 4.8647e-03, 4.3259e-02, 5.0059e-02,
        3.1866e-02, 5.8009e-02, 6.2668e-02, 1.1495e-03, 5.8940e-03, 4.6147e-02,
        8.7330e-03, 3.6489e-02, 5.0212e-02, 3.1088e-03, 7.2704e-02, 2.4374e-03,
        4.3128e-02, 4.5662e-03, 6.3710e-03, 8.0088e-02, 1.4400e-03, 3.5973e-03,
        5.3842e-02, 5.8328e-02, 4.6928e-02, 7.3098e-02, 5.1409e-02, 4.7570e-02,
        7.1550e-02, 2.2946e-02, 5.5461e-02, 7.4814e-02, 3.3902e-03, 1.4253e-06,
        5.0102e-02, 5.7895e-02, 6.3019e-02, 7.7168e-04, 7.8155e-02, 5.6341e-02,
        4.5785e-02, 1.3935e-02, 4.9193e-03, 1.6560e-03, 6.6934e-02, 6.4359e-02,
        5.7864e-02, 6.7644e-02, 5.6059e-02, 5.9919e-02, 2.6274e-02, 5.9408e-02,
        4.3842e-02, 2.9330e-03, 5.8294e-02, 2.0971e-02, 5.1175e-02, 5.8537e-02,
        7.0628e-02, 4.7398e-02, 1.4337e-03, 1.9559e-03, 5.8680e-07, 1.1499e-04,
        5.2996e-02, 8.2638e-02, 6.6284e-02, 5.1988e-02, 5.2922e-02, 5.0046e-02,
        4.0772e-02, 4.9261e-03, 4.7779e-02, 5.9430e-02, 6.2441e-02, 3.0737e-02,
        4.0459e-02, 3.6137e-02, 5.0026e-02, 4.6882e-02, 2.3335e-02, 2.2043e-03,
        4.5247e-02, 3.9286e-03, 1.9700e-03, 3.9792e-02, 1.6876e-06, 3.6565e-02,
        6.4755e-02, 6.4416e-02, 5.8968e-02, 1.1348e-03, 3.7732e-03, 8.0816e-02,
        3.6800e-02, 3.9615e-02, 2.0868e-02, 6.0068e-02, 6.3682e-03, 2.0739e-02,
        1.2603e-03, 4.6027e-02, 5.4679e-02, 1.3740e-03, 4.2707e-02, 3.9874e-02,
        7.3935e-02, 5.9692e-02, 2.5831e-03, 7.1523e-02, 6.7663e-03, 4.6971e-02,
        6.1157e-02, 6.4117e-02, 8.3152e-02, 5.5023e-03, 1.2203e-02, 1.2917e-03,
        1.2647e-01, 4.5599e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.9242e-02, 5.6421e-02, 6.1931e-02, 2.3168e-06, 1.5514e-06, 5.6684e-02,
        7.3654e-02, 5.0997e-02, 4.5477e-02, 1.4920e-06, 6.7927e-02, 5.9728e-02,
        5.3130e-02, 6.5236e-07, 5.3107e-02, 5.8568e-02, 6.1308e-02, 1.8316e-06,
        4.6090e-02, 5.8497e-02, 5.6406e-02, 6.0057e-02, 5.6654e-02, 3.8035e-07,
        5.4669e-02, 5.5880e-02, 6.2908e-02, 5.8257e-02, 5.5917e-02, 5.7195e-02,
        1.4920e-06, 5.5140e-02, 7.1052e-02, 5.5825e-02, 5.5817e-02, 1.2744e-01,
        5.9403e-02, 6.4536e-02, 5.0494e-02, 5.8578e-02, 5.7203e-02, 5.0942e-02,
        6.0706e-02, 5.1028e-02, 5.6188e-02, 6.0205e-02, 6.9019e-02, 5.6682e-07,
        3.7744e-07, 6.6874e-02, 6.0512e-02, 4.8300e-02, 6.0481e-02, 4.3899e-02,
        5.7914e-02, 6.6459e-02, 6.2069e-02, 5.6603e-02, 5.5885e-02, 5.1770e-02,
        5.3296e-02, 9.7560e-07, 5.3050e-02, 5.3241e-02, 7.2478e-02, 6.3112e-02,
        5.3377e-02, 5.7453e-02, 5.8259e-02, 4.7893e-02, 5.9921e-02, 5.6341e-02,
        4.9527e-02, 5.4446e-02, 4.9327e-02, 6.2889e-02, 1.9641e-06, 5.8689e-02,
        1.4500e-06, 5.6762e-02, 5.8493e-02, 6.1295e-02, 2.9940e-02, 4.8847e-02,
        6.6624e-02, 5.4773e-02, 5.6199e-02, 4.7643e-02, 5.3229e-02, 4.4545e-02,
        6.4765e-02, 7.0214e-02, 5.2950e-02, 1.5042e-06, 5.8821e-02, 5.5728e-02,
        5.8173e-02, 6.2396e-02, 4.5291e-07, 6.4360e-02, 5.7227e-02, 2.2106e-06,
        5.8571e-02, 2.1466e-07, 5.1114e-02, 6.7168e-02, 5.0207e-02, 7.4891e-02,
        6.1682e-02, 5.8005e-02, 5.8280e-02, 4.8770e-07, 6.2476e-02, 6.3533e-02,
        5.1747e-02, 5.2117e-02, 5.8943e-02, 5.9255e-02, 6.6534e-02, 6.1403e-02,
        5.6268e-02, 5.7874e-07, 5.7985e-02, 5.0484e-02, 5.7874e-07, 5.4254e-02,
        6.5533e-02, 2.1466e-07, 6.0163e-02, 6.1958e-02, 5.4473e-02, 5.2556e-02,
        6.7761e-02, 6.1499e-02, 5.9828e-02, 5.8930e-02, 5.2825e-02, 5.8594e-02,
        5.4833e-02, 4.7706e-07, 6.0106e-02, 5.8755e-02, 6.4766e-02, 5.9026e-02,
        5.4442e-02, 4.9640e-02, 6.2206e-02, 5.9524e-02, 4.7134e-02, 5.6186e-02,
        5.9644e-02, 5.1112e-07, 5.8472e-02, 6.1368e-02, 5.1729e-02, 6.1566e-02,
        6.0445e-02, 5.4768e-02, 5.5385e-02, 4.2484e-02, 5.9669e-02, 5.6970e-02,
        5.4111e-02, 6.0646e-02, 5.6801e-02, 5.1195e-02, 5.2730e-02, 5.4703e-02,
        6.0625e-02, 6.4965e-02, 6.6457e-02, 4.7668e-07, 7.1707e-07, 6.2939e-02,
        5.3350e-02, 3.4011e-02, 5.7556e-02, 6.1623e-02, 5.7695e-02, 5.5875e-02,
        9.9067e-03, 5.9060e-02, 5.4440e-02, 6.5140e-02, 5.3068e-02, 5.7821e-02,
        5.5899e-02, 5.0695e-02, 6.1751e-02, 5.0887e-02, 2.3168e-06, 1.8316e-06,
        5.6571e-02, 5.8066e-02, 6.5138e-02, 4.9909e-02, 5.9414e-02, 6.3830e-02,
        5.8543e-02, 6.0419e-02, 5.1678e-02, 6.5235e-02, 5.0355e-02, 7.1968e-07,
        6.1912e-02, 5.2908e-02, 6.2643e-02, 5.4131e-02, 5.4522e-02, 7.8096e-07,
        5.2907e-02, 5.2220e-02, 5.6947e-02, 5.7390e-02, 5.1137e-02, 5.8836e-02,
        5.6467e-02, 5.7326e-02, 6.0447e-02, 6.5149e-02, 5.4679e-02, 5.6337e-02,
        1.5238e-01, 4.8757e-02, 2.1466e-07, 5.5290e-02, 5.3088e-02, 5.3162e-02,
        5.7414e-02, 1.0598e-06, 1.5829e-06, 2.2964e-06, 5.4121e-02, 5.6737e-02,
        5.7938e-02, 5.6397e-02, 6.7384e-02, 5.9969e-02, 6.3982e-02, 6.0781e-02,
        5.4024e-02, 1.5042e-06, 7.1968e-07, 5.6436e-02, 5.4475e-02, 5.0072e-02,
        5.0794e-02, 5.5419e-02, 5.9962e-02, 6.2595e-02, 6.2157e-02, 5.4194e-02,
        6.0981e-02, 5.2275e-02, 4.8893e-02, 3.4049e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.9678e-07, 2.0398e-02, 5.1309e-02, 3.7973e-06, 5.9312e-02, 2.0573e-06,
        9.9811e-07, 5.5866e-02, 4.9934e-02, 6.0728e-02, 3.1239e-06, 2.0259e-06,
        1.4186e-06, 2.0227e-06, 2.9800e-06, 1.8992e-06, 7.3533e-02, 2.1615e-06,
        5.1740e-02, 2.7882e-06, 3.1119e-02, 6.6877e-06, 1.6159e-06, 9.5488e-06,
        6.5523e-02, 5.7667e-02, 1.1753e-06, 5.2264e-02, 9.0922e-07, 4.9841e-02,
        5.1605e-02, 6.3512e-02, 5.4928e-02, 1.1674e-06, 5.6347e-02, 5.0933e-06,
        6.2747e-02, 6.0956e-02, 5.8036e-02, 6.4087e-07, 1.9898e-06, 4.8185e-02,
        7.2266e-02, 4.2267e-02, 5.5339e-02, 5.4598e-02, 4.8691e-02, 1.9299e-02,
        5.7150e-02, 4.8559e-02, 6.3584e-02, 4.1658e-06, 4.5504e-02, 5.0330e-02,
        6.0462e-02, 4.7700e-02, 4.1420e-06, 5.7229e-02, 1.5874e-06, 1.2128e-06,
        2.7890e-06, 4.8286e-02, 9.4137e-07, 5.0510e-02, 5.8452e-02, 3.6823e-06,
        1.6189e-06, 1.2218e-01, 6.3713e-02, 5.0933e-06, 4.3032e-02, 5.3450e-02,
        6.9153e-02, 5.2871e-02, 5.0525e-02, 3.0061e-06, 3.8409e-06, 4.2964e-02,
        7.2828e-02, 1.2322e-06, 3.8429e-06, 6.0448e-02, 2.6247e-06, 1.5184e-06,
        4.1656e-06, 2.0597e-06, 4.0229e-02, 3.5506e-06, 5.3704e-02, 5.9454e-02,
        1.3344e-06, 1.2034e-01, 5.5404e-02, 2.9556e-06, 8.8669e-06, 6.4169e-07,
        5.6324e-02, 5.5406e-02, 1.1173e-06, 6.1970e-02, 3.4240e-06, 5.4942e-02,
        3.2610e-02, 5.3768e-02, 2.5816e-06, 5.3271e-02, 3.1246e-06, 5.3866e-02,
        4.1517e-06, 1.1163e-06, 4.3996e-02, 1.2222e-06, 2.0965e-06, 4.6556e-02,
        1.1753e-06, 4.4792e-02, 6.0700e-02, 5.8980e-02, 2.6238e-06, 3.8913e-02,
        5.4959e-02, 6.6765e-02, 5.2764e-02, 5.7619e-02, 4.0104e-02, 9.4487e-07,
        1.2129e-06, 5.6557e-02, 6.4079e-07, 4.9718e-02, 4.9719e-02, 4.6699e-02,
        4.8547e-06, 4.9513e-02, 5.0907e-06, 4.6947e-02, 5.8038e-02, 1.2645e-06,
        3.6741e-06, 1.8597e-06, 4.3970e-02, 2.3622e-06, 5.2354e-02, 1.8795e-06,
        2.3485e-06, 1.2085e-06, 7.1214e-02, 5.0585e-02, 5.1689e-02, 4.9153e-02,
        3.1246e-06, 6.2606e-02, 5.0345e-02, 1.2400e-06, 4.8937e-06, 6.2368e-02,
        4.7647e-02, 5.5384e-02, 2.7891e-06, 1.2506e-06, 6.4346e-02, 1.1972e-06,
        1.5838e-06, 1.7180e-06, 1.1015e-06, 1.6050e-06, 1.3029e-06, 6.4514e-02,
        6.2779e-02, 2.7888e-06, 3.1014e-06, 3.1196e-06, 5.9516e-02, 5.1447e-02,
        5.4294e-02, 2.1078e-06, 1.2583e-06, 6.0851e-02, 2.6227e-06, 3.4656e-02,
        7.5440e-02, 7.4115e-02, 5.1832e-02, 6.3334e-02, 4.6209e-02, 6.7319e-02,
        1.8141e-06, 2.4765e-06, 5.4002e-02, 6.5431e-02, 6.6116e-02, 7.5228e-02,
        1.4476e-06, 9.4577e-07, 1.8141e-06, 5.4313e-02, 2.8837e-06, 5.7709e-02,
        2.6237e-06, 2.5461e-06, 2.6774e-06, 3.1088e-06, 1.3570e-06, 1.5838e-06,
        5.3977e-02, 1.8149e-06, 6.0444e-02, 6.1124e-02, 1.6069e-06, 1.9941e-06,
        4.1671e-02, 4.1662e-06, 5.5363e-02, 5.7911e-02, 6.6806e-02, 7.5398e-02,
        2.9497e-02, 1.5740e-06, 3.3571e-02, 5.0865e-02, 2.8128e-06, 1.9543e-06,
        5.5596e-06, 2.8441e-07, 2.9050e-06, 5.8844e-02, 5.2435e-02, 1.4055e-06,
        1.3570e-06, 5.7245e-02, 5.7598e-02, 7.6011e-02, 7.9165e-02, 4.8313e-02,
        2.4502e-06, 5.6292e-02, 7.8964e-02, 1.3960e-06, 5.3627e-02, 5.9670e-07,
        1.3343e-06, 5.5300e-02, 6.2086e-02, 9.0972e-07, 3.8171e-02, 4.9529e-02,
        7.0298e-02, 4.6933e-02, 1.3596e-06, 1.5437e-06, 3.6826e-02, 7.0163e-02,
        5.6930e-02, 5.7537e-02, 2.3374e-06, 6.1842e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.8234e-02, 5.8571e-02, 3.2699e-02, 5.2052e-02, 3.9179e-02, 2.1383e-02,
        6.1589e-08, 5.2797e-02, 3.9315e-02, 2.7110e-02, 4.4266e-02, 4.3825e-02,
        8.2595e-08, 5.0113e-02, 3.0913e-02, 4.6829e-02, 4.0392e-02, 2.5356e-02,
        4.1769e-02, 3.8321e-02, 3.4549e-02, 3.1621e-03, 1.0299e-07, 4.8278e-02,
        4.5932e-02, 3.2954e-02, 7.2066e-04, 5.4030e-02, 4.7774e-02, 4.4915e-02,
        4.3561e-02, 4.9385e-02, 3.8079e-02, 4.9141e-03, 4.4255e-02, 1.3260e-07,
        5.6732e-02, 3.5463e-02, 3.7239e-02, 3.0893e-02, 5.0262e-02, 5.1590e-02,
        3.8242e-02, 6.0503e-02, 2.6484e-02, 3.4683e-02, 2.9742e-02, 3.7713e-02,
        4.8129e-02, 4.3987e-02, 4.1395e-02, 1.5879e-04, 3.1365e-02, 3.5393e-02,
        4.1467e-02, 3.6725e-02, 4.5876e-02, 3.2261e-02, 2.3593e-02, 7.5265e-08,
        5.2648e-08, 3.7096e-02, 3.0006e-02, 4.5851e-02, 2.6395e-02, 2.5762e-02,
        3.1872e-02, 4.2648e-02, 3.6782e-02, 3.0952e-07, 3.3285e-02, 4.8272e-02,
        3.8430e-02, 3.2454e-02, 3.4728e-02, 4.6216e-02, 4.7127e-02, 2.4797e-02,
        6.1444e-02, 1.3521e-01, 1.7563e-02, 4.1680e-02, 6.5741e-02, 5.2648e-08,
        5.3960e-08, 5.2785e-08, 3.8043e-02, 2.4061e-02, 3.8038e-02, 4.1541e-02,
        6.1589e-08, 3.3359e-02, 4.2877e-02, 6.1996e-02, 4.8248e-02, 5.1435e-02,
        4.1284e-02, 3.2437e-02, 3.8340e-02, 3.2103e-02, 1.6580e-07, 5.4902e-02,
        4.6017e-02, 3.9310e-02, 4.4974e-02, 5.6647e-02, 3.8058e-04, 3.6387e-02,
        3.5262e-02, 8.8014e-08, 4.2981e-02, 7.4999e-08, 1.0289e-07, 2.6383e-02,
        3.1331e-08, 4.3188e-02, 2.6737e-02, 4.7367e-02, 4.9835e-02, 3.8447e-02,
        3.3843e-02, 4.2234e-02, 4.8703e-02, 3.2963e-02, 3.3755e-02, 1.0299e-08,
        4.4695e-02, 3.6129e-02, 3.4107e-02, 4.6424e-02, 3.9485e-02, 3.9561e-02,
        1.3263e-07, 5.6746e-02, 4.2008e-02, 5.7365e-02, 4.0911e-02, 4.7845e-02,
        9.3315e-03, 4.0549e-02, 6.3168e-02, 1.0289e-07, 4.4730e-02, 6.0580e-08,
        4.8854e-02, 5.2648e-08, 3.5280e-02, 3.7150e-02, 4.6829e-02, 3.5269e-02,
        3.3824e-02, 3.8023e-02, 3.9207e-02, 5.1048e-08, 5.2785e-08, 4.5075e-02,
        5.3719e-02, 4.0290e-02, 1.4528e-02, 1.6580e-07, 4.5139e-02, 5.1527e-02,
        1.3263e-07, 1.3260e-07, 1.0613e-02, 2.8769e-02, 4.3965e-08, 4.8624e-02,
        4.5140e-02, 3.5660e-02, 4.9425e-08, 1.6896e-02, 3.2146e-02, 2.9744e-02,
        6.2473e-02, 3.5680e-02, 5.8312e-02, 4.1443e-02, 2.0121e-02, 3.1074e-02,
        4.2168e-02, 3.5897e-02, 3.9600e-02, 3.2220e-02, 3.7983e-02, 4.1756e-02,
        2.3051e-02, 3.1620e-02, 4.3685e-02, 7.4882e-02, 4.0753e-02, 5.3657e-02,
        1.1111e-07, 4.4807e-02, 4.5724e-02, 4.1827e-02, 4.9699e-02, 4.3177e-02,
        4.4088e-03, 1.6886e-02, 3.7431e-02, 3.0812e-02, 5.2420e-03, 6.8414e-08,
        4.3225e-02, 4.2488e-03, 5.1852e-02, 3.3655e-02, 5.4232e-02, 4.9171e-02,
        4.0450e-02, 1.3748e-07, 3.4747e-02, 4.6125e-02, 3.5954e-02, 4.3914e-02,
        2.2799e-01, 7.1329e-03, 2.7801e-02, 3.0679e-02, 1.1667e-02, 2.4413e-07,
        1.3748e-07, 4.6519e-02, 3.5289e-02, 4.3241e-02, 2.9385e-02, 3.7248e-02,
        1.5301e-02, 4.4271e-02, 5.5330e-02, 4.8300e-02, 3.1427e-02, 3.8505e-02,
        7.4971e-08, 5.9733e-02, 3.5778e-02, 4.0782e-02, 5.7340e-02, 3.1329e-02,
        7.7573e-08, 2.8943e-02, 1.6907e-02, 3.1418e-02, 2.7283e-02, 4.1895e-02,
        5.2940e-02, 3.6323e-02, 4.3248e-02, 1.9113e-02, 3.5532e-02, 3.7759e-02,
        3.0693e-02, 5.0051e-02, 5.5607e-03, 5.2850e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.0266e-06, 2.8867e-06, 6.9341e-02, 1.1893e-06, 8.6943e-02, 4.7413e-06,
        2.2224e-06, 9.4685e-07, 2.1756e-06, 2.5306e-06, 8.1705e-02, 2.5813e-06,
        7.8364e-07, 2.5846e-06, 4.0266e-06, 6.6332e-07, 1.0326e-06, 3.4600e-06,
        6.9460e-07, 4.7066e-06, 6.6332e-07, 3.2794e-06, 3.7692e-06, 1.8185e-06,
        3.6809e-07, 7.8139e-02, 1.1295e-06, 1.1295e-06, 2.3541e-06, 1.8743e-06,
        1.3664e-06, 1.1306e-06, 6.7756e-02, 2.0024e-06, 2.2224e-06, 6.5933e-02,
        9.6348e-02, 1.6572e-06, 1.8346e-06, 3.6809e-07, 2.5846e-06, 3.4600e-06,
        1.8346e-06, 5.6806e-02, 1.1893e-06, 2.9792e-06, 8.6085e-07, 2.5846e-06,
        8.1538e-02, 7.1063e-02, 1.4532e-06, 3.4290e-07, 2.2670e-06, 7.4834e-02,
        1.4787e-06, 1.2897e-06, 4.0266e-06, 1.8346e-06, 1.3664e-06, 1.1893e-06,
        7.8364e-07, 1.6572e-06, 2.5242e-06, 4.7066e-06, 1.4752e-06, 4.0266e-06,
        1.2918e-06, 3.4600e-06, 7.6423e-07, 7.6877e-02, 2.5813e-06, 3.0670e-06,
        3.4600e-06, 1.1295e-06, 2.1756e-06, 1.1295e-06, 7.0915e-02, 9.5462e-07,
        3.0670e-06, 2.2670e-06, 4.9973e-07, 1.0393e-06, 2.8867e-06, 1.3499e-06,
        1.6572e-06, 2.1944e-06, 5.6571e-02, 7.8364e-07, 2.7806e-06, 2.9795e-02,
        1.0326e-06, 1.0162e-06, 5.2735e-07, 1.0393e-06, 2.3669e-06, 2.5813e-06,
        1.6572e-06, 1.0162e-06, 6.0420e-02, 1.5703e-06, 4.9264e-02, 5.9301e-02,
        7.8364e-07, 1.2918e-06, 1.0326e-06, 9.5411e-07, 1.2969e-06, 1.1391e-06,
        4.9973e-07, 9.6971e-07, 2.9792e-06, 1.1740e-06, 3.4600e-06, 1.1295e-06,
        2.2224e-06, 5.9098e-02, 1.2918e-06, 6.5814e-02, 2.1489e-06, 1.1162e-06,
        1.3664e-06, 5.4286e-02, 6.5162e-02, 2.7132e-06, 6.4980e-07, 1.1893e-06,
        5.9860e-02, 6.5771e-02, 5.8059e-02, 1.6572e-06, 1.8346e-06, 5.2277e-02,
        1.9110e-06, 1.6572e-06, 1.1295e-06, 9.4685e-07, 9.4685e-07, 9.1569e-07,
        2.3669e-06, 1.1295e-06, 3.2509e-06, 2.3669e-06, 3.0670e-06, 1.9110e-06,
        9.5462e-07, 1.6783e-06, 6.2513e-02, 1.8346e-06, 3.2509e-06, 2.1944e-06,
        1.3581e-06, 1.3449e-01, 4.6360e-06, 2.1939e-06, 2.8333e-06, 8.6548e-02,
        6.7739e-02, 2.1939e-06, 1.8346e-06, 8.8739e-02, 7.8364e-07, 1.6572e-06,
        1.1572e-06, 1.8346e-06, 1.1162e-06, 1.0326e-06, 1.1162e-06, 6.8836e-07,
        1.1893e-06, 1.9110e-06, 2.5033e-06, 3.4600e-06, 1.0393e-06, 2.5813e-06,
        1.1893e-06, 3.5877e-06, 2.9792e-06, 1.1306e-06, 3.5877e-06, 3.6808e-07,
        2.6924e-06, 2.2224e-06, 8.0463e-02, 3.5877e-06, 8.4056e-02, 6.3376e-02,
        8.2431e-02, 1.2683e-06, 3.0645e-06, 1.8346e-06, 4.7066e-06, 9.4685e-07,
        3.2509e-06, 1.0435e-01, 1.6285e-06, 9.4685e-07, 1.1295e-06, 1.2918e-06,
        2.2670e-06, 1.8346e-06, 7.1337e-02, 8.6131e-07, 5.9249e-02, 4.6249e-06,
        5.5665e-02, 1.8346e-06, 2.1944e-06, 6.9945e-02, 7.8364e-07, 6.8836e-07,
        3.0670e-06, 1.8346e-06, 2.5306e-06, 7.2102e-02, 2.6464e-06, 1.7906e-06,
        4.3255e-06, 2.3541e-06, 4.9973e-07, 6.9598e-02, 1.1893e-06, 6.7814e-02,
        1.1295e-06, 8.9175e-07, 4.7066e-06, 7.4453e-02, 6.0559e-06, 1.3389e-06,
        2.2795e-06, 1.6572e-06, 1.2918e-06, 3.0670e-06, 4.2000e-02, 6.4216e-02,
        1.6490e-06, 2.5306e-06, 8.6131e-07, 6.8836e-07, 1.2918e-06, 4.7480e-02,
        7.6044e-02, 5.9896e-02, 3.6214e-06, 3.0645e-06, 9.4685e-07, 1.8694e-06,
        6.0559e-06, 8.4125e-07, 2.5813e-06, 5.3775e-02, 5.5985e-02, 9.4685e-07,
        7.3175e-02, 2.4424e-06, 4.9973e-07, 1.4787e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.0402e-07, 2.4047e-06, 1.1373e-06, 2.2827e-06, 1.1285e-06, 3.5378e-06,
        2.0541e-06, 2.3306e-06, 9.7239e-02, 3.0532e-06, 2.1076e-06, 3.1880e-06,
        1.3816e-06, 2.8574e-06, 2.3494e-06, 2.1302e-06, 7.9136e-02, 2.2642e-06,
        1.3418e-02, 7.7293e-07, 3.1355e-06, 5.7080e-06, 4.7355e-07, 1.9556e-06,
        1.3695e-06, 2.1249e-06, 2.6229e-06, 3.4268e-06, 1.4887e-06, 3.3911e-06,
        2.6951e-06, 2.9255e-02, 1.9858e-06, 1.8053e-06, 2.5256e-06, 2.7194e-07,
        1.8647e-06, 1.4557e-06, 4.3109e-06, 2.3598e-06, 2.1080e-06, 1.2685e-06,
        5.8101e-02, 2.9951e-02, 4.4126e-06, 1.6140e-02, 4.8251e-06, 3.1188e-06,
        1.2104e-06, 4.2990e-06, 1.9547e-06, 5.7082e-06, 1.3361e-06, 1.4980e-06,
        3.2788e-06, 3.9430e-06, 4.9613e-06, 1.9133e-06, 2.1067e-06, 2.2806e-06,
        2.4729e-06, 2.9281e-06, 2.1095e-06, 2.0591e-06, 1.4206e-06, 2.2479e-06,
        1.7017e-06, 2.2864e-06, 1.7470e-06, 2.6949e-06, 2.4507e-06, 2.2455e-06,
        4.4076e-06, 9.8571e-02, 1.9844e-06, 2.6792e-06, 2.0654e-06, 3.5142e-06,
        1.1102e-06, 1.2335e-06, 7.9999e-07, 8.6840e-07, 7.2427e-07, 5.3745e-06,
        2.2515e-06, 9.3096e-02, 2.8345e-06, 2.2643e-06, 2.3599e-06, 4.6292e-06,
        3.7988e-06, 3.3021e-02, 1.6804e-06, 1.7022e-06, 9.9060e-07, 1.7494e-06,
        5.9639e-06, 4.4051e-06, 2.5716e-06, 2.0235e-02, 2.1315e-06, 2.9020e-02,
        1.3141e-06, 1.2647e-06, 1.3025e-06, 2.6014e-06, 3.5407e-06, 1.0134e-06,
        5.7039e-06, 3.0181e-06, 1.3528e-06, 2.3210e-06, 2.8747e-02, 2.2733e-02,
        4.0088e-06, 2.2192e-06, 2.1073e-06, 1.5035e-06, 2.1066e-06, 3.2255e-07,
        4.3074e-06, 1.8850e-06, 1.4528e-06, 5.0402e-06, 2.9643e-02, 2.6671e-06,
        3.4096e-06, 1.2211e-06, 3.7976e-06, 1.1328e-06, 9.3483e-07, 1.7948e-06,
        5.7417e-06, 1.9467e-06, 1.9046e-06, 1.9082e-06, 2.8856e-06, 1.9843e-06,
        1.8725e-06, 1.5175e-06, 2.2316e-06, 2.5256e-06, 9.6442e-07, 2.2915e-06,
        1.9056e-06, 2.4835e-02, 2.6276e-06, 4.6796e-06, 1.2199e-06, 2.1966e-02,
        2.1033e-06, 5.4643e-06, 2.2988e-06, 8.0058e-06, 6.0731e-06, 1.2471e-06,
        1.9570e-06, 4.1971e-06, 1.4043e-06, 1.2448e-01, 2.6023e-06, 2.3719e-06,
        1.6957e-06, 2.5727e-06, 2.2194e-06, 6.7075e-06, 6.3445e-06, 1.3739e-01,
        4.0649e-06, 3.7220e-06, 4.6309e-06, 1.1327e-06, 5.0610e-06, 3.1276e-06,
        2.6218e-06, 3.2981e-02, 1.9552e-06, 1.3890e-01, 2.2100e-06, 3.0745e-06,
        2.6089e-06, 1.8380e-06, 1.1103e-06, 4.6332e-06, 1.5747e-06, 2.4165e-06,
        1.1494e-06, 6.1852e-06, 3.0159e-06, 9.9940e-02, 3.9763e-02, 4.3070e-06,
        1.8751e-06, 2.2202e-06, 1.1491e-06, 1.6327e-06, 9.6544e-07, 2.0626e-06,
        2.5254e-06, 7.7209e-07, 3.5766e-06, 3.5921e-06, 1.8772e-06, 2.2864e-06,
        2.8831e-06, 1.0645e-06, 6.5099e-06, 1.4825e-06, 1.7020e-06, 1.9813e-06,
        2.3314e-06, 1.6942e-06, 3.4952e-06, 1.5754e-06, 2.1826e-06, 1.3038e-01,
        1.7926e-01, 1.3539e-06, 1.8051e-06, 2.1737e-06, 3.1092e-06, 1.9620e-06,
        2.3350e-02, 5.0610e-06, 1.1270e-06, 6.1848e-06, 2.3689e-06, 2.6558e-06,
        9.3395e-07, 2.1528e-02, 3.4587e-06, 9.5355e-02, 3.4890e-06, 4.3073e-06,
        1.7021e-06, 2.6865e-06, 4.0884e-02, 4.4657e-06, 1.8043e-06, 3.6488e-06,
        1.4880e-06, 9.8592e-02, 1.9842e-06, 1.9087e-06, 2.6141e-06, 8.5508e-03,
        8.8266e-02, 9.3873e-07, 4.6304e-06, 4.3071e-06, 6.3331e-03, 1.0747e-01,
        1.9842e-06, 7.8801e-06, 4.6802e-06, 1.2965e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.1747e-06, 2.2662e-06, 6.4098e-07, 1.0229e-01, 3.1884e-06, 1.3581e-06,
        3.4834e-02, 1.1061e-06, 7.6818e-07, 2.2336e-06, 2.3138e-06, 7.6817e-07,
        1.5400e-06, 4.0039e-06, 1.4435e-06, 1.4221e-06, 1.1061e-06, 5.4325e-07,
        5.6748e-06, 1.4221e-06, 1.3482e-06, 1.6011e-06, 7.6818e-07, 2.0567e-06,
        2.1640e-06, 1.6011e-06, 2.6965e-06, 1.4817e-06, 6.3639e-06, 1.9379e-06,
        2.8177e-06, 4.2529e-06, 2.2366e-06, 2.8647e-06, 5.3571e-07, 3.4541e-06,
        2.9460e-06, 4.1112e-06, 7.3843e-06, 2.5125e-06, 1.5366e-06, 3.9169e-06,
        2.0707e-06, 3.5663e-06, 4.0039e-06, 7.6818e-07, 3.4541e-06, 3.0718e-02,
        6.4098e-07, 4.0024e-06, 2.0394e-06, 3.8790e-02, 5.6015e-06, 3.9688e-06,
        2.3046e-06, 5.1193e-02, 3.9668e-02, 3.9688e-06, 1.3581e-06, 5.7942e-06,
        2.2366e-06, 4.2033e-06, 1.5387e-06, 3.1661e-06, 2.9375e-06, 3.5168e-06,
        1.5386e-06, 3.0450e-06, 1.4221e-06, 7.8748e-07, 1.4221e-06, 1.2203e-02,
        2.5483e-06, 1.8589e-06, 3.6096e-06, 4.7202e-06, 4.3626e-02, 2.2793e-06,
        1.6561e-06, 5.6015e-06, 1.6224e-06, 2.6544e-06, 2.4671e-06, 4.2033e-06,
        1.6536e-06, 1.4817e-06, 1.4534e-06, 1.4221e-06, 2.9460e-06, 1.5400e-06,
        1.4221e-06, 2.2425e-02, 2.9241e-06, 2.9460e-06, 3.4603e-06, 4.0464e-02,
        4.1112e-06, 2.7511e-06, 2.8298e-06, 2.0943e-06, 2.7730e-06, 4.2033e-06,
        1.6561e-06, 1.1061e-06, 1.4221e-06, 1.8691e-06, 4.1906e-02, 1.4221e-06,
        5.8756e-06, 2.2366e-06, 9.3479e-07, 2.4418e-06, 2.2366e-06, 4.1670e-02,
        3.4080e-06, 2.5809e-06, 9.1702e-07, 2.0812e-06, 1.9379e-06, 2.0394e-06,
        3.0450e-06, 3.5168e-06, 1.3710e-06, 3.6096e-06, 1.8691e-06, 4.1112e-06,
        1.9755e-02, 1.1649e-06, 3.9688e-06, 2.0247e-06, 4.5482e-06, 2.0796e-06,
        4.5261e-06, 1.5366e-06, 1.6627e-06, 1.2452e-06, 2.1640e-06, 1.3223e-06,
        4.2033e-06, 7.9501e-07, 2.0597e-06, 1.5387e-06, 5.6015e-06, 4.9173e-06,
        2.7730e-06, 4.0039e-06, 2.0151e-06, 2.2366e-06, 2.7292e-06, 4.5482e-06,
        7.6818e-07, 9.8873e-07, 4.7202e-06, 3.5663e-06, 2.3161e-06, 2.4924e-06,
        3.5785e-02, 1.6545e-06, 5.1523e-06, 2.9460e-06, 2.2793e-06, 4.1112e-06,
        4.5482e-06, 1.6536e-06, 4.7202e-06, 5.0932e-02, 3.0373e-06, 2.8177e-06,
        1.1061e-06, 2.1095e-06, 2.0569e-06, 3.1330e-07, 2.2366e-06, 2.0812e-06,
        1.4534e-06, 4.5482e-06, 5.4325e-07, 3.6483e-06, 3.9688e-06, 4.2529e-06,
        1.3581e-06, 1.6224e-06, 1.7347e-06, 3.6483e-06, 5.1523e-06, 2.0394e-06,
        4.2529e-06, 2.2379e-06, 3.5168e-06, 1.2824e-06, 1.6536e-06, 2.1655e-06,
        5.6069e-02, 2.5125e-06, 2.0569e-06, 2.7909e-06, 5.4325e-07, 1.6536e-06,
        1.4221e-06, 1.8192e-06, 2.3161e-06, 2.4285e-06, 1.1035e-06, 3.0450e-06,
        1.1035e-06, 1.1061e-06, 1.9418e-06, 4.2529e-06, 4.0321e-06, 3.1661e-06,
        2.3161e-06, 2.3646e-06, 3.6096e-06, 2.8177e-06, 1.4623e-06, 3.0631e-06,
        9.8873e-07, 7.5423e-07, 3.4080e-06, 3.0631e-06, 8.9478e-07, 2.2366e-06,
        2.0569e-06, 2.8456e-06, 1.2232e-06, 3.4080e-06, 3.2074e-06, 2.1636e-06,
        9.1702e-07, 3.5584e-06, 4.2033e-06, 1.4011e-06, 1.5386e-06, 2.4346e-06,
        6.9980e-07, 2.3046e-06, 1.4221e-06, 2.1655e-06, 5.5032e-07, 3.1016e-06,
        1.1061e-06, 2.3161e-06, 3.0373e-06, 1.4534e-06, 1.5387e-06, 2.0597e-06,
        7.5423e-07, 1.4221e-06, 3.4603e-06, 8.4838e-07, 6.5596e-07, 3.4080e-06,
        7.6818e-07, 2.1640e-06, 2.2366e-06, 1.6011e-06, 1.6536e-06, 2.7016e-06,
        3.1016e-06, 1.3482e-06, 4.7349e-02, 1.9379e-06, 2.3161e-06, 4.2273e-02,
        3.6812e-06, 7.6818e-07, 6.5596e-07, 3.0450e-06, 2.2662e-06, 4.0906e-06,
        2.0569e-06, 1.1994e-06, 2.0569e-06, 3.5663e-06, 4.2529e-06, 2.6643e-02,
        1.9096e-06, 3.9767e-06, 7.8063e-07, 9.8873e-07, 4.0039e-06, 4.2529e-06,
        5.7942e-06, 2.1640e-06, 2.2366e-06, 4.1324e-06, 1.9556e-06, 6.2768e-07,
        3.2109e-06, 3.6096e-06, 1.6545e-06, 5.5706e-06, 3.4092e-06, 5.0101e-03,
        2.5042e-06, 2.6544e-06, 3.5663e-06, 2.6670e-06, 4.2683e-02, 1.6224e-06,
        1.4221e-06, 4.2033e-06, 2.1095e-06, 1.7674e-06, 1.7499e-06, 4.1112e-06,
        3.4541e-06, 4.2765e-02, 2.9460e-06, 4.1324e-06, 1.6011e-06, 3.4080e-06,
        2.2336e-06, 1.7347e-06, 2.2662e-06, 7.6817e-07, 2.0812e-06, 3.7144e-02,
        2.9752e-02, 1.4534e-06, 3.9531e-02, 2.0080e-06, 4.2794e-07, 3.0373e-06,
        1.6536e-06, 2.6907e-06, 9.2702e-07, 1.3581e-06, 4.6966e-06, 5.2485e-06,
        2.8177e-06, 2.2793e-06, 1.1649e-06, 3.4541e-06, 3.4603e-06, 2.6540e-06,
        1.3311e-06, 2.6681e-06, 2.3916e-06, 1.5400e-06, 1.8276e-06, 2.6210e-02,
        3.9688e-06, 2.2379e-06, 2.2658e-06, 3.9960e-06, 2.1700e-06, 9.8873e-07,
        3.4603e-06, 2.1655e-06, 1.5386e-06, 5.5706e-06, 6.4098e-07, 5.2245e-06,
        4.3476e-06, 4.1324e-06, 4.0391e-02, 4.2529e-06, 2.4549e-06, 1.5400e-06,
        3.4080e-06, 2.2336e-06, 5.6015e-06, 2.1981e-06, 4.0039e-06, 4.2033e-06,
        1.2452e-06, 2.8791e-06, 3.0818e-06, 2.4903e-06, 1.4605e-06, 3.0373e-06,
        4.0242e-02, 4.2033e-06, 1.6536e-06, 2.0812e-06, 3.8663e-02, 6.4098e-07,
        1.1035e-06, 2.7511e-06, 1.5400e-06, 3.8043e-02, 3.5209e-06, 2.9460e-06,
        4.5482e-06, 3.6096e-06, 5.4324e-07, 2.7228e-02, 2.1680e-06, 2.2366e-06,
        1.1941e-06, 4.4627e-02, 1.6536e-06, 3.2496e-06, 2.9460e-06, 2.4549e-06,
        4.0039e-06, 1.4221e-06, 1.6224e-06, 1.9379e-06, 3.1016e-06, 4.5634e-06,
        2.3492e-06, 7.6817e-07, 1.4011e-06, 5.6015e-06, 1.6224e-06, 4.2529e-06,
        1.4534e-06, 2.5809e-06, 4.0039e-06, 2.3161e-06, 4.0039e-06, 2.7016e-06,
        4.5261e-06, 2.2793e-06, 3.9968e-06, 1.4534e-06, 3.4603e-06, 2.2662e-06,
        2.2366e-06, 1.1061e-06, 5.6748e-06, 2.6544e-06, 1.3879e-06, 2.4764e-06,
        2.2662e-06, 1.7674e-06, 2.1131e-06, 2.1655e-06, 3.9169e-06, 1.5386e-06,
        1.1649e-06, 4.9631e-02, 2.6544e-06, 1.0058e-06, 1.4221e-06, 2.6991e-06,
        3.4092e-06, 1.6011e-06, 1.6536e-06, 4.0039e-06, 2.7667e-06, 2.7730e-06,
        1.4221e-06, 2.7730e-06, 1.3581e-06, 8.4838e-07, 2.3161e-06, 3.8609e-06,
        2.1095e-06, 2.9460e-06, 1.0493e-06, 7.8063e-07, 1.9096e-06, 5.4325e-07,
        3.9453e-02, 2.2366e-06, 4.5634e-06, 4.2033e-06, 1.5387e-06, 4.0739e-02,
        3.5168e-06, 1.7628e-06, 5.0011e-02, 3.1016e-06, 5.8271e-06, 3.0450e-06,
        1.6585e-06, 1.6561e-06, 4.7562e-02, 3.0818e-06, 1.6536e-06, 9.1702e-07,
        5.9142e-06, 1.2183e-06, 4.7202e-06, 4.0165e-02, 1.7628e-06, 3.5168e-06,
        2.3046e-06, 4.4276e-02, 2.5125e-06, 2.1095e-06, 1.7527e-06, 3.6096e-06,
        2.6540e-06, 1.8275e-06, 2.7016e-06, 2.2366e-06, 2.2662e-06, 3.8609e-06,
        3.5663e-06, 1.3710e-06, 5.5706e-06, 2.3046e-06, 1.6560e-06, 3.8609e-06,
        1.9096e-06, 5.2484e-06, 5.3900e-06, 1.4221e-06, 7.6818e-07, 1.8691e-06,
        1.3710e-06, 4.2529e-06, 2.8259e-02, 2.7796e-02, 2.9348e-06, 2.2379e-06,
        2.3161e-06, 4.7202e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.3000e-06, 2.5639e-06, 3.7283e-06, 1.1939e-05, 1.2056e-05, 1.4230e-06,
        3.6473e-06, 8.9155e-06, 1.2348e-06, 1.1939e-05, 1.1687e-05, 6.7568e-06,
        3.5213e-06, 5.7558e-06, 1.9552e-06, 1.4034e-06, 1.1776e-05, 4.2216e-06,
        4.8217e-06, 9.0072e-06, 5.7558e-06, 6.0039e-06, 1.8514e-05, 6.6201e-06,
        5.7558e-06, 6.1948e-06, 3.6955e-06, 5.7932e-06, 8.5454e-03, 6.9576e-02,
        7.9246e-06, 2.5963e-06, 1.1012e-05, 4.5708e-06, 6.7568e-06, 4.3037e-06,
        2.6399e-06, 4.9665e-06, 7.1041e-06, 1.1687e-05, 5.5503e-06, 3.3000e-06,
        7.3464e-06, 6.2668e-06, 2.7356e-06, 7.1489e-06, 4.5850e-06, 4.8217e-06,
        1.3025e-05, 3.0671e-06, 2.5963e-06, 3.7300e-06, 1.4230e-06, 1.3654e-05,
        1.1018e-05, 1.0257e-05, 1.5494e-06, 7.4962e-06, 2.6399e-06, 5.7558e-06,
        1.1939e-05, 5.9335e-06, 6.1948e-06, 7.3464e-06, 1.1018e-05, 1.2977e-05,
        3.8334e-06, 1.9841e-06, 8.3030e-03, 3.9993e-06, 3.5213e-06, 6.1948e-06,
        3.9724e-06, 9.3228e-06, 6.7559e-06, 5.7558e-06, 3.0671e-06, 3.7283e-06,
        4.8428e-06, 5.8045e-06, 5.1674e-06, 8.0051e-06, 1.5302e-05, 5.4611e-06,
        3.6955e-06, 9.8781e-03, 3.9993e-06, 5.4482e-02, 9.5048e-06, 7.6752e-06,
        3.5213e-06, 1.1939e-05, 9.8269e-06, 1.1012e-05, 6.6741e-06, 4.2025e-06,
        3.5213e-06, 1.4077e-05, 6.7568e-06, 8.0051e-06, 1.0760e-05, 1.1012e-05,
        5.2200e-06, 1.1012e-05, 2.6399e-06, 6.8241e-06, 4.8093e-03, 4.5686e-06,
        9.7864e-06, 3.9993e-06, 6.9341e-06, 2.4746e-06, 4.3543e-06, 9.5386e-06,
        9.8269e-06, 5.7558e-06, 4.8857e-06, 3.8334e-06, 4.5686e-06, 5.2137e-03,
        5.7559e-06, 7.5636e-06, 4.7308e-06, 6.7843e-06, 3.3000e-06, 1.1018e-05,
        3.6977e-06, 1.1018e-05, 1.0624e-05, 1.8220e-06, 1.1012e-05, 7.1847e-06,
        7.5501e-06, 8.1204e-02, 6.7568e-06, 3.0671e-06, 1.0625e-05, 7.1489e-06,
        3.3569e-06, 1.2410e-05, 3.9993e-06, 1.2348e-06, 1.1844e-05, 7.5636e-06,
        1.1012e-05, 6.2438e-06, 7.7501e-06, 1.0625e-05, 1.1854e-05, 1.1012e-05,
        1.2395e-05, 2.5963e-06, 7.5501e-06, 6.2438e-06, 7.6752e-06, 1.1018e-05,
        2.5963e-06, 1.0201e-05, 1.2395e-05, 1.0760e-05, 3.6473e-06, 2.8953e-06,
        1.7448e-06, 3.0254e-06, 5.7558e-06, 1.9685e-06, 3.7283e-06, 2.6399e-06,
        5.6808e-03, 2.4104e-06, 4.5708e-06, 7.3827e-06, 3.9993e-06, 7.1281e-06,
        7.2982e-06, 5.6798e-06, 2.5963e-06, 3.2475e-06, 9.3910e-06, 1.0117e-05,
        2.5963e-06, 2.8953e-06, 7.7067e-06, 6.9471e-06, 8.2208e-06, 3.6473e-06,
        4.9569e-06, 4.3037e-06, 1.0760e-05, 7.5473e-06, 7.6752e-06, 3.6955e-06,
        2.9331e-06, 3.6955e-06, 1.4230e-06, 5.9686e-06, 6.5767e-06, 2.1976e-05,
        1.2121e-05, 7.4823e-06, 9.8269e-06, 4.6063e-06, 4.7942e-06, 6.5957e-02,
        5.7558e-06, 4.3037e-06, 9.5386e-06, 7.5501e-06, 2.4920e-06, 4.8428e-06,
        6.2668e-06, 4.7942e-06, 7.1281e-06, 1.2395e-05, 6.7843e-06, 1.0760e-05,
        1.1687e-05, 6.1098e-06, 8.4248e-06, 2.9496e-06, 2.6399e-06, 4.7436e-06,
        6.5875e-03, 3.2601e-06, 8.8487e-06, 1.2410e-05, 6.7843e-06, 7.7501e-06,
        1.1018e-05, 3.0671e-06, 5.5503e-06, 6.6008e-06, 1.2949e-05, 4.6716e-06,
        3.0671e-06, 4.5686e-06, 5.6798e-06, 5.6605e-06, 3.4622e-06, 2.2397e-06,
        6.6201e-06, 1.8823e-05, 1.2348e-06, 2.7356e-06, 1.2691e-05, 2.2487e-06,
        3.0570e-06, 1.3633e-05, 2.5963e-06, 1.4232e-06, 5.3470e-06, 2.5864e-06,
        2.5412e-06, 3.9227e-06, 2.6399e-06, 3.3542e-06, 6.3729e-06, 1.1854e-05,
        1.9790e-05, 5.7559e-06, 1.2409e-05, 5.3912e-06, 7.9188e-06, 5.8376e-06,
        1.2410e-05, 6.7568e-06, 4.1244e-07, 1.0651e-02, 5.5016e-06, 1.5741e-06,
        6.7932e-06, 1.2395e-05, 4.9141e-06, 3.6955e-06, 4.3543e-06, 7.7067e-06,
        4.1245e-07, 6.8242e-06, 8.9155e-06, 1.2949e-05, 1.9841e-06, 7.4818e-06,
        6.0510e-06, 3.0385e-06, 7.6752e-06, 9.9888e-06, 9.7864e-06, 2.6399e-06,
        7.5501e-06, 3.6360e-06, 1.8966e-05, 2.1858e-06, 7.9246e-06, 3.6977e-06,
        7.7501e-06, 1.0760e-05, 1.2395e-05, 8.5746e-06, 4.0850e-06, 1.0760e-05,
        5.2962e-06, 6.2438e-06, 6.9341e-06, 8.9155e-06, 5.1313e-06, 2.7355e-06,
        4.6716e-06, 6.1948e-06, 4.6716e-06, 2.3701e-06, 1.5480e-05, 4.8440e-06,
        1.2348e-06, 7.9246e-06, 2.5864e-06, 4.6716e-06, 7.5501e-06, 2.6399e-06,
        3.4518e-06, 8.0807e-06, 6.0039e-06, 2.6399e-06, 3.2008e-02, 5.7558e-06,
        7.1041e-06, 1.8445e-06, 7.4962e-06, 3.6410e-06, 3.0526e-06, 7.1153e-06,
        7.3839e-02, 1.4230e-06, 1.0117e-05, 4.4774e-06, 7.1041e-06, 1.4652e-05,
        1.0625e-05, 1.2239e-02, 4.2325e-06, 6.9341e-06, 2.4104e-06, 1.1939e-05,
        3.2361e-06, 6.1948e-06, 1.1109e-05, 3.6473e-06, 4.6716e-06, 6.7854e-06,
        3.6473e-06, 2.5963e-06, 1.0625e-05, 6.0039e-06, 7.6006e-06, 1.4806e-05,
        3.6360e-06, 5.6230e-06, 8.4248e-06, 4.8217e-06, 5.1674e-06, 5.6459e-06,
        6.6201e-06, 3.6955e-06, 6.4005e-06, 7.7501e-06, 1.0002e-05, 1.9841e-06,
        2.6399e-06, 1.1939e-05, 1.5570e-05, 4.4214e-06, 3.0671e-06, 4.8217e-06,
        8.4072e-06, 4.3037e-06, 9.8269e-06, 6.2438e-06, 9.0072e-06, 3.6955e-06,
        1.0135e-05, 9.0471e-06, 4.6716e-06, 1.4230e-06, 3.0671e-06, 5.7559e-06,
        3.8560e-06, 1.0624e-05, 3.0671e-06, 6.2668e-06, 6.0782e-06, 1.2395e-05,
        1.2348e-06, 1.4230e-06, 7.8825e-06, 3.2973e-06, 8.0051e-06, 4.6716e-06,
        6.6201e-06, 3.0671e-06, 5.9792e-06, 5.5453e-06, 3.6360e-06, 7.5636e-06,
        8.9155e-06, 6.3729e-06, 4.6716e-06, 2.6399e-06, 2.6399e-06, 6.9471e-06,
        3.4141e-06, 3.0671e-06, 6.2668e-06, 8.0051e-06, 6.1098e-06, 4.5708e-06,
        7.6752e-06, 1.1109e-05, 5.7558e-06, 1.9841e-06, 5.3556e-06, 1.4823e-05,
        5.7558e-06, 1.1018e-05, 1.1495e-05, 3.0671e-06, 3.8334e-06, 3.0851e-06,
        1.1687e-05, 2.6399e-06, 5.6128e-06, 5.9016e-06, 1.0161e-05, 7.3464e-06,
        8.8307e-06, 3.3000e-06, 1.0257e-05, 7.6752e-06, 8.5068e-06, 1.0622e-05,
        6.0624e-06, 2.6399e-06, 4.7348e-06, 5.7558e-06, 3.2601e-06, 4.2603e-06,
        1.4652e-05, 4.2103e-06, 7.5501e-06, 4.2633e-06, 1.1854e-05, 9.3228e-06,
        1.1939e-05, 7.7052e-06, 8.0022e-06, 1.1018e-05, 5.9686e-06, 6.1948e-06,
        4.8217e-06, 6.4709e-02, 6.1948e-06, 1.1854e-05, 4.8217e-06, 1.1459e-05,
        6.6201e-06, 6.4666e-06, 3.0043e-06, 5.6128e-06, 1.2121e-05, 7.5501e-06,
        5.3912e-06, 8.3955e-06, 3.6360e-06, 5.7558e-06, 1.2348e-06, 3.1805e-06,
        2.3701e-06, 9.3228e-06, 5.7558e-06, 5.3470e-06, 2.6399e-06, 7.3590e-06,
        2.6842e-06, 2.4104e-06, 5.6729e-02, 2.1201e-06, 3.6977e-06, 6.2668e-06,
        1.1109e-05, 6.1948e-06, 9.4063e-06, 1.4230e-06, 6.8241e-06, 6.2925e-06,
        7.6752e-06, 1.0723e-02, 6.8488e-06, 4.7348e-06, 1.0622e-05, 1.2037e-02,
        1.2395e-05, 1.1939e-05, 1.5851e-02, 9.5756e-06, 8.5068e-06, 1.1854e-05,
        7.8352e-06, 1.9685e-06, 1.2977e-05, 2.5963e-06, 7.6752e-06, 1.6683e-05,
        3.9993e-06, 1.0002e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.1403e-08, 2.6943e-07, 2.4855e-07, 2.6276e-07, 2.4855e-07, 2.1605e-07,
        1.7020e-07, 1.9137e-07, 1.1680e-07, 6.1851e-07, 2.2833e-07, 2.9637e-07,
        2.6944e-07, 2.6818e-07, 3.2367e-07, 3.1402e-08, 3.1174e-07, 1.0312e-07,
        3.1403e-08, 1.7416e-07, 1.4549e-07, 5.5924e-08, 4.3681e-07, 1.3229e-07,
        3.2654e-07, 2.5521e-07, 1.7597e-07, 1.3229e-07, 8.9979e-02, 1.1606e-01,
        1.1680e-07, 1.1680e-07, 1.4295e-07, 6.3914e-07, 1.6156e-07, 2.6943e-07,
        2.1605e-07, 1.7597e-07, 1.0312e-07, 3.1403e-08, 4.6099e-07, 3.1174e-07,
        1.2020e-07, 1.7020e-07, 6.3914e-07, 2.7181e-07, 1.7020e-07, 4.6098e-07,
        3.7259e-07, 2.1605e-07, 8.8596e-08, 4.5556e-07, 2.3642e-07, 1.1657e-07,
        2.4605e-07, 1.1657e-07, 2.0775e-07, 3.0654e-07, 3.5901e-07, 2.2003e-07,
        8.9918e-08, 2.4581e-07, 2.8835e-07, 3.1403e-08, 8.8596e-08, 2.0775e-07,
        4.0158e-07, 4.7243e-07, 9.9394e-02, 2.6943e-07, 1.1657e-07, 3.3629e-07,
        2.7299e-07, 2.5563e-07, 3.3214e-07, 1.3229e-07, 3.5293e-07, 3.5901e-07,
        8.8596e-08, 2.6944e-07, 1.0312e-07, 1.0312e-07, 3.2890e-07, 5.5924e-08,
        1.0312e-07, 1.6134e-01, 2.3642e-07, 9.3984e-02, 2.6763e-07, 2.6276e-07,
        3.2780e-07, 1.6156e-07, 1.6156e-07, 2.5521e-07, 1.7597e-07, 2.6301e-07,
        2.4534e-07, 2.5521e-07, 2.0775e-07, 2.4855e-07, 2.0775e-07, 8.8596e-08,
        3.3163e-07, 2.7224e-07, 1.7114e-07, 2.1605e-07, 8.9347e-02, 2.0775e-07,
        3.3155e-07, 1.7020e-07, 2.9054e-07, 1.6156e-07, 2.7299e-07, 1.2763e-07,
        1.8027e-07, 1.5362e-07, 6.1851e-07, 1.4295e-07, 1.8146e-07, 4.3807e-02,
        2.0775e-07, 3.1398e-08, 1.8864e-07, 1.7597e-07, 1.7597e-07, 1.9514e-07,
        4.9614e-07, 1.4700e-07, 5.3596e-07, 3.5901e-07, 3.9385e-07, 1.4700e-07,
        2.6943e-07, 9.0622e-02, 3.1403e-08, 1.1680e-07, 1.7597e-07, 2.6943e-07,
        2.1605e-07, 3.1174e-07, 1.4295e-07, 2.0775e-07, 1.9498e-07, 4.6098e-07,
        3.5901e-07, 4.7243e-07, 6.3914e-07, 1.1680e-07, 3.1174e-07, 3.5901e-07,
        2.0775e-07, 2.1605e-07, 1.8633e-07, 1.7020e-07, 2.0491e-07, 4.5123e-07,
        3.1174e-07, 6.1851e-07, 1.4295e-07, 5.1203e-07, 1.1657e-07, 1.9498e-07,
        2.6943e-07, 3.3297e-07, 3.1403e-08, 3.1174e-07, 5.5924e-08, 3.2654e-07,
        4.3409e-02, 5.1203e-07, 9.1162e-08, 1.1657e-07, 3.9385e-07, 2.5521e-07,
        2.2833e-07, 2.0775e-07, 2.5001e-07, 2.5520e-07, 1.9514e-07, 2.1605e-07,
        3.2056e-07, 8.8596e-08, 5.3989e-07, 2.4374e-07, 2.6799e-07, 1.0312e-07,
        8.8596e-08, 2.1566e-07, 2.6276e-07, 1.9498e-07, 5.5924e-08, 4.0158e-07,
        5.3222e-07, 1.7020e-07, 6.1851e-07, 2.4581e-07, 4.5123e-07, 1.9514e-07,
        2.0775e-07, 1.0856e-07, 2.0775e-07, 2.0775e-07, 5.5924e-08, 1.3938e-01,
        2.2833e-07, 1.1680e-07, 2.6301e-07, 3.0180e-07, 4.5123e-07, 5.3222e-07,
        1.4296e-07, 2.3642e-07, 4.3727e-07, 6.8569e-07, 1.7597e-07, 1.1657e-07,
        2.7452e-07, 1.7114e-07, 1.1680e-07, 1.5597e-07, 6.1851e-07, 2.0775e-07,
        6.2246e-02, 1.2020e-07, 1.9498e-07, 5.0278e-07, 8.3375e-07, 2.4534e-07,
        1.6156e-07, 1.2763e-07, 1.2020e-07, 1.1657e-07, 3.5901e-07, 2.0775e-07,
        6.1851e-07, 4.7243e-07, 4.6828e-07, 4.9614e-07, 1.0312e-07, 1.1680e-07,
        2.4855e-07, 1.9498e-07, 2.1605e-07, 1.8636e-07, 2.4063e-07, 2.0775e-07,
        4.6098e-07, 3.2729e-07, 1.2763e-07, 6.4440e-03, 5.9206e-03, 4.5123e-07,
        2.1605e-07, 1.7114e-07, 3.4252e-07, 2.1605e-07, 1.8633e-07, 2.4855e-07,
        1.4296e-07, 1.4031e-07, 1.8633e-07, 6.0151e-08, 4.5123e-07, 1.1680e-07,
        1.1657e-07, 2.4855e-07, 3.2654e-07, 1.1802e-01, 2.5507e-07, 4.9614e-07,
        4.7243e-07, 5.5924e-08, 5.4438e-07, 2.5507e-07, 6.3914e-07, 1.8114e-07,
        1.1657e-07, 5.1370e-07, 1.4075e-07, 4.5873e-07, 4.7243e-07, 5.1405e-07,
        2.6276e-07, 5.0526e-07, 1.8237e-07, 1.7020e-07, 2.1605e-07, 2.1605e-07,
        8.1467e-08, 2.7181e-07, 6.1851e-07, 1.2763e-07, 3.5508e-07, 5.1203e-07,
        8.9003e-08, 2.6301e-07, 6.1851e-07, 1.4623e-07, 2.1605e-07, 2.2041e-07,
        1.9514e-07, 1.0312e-07, 4.9614e-07, 1.1680e-07, 2.2626e-03, 2.7299e-07,
        4.5123e-07, 2.5395e-07, 1.0312e-07, 1.1680e-07, 3.2654e-07, 2.4855e-07,
        2.6278e-07, 2.1566e-07, 2.6943e-07, 3.0654e-07, 1.3229e-07, 4.7243e-07,
        3.1174e-07, 8.8596e-08, 5.5924e-08, 2.5521e-07, 1.2030e-01, 1.7020e-07,
        1.2763e-07, 2.4855e-07, 8.8596e-08, 1.1657e-07, 2.5521e-07, 5.1412e-07,
        1.3658e-01, 2.5507e-07, 2.1605e-07, 6.2704e-04, 1.1657e-07, 2.5507e-07,
        1.2763e-07, 5.7899e-02, 2.5521e-07, 2.1566e-07, 1.1680e-07, 1.1657e-07,
        1.1657e-07, 2.6301e-07, 2.6818e-07, 2.6276e-07, 1.1657e-07, 1.6156e-07,
        2.6943e-07, 4.5111e-07, 6.1851e-07, 3.1174e-07, 3.1174e-07, 4.2799e-07,
        3.2890e-07, 2.9637e-07, 3.3214e-07, 3.7804e-07, 2.3243e-07, 2.4605e-07,
        2.0775e-07, 1.4295e-07, 2.7299e-07, 8.9917e-08, 4.7242e-07, 1.0853e-07,
        2.6301e-07, 1.3229e-07, 2.6276e-07, 1.3229e-07, 1.1680e-07, 4.2799e-07,
        6.1851e-07, 2.4855e-07, 5.5923e-08, 4.5123e-07, 3.4630e-07, 4.7050e-07,
        1.1657e-07, 1.4412e-07, 5.3222e-07, 4.6283e-07, 3.1174e-07, 3.4751e-07,
        4.3125e-07, 4.7633e-07, 1.7020e-07, 2.0775e-07, 2.5507e-07, 7.0193e-07,
        1.1517e-07, 1.8633e-07, 6.9838e-07, 1.0661e-03, 8.8596e-08, 4.7243e-07,
        2.3855e-07, 1.2020e-07, 3.2890e-07, 1.3980e-07, 8.8596e-08, 2.1605e-07,
        6.3914e-07, 1.9498e-07, 8.8596e-08, 2.5507e-07, 1.2763e-07, 2.5158e-07,
        2.6717e-07, 5.6740e-07, 1.9498e-07, 2.4855e-07, 2.6276e-07, 3.0655e-07,
        4.5123e-07, 6.5747e-07, 2.3719e-07, 3.1403e-08, 2.5521e-07, 4.3124e-07,
        5.5924e-08, 5.5924e-08, 3.9385e-07, 2.6276e-07, 1.5589e-07, 2.4534e-07,
        2.0727e-07, 2.4605e-07, 1.5590e-07, 2.4695e-07, 1.2199e-07, 1.0312e-07,
        4.4054e-07, 4.0158e-07, 2.6944e-07, 5.1405e-07, 2.3719e-07, 1.1038e-03,
        2.4855e-07, 2.5507e-07, 3.4253e-07, 1.1680e-07, 3.5710e-07, 1.6157e-07,
        5.5924e-08, 3.0654e-07, 1.9514e-07, 2.6276e-07, 3.0543e-07, 4.6525e-07,
        2.6944e-07, 2.6276e-07, 4.6098e-07, 4.7243e-07, 4.5123e-07, 8.8596e-08,
        2.4277e-07, 4.0825e-02, 9.2415e-07, 4.9614e-07, 2.9656e-07, 1.9498e-07,
        3.0654e-07, 2.6944e-07, 3.2028e-07, 3.5901e-07, 1.5176e-07, 2.6644e-07,
        1.3229e-07, 5.5924e-08, 1.4412e-07, 2.0776e-07, 3.2654e-07, 1.6156e-07,
        4.3124e-07, 1.1680e-07, 1.2020e-07, 3.1403e-08, 4.8668e-07, 2.0775e-07,
        1.9514e-07, 3.1403e-08, 1.0128e-01, 1.2763e-07, 8.8596e-08, 2.0330e-07,
        3.5800e-07, 2.6943e-07, 8.8596e-08, 1.2020e-07, 2.5521e-07, 1.7597e-07,
        1.4412e-07, 1.5614e-01, 4.5123e-07, 2.6276e-07, 4.9482e-07, 8.6800e-02,
        2.5271e-07, 1.6779e-07, 1.2399e-01, 3.3214e-07, 6.8569e-07, 1.0312e-07,
        4.1181e-07, 3.9415e-07, 2.6943e-07, 6.1851e-07, 3.8631e-07, 2.1566e-07,
        2.4535e-07, 1.1657e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.8234e-05, 5.7588e-06, 8.3910e-06, 1.1740e-05, 5.7588e-06, 1.3395e-05,
        1.0514e-06, 8.2082e-06, 5.0113e-06, 8.8311e-06, 1.5519e-05, 5.6507e-06,
        1.5599e-05, 8.1948e-06, 1.1342e-05, 1.5519e-05, 5.6592e-06, 4.8358e-06,
        1.4154e-05, 5.7342e-06, 9.0377e-06, 4.4605e-06, 4.9820e-06, 6.3816e-06,
        6.8045e-06, 8.1575e-06, 1.0859e-05, 4.8796e-06, 5.0179e-06, 5.4246e-06,
        5.6088e-06, 7.7135e-06, 8.3910e-06, 9.8606e-06, 1.1300e-05, 6.0936e-06,
        5.0004e-06, 2.9522e-06, 7.8119e-06, 7.8155e-07, 3.8621e-06, 7.6016e-06,
        5.4018e-06, 3.8661e-06, 8.9139e-06, 8.6446e-06, 9.0039e-06, 1.4349e-05,
        1.8234e-05, 1.0250e-05, 1.0618e-05, 1.5599e-05, 7.2094e-06, 6.6030e-06,
        6.1886e-06, 1.0859e-05, 3.3857e-06, 9.7871e-06, 5.0179e-06, 9.6843e-06,
        5.2406e-06, 1.0031e-05, 9.2228e-06, 1.6397e-05, 1.4276e-05, 1.0700e-05,
        5.6592e-06, 5.7404e-06, 3.4754e-06, 5.4736e-06, 1.9554e-06, 1.3350e-05,
        8.0115e-06, 6.3388e-06, 8.2047e-06, 3.5341e-06, 1.3369e-05, 1.4663e-06,
        3.1870e-06, 4.0031e-06, 6.9146e-06, 4.2315e-06, 7.0386e-06, 1.2082e-05,
        3.8121e-06, 6.6030e-06, 6.0936e-06, 3.4916e-06, 1.4803e-05, 1.2228e-05,
        3.5236e-06, 5.6592e-06, 7.1882e-06, 8.9300e-06, 2.6847e-06, 9.0039e-06,
        1.1329e-05, 5.3842e-06, 1.9958e-05, 5.9064e-07, 8.5888e-06, 4.8379e-06,
        8.3460e-06, 9.8738e-06, 4.2315e-06, 2.9627e-07, 1.2260e-06, 1.6920e-05,
        1.4225e-05, 2.5584e-06, 3.1870e-06, 2.5584e-06, 1.6397e-05, 9.0605e-06,
        3.2515e-06, 3.7089e-06, 1.2228e-05, 2.0883e-06, 7.9781e-06, 7.1204e-06,
        1.3703e-05, 4.8796e-06, 2.3872e-06, 9.5261e-06, 4.7631e-06, 3.8197e-06,
        3.5236e-06, 8.9195e-06, 3.4916e-06, 7.1981e-06, 8.2464e-06, 4.5151e-06,
        3.7031e-06, 2.1416e-06, 6.3388e-06, 5.4246e-06, 7.2789e-06, 3.7089e-06,
        1.2974e-05, 1.1025e-05, 1.2228e-05, 3.8621e-06, 3.2361e-06, 7.8317e-06,
        8.1948e-06, 3.0782e-06, 6.1014e-06, 3.7031e-06, 7.9585e-06, 4.8358e-06,
        5.5801e-06, 2.2133e-06, 3.6694e-06, 9.0774e-06, 2.3518e-06, 7.5217e-06,
        5.6532e-06, 3.4382e-07, 8.2499e-06, 3.2515e-06, 4.2315e-06, 3.2307e-06,
        7.5573e-06, 7.0136e-06, 9.7103e-06, 3.3287e-06, 4.2315e-06, 5.8681e-06,
        7.8964e-06, 5.7404e-06, 1.0897e-05, 1.0618e-05, 8.4620e-06, 4.5555e-06,
        4.9891e-06, 1.1250e-05, 1.3426e-05, 4.2315e-06, 7.6053e-06, 8.2004e-06,
        1.1546e-05, 2.0365e-06, 1.2974e-05, 1.3703e-05, 5.4246e-06, 4.2328e-06,
        5.7588e-06, 4.8562e-06, 1.0759e-05, 7.6016e-06, 7.1593e-06, 6.3816e-06,
        1.5519e-05, 7.7135e-06, 5.2406e-06, 1.9554e-06, 2.2684e-06, 9.7391e-06,
        1.2310e-05, 5.4052e-06, 5.2142e-06, 3.0803e-06, 1.3395e-05, 5.4246e-06,
        8.8852e-06, 7.6016e-06, 1.1250e-05, 1.1702e-05, 5.6087e-06, 3.1870e-06,
        8.6303e-06, 9.0774e-06, 2.3872e-06, 4.2315e-06, 9.7178e-06, 1.0728e-05,
        7.6016e-06, 6.5042e-06, 1.7970e-05, 1.3395e-05, 7.1593e-06, 2.3908e-06,
        7.1593e-06, 8.8311e-06, 2.1715e-06, 3.1870e-06, 2.0556e-05, 1.6395e-05,
        1.4276e-05, 5.2406e-06, 7.9225e-06, 1.0859e-05, 1.0078e-05, 4.7222e-06,
        6.7755e-06, 1.4836e-05, 1.1317e-05, 8.1232e-06, 6.1068e-06, 2.8433e-06,
        4.5915e-06, 1.2894e-06, 3.1870e-06, 1.1762e-05, 3.4754e-06, 3.1870e-06,
        3.0486e-06, 5.0179e-06, 7.5348e-06, 7.4445e-06, 3.5341e-06, 3.4382e-06,
        5.5830e-06, 6.2166e-06, 3.5492e-06, 1.0281e-05, 3.9142e-06, 6.1886e-06,
        4.5555e-06, 9.3556e-06, 8.0690e-06, 1.5511e-05, 6.0936e-06, 6.2057e-06,
        4.6035e-06, 5.2142e-06, 3.5030e-06, 9.8086e-06, 6.2435e-06, 7.1882e-06,
        8.8981e-06, 6.6579e-06, 5.8067e-06, 1.0618e-05, 4.0053e-06, 2.4328e-06,
        5.0113e-06, 7.8033e-06, 8.6446e-06, 3.9531e-06, 3.4382e-06, 2.0883e-06,
        6.2577e-06, 1.2974e-05, 7.0327e-06, 1.5519e-05, 3.8121e-06, 5.2406e-06,
        8.1733e-06, 8.4701e-06, 5.6507e-06, 9.7391e-06, 1.1250e-05, 1.7695e-05,
        8.1948e-06, 5.4246e-06, 1.1265e-05, 6.0936e-06, 9.5616e-06, 2.9193e-06,
        4.3174e-07, 5.5030e-06, 1.1300e-05, 8.6446e-06, 1.6397e-05, 5.4736e-06,
        9.8952e-06, 3.2699e-06, 8.3957e-06, 9.6707e-06, 8.3910e-06, 3.7058e-06,
        5.5015e-06, 4.1067e-06, 2.2886e-06, 5.6943e-06, 4.2315e-06, 2.8962e-06,
        1.5511e-05, 8.9138e-06, 3.8602e-06, 7.7755e-06, 7.5121e-06, 3.7045e-06,
        3.3996e-06, 7.7755e-06, 9.7251e-06, 1.2247e-05, 6.0936e-06, 3.5341e-06,
        8.7115e-06, 6.0790e-06, 1.2570e-05, 1.7335e-06, 3.3996e-06, 5.4246e-06,
        8.3910e-06, 6.6579e-06, 3.7031e-06, 8.1232e-06, 1.2228e-05, 3.8621e-06,
        6.0936e-06, 8.4701e-06, 2.5899e-06, 3.7031e-06, 7.6016e-06, 8.6303e-06,
        6.3989e-06, 7.1576e-06, 3.2393e-06, 6.0209e-06, 5.5871e-06, 2.3304e-06,
        2.4643e-06, 7.1593e-06, 6.0120e-06, 8.9300e-06, 4.6149e-06, 7.6866e-06,
        3.8621e-06, 3.5341e-06, 5.3755e-06, 1.2971e-06, 2.0503e-06, 6.2197e-06,
        5.8031e-06, 1.3951e-06, 1.0618e-05, 1.2380e-05, 5.7588e-06, 4.5555e-06,
        9.6639e-06, 1.2228e-05, 4.5555e-06, 7.5886e-06, 8.1733e-06, 6.0120e-06,
        2.7056e-06, 3.8602e-06, 1.2697e-05, 1.2265e-05, 2.9522e-06, 6.6579e-06,
        5.2142e-06, 7.6866e-06, 6.3388e-06, 1.2380e-05, 1.0031e-05, 9.3652e-06,
        8.4077e-06, 4.5555e-06, 7.1852e-06, 6.7215e-06, 2.4424e-06, 1.5977e-05,
        4.3174e-07, 5.1532e-06, 6.9857e-06, 1.0474e-05, 6.0936e-06, 8.0115e-06,
        3.8602e-06, 5.0179e-06, 8.7115e-06, 6.0936e-06, 9.3652e-06, 8.6485e-06,
        3.4916e-06, 2.3872e-06, 1.7695e-05, 6.0936e-06, 9.8606e-06, 2.6847e-06,
        6.3189e-06, 3.2515e-06, 6.3235e-06, 1.2971e-06, 7.6016e-06, 8.1948e-06,
        8.1940e-06, 4.6012e-06, 4.8379e-06, 9.8606e-06, 1.2231e-05, 5.0813e-06,
        1.2974e-05, 1.1435e-05, 3.8602e-06, 6.1014e-06, 2.1400e-06, 2.7441e-06,
        3.8602e-06, 7.5221e-06, 6.3016e-06, 3.2515e-06, 4.3230e-06, 1.2056e-05,
        3.4916e-06, 3.4916e-06, 7.5348e-06, 7.1593e-06, 7.6899e-06, 7.1593e-06,
        5.4246e-06, 7.5347e-06, 6.6307e-06, 3.8496e-06, 6.2194e-06, 2.9522e-06,
        9.6843e-06, 1.1762e-05, 3.4916e-06, 9.0774e-06, 9.8952e-06, 6.7097e-06,
        2.8962e-06, 1.1294e-05, 6.0790e-06, 2.0556e-05, 1.2231e-05, 7.6077e-06,
        2.3583e-06, 5.7355e-06, 3.2360e-06, 9.7531e-06, 7.4445e-06, 1.6235e-05,
        5.7404e-06, 6.0790e-06, 2.5620e-06, 6.2886e-06, 4.6149e-06, 3.8496e-06,
        4.2315e-06, 1.0859e-05, 6.3388e-06, 6.1014e-06, 5.9186e-06, 6.0628e-06,
        3.7031e-06, 2.8433e-06, 7.7135e-06, 5.3260e-06, 4.2315e-06, 7.8993e-06,
        9.9559e-06, 1.1384e-05, 1.0031e-05, 7.8603e-06, 7.0386e-06, 2.5620e-06,
        3.8621e-06, 6.6030e-06, 8.9869e-06, 9.8762e-06, 8.3910e-06, 4.8796e-06,
        8.3910e-06, 8.5888e-06, 5.4736e-06, 1.3574e-05, 1.2228e-05, 5.3411e-06,
        7.4596e-06, 5.3411e-06, 1.0572e-05, 4.2315e-06, 7.6016e-06, 9.6843e-06,
        9.8606e-06, 5.0113e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0319e-05, 1.4779e-05, 2.8048e-05, 3.3339e-05, 3.8039e-05, 2.3949e-05,
        6.0486e-05, 4.5254e-06, 1.8358e-05, 2.9144e-05, 3.9782e-05, 1.5584e-05,
        2.7002e-05, 2.7412e-05, 3.1069e-05, 1.2215e-05, 2.1530e-05, 1.8280e-05,
        2.3477e-05, 5.9669e-05, 4.5084e-05, 7.3682e-06, 6.7269e-06, 2.4656e-05,
        2.3584e-05, 3.8216e-05, 1.1673e-05, 4.8165e-05, 4.9734e-07, 9.0739e-06,
        5.3233e-05, 1.5706e-05, 1.2723e-05, 3.1167e-05, 2.8317e-05, 3.9557e-05,
        3.0362e-05, 3.3786e-05, 3.2488e-05, 2.9147e-05, 1.7426e-05, 2.6469e-05,
        3.4679e-05, 1.2428e-05, 2.8189e-05, 4.2666e-05, 6.2126e-05, 2.2314e-05,
        2.8572e-05, 2.7498e-05, 2.4514e-05, 1.4955e-05, 1.6562e-05, 1.4454e-05,
        3.5206e-05, 3.5665e-05, 4.2631e-05, 3.3438e-05, 4.3123e-05, 3.2240e-05,
        1.5126e-05, 1.9315e-05, 3.8899e-05, 3.0054e-05, 2.8774e-05, 2.1699e-05,
        4.3732e-05, 4.1607e-05, 2.3858e-05, 2.2371e-05, 4.8819e-06, 1.2426e-05,
        3.7073e-05, 2.2910e-05, 2.4296e-05, 2.0720e-05, 2.6468e-05, 2.9297e-05,
        3.9382e-05, 3.3474e-05, 3.1793e-05, 4.0226e-05, 6.0605e-05, 2.0947e-05,
        3.2095e-05, 2.5487e-05, 1.5474e-05, 1.8386e-05, 4.3227e-05, 2.8510e-05,
        3.2046e-05, 4.9637e-05, 3.3130e-05, 3.1556e-05, 3.3769e-05, 2.9153e-05,
        3.7757e-05, 1.3561e-05, 1.0271e-05, 2.5133e-05, 1.9484e-05, 3.4012e-05,
        2.3791e-05, 1.5699e-05, 2.7110e-05, 4.3227e-05, 3.6086e-05, 1.5408e-05,
        1.0817e-05, 2.2631e-05, 1.4880e-05, 3.9955e-05, 6.0055e-06, 8.2849e-06,
        2.2586e-05, 4.5851e-05, 3.4746e-05, 2.6160e-05, 3.9972e-05, 3.2269e-05,
        9.6577e-06, 3.7537e-05, 2.2404e-05, 2.4514e-05, 1.5714e-05, 1.7055e-05,
        2.8879e-05, 3.3744e-05, 5.2448e-05, 1.7853e-05, 2.4434e-05, 2.1549e-05,
        3.7537e-05, 3.1556e-05, 3.9689e-05, 2.8734e-05, 2.8276e-05, 1.4822e-05,
        1.9810e-05, 1.2716e-05, 1.8224e-05, 1.7080e-05, 2.3061e-05, 5.5656e-06,
        1.6273e-05, 3.7559e-05, 2.1457e-05, 3.3270e-05, 8.1620e-06, 1.6813e-05,
        1.2563e-05, 1.7131e-05, 2.8993e-05, 1.8690e-05, 2.1052e-05, 4.9351e-05,
        2.4462e-05, 9.7172e-06, 3.5372e-05, 5.7319e-05, 1.2674e-05, 2.4342e-05,
        1.4903e-05, 1.7196e-05, 7.1171e-06, 3.2396e-05, 9.9985e-06, 1.7588e-05,
        3.9972e-05, 1.2094e-05, 2.3248e-05, 2.9162e-05, 3.6253e-05, 9.6895e-06,
        3.3450e-05, 1.7082e-05, 2.4147e-05, 2.1121e-05, 2.8025e-05, 1.2661e-05,
        2.5383e-05, 3.0303e-05, 2.3764e-05, 1.7081e-05, 2.9827e-05, 2.0925e-05,
        1.9539e-05, 1.1573e-05, 3.5880e-05, 2.3538e-05, 7.9932e-06, 2.2149e-05,
        2.3812e-05, 1.8394e-05, 3.1574e-05, 4.3101e-05, 2.3088e-05, 1.7005e-05,
        2.7036e-05, 3.8784e-05, 4.2261e-05, 3.7948e-05, 2.0988e-05, 2.6469e-05,
        2.2281e-05, 2.4434e-05, 2.7809e-05, 9.2884e-06, 2.4669e-05, 2.9240e-05,
        9.0739e-06, 1.4879e-05, 5.4231e-05, 9.6608e-06, 1.0389e-05, 4.4784e-05,
        2.1349e-05, 4.0644e-05, 1.0170e-05, 3.7545e-05, 2.8734e-05, 4.1613e-05,
        3.7995e-05, 4.4345e-05, 1.3016e-05, 1.2192e-05, 1.5985e-05, 2.6044e-05,
        2.9716e-05, 1.7711e-05, 3.2523e-05, 1.7341e-05, 3.5943e-05, 2.5968e-05,
        4.1693e-05, 6.2872e-06, 1.2607e-05, 3.0956e-05, 5.5657e-06, 1.3139e-05,
        2.1552e-05, 2.7297e-05, 2.3728e-05, 2.9686e-05, 1.8224e-05, 1.7896e-05,
        2.6468e-05, 1.9484e-05, 9.7046e-06, 2.0079e-05, 1.7270e-05, 2.7689e-05,
        2.2660e-05, 1.5480e-05, 1.6426e-05, 2.8816e-05, 1.9063e-05, 2.8069e-05,
        2.5695e-05, 3.3742e-05, 2.7626e-05, 2.3343e-05, 4.3883e-05, 3.4026e-05,
        2.5499e-05, 3.1705e-05, 3.8302e-05, 1.7907e-05, 3.4424e-05, 1.7050e-05,
        2.2141e-05, 1.7372e-05, 6.5465e-05, 1.7644e-05, 2.0947e-05, 4.3184e-05,
        1.4019e-05, 2.4669e-05, 3.2044e-05, 1.8285e-05, 4.6280e-05, 9.6577e-06,
        1.9071e-05, 3.2766e-05, 1.1230e-05, 1.9513e-05, 5.2161e-05, 2.4514e-05,
        3.5943e-05, 2.9147e-05, 1.2298e-05, 2.8388e-05, 1.5893e-05, 3.6295e-05,
        9.2230e-06, 3.9935e-05, 3.2887e-05, 5.5156e-06, 1.9071e-05, 2.2631e-05,
        1.9929e-05, 2.7080e-05, 4.0449e-05, 2.1987e-05, 3.1330e-05, 3.6086e-05,
        1.6507e-05, 3.3020e-05, 2.4325e-05, 3.3174e-05, 3.9900e-05, 2.1521e-05,
        3.5943e-05, 1.2788e-05, 1.8654e-05, 3.6086e-05, 4.5870e-05, 2.4233e-05,
        1.8168e-05, 3.5157e-05, 2.7498e-05, 1.7920e-05, 1.1482e-05, 1.2716e-05,
        2.1421e-05, 4.6017e-05, 1.7079e-05, 8.2256e-06, 2.2910e-05, 6.8840e-05,
        1.8850e-05, 1.8250e-05, 4.2872e-05, 2.1549e-05, 3.8302e-05, 3.1361e-05,
        2.8595e-05, 3.4518e-05, 4.4414e-05, 2.8346e-05, 2.4615e-05, 2.4607e-05,
        2.4737e-05, 1.0861e-05, 2.8048e-05, 3.5926e-05, 6.3537e-05, 1.9762e-05,
        1.4367e-05, 2.0247e-05, 2.8067e-05, 3.3018e-05, 3.0399e-06, 2.1786e-05,
        3.2557e-05, 6.2872e-06, 7.7634e-06, 3.3138e-05, 3.3266e-05, 3.5943e-05,
        1.2716e-05, 1.7889e-05, 1.0218e-05, 4.5129e-05, 3.1798e-05, 2.3446e-05,
        4.1928e-05, 1.3081e-05, 3.2103e-05, 4.1972e-05, 2.7585e-05, 3.7559e-05,
        2.8394e-05, 5.3744e-05, 2.3535e-05, 6.0427e-05, 4.3984e-05, 3.6381e-05,
        1.2451e-05, 3.4805e-05, 3.4892e-05, 1.3182e-05, 2.5173e-05, 1.8553e-05,
        3.0540e-05, 1.6074e-05, 2.7936e-05, 2.2129e-05, 1.9619e-05, 3.0201e-05,
        3.7516e-05, 1.9397e-05, 4.3471e-05, 1.4143e-05, 2.4514e-05, 2.8634e-05,
        2.0008e-05, 4.5137e-05, 1.5122e-05, 2.7129e-05, 4.4231e-05, 3.1041e-05,
        3.6389e-05, 2.9613e-05, 2.7958e-05, 8.1255e-05, 3.4292e-05, 1.0981e-05,
        3.4012e-05, 3.5326e-05, 3.9451e-05, 7.7634e-06, 1.2192e-05, 6.0055e-06,
        2.6957e-05, 1.5855e-05, 2.0473e-05, 2.3476e-05, 2.4134e-05, 1.2936e-05,
        1.9619e-05, 2.9655e-05, 1.3894e-05, 4.2434e-05, 3.3688e-05, 3.1331e-05,
        1.2985e-05, 2.4572e-05, 2.6149e-05, 4.7479e-05, 9.4241e-06, 1.9619e-05,
        3.9006e-05, 2.5290e-05, 1.8907e-05, 1.4514e-05, 7.6847e-06, 2.4004e-05,
        2.7990e-05, 1.0051e-05, 3.0510e-05, 2.7300e-05, 1.9259e-05, 5.5656e-06,
        3.1705e-05, 2.3949e-05, 1.8726e-05, 4.7147e-05, 2.3812e-05, 2.7498e-05,
        1.3311e-05, 3.4619e-05, 1.0389e-05, 3.5109e-05, 2.7994e-05, 1.5042e-05,
        4.9239e-05, 4.7098e-05, 2.2831e-05, 1.9827e-05, 3.9589e-05, 3.9709e-05,
        1.9531e-05, 2.0129e-05, 2.3675e-05, 2.6468e-05, 1.7817e-05, 1.8059e-05,
        3.3152e-05, 3.9557e-05, 4.1810e-05, 2.7184e-05, 3.5663e-05, 4.8728e-05,
        5.2185e-05, 4.7038e-05, 1.0410e-05, 3.3438e-05, 2.4192e-05, 3.3339e-05,
        4.6325e-06, 4.6703e-06, 1.0874e-05, 8.1749e-06, 2.7781e-05, 2.2944e-05,
        3.6182e-05, 2.3219e-05, 2.8363e-05, 3.1707e-05, 1.7725e-05, 2.1987e-05,
        4.0511e-05, 4.5178e-06, 1.3800e-05, 1.9531e-05, 2.0116e-05, 3.9746e-06,
        5.4888e-05, 5.1226e-05, 1.3983e-05, 3.0804e-05, 3.9943e-05, 2.7781e-05,
        3.5659e-05, 1.1673e-05, 1.4857e-05, 3.5101e-05, 1.1408e-05, 2.7300e-05,
        5.5657e-06, 4.1167e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2439, 1.3206, 1.0410, 1.2001, 1.2567, 1.1056, 1.2017, 1.2385, 1.3072,
        1.3176, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217, 0.0217,
        0.0217], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03703703731298447, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6892361044883728, 0.6875, 1.0, 0.6892361044883728, 1.0, 1.0, 0.6892361044883728, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6892361044883728, 0.6875, 0.6892361044883728, 0.6875, 0.6892361044883728, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6892361044883728, 0.6875, 1.0, 0.6875, 0.6875, 0.6892361044883728, 0.6892361044883728, 1.0, 0.6875, 1.0, 0.6892361044883728, 0.6892361044883728, 0.694444477558136, 0.6875, 0.6875, 0.6927083134651184, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6909722089767456, 1.0, 1.0, 0.6892361044883728, 0.6892361044883728, 0.6875, 1.0, 0.6909722089767456, 0.6927083134651184, 0.6927083134651184, 0.6909722089767456, 0.6961805820465088, 1.0, 0.6875, 1.0, 0.6909722089767456, 1.0, 0.6892361044883728]

 sparsity of   [1.0, 1.0, 0.3333333432674408, 0.3298611044883728, 0.3298611044883728, 0.3298611044883728, 0.34375, 0.3819444477558136, 0.3350694477558136, 0.328125, 0.3298611044883728, 0.3368055522441864, 0.3298611044883728, 0.331597238779068, 0.3385416567325592, 0.3333333432674408, 0.3333333432674408, 0.331597238779068, 0.3298611044883728, 0.3298611044883728, 0.3333333432674408, 1.0, 1.0, 0.328125, 1.0, 0.3298611044883728, 0.3385416567325592, 0.331597238779068, 0.3368055522441864, 0.328125, 0.3333333432674408, 1.0, 0.3402777910232544, 0.3385416567325592, 0.3298611044883728, 0.328125, 0.3368055522441864, 0.331597238779068, 0.3350694477558136, 0.3420138955116272, 0.3333333432674408, 0.3559027910232544, 0.328125, 0.331597238779068, 0.3298611044883728, 0.331597238779068, 0.331597238779068, 1.0, 0.3385416567325592, 0.328125, 1.0, 1.0, 0.3368055522441864, 0.3298611044883728, 0.3489583432674408, 0.331597238779068, 0.3298611044883728, 0.3298611044883728, 0.328125, 0.328125, 0.331597238779068, 0.331597238779068, 0.331597238779068, 0.3298611044883728]

 sparsity of   [0.1458333283662796, 1.0, 1.0, 0.142361119389534, 1.0, 0.1458333283662796, 1.0, 0.1440972238779068, 1.0, 0.1510416716337204, 0.142361119389534, 0.1440972238779068, 0.1475694477558136, 0.1510416716337204, 0.142361119389534, 0.1458333283662796, 1.0, 0.1770833283662796, 1.0, 0.1458333283662796, 1.0, 1.0, 1.0, 0.1458333283662796, 0.142361119389534, 0.142361119389534, 0.142361119389534, 1.0, 0.1440972238779068, 1.0, 0.1475694477558136, 0.142361119389534, 1.0, 0.1458333283662796, 0.1475694477558136, 0.1458333283662796, 0.142361119389534, 1.0, 1.0, 1.0, 0.1597222238779068, 1.0, 1.0, 0.1493055522441864, 1.0, 0.1440972238779068, 0.140625, 0.1493055522441864, 0.1475694477558136, 1.0, 1.0, 0.1493055522441864, 0.1475694477558136, 0.140625, 0.1458333283662796, 0.1458333283662796, 1.0, 1.0, 0.142361119389534, 0.1440972238779068, 0.140625, 1.0, 0.1510416716337204, 0.142361119389534]

 sparsity of   [0.3715277910232544, 0.3541666567325592, 0.3697916567325592, 0.3767361044883728, 0.3854166567325592, 0.359375, 0.3871527910232544, 0.4166666567325592, 0.3975694477558136, 0.347222238779068, 0.3871527910232544, 0.3611111044883728, 0.3836805522441864, 0.394097238779068, 0.3923611044883728, 0.3559027910232544, 0.3645833432674408, 0.3645833432674408, 0.3697916567325592, 0.3732638955116272, 0.390625, 0.3715277910232544, 0.3524305522441864, 0.3871527910232544, 0.3541666567325592, 0.3958333432674408, 0.362847238779068, 0.3819444477558136, 0.4357638955116272, 0.3506944477558136, 0.3697916567325592, 0.3680555522441864, 0.3993055522441864, 0.3732638955116272, 0.3680555522441864, 0.3506944477558136, 0.3697916567325592, 0.359375, 0.359375, 1.0, 0.3767361044883728, 0.3802083432674408, 0.3958333432674408, 0.3836805522441864, 0.3697916567325592, 0.3697916567325592, 0.3836805522441864, 0.3663194477558136, 0.390625, 0.390625, 0.3576388955116272, 0.3576388955116272, 0.3836805522441864, 0.359375, 0.3680555522441864, 0.3958333432674408, 0.3611111044883728, 0.3697916567325592, 0.3715277910232544, 0.378472238779068, 0.3680555522441864, 0.3819444477558136, 0.378472238779068, 0.3697916567325592]

 sparsity of   [0.0069444444961845875, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0086805559694767, 1.0, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.0, 0.010416666977107525, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.02083333395421505, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 0.0086805559694767, 0.0, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0052083334885537624, 0.010416666977107525, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.0, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0069444444961845875, 0.0052083334885537624, 0.0017361111240461469, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.010416666977107525, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0086805559694767, 0.0, 0.0173611119389534, 0.01215277798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.013888888992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 1.0]

 sparsity of   [0.046875, 0.0607638880610466, 0.9609375, 0.0477430559694767, 0.0512152798473835, 0.0486111119389534, 0.0451388880610466, 0.0529513880610466, 0.0460069440305233, 0.2456597238779068, 0.0494791679084301, 0.0451388880610466, 0.0477430559694767, 0.0451388880610466, 0.0529513880610466, 0.2005208283662796, 0.1605902761220932, 0.0477430559694767, 0.0659722238779068, 0.0512152798473835, 0.0494791679084301, 0.0512152798473835, 0.0503472238779068, 0.0572916679084301, 0.046875, 0.0477430559694767, 0.0477430559694767, 1.0, 0.0546875, 0.0442708320915699, 0.0529513880610466, 0.046875, 0.0442708320915699, 1.0, 0.0477430559694767, 0.0486111119389534, 0.0486111119389534, 0.046875, 0.046875, 0.046875, 0.1223958358168602, 0.2595486044883728, 0.0503472238779068, 0.0494791679084301, 0.0442708320915699, 0.0503472238779068, 0.0486111119389534, 0.0512152798473835, 0.0460069440305233, 0.0477430559694767, 0.487847238779068, 0.0494791679084301, 1.0, 0.0425347238779068, 0.0538194440305233, 0.0460069440305233, 0.0451388880610466, 0.0572916679084301, 0.0460069440305233, 0.0494791679084301, 0.0503472238779068, 0.0486111119389534, 0.0477430559694767, 0.2916666567325592, 0.0503472238779068, 0.0477430559694767, 1.0, 0.0512152798473835, 0.0486111119389534, 0.0486111119389534, 0.0460069440305233, 0.0503472238779068, 0.0477430559694767, 0.0503472238779068, 0.0477430559694767, 0.0451388880610466, 0.046875, 0.046875, 0.0486111119389534, 0.0460069440305233, 0.0850694477558136, 0.0503472238779068, 0.0546875, 0.0460069440305233, 0.0520833320915699, 0.0659722238779068, 0.0503472238779068, 0.0503472238779068, 0.046875, 0.125, 0.046875, 0.046875, 0.1953125, 0.0503472238779068, 0.0520833320915699, 0.0477430559694767, 0.0546875, 0.0451388880610466, 0.0494791679084301, 0.0442708320915699, 0.0477430559694767, 0.0477430559694767, 0.0460069440305233, 0.0529513880610466, 0.0451388880610466, 0.0442708320915699, 0.0529513880610466, 0.0538194440305233, 0.0442708320915699, 0.0642361119389534, 0.0477430559694767, 0.0477430559694767, 0.0477430559694767, 0.0460069440305233, 0.0555555559694767, 0.0598958320915699, 0.0494791679084301, 0.0494791679084301, 0.046875, 0.0529513880610466, 0.0477430559694767, 0.0494791679084301, 0.0486111119389534, 0.046875, 0.0451388880610466, 0.0520833320915699, 0.0512152798473835, 0.0512152798473835]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 1.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.02690972201526165, 0.0329861119389534, 1.0, 0.02864583395421505, 0.02690972201526165, 0.02864583395421505, 1.0, 1.0, 1.0, 0.02951388992369175, 0.02690972201526165, 0.02951388992369175, 0.0321180559694767, 1.0, 0.02690972201526165, 0.02864583395421505, 1.0, 0.02690972201526165, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 1.0, 0.0321180559694767, 1.0, 0.02864583395421505, 0.02777777798473835, 0.0243055559694767, 0.0251736119389534, 1.0, 0.0303819440305233, 0.0303819440305233, 0.02690972201526165, 1.0, 1.0, 1.0, 0.02777777798473835, 0.02864583395421505, 0.02777777798473835, 0.02864583395421505, 0.02864583395421505, 0.02777777798473835, 0.02690972201526165, 0.02864583395421505, 0.0251736119389534, 0.0251736119389534, 0.0243055559694767, 1.0, 0.02604166604578495, 0.0347222238779068, 0.02951388992369175, 0.02604166604578495, 1.0, 0.02777777798473835, 0.02690972201526165, 0.0251736119389534, 0.02690972201526165, 0.0381944440305233, 1.0, 0.02604166604578495, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 0.02864583395421505, 0.02690972201526165, 1.0, 1.0, 1.0, 1.0, 1.0, 0.02690972201526165, 1.0, 1.0, 0.02951388992369175, 0.02864583395421505, 1.0, 0.02690972201526165, 0.02951388992369175, 1.0, 0.02690972201526165, 0.0529513880610466, 0.0243055559694767, 0.02864583395421505, 1.0, 0.02864583395421505, 1.0, 0.02604166604578495, 0.0303819440305233, 1.0, 0.02690972201526165, 0.02951388992369175, 0.02690972201526165, 1.0, 0.0321180559694767, 0.0303819440305233, 1.0, 0.0303819440305233, 0.02864583395421505, 0.0303819440305233, 0.02690972201526165, 0.0329861119389534, 0.0303819440305233, 0.0329861119389534, 1.0, 0.02777777798473835, 1.0, 0.02864583395421505, 1.0, 1.0, 0.02604166604578495, 0.03125, 0.0338541679084301, 0.02777777798473835, 0.03125, 1.0, 0.02951388992369175, 0.0251736119389534, 1.0, 1.0, 0.02951388992369175, 0.02777777798473835, 0.02777777798473835, 1.0, 1.0, 0.0251736119389534, 0.0234375]

 sparsity of   [0.3368055522441864, 0.331597238779068, 0.3307291567325592, 0.3602430522441864, 0.3298611044883728, 0.3576388955116272, 0.3359375, 0.3689236044883728, 0.3324652910232544, 0.3793402910232544, 0.3324652910232544, 0.3298611044883728, 0.3385416567325592, 0.3298611044883728, 0.3324652910232544, 0.4826388955116272, 0.375, 0.3333333432674408, 0.3645833432674408, 0.3350694477558136, 0.3359375, 0.4053819477558136, 0.3333333432674408, 0.4131944477558136, 0.3342013955116272, 0.378472238779068, 0.362847238779068, 0.3298611044883728, 0.4817708432674408, 0.3932291567325592, 0.3333333432674408, 0.3385416567325592, 0.3359375, 0.3289930522441864, 0.3342013955116272, 0.3350694477558136, 0.331597238779068, 0.3411458432674408, 0.3324652910232544, 0.331597238779068, 0.3914930522441864, 1.0, 0.3368055522441864, 0.3333333432674408, 0.3333333432674408, 0.5824652910232544, 0.331597238779068, 0.331597238779068, 0.3350694477558136, 0.3567708432674408, 0.3645833432674408, 0.4496527910232544, 0.3350694477558136, 0.3307291567325592, 0.3307291567325592, 0.3307291567325592, 0.3342013955116272, 0.3342013955116272, 0.3350694477558136, 0.3350694477558136, 0.3359375, 0.3993055522441864, 0.331597238779068, 0.3368055522441864, 0.3333333432674408, 0.3298611044883728, 0.3324652910232544, 0.3307291567325592, 0.4861111044883728, 0.448784738779068, 1.0, 0.9947916865348816, 0.3376736044883728, 0.3307291567325592, 0.3333333432674408, 0.3350694477558136, 0.3324652910232544, 0.3359375, 0.339409738779068, 0.3723958432674408, 0.3350694477558136, 0.3333333432674408, 0.331597238779068, 0.3368055522441864, 0.3385416567325592, 0.3342013955116272, 0.331597238779068, 0.3324652910232544, 0.3498263955116272, 0.4114583432674408, 0.3359375, 0.3723958432674408, 0.440972238779068, 0.3333333432674408, 1.0, 0.3376736044883728, 0.331597238779068, 0.3350694477558136, 0.3298611044883728, 0.5052083134651184, 0.3758680522441864, 0.331597238779068, 0.3376736044883728, 0.3359375, 0.3428819477558136, 0.3307291567325592, 0.3611111044883728, 0.34375, 0.4930555522441864, 0.3333333432674408, 0.3324652910232544, 0.4670138955116272, 0.3359375, 0.3376736044883728, 0.3298611044883728, 0.331597238779068, 0.409722238779068, 0.3298611044883728, 0.3680555522441864, 0.3342013955116272, 0.3333333432674408, 0.3342013955116272, 0.3289930522441864, 0.3715277910232544, 0.355034738779068, 0.472222238779068, 0.3289930522441864, 0.3333333432674408]

 sparsity of   [0.0034722222480922937, 0.0069444444961845875, 0.0078125, 1.0, 1.0, 0.0078125, 0.0052083334885537624, 0.00434027798473835, 0.013020833022892475, 1.0, 0.0052083334885537624, 0.00434027798473835, 0.01128472201526165, 1.0, 0.0078125, 0.009548611007630825, 0.0069444444961845875, 1.0, 0.006076388992369175, 0.0086805559694767, 0.0078125, 0.0034722222480922937, 0.0078125, 1.0, 0.00434027798473835, 0.0052083334885537624, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.00434027798473835, 1.0, 0.0034722222480922937, 0.0026041667442768812, 0.0034722222480922937, 0.0026041667442768812, 0.0052083334885537624, 0.0069444444961845875, 0.006076388992369175, 0.0086805559694767, 0.0052083334885537624, 0.0086805559694767, 0.006076388992369175, 0.006076388992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 1.0, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.006076388992369175, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0078125, 0.0008680555620230734, 0.0086805559694767, 0.009548611007630825, 0.0052083334885537624, 1.0, 0.0086805559694767, 0.0052083334885537624, 0.0078125, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 0.009548611007630825, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.006076388992369175, 0.0034722222480922937, 0.006076388992369175, 1.0, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.009548611007630825, 0.00434027798473835, 0.00434027798473835, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0086805559694767, 0.006076388992369175, 0.0052083334885537624, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.006076388992369175, 0.0069444444961845875, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0078125, 1.0, 0.00434027798473835, 1.0, 0.0034722222480922937, 0.0026041667442768812, 0.009548611007630825, 0.00434027798473835, 0.0052083334885537624, 0.0078125, 0.0086805559694767, 1.0, 0.0052083334885537624, 0.015625, 0.0052083334885537624, 0.00434027798473835, 0.00434027798473835, 0.0069444444961845875, 0.00434027798473835, 0.0017361111240461469, 0.0034722222480922937, 1.0, 0.0017361111240461469, 0.0078125, 1.0, 0.01128472201526165, 0.0026041667442768812, 1.0, 0.0034722222480922937, 0.010416666977107525, 0.009548611007630825, 0.0086805559694767, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0086805559694767, 0.0078125, 0.0034722222480922937, 1.0, 0.0078125, 0.006076388992369175, 0.009548611007630825, 0.0026041667442768812, 0.010416666977107525, 0.009548611007630825, 0.006076388992369175, 0.00434027798473835, 0.0069444444961845875, 0.0052083334885537624, 0.006076388992369175, 1.0, 0.00434027798473835, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.006076388992369175, 0.006076388992369175, 0.0069444444961845875, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0078125, 0.0078125, 0.00434027798473835, 0.00434027798473835, 0.0052083334885537624, 1.0, 1.0, 0.0069444444961845875, 0.00434027798473835, 0.0086805559694767, 0.0069444444961845875, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.02777777798473835, 0.0052083334885537624, 0.0069444444961845875, 0.0069444444961845875, 0.006076388992369175, 0.0034722222480922937, 0.0008680555620230734, 0.0052083334885537624, 0.0052083334885537624, 0.0078125, 1.0, 1.0, 0.006076388992369175, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175, 0.006076388992369175, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 1.0, 0.0069444444961845875, 0.0086805559694767, 0.0086805559694767, 0.0052083334885537624, 0.009548611007630825, 1.0, 0.013888888992369175, 0.0026041667442768812, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.006076388992369175, 0.0034722222480922937, 0.0008680555620230734, 0.0052083334885537624, 0.0034722222480922937, 0.006076388992369175, 0.00434027798473835, 0.0078125, 0.0078125, 1.0, 0.0052083334885537624, 0.0078125, 0.009548611007630825, 0.00434027798473835, 1.0, 1.0, 1.0, 0.00434027798473835, 0.0078125, 0.0069444444961845875, 0.0034722222480922937, 0.0078125, 0.0034722222480922937, 0.0069444444961845875, 0.00434027798473835, 0.006076388992369175, 1.0, 1.0, 0.00434027798473835, 0.0078125, 0.0078125, 0.0052083334885537624, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 0.006076388992369175, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0052083334885537624, 1.0]

 sparsity of   [1.0, 0.154079869389534, 0.1419270783662796, 1.0, 0.1388888955116272, 1.0, 1.0, 0.1388888955116272, 0.1414930522441864, 0.1410590261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1414930522441864, 1.0, 0.142361119389534, 1.0, 0.1480034738779068, 1.0, 1.0, 1.0, 0.1375868022441864, 0.1393229216337204, 1.0, 0.1401909738779068, 1.0, 0.1388888955116272, 0.1401909738779068, 0.1427951455116272, 0.1410590261220932, 1.0, 0.1397569477558136, 1.0, 0.138454869389534, 0.1401909738779068, 0.1397569477558136, 1.0, 1.0, 0.1427951455116272, 0.13671875, 0.1410590261220932, 0.1410590261220932, 0.1397569477558136, 0.1380208283662796, 0.1549479216337204, 0.1388888955116272, 0.142361119389534, 0.1397569477558136, 1.0, 0.1401909738779068, 0.1401909738779068, 0.1362847238779068, 0.1440972238779068, 1.0, 0.1375868022441864, 1.0, 1.0, 1.0, 0.1427951455116272, 1.0, 0.14453125, 0.1393229216337204, 1.0, 1.0, 0.1375868022441864, 0.1393229216337204, 1.0, 0.142361119389534, 0.1401909738779068, 0.138454869389534, 0.138454869389534, 0.1401909738779068, 1.0, 1.0, 0.1414930522441864, 0.1388888955116272, 1.0, 1.0, 0.142361119389534, 1.0, 1.0, 1.0, 1.0, 0.1427951455116272, 1.0, 0.138454869389534, 0.1375868022441864, 1.0, 0.138454869389534, 0.1401909738779068, 1.0, 1.0, 1.0, 0.14453125, 0.1397569477558136, 1.0, 0.1397569477558136, 1.0, 0.1388888955116272, 0.1475694477558136, 0.138454869389534, 1.0, 0.140625, 1.0, 0.1419270783662796, 1.0, 1.0, 0.142361119389534, 1.0, 1.0, 0.14453125, 1.0, 0.146267369389534, 0.1458333283662796, 0.140625, 1.0, 0.1414930522441864, 0.1388888955116272, 0.1375868022441864, 0.1401909738779068, 0.1375868022441864, 0.1419270783662796, 1.0, 1.0, 0.1375868022441864, 1.0, 0.1401909738779068, 0.142361119389534, 0.1414930522441864, 1.0, 0.1375868022441864, 1.0, 0.1401909738779068, 0.1393229216337204, 1.0, 1.0, 1.0, 0.1432291716337204, 1.0, 0.1401909738779068, 1.0, 1.0, 1.0, 0.1388888955116272, 0.1388888955116272, 0.1375868022441864, 0.1410590261220932, 1.0, 0.138454869389534, 0.1388888955116272, 1.0, 1.0, 0.1401909738779068, 0.142361119389534, 0.140625, 1.0, 1.0, 0.1401909738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1371527761220932, 0.1393229216337204, 1.0, 1.0, 1.0, 0.1371527761220932, 0.1410590261220932, 0.1362847238779068, 1.0, 1.0, 0.1375868022441864, 1.0, 0.1453993022441864, 0.13671875, 0.13671875, 0.142361119389534, 0.1393229216337204, 0.1410590261220932, 0.1380208283662796, 1.0, 1.0, 0.138454869389534, 0.138454869389534, 0.1375868022441864, 0.1358506977558136, 1.0, 1.0, 1.0, 0.1393229216337204, 1.0, 0.1393229216337204, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1397569477558136, 1.0, 0.138454869389534, 0.1388888955116272, 1.0, 1.0, 0.1453993022441864, 1.0, 0.1397569477558136, 0.1397569477558136, 0.138454869389534, 0.1388888955116272, 0.1467013955116272, 1.0, 0.1475694477558136, 0.1419270783662796, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13671875, 0.140625, 1.0, 1.0, 0.1427951455116272, 0.1375868022441864, 0.1397569477558136, 0.1380208283662796, 0.1414930522441864, 1.0, 0.1388888955116272, 0.1371527761220932, 1.0, 0.1393229216337204, 1.0, 1.0, 0.1414930522441864, 0.1388888955116272, 1.0, 0.1471354216337204, 0.1397569477558136, 0.13671875, 0.1401909738779068, 1.0, 1.0, 0.1419270783662796, 0.1393229216337204, 0.1401909738779068, 0.140625, 1.0, 0.1380208283662796]

 sparsity of   [0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.015625, 0.0078125, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0390625, 1.0, 0.0234375, 0.0, 0.0, 0.1484375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.015625, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.546875, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 0.015625, 0.0, 1.0, 0.015625, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.2578125, 0.015625, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 1.0, 0.0078125, 1.0, 0.0, 1.0, 0.0078125, 0.0234375, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 1.0, 1.0, 0.0078125, 0.015625, 0.015625, 0.0, 1.0, 0.0078125, 0.0, 1.0, 1.0, 0.015625, 0.0078125, 1.0, 0.0, 0.0078125, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0234375, 0.0, 0.0, 0.0234375, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0625, 0.0078125, 0.0, 0.0, 0.015625, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0078125, 0.0078125, 1.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 1.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0234375, 0.0, 0.0078125, 0.015625, 0.0, 0.015625, 0.0]

 sparsity of   [1.0, 1.0, 0.1236979141831398, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1284722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1323784738779068, 1.0, 1.0, 0.1328125, 0.130642369389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1375868022441864, 1.0, 1.0, 1.0, 1.0, 0.1263020783662796, 0.1310763955116272, 1.0, 1.0, 1.0, 0.1319444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.134548619389534, 1.0, 1.0, 0.1488715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1362847238779068, 1.0, 0.1358506977558136, 0.1284722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.130642369389534, 1.0, 0.1371527761220932, 1.0, 1.0, 1.0, 0.1293402761220932, 0.126736119389534, 1.0, 1.0, 1.0, 0.1336805522441864, 0.1297743022441864, 0.1319444477558136, 1.0, 1.0, 0.1362847238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.1241319477558136, 1.0, 1.0, 1.0, 0.1228298619389534, 0.1293402761220932, 1.0, 1.0, 0.1293402761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1293402761220932, 1.0, 0.1215277761220932, 0.1315104216337204, 0.1293402761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1241319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.1310763955116272, 1.0, 0.1263020783662796, 1.0, 1.0, 0.1323784738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1302083283662796, 1.0, 0.130642369389534, 1.0, 1.0, 1.0, 0.1341145783662796, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1432291716337204, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1319444477558136, 0.1302083283662796, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1336805522441864, 0.1336805522441864, 1.0, 0.126736119389534, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 0.7986111044883728, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 0.7990451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799913227558136, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7947048544883728, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7981770634651184, 1.0, 0.7990451455116272, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 0.8042534589767456, 0.796875, 1.0, 1.0, 1.0, 0.8016493320465088, 0.7977430820465088, 1.0, 1.0, 1.0, 0.7973090410232544]

 sparsity of   [1.0, 1.0, 1.0, 0.1046006977558136, 1.0, 1.0, 0.1158854141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 0.118055559694767, 1.0, 1.0, 1.0, 0.1111111119389534, 0.1111111119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1263020783662796, 1.0, 1.0, 1.0, 0.1115451380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1206597238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1154513880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.114149309694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 0.1085069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2022569477558136, 1.0, 1.0, 1.0, 1.0, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1137152761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.1193576380610466, 1.0, 0.1150173619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1145833358168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1128472238779068, 1.0, 1.0, 1.0, 0.1145833358168602, 1.0, 1.0, 1.0, 1.0, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1206597238779068, 1.0, 1.0, 1.0, 0.1119791641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.1119791641831398, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1154513880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1158854141831398, 1.0, 1.0, 1.0, 0.1085069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1254340261220932, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9186198115348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9173176884651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.3359375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.01953125, 0.017578125, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375]

Total parameter pruned: 9920657.003497265 (unstructured) 9576676 (structured)

Test: [0/79]	Time 0.131 (0.131)	Loss 0.3459 (0.3459) ([0.224]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 94.680

 Total elapsed time  1:18:03.964260 
 FINETUNING


 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03703703731298447, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.6892361044883728, 0.6875, 1.0, 0.6892361044883728, 1.0, 1.0, 0.6892361044883728, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6892361044883728, 0.6875, 0.6892361044883728, 0.6875, 0.6892361044883728, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6892361044883728, 0.6875, 1.0, 0.6875, 0.6875, 0.6892361044883728, 0.6892361044883728, 1.0, 0.6875, 1.0, 0.6892361044883728, 0.6892361044883728, 0.694444477558136, 0.6875, 0.6875, 0.6927083134651184, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6909722089767456, 1.0, 1.0, 0.6892361044883728, 0.6892361044883728, 0.6875, 1.0, 0.6909722089767456, 0.6927083134651184, 0.6927083134651184, 0.6909722089767456, 0.6961805820465088, 1.0, 0.6875, 1.0, 0.6909722089767456, 1.0, 0.6892361044883728]

 sparsity of   [1.0, 1.0, 0.3333333432674408, 0.3298611044883728, 0.3298611044883728, 0.3298611044883728, 0.34375, 0.3819444477558136, 0.3350694477558136, 0.328125, 0.3298611044883728, 0.3368055522441864, 0.3298611044883728, 0.331597238779068, 0.3385416567325592, 0.3333333432674408, 0.3333333432674408, 0.331597238779068, 0.3298611044883728, 0.3298611044883728, 0.3333333432674408, 1.0, 1.0, 0.328125, 1.0, 0.3298611044883728, 0.3385416567325592, 0.331597238779068, 0.3368055522441864, 0.328125, 0.3333333432674408, 1.0, 0.3402777910232544, 0.3385416567325592, 0.3298611044883728, 0.328125, 0.3368055522441864, 0.331597238779068, 0.3350694477558136, 0.3420138955116272, 0.3333333432674408, 0.3559027910232544, 0.328125, 0.331597238779068, 0.3298611044883728, 0.331597238779068, 0.331597238779068, 1.0, 0.3385416567325592, 0.328125, 1.0, 1.0, 0.3368055522441864, 0.3298611044883728, 0.3489583432674408, 0.331597238779068, 0.3298611044883728, 0.3298611044883728, 0.328125, 0.328125, 0.331597238779068, 0.331597238779068, 0.331597238779068, 0.3298611044883728]

 sparsity of   [0.1458333283662796, 1.0, 1.0, 0.142361119389534, 1.0, 0.1458333283662796, 1.0, 0.1440972238779068, 1.0, 0.1510416716337204, 0.142361119389534, 0.1440972238779068, 0.1475694477558136, 0.1510416716337204, 0.142361119389534, 0.1458333283662796, 1.0, 0.1770833283662796, 1.0, 0.1458333283662796, 1.0, 1.0, 1.0, 0.1458333283662796, 0.142361119389534, 0.142361119389534, 0.142361119389534, 1.0, 0.1440972238779068, 1.0, 0.1475694477558136, 0.142361119389534, 1.0, 0.1458333283662796, 0.1475694477558136, 0.1458333283662796, 0.142361119389534, 1.0, 1.0, 1.0, 0.1597222238779068, 1.0, 1.0, 0.1493055522441864, 1.0, 0.1440972238779068, 0.140625, 0.1493055522441864, 0.1475694477558136, 1.0, 1.0, 0.1493055522441864, 0.1475694477558136, 0.140625, 0.1458333283662796, 0.1458333283662796, 1.0, 1.0, 0.142361119389534, 0.1440972238779068, 0.140625, 1.0, 0.1510416716337204, 0.142361119389534]

 sparsity of   [0.3715277910232544, 0.3541666567325592, 0.3697916567325592, 0.3767361044883728, 0.3854166567325592, 0.359375, 0.3871527910232544, 0.4166666567325592, 0.3975694477558136, 0.347222238779068, 0.3871527910232544, 0.3611111044883728, 0.3836805522441864, 0.394097238779068, 0.3923611044883728, 0.3559027910232544, 0.3645833432674408, 0.3645833432674408, 0.3697916567325592, 0.3732638955116272, 0.390625, 0.3715277910232544, 0.3524305522441864, 0.3871527910232544, 0.3541666567325592, 0.3958333432674408, 0.362847238779068, 0.3819444477558136, 0.4357638955116272, 0.3506944477558136, 0.3697916567325592, 0.3680555522441864, 0.3993055522441864, 0.3732638955116272, 0.3680555522441864, 0.3506944477558136, 0.3697916567325592, 0.359375, 0.359375, 1.0, 0.3767361044883728, 0.3802083432674408, 0.3958333432674408, 0.3836805522441864, 0.3697916567325592, 0.3697916567325592, 0.3836805522441864, 0.3663194477558136, 0.390625, 0.390625, 0.3576388955116272, 0.3576388955116272, 0.3836805522441864, 0.359375, 0.3680555522441864, 0.3958333432674408, 0.3611111044883728, 0.3697916567325592, 0.3715277910232544, 0.378472238779068, 0.3680555522441864, 0.3819444477558136, 0.378472238779068, 0.3697916567325592]

 sparsity of   [0.0069444444961845875, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0086805559694767, 1.0, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0086805559694767, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0086805559694767, 0.0, 0.010416666977107525, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.02083333395421505, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 0.0086805559694767, 0.0, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.0052083334885537624, 0.010416666977107525, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.0, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0086805559694767, 0.0017361111240461469, 0.0017361111240461469, 0.0069444444961845875, 0.0069444444961845875, 0.0052083334885537624, 0.0017361111240461469, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0069444444961845875, 1.0, 1.0, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.010416666977107525, 0.0069444444961845875, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0086805559694767, 0.0, 0.0173611119389534, 0.01215277798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.0, 0.0017361111240461469, 0.013888888992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0086805559694767, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 1.0]

 sparsity of   [0.046875, 0.0607638880610466, 0.9609375, 0.0477430559694767, 0.0512152798473835, 0.0486111119389534, 0.0451388880610466, 0.0529513880610466, 0.0460069440305233, 0.2456597238779068, 0.0494791679084301, 0.0451388880610466, 0.0477430559694767, 0.0451388880610466, 0.0529513880610466, 0.2005208283662796, 0.1605902761220932, 0.0477430559694767, 0.0659722238779068, 0.0512152798473835, 0.0494791679084301, 0.0512152798473835, 0.0503472238779068, 0.0572916679084301, 0.046875, 0.0477430559694767, 0.0477430559694767, 1.0, 0.0546875, 0.0442708320915699, 0.0529513880610466, 0.046875, 0.0442708320915699, 1.0, 0.0477430559694767, 0.0486111119389534, 0.0486111119389534, 0.046875, 0.046875, 0.046875, 0.1223958358168602, 0.2595486044883728, 0.0503472238779068, 0.0494791679084301, 0.0442708320915699, 0.0503472238779068, 0.0486111119389534, 0.0512152798473835, 0.0460069440305233, 0.0477430559694767, 0.487847238779068, 0.0494791679084301, 1.0, 0.0425347238779068, 0.0538194440305233, 0.0460069440305233, 0.0451388880610466, 0.0572916679084301, 0.0460069440305233, 0.0494791679084301, 0.0503472238779068, 0.0486111119389534, 0.0477430559694767, 0.2916666567325592, 0.0503472238779068, 0.0477430559694767, 1.0, 0.0512152798473835, 0.0486111119389534, 0.0486111119389534, 0.0460069440305233, 0.0503472238779068, 0.0477430559694767, 0.0503472238779068, 0.0477430559694767, 0.0451388880610466, 0.046875, 0.046875, 0.0486111119389534, 0.0460069440305233, 0.0850694477558136, 0.0503472238779068, 0.0546875, 0.0460069440305233, 0.0520833320915699, 0.0659722238779068, 0.0503472238779068, 0.0503472238779068, 0.046875, 0.125, 0.046875, 0.046875, 0.1953125, 0.0503472238779068, 0.0520833320915699, 0.0477430559694767, 0.0546875, 0.0451388880610466, 0.0494791679084301, 0.0442708320915699, 0.0477430559694767, 0.0477430559694767, 0.0460069440305233, 0.0529513880610466, 0.0451388880610466, 0.0442708320915699, 0.0529513880610466, 0.0538194440305233, 0.0442708320915699, 0.0642361119389534, 0.0477430559694767, 0.0477430559694767, 0.0477430559694767, 0.0460069440305233, 0.0555555559694767, 0.0598958320915699, 0.0494791679084301, 0.0494791679084301, 0.046875, 0.0529513880610466, 0.0477430559694767, 0.0494791679084301, 0.0486111119389534, 0.046875, 0.0451388880610466, 0.0520833320915699, 0.0512152798473835, 0.0512152798473835]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 1.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.015625, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.02690972201526165, 0.0329861119389534, 1.0, 0.02864583395421505, 0.02690972201526165, 0.02864583395421505, 1.0, 1.0, 1.0, 0.02951388992369175, 0.02690972201526165, 0.02951388992369175, 0.0321180559694767, 1.0, 0.02690972201526165, 0.02864583395421505, 1.0, 0.02690972201526165, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 1.0, 0.0321180559694767, 1.0, 0.02864583395421505, 0.02777777798473835, 0.0243055559694767, 0.0251736119389534, 1.0, 0.0303819440305233, 0.0303819440305233, 0.02690972201526165, 1.0, 1.0, 1.0, 0.02777777798473835, 0.02864583395421505, 0.02777777798473835, 0.02864583395421505, 0.02864583395421505, 0.02777777798473835, 0.02690972201526165, 0.02864583395421505, 0.0251736119389534, 0.0251736119389534, 0.0243055559694767, 1.0, 0.02604166604578495, 0.0347222238779068, 0.02951388992369175, 0.02604166604578495, 1.0, 0.02777777798473835, 0.02690972201526165, 0.0251736119389534, 0.02690972201526165, 0.0381944440305233, 1.0, 0.02604166604578495, 0.02951388992369175, 1.0, 0.0251736119389534, 1.0, 0.02864583395421505, 0.02690972201526165, 1.0, 1.0, 1.0, 1.0, 1.0, 0.02690972201526165, 1.0, 1.0, 0.02951388992369175, 0.02864583395421505, 1.0, 0.02690972201526165, 0.02951388992369175, 1.0, 0.02690972201526165, 0.0529513880610466, 0.0243055559694767, 0.02864583395421505, 1.0, 0.02864583395421505, 1.0, 0.02604166604578495, 0.0303819440305233, 1.0, 0.02690972201526165, 0.02951388992369175, 0.02690972201526165, 1.0, 0.0321180559694767, 0.0303819440305233, 1.0, 0.0303819440305233, 0.02864583395421505, 0.0303819440305233, 0.02690972201526165, 0.0329861119389534, 0.0303819440305233, 0.0329861119389534, 1.0, 0.02777777798473835, 1.0, 0.02864583395421505, 1.0, 1.0, 0.02604166604578495, 0.03125, 0.0338541679084301, 0.02777777798473835, 0.03125, 1.0, 0.02951388992369175, 0.0251736119389534, 1.0, 1.0, 0.02951388992369175, 0.02777777798473835, 0.02777777798473835, 1.0, 1.0, 0.0251736119389534, 0.0234375]

 sparsity of   [0.3368055522441864, 0.331597238779068, 0.3307291567325592, 0.3602430522441864, 0.3298611044883728, 0.3576388955116272, 0.3359375, 0.3689236044883728, 0.3324652910232544, 0.3793402910232544, 0.3324652910232544, 0.3298611044883728, 0.3385416567325592, 0.3298611044883728, 0.3324652910232544, 0.4826388955116272, 0.375, 0.3333333432674408, 0.3645833432674408, 0.3350694477558136, 0.3359375, 0.4053819477558136, 0.3333333432674408, 0.4131944477558136, 0.3342013955116272, 0.378472238779068, 0.362847238779068, 0.3298611044883728, 0.4817708432674408, 0.3932291567325592, 0.3333333432674408, 0.3385416567325592, 0.3359375, 0.3289930522441864, 0.3342013955116272, 0.3350694477558136, 0.331597238779068, 0.3411458432674408, 0.3324652910232544, 0.331597238779068, 0.3914930522441864, 1.0, 0.3368055522441864, 0.3333333432674408, 0.3333333432674408, 0.5824652910232544, 0.331597238779068, 0.331597238779068, 0.3350694477558136, 0.3567708432674408, 0.3645833432674408, 0.4496527910232544, 0.3350694477558136, 0.3307291567325592, 0.3307291567325592, 0.3307291567325592, 0.3342013955116272, 0.3342013955116272, 0.3350694477558136, 0.3350694477558136, 0.3359375, 0.3993055522441864, 0.331597238779068, 0.3368055522441864, 0.3333333432674408, 0.3298611044883728, 0.3324652910232544, 0.3307291567325592, 0.4861111044883728, 0.448784738779068, 1.0, 0.9947916865348816, 0.3376736044883728, 0.3307291567325592, 0.3333333432674408, 0.3350694477558136, 0.3324652910232544, 0.3359375, 0.339409738779068, 0.3723958432674408, 0.3350694477558136, 0.3333333432674408, 0.331597238779068, 0.3368055522441864, 0.3385416567325592, 0.3342013955116272, 0.331597238779068, 0.3324652910232544, 0.3498263955116272, 0.4114583432674408, 0.3359375, 0.3723958432674408, 0.440972238779068, 0.3333333432674408, 1.0, 0.3376736044883728, 0.331597238779068, 0.3350694477558136, 0.3298611044883728, 0.5052083134651184, 0.3758680522441864, 0.331597238779068, 0.3376736044883728, 0.3359375, 0.3428819477558136, 0.3307291567325592, 0.3611111044883728, 0.34375, 0.4930555522441864, 0.3333333432674408, 0.3324652910232544, 0.4670138955116272, 0.3359375, 0.3376736044883728, 0.3298611044883728, 0.331597238779068, 0.409722238779068, 0.3298611044883728, 0.3680555522441864, 0.3342013955116272, 0.3333333432674408, 0.3342013955116272, 0.3289930522441864, 0.3715277910232544, 0.355034738779068, 0.472222238779068, 0.3289930522441864, 0.3333333432674408]

 sparsity of   [0.0034722222480922937, 0.0069444444961845875, 0.0078125, 1.0, 1.0, 0.0078125, 0.0052083334885537624, 0.00434027798473835, 0.013020833022892475, 1.0, 0.0052083334885537624, 0.00434027798473835, 0.01128472201526165, 1.0, 0.0078125, 0.009548611007630825, 0.0069444444961845875, 1.0, 0.006076388992369175, 0.0086805559694767, 0.0078125, 0.0034722222480922937, 0.0078125, 1.0, 0.00434027798473835, 0.0052083334885537624, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.00434027798473835, 1.0, 0.0034722222480922937, 0.0026041667442768812, 0.0034722222480922937, 0.0026041667442768812, 0.0052083334885537624, 0.0069444444961845875, 0.006076388992369175, 0.0086805559694767, 0.0052083334885537624, 0.0086805559694767, 0.006076388992369175, 0.006076388992369175, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 1.0, 1.0, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.006076388992369175, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0078125, 0.0008680555620230734, 0.0086805559694767, 0.009548611007630825, 0.0052083334885537624, 1.0, 0.0086805559694767, 0.0052083334885537624, 0.0078125, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 0.009548611007630825, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.006076388992369175, 0.0034722222480922937, 0.006076388992369175, 1.0, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0086805559694767, 0.009548611007630825, 0.00434027798473835, 0.00434027798473835, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0086805559694767, 0.006076388992369175, 0.0052083334885537624, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.006076388992369175, 0.0069444444961845875, 0.0069444444961845875, 1.0, 0.0052083334885537624, 0.0078125, 1.0, 0.00434027798473835, 1.0, 0.0034722222480922937, 0.0026041667442768812, 0.009548611007630825, 0.00434027798473835, 0.0052083334885537624, 0.0078125, 0.0086805559694767, 1.0, 0.0052083334885537624, 0.015625, 0.0052083334885537624, 0.00434027798473835, 0.00434027798473835, 0.0069444444961845875, 0.00434027798473835, 0.0017361111240461469, 0.0034722222480922937, 1.0, 0.0017361111240461469, 0.0078125, 1.0, 0.01128472201526165, 0.0026041667442768812, 1.0, 0.0034722222480922937, 0.010416666977107525, 0.009548611007630825, 0.0086805559694767, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0086805559694767, 0.0078125, 0.0034722222480922937, 1.0, 0.0078125, 0.006076388992369175, 0.009548611007630825, 0.0026041667442768812, 0.010416666977107525, 0.009548611007630825, 0.006076388992369175, 0.00434027798473835, 0.0069444444961845875, 0.0052083334885537624, 0.006076388992369175, 1.0, 0.00434027798473835, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.00434027798473835, 0.006076388992369175, 0.006076388992369175, 0.0069444444961845875, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.0078125, 0.0078125, 0.00434027798473835, 0.00434027798473835, 0.0052083334885537624, 1.0, 1.0, 0.0069444444961845875, 0.00434027798473835, 0.0086805559694767, 0.0069444444961845875, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.02777777798473835, 0.0052083334885537624, 0.0069444444961845875, 0.0069444444961845875, 0.006076388992369175, 0.0034722222480922937, 0.0008680555620230734, 0.0052083334885537624, 0.0052083334885537624, 0.0078125, 1.0, 1.0, 0.006076388992369175, 0.0026041667442768812, 0.0052083334885537624, 0.0052083334885537624, 0.009548611007630825, 0.006076388992369175, 0.006076388992369175, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 1.0, 0.0069444444961845875, 0.0086805559694767, 0.0086805559694767, 0.0052083334885537624, 0.009548611007630825, 1.0, 0.013888888992369175, 0.0026041667442768812, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.006076388992369175, 0.0034722222480922937, 0.0008680555620230734, 0.0052083334885537624, 0.0034722222480922937, 0.006076388992369175, 0.00434027798473835, 0.0078125, 0.0078125, 1.0, 0.0052083334885537624, 0.0078125, 0.009548611007630825, 0.00434027798473835, 1.0, 1.0, 1.0, 0.00434027798473835, 0.0078125, 0.0069444444961845875, 0.0034722222480922937, 0.0078125, 0.0034722222480922937, 0.0069444444961845875, 0.00434027798473835, 0.006076388992369175, 1.0, 1.0, 0.00434027798473835, 0.0078125, 0.0078125, 0.0052083334885537624, 0.00434027798473835, 0.006076388992369175, 0.0069444444961845875, 0.006076388992369175, 0.0078125, 0.006076388992369175, 0.0052083334885537624, 0.0052083334885537624, 1.0]

 sparsity of   [1.0, 0.154079869389534, 0.1419270783662796, 1.0, 0.1388888955116272, 1.0, 1.0, 0.1388888955116272, 0.1414930522441864, 0.1410590261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1414930522441864, 1.0, 0.142361119389534, 1.0, 0.1480034738779068, 1.0, 1.0, 1.0, 0.1375868022441864, 0.1393229216337204, 1.0, 0.1401909738779068, 1.0, 0.1388888955116272, 0.1401909738779068, 0.1427951455116272, 0.1410590261220932, 1.0, 0.1397569477558136, 1.0, 0.138454869389534, 0.1401909738779068, 0.1397569477558136, 1.0, 1.0, 0.1427951455116272, 0.13671875, 0.1410590261220932, 0.1410590261220932, 0.1397569477558136, 0.1380208283662796, 0.1549479216337204, 0.1388888955116272, 0.142361119389534, 0.1397569477558136, 1.0, 0.1401909738779068, 0.1401909738779068, 0.1362847238779068, 0.1440972238779068, 1.0, 0.1375868022441864, 1.0, 1.0, 1.0, 0.1427951455116272, 1.0, 0.14453125, 0.1393229216337204, 1.0, 1.0, 0.1375868022441864, 0.1393229216337204, 1.0, 0.142361119389534, 0.1401909738779068, 0.138454869389534, 0.138454869389534, 0.1401909738779068, 1.0, 1.0, 0.1414930522441864, 0.1388888955116272, 1.0, 1.0, 0.142361119389534, 1.0, 1.0, 1.0, 1.0, 0.1427951455116272, 1.0, 0.138454869389534, 0.1375868022441864, 1.0, 0.138454869389534, 0.1401909738779068, 1.0, 1.0, 1.0, 0.14453125, 0.1397569477558136, 1.0, 0.1397569477558136, 1.0, 0.1388888955116272, 0.1475694477558136, 0.138454869389534, 1.0, 0.140625, 1.0, 0.1419270783662796, 1.0, 1.0, 0.142361119389534, 1.0, 1.0, 0.14453125, 1.0, 0.146267369389534, 0.1458333283662796, 0.140625, 1.0, 0.1414930522441864, 0.1388888955116272, 0.1375868022441864, 0.1401909738779068, 0.1375868022441864, 0.1419270783662796, 1.0, 1.0, 0.1375868022441864, 1.0, 0.1401909738779068, 0.142361119389534, 0.1414930522441864, 1.0, 0.1375868022441864, 1.0, 0.1401909738779068, 0.1393229216337204, 1.0, 1.0, 1.0, 0.1432291716337204, 1.0, 0.1401909738779068, 1.0, 1.0, 1.0, 0.1388888955116272, 0.1388888955116272, 0.1375868022441864, 0.1410590261220932, 1.0, 0.138454869389534, 0.1388888955116272, 1.0, 1.0, 0.1401909738779068, 0.142361119389534, 0.140625, 1.0, 1.0, 0.1401909738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1371527761220932, 0.1393229216337204, 1.0, 1.0, 1.0, 0.1371527761220932, 0.1410590261220932, 0.1362847238779068, 1.0, 1.0, 0.1375868022441864, 1.0, 0.1453993022441864, 0.13671875, 0.13671875, 0.142361119389534, 0.1393229216337204, 0.1410590261220932, 0.1380208283662796, 1.0, 1.0, 0.138454869389534, 0.138454869389534, 0.1375868022441864, 0.1358506977558136, 1.0, 1.0, 1.0, 0.1393229216337204, 1.0, 0.1393229216337204, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1397569477558136, 1.0, 0.138454869389534, 0.1388888955116272, 1.0, 1.0, 0.1453993022441864, 1.0, 0.1397569477558136, 0.1397569477558136, 0.138454869389534, 0.1388888955116272, 0.1467013955116272, 1.0, 0.1475694477558136, 0.1419270783662796, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13671875, 0.140625, 1.0, 1.0, 0.1427951455116272, 0.1375868022441864, 0.1397569477558136, 0.1380208283662796, 0.1414930522441864, 1.0, 0.1388888955116272, 0.1371527761220932, 1.0, 0.1393229216337204, 1.0, 1.0, 0.1414930522441864, 0.1388888955116272, 1.0, 0.1471354216337204, 0.1397569477558136, 0.13671875, 0.1401909738779068, 1.0, 1.0, 0.1419270783662796, 0.1393229216337204, 0.1401909738779068, 0.140625, 1.0, 0.1380208283662796]

 sparsity of   [0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 1.0, 0.0, 0.015625, 0.0078125, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0390625, 1.0, 0.0234375, 0.0, 0.0, 0.1484375, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.015625, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.546875, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 0.015625, 0.0, 1.0, 0.015625, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.015625, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.015625, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.2578125, 0.015625, 0.0078125, 1.0, 0.0078125, 1.0, 1.0, 0.0, 1.0, 0.0078125, 0.0, 0.015625, 0.015625, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 1.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0078125, 1.0, 0.0078125, 1.0, 0.0, 1.0, 0.0078125, 0.0234375, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 1.0, 1.0, 0.0078125, 0.015625, 0.015625, 0.0, 1.0, 0.0078125, 0.0, 1.0, 1.0, 0.015625, 0.0078125, 1.0, 0.0, 0.0078125, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0234375, 0.0, 0.0, 0.0234375, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0625, 0.0078125, 0.0, 0.0, 0.015625, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0078125, 1.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.015625, 0.0078125, 0.0078125, 1.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.0, 1.0, 0.0, 0.0078125, 0.0, 0.0078125, 0.0, 1.0, 0.0078125, 0.0078125, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0234375, 0.0, 0.0078125, 0.015625, 0.0, 0.015625, 0.0]

 sparsity of   [1.0, 1.0, 0.1236979141831398, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1284722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1323784738779068, 1.0, 1.0, 0.1328125, 0.130642369389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1375868022441864, 1.0, 1.0, 1.0, 1.0, 0.1263020783662796, 0.1310763955116272, 1.0, 1.0, 1.0, 0.1319444477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.134548619389534, 1.0, 1.0, 0.1488715261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1362847238779068, 1.0, 0.1358506977558136, 0.1284722238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.130642369389534, 1.0, 0.1371527761220932, 1.0, 1.0, 1.0, 0.1293402761220932, 0.126736119389534, 1.0, 1.0, 1.0, 0.1336805522441864, 0.1297743022441864, 0.1319444477558136, 1.0, 1.0, 0.1362847238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12890625, 1.0, 1.0, 1.0, 1.0, 0.1241319477558136, 1.0, 1.0, 1.0, 0.1228298619389534, 0.1293402761220932, 1.0, 1.0, 0.1293402761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1293402761220932, 1.0, 0.1215277761220932, 0.1315104216337204, 0.1293402761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1241319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.1310763955116272, 1.0, 0.1263020783662796, 1.0, 1.0, 0.1323784738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1302083283662796, 1.0, 0.130642369389534, 1.0, 1.0, 1.0, 0.1341145783662796, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1432291716337204, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1319444477558136, 0.1302083283662796, 0.1258680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1336805522441864, 0.1336805522441864, 1.0, 0.126736119389534, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 0.7986111044883728, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 0.7990451455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799913227558136, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7973090410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 0.7981770634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7947048544883728, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7981770634651184, 1.0, 0.7990451455116272, 1.0, 1.0, 1.0, 1.0, 0.7986111044883728, 1.0, 1.0, 1.0, 1.0, 0.7977430820465088, 1.0, 1.0, 1.0, 0.8042534589767456, 0.796875, 1.0, 1.0, 1.0, 0.8016493320465088, 0.7977430820465088, 1.0, 1.0, 1.0, 0.7973090410232544]

 sparsity of   [1.0, 1.0, 1.0, 0.1046006977558136, 1.0, 1.0, 0.1158854141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 0.118055559694767, 1.0, 1.0, 1.0, 0.1111111119389534, 0.1111111119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15625, 1.0, 1.0, 1.0, 1.0, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1263020783662796, 1.0, 1.0, 1.0, 0.1115451380610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1280381977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1206597238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1154513880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.114149309694767, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 0.1085069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2022569477558136, 1.0, 1.0, 1.0, 1.0, 0.1106770858168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1137152761220932, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.1193576380610466, 1.0, 0.1150173619389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1245659738779068, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1145833358168602, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1128472238779068, 1.0, 1.0, 1.0, 0.1145833358168602, 1.0, 1.0, 1.0, 1.0, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1206597238779068, 1.0, 1.0, 1.0, 0.1119791641831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 0.1119791641831398, 1.0, 1.0, 0.1080729141831398, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1154513880610466, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1158854141831398, 1.0, 1.0, 1.0, 0.1085069477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1254340261220932, 0.1176215261220932, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9186198115348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9181857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.917100727558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9173176884651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9164496660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 1.0, 0.9168837070465088, 1.0, 1.0, 0.9175347089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 0.10546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14453125, 0.1484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.3359375, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.11328125, 1.0, 1.0, 0.109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.01953125, 0.017578125, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375]

Total parameter pruned: 9920657.003497265 (unstructured) 9576676 (structured)

Test: [0/79]	Time 0.137 (0.137)	Loss 0.3460 (0.3460) ([0.224]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-03
Grad=  tensor(0.0747, device='cuda:0')
Epoch: [300][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [300][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0029) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [300][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [300][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0076 (0.0031) ([0.008]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2252 (0.2252) ([0.225]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-03
Grad=  tensor(0.0014, device='cuda:0')
Epoch: [301][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [301][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0103 (0.0030) ([0.010]+[0.000])	Prec@1 99.219 (99.981)
Epoch: [301][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0031) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2190 (0.2190) ([0.219]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.1034, device='cuda:0')
Epoch: [302][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0033) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [302][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0114 (0.0032) ([0.011]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [302][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0032) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2303 (0.2303) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-03
Grad=  tensor(0.7506, device='cuda:0')
Epoch: [303][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [303][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [303][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0072 (0.0030) ([0.007]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2512 (0.2512) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.0214, device='cuda:0')
Epoch: [304][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [304][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [304][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0031 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2460 (0.2460) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.0203, device='cuda:0')
Epoch: [305][0/391]	Time 0.163 (0.163)	Data 0.137 (0.137)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0030) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [305][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0030) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [305][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0033 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2415 (0.2415) ([0.241]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.800
current lr 1.00000e-03
Grad=  tensor(0.0526, device='cuda:0')
Epoch: [306][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [306][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0066 (0.0029) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [306][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0033 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2639 (0.2639) ([0.264]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-03
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [307][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0037 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [307][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [307][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2805 (0.2805) ([0.281]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.0018, device='cuda:0')
Epoch: [308][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [308][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [308][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0090 (0.0029) ([0.009]+[0.000])	Prec@1 99.219 (99.966)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2598 (0.2598) ([0.260]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.0144, device='cuda:0')
Epoch: [309][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [309][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0032 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [309][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2453 (0.2453) ([0.245]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.3048, device='cuda:0')
Epoch: [310][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0102 (0.0031) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [310][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0040 (0.0029) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [310][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2386 (0.2386) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-03
Grad=  tensor(2.4909, device='cuda:0')
Epoch: [311][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0093 (0.0093) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0038 (0.0027) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [311][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [311][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2216 (0.2216) ([0.222]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.1931, device='cuda:0')
Epoch: [312][0/391]	Time 0.165 (0.165)	Data 0.139 (0.139)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0037 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [312][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0055 (0.0027) ([0.005]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [312][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2393 (0.2393) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.0048, device='cuda:0')
Epoch: [313][0/391]	Time 0.158 (0.158)	Data 0.132 (0.132)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [313][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [313][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0024 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2587 (0.2587) ([0.259]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0861, device='cuda:0')
Epoch: [314][0/391]	Time 0.152 (0.152)	Data 0.128 (0.128)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0042 (0.0030) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [314][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0029) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [314][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0028) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2572 (0.2572) ([0.257]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0088, device='cuda:0')
Epoch: [315][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [315][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [315][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2549 (0.2549) ([0.255]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.0451, device='cuda:0')
Epoch: [316][0/391]	Time 0.148 (0.148)	Data 0.123 (0.123)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [316][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [316][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0029 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2645 (0.2645) ([0.264]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0054, device='cuda:0')
Epoch: [317][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0031) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [317][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0037 (0.0028) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [317][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2479 (0.2479) ([0.248]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.780
current lr 1.00000e-03
Grad=  tensor(0.0131, device='cuda:0')
Epoch: [318][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [318][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2723 (0.2723) ([0.272]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [319][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [319][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0209 (0.0030) ([0.021]+[0.000])	Prec@1 99.219 (99.969)
Epoch: [319][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0006 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [319][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2529 (0.2529) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-03
Grad=  tensor(0.0285, device='cuda:0')
Epoch: [320][0/391]	Time 0.148 (0.148)	Data 0.122 (0.122)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [320][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [320][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2276 (0.2276) ([0.228]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.0428, device='cuda:0')
Epoch: [321][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [321][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0034 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [321][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0029 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2557 (0.2557) ([0.256]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-03
Grad=  tensor(0.0096, device='cuda:0')
Epoch: [322][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0042 (0.0024) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [322][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [322][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0031 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2624 (0.2624) ([0.262]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.2662, device='cuda:0')
Epoch: [323][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [323][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0081 (0.0026) ([0.008]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [323][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [323][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2620 (0.2620) ([0.262]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0528, device='cuda:0')
Epoch: [324][0/391]	Time 0.148 (0.148)	Data 0.122 (0.122)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [324][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [324][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2601 (0.2601) ([0.260]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0039, device='cuda:0')
Epoch: [325][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [325][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [325][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0033 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2532 (0.2532) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0041, device='cuda:0')
Epoch: [326][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [326][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [326][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2382 (0.2382) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-03
Grad=  tensor(0.0038, device='cuda:0')
Epoch: [327][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [327][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0027 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2333 (0.2333) ([0.233]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-03
Grad=  tensor(0.0081, device='cuda:0')
Epoch: [328][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0027) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [328][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [328][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2337 (0.2337) ([0.234]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-03
Grad=  tensor(0.0069, device='cuda:0')
Epoch: [329][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [329][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0067 (0.0023) ([0.007]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [329][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2336 (0.2336) ([0.234]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.0034, device='cuda:0')
Epoch: [330][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0082 (0.0025) ([0.008]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [330][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0027) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [330][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2278 (0.2278) ([0.228]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.0666, device='cuda:0')
Epoch: [331][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [331][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2288 (0.2288) ([0.229]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.2473, device='cuda:0')
Epoch: [332][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [332][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2456 (0.2456) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.0030, device='cuda:0')
Epoch: [333][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [333][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [333][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2583 (0.2583) ([0.258]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.0191, device='cuda:0')
Epoch: [334][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0080 (0.0022) ([0.008]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [334][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0122 (0.0022) ([0.012]+[0.000])	Prec@1 99.219 (99.995)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2482 (0.2482) ([0.248]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(7.7224, device='cuda:0')
Epoch: [335][0/391]	Time 0.161 (0.161)	Data 0.135 (0.135)	Loss 0.0113 (0.0113) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [335][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0028) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [335][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0026) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [335][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0037 (0.0025) ([0.004]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2334 (0.2334) ([0.233]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-03
Grad=  tensor(0.0241, device='cuda:0')
Epoch: [336][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [336][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [336][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0066 (0.0022) ([0.007]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2482 (0.2482) ([0.248]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.690
current lr 1.00000e-03
Grad=  tensor(0.1364, device='cuda:0')
Epoch: [337][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0044 (0.0022) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [337][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [337][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0033 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2610 (0.2610) ([0.261]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.0659, device='cuda:0')
Epoch: [338][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [338][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2519 (0.2519) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-03
Grad=  tensor(0.0210, device='cuda:0')
Epoch: [339][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0024) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [339][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [339][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2654 (0.2654) ([0.265]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-03
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [340][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [340][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [340][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0040 (0.0024) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2552 (0.2552) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-03
Grad=  tensor(0.0044, device='cuda:0')
Epoch: [341][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [341][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2509 (0.2509) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.0036, device='cuda:0')
Epoch: [342][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0089 (0.0023) ([0.009]+[0.000])	Prec@1 99.219 (99.992)
Epoch: [342][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [342][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2352 (0.2352) ([0.235]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.0759, device='cuda:0')
Epoch: [343][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2362 (0.2362) ([0.236]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(0.2958, device='cuda:0')
Epoch: [344][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0024) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [344][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [344][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0043 (0.0022) ([0.004]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2405 (0.2405) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-03
Grad=  tensor(1.7300, device='cuda:0')
Epoch: [345][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0090 (0.0090) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2431 (0.2431) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-03
Grad=  tensor(0.0169, device='cuda:0')
Epoch: [346][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [346][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [346][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2352 (0.2352) ([0.235]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-03
Grad=  tensor(0.0117, device='cuda:0')
Epoch: [347][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [347][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0051 (0.0022) ([0.005]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2321 (0.2321) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-03
Grad=  tensor(0.0812, device='cuda:0')
Epoch: [348][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [348][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2369 (0.2369) ([0.237]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.710
current lr 1.00000e-03
Grad=  tensor(0.0025, device='cuda:0')
Epoch: [349][0/391]	Time 0.157 (0.157)	Data 0.131 (0.131)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [349][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0025) ([0.001]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [349][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0100 (0.0023) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2263 (0.2263) ([0.226]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-04
Grad=  tensor(0.7670, device='cuda:0')
Epoch: [350][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [350][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0043 (0.0023) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2347 (0.2347) ([0.235]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [351][0/391]	Time 0.155 (0.155)	Data 0.128 (0.128)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [351][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [351][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2421 (0.2421) ([0.242]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.670
current lr 1.00000e-04
Grad=  tensor(0.1312, device='cuda:0')
Epoch: [352][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [352][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [352][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2458 (0.2458) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-04
Grad=  tensor(0.0377, device='cuda:0')
Epoch: [353][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [353][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0046 (0.0020) ([0.005]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [353][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2352 (0.2352) ([0.235]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.680
current lr 1.00000e-04
Grad=  tensor(1.0749, device='cuda:0')
Epoch: [354][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0075 (0.0075) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [354][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [354][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2502 (0.2502) ([0.250]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-04
Grad=  tensor(0.0109, device='cuda:0')
Epoch: [355][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [355][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0042 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2292 (0.2292) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-04
Grad=  tensor(0.0025, device='cuda:0')
Epoch: [356][0/391]	Time 0.159 (0.159)	Data 0.134 (0.134)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [356][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0020) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [356][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2468 (0.2468) ([0.247]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-04
Grad=  tensor(0.9568, device='cuda:0')
Epoch: [357][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0065 (0.0065) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [357][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [357][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2379 (0.2379) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-04
Grad=  tensor(0.0118, device='cuda:0')
Epoch: [358][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [358][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [358][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2468 (0.2468) ([0.247]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-04
Grad=  tensor(0.0390, device='cuda:0')
Epoch: [359][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [359][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [359][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2635 (0.2635) ([0.264]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-04
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [360][0/391]	Time 0.148 (0.148)	Data 0.123 (0.123)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0023) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [360][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [360][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0026 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2476 (0.2476) ([0.248]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0878, device='cuda:0')
Epoch: [361][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [361][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [361][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2195 (0.2195) ([0.219]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-04
Grad=  tensor(0.0020, device='cuda:0')
Epoch: [362][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [362][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [362][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2436 (0.2436) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-04
Grad=  tensor(0.0829, device='cuda:0')
Epoch: [363][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [363][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [363][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2377 (0.2377) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.780
current lr 1.00000e-04
Grad=  tensor(0.0344, device='cuda:0')
Epoch: [364][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [364][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [364][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0041 (0.0022) ([0.004]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2492 (0.2492) ([0.249]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-04
Grad=  tensor(0.8999, device='cuda:0')
Epoch: [365][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [365][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [365][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2432 (0.2432) ([0.243]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [366][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [366][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2586 (0.2586) ([0.259]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0027, device='cuda:0')
Epoch: [367][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [367][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [367][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2511 (0.2511) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-04
Grad=  tensor(0.0102, device='cuda:0')
Epoch: [368][0/391]	Time 0.166 (0.166)	Data 0.140 (0.140)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2255 (0.2255) ([0.225]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-04
Grad=  tensor(0.0143, device='cuda:0')
Epoch: [369][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0026 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.115 (0.115)	Loss 0.2588 (0.2588) ([0.259]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-04
Grad=  tensor(0.0396, device='cuda:0')
Epoch: [370][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [370][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [370][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0029 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2451 (0.2451) ([0.245]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-04
Grad=  tensor(0.0082, device='cuda:0')
Epoch: [371][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [371][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2478 (0.2478) ([0.248]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.610
current lr 1.00000e-04
Grad=  tensor(0.0124, device='cuda:0')
Epoch: [372][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [372][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [372][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2380 (0.2380) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-04
Grad=  tensor(0.0427, device='cuda:0')
Epoch: [373][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [373][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2532 (0.2532) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.820
current lr 1.00000e-04
Grad=  tensor(0.0009, device='cuda:0')
Epoch: [374][0/391]	Time 0.160 (0.160)	Data 0.135 (0.135)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [374][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [374][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2288 (0.2288) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-04
Grad=  tensor(0.0135, device='cuda:0')
Epoch: [375][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [375][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2298 (0.2298) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-04
Grad=  tensor(0.0112, device='cuda:0')
Epoch: [376][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [376][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [376][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2236 (0.2236) ([0.224]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-04
Grad=  tensor(0.0127, device='cuda:0')
Epoch: [377][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0058 (0.0019) ([0.006]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [377][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2302 (0.2302) ([0.230]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0315, device='cuda:0')
Epoch: [378][0/391]	Time 0.158 (0.158)	Data 0.133 (0.133)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [378][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [378][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2402 (0.2402) ([0.240]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0229, device='cuda:0')
Epoch: [379][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [379][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.2351 (0.2351) ([0.235]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0039, device='cuda:0')
Epoch: [380][0/391]	Time 0.166 (0.166)	Data 0.141 (0.141)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [380][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2398 (0.2398) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.780
current lr 1.00000e-04
Grad=  tensor(0.0012, device='cuda:0')
Epoch: [381][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2438 (0.2438) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-04
Grad=  tensor(0.0060, device='cuda:0')
Epoch: [382][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [382][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0042 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [382][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2500 (0.2500) ([0.250]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.730
current lr 1.00000e-04
Grad=  tensor(0.1689, device='cuda:0')
Epoch: [383][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0026) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [383][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0023) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [383][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2231 (0.2231) ([0.223]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0052, device='cuda:0')
Epoch: [384][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [384][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [384][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0027 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2280 (0.2280) ([0.228]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-04
Grad=  tensor(0.0022, device='cuda:0')
Epoch: [385][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [385][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2371 (0.2371) ([0.237]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-04
Grad=  tensor(0.0125, device='cuda:0')
Epoch: [386][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0074 (0.0020) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2301 (0.2301) ([0.230]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.640
current lr 1.00000e-04
Grad=  tensor(0.0563, device='cuda:0')
Epoch: [387][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [387][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [387][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [387][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2430 (0.2430) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.590
current lr 1.00000e-04
Grad=  tensor(0.0489, device='cuda:0')
Epoch: [388][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [388][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [388][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0025 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2395 (0.2395) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.610
current lr 1.00000e-04
Grad=  tensor(0.0242, device='cuda:0')
Epoch: [389][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [389][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [389][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2382 (0.2382) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-04
Grad=  tensor(0.0053, device='cuda:0')
Epoch: [390][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [390][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [390][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0080 (0.0020) ([0.008]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2528 (0.2528) ([0.253]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.760
current lr 1.00000e-04
Grad=  tensor(0.0026, device='cuda:0')
Epoch: [391][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [391][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [391][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2663 (0.2663) ([0.266]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-04
Grad=  tensor(0.0130, device='cuda:0')
Epoch: [392][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [392][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2635 (0.2635) ([0.263]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-04
Grad=  tensor(0.1063, device='cuda:0')
Epoch: [393][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2433 (0.2433) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-04
Grad=  tensor(0.0035, device='cuda:0')
Epoch: [394][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [394][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [394][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2342 (0.2342) ([0.234]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-04
Grad=  tensor(0.0519, device='cuda:0')
Epoch: [395][0/391]	Time 0.165 (0.165)	Data 0.138 (0.138)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [395][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2554 (0.2554) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-04
Grad=  tensor(0.1315, device='cuda:0')
Epoch: [396][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [396][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [396][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2453 (0.2453) ([0.245]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-04
Grad=  tensor(0.0017, device='cuda:0')
Epoch: [397][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0035 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2490 (0.2490) ([0.249]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-04
Grad=  tensor(0.0246, device='cuda:0')
Epoch: [398][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [398][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [398][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2429 (0.2429) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-04
Grad=  tensor(0.0027, device='cuda:0')
Epoch: [399][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [399][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [399][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0030 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2385 (0.2385) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.0051, device='cuda:0')
Epoch: [400][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [400][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [400][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2419 (0.2419) ([0.242]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-05
Grad=  tensor(0.1072, device='cuda:0')
Epoch: [401][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0034 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [401][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [401][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2443 (0.2443) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-05
Grad=  tensor(0.0859, device='cuda:0')
Epoch: [402][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [402][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [402][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2338 (0.2338) ([0.234]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.0178, device='cuda:0')
Epoch: [403][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [403][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2375 (0.2375) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.1567, device='cuda:0')
Epoch: [404][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [404][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2337 (0.2337) ([0.234]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-05
Grad=  tensor(0.0018, device='cuda:0')
Epoch: [405][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [405][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [405][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2553 (0.2553) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-05
Grad=  tensor(0.0213, device='cuda:0')
Epoch: [406][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [406][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2577 (0.2577) ([0.258]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-05
Grad=  tensor(0.0007, device='cuda:0')
Epoch: [407][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2493 (0.2493) ([0.249]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-05
Grad=  tensor(0.0103, device='cuda:0')
Epoch: [408][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [408][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0063 (0.0019) ([0.006]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [408][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2405 (0.2405) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-05
Grad=  tensor(0.0034, device='cuda:0')
Epoch: [409][0/391]	Time 0.177 (0.177)	Data 0.151 (0.151)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.019 (0.021)	Data 0.000 (0.002)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [409][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [409][300/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2520 (0.2520) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-05
Grad=  tensor(0.0159, device='cuda:0')
Epoch: [410][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [410][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [410][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2370 (0.2370) ([0.237]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.0031, device='cuda:0')
Epoch: [411][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [411][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2357 (0.2357) ([0.236]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.790
current lr 1.00000e-05
Grad=  tensor(0.0010, device='cuda:0')
Epoch: [412][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [412][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0118 (0.0019) ([0.012]+[0.000])	Prec@1 99.219 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2311 (0.2311) ([0.231]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.680
current lr 1.00000e-05
Grad=  tensor(0.0205, device='cuda:0')
Epoch: [413][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [413][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [413][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2190 (0.2190) ([0.219]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-05
Grad=  tensor(0.0278, device='cuda:0')
Epoch: [414][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2273 (0.2273) ([0.227]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.640
current lr 1.00000e-05
Grad=  tensor(0.0513, device='cuda:0')
Epoch: [415][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [415][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [415][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2391 (0.2391) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.0677, device='cuda:0')
Epoch: [416][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [416][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [416][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2388 (0.2388) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0068, device='cuda:0')
Epoch: [417][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [417][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2440 (0.2440) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-05
Grad=  tensor(0.0014, device='cuda:0')
Epoch: [418][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [418][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [418][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0037 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2288 (0.2288) ([0.229]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.770
current lr 1.00000e-05
Grad=  tensor(0.0038, device='cuda:0')
Epoch: [419][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0022) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [419][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [419][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0022) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2319 (0.2319) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0425, device='cuda:0')
Epoch: [420][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [420][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0042 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2531 (0.2531) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.0041, device='cuda:0')
Epoch: [421][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [421][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [421][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2402 (0.2402) ([0.240]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0116, device='cuda:0')
Epoch: [422][0/391]	Time 0.154 (0.154)	Data 0.128 (0.128)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2291 (0.2291) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-05
Grad=  tensor(0.0260, device='cuda:0')
Epoch: [423][0/391]	Time 0.156 (0.156)	Data 0.131 (0.131)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [423][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [423][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2269 (0.2269) ([0.227]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.0094, device='cuda:0')
Epoch: [424][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [424][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [424][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2366 (0.2366) ([0.237]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.0132, device='cuda:0')
Epoch: [425][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2254 (0.2254) ([0.225]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-05
Grad=  tensor(0.0230, device='cuda:0')
Epoch: [426][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [426][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [426][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0029 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2533 (0.2533) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.0069, device='cuda:0')
Epoch: [427][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [427][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [427][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2291 (0.2291) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0114, device='cuda:0')
Epoch: [428][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [428][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [428][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2323 (0.2323) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.5448, device='cuda:0')
Epoch: [429][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0040 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0036 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2283 (0.2283) ([0.228]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.660
current lr 1.00000e-05
Grad=  tensor(0.0191, device='cuda:0')
Epoch: [430][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [430][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [430][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0045 (0.0019) ([0.005]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2313 (0.2313) ([0.231]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.0064, device='cuda:0')
Epoch: [431][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [431][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [431][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2500 (0.2500) ([0.250]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.0111, device='cuda:0')
Epoch: [432][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2444 (0.2444) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.0054, device='cuda:0')
Epoch: [433][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [433][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [433][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2303 (0.2303) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-05
Grad=  tensor(0.0047, device='cuda:0')
Epoch: [434][0/391]	Time 0.151 (0.151)	Data 0.124 (0.124)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [434][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0033 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2302 (0.2302) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.0006, device='cuda:0')
Epoch: [435][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [435][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [435][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2331 (0.2331) ([0.233]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-05
Grad=  tensor(0.0179, device='cuda:0')
Epoch: [436][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [436][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [436][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2354 (0.2354) ([0.235]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.1878, device='cuda:0')
Epoch: [437][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [437][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [437][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0038 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2390 (0.2390) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.3580, device='cuda:0')
Epoch: [438][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [438][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [438][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2238 (0.2238) ([0.224]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-05
Grad=  tensor(0.1666, device='cuda:0')
Epoch: [439][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [439][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [439][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2497 (0.2497) ([0.250]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-05
Grad=  tensor(0.2915, device='cuda:0')
Epoch: [440][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [440][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [440][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2582 (0.2582) ([0.258]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-05
Grad=  tensor(0.0176, device='cuda:0')
Epoch: [441][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [441][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2475 (0.2475) ([0.248]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.3590, device='cuda:0')
Epoch: [442][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [442][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2234 (0.2234) ([0.223]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-05
Grad=  tensor(0.0084, device='cuda:0')
Epoch: [443][0/391]	Time 0.149 (0.149)	Data 0.123 (0.123)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0044 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2292 (0.2292) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0084, device='cuda:0')
Epoch: [444][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [444][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2331 (0.2331) ([0.233]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-05
Grad=  tensor(0.0107, device='cuda:0')
Epoch: [445][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0034 (0.0017) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [445][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [445][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2511 (0.2511) ([0.251]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.740
current lr 1.00000e-05
Grad=  tensor(0.0070, device='cuda:0')
Epoch: [446][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [446][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [446][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2401 (0.2401) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-05
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [447][0/391]	Time 0.152 (0.152)	Data 0.127 (0.127)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [447][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2416 (0.2416) ([0.242]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-05
Grad=  tensor(0.0433, device='cuda:0')
Epoch: [448][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [448][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [448][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2323 (0.2323) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-05
Grad=  tensor(0.0044, device='cuda:0')
Epoch: [449][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [449][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [449][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0022 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.116 (0.116)	Loss 0.2403 (0.2403) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0211, device='cuda:0')
Epoch: [450][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [450][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [450][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0031 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2457 (0.2457) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-06
Grad=  tensor(0.0193, device='cuda:0')
Epoch: [451][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [451][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [451][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2544 (0.2544) ([0.254]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-06
Grad=  tensor(0.0075, device='cuda:0')
Epoch: [452][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [452][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [452][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2389 (0.2389) ([0.239]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.630
current lr 1.00000e-06
Grad=  tensor(0.0029, device='cuda:0')
Epoch: [453][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [453][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0033 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [453][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2398 (0.2398) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.730
current lr 1.00000e-06
Grad=  tensor(0.1258, device='cuda:0')
Epoch: [454][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [454][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2268 (0.2268) ([0.227]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.660
current lr 1.00000e-06
Grad=  tensor(0.0185, device='cuda:0')
Epoch: [455][0/391]	Time 0.152 (0.152)	Data 0.128 (0.128)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [455][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0039 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [455][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2527 (0.2527) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.570
current lr 1.00000e-06
Grad=  tensor(0.0179, device='cuda:0')
Epoch: [456][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0026 (0.0018) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2405 (0.2405) ([0.241]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-06
Grad=  tensor(0.0850, device='cuda:0')
Epoch: [457][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [457][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2353 (0.2353) ([0.235]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.770
current lr 1.00000e-06
Grad=  tensor(0.0158, device='cuda:0')
Epoch: [458][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [458][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.117 (0.117)	Loss 0.2326 (0.2326) ([0.233]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.0552, device='cuda:0')
Epoch: [459][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [459][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [459][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2310 (0.2310) ([0.231]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.0374, device='cuda:0')
Epoch: [460][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0026 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [460][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2322 (0.2322) ([0.232]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.0104, device='cuda:0')
Epoch: [461][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [461][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [461][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2403 (0.2403) ([0.240]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.0426, device='cuda:0')
Epoch: [462][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [462][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [462][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2459 (0.2459) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-06
Grad=  tensor(0.0070, device='cuda:0')
Epoch: [463][0/391]	Time 0.153 (0.153)	Data 0.129 (0.129)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [463][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2635 (0.2635) ([0.264]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.0080, device='cuda:0')
Epoch: [464][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0016) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [464][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2384 (0.2384) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0008, device='cuda:0')
Epoch: [465][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0007 (0.0007) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [465][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2577 (0.2577) ([0.258]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.720
current lr 1.00000e-06
Grad=  tensor(0.0609, device='cuda:0')
Epoch: [466][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [466][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [466][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0028 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2264 (0.2264) ([0.226]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.760
current lr 1.00000e-06
Grad=  tensor(0.0307, device='cuda:0')
Epoch: [467][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [467][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [467][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0016 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2285 (0.2285) ([0.228]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.650
current lr 1.00000e-06
Grad=  tensor(0.0989, device='cuda:0')
Epoch: [468][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [468][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [468][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0032 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2289 (0.2289) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.2964, device='cuda:0')
Epoch: [469][0/391]	Time 0.151 (0.151)	Data 0.127 (0.127)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [469][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [469][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2297 (0.2297) ([0.230]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.0209, device='cuda:0')
Epoch: [470][0/391]	Time 0.151 (0.151)	Data 0.127 (0.127)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0036 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [470][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [470][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0023 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2378 (0.2378) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.0028, device='cuda:0')
Epoch: [471][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0023 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [471][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [471][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0051 (0.0020) ([0.005]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2520 (0.2520) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-06
Grad=  tensor(0.0652, device='cuda:0')
Epoch: [472][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [472][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2404 (0.2404) ([0.240]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.650
current lr 1.00000e-06
Grad=  tensor(0.0031, device='cuda:0')
Epoch: [473][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0038 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [473][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0030 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [473][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.2428 (0.2428) ([0.243]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.630
current lr 1.00000e-06
Grad=  tensor(0.0022, device='cuda:0')
Epoch: [474][0/391]	Time 0.151 (0.151)	Data 0.125 (0.125)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0038 (0.0021) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [474][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [474][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2349 (0.2349) ([0.235]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.690
current lr 1.00000e-06
Grad=  tensor(0.0667, device='cuda:0')
Epoch: [475][0/391]	Time 0.153 (0.153)	Data 0.129 (0.129)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0022 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [475][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [475][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0017 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2412 (0.2412) ([0.241]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0097, device='cuda:0')
Epoch: [476][0/391]	Time 0.153 (0.153)	Data 0.128 (0.128)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [476][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [476][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2419 (0.2419) ([0.242]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-06
Grad=  tensor(0.0019, device='cuda:0')
Epoch: [477][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0040 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [477][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0031 (0.0019) ([0.003]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [477][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2312 (0.2312) ([0.231]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.820
current lr 1.00000e-06
Grad=  tensor(0.0658, device='cuda:0')
Epoch: [478][0/391]	Time 0.159 (0.159)	Data 0.134 (0.134)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0027 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0043 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.124 (0.124)	Loss 0.2289 (0.2289) ([0.229]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.0003, device='cuda:0')
Epoch: [479][0/391]	Time 0.156 (0.156)	Data 0.130 (0.130)	Loss 0.0006 (0.0006) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Test: [0/79]	Time 0.126 (0.126)	Loss 0.2531 (0.2531) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.0163, device='cuda:0')
Epoch: [480][0/391]	Time 0.156 (0.156)	Data 0.131 (0.131)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [480][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [480][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0007 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.127 (0.127)	Loss 0.2304 (0.2304) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.750
current lr 1.00000e-06
Grad=  tensor(0.0080, device='cuda:0')
Epoch: [481][0/391]	Time 0.158 (0.158)	Data 0.133 (0.133)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0024 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [481][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0046 (0.0019) ([0.005]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2547 (0.2547) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(0.1172, device='cuda:0')
Epoch: [482][0/391]	Time 0.156 (0.156)	Data 0.131 (0.131)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [482][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0010 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [482][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0028 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2524 (0.2524) ([0.252]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-06
Grad=  tensor(0.0168, device='cuda:0')
Epoch: [483][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0015 (0.0015) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [483][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [483][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0021) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2297 (0.2297) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0012, device='cuda:0')
Epoch: [484][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0009 (0.0009) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [484][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [484][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2373 (0.2373) ([0.237]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.700
current lr 1.00000e-06
Grad=  tensor(0.0708, device='cuda:0')
Epoch: [485][0/391]	Time 0.154 (0.154)	Data 0.129 (0.129)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [485][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0009 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [485][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2383 (0.2383) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.620
current lr 1.00000e-06
Grad=  tensor(0.0010, device='cuda:0')
Epoch: [486][0/391]	Time 0.155 (0.155)	Data 0.130 (0.130)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0007 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0052 (0.0017) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0011 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2464 (0.2464) ([0.246]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-06
Grad=  tensor(0.0203, device='cuda:0')
Epoch: [487][0/391]	Time 0.155 (0.155)	Data 0.129 (0.129)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2377 (0.2377) ([0.238]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.630
current lr 1.00000e-06
Grad=  tensor(0.0078, device='cuda:0')
Epoch: [488][0/391]	Time 0.157 (0.157)	Data 0.132 (0.132)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0016 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.119 (0.119)	Loss 0.2544 (0.2544) ([0.254]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-06
Grad=  tensor(0.0025, device='cuda:0')
Epoch: [489][0/391]	Time 0.165 (0.165)	Data 0.140 (0.140)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0021 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [489][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [489][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0013 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2299 (0.2299) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.620
current lr 1.00000e-06
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [490][0/391]	Time 0.149 (0.149)	Data 0.124 (0.124)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [490][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [490][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2390 (0.2390) ([0.239]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.770
current lr 1.00000e-06
Grad=  tensor(0.0066, device='cuda:0')
Epoch: [491][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [491][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0014 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [491][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2546 (0.2546) ([0.255]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-06
Grad=  tensor(0.0087, device='cuda:0')
Epoch: [492][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0028 (0.0020) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [492][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [492][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.120 (0.120)	Loss 0.2448 (0.2448) ([0.245]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0086, device='cuda:0')
Epoch: [493][0/391]	Time 0.150 (0.150)	Data 0.124 (0.124)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0029 (0.0021) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [493][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [493][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0041 (0.0018) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2407 (0.2407) ([0.241]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.700
current lr 1.00000e-06
Grad=  tensor(0.0688, device='cuda:0')
Epoch: [494][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [494][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0041 (0.0019) ([0.004]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [494][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0008 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.122 (0.122)	Loss 0.2221 (0.2221) ([0.222]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.660
current lr 1.00000e-06
Grad=  tensor(0.0317, device='cuda:0')
Epoch: [495][0/391]	Time 0.152 (0.152)	Data 0.126 (0.126)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0012 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [495][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0018 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [495][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0012 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2301 (0.2301) ([0.230]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.710
current lr 1.00000e-06
Grad=  tensor(7.8041, device='cuda:0')
Epoch: [496][0/391]	Time 0.153 (0.153)	Data 0.127 (0.127)	Loss 0.0138 (0.0138) ([0.014]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [496][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0011 (0.0020) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [496][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [496][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.118 (0.118)	Loss 0.2442 (0.2442) ([0.244]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.810
current lr 1.00000e-06
Grad=  tensor(0.0300, device='cuda:0')
Epoch: [497][0/391]	Time 0.151 (0.151)	Data 0.126 (0.126)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0008 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0025 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [497][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2497 (0.2497) ([0.250]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.670
current lr 1.00000e-06
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [498][0/391]	Time 0.150 (0.150)	Data 0.125 (0.125)	Loss 0.0008 (0.0008) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [498][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0013 (0.0018) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [498][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0015 (0.0019) ([0.001]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.123 (0.123)	Loss 0.2530 (0.2530) ([0.253]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.680
current lr 1.00000e-06
Grad=  tensor(0.0483, device='cuda:0')
Epoch: [499][0/391]	Time 0.156 (0.156)	Data 0.129 (0.129)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [499][200/391]	Time 0.019 (0.020)	Data 0.000 (0.001)	Loss 0.0015 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [499][300/391]	Time 0.019 (0.019)	Data 0.000 (0.001)	Loss 0.0010 (0.0017) ([0.001]+[0.000])	Prec@1 100.000 (99.997)
Test: [0/79]	Time 0.121 (0.121)	Loss 0.2322 (0.2322) ([0.232]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.640

 Elapsed time for training  1:45:49.534328

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.328125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.017578125, 0.01953125, 0.017578125, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375, 0.2109375]
Total parameter pruned: 9589592.0 (unstructured) 9578980 (structured)
Test: [0/79]	Time 0.125 (0.125)	Loss 0.2322 (0.2322) ([0.232]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.640
Best accuracy:  94.82
