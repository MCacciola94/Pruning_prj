V0.0.1-_vgg16_Cifar10_lr0.1_l1.0_a0.3_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp6_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.6437135934829712, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.3027346432209015, Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.1490456759929657, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.13979694247245789, Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.20777323842048645, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.10516299307346344, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.10126874595880508, Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.09506335854530334, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.05375269055366516, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.050536274909973145, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.04137488082051277, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.027590204030275345, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)): 0.03216041252017021, Linear(in_features=512, out_features=10, bias=True): 0.3597226142883301}
current lr 1.00000e-01
Grad=  tensor(67.4607, device='cuda:0')
Epoch: [0][0/391]	Time 0.225 (0.225)	Data 0.192 (0.192)	Loss 2.8668 (2.8668) ([2.302]+[0.565])	Prec@1 12.500 (12.500)
Epoch: [0][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 2.7154 (2.9916) ([2.015]+[0.701])	Prec@1 17.188 (15.006)
Epoch: [0][200/391]	Time 0.030 (0.026)	Data 0.000 (0.001)	Loss 2.4924 (2.8201) ([1.886]+[0.607])	Prec@1 32.031 (18.046)
Epoch: [0][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 2.3808 (2.7073) ([1.866]+[0.515])	Prec@1 28.906 (19.718)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.2123 (2.2123) ([1.774]+[0.438])	Prec@1 33.594 (33.594)
 * Prec@1 26.680
current lr 1.00000e-01
Grad=  tensor(0.3426, device='cuda:0')
Epoch: [1][0/391]	Time 0.221 (0.221)	Data 0.190 (0.190)	Loss 2.3590 (2.3590) ([1.921]+[0.438])	Prec@1 24.219 (24.219)
Epoch: [1][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 2.0898 (2.1638) ([1.721]+[0.369])	Prec@1 35.156 (31.320)
Epoch: [1][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.8874 (2.0860) ([1.579]+[0.309])	Prec@1 35.156 (33.547)
Epoch: [1][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.9307 (2.0229) ([1.671]+[0.260])	Prec@1 34.375 (34.749)
Test: [0/79]	Time 0.183 (0.183)	Loss 1.9590 (1.9590) ([1.728]+[0.231])	Prec@1 37.500 (37.500)
 * Prec@1 36.100
current lr 1.00000e-01
Grad=  tensor(1.1470, device='cuda:0')
Epoch: [2][0/391]	Time 0.216 (0.216)	Data 0.184 (0.184)	Loss 1.7969 (1.7969) ([1.566]+[0.231])	Prec@1 43.750 (43.750)
Epoch: [2][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 1.6350 (1.6995) ([1.417]+[0.218])	Prec@1 45.312 (44.284)
Epoch: [2][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.8052 (1.6610) ([1.587]+[0.218])	Prec@1 40.625 (46.447)
Epoch: [2][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.4638 (1.6254) ([1.240]+[0.223])	Prec@1 58.594 (48.126)
Test: [0/79]	Time 0.174 (0.174)	Loss 1.4697 (1.4697) ([1.248]+[0.222])	Prec@1 50.000 (50.000)
 * Prec@1 55.170
current lr 1.00000e-01
Grad=  tensor(1.9882, device='cuda:0')
Epoch: [3][0/391]	Time 0.211 (0.211)	Data 0.177 (0.177)	Loss 1.5047 (1.5047) ([1.283]+[0.222])	Prec@1 54.688 (54.688)
Epoch: [3][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 1.6196 (1.4426) ([1.389]+[0.231])	Prec@1 53.125 (57.441)
Epoch: [3][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.2973 (1.4077) ([1.062]+[0.235])	Prec@1 59.375 (58.897)
Epoch: [3][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 1.2375 (1.3809) ([1.001]+[0.237])	Prec@1 67.188 (60.078)
Test: [0/79]	Time 0.218 (0.218)	Loss 1.4271 (1.4271) ([1.191]+[0.237])	Prec@1 54.688 (54.688)
 * Prec@1 60.270
current lr 1.00000e-01
Grad=  tensor(2.2174, device='cuda:0')
Epoch: [4][0/391]	Time 0.250 (0.250)	Data 0.216 (0.216)	Loss 1.5134 (1.5134) ([1.277]+[0.237])	Prec@1 57.031 (57.031)
Epoch: [4][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 1.2625 (1.2700) ([1.016]+[0.246])	Prec@1 65.625 (64.318)
Epoch: [4][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 1.2847 (1.2504) ([1.042]+[0.243])	Prec@1 62.500 (65.256)
Epoch: [4][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 1.3490 (1.2325) ([1.105]+[0.244])	Prec@1 61.719 (66.123)
Test: [0/79]	Time 0.193 (0.193)	Loss 1.6482 (1.6482) ([1.398]+[0.250])	Prec@1 53.125 (53.125)
 * Prec@1 55.730
current lr 1.00000e-01
Grad=  tensor(1.7840, device='cuda:0')
Epoch: [5][0/391]	Time 0.226 (0.226)	Data 0.197 (0.197)	Loss 1.3158 (1.3158) ([1.065]+[0.250])	Prec@1 60.938 (60.938)
Epoch: [5][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 1.1115 (1.1515) ([0.859]+[0.253])	Prec@1 69.531 (69.810)
Epoch: [5][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.1195 (1.1407) ([0.866]+[0.253])	Prec@1 67.188 (70.134)
Epoch: [5][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0927 (1.1318) ([0.838]+[0.254])	Prec@1 71.094 (70.634)
Test: [0/79]	Time 0.217 (0.217)	Loss 1.5530 (1.5530) ([1.299]+[0.254])	Prec@1 56.250 (56.250)
 * Prec@1 59.710
current lr 1.00000e-01
Grad=  tensor(1.8128, device='cuda:0')
Epoch: [6][0/391]	Time 0.251 (0.251)	Data 0.217 (0.217)	Loss 1.0843 (1.0843) ([0.830]+[0.254])	Prec@1 71.875 (71.875)
Epoch: [6][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 1.1518 (1.1170) ([0.896]+[0.256])	Prec@1 71.094 (71.109)
Epoch: [6][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0832 (1.0982) ([0.826]+[0.257])	Prec@1 68.750 (71.941)
Epoch: [6][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8951 (1.0861) ([0.638]+[0.257])	Prec@1 79.688 (72.441)
Test: [0/79]	Time 0.184 (0.184)	Loss 1.2685 (1.2685) ([1.010]+[0.259])	Prec@1 69.531 (69.531)
 * Prec@1 70.080
current lr 1.00000e-01
Grad=  tensor(1.8744, device='cuda:0')
Epoch: [7][0/391]	Time 0.214 (0.214)	Data 0.185 (0.185)	Loss 1.0399 (1.0399) ([0.781]+[0.259])	Prec@1 73.438 (73.438)
Epoch: [7][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 1.0351 (1.0276) ([0.779]+[0.256])	Prec@1 71.875 (74.497)
Epoch: [7][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 1.1116 (1.0179) ([0.857]+[0.255])	Prec@1 70.312 (74.949)
Epoch: [7][300/391]	Time 0.026 (0.024)	Data 0.000 (0.001)	Loss 1.1606 (1.0211) ([0.905]+[0.255])	Prec@1 69.531 (74.873)
Test: [0/79]	Time 0.181 (0.181)	Loss 1.0855 (1.0855) ([0.828]+[0.258])	Prec@1 73.438 (73.438)
 * Prec@1 69.680
current lr 1.00000e-01
Grad=  tensor(1.7652, device='cuda:0')
Epoch: [8][0/391]	Time 0.214 (0.214)	Data 0.182 (0.182)	Loss 0.8814 (0.8814) ([0.624]+[0.258])	Prec@1 78.906 (78.906)
Epoch: [8][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.9784 (0.9821) ([0.724]+[0.254])	Prec@1 73.438 (76.454)
Epoch: [8][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.9380 (0.9842) ([0.682]+[0.256])	Prec@1 78.906 (76.259)
Epoch: [8][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.0415 (0.9863) ([0.782]+[0.260])	Prec@1 76.562 (76.163)
Test: [0/79]	Time 0.191 (0.191)	Loss 1.7677 (1.7677) ([1.507]+[0.260])	Prec@1 59.375 (59.375)
 * Prec@1 63.120
current lr 1.00000e-01
Grad=  tensor(2.5502, device='cuda:0')
Epoch: [9][0/391]	Time 0.254 (0.254)	Data 0.224 (0.224)	Loss 1.0058 (1.0058) ([0.745]+[0.260])	Prec@1 75.781 (75.781)
Epoch: [9][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.9595 (0.9683) ([0.705]+[0.255])	Prec@1 72.656 (76.934)
Epoch: [9][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.8543 (0.9559) ([0.598]+[0.256])	Prec@1 78.906 (77.278)
Epoch: [9][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8376 (0.9644) ([0.582]+[0.256])	Prec@1 82.031 (77.006)
Test: [0/79]	Time 0.191 (0.191)	Loss 1.0448 (1.0448) ([0.789]+[0.256])	Prec@1 78.125 (78.125)
 * Prec@1 71.310
current lr 1.00000e-01
Grad=  tensor(2.4582, device='cuda:0')
Epoch: [10][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.9705 (0.9705) ([0.715]+[0.256])	Prec@1 75.781 (75.781)
Epoch: [10][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.9091 (0.9415) ([0.655]+[0.254])	Prec@1 76.562 (77.452)
Epoch: [10][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.8341 (0.9357) ([0.580]+[0.254])	Prec@1 82.812 (77.729)
Epoch: [10][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.8492 (0.9335) ([0.599]+[0.251])	Prec@1 77.344 (77.829)
Test: [0/79]	Time 0.210 (0.210)	Loss 1.1062 (1.1062) ([0.854]+[0.252])	Prec@1 71.094 (71.094)
 * Prec@1 71.310
current lr 1.00000e-01
Grad=  tensor(1.5504, device='cuda:0')
Epoch: [11][0/391]	Time 0.239 (0.239)	Data 0.206 (0.206)	Loss 0.9288 (0.9288) ([0.676]+[0.252])	Prec@1 75.000 (75.000)
Epoch: [11][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 1.1034 (0.8884) ([0.853]+[0.251])	Prec@1 72.656 (79.695)
Epoch: [11][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8821 (0.9092) ([0.629]+[0.253])	Prec@1 79.688 (78.933)
Epoch: [11][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8500 (0.9162) ([0.598]+[0.252])	Prec@1 79.688 (78.616)
Test: [0/79]	Time 0.185 (0.185)	Loss 1.2396 (1.2396) ([0.989]+[0.251])	Prec@1 65.625 (65.625)
 * Prec@1 70.990
current lr 1.00000e-01
Grad=  tensor(2.2237, device='cuda:0')
Epoch: [12][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.9130 (0.9130) ([0.662]+[0.251])	Prec@1 79.688 (79.688)
Epoch: [12][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.9242 (0.8981) ([0.674]+[0.250])	Prec@1 80.469 (78.953)
Epoch: [12][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9915 (0.8927) ([0.744]+[0.248])	Prec@1 75.781 (79.085)
Epoch: [12][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9268 (0.8940) ([0.678]+[0.249])	Prec@1 78.906 (79.046)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.0662 (1.0662) ([0.818]+[0.248])	Prec@1 68.750 (68.750)
 * Prec@1 72.610
current lr 1.00000e-01
Grad=  tensor(1.5867, device='cuda:0')
Epoch: [13][0/391]	Time 0.208 (0.208)	Data 0.169 (0.169)	Loss 0.7772 (0.7772) ([0.529]+[0.248])	Prec@1 83.594 (83.594)
Epoch: [13][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.9535 (0.8845) ([0.707]+[0.247])	Prec@1 78.125 (79.425)
Epoch: [13][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 1.0496 (0.8786) ([0.804]+[0.246])	Prec@1 78.906 (79.559)
Epoch: [13][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 1.1222 (0.8831) ([0.876]+[0.246])	Prec@1 72.656 (79.368)
Test: [0/79]	Time 0.175 (0.175)	Loss 1.3086 (1.3086) ([1.066]+[0.243])	Prec@1 66.406 (66.406)
 * Prec@1 65.760
current lr 1.00000e-01
Grad=  tensor(1.2055, device='cuda:0')
Epoch: [14][0/391]	Time 0.209 (0.209)	Data 0.178 (0.178)	Loss 0.7862 (0.7862) ([0.543]+[0.243])	Prec@1 83.594 (83.594)
Epoch: [14][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.9835 (0.8777) ([0.740]+[0.244])	Prec@1 74.219 (79.479)
Epoch: [14][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6320 (0.8733) ([0.389]+[0.243])	Prec@1 86.719 (79.688)
Epoch: [14][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 1.0137 (0.8660) ([0.771]+[0.243])	Prec@1 75.000 (79.976)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.9387 (0.9387) ([0.697]+[0.242])	Prec@1 79.688 (79.688)
 * Prec@1 78.850
current lr 1.00000e-01
Grad=  tensor(1.3393, device='cuda:0')
Epoch: [15][0/391]	Time 0.217 (0.217)	Data 0.178 (0.178)	Loss 0.7541 (0.7541) ([0.512]+[0.242])	Prec@1 86.719 (86.719)
Epoch: [15][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8209 (0.8582) ([0.577]+[0.243])	Prec@1 82.031 (80.399)
Epoch: [15][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8457 (0.8538) ([0.605]+[0.241])	Prec@1 80.469 (80.535)
Epoch: [15][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.8903 (0.8574) ([0.650]+[0.240])	Prec@1 78.125 (80.344)
Test: [0/79]	Time 0.197 (0.197)	Loss 1.0621 (1.0621) ([0.824]+[0.239])	Prec@1 71.094 (71.094)
 * Prec@1 73.210
current lr 1.00000e-01
Grad=  tensor(2.8267, device='cuda:0')
Epoch: [16][0/391]	Time 0.214 (0.214)	Data 0.184 (0.184)	Loss 0.8904 (0.8904) ([0.652]+[0.239])	Prec@1 77.344 (77.344)
Epoch: [16][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.9449 (0.8497) ([0.705]+[0.240])	Prec@1 73.438 (79.997)
Epoch: [16][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9645 (0.8518) ([0.727]+[0.238])	Prec@1 71.094 (80.049)
Epoch: [16][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7373 (0.8537) ([0.500]+[0.237])	Prec@1 82.812 (80.087)
Test: [0/79]	Time 0.191 (0.191)	Loss 1.5646 (1.5646) ([1.328]+[0.237])	Prec@1 59.375 (59.375)
 * Prec@1 61.540
current lr 1.00000e-01
Grad=  tensor(2.5484, device='cuda:0')
Epoch: [17][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.8092 (0.8092) ([0.572]+[0.237])	Prec@1 82.031 (82.031)
Epoch: [17][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.7510 (0.8376) ([0.514]+[0.237])	Prec@1 82.812 (80.523)
Epoch: [17][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.8101 (0.8335) ([0.574]+[0.236])	Prec@1 82.812 (80.842)
Epoch: [17][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 1.0054 (0.8308) ([0.770]+[0.235])	Prec@1 74.219 (80.858)
Test: [0/79]	Time 0.207 (0.207)	Loss 1.1462 (1.1462) ([0.911]+[0.235])	Prec@1 73.438 (73.438)
 * Prec@1 74.010
current lr 1.00000e-01
Grad=  tensor(1.5872, device='cuda:0')
Epoch: [18][0/391]	Time 0.217 (0.217)	Data 0.178 (0.178)	Loss 0.7210 (0.7210) ([0.485]+[0.235])	Prec@1 82.812 (82.812)
Epoch: [18][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7229 (0.7997) ([0.489]+[0.234])	Prec@1 82.812 (81.660)
Epoch: [18][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8755 (0.8082) ([0.642]+[0.234])	Prec@1 75.000 (81.297)
Epoch: [18][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8435 (0.8013) ([0.613]+[0.231])	Prec@1 82.812 (81.559)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.5797 (1.5797) ([1.348]+[0.232])	Prec@1 59.375 (59.375)
 * Prec@1 64.780
current lr 1.00000e-01
Grad=  tensor(2.1483, device='cuda:0')
Epoch: [19][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.9012 (0.9012) ([0.669]+[0.232])	Prec@1 80.469 (80.469)
Epoch: [19][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.8773 (0.8283) ([0.643]+[0.234])	Prec@1 80.469 (80.569)
Epoch: [19][200/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.7157 (0.8273) ([0.483]+[0.232])	Prec@1 85.156 (80.772)
Epoch: [19][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7464 (0.8257) ([0.514]+[0.232])	Prec@1 84.375 (80.793)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.6969 (0.6969) ([0.467]+[0.230])	Prec@1 85.938 (85.938)
 * Prec@1 81.010
current lr 1.00000e-01
Grad=  tensor(2.1510, device='cuda:0')
Epoch: [20][0/391]	Time 0.213 (0.213)	Data 0.182 (0.182)	Loss 0.7482 (0.7482) ([0.519]+[0.230])	Prec@1 85.156 (85.156)
Epoch: [20][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7364 (0.7864) ([0.509]+[0.228])	Prec@1 85.938 (81.598)
Epoch: [20][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8757 (0.8016) ([0.647]+[0.229])	Prec@1 78.906 (81.359)
Epoch: [20][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8257 (0.8122) ([0.596]+[0.230])	Prec@1 80.469 (81.208)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.2785 (1.2785) ([1.049]+[0.229])	Prec@1 70.312 (70.312)
 * Prec@1 69.350
current lr 1.00000e-01
Grad=  tensor(1.8488, device='cuda:0')
Epoch: [21][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.6593 (0.6593) ([0.430]+[0.229])	Prec@1 84.375 (84.375)
Epoch: [21][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8427 (0.7968) ([0.614]+[0.229])	Prec@1 78.906 (81.583)
Epoch: [21][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6501 (0.8021) ([0.422]+[0.228])	Prec@1 87.500 (81.332)
Epoch: [21][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9649 (0.8046) ([0.737]+[0.228])	Prec@1 76.562 (81.351)
Test: [0/79]	Time 0.228 (0.228)	Loss 0.9876 (0.9876) ([0.761]+[0.226])	Prec@1 73.438 (73.438)
 * Prec@1 77.670
current lr 1.00000e-01
Grad=  tensor(2.2211, device='cuda:0')
Epoch: [22][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.7061 (0.7061) ([0.480]+[0.226])	Prec@1 84.375 (84.375)
Epoch: [22][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7614 (0.7731) ([0.534]+[0.227])	Prec@1 83.594 (82.457)
Epoch: [22][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6511 (0.7735) ([0.425]+[0.226])	Prec@1 83.594 (82.482)
Epoch: [22][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6652 (0.7782) ([0.441]+[0.224])	Prec@1 85.156 (82.327)
Test: [0/79]	Time 0.187 (0.187)	Loss 1.2787 (1.2787) ([1.054]+[0.225])	Prec@1 68.750 (68.750)
 * Prec@1 72.860
current lr 1.00000e-01
Grad=  tensor(1.7835, device='cuda:0')
Epoch: [23][0/391]	Time 0.197 (0.197)	Data 0.168 (0.168)	Loss 0.6929 (0.6929) ([0.468]+[0.225])	Prec@1 83.594 (83.594)
Epoch: [23][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7316 (0.7581) ([0.508]+[0.223])	Prec@1 85.156 (83.083)
Epoch: [23][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.6826 (0.7670) ([0.459]+[0.224])	Prec@1 85.938 (82.513)
Epoch: [23][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6571 (0.7723) ([0.434]+[0.223])	Prec@1 88.281 (82.317)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.8735 (0.8735) ([0.649]+[0.224])	Prec@1 75.781 (75.781)
 * Prec@1 76.980
current lr 1.00000e-01
Grad=  tensor(1.4203, device='cuda:0')
Epoch: [24][0/391]	Time 0.240 (0.240)	Data 0.202 (0.202)	Loss 0.6295 (0.6295) ([0.405]+[0.224])	Prec@1 85.938 (85.938)
Epoch: [24][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.9640 (0.7914) ([0.739]+[0.225])	Prec@1 75.781 (81.505)
Epoch: [24][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7389 (0.7852) ([0.515]+[0.224])	Prec@1 85.156 (81.860)
Epoch: [24][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7426 (0.7801) ([0.519]+[0.224])	Prec@1 85.156 (82.049)
Test: [0/79]	Time 0.195 (0.195)	Loss 1.1347 (1.1347) ([0.912]+[0.223])	Prec@1 72.656 (72.656)
 * Prec@1 73.110
current lr 1.00000e-01
Grad=  tensor(2.1716, device='cuda:0')
Epoch: [25][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.7888 (0.7888) ([0.566]+[0.223])	Prec@1 82.031 (82.031)
Epoch: [25][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7403 (0.7772) ([0.517]+[0.223])	Prec@1 82.031 (81.877)
Epoch: [25][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7050 (0.7707) ([0.484]+[0.221])	Prec@1 82.031 (82.206)
Epoch: [25][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8106 (0.7739) ([0.590]+[0.221])	Prec@1 77.344 (82.112)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.9706 (0.9706) ([0.751]+[0.219])	Prec@1 75.000 (75.000)
 * Prec@1 73.120
current lr 1.00000e-01
Grad=  tensor(1.7166, device='cuda:0')
Epoch: [26][0/391]	Time 0.239 (0.239)	Data 0.207 (0.207)	Loss 0.8014 (0.8014) ([0.582]+[0.219])	Prec@1 81.250 (81.250)
Epoch: [26][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.9552 (0.7593) ([0.736]+[0.219])	Prec@1 77.344 (82.681)
Epoch: [26][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6550 (0.7605) ([0.436]+[0.219])	Prec@1 83.594 (82.564)
Epoch: [26][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7969 (0.7666) ([0.577]+[0.220])	Prec@1 79.688 (82.413)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.7630 (0.7630) ([0.543]+[0.220])	Prec@1 79.688 (79.688)
 * Prec@1 79.950
current lr 1.00000e-01
Grad=  tensor(1.5223, device='cuda:0')
Epoch: [27][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.6485 (0.6485) ([0.428]+[0.220])	Prec@1 87.500 (87.500)
Epoch: [27][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7473 (0.7407) ([0.528]+[0.220])	Prec@1 80.469 (82.805)
Epoch: [27][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7722 (0.7374) ([0.552]+[0.220])	Prec@1 82.031 (83.050)
Epoch: [27][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8953 (0.7445) ([0.677]+[0.218])	Prec@1 76.562 (82.844)
Test: [0/79]	Time 0.188 (0.188)	Loss 1.2376 (1.2376) ([1.019]+[0.218])	Prec@1 69.531 (69.531)
 * Prec@1 71.740
current lr 1.00000e-01
Grad=  tensor(1.4575, device='cuda:0')
Epoch: [28][0/391]	Time 0.209 (0.209)	Data 0.177 (0.177)	Loss 0.6440 (0.6440) ([0.426]+[0.218])	Prec@1 85.156 (85.156)
Epoch: [28][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.8818 (0.7516) ([0.663]+[0.218])	Prec@1 82.812 (82.712)
Epoch: [28][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.8730 (0.7453) ([0.655]+[0.218])	Prec@1 76.562 (82.778)
Epoch: [28][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7970 (0.7570) ([0.579]+[0.218])	Prec@1 82.812 (82.493)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.8319 (0.8319) ([0.614]+[0.218])	Prec@1 78.125 (78.125)
 * Prec@1 77.870
current lr 1.00000e-01
Grad=  tensor(1.8124, device='cuda:0')
Epoch: [29][0/391]	Time 0.202 (0.202)	Data 0.172 (0.172)	Loss 0.7780 (0.7780) ([0.560]+[0.218])	Prec@1 82.031 (82.031)
Epoch: [29][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7149 (0.7296) ([0.497]+[0.218])	Prec@1 83.594 (83.354)
Epoch: [29][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7652 (0.7447) ([0.547]+[0.218])	Prec@1 80.469 (82.836)
Epoch: [29][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7818 (0.7486) ([0.564]+[0.218])	Prec@1 78.125 (82.631)
Test: [0/79]	Time 0.196 (0.196)	Loss 1.1731 (1.1731) ([0.955]+[0.218])	Prec@1 71.875 (71.875)
 * Prec@1 73.510
current lr 1.00000e-01
Grad=  tensor(2.9607, device='cuda:0')
Epoch: [30][0/391]	Time 0.233 (0.233)	Data 0.201 (0.201)	Loss 0.8825 (0.8825) ([0.664]+[0.218])	Prec@1 75.781 (75.781)
Epoch: [30][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7348 (0.7185) ([0.518]+[0.217])	Prec@1 82.031 (83.547)
Epoch: [30][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6431 (0.7350) ([0.427]+[0.216])	Prec@1 85.156 (83.135)
Epoch: [30][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.7740 (0.7396) ([0.558]+[0.216])	Prec@1 78.125 (83.018)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.1055 (1.1055) ([0.890]+[0.216])	Prec@1 71.094 (71.094)
 * Prec@1 74.310
current lr 1.00000e-01
Grad=  tensor(1.9864, device='cuda:0')
Epoch: [31][0/391]	Time 0.256 (0.256)	Data 0.223 (0.223)	Loss 0.8161 (0.8161) ([0.601]+[0.216])	Prec@1 80.469 (80.469)
Epoch: [31][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.8184 (0.7224) ([0.604]+[0.215])	Prec@1 83.594 (83.671)
Epoch: [31][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6357 (0.7363) ([0.422]+[0.214])	Prec@1 85.938 (83.158)
Epoch: [31][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7665 (0.7387) ([0.552]+[0.215])	Prec@1 81.250 (83.145)
Test: [0/79]	Time 0.193 (0.193)	Loss 1.5829 (1.5829) ([1.369]+[0.214])	Prec@1 61.719 (61.719)
 * Prec@1 66.330
current lr 1.00000e-01
Grad=  tensor(1.7762, device='cuda:0')
Epoch: [32][0/391]	Time 0.207 (0.207)	Data 0.176 (0.176)	Loss 0.7577 (0.7577) ([0.544]+[0.214])	Prec@1 82.812 (82.812)
Epoch: [32][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.6391 (0.7288) ([0.425]+[0.214])	Prec@1 86.719 (83.447)
Epoch: [32][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6772 (0.7457) ([0.462]+[0.216])	Prec@1 84.375 (82.778)
Epoch: [32][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8340 (0.7523) ([0.618]+[0.216])	Prec@1 79.688 (82.597)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.7880 (0.7880) ([0.574]+[0.214])	Prec@1 82.812 (82.812)
 * Prec@1 79.850
current lr 1.00000e-01
Grad=  tensor(2.8179, device='cuda:0')
Epoch: [33][0/391]	Time 0.257 (0.257)	Data 0.224 (0.224)	Loss 0.8066 (0.8066) ([0.593]+[0.214])	Prec@1 82.031 (82.031)
Epoch: [33][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.6279 (0.7040) ([0.415]+[0.213])	Prec@1 87.500 (84.305)
Epoch: [33][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.8262 (0.7271) ([0.613]+[0.214])	Prec@1 76.562 (83.333)
Epoch: [33][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.9233 (0.7257) ([0.712]+[0.212])	Prec@1 72.656 (83.298)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.7938 (0.7938) ([0.582]+[0.212])	Prec@1 82.812 (82.812)
 * Prec@1 80.990
current lr 1.00000e-01
Grad=  tensor(2.4327, device='cuda:0')
Epoch: [34][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.7388 (0.7388) ([0.527]+[0.212])	Prec@1 83.594 (83.594)
Epoch: [34][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8044 (0.7324) ([0.592]+[0.212])	Prec@1 79.688 (83.137)
Epoch: [34][200/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.8070 (0.7328) ([0.596]+[0.211])	Prec@1 82.031 (83.096)
Epoch: [34][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7365 (0.7349) ([0.527]+[0.210])	Prec@1 82.812 (83.119)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.8109 (0.8109) ([0.600]+[0.210])	Prec@1 78.906 (78.906)
 * Prec@1 79.780
current lr 1.00000e-01
Grad=  tensor(1.4301, device='cuda:0')
Epoch: [35][0/391]	Time 0.239 (0.239)	Data 0.206 (0.206)	Loss 0.5962 (0.5962) ([0.386]+[0.210])	Prec@1 85.938 (85.938)
Epoch: [35][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6845 (0.7230) ([0.474]+[0.211])	Prec@1 80.469 (83.145)
Epoch: [35][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6763 (0.7392) ([0.466]+[0.210])	Prec@1 88.281 (82.832)
Epoch: [35][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6894 (0.7353) ([0.480]+[0.210])	Prec@1 82.031 (82.893)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.7912 (0.7912) ([0.582]+[0.209])	Prec@1 81.250 (81.250)
 * Prec@1 76.250
current lr 1.00000e-01
Grad=  tensor(1.5547, device='cuda:0')
Epoch: [36][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.6606 (0.6606) ([0.451]+[0.209])	Prec@1 85.156 (85.156)
Epoch: [36][100/391]	Time 0.027 (0.028)	Data 0.000 (0.002)	Loss 0.7040 (0.7300) ([0.495]+[0.209])	Prec@1 85.156 (83.153)
Epoch: [36][200/391]	Time 0.024 (0.027)	Data 0.000 (0.001)	Loss 0.6544 (0.7422) ([0.444]+[0.210])	Prec@1 85.938 (82.933)
Epoch: [36][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6995 (0.7427) ([0.490]+[0.210])	Prec@1 84.375 (82.911)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.7959 (0.7959) ([0.587]+[0.209])	Prec@1 80.469 (80.469)
 * Prec@1 77.430
current lr 1.00000e-01
Grad=  tensor(3.1964, device='cuda:0')
Epoch: [37][0/391]	Time 0.209 (0.209)	Data 0.176 (0.176)	Loss 0.7311 (0.7311) ([0.522]+[0.209])	Prec@1 84.375 (84.375)
Epoch: [37][100/391]	Time 0.027 (0.027)	Data 0.000 (0.002)	Loss 0.6427 (0.7044) ([0.434]+[0.209])	Prec@1 85.156 (83.625)
Epoch: [37][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6006 (0.7094) ([0.393]+[0.208])	Prec@1 85.938 (83.691)
Epoch: [37][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7316 (0.7205) ([0.524]+[0.208])	Prec@1 84.375 (83.376)
Test: [0/79]	Time 0.183 (0.183)	Loss 1.1891 (1.1891) ([0.982]+[0.208])	Prec@1 71.875 (71.875)
 * Prec@1 66.910
current lr 1.00000e-01
Grad=  tensor(2.6273, device='cuda:0')
Epoch: [38][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.7842 (0.7842) ([0.577]+[0.208])	Prec@1 80.469 (80.469)
Epoch: [38][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.7185 (0.7112) ([0.512]+[0.206])	Prec@1 83.594 (83.609)
Epoch: [38][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.8068 (0.7161) ([0.599]+[0.208])	Prec@1 79.688 (83.489)
Epoch: [38][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7602 (0.7253) ([0.552]+[0.208])	Prec@1 82.031 (83.215)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.8546 (0.8546) ([0.648]+[0.207])	Prec@1 78.906 (78.906)
 * Prec@1 78.530
current lr 1.00000e-01
Grad=  tensor(2.4528, device='cuda:0')
Epoch: [39][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.7174 (0.7174) ([0.510]+[0.207])	Prec@1 81.250 (81.250)
Epoch: [39][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.7864 (0.7139) ([0.579]+[0.207])	Prec@1 82.031 (83.571)
Epoch: [39][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.7912 (0.7111) ([0.584]+[0.207])	Prec@1 79.688 (83.489)
Epoch: [39][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7912 (0.7103) ([0.587]+[0.205])	Prec@1 82.031 (83.615)
Test: [0/79]	Time 0.210 (0.210)	Loss 1.0963 (1.0963) ([0.892]+[0.204])	Prec@1 70.312 (70.312)
 * Prec@1 74.880
current lr 1.00000e-01
Grad=  tensor(1.3113, device='cuda:0')
Epoch: [40][0/391]	Time 0.261 (0.261)	Data 0.223 (0.223)	Loss 0.6057 (0.6057) ([0.401]+[0.204])	Prec@1 87.500 (87.500)
Epoch: [40][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.6116 (0.6920) ([0.407]+[0.204])	Prec@1 87.500 (84.135)
Epoch: [40][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7225 (0.6989) ([0.518]+[0.205])	Prec@1 81.250 (84.006)
Epoch: [40][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7361 (0.7141) ([0.530]+[0.206])	Prec@1 84.375 (83.448)
Test: [0/79]	Time 0.183 (0.183)	Loss 1.1659 (1.1659) ([0.960]+[0.206])	Prec@1 71.875 (71.875)
 * Prec@1 75.930
current lr 1.00000e-01
Grad=  tensor(2.8034, device='cuda:0')
Epoch: [41][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.7045 (0.7045) ([0.498]+[0.206])	Prec@1 81.250 (81.250)
Epoch: [41][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7680 (0.7043) ([0.561]+[0.207])	Prec@1 85.156 (83.849)
Epoch: [41][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7704 (0.7137) ([0.564]+[0.206])	Prec@1 82.031 (83.648)
Epoch: [41][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6754 (0.7142) ([0.469]+[0.206])	Prec@1 84.375 (83.609)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.9463 (0.9463) ([0.740]+[0.206])	Prec@1 78.125 (78.125)
 * Prec@1 73.680
current lr 1.00000e-01
Grad=  tensor(2.2563, device='cuda:0')
Epoch: [42][0/391]	Time 0.239 (0.239)	Data 0.205 (0.205)	Loss 0.7982 (0.7982) ([0.592]+[0.206])	Prec@1 79.688 (79.688)
Epoch: [42][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.7455 (0.7194) ([0.539]+[0.206])	Prec@1 82.031 (83.632)
Epoch: [42][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.7139 (0.7112) ([0.508]+[0.206])	Prec@1 85.156 (83.726)
Epoch: [42][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.8447 (0.7116) ([0.639]+[0.205])	Prec@1 78.125 (83.716)
Test: [0/79]	Time 0.199 (0.199)	Loss 1.0889 (1.0889) ([0.884]+[0.205])	Prec@1 71.875 (71.875)
 * Prec@1 72.340
current lr 1.00000e-01
Grad=  tensor(2.4971, device='cuda:0')
Epoch: [43][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.6549 (0.6549) ([0.450]+[0.205])	Prec@1 86.719 (86.719)
Epoch: [43][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7151 (0.6788) ([0.511]+[0.204])	Prec@1 85.156 (84.236)
Epoch: [43][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6285 (0.6928) ([0.424]+[0.204])	Prec@1 85.938 (83.843)
Epoch: [43][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6687 (0.7001) ([0.465]+[0.204])	Prec@1 83.594 (83.718)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.9102 (0.9102) ([0.707]+[0.203])	Prec@1 77.344 (77.344)
 * Prec@1 78.460
current lr 1.00000e-01
Grad=  tensor(2.9095, device='cuda:0')
Epoch: [44][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.8318 (0.8318) ([0.628]+[0.203])	Prec@1 85.156 (85.156)
Epoch: [44][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6661 (0.6941) ([0.461]+[0.205])	Prec@1 88.281 (84.313)
Epoch: [44][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6819 (0.7014) ([0.477]+[0.205])	Prec@1 85.938 (84.017)
Epoch: [44][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8616 (0.7095) ([0.657]+[0.205])	Prec@1 77.344 (83.724)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.7987 (0.7987) ([0.594]+[0.204])	Prec@1 78.125 (78.125)
 * Prec@1 79.140
current lr 1.00000e-01
Grad=  tensor(1.8724, device='cuda:0')
Epoch: [45][0/391]	Time 0.204 (0.204)	Data 0.173 (0.173)	Loss 0.7078 (0.7078) ([0.503]+[0.204])	Prec@1 82.812 (82.812)
Epoch: [45][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.8218 (0.6959) ([0.618]+[0.204])	Prec@1 80.469 (83.926)
Epoch: [45][200/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 0.7063 (0.6934) ([0.501]+[0.205])	Prec@1 82.031 (84.037)
Epoch: [45][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6675 (0.7100) ([0.462]+[0.206])	Prec@1 82.812 (83.625)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.7858 (0.7858) ([0.581]+[0.205])	Prec@1 81.250 (81.250)
 * Prec@1 75.850
current lr 1.00000e-01
Grad=  tensor(2.9223, device='cuda:0')
Epoch: [46][0/391]	Time 0.243 (0.243)	Data 0.212 (0.212)	Loss 0.6600 (0.6600) ([0.456]+[0.205])	Prec@1 82.812 (82.812)
Epoch: [46][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.7746 (0.6960) ([0.571]+[0.204])	Prec@1 82.812 (84.298)
Epoch: [46][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.6095 (0.6977) ([0.405]+[0.205])	Prec@1 88.281 (84.029)
Epoch: [46][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7735 (0.7078) ([0.569]+[0.204])	Prec@1 80.469 (83.708)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.7882 (0.7882) ([0.584]+[0.204])	Prec@1 81.250 (81.250)
 * Prec@1 80.440
current lr 1.00000e-01
Grad=  tensor(2.4898, device='cuda:0')
Epoch: [47][0/391]	Time 0.243 (0.243)	Data 0.211 (0.211)	Loss 0.7153 (0.7153) ([0.511]+[0.204])	Prec@1 82.031 (82.031)
Epoch: [47][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.8152 (0.6875) ([0.613]+[0.203])	Prec@1 81.250 (83.988)
Epoch: [47][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7602 (0.6937) ([0.557]+[0.204])	Prec@1 81.250 (84.068)
Epoch: [47][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6890 (0.7038) ([0.485]+[0.204])	Prec@1 85.156 (83.742)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.8669 (0.8669) ([0.663]+[0.204])	Prec@1 77.344 (77.344)
 * Prec@1 75.520
current lr 1.00000e-01
Grad=  tensor(3.2490, device='cuda:0')
Epoch: [48][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.7270 (0.7270) ([0.523]+[0.204])	Prec@1 80.469 (80.469)
Epoch: [48][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.5804 (0.6899) ([0.376]+[0.204])	Prec@1 87.500 (84.066)
Epoch: [48][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6167 (0.7017) ([0.413]+[0.204])	Prec@1 83.594 (83.808)
Epoch: [48][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.6622 (0.7041) ([0.458]+[0.204])	Prec@1 83.594 (83.729)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.8776 (0.8776) ([0.674]+[0.204])	Prec@1 77.344 (77.344)
 * Prec@1 76.660
current lr 1.00000e-01
Grad=  tensor(1.8176, device='cuda:0')
Epoch: [49][0/391]	Time 0.209 (0.209)	Data 0.178 (0.178)	Loss 0.6921 (0.6921) ([0.488]+[0.204])	Prec@1 84.375 (84.375)
Epoch: [49][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.6173 (0.6844) ([0.413]+[0.205])	Prec@1 86.719 (84.011)
Epoch: [49][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7790 (0.6942) ([0.575]+[0.204])	Prec@1 78.906 (83.854)
Epoch: [49][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.7870 (0.6964) ([0.584]+[0.204])	Prec@1 77.344 (83.812)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.0373 (1.0373) ([0.832]+[0.205])	Prec@1 77.344 (77.344)
 * Prec@1 76.190
current lr 1.00000e-01
Grad=  tensor(1.7504, device='cuda:0')
Epoch: [50][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.8237 (0.8237) ([0.619]+[0.205])	Prec@1 83.594 (83.594)
Epoch: [50][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.9521 (0.7005) ([0.747]+[0.205])	Prec@1 78.906 (83.656)
Epoch: [50][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6210 (0.7000) ([0.417]+[0.204])	Prec@1 89.062 (83.734)
Epoch: [50][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.6542 (0.7025) ([0.450]+[0.204])	Prec@1 88.281 (83.718)
Test: [0/79]	Time 0.186 (0.186)	Loss 1.0550 (1.0550) ([0.851]+[0.204])	Prec@1 77.344 (77.344)
 * Prec@1 74.280
current lr 1.00000e-01
Grad=  tensor(3.3501, device='cuda:0')
Epoch: [51][0/391]	Time 0.212 (0.212)	Data 0.179 (0.179)	Loss 0.6197 (0.6197) ([0.416]+[0.204])	Prec@1 85.156 (85.156)
Epoch: [51][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.8166 (0.6929) ([0.613]+[0.203])	Prec@1 81.250 (83.779)
Epoch: [51][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.5645 (0.6982) ([0.360]+[0.204])	Prec@1 89.062 (83.679)
Epoch: [51][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8039 (0.7011) ([0.600]+[0.204])	Prec@1 81.250 (83.591)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.8065 (0.8065) ([0.604]+[0.203])	Prec@1 82.031 (82.031)
 * Prec@1 80.160
current lr 1.00000e-01
Grad=  tensor(2.6485, device='cuda:0')
Epoch: [52][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.8541 (0.8541) ([0.651]+[0.203])	Prec@1 79.688 (79.688)
Epoch: [52][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.6185 (0.6968) ([0.416]+[0.202])	Prec@1 87.500 (84.073)
Epoch: [52][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6978 (0.6897) ([0.496]+[0.202])	Prec@1 84.375 (84.414)
Epoch: [52][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7073 (0.6913) ([0.505]+[0.202])	Prec@1 87.500 (84.323)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.7933 (0.7933) ([0.592]+[0.202])	Prec@1 82.812 (82.812)
 * Prec@1 80.550
current lr 1.00000e-01
Grad=  tensor(1.7559, device='cuda:0')
Epoch: [53][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.5408 (0.5408) ([0.339]+[0.202])	Prec@1 89.062 (89.062)
Epoch: [53][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.7502 (0.6896) ([0.548]+[0.202])	Prec@1 83.594 (84.638)
Epoch: [53][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7380 (0.6980) ([0.535]+[0.203])	Prec@1 85.938 (84.188)
Epoch: [53][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7171 (0.7020) ([0.515]+[0.202])	Prec@1 82.812 (83.965)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.7779 (0.7779) ([0.575]+[0.203])	Prec@1 82.031 (82.031)
 * Prec@1 78.950
current lr 1.00000e-01
Grad=  tensor(1.7391, device='cuda:0')
Epoch: [54][0/391]	Time 0.212 (0.212)	Data 0.179 (0.179)	Loss 0.5437 (0.5437) ([0.341]+[0.203])	Prec@1 89.844 (89.844)
Epoch: [54][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.5678 (0.6842) ([0.365]+[0.203])	Prec@1 88.281 (84.445)
Epoch: [54][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6187 (0.6958) ([0.415]+[0.203])	Prec@1 88.281 (83.975)
Epoch: [54][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.5515 (0.6947) ([0.349]+[0.202])	Prec@1 86.719 (84.014)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.9173 (0.9173) ([0.715]+[0.202])	Prec@1 77.344 (77.344)
 * Prec@1 79.290
current lr 1.00000e-01
Grad=  tensor(1.7232, device='cuda:0')
Epoch: [55][0/391]	Time 0.204 (0.204)	Data 0.172 (0.172)	Loss 0.6850 (0.6850) ([0.482]+[0.202])	Prec@1 84.375 (84.375)
Epoch: [55][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.5978 (0.7053) ([0.395]+[0.203])	Prec@1 85.938 (83.702)
Epoch: [55][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5764 (0.7033) ([0.374]+[0.202])	Prec@1 85.938 (83.703)
Epoch: [55][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6745 (0.7028) ([0.472]+[0.203])	Prec@1 86.719 (83.659)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.9228 (0.9228) ([0.720]+[0.203])	Prec@1 76.562 (76.562)
 * Prec@1 74.270
current lr 1.00000e-01
Grad=  tensor(2.4171, device='cuda:0')
Epoch: [56][0/391]	Time 0.248 (0.248)	Data 0.217 (0.217)	Loss 0.6492 (0.6492) ([0.446]+[0.203])	Prec@1 86.719 (86.719)
Epoch: [56][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6446 (0.6876) ([0.442]+[0.202])	Prec@1 85.938 (84.073)
Epoch: [56][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7042 (0.6860) ([0.502]+[0.202])	Prec@1 86.719 (84.266)
Epoch: [56][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7639 (0.6918) ([0.562]+[0.202])	Prec@1 82.812 (84.113)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.8749 (0.8749) ([0.673]+[0.202])	Prec@1 77.344 (77.344)
 * Prec@1 74.970
current lr 1.00000e-01
Grad=  tensor(3.2832, device='cuda:0')
Epoch: [57][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.6791 (0.6791) ([0.477]+[0.202])	Prec@1 85.938 (85.938)
Epoch: [57][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7854 (0.6984) ([0.582]+[0.203])	Prec@1 82.031 (84.182)
Epoch: [57][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7424 (0.7059) ([0.538]+[0.204])	Prec@1 81.250 (83.691)
Epoch: [57][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7293 (0.7108) ([0.525]+[0.204])	Prec@1 81.250 (83.503)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.8360 (0.8360) ([0.633]+[0.203])	Prec@1 83.594 (83.594)
 * Prec@1 80.190
current lr 1.00000e-01
Grad=  tensor(1.2473, device='cuda:0')
Epoch: [58][0/391]	Time 0.234 (0.234)	Data 0.203 (0.203)	Loss 0.5671 (0.5671) ([0.365]+[0.203])	Prec@1 87.500 (87.500)
Epoch: [58][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.6471 (0.6770) ([0.445]+[0.202])	Prec@1 85.156 (84.692)
Epoch: [58][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.6229 (0.6847) ([0.421]+[0.202])	Prec@1 85.156 (84.344)
Epoch: [58][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6624 (0.6933) ([0.460]+[0.203])	Prec@1 84.375 (84.030)
Test: [0/79]	Time 0.220 (0.220)	Loss 1.0204 (1.0204) ([0.818]+[0.202])	Prec@1 71.875 (71.875)
 * Prec@1 73.320
current lr 1.00000e-01
Grad=  tensor(1.9278, device='cuda:0')
Epoch: [59][0/391]	Time 0.254 (0.254)	Data 0.221 (0.221)	Loss 0.7105 (0.7105) ([0.508]+[0.202])	Prec@1 82.031 (82.031)
Epoch: [59][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.7784 (0.7149) ([0.575]+[0.203])	Prec@1 81.250 (83.516)
Epoch: [59][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.8782 (0.7070) ([0.675]+[0.203])	Prec@1 78.125 (83.683)
Epoch: [59][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7085 (0.7059) ([0.506]+[0.202])	Prec@1 84.375 (83.713)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.9704 (0.9704) ([0.768]+[0.203])	Prec@1 76.562 (76.562)
 * Prec@1 75.360
current lr 1.00000e-01
Grad=  tensor(1.7225, device='cuda:0')
Epoch: [60][0/391]	Time 0.216 (0.216)	Data 0.186 (0.186)	Loss 0.7143 (0.7143) ([0.512]+[0.203])	Prec@1 85.156 (85.156)
Epoch: [60][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.6800 (0.7137) ([0.477]+[0.203])	Prec@1 83.594 (83.547)
Epoch: [60][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.5687 (0.6998) ([0.366]+[0.202])	Prec@1 89.062 (83.971)
Epoch: [60][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.8607 (0.7074) ([0.658]+[0.203])	Prec@1 76.562 (83.716)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.8217 (0.8217) ([0.619]+[0.203])	Prec@1 82.812 (82.812)
 * Prec@1 77.620
current lr 1.00000e-01
Grad=  tensor(1.3006, device='cuda:0')
Epoch: [61][0/391]	Time 0.234 (0.234)	Data 0.202 (0.202)	Loss 0.5742 (0.5742) ([0.371]+[0.203])	Prec@1 90.625 (90.625)
Epoch: [61][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7596 (0.6763) ([0.557]+[0.202])	Prec@1 80.469 (84.661)
Epoch: [61][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7721 (0.6843) ([0.570]+[0.202])	Prec@1 78.125 (84.305)
Epoch: [61][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7841 (0.7010) ([0.581]+[0.204])	Prec@1 82.031 (83.882)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.9597 (0.9597) ([0.757]+[0.203])	Prec@1 76.562 (76.562)
 * Prec@1 75.480
current lr 1.00000e-01
Grad=  tensor(1.5400, device='cuda:0')
Epoch: [62][0/391]	Time 0.212 (0.212)	Data 0.184 (0.184)	Loss 0.6564 (0.6564) ([0.453]+[0.203])	Prec@1 85.938 (85.938)
Epoch: [62][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.7889 (0.6776) ([0.587]+[0.202])	Prec@1 78.906 (84.360)
Epoch: [62][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.6947 (0.6816) ([0.493]+[0.201])	Prec@1 85.156 (84.340)
Epoch: [62][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.9122 (0.6859) ([0.711]+[0.202])	Prec@1 80.469 (84.240)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.8778 (0.8778) ([0.676]+[0.202])	Prec@1 78.906 (78.906)
 * Prec@1 76.850
current lr 1.00000e-01
Grad=  tensor(1.8398, device='cuda:0')
Epoch: [63][0/391]	Time 0.239 (0.239)	Data 0.208 (0.208)	Loss 0.6398 (0.6398) ([0.438]+[0.202])	Prec@1 86.719 (86.719)
Epoch: [63][100/391]	Time 0.029 (0.027)	Data 0.000 (0.002)	Loss 0.7075 (0.6705) ([0.506]+[0.201])	Prec@1 86.719 (84.901)
Epoch: [63][200/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.5809 (0.6813) ([0.379]+[0.202])	Prec@1 85.938 (84.612)
Epoch: [63][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6073 (0.6932) ([0.405]+[0.202])	Prec@1 85.156 (84.157)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.7445 (0.7445) ([0.542]+[0.202])	Prec@1 79.688 (79.688)
 * Prec@1 81.070
current lr 1.00000e-01
Grad=  tensor(0.9155, device='cuda:0')
Epoch: [64][0/391]	Time 0.241 (0.241)	Data 0.212 (0.212)	Loss 0.5637 (0.5637) ([0.361]+[0.202])	Prec@1 91.406 (91.406)
Epoch: [64][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7587 (0.6852) ([0.556]+[0.203])	Prec@1 79.688 (84.081)
Epoch: [64][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7814 (0.6934) ([0.578]+[0.203])	Prec@1 82.812 (83.979)
Epoch: [64][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.7422 (0.6950) ([0.539]+[0.203])	Prec@1 78.906 (83.900)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.8778 (0.8778) ([0.674]+[0.204])	Prec@1 79.688 (79.688)
 * Prec@1 77.030
current lr 1.00000e-01
Grad=  tensor(1.2073, device='cuda:0')
Epoch: [65][0/391]	Time 0.216 (0.216)	Data 0.184 (0.184)	Loss 0.5932 (0.5932) ([0.390]+[0.204])	Prec@1 88.281 (88.281)
Epoch: [65][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7504 (0.6696) ([0.548]+[0.203])	Prec@1 82.812 (84.947)
Epoch: [65][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7252 (0.6863) ([0.522]+[0.203])	Prec@1 83.594 (84.461)
Epoch: [65][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6922 (0.6976) ([0.488]+[0.204])	Prec@1 85.938 (83.978)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.8268 (0.8268) ([0.624]+[0.203])	Prec@1 78.125 (78.125)
 * Prec@1 80.460
current lr 1.00000e-01
Grad=  tensor(3.0773, device='cuda:0')
Epoch: [66][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.6982 (0.6982) ([0.496]+[0.203])	Prec@1 85.156 (85.156)
Epoch: [66][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.8098 (0.6940) ([0.607]+[0.203])	Prec@1 79.688 (84.151)
Epoch: [66][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7542 (0.6868) ([0.552]+[0.202])	Prec@1 78.906 (84.356)
Epoch: [66][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7524 (0.6869) ([0.549]+[0.203])	Prec@1 81.250 (84.352)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.8945 (0.8945) ([0.692]+[0.202])	Prec@1 81.250 (81.250)
 * Prec@1 76.400
current lr 1.00000e-01
Grad=  tensor(2.3975, device='cuda:0')
Epoch: [67][0/391]	Time 0.241 (0.241)	Data 0.210 (0.210)	Loss 0.6641 (0.6641) ([0.462]+[0.202])	Prec@1 82.812 (82.812)
Epoch: [67][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7385 (0.6639) ([0.537]+[0.201])	Prec@1 81.250 (84.862)
Epoch: [67][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7802 (0.6778) ([0.579]+[0.202])	Prec@1 75.781 (84.356)
Epoch: [67][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.9751 (0.6845) ([0.773]+[0.202])	Prec@1 78.125 (84.336)
Test: [0/79]	Time 0.195 (0.195)	Loss 1.4399 (1.4399) ([1.237]+[0.203])	Prec@1 67.969 (67.969)
 * Prec@1 62.400
current lr 1.00000e-01
Grad=  tensor(3.1083, device='cuda:0')
Epoch: [68][0/391]	Time 0.225 (0.225)	Data 0.193 (0.193)	Loss 0.8138 (0.8138) ([0.611]+[0.203])	Prec@1 80.469 (80.469)
Epoch: [68][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.7286 (0.6758) ([0.526]+[0.202])	Prec@1 82.812 (84.708)
Epoch: [68][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6363 (0.6862) ([0.433]+[0.203])	Prec@1 85.156 (84.270)
Epoch: [68][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7418 (0.6916) ([0.539]+[0.203])	Prec@1 82.031 (84.009)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.7501 (0.7501) ([0.548]+[0.202])	Prec@1 76.562 (76.562)
 * Prec@1 80.280
current lr 1.00000e-01
Grad=  tensor(1.8282, device='cuda:0')
Epoch: [69][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.6004 (0.6004) ([0.399]+[0.202])	Prec@1 86.719 (86.719)
Epoch: [69][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.6136 (0.6894) ([0.411]+[0.202])	Prec@1 89.062 (84.097)
Epoch: [69][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.6736 (0.6844) ([0.472]+[0.202])	Prec@1 83.594 (84.480)
Epoch: [69][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6166 (0.6881) ([0.415]+[0.202])	Prec@1 85.156 (84.333)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.8942 (0.8942) ([0.692]+[0.202])	Prec@1 75.781 (75.781)
 * Prec@1 78.250
current lr 1.00000e-01
Grad=  tensor(1.3553, device='cuda:0')
Epoch: [70][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.5008 (0.5008) ([0.299]+[0.202])	Prec@1 92.969 (92.969)
Epoch: [70][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.6592 (0.6831) ([0.457]+[0.202])	Prec@1 86.719 (84.367)
Epoch: [70][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.6685 (0.6935) ([0.467]+[0.202])	Prec@1 84.375 (84.064)
Epoch: [70][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.5850 (0.6933) ([0.383]+[0.202])	Prec@1 87.500 (84.038)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.8480 (0.8480) ([0.647]+[0.201])	Prec@1 80.469 (80.469)
 * Prec@1 79.590
current lr 1.00000e-01
Grad=  tensor(2.1347, device='cuda:0')
Epoch: [71][0/391]	Time 0.255 (0.255)	Data 0.221 (0.221)	Loss 0.8824 (0.8824) ([0.681]+[0.201])	Prec@1 79.688 (79.688)
Epoch: [71][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.6424 (0.6752) ([0.441]+[0.201])	Prec@1 85.938 (84.530)
Epoch: [71][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.6970 (0.6812) ([0.496]+[0.201])	Prec@1 81.250 (84.266)
Epoch: [71][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.7132 (0.6847) ([0.512]+[0.201])	Prec@1 85.156 (84.230)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.7550 (0.7550) ([0.554]+[0.201])	Prec@1 82.031 (82.031)
 * Prec@1 78.210
current lr 1.00000e-01
Grad=  tensor(1.2899, device='cuda:0')
Epoch: [72][0/391]	Time 0.220 (0.220)	Data 0.190 (0.190)	Loss 0.5116 (0.5116) ([0.311]+[0.201])	Prec@1 87.500 (87.500)
Epoch: [72][100/391]	Time 0.026 (0.027)	Data 0.000 (0.002)	Loss 0.7913 (0.6881) ([0.590]+[0.201])	Prec@1 81.250 (83.779)
Epoch: [72][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.5627 (0.6855) ([0.362]+[0.201])	Prec@1 89.062 (84.126)
Epoch: [72][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.8407 (0.6935) ([0.640]+[0.201])	Prec@1 78.906 (84.077)
Test: [0/79]	Time 0.183 (0.183)	Loss 1.0018 (1.0018) ([0.801]+[0.201])	Prec@1 75.781 (75.781)
 * Prec@1 76.640
current lr 1.00000e-01
Grad=  tensor(2.6102, device='cuda:0')
Epoch: [73][0/391]	Time 0.216 (0.216)	Data 0.184 (0.184)	Loss 0.7650 (0.7650) ([0.564]+[0.201])	Prec@1 82.031 (82.031)
Epoch: [73][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.7900 (0.6839) ([0.588]+[0.202])	Prec@1 77.344 (84.267)
Epoch: [73][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.7045 (0.6813) ([0.503]+[0.201])	Prec@1 82.812 (84.519)
Epoch: [73][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.6042 (0.6837) ([0.403]+[0.201])	Prec@1 83.594 (84.365)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.9584 (0.9584) ([0.758]+[0.200])	Prec@1 76.562 (76.562)
 * Prec@1 73.670
current lr 1.00000e-01
Grad=  tensor(2.2689, device='cuda:0')
Epoch: [74][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.5679 (0.5679) ([0.367]+[0.200])	Prec@1 86.719 (86.719)
Epoch: [74][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.7071 (0.6870) ([0.505]+[0.202])	Prec@1 83.594 (84.298)
Epoch: [74][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.7763 (0.6879) ([0.574]+[0.202])	Prec@1 80.469 (84.321)
Epoch: [74][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.7136 (0.6904) ([0.511]+[0.202])	Prec@1 85.156 (84.333)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.7991 (0.7991) ([0.597]+[0.202])	Prec@1 79.688 (79.688)
 * Prec@1 77.590
current lr 1.00000e-02
Grad=  tensor(1.7592, device='cuda:0')
Epoch: [75][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.6597 (0.6597) ([0.458]+[0.202])	Prec@1 85.156 (85.156)
Epoch: [75][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.4822 (0.5299) ([0.292]+[0.190])	Prec@1 88.281 (89.039)
Epoch: [75][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.3348 (0.4986) ([0.146]+[0.188])	Prec@1 93.750 (89.937)
Epoch: [75][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.4179 (0.4811) ([0.231]+[0.187])	Prec@1 92.969 (90.526)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4033 (0.4033) ([0.218]+[0.185])	Prec@1 93.750 (93.750)
 * Prec@1 90.350
current lr 1.00000e-02
Grad=  tensor(1.0549, device='cuda:0')
Epoch: [76][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.3601 (0.3601) ([0.175]+[0.185])	Prec@1 94.531 (94.531)
Epoch: [76][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.4239 (0.4064) ([0.241]+[0.183])	Prec@1 94.531 (92.752)
Epoch: [76][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.4198 (0.4031) ([0.238]+[0.182])	Prec@1 92.969 (92.852)
Epoch: [76][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.5815 (0.4064) ([0.401]+[0.180])	Prec@1 89.844 (92.735)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.4388 (0.4388) ([0.260]+[0.179])	Prec@1 89.062 (89.062)
 * Prec@1 90.750
current lr 1.00000e-02
Grad=  tensor(1.6581, device='cuda:0')
Epoch: [77][0/391]	Time 0.217 (0.217)	Data 0.184 (0.184)	Loss 0.3575 (0.3575) ([0.179]+[0.179])	Prec@1 93.750 (93.750)
Epoch: [77][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.4584 (0.3825) ([0.281]+[0.177])	Prec@1 91.406 (93.216)
Epoch: [77][200/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.4010 (0.3809) ([0.225]+[0.176])	Prec@1 92.188 (93.167)
Epoch: [77][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.4298 (0.3799) ([0.255]+[0.175])	Prec@1 91.406 (93.205)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.4072 (0.4072) ([0.234]+[0.173])	Prec@1 92.969 (92.969)
 * Prec@1 91.410
current lr 1.00000e-02
Grad=  tensor(1.9693, device='cuda:0')
Epoch: [78][0/391]	Time 0.260 (0.260)	Data 0.227 (0.227)	Loss 0.3276 (0.3276) ([0.154]+[0.173])	Prec@1 95.312 (95.312)
Epoch: [78][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.3669 (0.3538) ([0.195]+[0.172])	Prec@1 93.750 (93.982)
Epoch: [78][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.3546 (0.3541) ([0.184]+[0.171])	Prec@1 92.969 (94.003)
Epoch: [78][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3216 (0.3538) ([0.152]+[0.169])	Prec@1 95.312 (93.939)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.3914 (0.3914) ([0.223]+[0.168])	Prec@1 93.750 (93.750)
 * Prec@1 91.460
current lr 1.00000e-02
Grad=  tensor(1.2082, device='cuda:0')
Epoch: [79][0/391]	Time 0.225 (0.225)	Data 0.194 (0.194)	Loss 0.2841 (0.2841) ([0.116]+[0.168])	Prec@1 96.094 (96.094)
Epoch: [79][100/391]	Time 0.029 (0.028)	Data 0.000 (0.002)	Loss 0.3035 (0.3313) ([0.137]+[0.167])	Prec@1 94.531 (94.686)
Epoch: [79][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2668 (0.3317) ([0.101]+[0.165])	Prec@1 97.656 (94.652)
Epoch: [79][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3107 (0.3326) ([0.146]+[0.164])	Prec@1 95.312 (94.510)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3685 (0.3685) ([0.205]+[0.163])	Prec@1 93.750 (93.750)
 * Prec@1 91.280
current lr 1.00000e-02
Grad=  tensor(1.7027, device='cuda:0')
Epoch: [80][0/391]	Time 0.199 (0.199)	Data 0.170 (0.170)	Loss 0.2795 (0.2795) ([0.116]+[0.163])	Prec@1 95.312 (95.312)
Epoch: [80][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.3954 (0.3183) ([0.233]+[0.162])	Prec@1 91.406 (94.771)
Epoch: [80][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3461 (0.3151) ([0.185]+[0.161])	Prec@1 93.750 (94.764)
Epoch: [80][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.3564 (0.3157) ([0.197]+[0.160])	Prec@1 94.531 (94.770)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.4018 (0.4018) ([0.243]+[0.159])	Prec@1 92.188 (92.188)
 * Prec@1 91.690
current lr 1.00000e-02
Grad=  tensor(2.1094, device='cuda:0')
Epoch: [81][0/391]	Time 0.244 (0.244)	Data 0.212 (0.212)	Loss 0.2720 (0.2720) ([0.113]+[0.159])	Prec@1 94.531 (94.531)
Epoch: [81][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2892 (0.2974) ([0.132]+[0.158])	Prec@1 95.312 (95.343)
Epoch: [81][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2701 (0.3040) ([0.114]+[0.157])	Prec@1 96.875 (95.145)
Epoch: [81][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2342 (0.3036) ([0.079]+[0.156])	Prec@1 98.438 (95.165)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.4172 (0.4172) ([0.263]+[0.155])	Prec@1 91.406 (91.406)
 * Prec@1 91.510
current lr 1.00000e-02
Grad=  tensor(1.1674, device='cuda:0')
Epoch: [82][0/391]	Time 0.260 (0.260)	Data 0.228 (0.228)	Loss 0.2302 (0.2302) ([0.075]+[0.155])	Prec@1 98.438 (98.438)
Epoch: [82][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.2166 (0.2870) ([0.063]+[0.154])	Prec@1 98.438 (95.506)
Epoch: [82][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2411 (0.2884) ([0.088]+[0.153])	Prec@1 95.312 (95.484)
Epoch: [82][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2623 (0.2925) ([0.111]+[0.152])	Prec@1 99.219 (95.336)
Test: [0/79]	Time 0.191 (0.191)	Loss 0.3605 (0.3605) ([0.210]+[0.151])	Prec@1 93.750 (93.750)
 * Prec@1 91.520
current lr 1.00000e-02
Grad=  tensor(1.8397, device='cuda:0')
Epoch: [83][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.2860 (0.2860) ([0.135]+[0.151])	Prec@1 96.094 (96.094)
Epoch: [83][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2173 (0.2727) ([0.067]+[0.150])	Prec@1 98.438 (95.846)
Epoch: [83][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.3046 (0.2791) ([0.155]+[0.149])	Prec@1 96.094 (95.666)
Epoch: [83][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2951 (0.2795) ([0.147]+[0.148])	Prec@1 94.531 (95.686)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3466 (0.3466) ([0.199]+[0.147])	Prec@1 92.969 (92.969)
 * Prec@1 91.640
current lr 1.00000e-02
Grad=  tensor(3.2736, device='cuda:0')
Epoch: [84][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.2805 (0.2805) ([0.133]+[0.147])	Prec@1 96.875 (96.875)
Epoch: [84][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2658 (0.2709) ([0.119]+[0.147])	Prec@1 96.875 (95.831)
Epoch: [84][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1917 (0.2725) ([0.046]+[0.146])	Prec@1 99.219 (95.639)
Epoch: [84][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3943 (0.2732) ([0.249]+[0.145])	Prec@1 93.750 (95.699)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3576 (0.3576) ([0.213]+[0.144])	Prec@1 92.969 (92.969)
 * Prec@1 91.320
current lr 1.00000e-02
Grad=  tensor(5.4665, device='cuda:0')
Epoch: [85][0/391]	Time 0.205 (0.205)	Data 0.174 (0.174)	Loss 0.3243 (0.3243) ([0.180]+[0.144])	Prec@1 94.531 (94.531)
Epoch: [85][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2206 (0.2555) ([0.077]+[0.144])	Prec@1 97.656 (96.272)
Epoch: [85][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2513 (0.2603) ([0.108]+[0.143])	Prec@1 96.094 (95.993)
Epoch: [85][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2153 (0.2618) ([0.073]+[0.142])	Prec@1 96.875 (95.899)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3189 (0.3189) ([0.177]+[0.142])	Prec@1 91.406 (91.406)
 * Prec@1 90.720
current lr 1.00000e-02
Grad=  tensor(5.7229, device='cuda:0')
Epoch: [86][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.3175 (0.3175) ([0.176]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [86][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2236 (0.2517) ([0.083]+[0.141])	Prec@1 97.656 (96.372)
Epoch: [86][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2142 (0.2539) ([0.074]+[0.140])	Prec@1 98.438 (96.288)
Epoch: [86][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2447 (0.2599) ([0.105]+[0.140])	Prec@1 96.094 (96.081)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3521 (0.3521) ([0.213]+[0.139])	Prec@1 92.188 (92.188)
 * Prec@1 90.980
current lr 1.00000e-02
Grad=  tensor(4.5348, device='cuda:0')
Epoch: [87][0/391]	Time 0.201 (0.201)	Data 0.171 (0.171)	Loss 0.2511 (0.2511) ([0.112]+[0.139])	Prec@1 96.094 (96.094)
Epoch: [87][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1799 (0.2467) ([0.042]+[0.138])	Prec@1 99.219 (96.419)
Epoch: [87][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1973 (0.2512) ([0.060]+[0.138])	Prec@1 99.219 (96.241)
Epoch: [87][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2090 (0.2514) ([0.072]+[0.137])	Prec@1 97.656 (96.192)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.3491 (0.3491) ([0.213]+[0.137])	Prec@1 94.531 (94.531)
 * Prec@1 91.310
current lr 1.00000e-02
Grad=  tensor(3.1027, device='cuda:0')
Epoch: [88][0/391]	Time 0.244 (0.244)	Data 0.212 (0.212)	Loss 0.1970 (0.1970) ([0.060]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [88][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2368 (0.2340) ([0.101]+[0.136])	Prec@1 97.656 (96.813)
Epoch: [88][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.2806 (0.2443) ([0.145]+[0.136])	Prec@1 94.531 (96.397)
Epoch: [88][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2517 (0.2452) ([0.117]+[0.135])	Prec@1 95.312 (96.361)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.4326 (0.4326) ([0.298]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 90.580
current lr 1.00000e-02
Grad=  tensor(4.4345, device='cuda:0')
Epoch: [89][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.2339 (0.2339) ([0.099]+[0.135])	Prec@1 96.875 (96.875)
Epoch: [89][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2236 (0.2352) ([0.089]+[0.134])	Prec@1 96.094 (96.535)
Epoch: [89][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1929 (0.2368) ([0.059]+[0.134])	Prec@1 97.656 (96.482)
Epoch: [89][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2529 (0.2399) ([0.120]+[0.133])	Prec@1 97.656 (96.366)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.3389 (0.3389) ([0.206]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 90.790
current lr 1.00000e-02
Grad=  tensor(1.9770, device='cuda:0')
Epoch: [90][0/391]	Time 0.235 (0.235)	Data 0.197 (0.197)	Loss 0.2041 (0.2041) ([0.071]+[0.133])	Prec@1 98.438 (98.438)
Epoch: [90][100/391]	Time 0.024 (0.028)	Data 0.000 (0.002)	Loss 0.2746 (0.2285) ([0.142]+[0.132])	Prec@1 93.750 (96.875)
Epoch: [90][200/391]	Time 0.030 (0.027)	Data 0.000 (0.001)	Loss 0.2873 (0.2358) ([0.155]+[0.132])	Prec@1 95.312 (96.587)
Epoch: [90][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3319 (0.2416) ([0.200]+[0.132])	Prec@1 95.312 (96.338)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4088 (0.4088) ([0.278]+[0.131])	Prec@1 92.969 (92.969)
 * Prec@1 90.890
current lr 1.00000e-02
Grad=  tensor(4.0093, device='cuda:0')
Epoch: [91][0/391]	Time 0.253 (0.253)	Data 0.222 (0.222)	Loss 0.2184 (0.2184) ([0.087]+[0.131])	Prec@1 96.875 (96.875)
Epoch: [91][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2644 (0.2402) ([0.134]+[0.131])	Prec@1 93.750 (96.287)
Epoch: [91][200/391]	Time 0.030 (0.027)	Data 0.000 (0.001)	Loss 0.2906 (0.2388) ([0.160]+[0.130])	Prec@1 93.750 (96.343)
Epoch: [91][300/391]	Time 0.028 (0.027)	Data 0.000 (0.001)	Loss 0.2869 (0.2387) ([0.157]+[0.130])	Prec@1 96.094 (96.366)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3566 (0.3566) ([0.227]+[0.130])	Prec@1 91.406 (91.406)
 * Prec@1 90.350
current lr 1.00000e-02
Grad=  tensor(7.1638, device='cuda:0')
Epoch: [92][0/391]	Time 0.214 (0.214)	Data 0.185 (0.185)	Loss 0.2080 (0.2080) ([0.078]+[0.130])	Prec@1 96.094 (96.094)
Epoch: [92][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.2436 (0.2265) ([0.114]+[0.129])	Prec@1 97.656 (96.798)
Epoch: [92][200/391]	Time 0.023 (0.026)	Data 0.000 (0.001)	Loss 0.1665 (0.2277) ([0.038]+[0.129])	Prec@1 99.219 (96.727)
Epoch: [92][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2530 (0.2317) ([0.124]+[0.129])	Prec@1 96.875 (96.574)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3427 (0.3427) ([0.214]+[0.128])	Prec@1 93.750 (93.750)
 * Prec@1 89.980
current lr 1.00000e-02
Grad=  tensor(3.0222, device='cuda:0')
Epoch: [93][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.1964 (0.1964) ([0.068]+[0.128])	Prec@1 97.656 (97.656)
Epoch: [93][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2324 (0.2358) ([0.104]+[0.128])	Prec@1 96.875 (96.527)
Epoch: [93][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2070 (0.2360) ([0.079]+[0.128])	Prec@1 97.656 (96.459)
Epoch: [93][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.2092 (0.2391) ([0.082]+[0.128])	Prec@1 96.875 (96.348)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4881 (0.4881) ([0.361]+[0.127])	Prec@1 91.406 (91.406)
 * Prec@1 90.990
current lr 1.00000e-02
Grad=  tensor(1.5622, device='cuda:0')
Epoch: [94][0/391]	Time 0.210 (0.210)	Data 0.173 (0.173)	Loss 0.1594 (0.1594) ([0.032]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [94][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2383 (0.2294) ([0.111]+[0.127])	Prec@1 96.875 (96.651)
Epoch: [94][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2634 (0.2324) ([0.136]+[0.127])	Prec@1 95.312 (96.560)
Epoch: [94][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2468 (0.2349) ([0.120]+[0.127])	Prec@1 96.094 (96.439)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.3907 (0.3907) ([0.264]+[0.126])	Prec@1 93.750 (93.750)
 * Prec@1 89.950
current lr 1.00000e-02
Grad=  tensor(3.9089, device='cuda:0')
Epoch: [95][0/391]	Time 0.211 (0.211)	Data 0.179 (0.179)	Loss 0.2619 (0.2619) ([0.136]+[0.126])	Prec@1 96.875 (96.875)
Epoch: [95][100/391]	Time 0.024 (0.028)	Data 0.000 (0.002)	Loss 0.2020 (0.2397) ([0.076]+[0.126])	Prec@1 97.656 (96.272)
Epoch: [95][200/391]	Time 0.024 (0.027)	Data 0.000 (0.001)	Loss 0.1650 (0.2344) ([0.039]+[0.126])	Prec@1 99.219 (96.366)
Epoch: [95][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3424 (0.2327) ([0.217]+[0.126])	Prec@1 92.188 (96.392)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3945 (0.3945) ([0.269]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(7.7404, device='cuda:0')
Epoch: [96][0/391]	Time 0.239 (0.239)	Data 0.206 (0.206)	Loss 0.2671 (0.2671) ([0.142]+[0.125])	Prec@1 95.312 (95.312)
Epoch: [96][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2505 (0.2282) ([0.125]+[0.125])	Prec@1 96.875 (96.558)
Epoch: [96][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1935 (0.2315) ([0.068]+[0.125])	Prec@1 96.875 (96.428)
Epoch: [96][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2807 (0.2356) ([0.156]+[0.125])	Prec@1 93.750 (96.265)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.4110 (0.4110) ([0.286]+[0.125])	Prec@1 93.750 (93.750)
 * Prec@1 90.520
current lr 1.00000e-02
Grad=  tensor(2.2304, device='cuda:0')
Epoch: [97][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.1709 (0.1709) ([0.046]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [97][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2401 (0.2302) ([0.115]+[0.125])	Prec@1 95.312 (96.527)
Epoch: [97][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3156 (0.2353) ([0.191]+[0.125])	Prec@1 94.531 (96.234)
Epoch: [97][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2389 (0.2374) ([0.114]+[0.125])	Prec@1 96.094 (96.192)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3053 (0.3053) ([0.181]+[0.125])	Prec@1 93.750 (93.750)
 * Prec@1 89.960
current lr 1.00000e-02
Grad=  tensor(5.9133, device='cuda:0')
Epoch: [98][0/391]	Time 0.206 (0.206)	Data 0.174 (0.174)	Loss 0.1967 (0.1967) ([0.072]+[0.125])	Prec@1 97.656 (97.656)
Epoch: [98][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2364 (0.2225) ([0.112]+[0.124])	Prec@1 96.094 (96.759)
Epoch: [98][200/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.2260 (0.2283) ([0.102]+[0.124])	Prec@1 96.875 (96.661)
Epoch: [98][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2598 (0.2323) ([0.136]+[0.124])	Prec@1 94.531 (96.457)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.4568 (0.4568) ([0.333]+[0.124])	Prec@1 92.188 (92.188)
 * Prec@1 88.930
current lr 1.00000e-02
Grad=  tensor(3.6631, device='cuda:0')
Epoch: [99][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.1969 (0.1969) ([0.073]+[0.124])	Prec@1 98.438 (98.438)
Epoch: [99][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2009 (0.2296) ([0.077]+[0.124])	Prec@1 96.094 (96.597)
Epoch: [99][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1944 (0.2287) ([0.071]+[0.124])	Prec@1 97.656 (96.618)
Epoch: [99][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2056 (0.2304) ([0.082]+[0.123])	Prec@1 97.656 (96.561)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.3923 (0.3923) ([0.269]+[0.123])	Prec@1 92.188 (92.188)
 * Prec@1 90.160
current lr 1.00000e-02
Grad=  tensor(9.4674, device='cuda:0')
Epoch: [100][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.2639 (0.2639) ([0.140]+[0.123])	Prec@1 94.531 (94.531)
Epoch: [100][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1759 (0.2367) ([0.053]+[0.123])	Prec@1 98.438 (96.117)
Epoch: [100][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1983 (0.2328) ([0.075]+[0.123])	Prec@1 97.656 (96.308)
Epoch: [100][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1958 (0.2346) ([0.073]+[0.123])	Prec@1 97.656 (96.252)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4791 (0.4791) ([0.356]+[0.123])	Prec@1 91.406 (91.406)
 * Prec@1 89.670
current lr 1.00000e-02
Grad=  tensor(5.7084, device='cuda:0')
Epoch: [101][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.2027 (0.2027) ([0.080]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [101][100/391]	Time 0.029 (0.027)	Data 0.000 (0.002)	Loss 0.2089 (0.2233) ([0.086]+[0.123])	Prec@1 95.312 (96.697)
Epoch: [101][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2433 (0.2290) ([0.120]+[0.123])	Prec@1 93.750 (96.521)
Epoch: [101][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2535 (0.2325) ([0.131]+[0.123])	Prec@1 96.094 (96.361)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3756 (0.3756) ([0.253]+[0.123])	Prec@1 91.406 (91.406)
 * Prec@1 90.260
current lr 1.00000e-02
Grad=  tensor(10.9753, device='cuda:0')
Epoch: [102][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.2406 (0.2406) ([0.118]+[0.123])	Prec@1 96.094 (96.094)
Epoch: [102][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2296 (0.2318) ([0.107]+[0.123])	Prec@1 96.875 (96.303)
Epoch: [102][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3865 (0.2313) ([0.264]+[0.123])	Prec@1 91.406 (96.366)
Epoch: [102][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1677 (0.2332) ([0.045]+[0.123])	Prec@1 99.219 (96.309)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.4489 (0.4489) ([0.326]+[0.123])	Prec@1 91.406 (91.406)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(6.2696, device='cuda:0')
Epoch: [103][0/391]	Time 0.243 (0.243)	Data 0.212 (0.212)	Loss 0.2108 (0.2108) ([0.088]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [103][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2263 (0.2258) ([0.104]+[0.123])	Prec@1 96.875 (96.550)
Epoch: [103][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2278 (0.2297) ([0.105]+[0.122])	Prec@1 96.094 (96.385)
Epoch: [103][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2367 (0.2331) ([0.114]+[0.123])	Prec@1 95.312 (96.268)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.3517 (0.3517) ([0.229]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 90.330
current lr 1.00000e-02
Grad=  tensor(7.8069, device='cuda:0')
Epoch: [104][0/391]	Time 0.204 (0.204)	Data 0.171 (0.171)	Loss 0.2419 (0.2419) ([0.119]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [104][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1736 (0.2291) ([0.051]+[0.122])	Prec@1 99.219 (96.372)
Epoch: [104][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2597 (0.2329) ([0.137]+[0.122])	Prec@1 95.312 (96.269)
Epoch: [104][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2197 (0.2363) ([0.097]+[0.122])	Prec@1 96.875 (96.151)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.5024 (0.5024) ([0.380]+[0.123])	Prec@1 90.625 (90.625)
 * Prec@1 89.830
current lr 1.00000e-02
Grad=  tensor(8.1146, device='cuda:0')
Epoch: [105][0/391]	Time 0.257 (0.257)	Data 0.223 (0.223)	Loss 0.2238 (0.2238) ([0.101]+[0.123])	Prec@1 96.875 (96.875)
Epoch: [105][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2138 (0.2287) ([0.091]+[0.122])	Prec@1 96.875 (96.527)
Epoch: [105][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2191 (0.2316) ([0.097]+[0.122])	Prec@1 96.094 (96.444)
Epoch: [105][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2697 (0.2352) ([0.147]+[0.122])	Prec@1 94.531 (96.322)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.5214 (0.5214) ([0.399]+[0.122])	Prec@1 89.844 (89.844)
 * Prec@1 89.270
current lr 1.00000e-02
Grad=  tensor(14.5686, device='cuda:0')
Epoch: [106][0/391]	Time 0.213 (0.213)	Data 0.184 (0.184)	Loss 0.2680 (0.2680) ([0.146]+[0.122])	Prec@1 92.969 (92.969)
Epoch: [106][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2505 (0.2262) ([0.128]+[0.122])	Prec@1 96.875 (96.581)
Epoch: [106][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2160 (0.2285) ([0.094]+[0.122])	Prec@1 96.875 (96.506)
Epoch: [106][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2760 (0.2295) ([0.154]+[0.122])	Prec@1 94.531 (96.444)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4052 (0.4052) ([0.283]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(7.3743, device='cuda:0')
Epoch: [107][0/391]	Time 0.240 (0.240)	Data 0.208 (0.208)	Loss 0.2142 (0.2142) ([0.092]+[0.122])	Prec@1 95.312 (95.312)
Epoch: [107][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1986 (0.2311) ([0.076]+[0.122])	Prec@1 96.875 (96.264)
Epoch: [107][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1554 (0.2348) ([0.033]+[0.122])	Prec@1 99.219 (96.152)
Epoch: [107][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1717 (0.2347) ([0.049]+[0.122])	Prec@1 97.656 (96.174)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.4818 (0.4818) ([0.360]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(10.0633, device='cuda:0')
Epoch: [108][0/391]	Time 0.224 (0.224)	Data 0.193 (0.193)	Loss 0.2303 (0.2303) ([0.108]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [108][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1829 (0.2284) ([0.061]+[0.122])	Prec@1 96.875 (96.442)
Epoch: [108][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1955 (0.2247) ([0.073]+[0.122])	Prec@1 96.875 (96.517)
Epoch: [108][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3054 (0.2283) ([0.183]+[0.122])	Prec@1 95.312 (96.408)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.3744 (0.3744) ([0.252]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 89.970
current lr 1.00000e-02
Grad=  tensor(6.6719, device='cuda:0')
Epoch: [109][0/391]	Time 0.207 (0.207)	Data 0.173 (0.173)	Loss 0.1973 (0.1973) ([0.075]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [109][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2550 (0.2250) ([0.133]+[0.122])	Prec@1 95.312 (96.627)
Epoch: [109][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2051 (0.2321) ([0.083]+[0.122])	Prec@1 97.656 (96.354)
Epoch: [109][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2156 (0.2359) ([0.093]+[0.122])	Prec@1 96.094 (96.226)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.4385 (0.4385) ([0.316]+[0.122])	Prec@1 89.844 (89.844)
 * Prec@1 90.020
current lr 1.00000e-02
Grad=  tensor(8.2640, device='cuda:0')
Epoch: [110][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.2998 (0.2998) ([0.178]+[0.122])	Prec@1 94.531 (94.531)
Epoch: [110][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1823 (0.2232) ([0.060]+[0.122])	Prec@1 98.438 (96.550)
Epoch: [110][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1933 (0.2267) ([0.071]+[0.122])	Prec@1 96.094 (96.482)
Epoch: [110][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2117 (0.2286) ([0.090]+[0.122])	Prec@1 96.094 (96.436)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.3632 (0.3632) ([0.241]+[0.122])	Prec@1 91.406 (91.406)
 * Prec@1 90.710
current lr 1.00000e-02
Grad=  tensor(9.4760, device='cuda:0')
Epoch: [111][0/391]	Time 0.247 (0.247)	Data 0.215 (0.215)	Loss 0.1820 (0.1820) ([0.060]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [111][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2469 (0.2385) ([0.125]+[0.122])	Prec@1 95.312 (96.303)
Epoch: [111][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2533 (0.2359) ([0.131]+[0.122])	Prec@1 94.531 (96.261)
Epoch: [111][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2613 (0.2391) ([0.139]+[0.122])	Prec@1 94.531 (96.151)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.4342 (0.4342) ([0.312]+[0.122])	Prec@1 90.625 (90.625)
 * Prec@1 90.140
current lr 1.00000e-02
Grad=  tensor(5.9392, device='cuda:0')
Epoch: [112][0/391]	Time 0.205 (0.205)	Data 0.174 (0.174)	Loss 0.2053 (0.2053) ([0.083]+[0.122])	Prec@1 96.875 (96.875)
Epoch: [112][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1855 (0.2250) ([0.063]+[0.122])	Prec@1 98.438 (96.635)
Epoch: [112][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1847 (0.2317) ([0.062]+[0.122])	Prec@1 97.656 (96.358)
Epoch: [112][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2244 (0.2322) ([0.102]+[0.122])	Prec@1 97.656 (96.346)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.4025 (0.4025) ([0.280]+[0.122])	Prec@1 93.750 (93.750)
 * Prec@1 90.340
current lr 1.00000e-02
Grad=  tensor(11.3022, device='cuda:0')
Epoch: [113][0/391]	Time 0.235 (0.235)	Data 0.203 (0.203)	Loss 0.2760 (0.2760) ([0.154]+[0.122])	Prec@1 96.094 (96.094)
Epoch: [113][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.3124 (0.2274) ([0.190]+[0.122])	Prec@1 90.625 (96.334)
Epoch: [113][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1889 (0.2325) ([0.066]+[0.122])	Prec@1 99.219 (96.261)
Epoch: [113][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.3325 (0.2377) ([0.210]+[0.123])	Prec@1 92.188 (96.029)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.4945 (0.4945) ([0.372]+[0.123])	Prec@1 89.062 (89.062)
 * Prec@1 89.310
current lr 1.00000e-02
Grad=  tensor(6.3817, device='cuda:0')
Epoch: [114][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.1906 (0.1906) ([0.068]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [114][100/391]	Time 0.023 (0.027)	Data 0.000 (0.002)	Loss 0.2639 (0.2211) ([0.141]+[0.122])	Prec@1 94.531 (96.713)
Epoch: [114][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2110 (0.2312) ([0.088]+[0.123])	Prec@1 96.875 (96.339)
Epoch: [114][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1943 (0.2333) ([0.072]+[0.123])	Prec@1 97.656 (96.265)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.4679 (0.4679) ([0.345]+[0.123])	Prec@1 89.844 (89.844)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(3.1965, device='cuda:0')
Epoch: [115][0/391]	Time 0.241 (0.241)	Data 0.202 (0.202)	Loss 0.1734 (0.1734) ([0.051]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [115][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2895 (0.2307) ([0.167]+[0.123])	Prec@1 96.875 (96.473)
Epoch: [115][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2138 (0.2288) ([0.091]+[0.123])	Prec@1 96.094 (96.486)
Epoch: [115][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2453 (0.2299) ([0.123]+[0.123])	Prec@1 95.312 (96.431)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4431 (0.4431) ([0.320]+[0.123])	Prec@1 91.406 (91.406)
 * Prec@1 90.340
current lr 1.00000e-02
Grad=  tensor(9.8689, device='cuda:0')
Epoch: [116][0/391]	Time 0.252 (0.252)	Data 0.219 (0.219)	Loss 0.2337 (0.2337) ([0.111]+[0.123])	Prec@1 96.094 (96.094)
Epoch: [116][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2093 (0.2285) ([0.087]+[0.123])	Prec@1 96.875 (96.473)
Epoch: [116][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1800 (0.2270) ([0.057]+[0.123])	Prec@1 98.438 (96.389)
Epoch: [116][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2466 (0.2332) ([0.124]+[0.123])	Prec@1 94.531 (96.177)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4278 (0.4278) ([0.305]+[0.123])	Prec@1 91.406 (91.406)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(8.7373, device='cuda:0')
Epoch: [117][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.1804 (0.1804) ([0.057]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [117][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1831 (0.2274) ([0.060]+[0.123])	Prec@1 96.875 (96.388)
Epoch: [117][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2018 (0.2301) ([0.079]+[0.123])	Prec@1 96.094 (96.370)
Epoch: [117][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1659 (0.2309) ([0.043]+[0.123])	Prec@1 99.219 (96.358)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.4642 (0.4642) ([0.341]+[0.123])	Prec@1 90.625 (90.625)
 * Prec@1 89.800
current lr 1.00000e-02
Grad=  tensor(6.8641, device='cuda:0')
Epoch: [118][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.2006 (0.2006) ([0.078]+[0.123])	Prec@1 96.875 (96.875)
Epoch: [118][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2543 (0.2317) ([0.131]+[0.123])	Prec@1 95.312 (96.210)
Epoch: [118][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.3105 (0.2336) ([0.188]+[0.123])	Prec@1 93.750 (96.133)
Epoch: [118][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3398 (0.2353) ([0.217]+[0.123])	Prec@1 95.312 (96.143)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.4646 (0.4646) ([0.342]+[0.123])	Prec@1 89.844 (89.844)
 * Prec@1 89.970
current lr 1.00000e-02
Grad=  tensor(5.8566, device='cuda:0')
Epoch: [119][0/391]	Time 0.223 (0.223)	Data 0.191 (0.191)	Loss 0.2183 (0.2183) ([0.095]+[0.123])	Prec@1 96.875 (96.875)
Epoch: [119][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.2432 (0.2196) ([0.120]+[0.123])	Prec@1 96.094 (96.620)
Epoch: [119][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2147 (0.2252) ([0.092]+[0.123])	Prec@1 97.656 (96.525)
Epoch: [119][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2640 (0.2276) ([0.141]+[0.123])	Prec@1 95.312 (96.429)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.3614 (0.3614) ([0.238]+[0.123])	Prec@1 92.969 (92.969)
 * Prec@1 90.820
current lr 1.00000e-02
Grad=  tensor(3.6028, device='cuda:0')
Epoch: [120][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.1653 (0.1653) ([0.042]+[0.123])	Prec@1 98.438 (98.438)
Epoch: [120][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2511 (0.2228) ([0.128]+[0.123])	Prec@1 96.094 (96.728)
Epoch: [120][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2360 (0.2249) ([0.113]+[0.123])	Prec@1 96.875 (96.653)
Epoch: [120][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2448 (0.2274) ([0.122]+[0.123])	Prec@1 92.969 (96.589)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.5013 (0.5013) ([0.378]+[0.123])	Prec@1 89.844 (89.844)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(10.8825, device='cuda:0')
Epoch: [121][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.2300 (0.2300) ([0.107]+[0.123])	Prec@1 94.531 (94.531)
Epoch: [121][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2351 (0.2267) ([0.112]+[0.123])	Prec@1 96.875 (96.635)
Epoch: [121][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2282 (0.2285) ([0.105]+[0.123])	Prec@1 96.875 (96.490)
Epoch: [121][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.2382 (0.2312) ([0.115]+[0.123])	Prec@1 95.312 (96.431)
Test: [0/79]	Time 0.219 (0.219)	Loss 0.4211 (0.4211) ([0.298]+[0.123])	Prec@1 89.844 (89.844)
 * Prec@1 89.670
current lr 1.00000e-02
Grad=  tensor(5.3161, device='cuda:0')
Epoch: [122][0/391]	Time 0.251 (0.251)	Data 0.218 (0.218)	Loss 0.1890 (0.1890) ([0.066]+[0.123])	Prec@1 96.875 (96.875)
Epoch: [122][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2392 (0.2253) ([0.116]+[0.123])	Prec@1 94.531 (96.558)
Epoch: [122][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.2371 (0.2273) ([0.114]+[0.123])	Prec@1 94.531 (96.510)
Epoch: [122][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1779 (0.2297) ([0.055]+[0.123])	Prec@1 98.438 (96.400)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.5060 (0.5060) ([0.383]+[0.123])	Prec@1 89.062 (89.062)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(11.9343, device='cuda:0')
Epoch: [123][0/391]	Time 0.226 (0.226)	Data 0.193 (0.193)	Loss 0.2581 (0.2581) ([0.135]+[0.123])	Prec@1 96.094 (96.094)
Epoch: [123][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2230 (0.2301) ([0.100]+[0.123])	Prec@1 97.656 (96.357)
Epoch: [123][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1639 (0.2278) ([0.041]+[0.123])	Prec@1 99.219 (96.467)
Epoch: [123][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2540 (0.2306) ([0.131]+[0.123])	Prec@1 96.094 (96.356)
Test: [0/79]	Time 0.203 (0.203)	Loss 0.3921 (0.3921) ([0.269]+[0.123])	Prec@1 89.844 (89.844)
 * Prec@1 89.110
current lr 1.00000e-02
Grad=  tensor(8.2049, device='cuda:0')
Epoch: [124][0/391]	Time 0.210 (0.210)	Data 0.178 (0.178)	Loss 0.1969 (0.1969) ([0.073]+[0.123])	Prec@1 97.656 (97.656)
Epoch: [124][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2504 (0.2179) ([0.127]+[0.123])	Prec@1 96.094 (96.875)
Epoch: [124][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2038 (0.2233) ([0.080]+[0.124])	Prec@1 96.875 (96.634)
Epoch: [124][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1879 (0.2311) ([0.064]+[0.124])	Prec@1 98.438 (96.379)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.4689 (0.4689) ([0.345]+[0.124])	Prec@1 91.406 (91.406)
 * Prec@1 90.410
current lr 1.00000e-02
Grad=  tensor(6.0127, device='cuda:0')
Epoch: [125][0/391]	Time 0.260 (0.260)	Data 0.227 (0.227)	Loss 0.1959 (0.1959) ([0.072]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [125][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1951 (0.2179) ([0.071]+[0.124])	Prec@1 96.094 (96.759)
Epoch: [125][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2359 (0.2227) ([0.112]+[0.124])	Prec@1 96.875 (96.677)
Epoch: [125][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1896 (0.2282) ([0.066]+[0.124])	Prec@1 96.094 (96.468)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.4105 (0.4105) ([0.287]+[0.124])	Prec@1 92.969 (92.969)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(11.6954, device='cuda:0')
Epoch: [126][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.2811 (0.2811) ([0.157]+[0.124])	Prec@1 93.750 (93.750)
Epoch: [126][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2207 (0.2305) ([0.097]+[0.124])	Prec@1 97.656 (96.504)
Epoch: [126][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1745 (0.2246) ([0.051]+[0.124])	Prec@1 96.875 (96.665)
Epoch: [126][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.3063 (0.2246) ([0.183]+[0.124])	Prec@1 94.531 (96.634)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.4918 (0.4918) ([0.368]+[0.124])	Prec@1 89.844 (89.844)
 * Prec@1 90.390
current lr 1.00000e-02
Grad=  tensor(2.5701, device='cuda:0')
Epoch: [127][0/391]	Time 0.200 (0.200)	Data 0.169 (0.169)	Loss 0.1573 (0.1573) ([0.034]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [127][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1828 (0.2167) ([0.059]+[0.124])	Prec@1 99.219 (96.774)
Epoch: [127][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1804 (0.2208) ([0.057]+[0.124])	Prec@1 97.656 (96.650)
Epoch: [127][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2216 (0.2248) ([0.098]+[0.124])	Prec@1 97.656 (96.605)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2681 (0.2681) ([0.144]+[0.124])	Prec@1 95.312 (95.312)
 * Prec@1 89.840
current lr 1.00000e-02
Grad=  tensor(11.1427, device='cuda:0')
Epoch: [128][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.2305 (0.2305) ([0.107]+[0.124])	Prec@1 94.531 (94.531)
Epoch: [128][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1977 (0.2252) ([0.074]+[0.124])	Prec@1 97.656 (96.511)
Epoch: [128][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1619 (0.2289) ([0.038]+[0.124])	Prec@1 99.219 (96.471)
Epoch: [128][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2764 (0.2305) ([0.152]+[0.124])	Prec@1 92.969 (96.423)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3335 (0.3335) ([0.209]+[0.124])	Prec@1 94.531 (94.531)
 * Prec@1 89.690
current lr 1.00000e-02
Grad=  tensor(8.3039, device='cuda:0')
Epoch: [129][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.2028 (0.2028) ([0.079]+[0.124])	Prec@1 97.656 (97.656)
Epoch: [129][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2403 (0.2196) ([0.116]+[0.124])	Prec@1 95.312 (96.960)
Epoch: [129][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2036 (0.2211) ([0.080]+[0.124])	Prec@1 97.656 (96.797)
Epoch: [129][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.3073 (0.2241) ([0.183]+[0.124])	Prec@1 93.750 (96.675)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.4275 (0.4275) ([0.303]+[0.124])	Prec@1 91.406 (91.406)
 * Prec@1 90.170
current lr 1.00000e-02
Grad=  tensor(10.0481, device='cuda:0')
Epoch: [130][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.2252 (0.2252) ([0.101]+[0.124])	Prec@1 96.875 (96.875)
Epoch: [130][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2199 (0.2264) ([0.096]+[0.124])	Prec@1 97.656 (96.627)
Epoch: [130][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2047 (0.2236) ([0.081]+[0.124])	Prec@1 96.094 (96.618)
Epoch: [130][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2091 (0.2266) ([0.085]+[0.124])	Prec@1 96.094 (96.564)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.5133 (0.5133) ([0.389]+[0.124])	Prec@1 88.281 (88.281)
 * Prec@1 89.800
current lr 1.00000e-02
Grad=  tensor(3.9125, device='cuda:0')
Epoch: [131][0/391]	Time 0.256 (0.256)	Data 0.222 (0.222)	Loss 0.1997 (0.1997) ([0.076]+[0.124])	Prec@1 98.438 (98.438)
Epoch: [131][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1712 (0.2166) ([0.047]+[0.124])	Prec@1 99.219 (97.138)
Epoch: [131][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1896 (0.2217) ([0.066]+[0.124])	Prec@1 98.438 (96.828)
Epoch: [131][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2379 (0.2270) ([0.114]+[0.124])	Prec@1 95.312 (96.628)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.3810 (0.3810) ([0.257]+[0.124])	Prec@1 92.188 (92.188)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(3.0915, device='cuda:0')
Epoch: [132][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.1736 (0.1736) ([0.049]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [132][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.3704 (0.2137) ([0.246]+[0.124])	Prec@1 89.844 (96.952)
Epoch: [132][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2523 (0.2272) ([0.128]+[0.124])	Prec@1 95.312 (96.490)
Epoch: [132][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1504 (0.2292) ([0.026]+[0.124])	Prec@1 99.219 (96.444)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.3759 (0.3759) ([0.251]+[0.124])	Prec@1 93.750 (93.750)
 * Prec@1 89.410
current lr 1.00000e-02
Grad=  tensor(3.7994, device='cuda:0')
Epoch: [133][0/391]	Time 0.202 (0.202)	Data 0.171 (0.171)	Loss 0.1775 (0.1775) ([0.053]+[0.124])	Prec@1 98.438 (98.438)
Epoch: [133][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1737 (0.2170) ([0.049]+[0.124])	Prec@1 97.656 (96.821)
Epoch: [133][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2325 (0.2243) ([0.108]+[0.124])	Prec@1 96.094 (96.669)
Epoch: [133][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.2355 (0.2302) ([0.111]+[0.125])	Prec@1 98.438 (96.470)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.3872 (0.3872) ([0.263]+[0.125])	Prec@1 92.969 (92.969)
 * Prec@1 90.690
current lr 1.00000e-02
Grad=  tensor(2.9011, device='cuda:0')
Epoch: [134][0/391]	Time 0.249 (0.249)	Data 0.216 (0.216)	Loss 0.1764 (0.1764) ([0.052]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [134][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1962 (0.2155) ([0.072]+[0.125])	Prec@1 97.656 (97.006)
Epoch: [134][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2486 (0.2224) ([0.124]+[0.125])	Prec@1 94.531 (96.774)
Epoch: [134][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2803 (0.2267) ([0.156]+[0.125])	Prec@1 94.531 (96.641)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.4460 (0.4460) ([0.321]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 90.570
current lr 1.00000e-02
Grad=  tensor(5.5770, device='cuda:0')
Epoch: [135][0/391]	Time 0.209 (0.209)	Data 0.178 (0.178)	Loss 0.2016 (0.2016) ([0.077]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [135][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3065 (0.2277) ([0.182]+[0.125])	Prec@1 95.312 (96.635)
Epoch: [135][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2301 (0.2288) ([0.105]+[0.125])	Prec@1 98.438 (96.568)
Epoch: [135][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2026 (0.2324) ([0.078]+[0.125])	Prec@1 98.438 (96.431)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.4296 (0.4296) ([0.305]+[0.125])	Prec@1 92.188 (92.188)
 * Prec@1 89.690
current lr 1.00000e-02
Grad=  tensor(2.6842, device='cuda:0')
Epoch: [136][0/391]	Time 0.251 (0.251)	Data 0.218 (0.218)	Loss 0.1662 (0.1662) ([0.041]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [136][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2497 (0.2227) ([0.125]+[0.125])	Prec@1 96.094 (96.836)
Epoch: [136][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.3046 (0.2303) ([0.180]+[0.125])	Prec@1 93.750 (96.541)
Epoch: [136][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2898 (0.2281) ([0.165]+[0.125])	Prec@1 93.750 (96.566)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.5469 (0.5469) ([0.422]+[0.125])	Prec@1 88.281 (88.281)
 * Prec@1 90.560
current lr 1.00000e-02
Grad=  tensor(16.5049, device='cuda:0')
Epoch: [137][0/391]	Time 0.217 (0.217)	Data 0.185 (0.185)	Loss 0.3388 (0.3388) ([0.214]+[0.125])	Prec@1 92.188 (92.188)
Epoch: [137][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1855 (0.2234) ([0.060]+[0.125])	Prec@1 98.438 (96.612)
Epoch: [137][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2897 (0.2259) ([0.165]+[0.125])	Prec@1 94.531 (96.502)
Epoch: [137][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.1950 (0.2276) ([0.070]+[0.125])	Prec@1 98.438 (96.582)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4552 (0.4552) ([0.330]+[0.125])	Prec@1 89.844 (89.844)
 * Prec@1 89.380
current lr 1.00000e-02
Grad=  tensor(5.9683, device='cuda:0')
Epoch: [138][0/391]	Time 0.251 (0.251)	Data 0.218 (0.218)	Loss 0.2056 (0.2056) ([0.081]+[0.125])	Prec@1 96.875 (96.875)
Epoch: [138][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1947 (0.2162) ([0.070]+[0.125])	Prec@1 98.438 (96.929)
Epoch: [138][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.2754 (0.2189) ([0.150]+[0.125])	Prec@1 93.750 (96.863)
Epoch: [138][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.2926 (0.2259) ([0.168]+[0.125])	Prec@1 95.312 (96.602)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.5069 (0.5069) ([0.382]+[0.125])	Prec@1 89.844 (89.844)
 * Prec@1 90.470
current lr 1.00000e-02
Grad=  tensor(7.1948, device='cuda:0')
Epoch: [139][0/391]	Time 0.212 (0.212)	Data 0.182 (0.182)	Loss 0.2470 (0.2470) ([0.122]+[0.125])	Prec@1 96.875 (96.875)
Epoch: [139][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1841 (0.2093) ([0.059]+[0.125])	Prec@1 96.875 (97.285)
Epoch: [139][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1888 (0.2235) ([0.064]+[0.125])	Prec@1 99.219 (96.727)
Epoch: [139][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1876 (0.2280) ([0.063]+[0.125])	Prec@1 99.219 (96.538)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3721 (0.3721) ([0.247]+[0.125])	Prec@1 90.625 (90.625)
 * Prec@1 90.510
current lr 1.00000e-02
Grad=  tensor(4.7487, device='cuda:0')
Epoch: [140][0/391]	Time 0.255 (0.255)	Data 0.220 (0.220)	Loss 0.1810 (0.1810) ([0.056]+[0.125])	Prec@1 96.875 (96.875)
Epoch: [140][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1812 (0.2217) ([0.056]+[0.125])	Prec@1 96.875 (96.906)
Epoch: [140][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3363 (0.2202) ([0.211]+[0.125])	Prec@1 92.969 (96.871)
Epoch: [140][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2200 (0.2284) ([0.095]+[0.125])	Prec@1 97.656 (96.574)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.4918 (0.4918) ([0.366]+[0.125])	Prec@1 88.281 (88.281)
 * Prec@1 89.960
current lr 1.00000e-02
Grad=  tensor(3.9101, device='cuda:0')
Epoch: [141][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.1807 (0.1807) ([0.055]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [141][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.3417 (0.2207) ([0.216]+[0.125])	Prec@1 92.969 (96.798)
Epoch: [141][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2576 (0.2189) ([0.132]+[0.125])	Prec@1 96.875 (96.844)
Epoch: [141][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.2472 (0.2219) ([0.122]+[0.125])	Prec@1 94.531 (96.774)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.3439 (0.3439) ([0.219]+[0.125])	Prec@1 93.750 (93.750)
 * Prec@1 88.790
current lr 1.00000e-02
Grad=  tensor(8.4658, device='cuda:0')
Epoch: [142][0/391]	Time 0.220 (0.220)	Data 0.186 (0.186)	Loss 0.2021 (0.2021) ([0.077]+[0.125])	Prec@1 96.094 (96.094)
Epoch: [142][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1893 (0.2185) ([0.064]+[0.125])	Prec@1 98.438 (96.805)
Epoch: [142][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.3613 (0.2278) ([0.236]+[0.125])	Prec@1 92.188 (96.525)
Epoch: [142][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.2595 (0.2258) ([0.134]+[0.125])	Prec@1 96.094 (96.610)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.3890 (0.3890) ([0.264]+[0.125])	Prec@1 90.625 (90.625)
 * Prec@1 89.500
current lr 1.00000e-02
Grad=  tensor(6.8545, device='cuda:0')
Epoch: [143][0/391]	Time 0.261 (0.261)	Data 0.226 (0.226)	Loss 0.2114 (0.2114) ([0.086]+[0.125])	Prec@1 96.875 (96.875)
Epoch: [143][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.2525 (0.2283) ([0.127]+[0.125])	Prec@1 95.312 (96.535)
Epoch: [143][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.2407 (0.2262) ([0.115]+[0.125])	Prec@1 96.875 (96.615)
Epoch: [143][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2098 (0.2263) ([0.085]+[0.125])	Prec@1 96.094 (96.584)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.4107 (0.4107) ([0.285]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 90.040
current lr 1.00000e-02
Grad=  tensor(8.3079, device='cuda:0')
Epoch: [144][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.2557 (0.2557) ([0.130]+[0.125])	Prec@1 96.094 (96.094)
Epoch: [144][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2470 (0.2116) ([0.122]+[0.125])	Prec@1 96.875 (97.045)
Epoch: [144][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2277 (0.2166) ([0.102]+[0.125])	Prec@1 96.875 (96.891)
Epoch: [144][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2368 (0.2204) ([0.112]+[0.125])	Prec@1 96.094 (96.719)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3924 (0.3924) ([0.267]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 89.980
current lr 1.00000e-02
Grad=  tensor(6.0614, device='cuda:0')
Epoch: [145][0/391]	Time 0.206 (0.206)	Data 0.178 (0.178)	Loss 0.2087 (0.2087) ([0.083]+[0.125])	Prec@1 96.875 (96.875)
Epoch: [145][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2391 (0.2153) ([0.114]+[0.125])	Prec@1 96.875 (97.022)
Epoch: [145][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2124 (0.2208) ([0.087]+[0.125])	Prec@1 97.656 (96.828)
Epoch: [145][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1564 (0.2254) ([0.031]+[0.125])	Prec@1 100.000 (96.699)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.3990 (0.3990) ([0.273]+[0.125])	Prec@1 91.406 (91.406)
 * Prec@1 89.940
current lr 1.00000e-02
Grad=  tensor(4.4650, device='cuda:0')
Epoch: [146][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1845 (0.1845) ([0.059]+[0.125])	Prec@1 98.438 (98.438)
Epoch: [146][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.2540 (0.2136) ([0.129]+[0.125])	Prec@1 96.094 (97.053)
Epoch: [146][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2680 (0.2169) ([0.143]+[0.125])	Prec@1 96.094 (96.891)
Epoch: [146][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.2990 (0.2192) ([0.174]+[0.125])	Prec@1 93.750 (96.789)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.4567 (0.4567) ([0.331]+[0.126])	Prec@1 90.625 (90.625)
 * Prec@1 88.830
current lr 1.00000e-02
Grad=  tensor(3.0970, device='cuda:0')
Epoch: [147][0/391]	Time 0.253 (0.253)	Data 0.220 (0.220)	Loss 0.1756 (0.1756) ([0.050]+[0.126])	Prec@1 98.438 (98.438)
Epoch: [147][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.2168 (0.2133) ([0.091]+[0.125])	Prec@1 96.875 (96.999)
Epoch: [147][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1792 (0.2229) ([0.054]+[0.126])	Prec@1 97.656 (96.755)
Epoch: [147][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.2535 (0.2275) ([0.128]+[0.126])	Prec@1 96.094 (96.551)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.4918 (0.4918) ([0.366]+[0.126])	Prec@1 85.938 (85.938)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(1.5095, device='cuda:0')
Epoch: [148][0/391]	Time 0.211 (0.211)	Data 0.180 (0.180)	Loss 0.1578 (0.1578) ([0.032]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [148][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1593 (0.2075) ([0.034]+[0.125])	Prec@1 99.219 (97.300)
Epoch: [148][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2190 (0.2178) ([0.094]+[0.126])	Prec@1 96.094 (96.902)
Epoch: [148][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.2243 (0.2250) ([0.099]+[0.126])	Prec@1 97.656 (96.693)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.4789 (0.4789) ([0.353]+[0.126])	Prec@1 91.406 (91.406)
 * Prec@1 89.820
current lr 1.00000e-02
Grad=  tensor(10.1463, device='cuda:0')
Epoch: [149][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.1755 (0.1755) ([0.050]+[0.126])	Prec@1 96.875 (96.875)
Epoch: [149][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.2104 (0.2165) ([0.085]+[0.126])	Prec@1 96.094 (97.045)
Epoch: [149][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.2349 (0.2172) ([0.109]+[0.125])	Prec@1 96.094 (96.937)
Epoch: [149][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.2605 (0.2203) ([0.135]+[0.125])	Prec@1 93.750 (96.833)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.5171 (0.5171) ([0.392]+[0.126])	Prec@1 90.625 (90.625)
 * Prec@1 90.500
current lr 1.00000e-03
Grad=  tensor(7.1101, device='cuda:0')
Epoch: [150][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.1839 (0.1839) ([0.058]+[0.126])	Prec@1 97.656 (97.656)
Epoch: [150][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1579 (0.1924) ([0.033]+[0.124])	Prec@1 99.219 (97.780)
Epoch: [150][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1645 (0.1841) ([0.040]+[0.124])	Prec@1 99.219 (98.010)
Epoch: [150][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1738 (0.1776) ([0.049]+[0.124])	Prec@1 98.438 (98.230)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.4201 (0.4201) ([0.296]+[0.124])	Prec@1 93.750 (93.750)
 * Prec@1 92.390
current lr 1.00000e-03
Grad=  tensor(3.4228, device='cuda:0')
Epoch: [151][0/391]	Time 0.223 (0.223)	Data 0.194 (0.194)	Loss 0.1761 (0.1761) ([0.052]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [151][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1467 (0.1565) ([0.023]+[0.124])	Prec@1 99.219 (99.025)
Epoch: [151][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1677 (0.1567) ([0.044]+[0.124])	Prec@1 98.438 (99.024)
Epoch: [151][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1756 (0.1557) ([0.052]+[0.124])	Prec@1 98.438 (99.032)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.4320 (0.4320) ([0.308]+[0.124])	Prec@1 92.188 (92.188)
 * Prec@1 92.860
current lr 1.00000e-03
Grad=  tensor(1.2843, device='cuda:0')
Epoch: [152][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1348 (0.1348) ([0.011]+[0.124])	Prec@1 99.219 (99.219)
Epoch: [152][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1536 (0.1474) ([0.030]+[0.124])	Prec@1 99.219 (99.288)
Epoch: [152][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1426 (0.1475) ([0.019]+[0.124])	Prec@1 100.000 (99.324)
Epoch: [152][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1484 (0.1473) ([0.025]+[0.124])	Prec@1 99.219 (99.336)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.4144 (0.4144) ([0.291]+[0.123])	Prec@1 92.969 (92.969)
 * Prec@1 92.840
current lr 1.00000e-03
Grad=  tensor(8.8317, device='cuda:0')
Epoch: [153][0/391]	Time 0.261 (0.261)	Data 0.226 (0.226)	Loss 0.1779 (0.1779) ([0.054]+[0.123])	Prec@1 98.438 (98.438)
Epoch: [153][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1608 (0.1473) ([0.037]+[0.123])	Prec@1 98.438 (99.273)
Epoch: [153][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.1335 (0.1459) ([0.010]+[0.123])	Prec@1 100.000 (99.339)
Epoch: [153][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1416 (0.1458) ([0.018]+[0.123])	Prec@1 99.219 (99.362)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.4321 (0.4321) ([0.309]+[0.123])	Prec@1 92.188 (92.188)
 * Prec@1 93.090
current lr 1.00000e-03
Grad=  tensor(5.0906, device='cuda:0')
Epoch: [154][0/391]	Time 0.232 (0.232)	Data 0.200 (0.200)	Loss 0.1436 (0.1436) ([0.020]+[0.123])	Prec@1 99.219 (99.219)
Epoch: [154][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1419 (0.1422) ([0.019]+[0.123])	Prec@1 99.219 (99.350)
Epoch: [154][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1295 (0.1408) ([0.007]+[0.123])	Prec@1 100.000 (99.452)
Epoch: [154][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1285 (0.1416) ([0.006]+[0.123])	Prec@1 100.000 (99.426)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.4531 (0.4531) ([0.330]+[0.123])	Prec@1 92.188 (92.188)
 * Prec@1 93.100
current lr 1.00000e-03
Grad=  tensor(3.2968, device='cuda:0')
Epoch: [155][0/391]	Time 0.205 (0.205)	Data 0.173 (0.173)	Loss 0.1520 (0.1520) ([0.029]+[0.123])	Prec@1 99.219 (99.219)
Epoch: [155][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1417 (0.1423) ([0.019]+[0.123])	Prec@1 99.219 (99.373)
Epoch: [155][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1422 (0.1406) ([0.020]+[0.123])	Prec@1 100.000 (99.475)
Epoch: [155][300/391]	Time 0.028 (0.025)	Data 0.000 (0.001)	Loss 0.1339 (0.1400) ([0.011]+[0.122])	Prec@1 99.219 (99.496)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4200 (0.4200) ([0.298]+[0.122])	Prec@1 93.750 (93.750)
 * Prec@1 92.880
current lr 1.00000e-03
Grad=  tensor(0.2970, device='cuda:0')
Epoch: [156][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.1281 (0.1281) ([0.006]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [156][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1438 (0.1389) ([0.021]+[0.122])	Prec@1 99.219 (99.489)
Epoch: [156][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1362 (0.1383) ([0.014]+[0.122])	Prec@1 100.000 (99.510)
Epoch: [156][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1258 (0.1390) ([0.004]+[0.122])	Prec@1 100.000 (99.483)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4294 (0.4294) ([0.307]+[0.122])	Prec@1 93.750 (93.750)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(8.5912, device='cuda:0')
Epoch: [157][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1556 (0.1556) ([0.034]+[0.122])	Prec@1 98.438 (98.438)
Epoch: [157][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1461 (0.1370) ([0.024]+[0.122])	Prec@1 99.219 (99.567)
Epoch: [157][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1335 (0.1369) ([0.012]+[0.122])	Prec@1 100.000 (99.549)
Epoch: [157][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1560 (0.1361) ([0.034]+[0.122])	Prec@1 98.438 (99.577)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.4585 (0.4585) ([0.337]+[0.122])	Prec@1 92.188 (92.188)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(2.8209, device='cuda:0')
Epoch: [158][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.1361 (0.1361) ([0.014]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [158][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1243 (0.1348) ([0.003]+[0.122])	Prec@1 100.000 (99.598)
Epoch: [158][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1248 (0.1349) ([0.003]+[0.122])	Prec@1 100.000 (99.600)
Epoch: [158][300/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 0.1306 (0.1353) ([0.009]+[0.121])	Prec@1 100.000 (99.585)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4720 (0.4720) ([0.351]+[0.121])	Prec@1 92.188 (92.188)
 * Prec@1 93.180
current lr 1.00000e-03
Grad=  tensor(3.9831, device='cuda:0')
Epoch: [159][0/391]	Time 0.216 (0.216)	Data 0.186 (0.186)	Loss 0.1375 (0.1375) ([0.016]+[0.121])	Prec@1 99.219 (99.219)
Epoch: [159][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1540 (0.1352) ([0.033]+[0.121])	Prec@1 99.219 (99.551)
Epoch: [159][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1365 (0.1346) ([0.015]+[0.121])	Prec@1 99.219 (99.584)
Epoch: [159][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1244 (0.1339) ([0.003]+[0.121])	Prec@1 100.000 (99.618)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.4635 (0.4635) ([0.342]+[0.121])	Prec@1 92.188 (92.188)
 * Prec@1 93.160
current lr 1.00000e-03
Grad=  tensor(0.6775, device='cuda:0')
Epoch: [160][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1287 (0.1287) ([0.008]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [160][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1260 (0.1313) ([0.005]+[0.121])	Prec@1 100.000 (99.729)
Epoch: [160][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1283 (0.1314) ([0.007]+[0.121])	Prec@1 100.000 (99.728)
Epoch: [160][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1334 (0.1317) ([0.013]+[0.121])	Prec@1 100.000 (99.707)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.4598 (0.4598) ([0.339]+[0.121])	Prec@1 92.188 (92.188)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.2311, device='cuda:0')
Epoch: [161][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.1247 (0.1247) ([0.004]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [161][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1472 (0.1312) ([0.027]+[0.121])	Prec@1 99.219 (99.691)
Epoch: [161][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1234 (0.1311) ([0.003]+[0.120])	Prec@1 100.000 (99.701)
Epoch: [161][300/391]	Time 0.028 (0.025)	Data 0.000 (0.001)	Loss 0.1270 (0.1318) ([0.007]+[0.120])	Prec@1 100.000 (99.668)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.4578 (0.4578) ([0.337]+[0.120])	Prec@1 93.750 (93.750)
 * Prec@1 93.030
current lr 1.00000e-03
Grad=  tensor(0.1942, device='cuda:0')
Epoch: [162][0/391]	Time 0.236 (0.236)	Data 0.206 (0.206)	Loss 0.1251 (0.1251) ([0.005]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [162][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1236 (0.1305) ([0.003]+[0.120])	Prec@1 100.000 (99.745)
Epoch: [162][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1316 (0.1312) ([0.011]+[0.120])	Prec@1 100.000 (99.697)
Epoch: [162][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1282 (0.1312) ([0.008]+[0.120])	Prec@1 100.000 (99.670)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.4981 (0.4981) ([0.378]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(0.9544, device='cuda:0')
Epoch: [163][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.1490 (0.1490) ([0.029]+[0.120])	Prec@1 99.219 (99.219)
Epoch: [163][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1297 (0.1284) ([0.010]+[0.120])	Prec@1 99.219 (99.791)
Epoch: [163][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1269 (0.1286) ([0.007]+[0.120])	Prec@1 100.000 (99.775)
Epoch: [163][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1350 (0.1286) ([0.015]+[0.120])	Prec@1 99.219 (99.756)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.4969 (0.4969) ([0.377]+[0.120])	Prec@1 91.406 (91.406)
 * Prec@1 93.120
current lr 1.00000e-03
Grad=  tensor(0.9331, device='cuda:0')
Epoch: [164][0/391]	Time 0.228 (0.228)	Data 0.196 (0.196)	Loss 0.1279 (0.1279) ([0.008]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [164][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1286 (0.1271) ([0.009]+[0.120])	Prec@1 100.000 (99.791)
Epoch: [164][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1214 (0.1275) ([0.002]+[0.119])	Prec@1 100.000 (99.755)
Epoch: [164][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1221 (0.1277) ([0.003]+[0.119])	Prec@1 100.000 (99.756)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.4551 (0.4551) ([0.336]+[0.119])	Prec@1 91.406 (91.406)
 * Prec@1 92.950
current lr 1.00000e-03
Grad=  tensor(1.6189, device='cuda:0')
Epoch: [165][0/391]	Time 0.227 (0.227)	Data 0.198 (0.198)	Loss 0.1303 (0.1303) ([0.011]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [165][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1256 (0.1275) ([0.006]+[0.119])	Prec@1 100.000 (99.729)
Epoch: [165][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1217 (0.1275) ([0.003]+[0.119])	Prec@1 100.000 (99.747)
Epoch: [165][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1212 (0.1280) ([0.002]+[0.119])	Prec@1 100.000 (99.738)
Test: [0/79]	Time 0.232 (0.232)	Loss 0.4778 (0.4778) ([0.359]+[0.119])	Prec@1 91.406 (91.406)
 * Prec@1 93.160
current lr 1.00000e-03
Grad=  tensor(1.0197, device='cuda:0')
Epoch: [166][0/391]	Time 0.231 (0.231)	Data 0.202 (0.202)	Loss 0.1258 (0.1258) ([0.007]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [166][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1213 (0.1273) ([0.002]+[0.119])	Prec@1 100.000 (99.760)
Epoch: [166][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1241 (0.1275) ([0.005]+[0.119])	Prec@1 100.000 (99.778)
Epoch: [166][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1254 (0.1272) ([0.007]+[0.119])	Prec@1 100.000 (99.785)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.4923 (0.4923) ([0.374]+[0.119])	Prec@1 92.188 (92.188)
 * Prec@1 93.080
current lr 1.00000e-03
Grad=  tensor(1.5224, device='cuda:0')
Epoch: [167][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1257 (0.1257) ([0.007]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [167][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1196 (0.1275) ([0.001]+[0.119])	Prec@1 100.000 (99.737)
Epoch: [167][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1255 (0.1266) ([0.007]+[0.118])	Prec@1 100.000 (99.786)
Epoch: [167][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1207 (0.1271) ([0.002]+[0.118])	Prec@1 100.000 (99.746)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4913 (0.4913) ([0.373]+[0.118])	Prec@1 92.188 (92.188)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(0.4123, device='cuda:0')
Epoch: [168][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.1213 (0.1213) ([0.003]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [168][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1195 (0.1250) ([0.001]+[0.118])	Prec@1 100.000 (99.830)
Epoch: [168][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1204 (0.1255) ([0.002]+[0.118])	Prec@1 100.000 (99.817)
Epoch: [168][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1263 (0.1253) ([0.008]+[0.118])	Prec@1 99.219 (99.803)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.4514 (0.4514) ([0.333]+[0.118])	Prec@1 92.188 (92.188)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(4.3053, device='cuda:0')
Epoch: [169][0/391]	Time 0.212 (0.212)	Data 0.184 (0.184)	Loss 0.1578 (0.1578) ([0.040]+[0.118])	Prec@1 98.438 (98.438)
Epoch: [169][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1196 (0.1254) ([0.002]+[0.118])	Prec@1 100.000 (99.737)
Epoch: [169][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1233 (0.1249) ([0.006]+[0.118])	Prec@1 100.000 (99.790)
Epoch: [169][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.1238 (0.1254) ([0.006]+[0.118])	Prec@1 100.000 (99.774)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4543 (0.4543) ([0.337]+[0.118])	Prec@1 92.188 (92.188)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(0.1340, device='cuda:0')
Epoch: [170][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.1195 (0.1195) ([0.002]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [170][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1217 (0.1249) ([0.004]+[0.118])	Prec@1 100.000 (99.822)
Epoch: [170][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1642 (0.1246) ([0.047]+[0.117])	Prec@1 98.438 (99.841)
Epoch: [170][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1230 (0.1246) ([0.006]+[0.117])	Prec@1 100.000 (99.821)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.4459 (0.4459) ([0.329]+[0.117])	Prec@1 91.406 (91.406)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(0.0902, device='cuda:0')
Epoch: [171][0/391]	Time 0.227 (0.227)	Data 0.194 (0.194)	Loss 0.1183 (0.1183) ([0.001]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [171][100/391]	Time 0.028 (0.027)	Data 0.000 (0.002)	Loss 0.1293 (0.1235) ([0.012]+[0.117])	Prec@1 99.219 (99.838)
Epoch: [171][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1345 (0.1236) ([0.017]+[0.117])	Prec@1 99.219 (99.821)
Epoch: [171][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1189 (0.1237) ([0.002]+[0.117])	Prec@1 100.000 (99.821)
Test: [0/79]	Time 0.223 (0.223)	Loss 0.4631 (0.4631) ([0.346]+[0.117])	Prec@1 92.188 (92.188)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(1.2220, device='cuda:0')
Epoch: [172][0/391]	Time 0.251 (0.251)	Data 0.217 (0.217)	Loss 0.1222 (0.1222) ([0.005]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [172][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1266 (0.1235) ([0.010]+[0.117])	Prec@1 99.219 (99.776)
Epoch: [172][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1201 (0.1246) ([0.003]+[0.117])	Prec@1 100.000 (99.767)
Epoch: [172][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1314 (0.1245) ([0.015]+[0.117])	Prec@1 99.219 (99.779)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4636 (0.4636) ([0.347]+[0.117])	Prec@1 92.188 (92.188)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(11.1453, device='cuda:0')
Epoch: [173][0/391]	Time 0.216 (0.216)	Data 0.180 (0.180)	Loss 0.1309 (0.1309) ([0.014]+[0.117])	Prec@1 99.219 (99.219)
Epoch: [173][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1205 (0.1229) ([0.004]+[0.117])	Prec@1 100.000 (99.807)
Epoch: [173][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1211 (0.1224) ([0.005]+[0.116])	Prec@1 100.000 (99.841)
Epoch: [173][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1179 (0.1220) ([0.002]+[0.116])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4309 (0.4309) ([0.315]+[0.116])	Prec@1 91.406 (91.406)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(5.2880, device='cuda:0')
Epoch: [174][0/391]	Time 0.267 (0.267)	Data 0.232 (0.232)	Loss 0.1328 (0.1328) ([0.017]+[0.116])	Prec@1 99.219 (99.219)
Epoch: [174][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1190 (0.1223) ([0.003]+[0.116])	Prec@1 100.000 (99.814)
Epoch: [174][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1187 (0.1235) ([0.003]+[0.116])	Prec@1 100.000 (99.771)
Epoch: [174][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1267 (0.1227) ([0.011]+[0.116])	Prec@1 99.219 (99.805)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3932 (0.3932) ([0.277]+[0.116])	Prec@1 92.969 (92.969)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(0.1062, device='cuda:0')
Epoch: [175][0/391]	Time 0.219 (0.219)	Data 0.180 (0.180)	Loss 0.1180 (0.1180) ([0.002]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [175][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1184 (0.1213) ([0.003]+[0.116])	Prec@1 100.000 (99.876)
Epoch: [175][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1234 (0.1209) ([0.008]+[0.116])	Prec@1 100.000 (99.907)
Epoch: [175][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1227 (0.1205) ([0.007]+[0.116])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4342 (0.4342) ([0.319]+[0.116])	Prec@1 92.969 (92.969)
 * Prec@1 93.160
current lr 1.00000e-03
Grad=  tensor(11.1810, device='cuda:0')
Epoch: [176][0/391]	Time 0.256 (0.256)	Data 0.224 (0.224)	Loss 0.1364 (0.1364) ([0.021]+[0.116])	Prec@1 98.438 (98.438)
Epoch: [176][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1206 (0.1219) ([0.005]+[0.116])	Prec@1 100.000 (99.822)
Epoch: [176][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1209 (0.1212) ([0.005]+[0.115])	Prec@1 100.000 (99.868)
Epoch: [176][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1259 (0.1212) ([0.011]+[0.115])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.4495 (0.4495) ([0.334]+[0.115])	Prec@1 91.406 (91.406)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.1004, device='cuda:0')
Epoch: [177][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.1166 (0.1166) ([0.001]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [177][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1313 (0.1220) ([0.016]+[0.115])	Prec@1 99.219 (99.822)
Epoch: [177][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1196 (0.1215) ([0.004]+[0.115])	Prec@1 100.000 (99.841)
Epoch: [177][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1423 (0.1216) ([0.027]+[0.115])	Prec@1 99.219 (99.842)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4302 (0.4302) ([0.315]+[0.115])	Prec@1 91.406 (91.406)
 * Prec@1 93.480
current lr 1.00000e-03
Grad=  tensor(3.8004, device='cuda:0')
Epoch: [178][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.1265 (0.1265) ([0.012]+[0.115])	Prec@1 99.219 (99.219)
Epoch: [178][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1167 (0.1192) ([0.002]+[0.115])	Prec@1 100.000 (99.923)
Epoch: [178][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1330 (0.1197) ([0.018]+[0.115])	Prec@1 99.219 (99.903)
Epoch: [178][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1176 (0.1200) ([0.003]+[0.115])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.4460 (0.4460) ([0.331]+[0.115])	Prec@1 92.969 (92.969)
 * Prec@1 93.140
current lr 1.00000e-03
Grad=  tensor(0.0887, device='cuda:0')
Epoch: [179][0/391]	Time 0.201 (0.201)	Data 0.172 (0.172)	Loss 0.1155 (0.1155) ([0.001]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [179][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1177 (0.1202) ([0.003]+[0.115])	Prec@1 100.000 (99.830)
Epoch: [179][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1245 (0.1194) ([0.010]+[0.114])	Prec@1 99.219 (99.872)
Epoch: [179][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1352 (0.1197) ([0.021]+[0.114])	Prec@1 99.219 (99.857)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.4303 (0.4303) ([0.316]+[0.114])	Prec@1 90.625 (90.625)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(0.1321, device='cuda:0')
Epoch: [180][0/391]	Time 0.237 (0.237)	Data 0.198 (0.198)	Loss 0.1157 (0.1157) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [180][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1186 (0.1196) ([0.004]+[0.114])	Prec@1 100.000 (99.830)
Epoch: [180][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1151 (0.1196) ([0.001]+[0.114])	Prec@1 100.000 (99.841)
Epoch: [180][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1265 (0.1199) ([0.012]+[0.114])	Prec@1 99.219 (99.847)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.4344 (0.4344) ([0.320]+[0.114])	Prec@1 92.188 (92.188)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.0892, device='cuda:0')
Epoch: [181][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1149 (0.1149) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [181][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1173 (0.1183) ([0.003]+[0.114])	Prec@1 100.000 (99.876)
Epoch: [181][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1149 (0.1186) ([0.001]+[0.114])	Prec@1 100.000 (99.864)
Epoch: [181][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1188 (0.1190) ([0.005]+[0.114])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.4338 (0.4338) ([0.320]+[0.114])	Prec@1 92.969 (92.969)
 * Prec@1 93.400
current lr 1.00000e-03
Grad=  tensor(0.1120, device='cuda:0')
Epoch: [182][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1150 (0.1150) ([0.001]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [182][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1156 (0.1179) ([0.002]+[0.114])	Prec@1 100.000 (99.899)
Epoch: [182][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1155 (0.1176) ([0.002]+[0.114])	Prec@1 100.000 (99.903)
Epoch: [182][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1269 (0.1185) ([0.013]+[0.113])	Prec@1 99.219 (99.849)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.4385 (0.4385) ([0.325]+[0.113])	Prec@1 93.750 (93.750)
 * Prec@1 93.190
current lr 1.00000e-03
Grad=  tensor(0.2372, device='cuda:0')
Epoch: [183][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.1172 (0.1172) ([0.004]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [183][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1153 (0.1179) ([0.002]+[0.113])	Prec@1 100.000 (99.845)
Epoch: [183][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1147 (0.1181) ([0.002]+[0.113])	Prec@1 100.000 (99.848)
Epoch: [183][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1174 (0.1181) ([0.004]+[0.113])	Prec@1 100.000 (99.857)
Test: [0/79]	Time 0.204 (0.204)	Loss 0.4329 (0.4329) ([0.320]+[0.113])	Prec@1 92.188 (92.188)
 * Prec@1 93.150
current lr 1.00000e-03
Grad=  tensor(0.1574, device='cuda:0')
Epoch: [184][0/391]	Time 0.258 (0.258)	Data 0.226 (0.226)	Loss 0.1150 (0.1150) ([0.002]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [184][100/391]	Time 0.025 (0.028)	Data 0.000 (0.003)	Loss 0.1192 (0.1167) ([0.006]+[0.113])	Prec@1 100.000 (99.899)
Epoch: [184][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1192 (0.1173) ([0.006]+[0.113])	Prec@1 100.000 (99.891)
Epoch: [184][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1161 (0.1176) ([0.003]+[0.113])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.4715 (0.4715) ([0.359]+[0.113])	Prec@1 92.969 (92.969)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.1283, device='cuda:0')
Epoch: [185][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.1147 (0.1147) ([0.002]+[0.113])	Prec@1 100.000 (100.000)
Epoch: [185][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1160 (0.1167) ([0.003]+[0.113])	Prec@1 100.000 (99.884)
Epoch: [185][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1134 (0.1171) ([0.001]+[0.113])	Prec@1 100.000 (99.864)
Epoch: [185][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1364 (0.1173) ([0.024]+[0.112])	Prec@1 99.219 (99.865)
Test: [0/79]	Time 0.238 (0.238)	Loss 0.5141 (0.5141) ([0.402]+[0.112])	Prec@1 92.969 (92.969)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.1044, device='cuda:0')
Epoch: [186][0/391]	Time 0.302 (0.302)	Data 0.271 (0.271)	Loss 0.1134 (0.1134) ([0.001]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [186][100/391]	Time 0.025 (0.028)	Data 0.000 (0.003)	Loss 0.1173 (0.1177) ([0.005]+[0.112])	Prec@1 100.000 (99.853)
Epoch: [186][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1141 (0.1175) ([0.002]+[0.112])	Prec@1 100.000 (99.864)
Epoch: [186][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1544 (0.1173) ([0.042]+[0.112])	Prec@1 99.219 (99.868)
Test: [0/79]	Time 0.205 (0.205)	Loss 0.4823 (0.4823) ([0.370]+[0.112])	Prec@1 92.969 (92.969)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.0906, device='cuda:0')
Epoch: [187][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1131 (0.1131) ([0.001]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [187][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1147 (0.1161) ([0.003]+[0.112])	Prec@1 100.000 (99.892)
Epoch: [187][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1142 (0.1163) ([0.002]+[0.112])	Prec@1 100.000 (99.887)
Epoch: [187][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1177 (0.1170) ([0.006]+[0.112])	Prec@1 100.000 (99.849)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.4741 (0.4741) ([0.362]+[0.112])	Prec@1 92.188 (92.188)
 * Prec@1 93.400
current lr 1.00000e-03
Grad=  tensor(0.2327, device='cuda:0')
Epoch: [188][0/391]	Time 0.211 (0.211)	Data 0.180 (0.180)	Loss 0.1139 (0.1139) ([0.002]+[0.112])	Prec@1 100.000 (100.000)
Epoch: [188][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1145 (0.1164) ([0.003]+[0.112])	Prec@1 100.000 (99.930)
Epoch: [188][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1125 (0.1162) ([0.001]+[0.112])	Prec@1 100.000 (99.911)
Epoch: [188][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1147 (0.1162) ([0.003]+[0.112])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.180 (0.180)	Loss 0.4735 (0.4735) ([0.362]+[0.111])	Prec@1 92.969 (92.969)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(0.1166, device='cuda:0')
Epoch: [189][0/391]	Time 0.254 (0.254)	Data 0.222 (0.222)	Loss 0.1129 (0.1129) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [189][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1136 (0.1146) ([0.002]+[0.111])	Prec@1 100.000 (99.946)
Epoch: [189][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1123 (0.1148) ([0.001]+[0.111])	Prec@1 100.000 (99.938)
Epoch: [189][300/391]	Time 0.027 (0.026)	Data 0.000 (0.001)	Loss 0.1124 (0.1152) ([0.001]+[0.111])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.4811 (0.4811) ([0.370]+[0.111])	Prec@1 92.969 (92.969)
 * Prec@1 93.010
current lr 1.00000e-03
Grad=  tensor(0.4009, device='cuda:0')
Epoch: [190][0/391]	Time 0.242 (0.242)	Data 0.209 (0.209)	Loss 0.1139 (0.1139) ([0.003]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [190][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1174 (0.1149) ([0.006]+[0.111])	Prec@1 100.000 (99.915)
Epoch: [190][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1575 (0.1149) ([0.047]+[0.111])	Prec@1 98.438 (99.899)
Epoch: [190][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1154 (0.1152) ([0.004]+[0.111])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.4389 (0.4389) ([0.328]+[0.111])	Prec@1 92.969 (92.969)
 * Prec@1 93.220
current lr 1.00000e-03
Grad=  tensor(0.1123, device='cuda:0')
Epoch: [191][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.1120 (0.1120) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [191][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1138 (0.1146) ([0.003]+[0.111])	Prec@1 100.000 (99.915)
Epoch: [191][200/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.1113 (0.1147) ([0.001]+[0.111])	Prec@1 100.000 (99.887)
Epoch: [191][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1116 (0.1148) ([0.001]+[0.111])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4449 (0.4449) ([0.334]+[0.111])	Prec@1 94.531 (94.531)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.0878, device='cuda:0')
Epoch: [192][0/391]	Time 0.211 (0.211)	Data 0.181 (0.181)	Loss 0.1114 (0.1114) ([0.001]+[0.111])	Prec@1 100.000 (100.000)
Epoch: [192][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1112 (0.1151) ([0.001]+[0.110])	Prec@1 100.000 (99.861)
Epoch: [192][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1118 (0.1145) ([0.001]+[0.110])	Prec@1 100.000 (99.891)
Epoch: [192][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1114 (0.1146) ([0.001]+[0.110])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.4532 (0.4532) ([0.343]+[0.110])	Prec@1 92.188 (92.188)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(0.9712, device='cuda:0')
Epoch: [193][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.1136 (0.1136) ([0.003]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [193][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1107 (0.1146) ([0.001]+[0.110])	Prec@1 100.000 (99.869)
Epoch: [193][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1117 (0.1144) ([0.002]+[0.110])	Prec@1 100.000 (99.876)
Epoch: [193][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1142 (0.1143) ([0.004]+[0.110])	Prec@1 100.000 (99.870)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.4308 (0.4308) ([0.321]+[0.110])	Prec@1 93.750 (93.750)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(0.1323, device='cuda:0')
Epoch: [194][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.1121 (0.1121) ([0.002]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [194][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1214 (0.1142) ([0.012]+[0.110])	Prec@1 99.219 (99.899)
Epoch: [194][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1107 (0.1137) ([0.001]+[0.110])	Prec@1 100.000 (99.903)
Epoch: [194][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1125 (0.1138) ([0.003]+[0.110])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.222 (0.222)	Loss 0.4470 (0.4470) ([0.337]+[0.110])	Prec@1 93.750 (93.750)
 * Prec@1 93.120
current lr 1.00000e-03
Grad=  tensor(5.5751, device='cuda:0')
Epoch: [195][0/391]	Time 0.251 (0.251)	Data 0.215 (0.215)	Loss 0.1220 (0.1220) ([0.012]+[0.110])	Prec@1 100.000 (100.000)
Epoch: [195][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1107 (0.1134) ([0.001]+[0.110])	Prec@1 100.000 (99.884)
Epoch: [195][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1115 (0.1136) ([0.002]+[0.109])	Prec@1 100.000 (99.891)
Epoch: [195][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1105 (0.1138) ([0.001]+[0.109])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.217 (0.217)	Loss 0.4167 (0.4167) ([0.307]+[0.109])	Prec@1 92.969 (92.969)
 * Prec@1 93.270
current lr 1.00000e-03
Grad=  tensor(0.0971, device='cuda:0')
Epoch: [196][0/391]	Time 0.245 (0.245)	Data 0.212 (0.212)	Loss 0.1103 (0.1103) ([0.001]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [196][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1123 (0.1132) ([0.003]+[0.109])	Prec@1 100.000 (99.923)
Epoch: [196][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1127 (0.1127) ([0.004]+[0.109])	Prec@1 100.000 (99.930)
Epoch: [196][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1099 (0.1134) ([0.001]+[0.109])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.4576 (0.4576) ([0.349]+[0.109])	Prec@1 92.188 (92.188)
 * Prec@1 93.370
current lr 1.00000e-03
Grad=  tensor(0.1524, device='cuda:0')
Epoch: [197][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.1104 (0.1104) ([0.001]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [197][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1102 (0.1140) ([0.001]+[0.109])	Prec@1 100.000 (99.853)
Epoch: [197][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1097 (0.1134) ([0.001]+[0.109])	Prec@1 100.000 (99.872)
Epoch: [197][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1142 (0.1133) ([0.005]+[0.109])	Prec@1 100.000 (99.873)
Test: [0/79]	Time 0.234 (0.234)	Loss 0.4556 (0.4556) ([0.347]+[0.109])	Prec@1 91.406 (91.406)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(0.1721, device='cuda:0')
Epoch: [198][0/391]	Time 0.244 (0.244)	Data 0.213 (0.213)	Loss 0.1101 (0.1101) ([0.001]+[0.109])	Prec@1 100.000 (100.000)
Epoch: [198][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1095 (0.1136) ([0.001]+[0.109])	Prec@1 100.000 (99.830)
Epoch: [198][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1091 (0.1126) ([0.001]+[0.109])	Prec@1 100.000 (99.887)
Epoch: [198][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1112 (0.1124) ([0.003]+[0.108])	Prec@1 100.000 (99.894)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4440 (0.4440) ([0.336]+[0.108])	Prec@1 92.188 (92.188)
 * Prec@1 93.190
current lr 1.00000e-03
Grad=  tensor(0.0889, device='cuda:0')
Epoch: [199][0/391]	Time 0.216 (0.216)	Data 0.185 (0.185)	Loss 0.1093 (0.1093) ([0.001]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [199][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1088 (0.1124) ([0.001]+[0.108])	Prec@1 100.000 (99.892)
Epoch: [199][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1109 (0.1117) ([0.003]+[0.108])	Prec@1 100.000 (99.918)
Epoch: [199][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1122 (0.1123) ([0.004]+[0.108])	Prec@1 100.000 (99.891)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.4355 (0.4355) ([0.327]+[0.108])	Prec@1 91.406 (91.406)
 * Prec@1 93.060
current lr 1.00000e-03
Grad=  tensor(1.2517, device='cuda:0')
Epoch: [200][0/391]	Time 0.210 (0.210)	Data 0.181 (0.181)	Loss 0.1143 (0.1143) ([0.006]+[0.108])	Prec@1 100.000 (100.000)
Epoch: [200][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1096 (0.1112) ([0.002]+[0.108])	Prec@1 100.000 (99.946)
Epoch: [200][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1091 (0.1121) ([0.001]+[0.108])	Prec@1 100.000 (99.899)
Epoch: [200][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1232 (0.1124) ([0.015]+[0.108])	Prec@1 99.219 (99.878)
Test: [0/79]	Time 0.207 (0.207)	Loss 0.4246 (0.4246) ([0.317]+[0.108])	Prec@1 91.406 (91.406)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(12.1380, device='cuda:0')
Epoch: [201][0/391]	Time 0.211 (0.211)	Data 0.182 (0.182)	Loss 0.1184 (0.1184) ([0.011]+[0.108])	Prec@1 99.219 (99.219)
Epoch: [201][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1096 (0.1119) ([0.002]+[0.108])	Prec@1 100.000 (99.892)
Epoch: [201][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1090 (0.1113) ([0.001]+[0.108])	Prec@1 100.000 (99.918)
Epoch: [201][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1084 (0.1112) ([0.001]+[0.108])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.4458 (0.4458) ([0.338]+[0.107])	Prec@1 92.969 (92.969)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(0.5475, device='cuda:0')
Epoch: [202][0/391]	Time 0.201 (0.201)	Data 0.172 (0.172)	Loss 0.1096 (0.1096) ([0.002]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [202][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1083 (0.1113) ([0.001]+[0.107])	Prec@1 100.000 (99.884)
Epoch: [202][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1080 (0.1111) ([0.001]+[0.107])	Prec@1 100.000 (99.895)
Epoch: [202][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1104 (0.1112) ([0.003]+[0.107])	Prec@1 100.000 (99.888)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.4522 (0.4522) ([0.345]+[0.107])	Prec@1 92.188 (92.188)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(6.8188, device='cuda:0')
Epoch: [203][0/391]	Time 0.215 (0.215)	Data 0.185 (0.185)	Loss 0.1174 (0.1174) ([0.010]+[0.107])	Prec@1 99.219 (99.219)
Epoch: [203][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1099 (0.1109) ([0.003]+[0.107])	Prec@1 100.000 (99.899)
Epoch: [203][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1119 (0.1105) ([0.005]+[0.107])	Prec@1 100.000 (99.907)
Epoch: [203][300/391]	Time 0.025 (0.024)	Data 0.000 (0.001)	Loss 0.1097 (0.1108) ([0.003]+[0.107])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.4738 (0.4738) ([0.367]+[0.107])	Prec@1 92.188 (92.188)
 * Prec@1 93.070
current lr 1.00000e-03
Grad=  tensor(0.0918, device='cuda:0')
Epoch: [204][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.1077 (0.1077) ([0.001]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [204][100/391]	Time 0.028 (0.028)	Data 0.000 (0.002)	Loss 0.1075 (0.1107) ([0.001]+[0.107])	Prec@1 100.000 (99.884)
Epoch: [204][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1103 (0.1108) ([0.004]+[0.107])	Prec@1 100.000 (99.880)
Epoch: [204][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1085 (0.1106) ([0.002]+[0.107])	Prec@1 100.000 (99.886)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.4897 (0.4897) ([0.383]+[0.107])	Prec@1 92.188 (92.188)
 * Prec@1 93.120
current lr 1.00000e-03
Grad=  tensor(0.1030, device='cuda:0')
Epoch: [205][0/391]	Time 0.203 (0.203)	Data 0.175 (0.175)	Loss 0.1078 (0.1078) ([0.001]+[0.107])	Prec@1 100.000 (100.000)
Epoch: [205][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1077 (0.1104) ([0.001]+[0.106])	Prec@1 100.000 (99.899)
Epoch: [205][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.1076 (0.1102) ([0.001]+[0.106])	Prec@1 100.000 (99.907)
Epoch: [205][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1261 (0.1101) ([0.020]+[0.106])	Prec@1 99.219 (99.907)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.4649 (0.4649) ([0.359]+[0.106])	Prec@1 92.969 (92.969)
 * Prec@1 93.200
current lr 1.00000e-03
Grad=  tensor(0.6733, device='cuda:0')
Epoch: [206][0/391]	Time 0.207 (0.207)	Data 0.178 (0.178)	Loss 0.1113 (0.1113) ([0.005]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [206][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.1077 (0.1101) ([0.002]+[0.106])	Prec@1 100.000 (99.915)
Epoch: [206][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1092 (0.1104) ([0.003]+[0.106])	Prec@1 100.000 (99.887)
Epoch: [206][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1070 (0.1104) ([0.001]+[0.106])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.4350 (0.4350) ([0.329]+[0.106])	Prec@1 94.531 (94.531)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.1665, device='cuda:0')
Epoch: [207][0/391]	Time 0.264 (0.264)	Data 0.230 (0.230)	Loss 0.1075 (0.1075) ([0.002]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [207][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1068 (0.1092) ([0.001]+[0.106])	Prec@1 100.000 (99.907)
Epoch: [207][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1067 (0.1090) ([0.001]+[0.106])	Prec@1 100.000 (99.922)
Epoch: [207][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1071 (0.1090) ([0.001]+[0.106])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.4326 (0.4326) ([0.327]+[0.106])	Prec@1 94.531 (94.531)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(0.0994, device='cuda:0')
Epoch: [208][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1070 (0.1070) ([0.001]+[0.106])	Prec@1 100.000 (100.000)
Epoch: [208][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1076 (0.1095) ([0.002]+[0.106])	Prec@1 100.000 (99.884)
Epoch: [208][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1075 (0.1092) ([0.002]+[0.106])	Prec@1 100.000 (99.899)
Epoch: [208][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1082 (0.1090) ([0.003]+[0.105])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4524 (0.4524) ([0.347]+[0.105])	Prec@1 92.969 (92.969)
 * Prec@1 93.460
current lr 1.00000e-03
Grad=  tensor(0.1547, device='cuda:0')
Epoch: [209][0/391]	Time 0.202 (0.202)	Data 0.173 (0.173)	Loss 0.1068 (0.1068) ([0.001]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [209][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.1063 (0.1084) ([0.001]+[0.105])	Prec@1 100.000 (99.915)
Epoch: [209][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1061 (0.1087) ([0.001]+[0.105])	Prec@1 100.000 (99.911)
Epoch: [209][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1066 (0.1085) ([0.001]+[0.105])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.213 (0.213)	Loss 0.4504 (0.4504) ([0.345]+[0.105])	Prec@1 93.750 (93.750)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(0.0872, device='cuda:0')
Epoch: [210][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.1056 (0.1056) ([0.000]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [210][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1345 (0.1096) ([0.030]+[0.105])	Prec@1 99.219 (99.892)
Epoch: [210][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1064 (0.1105) ([0.001]+[0.105])	Prec@1 100.000 (99.841)
Epoch: [210][300/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1060 (0.1101) ([0.001]+[0.105])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.183 (0.183)	Loss 0.4202 (0.4202) ([0.315]+[0.105])	Prec@1 91.406 (91.406)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.0890, device='cuda:0')
Epoch: [211][0/391]	Time 0.212 (0.212)	Data 0.180 (0.180)	Loss 0.1057 (0.1057) ([0.001]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [211][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1461 (0.1085) ([0.041]+[0.105])	Prec@1 98.438 (99.876)
Epoch: [211][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1115 (0.1081) ([0.007]+[0.105])	Prec@1 99.219 (99.907)
Epoch: [211][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1083 (0.1088) ([0.004]+[0.105])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.4737 (0.4737) ([0.369]+[0.105])	Prec@1 92.969 (92.969)
 * Prec@1 93.210
current lr 1.00000e-03
Grad=  tensor(0.0913, device='cuda:0')
Epoch: [212][0/391]	Time 0.229 (0.229)	Data 0.199 (0.199)	Loss 0.1053 (0.1053) ([0.001]+[0.105])	Prec@1 100.000 (100.000)
Epoch: [212][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1057 (0.1065) ([0.001]+[0.104])	Prec@1 100.000 (99.969)
Epoch: [212][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1503 (0.1070) ([0.046]+[0.104])	Prec@1 99.219 (99.953)
Epoch: [212][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1050 (0.1070) ([0.001]+[0.104])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.4537 (0.4537) ([0.349]+[0.104])	Prec@1 91.406 (91.406)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(0.1021, device='cuda:0')
Epoch: [213][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1052 (0.1052) ([0.001]+[0.104])	Prec@1 100.000 (100.000)
Epoch: [213][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1186 (0.1080) ([0.014]+[0.104])	Prec@1 99.219 (99.892)
Epoch: [213][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1046 (0.1074) ([0.001]+[0.104])	Prec@1 100.000 (99.918)
Epoch: [213][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.1054 (0.1073) ([0.001]+[0.104])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.4753 (0.4753) ([0.371]+[0.104])	Prec@1 91.406 (91.406)
 * Prec@1 93.100
current lr 1.00000e-03
Grad=  tensor(0.0888, device='cuda:0')
Epoch: [214][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1045 (0.1045) ([0.001]+[0.104])	Prec@1 100.000 (100.000)
Epoch: [214][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1047 (0.1071) ([0.001]+[0.104])	Prec@1 100.000 (99.915)
Epoch: [214][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1043 (0.1071) ([0.001]+[0.104])	Prec@1 100.000 (99.922)
Epoch: [214][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1093 (0.1070) ([0.006]+[0.104])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.4773 (0.4773) ([0.374]+[0.104])	Prec@1 91.406 (91.406)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.3125, device='cuda:0')
Epoch: [215][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.1051 (0.1051) ([0.002]+[0.104])	Prec@1 100.000 (100.000)
Epoch: [215][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1042 (0.1066) ([0.001]+[0.103])	Prec@1 100.000 (99.923)
Epoch: [215][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1049 (0.1068) ([0.002]+[0.103])	Prec@1 100.000 (99.914)
Epoch: [215][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1043 (0.1067) ([0.001]+[0.103])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.4825 (0.4825) ([0.379]+[0.103])	Prec@1 90.625 (90.625)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(1.1234, device='cuda:0')
Epoch: [216][0/391]	Time 0.215 (0.215)	Data 0.183 (0.183)	Loss 0.1073 (0.1073) ([0.004]+[0.103])	Prec@1 100.000 (100.000)
Epoch: [216][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1047 (0.1066) ([0.002]+[0.103])	Prec@1 100.000 (99.907)
Epoch: [216][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1057 (0.1066) ([0.003]+[0.103])	Prec@1 100.000 (99.903)
Epoch: [216][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1041 (0.1064) ([0.001]+[0.103])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.4530 (0.4530) ([0.350]+[0.103])	Prec@1 92.188 (92.188)
 * Prec@1 93.260
current lr 1.00000e-03
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [217][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.1035 (0.1035) ([0.001]+[0.103])	Prec@1 100.000 (100.000)
Epoch: [217][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1037 (0.1064) ([0.001]+[0.103])	Prec@1 100.000 (99.899)
Epoch: [217][200/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 0.1036 (0.1065) ([0.001]+[0.103])	Prec@1 100.000 (99.911)
Epoch: [217][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1035 (0.1064) ([0.001]+[0.103])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.4626 (0.4626) ([0.360]+[0.103])	Prec@1 92.969 (92.969)
 * Prec@1 93.370
current lr 1.00000e-03
Grad=  tensor(7.6935, device='cuda:0')
Epoch: [218][0/391]	Time 0.206 (0.206)	Data 0.175 (0.175)	Loss 0.1099 (0.1099) ([0.007]+[0.103])	Prec@1 99.219 (99.219)
Epoch: [218][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1077 (0.1055) ([0.005]+[0.102])	Prec@1 100.000 (99.930)
Epoch: [218][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1034 (0.1058) ([0.001]+[0.102])	Prec@1 100.000 (99.911)
Epoch: [218][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1077 (0.1056) ([0.005]+[0.102])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4486 (0.4486) ([0.346]+[0.102])	Prec@1 92.969 (92.969)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(11.9044, device='cuda:0')
Epoch: [219][0/391]	Time 0.205 (0.205)	Data 0.174 (0.174)	Loss 0.1123 (0.1123) ([0.010]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [219][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1043 (0.1051) ([0.002]+[0.102])	Prec@1 100.000 (99.946)
Epoch: [219][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1032 (0.1052) ([0.001]+[0.102])	Prec@1 100.000 (99.934)
Epoch: [219][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1052 (0.1051) ([0.003]+[0.102])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.4377 (0.4377) ([0.336]+[0.102])	Prec@1 94.531 (94.531)
 * Prec@1 93.240
current lr 1.00000e-03
Grad=  tensor(18.5882, device='cuda:0')
Epoch: [220][0/391]	Time 0.231 (0.231)	Data 0.202 (0.202)	Loss 0.1133 (0.1133) ([0.011]+[0.102])	Prec@1 99.219 (99.219)
Epoch: [220][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1041 (0.1050) ([0.002]+[0.102])	Prec@1 100.000 (99.938)
Epoch: [220][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1130 (0.1054) ([0.011]+[0.102])	Prec@1 99.219 (99.918)
Epoch: [220][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1129 (0.1053) ([0.011]+[0.102])	Prec@1 99.219 (99.922)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.4297 (0.4297) ([0.328]+[0.102])	Prec@1 92.969 (92.969)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(1.8400, device='cuda:0')
Epoch: [221][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.1060 (0.1060) ([0.004]+[0.102])	Prec@1 100.000 (100.000)
Epoch: [221][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1026 (0.1047) ([0.001]+[0.102])	Prec@1 100.000 (99.915)
Epoch: [221][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1341 (0.1053) ([0.033]+[0.102])	Prec@1 99.219 (99.887)
Epoch: [221][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1044 (0.1054) ([0.003]+[0.101])	Prec@1 100.000 (99.896)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4441 (0.4441) ([0.343]+[0.101])	Prec@1 92.188 (92.188)
 * Prec@1 93.170
current lr 1.00000e-03
Grad=  tensor(0.1408, device='cuda:0')
Epoch: [222][0/391]	Time 0.264 (0.264)	Data 0.228 (0.228)	Loss 0.1030 (0.1030) ([0.002]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [222][100/391]	Time 0.026 (0.028)	Data 0.000 (0.002)	Loss 0.1024 (0.1051) ([0.001]+[0.101])	Prec@1 100.000 (99.915)
Epoch: [222][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1019 (0.1048) ([0.001]+[0.101])	Prec@1 100.000 (99.930)
Epoch: [222][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1084 (0.1046) ([0.007]+[0.101])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.236 (0.236)	Loss 0.4926 (0.4926) ([0.391]+[0.101])	Prec@1 92.969 (92.969)
 * Prec@1 93.360
current lr 1.00000e-03
Grad=  tensor(0.0882, device='cuda:0')
Epoch: [223][0/391]	Time 0.258 (0.258)	Data 0.226 (0.226)	Loss 0.1018 (0.1018) ([0.001]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [223][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1058 (0.1046) ([0.005]+[0.101])	Prec@1 100.000 (99.923)
Epoch: [223][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1016 (0.1052) ([0.001]+[0.101])	Prec@1 100.000 (99.891)
Epoch: [223][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1016 (0.1048) ([0.001]+[0.101])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.4942 (0.4942) ([0.393]+[0.101])	Prec@1 93.750 (93.750)
 * Prec@1 93.090
current lr 1.00000e-03
Grad=  tensor(0.1261, device='cuda:0')
Epoch: [224][0/391]	Time 0.213 (0.213)	Data 0.182 (0.182)	Loss 0.1023 (0.1023) ([0.001]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [224][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.1014 (0.1034) ([0.001]+[0.101])	Prec@1 100.000 (99.961)
Epoch: [224][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1019 (0.1039) ([0.001]+[0.101])	Prec@1 100.000 (99.942)
Epoch: [224][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1022 (0.1038) ([0.002]+[0.101])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.4209 (0.4209) ([0.320]+[0.101])	Prec@1 94.531 (94.531)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(0.5974, device='cuda:0')
Epoch: [225][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.1032 (0.1032) ([0.003]+[0.101])	Prec@1 100.000 (100.000)
Epoch: [225][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1018 (0.1033) ([0.001]+[0.100])	Prec@1 100.000 (99.923)
Epoch: [225][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1050 (0.1040) ([0.005]+[0.100])	Prec@1 100.000 (99.911)
Epoch: [225][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1018 (0.1039) ([0.002]+[0.100])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.209 (0.209)	Loss 0.4580 (0.4580) ([0.358]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.130
current lr 1.00000e-03
Grad=  tensor(0.0881, device='cuda:0')
Epoch: [226][0/391]	Time 0.214 (0.214)	Data 0.182 (0.182)	Loss 0.1011 (0.1011) ([0.001]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [226][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1008 (0.1036) ([0.001]+[0.100])	Prec@1 100.000 (99.938)
Epoch: [226][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1046 (0.1035) ([0.005]+[0.100])	Prec@1 100.000 (99.926)
Epoch: [226][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1008 (0.1035) ([0.001]+[0.100])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.4485 (0.4485) ([0.349]+[0.100])	Prec@1 92.188 (92.188)
 * Prec@1 93.030
current lr 1.00000e-03
Grad=  tensor(0.8927, device='cuda:0')
Epoch: [227][0/391]	Time 0.208 (0.208)	Data 0.177 (0.177)	Loss 0.1035 (0.1035) ([0.004]+[0.100])	Prec@1 100.000 (100.000)
Epoch: [227][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.1008 (0.1030) ([0.001]+[0.100])	Prec@1 100.000 (99.907)
Epoch: [227][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1004 (0.1029) ([0.001]+[0.100])	Prec@1 100.000 (99.922)
Epoch: [227][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1009 (0.1032) ([0.001]+[0.100])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.4308 (0.4308) ([0.331]+[0.100])	Prec@1 93.750 (93.750)
 * Prec@1 93.310
current lr 1.00000e-03
Grad=  tensor(24.6980, device='cuda:0')
Epoch: [228][0/391]	Time 0.209 (0.209)	Data 0.179 (0.179)	Loss 0.1324 (0.1324) ([0.033]+[0.100])	Prec@1 99.219 (99.219)
Epoch: [228][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1010 (0.1032) ([0.001]+[0.100])	Prec@1 100.000 (99.907)
Epoch: [228][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1083 (0.1026) ([0.009]+[0.099])	Prec@1 100.000 (99.918)
Epoch: [228][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1002 (0.1026) ([0.001]+[0.099])	Prec@1 100.000 (99.920)
Test: [0/79]	Time 0.225 (0.225)	Loss 0.4451 (0.4451) ([0.346]+[0.099])	Prec@1 92.969 (92.969)
 * Prec@1 93.090
current lr 1.00000e-03
Grad=  tensor(0.0995, device='cuda:0')
Epoch: [229][0/391]	Time 0.223 (0.223)	Data 0.190 (0.190)	Loss 0.1003 (0.1003) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [229][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1003 (0.1017) ([0.001]+[0.099])	Prec@1 100.000 (99.961)
Epoch: [229][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.1002 (0.1021) ([0.001]+[0.099])	Prec@1 100.000 (99.926)
Epoch: [229][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.1003 (0.1023) ([0.001]+[0.099])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.221 (0.221)	Loss 0.4322 (0.4322) ([0.333]+[0.099])	Prec@1 94.531 (94.531)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(0.0929, device='cuda:0')
Epoch: [230][0/391]	Time 0.257 (0.257)	Data 0.223 (0.223)	Loss 0.1000 (0.1000) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [230][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1162 (0.1020) ([0.017]+[0.099])	Prec@1 99.219 (99.954)
Epoch: [230][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.1000 (0.1020) ([0.001]+[0.099])	Prec@1 100.000 (99.946)
Epoch: [230][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0995 (0.1020) ([0.001]+[0.099])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.4357 (0.4357) ([0.337]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.0879, device='cuda:0')
Epoch: [231][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.0994 (0.0994) ([0.001]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [231][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1016 (0.1017) ([0.003]+[0.099])	Prec@1 100.000 (99.938)
Epoch: [231][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0993 (0.1022) ([0.001]+[0.099])	Prec@1 100.000 (99.907)
Epoch: [231][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1027 (0.1021) ([0.004]+[0.099])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.4528 (0.4528) ([0.354]+[0.099])	Prec@1 93.750 (93.750)
 * Prec@1 93.350
current lr 1.00000e-03
Grad=  tensor(3.8750, device='cuda:0')
Epoch: [232][0/391]	Time 0.201 (0.201)	Data 0.173 (0.173)	Loss 0.1017 (0.1017) ([0.003]+[0.099])	Prec@1 100.000 (100.000)
Epoch: [232][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1001 (0.1012) ([0.002]+[0.098])	Prec@1 100.000 (99.930)
Epoch: [232][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0992 (0.1017) ([0.001]+[0.098])	Prec@1 100.000 (99.907)
Epoch: [232][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0994 (0.1016) ([0.001]+[0.098])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.4534 (0.4534) ([0.355]+[0.098])	Prec@1 93.750 (93.750)
 * Prec@1 93.190
current lr 1.00000e-03
Grad=  tensor(0.0875, device='cuda:0')
Epoch: [233][0/391]	Time 0.223 (0.223)	Data 0.194 (0.194)	Loss 0.0988 (0.0988) ([0.001]+[0.098])	Prec@1 100.000 (100.000)
Epoch: [233][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0992 (0.1017) ([0.001]+[0.098])	Prec@1 100.000 (99.907)
Epoch: [233][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.1123 (0.1020) ([0.014]+[0.098])	Prec@1 99.219 (99.903)
Epoch: [233][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0994 (0.1021) ([0.001]+[0.098])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3998 (0.3998) ([0.302]+[0.098])	Prec@1 93.750 (93.750)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(0.8601, device='cuda:0')
Epoch: [234][0/391]	Time 0.205 (0.205)	Data 0.176 (0.176)	Loss 0.1000 (0.1000) ([0.002]+[0.098])	Prec@1 100.000 (100.000)
Epoch: [234][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1038 (0.1017) ([0.006]+[0.098])	Prec@1 100.000 (99.930)
Epoch: [234][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0995 (0.1014) ([0.002]+[0.098])	Prec@1 100.000 (99.926)
Epoch: [234][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0990 (0.1010) ([0.001]+[0.098])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4503 (0.4503) ([0.353]+[0.098])	Prec@1 92.188 (92.188)
 * Prec@1 93.190
current lr 1.00000e-03
Grad=  tensor(0.1364, device='cuda:0')
Epoch: [235][0/391]	Time 0.215 (0.215)	Data 0.183 (0.183)	Loss 0.0987 (0.0987) ([0.001]+[0.098])	Prec@1 100.000 (100.000)
Epoch: [235][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1048 (0.1002) ([0.007]+[0.098])	Prec@1 99.219 (99.938)
Epoch: [235][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.1047 (0.1001) ([0.007]+[0.098])	Prec@1 99.219 (99.946)
Epoch: [235][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0983 (0.1007) ([0.001]+[0.097])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.211 (0.211)	Loss 0.4436 (0.4436) ([0.346]+[0.097])	Prec@1 94.531 (94.531)
 * Prec@1 93.300
current lr 1.00000e-03
Grad=  tensor(0.1304, device='cuda:0')
Epoch: [236][0/391]	Time 0.214 (0.214)	Data 0.180 (0.180)	Loss 0.0985 (0.0985) ([0.001]+[0.097])	Prec@1 100.000 (100.000)
Epoch: [236][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0980 (0.0997) ([0.001]+[0.097])	Prec@1 100.000 (99.938)
Epoch: [236][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0984 (0.1001) ([0.001]+[0.097])	Prec@1 100.000 (99.930)
Epoch: [236][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1157 (0.1000) ([0.019]+[0.097])	Prec@1 99.219 (99.935)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.4705 (0.4705) ([0.373]+[0.097])	Prec@1 92.969 (92.969)
 * Prec@1 93.500
current lr 1.00000e-03
Grad=  tensor(0.1113, device='cuda:0')
Epoch: [237][0/391]	Time 0.261 (0.261)	Data 0.227 (0.227)	Loss 0.0984 (0.0984) ([0.001]+[0.097])	Prec@1 100.000 (100.000)
Epoch: [237][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.1005 (0.0990) ([0.003]+[0.097])	Prec@1 100.000 (99.969)
Epoch: [237][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1110 (0.0996) ([0.014]+[0.097])	Prec@1 99.219 (99.938)
Epoch: [237][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.1017 (0.0995) ([0.005]+[0.097])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.3870 (0.3870) ([0.290]+[0.097])	Prec@1 95.312 (95.312)
 * Prec@1 93.250
current lr 1.00000e-03
Grad=  tensor(0.0918, device='cuda:0')
Epoch: [238][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.0978 (0.0978) ([0.001]+[0.097])	Prec@1 100.000 (100.000)
Epoch: [238][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.0976 (0.0992) ([0.001]+[0.097])	Prec@1 100.000 (99.930)
Epoch: [238][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1026 (0.0999) ([0.006]+[0.097])	Prec@1 100.000 (99.911)
Epoch: [238][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.1043 (0.1000) ([0.008]+[0.097])	Prec@1 99.219 (99.909)
Test: [0/79]	Time 0.184 (0.184)	Loss 0.3481 (0.3481) ([0.252]+[0.097])	Prec@1 95.312 (95.312)
 * Prec@1 93.450
current lr 1.00000e-03
Grad=  tensor(4.2528, device='cuda:0')
Epoch: [239][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.1057 (0.1057) ([0.009]+[0.097])	Prec@1 99.219 (99.219)
Epoch: [239][100/391]	Time 0.024 (0.025)	Data 0.000 (0.002)	Loss 0.0972 (0.0989) ([0.001]+[0.097])	Prec@1 100.000 (99.938)
Epoch: [239][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0994 (0.0988) ([0.003]+[0.096])	Prec@1 100.000 (99.949)
Epoch: [239][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0975 (0.0992) ([0.001]+[0.096])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3516 (0.3516) ([0.255]+[0.096])	Prec@1 95.312 (95.312)
 * Prec@1 93.290
current lr 1.00000e-03
Grad=  tensor(2.1445, device='cuda:0')
Epoch: [240][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.1020 (0.1020) ([0.006]+[0.096])	Prec@1 100.000 (100.000)
Epoch: [240][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0970 (0.0990) ([0.001]+[0.096])	Prec@1 100.000 (99.930)
Epoch: [240][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0985 (0.0986) ([0.002]+[0.096])	Prec@1 100.000 (99.949)
Epoch: [240][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0968 (0.0989) ([0.001]+[0.096])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.229 (0.229)	Loss 0.3727 (0.3727) ([0.277]+[0.096])	Prec@1 93.750 (93.750)
 * Prec@1 93.430
current lr 1.00000e-03
Grad=  tensor(0.0971, device='cuda:0')
Epoch: [241][0/391]	Time 0.227 (0.227)	Data 0.196 (0.196)	Loss 0.0971 (0.0971) ([0.001]+[0.096])	Prec@1 100.000 (100.000)
Epoch: [241][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0980 (0.0999) ([0.002]+[0.096])	Prec@1 100.000 (99.907)
Epoch: [241][200/391]	Time 0.030 (0.025)	Data 0.000 (0.001)	Loss 0.0967 (0.0998) ([0.001]+[0.096])	Prec@1 100.000 (99.903)
Epoch: [241][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0985 (0.0994) ([0.003]+[0.096])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.3837 (0.3837) ([0.288]+[0.096])	Prec@1 94.531 (94.531)
 * Prec@1 93.420
current lr 1.00000e-03
Grad=  tensor(0.0874, device='cuda:0')
Epoch: [242][0/391]	Time 0.220 (0.220)	Data 0.190 (0.190)	Loss 0.0964 (0.0964) ([0.001]+[0.096])	Prec@1 100.000 (100.000)
Epoch: [242][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0964 (0.0986) ([0.001]+[0.096])	Prec@1 100.000 (99.907)
Epoch: [242][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0970 (0.0990) ([0.001]+[0.096])	Prec@1 100.000 (99.899)
Epoch: [242][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0973 (0.0990) ([0.002]+[0.096])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.3789 (0.3789) ([0.283]+[0.096])	Prec@1 95.312 (95.312)
 * Prec@1 93.390
current lr 1.00000e-03
Grad=  tensor(0.1221, device='cuda:0')
Epoch: [243][0/391]	Time 0.208 (0.208)	Data 0.179 (0.179)	Loss 0.0970 (0.0970) ([0.002]+[0.096])	Prec@1 100.000 (100.000)
Epoch: [243][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0970 (0.0978) ([0.002]+[0.095])	Prec@1 100.000 (99.954)
Epoch: [243][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0962 (0.0983) ([0.001]+[0.095])	Prec@1 100.000 (99.930)
Epoch: [243][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0969 (0.0984) ([0.002]+[0.095])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.3995 (0.3995) ([0.304]+[0.095])	Prec@1 94.531 (94.531)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(1.2703, device='cuda:0')
Epoch: [244][0/391]	Time 0.211 (0.211)	Data 0.182 (0.182)	Loss 0.0987 (0.0987) ([0.003]+[0.095])	Prec@1 100.000 (100.000)
Epoch: [244][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0960 (0.0979) ([0.001]+[0.095])	Prec@1 100.000 (99.946)
Epoch: [244][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0959 (0.0980) ([0.001]+[0.095])	Prec@1 100.000 (99.938)
Epoch: [244][300/391]	Time 0.026 (0.024)	Data 0.000 (0.001)	Loss 0.0966 (0.0981) ([0.002]+[0.095])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.3858 (0.3858) ([0.291]+[0.095])	Prec@1 94.531 (94.531)
 * Prec@1 93.230
current lr 1.00000e-03
Grad=  tensor(0.2068, device='cuda:0')
Epoch: [245][0/391]	Time 0.260 (0.260)	Data 0.227 (0.227)	Loss 0.0967 (0.0967) ([0.002]+[0.095])	Prec@1 100.000 (100.000)
Epoch: [245][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.1032 (0.0975) ([0.008]+[0.095])	Prec@1 99.219 (99.923)
Epoch: [245][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0957 (0.0976) ([0.001]+[0.095])	Prec@1 100.000 (99.926)
Epoch: [245][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0955 (0.0979) ([0.001]+[0.095])	Prec@1 100.000 (99.912)
Test: [0/79]	Time 0.196 (0.196)	Loss 0.4068 (0.4068) ([0.312]+[0.095])	Prec@1 94.531 (94.531)
 * Prec@1 93.280
current lr 1.00000e-03
Grad=  tensor(0.0960, device='cuda:0')
Epoch: [246][0/391]	Time 0.218 (0.218)	Data 0.187 (0.187)	Loss 0.0959 (0.0959) ([0.001]+[0.095])	Prec@1 100.000 (100.000)
Epoch: [246][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0953 (0.0982) ([0.001]+[0.095])	Prec@1 100.000 (99.915)
Epoch: [246][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0952 (0.0982) ([0.001]+[0.095])	Prec@1 100.000 (99.922)
Epoch: [246][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0961 (0.0979) ([0.002]+[0.095])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.4570 (0.4570) ([0.363]+[0.094])	Prec@1 93.750 (93.750)
 * Prec@1 93.330
current lr 1.00000e-03
Grad=  tensor(0.0877, device='cuda:0')
Epoch: [247][0/391]	Time 0.236 (0.236)	Data 0.201 (0.201)	Loss 0.0951 (0.0951) ([0.001]+[0.094])	Prec@1 100.000 (100.000)
Epoch: [247][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0952 (0.0968) ([0.001]+[0.094])	Prec@1 100.000 (99.954)
Epoch: [247][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.0978 (0.0969) ([0.003]+[0.094])	Prec@1 100.000 (99.961)
Epoch: [247][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0949 (0.0970) ([0.001]+[0.094])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.190 (0.190)	Loss 0.4312 (0.4312) ([0.337]+[0.094])	Prec@1 92.969 (92.969)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(0.0884, device='cuda:0')
Epoch: [248][0/391]	Time 0.256 (0.256)	Data 0.222 (0.222)	Loss 0.0950 (0.0950) ([0.001]+[0.094])	Prec@1 100.000 (100.000)
Epoch: [248][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0949 (0.0965) ([0.001]+[0.094])	Prec@1 100.000 (99.961)
Epoch: [248][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0950 (0.0965) ([0.001]+[0.094])	Prec@1 100.000 (99.953)
Epoch: [248][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0946 (0.0965) ([0.001]+[0.094])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.4042 (0.4042) ([0.310]+[0.094])	Prec@1 92.969 (92.969)
 * Prec@1 93.110
current lr 1.00000e-03
Grad=  tensor(0.6081, device='cuda:0')
Epoch: [249][0/391]	Time 0.235 (0.235)	Data 0.203 (0.203)	Loss 0.0960 (0.0960) ([0.002]+[0.094])	Prec@1 100.000 (100.000)
Epoch: [249][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0955 (0.0977) ([0.002]+[0.094])	Prec@1 100.000 (99.884)
Epoch: [249][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0945 (0.0975) ([0.001]+[0.094])	Prec@1 100.000 (99.903)
Epoch: [249][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1070 (0.0972) ([0.013]+[0.094])	Prec@1 99.219 (99.912)
Test: [0/79]	Time 0.201 (0.201)	Loss 0.4140 (0.4140) ([0.320]+[0.094])	Prec@1 93.750 (93.750)
 * Prec@1 93.190
current lr 1.00000e-04
Grad=  tensor(0.9982, device='cuda:0')
Epoch: [250][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.0959 (0.0959) ([0.002]+[0.094])	Prec@1 100.000 (100.000)
Epoch: [250][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.1025 (0.0977) ([0.009]+[0.094])	Prec@1 99.219 (99.923)
Epoch: [250][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0944 (0.0970) ([0.001]+[0.094])	Prec@1 100.000 (99.926)
Epoch: [250][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1008 (0.0969) ([0.007]+[0.093])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3961 (0.3961) ([0.303]+[0.093])	Prec@1 92.188 (92.188)
 * Prec@1 93.360
current lr 1.00000e-04
Grad=  tensor(8.6907, device='cuda:0')
Epoch: [251][0/391]	Time 0.209 (0.209)	Data 0.178 (0.178)	Loss 0.1016 (0.1016) ([0.008]+[0.093])	Prec@1 99.219 (99.219)
Epoch: [251][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.0951 (0.0963) ([0.002]+[0.093])	Prec@1 100.000 (99.923)
Epoch: [251][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0942 (0.0960) ([0.001]+[0.093])	Prec@1 100.000 (99.930)
Epoch: [251][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0945 (0.0960) ([0.001]+[0.093])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.3788 (0.3788) ([0.285]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.0993, device='cuda:0')
Epoch: [252][0/391]	Time 0.207 (0.207)	Data 0.178 (0.178)	Loss 0.0946 (0.0946) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [252][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0952 (0.0965) ([0.002]+[0.093])	Prec@1 100.000 (99.954)
Epoch: [252][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0953 (0.0964) ([0.002]+[0.093])	Prec@1 100.000 (99.938)
Epoch: [252][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0955 (0.0963) ([0.002]+[0.093])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3834 (0.3834) ([0.290]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(0.0949, device='cuda:0')
Epoch: [253][0/391]	Time 0.202 (0.202)	Data 0.172 (0.172)	Loss 0.0945 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [253][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0941 (0.0959) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [253][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0957 (0.0959) ([0.002]+[0.093])	Prec@1 100.000 (99.946)
Epoch: [253][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0962) ([0.001]+[0.093])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.3769 (0.3769) ([0.283]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-04
Grad=  tensor(0.0879, device='cuda:0')
Epoch: [254][0/391]	Time 0.234 (0.234)	Data 0.203 (0.203)	Loss 0.0943 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0940 (0.0961) ([0.001]+[0.093])	Prec@1 100.000 (99.930)
Epoch: [254][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0941 (0.0960) ([0.001]+[0.093])	Prec@1 100.000 (99.942)
Epoch: [254][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0947 (0.0958) ([0.001]+[0.093])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.3906 (0.3906) ([0.297]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(0.0974, device='cuda:0')
Epoch: [255][0/391]	Time 0.208 (0.208)	Data 0.180 (0.180)	Loss 0.0943 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1090 (0.0957) ([0.016]+[0.093])	Prec@1 99.219 (99.938)
Epoch: [255][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0963 (0.0955) ([0.003]+[0.093])	Prec@1 100.000 (99.946)
Epoch: [255][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0954 (0.0954) ([0.002]+[0.093])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.195 (0.195)	Loss 0.3839 (0.3839) ([0.291]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.470
current lr 1.00000e-04
Grad=  tensor(0.0975, device='cuda:0')
Epoch: [256][0/391]	Time 0.204 (0.204)	Data 0.174 (0.174)	Loss 0.0947 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0959 (0.0947) ([0.003]+[0.093])	Prec@1 100.000 (99.985)
Epoch: [256][200/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.0939 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [256][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0949 (0.0953) ([0.002]+[0.093])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.3859 (0.3859) ([0.293]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-04
Grad=  tensor(0.1030, device='cuda:0')
Epoch: [257][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.0942 (0.0942) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [257][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0947 (0.0961) ([0.001]+[0.093])	Prec@1 100.000 (99.923)
Epoch: [257][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0956 (0.0955) ([0.002]+[0.093])	Prec@1 100.000 (99.953)
Epoch: [257][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0953) ([0.001]+[0.093])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.208 (0.208)	Loss 0.3834 (0.3834) ([0.290]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-04
Grad=  tensor(0.1388, device='cuda:0')
Epoch: [258][0/391]	Time 0.208 (0.208)	Data 0.178 (0.178)	Loss 0.0948 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.0938 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.992)
Epoch: [258][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0944 (0.0952) ([0.001]+[0.093])	Prec@1 100.000 (99.973)
Epoch: [258][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0940 (0.0954) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3984 (0.3984) ([0.305]+[0.093])	Prec@1 92.969 (92.969)
 * Prec@1 93.420
current lr 1.00000e-04
Grad=  tensor(0.3391, device='cuda:0')
Epoch: [259][0/391]	Time 0.209 (0.209)	Data 0.180 (0.180)	Loss 0.0952 (0.0952) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [259][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.0941 (0.0959) ([0.001]+[0.093])	Prec@1 100.000 (99.938)
Epoch: [259][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0954 (0.0956) ([0.002]+[0.093])	Prec@1 100.000 (99.946)
Epoch: [259][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1273 (0.0956) ([0.034]+[0.093])	Prec@1 99.219 (99.951)
Test: [0/79]	Time 0.224 (0.224)	Loss 0.3924 (0.3924) ([0.299]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.510
current lr 1.00000e-04
Grad=  tensor(0.1395, device='cuda:0')
Epoch: [260][0/391]	Time 0.261 (0.261)	Data 0.228 (0.228)	Loss 0.0945 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0940 (0.0958) ([0.001]+[0.093])	Prec@1 100.000 (99.946)
Epoch: [260][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0939 (0.0955) ([0.001]+[0.093])	Prec@1 100.000 (99.949)
Epoch: [260][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0953 (0.0955) ([0.002]+[0.093])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.3906 (0.3906) ([0.297]+[0.093])	Prec@1 92.969 (92.969)
 * Prec@1 93.420
current lr 1.00000e-04
Grad=  tensor(0.0877, device='cuda:0')
Epoch: [261][0/391]	Time 0.209 (0.209)	Data 0.177 (0.177)	Loss 0.0939 (0.0939) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [261][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0938 (0.0955) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [261][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0978 (0.0954) ([0.005]+[0.093])	Prec@1 100.000 (99.965)
Epoch: [261][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0937 (0.0955) ([0.001]+[0.093])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.235 (0.235)	Loss 0.4025 (0.4025) ([0.309]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(0.4560, device='cuda:0')
Epoch: [262][0/391]	Time 0.231 (0.231)	Data 0.201 (0.201)	Loss 0.0955 (0.0955) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [262][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0938 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [262][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0941 (0.0953) ([0.001]+[0.093])	Prec@1 100.000 (99.957)
Epoch: [262][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0942 (0.0952) ([0.001]+[0.093])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.198 (0.198)	Loss 0.3824 (0.3824) ([0.289]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.530
current lr 1.00000e-04
Grad=  tensor(1.4947, device='cuda:0')
Epoch: [263][0/391]	Time 0.203 (0.203)	Data 0.173 (0.173)	Loss 0.0963 (0.0963) ([0.003]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0941 (0.0953) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [263][200/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0952) ([0.001]+[0.093])	Prec@1 100.000 (99.957)
Epoch: [263][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0940 (0.0954) ([0.001]+[0.093])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.3795 (0.3795) ([0.286]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.290
current lr 1.00000e-04
Grad=  tensor(0.0880, device='cuda:0')
Epoch: [264][0/391]	Time 0.205 (0.205)	Data 0.175 (0.175)	Loss 0.0939 (0.0939) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0941 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.954)
Epoch: [264][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0951) ([0.001]+[0.093])	Prec@1 100.000 (99.949)
Epoch: [264][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0940 (0.0950) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3760 (0.3760) ([0.283]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.360
current lr 1.00000e-04
Grad=  tensor(0.4945, device='cuda:0')
Epoch: [265][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.0947 (0.0947) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0937 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [265][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.1053 (0.0950) ([0.012]+[0.093])	Prec@1 99.219 (99.965)
Epoch: [265][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.0938 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.3896 (0.3896) ([0.297]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.400
current lr 1.00000e-04
Grad=  tensor(0.0904, device='cuda:0')
Epoch: [266][0/391]	Time 0.247 (0.247)	Data 0.213 (0.213)	Loss 0.0939 (0.0939) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0939 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [266][200/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.0964 (0.0947) ([0.003]+[0.093])	Prec@1 100.000 (99.981)
Epoch: [266][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0938 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.3781 (0.3781) ([0.285]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.400
current lr 1.00000e-04
Grad=  tensor(0.0913, device='cuda:0')
Epoch: [267][0/391]	Time 0.218 (0.218)	Data 0.188 (0.188)	Loss 0.0940 (0.0940) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0940 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [267][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0937 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [267][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0937 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.3754 (0.3754) ([0.282]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.370
current lr 1.00000e-04
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [268][0/391]	Time 0.204 (0.204)	Data 0.175 (0.175)	Loss 0.0937 (0.0937) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0937 (0.0957) ([0.001]+[0.093])	Prec@1 100.000 (99.930)
Epoch: [268][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0936 (0.0950) ([0.001]+[0.093])	Prec@1 100.000 (99.953)
Epoch: [268][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.0937 (0.0951) ([0.001]+[0.093])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.3839 (0.3839) ([0.291]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.370
current lr 1.00000e-04
Grad=  tensor(0.0894, device='cuda:0')
Epoch: [269][0/391]	Time 0.229 (0.229)	Data 0.198 (0.198)	Loss 0.0939 (0.0939) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0936 (0.0946) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [269][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0937 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [269][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0942 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.220 (0.220)	Loss 0.3742 (0.3742) ([0.281]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [270][0/391]	Time 0.246 (0.246)	Data 0.213 (0.213)	Loss 0.0935 (0.0935) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0935 (0.0946) ([0.001]+[0.093])	Prec@1 100.000 (99.985)
Epoch: [270][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0936 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.973)
Epoch: [270][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0937 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.206 (0.206)	Loss 0.3686 (0.3686) ([0.276]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(1.9190, device='cuda:0')
Epoch: [271][0/391]	Time 0.225 (0.225)	Data 0.195 (0.195)	Loss 0.0965 (0.0965) ([0.004]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0952 (0.0946) ([0.002]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [271][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0944 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.981)
Epoch: [271][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0935 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.3732 (0.3732) ([0.280]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(0.0872, device='cuda:0')
Epoch: [272][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.0934 (0.0934) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0934 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [272][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0936 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [272][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0936 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.226 (0.226)	Loss 0.3659 (0.3659) ([0.273]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.0876, device='cuda:0')
Epoch: [273][0/391]	Time 0.237 (0.237)	Data 0.206 (0.206)	Loss 0.0935 (0.0935) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0966 (0.0944) ([0.004]+[0.093])	Prec@1 100.000 (99.985)
Epoch: [273][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0968 (0.0944) ([0.004]+[0.093])	Prec@1 100.000 (99.988)
Epoch: [273][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0937 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.3637 (0.3637) ([0.271]+[0.093])	Prec@1 95.312 (95.312)
 * Prec@1 93.370
current lr 1.00000e-04
Grad=  tensor(0.0876, device='cuda:0')
Epoch: [274][0/391]	Time 0.211 (0.211)	Data 0.180 (0.180)	Loss 0.0936 (0.0936) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0958 (0.0944) ([0.003]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [274][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0936 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [274][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0939 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3674 (0.3674) ([0.275]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(1.1847, device='cuda:0')
Epoch: [275][0/391]	Time 0.202 (0.202)	Data 0.171 (0.171)	Loss 0.0953 (0.0953) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0937 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [275][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0935 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [275][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.200 (0.200)	Loss 0.3714 (0.3714) ([0.279]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.500
current lr 1.00000e-04
Grad=  tensor(0.0883, device='cuda:0')
Epoch: [276][0/391]	Time 0.211 (0.211)	Data 0.174 (0.174)	Loss 0.0936 (0.0936) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0937 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [276][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0935 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [276][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.0941 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.3767 (0.3767) ([0.284]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(0.0911, device='cuda:0')
Epoch: [277][0/391]	Time 0.214 (0.214)	Data 0.183 (0.183)	Loss 0.0935 (0.0935) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0934 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [277][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0933 (0.0946) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [277][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0937 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.3804 (0.3804) ([0.288]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.540
current lr 1.00000e-04
Grad=  tensor(0.0954, device='cuda:0')
Epoch: [278][0/391]	Time 0.276 (0.276)	Data 0.241 (0.241)	Loss 0.0938 (0.0938) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.025 (0.028)	Data 0.000 (0.003)	Loss 0.0937 (0.0942) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [278][200/391]	Time 0.025 (0.027)	Data 0.000 (0.001)	Loss 0.0941 (0.0942) ([0.001]+[0.093])	Prec@1 100.000 (99.973)
Epoch: [278][300/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0934 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.187 (0.187)	Loss 0.3726 (0.3726) ([0.280]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.430
current lr 1.00000e-04
Grad=  tensor(0.0920, device='cuda:0')
Epoch: [279][0/391]	Time 0.212 (0.212)	Data 0.181 (0.181)	Loss 0.0939 (0.0939) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0936 (0.0940) ([0.001]+[0.093])	Prec@1 100.000 (99.985)
Epoch: [279][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0933 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.973)
Epoch: [279][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0938 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.214 (0.214)	Loss 0.3822 (0.3822) ([0.290]+[0.093])	Prec@1 92.969 (92.969)
 * Prec@1 93.500
current lr 1.00000e-04
Grad=  tensor(0.2175, device='cuda:0')
Epoch: [280][0/391]	Time 0.213 (0.213)	Data 0.183 (0.183)	Loss 0.0944 (0.0944) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [280][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.0933 (0.0940) ([0.001]+[0.093])	Prec@1 100.000 (99.992)
Epoch: [280][200/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0942 (0.0943) ([0.002]+[0.093])	Prec@1 100.000 (99.981)
Epoch: [280][300/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0942 (0.0942) ([0.002]+[0.093])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.3814 (0.3814) ([0.289]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.500
current lr 1.00000e-04
Grad=  tensor(0.0876, device='cuda:0')
Epoch: [281][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.0934 (0.0934) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [281][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.0932 (0.0941) ([0.001]+[0.093])	Prec@1 100.000 (99.977)
Epoch: [281][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0932 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.969)
Epoch: [281][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0933 (0.0943) ([0.001]+[0.093])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.192 (0.192)	Loss 0.3780 (0.3780) ([0.285]+[0.093])	Prec@1 92.969 (92.969)
 * Prec@1 93.450
current lr 1.00000e-04
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [282][0/391]	Time 0.201 (0.201)	Data 0.170 (0.170)	Loss 0.0932 (0.0932) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0938 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [282][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0935 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.965)
Epoch: [282][300/391]	Time 0.026 (0.025)	Data 0.000 (0.001)	Loss 0.0933 (0.0942) ([0.001]+[0.093])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.199 (0.199)	Loss 0.3900 (0.3900) ([0.297]+[0.093])	Prec@1 94.531 (94.531)
 * Prec@1 93.500
current lr 1.00000e-04
Grad=  tensor(0.1000, device='cuda:0')
Epoch: [283][0/391]	Time 0.206 (0.206)	Data 0.177 (0.177)	Loss 0.0937 (0.0937) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.026 (0.026)	Data 0.000 (0.002)	Loss 0.0962 (0.0943) ([0.004]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [283][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0941 (0.0943) ([0.002]+[0.093])	Prec@1 100.000 (99.965)
Epoch: [283][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1151 (0.0943) ([0.023]+[0.093])	Prec@1 99.219 (99.964)
Test: [0/79]	Time 0.231 (0.231)	Loss 0.3824 (0.3824) ([0.290]+[0.093])	Prec@1 92.969 (92.969)
 * Prec@1 93.470
current lr 1.00000e-04
Grad=  tensor(0.0884, device='cuda:0')
Epoch: [284][0/391]	Time 0.250 (0.250)	Data 0.219 (0.219)	Loss 0.0931 (0.0931) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [284][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0940 (0.0947) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [284][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0933 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.965)
Epoch: [284][300/391]	Time 0.029 (0.026)	Data 0.000 (0.001)	Loss 0.0936 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.3898 (0.3898) ([0.297]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.480
current lr 1.00000e-04
Grad=  tensor(0.3037, device='cuda:0')
Epoch: [285][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.0947 (0.0947) ([0.002]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0933 (0.0949) ([0.001]+[0.093])	Prec@1 100.000 (99.954)
Epoch: [285][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0931 (0.0945) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [285][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0934 (0.0944) ([0.001]+[0.093])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.3825 (0.3825) ([0.290]+[0.093])	Prec@1 93.750 (93.750)
 * Prec@1 93.510
current lr 1.00000e-04
Grad=  tensor(0.1295, device='cuda:0')
Epoch: [286][0/391]	Time 0.225 (0.225)	Data 0.195 (0.195)	Loss 0.0936 (0.0936) ([0.001]+[0.093])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.025 (0.026)	Data 0.000 (0.002)	Loss 0.0933 (0.0948) ([0.001]+[0.093])	Prec@1 100.000 (99.961)
Epoch: [286][200/391]	Time 0.024 (0.026)	Data 0.000 (0.001)	Loss 0.0931 (0.0944) ([0.001]+[0.092])	Prec@1 100.000 (99.965)
Epoch: [286][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.1311 (0.0944) ([0.039]+[0.092])	Prec@1 99.219 (99.966)
Test: [0/79]	Time 0.197 (0.197)	Loss 0.3670 (0.3670) ([0.275]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.440
current lr 1.00000e-04
Grad=  tensor(0.0900, device='cuda:0')
Epoch: [287][0/391]	Time 0.206 (0.206)	Data 0.176 (0.176)	Loss 0.0933 (0.0933) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0972 (0.0939) ([0.005]+[0.092])	Prec@1 100.000 (99.977)
Epoch: [287][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0933 (0.0944) ([0.001]+[0.092])	Prec@1 100.000 (99.957)
Epoch: [287][300/391]	Time 0.029 (0.025)	Data 0.000 (0.001)	Loss 0.0932 (0.0943) ([0.001]+[0.092])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.188 (0.188)	Loss 0.3740 (0.3740) ([0.282]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.400
current lr 1.00000e-04
Grad=  tensor(0.0883, device='cuda:0')
Epoch: [288][0/391]	Time 0.211 (0.211)	Data 0.177 (0.177)	Loss 0.0932 (0.0932) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0934 (0.0938) ([0.001]+[0.092])	Prec@1 100.000 (99.977)
Epoch: [288][200/391]	Time 0.028 (0.026)	Data 0.000 (0.001)	Loss 0.0931 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.977)
Epoch: [288][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0931 (0.0940) ([0.001]+[0.092])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.227 (0.227)	Loss 0.3788 (0.3788) ([0.286]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.520
current lr 1.00000e-04
Grad=  tensor(0.0875, device='cuda:0')
Epoch: [289][0/391]	Time 0.230 (0.230)	Data 0.197 (0.197)	Loss 0.0930 (0.0930) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.024 (0.027)	Data 0.000 (0.002)	Loss 0.0931 (0.0943) ([0.001]+[0.092])	Prec@1 100.000 (99.961)
Epoch: [289][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0951 (0.0941) ([0.003]+[0.092])	Prec@1 100.000 (99.965)
Epoch: [289][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0932 (0.0941) ([0.001]+[0.092])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3726 (0.3726) ([0.280]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.390
current lr 1.00000e-04
Grad=  tensor(0.0876, device='cuda:0')
Epoch: [290][0/391]	Time 0.207 (0.207)	Data 0.174 (0.174)	Loss 0.0931 (0.0931) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0952 (0.0940) ([0.003]+[0.092])	Prec@1 100.000 (99.969)
Epoch: [290][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0930 (0.0940) ([0.001]+[0.092])	Prec@1 100.000 (99.973)
Epoch: [290][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0939 (0.0941) ([0.002]+[0.092])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.3824 (0.3824) ([0.290]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.490
current lr 1.00000e-04
Grad=  tensor(0.0884, device='cuda:0')
Epoch: [291][0/391]	Time 0.216 (0.216)	Data 0.184 (0.184)	Loss 0.0931 (0.0931) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.025 (0.028)	Data 0.000 (0.002)	Loss 0.0932 (0.0940) ([0.001]+[0.092])	Prec@1 100.000 (99.969)
Epoch: [291][200/391]	Time 0.026 (0.027)	Data 0.000 (0.001)	Loss 0.0935 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.973)
Epoch: [291][300/391]	Time 0.026 (0.026)	Data 0.000 (0.001)	Loss 0.0929 (0.0940) ([0.001]+[0.092])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.212 (0.212)	Loss 0.3884 (0.3884) ([0.296]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.470
current lr 1.00000e-04
Grad=  tensor(0.1461, device='cuda:0')
Epoch: [292][0/391]	Time 0.208 (0.208)	Data 0.176 (0.176)	Loss 0.0937 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [292][100/391]	Time 0.025 (0.027)	Data 0.000 (0.002)	Loss 0.0935 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (99.992)
Epoch: [292][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0937 (0.0941) ([0.001]+[0.092])	Prec@1 100.000 (99.969)
Epoch: [292][300/391]	Time 0.023 (0.025)	Data 0.000 (0.001)	Loss 0.0972 (0.0940) ([0.005]+[0.092])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.202 (0.202)	Loss 0.3841 (0.3841) ([0.292]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.530
current lr 1.00000e-04
Grad=  tensor(0.0875, device='cuda:0')
Epoch: [293][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.0930 (0.0930) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0929 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.985)
Epoch: [293][200/391]	Time 0.025 (0.026)	Data 0.000 (0.001)	Loss 0.0980 (0.0937) ([0.006]+[0.092])	Prec@1 100.000 (99.988)
Epoch: [293][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0929 (0.0936) ([0.001]+[0.092])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.3934 (0.3934) ([0.301]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.470
current lr 1.00000e-04
Grad=  tensor(0.1062, device='cuda:0')
Epoch: [294][0/391]	Time 0.210 (0.210)	Data 0.180 (0.180)	Loss 0.0932 (0.0932) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0930 (0.0948) ([0.001]+[0.092])	Prec@1 100.000 (99.954)
Epoch: [294][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0929 (0.0944) ([0.001]+[0.092])	Prec@1 100.000 (99.969)
Epoch: [294][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0930 (0.0941) ([0.001]+[0.092])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.186 (0.186)	Loss 0.3781 (0.3781) ([0.286]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.460
current lr 1.00000e-04
Grad=  tensor(0.0882, device='cuda:0')
Epoch: [295][0/391]	Time 0.216 (0.216)	Data 0.180 (0.180)	Loss 0.0931 (0.0931) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0993 (0.0936) ([0.007]+[0.092])	Prec@1 99.219 (99.969)
Epoch: [295][200/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0928 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (99.977)
Epoch: [295][300/391]	Time 0.025 (0.025)	Data 0.000 (0.001)	Loss 0.0930 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.194 (0.194)	Loss 0.3983 (0.3983) ([0.306]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.470
current lr 1.00000e-04
Grad=  tensor(0.0873, device='cuda:0')
Epoch: [296][0/391]	Time 0.213 (0.213)	Data 0.181 (0.181)	Loss 0.0930 (0.0930) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.023 (0.026)	Data 0.000 (0.002)	Loss 0.0941 (0.0938) ([0.002]+[0.092])	Prec@1 100.000 (99.977)
Epoch: [296][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0928 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.981)
Epoch: [296][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0985 (0.0941) ([0.006]+[0.092])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.3903 (0.3903) ([0.298]+[0.092])	Prec@1 94.531 (94.531)
 * Prec@1 93.510
current lr 1.00000e-04
Grad=  tensor(0.1115, device='cuda:0')
Epoch: [297][0/391]	Time 0.210 (0.210)	Data 0.179 (0.179)	Loss 0.0933 (0.0933) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.0930 (0.0936) ([0.001]+[0.092])	Prec@1 100.000 (99.985)
Epoch: [297][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0928 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (99.973)
Epoch: [297][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0933 (0.0938) ([0.001]+[0.092])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3958 (0.3958) ([0.304]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.380
current lr 1.00000e-04
Grad=  tensor(0.1283, device='cuda:0')
Epoch: [298][0/391]	Time 0.225 (0.225)	Data 0.194 (0.194)	Loss 0.0934 (0.0934) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.024 (0.026)	Data 0.000 (0.002)	Loss 0.1032 (0.0941) ([0.011]+[0.092])	Prec@1 99.219 (99.946)
Epoch: [298][200/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0948 (0.0939) ([0.003]+[0.092])	Prec@1 100.000 (99.965)
Epoch: [298][300/391]	Time 0.024 (0.025)	Data 0.000 (0.001)	Loss 0.0928 (0.0937) ([0.001]+[0.092])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.218 (0.218)	Loss 0.3792 (0.3792) ([0.287]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.410
current lr 1.00000e-04
Grad=  tensor(0.0874, device='cuda:0')
Epoch: [299][0/391]	Time 0.207 (0.207)	Data 0.177 (0.177)	Loss 0.0928 (0.0928) ([0.001]+[0.092])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.023 (0.025)	Data 0.000 (0.002)	Loss 0.0935 (0.0941) ([0.001]+[0.092])	Prec@1 100.000 (99.961)
Epoch: [299][200/391]	Time 0.023 (0.024)	Data 0.000 (0.001)	Loss 0.0935 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.965)
Epoch: [299][300/391]	Time 0.024 (0.024)	Data 0.000 (0.001)	Loss 0.0928 (0.0939) ([0.001]+[0.092])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.185 (0.185)	Loss 0.3987 (0.3987) ([0.307]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.520

 Elapsed time for training  0:54:05.737221

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666865348816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5185185074806213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8518518805503845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.1506076455116272, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.9986979365348816, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.1566840261220932, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.1115451380610466, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.1111111119389534, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0785590261220932, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.4361979067325592, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.1080729141831398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0933159738779068, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.5755208134651184, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.234375, 0.43359375, 0.43359375, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9986979365348816, 0.358940988779068, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.4709201455116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.2369791716337204, 0.0, 0.398003488779068, 0.3958333432674408, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.43359375, 0.999131977558136, 0.9986979365348816, 0.4405381977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.4318576455116272, 0.0, 0.9995659589767456, 0.0, 0.0, 0.1067708358168602, 0.999131977558136, 0.38671875, 0.999131977558136, 0.43359375, 0.43359375, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.2764756977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.378472238779068, 0.999131977558136, 0.2994791567325592, 0.4427083432674408, 0.4678819477558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.3650173544883728, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.43359375, 0.9986979365348816, 0.0, 0.0, 0.0772569477558136, 0.999131977558136, 0.0, 0.2864583432674408, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.3285590410232544, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0768229141831398, 0.0, 0.0, 0.9270833134651184, 0.43359375, 0.3975694477558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.4314236044883728, 0.8606770634651184, 0.0941840261220932, 0.0, 0.999131977558136, 0.999131977558136, 0.43359375, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0755208358168602, 0.0, 0.0, 0.40625, 0.0, 0.9995659589767456, 0.43359375, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.43359375, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0451388880610466, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0902777761220932, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.2061631977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.0989583358168602, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.126736119389534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.23828125, 0.999131977558136, 0.3932291567325592, 0.0, 0.9986979365348816, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.43359375, 0.15625, 0.0, 0.0, 0.999131977558136]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.1284722238779068, 0.0, 0.1966145783662796, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0998263880610466, 0.0572916679084301, 0.1163194477558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.0, 0.4040798544883728, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.2113715261220932, 0.9995659589767456, 0.0, 0.1137152761220932, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.1740451455116272, 0.1271701455116272, 0.9995659589767456, 0.0876736119389534, 0.999131977558136, 0.9995659589767456, 0.257378488779068, 0.999131977558136, 0.3997395932674408, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.0933159738779068, 0.9995659589767456, 0.2018229216337204, 0.9995659589767456, 0.3463541567325592, 0.0, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.17578125, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.530381977558136, 0.9995659589767456, 0.999131977558136, 0.185329869389534, 0.999131977558136, 0.5625, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.1006944477558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.1714409738779068, 0.0, 0.999131977558136, 0.1085069477558136, 0.296875, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4040798544883728, 0.999131977558136, 0.8637152910232544, 0.0642361119389534, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0555555559694767, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0989583358168602, 0.1202256977558136, 0.1419270783662796, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.421875, 0.2044270783662796, 0.999131977558136, 0.3372395932674408, 0.4092881977558136, 0.1050347238779068, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1979166716337204, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0638020858168602, 0.999131977558136, 0.1076388880610466, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.067274309694767, 0.3385416567325592, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.1232638880610466, 0.999131977558136, 0.999131977558136, 0.0651041641831398, 0.999131977558136, 0.999131977558136, 0.1002604141831398, 0.999131977558136, 0.4114583432674408, 0.9995659589767456, 0.999131977558136, 0.0, 0.8315972089767456, 0.9986979365348816, 0.999131977558136, 0.0, 0.9995659589767456, 0.8294270634651184, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.071180559694767, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0603298619389534, 0.0794270858168602, 0.999131977558136, 0.1710069477558136, 0.2916666567325592, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.8602430820465088, 0.9995659589767456, 0.0, 0.3224826455116272, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0911458358168602, 0.0, 0.1145833358168602, 0.5737847089767456, 0.9995659589767456, 0.2105034738779068, 0.999131977558136, 0.0, 0.4379340410232544, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0724826380610466, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0460069440305233, 0.999131977558136, 0.1857638955116272, 0.0, 0.0590277798473835, 0.3216145932674408, 0.453125, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.1258680522441864, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.398003488779068, 0.169704869389534, 0.0494791679084301, 0.1480034738779068, 0.999131977558136, 0.4006076455116272, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0889756977558136, 0.1948784738779068, 0.8611111044883728, 0.6801215410232544, 0.999131977558136, 0.999131977558136, 0.0603298619389534, 0.874131977558136, 0.2947048544883728, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.4123263955116272, 0.999131977558136, 0.999131977558136, 0.1948784738779068, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.1137152761220932, 0.8528645634651184, 0.999131977558136, 0.999131977558136, 0.0768229141831398, 0.2747395932674408, 0.6870659589767456, 0.0, 0.2894965410232544, 0.9995659589767456, 0.0651041641831398, 0.999131977558136, 0.999131977558136, 0.0568576380610466, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0629340261220932, 0.9995659589767456, 0.200954869389534, 0.9995659589767456, 0.0642361119389534, 0.9995659589767456, 0.1571180522441864, 0.3914930522441864, 0.999131977558136, 0.1080729141831398, 0.0850694477558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.1840277761220932, 0.999131977558136, 0.0, 0.0959201380610466, 0.9995659589767456, 0.0894097238779068, 0.999131977558136, 0.2018229216337204, 0.9995659589767456, 0.999131977558136, 0.5625, 0.0720486119389534, 0.0642361119389534, 0.999131977558136, 0.3194444477558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3168402910232544, 0.999131977558136, 0.8463541865348816, 0.792100727558136, 0.999131977558136, 0.0438368059694767, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.1050347238779068, 0.9995659589767456, 0.126736119389534, 0.1918402761220932, 0.999131977558136, 0.9986979365348816, 0.1840277761220932, 0.0694444477558136, 0.9986979365348816, 0.999131977558136, 0.1766493022441864, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.8181423544883728, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0594618059694767, 0.999131977558136, 0.999131977558136, 0.102430559694767, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.4574652910232544, 0.9995659589767456, 0.999131977558136, 0.8402777910232544, 0.823350727558136, 0.1358506977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.063368059694767, 0.999131977558136, 0.999131977558136, 0.240017369389534, 0.171875, 0.999131977558136, 0.1349826455116272, 0.0512152798473835, 0.0464409738779068, 0.0616319440305233, 0.0924479141831398, 0.6427951455116272, 0.999131977558136, 0.1046006977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.09765625, 0.9995659589767456, 0.2183159738779068, 0.0859375, 0.42578125, 0.0, 0.0768229141831398, 0.3971354067325592, 0.4110243022441864, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.1653645783662796, 0.999131977558136, 0.9995659589767456, 0.1818576455116272, 0.9995659589767456, 0.1119791641831398, 0.0924479141831398, 0.0, 0.9995659589767456, 0.0, 0.1762152761220932, 0.1154513880610466, 0.999131977558136, 0.8502604365348816, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.1953125, 0.0490451380610466, 0.1189236119389534, 0.8472222089767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.10546875, 0.999131977558136, 0.1145833358168602, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0707465261220932, 0.067274309694767, 0.1011284738779068, 0.1115451380610466, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.1731770783662796, 0.3259548544883728, 0.9995659589767456, 0.19140625, 0.1974826455116272, 0.1254340261220932, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9986979365348816, 0.1158854141831398, 0.999131977558136, 0.0833333358168602, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.8541666865348816, 0.999131977558136, 0.0572916679084301, 0.999131977558136, 0.1080729141831398, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.55859375, 0.999131977558136, 0.2625868022441864, 0.999131977558136, 0.0559895820915699, 0.999131977558136, 0.0681423619389534, 0.0, 0.0638020858168602, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.2660590410232544, 0.0998263880610466, 0.999131977558136, 0.999131977558136, 0.5625, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0842013880610466, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0724826380610466, 0.2938368022441864, 0.0, 0.760850727558136, 0.5625, 0.130642369389534, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.4045138955116272, 0.121961809694767, 0.999131977558136, 0.0, 0.0, 0.2599826455116272, 0.999131977558136, 0.999131977558136]

 sparsity of   [0.01605902798473835, 0.0933159738779068, 0.248046875, 0.1705729216337204, 0.02365451492369175, 0.0301649309694767, 0.6486545205116272, 0.1553819477558136, 0.064453125, 0.1577690988779068, 0.9817708134651184, 0.9997829794883728, 0.136501744389534, 0.0989583358168602, 0.0531684048473835, 0.0815972238779068, 0.0316840298473835, 0.0323350690305233, 0.0416666679084301, 0.2447916716337204, 0.0826822891831398, 0.3012152910232544, 0.0360243059694767, 0.9312065839767456, 0.0681423619389534, 0.6469184160232544, 0.0577256940305233, 0.077039934694767, 0.0753038227558136, 0.0232204869389534, 0.7445746660232544, 0.0687934011220932, 0.2879774272441864, 0.02994791604578495, 0.5002170205116272, 0.02994791604578495, 0.0501302070915699, 0.0520833320915699, 0.0353732630610466, 0.0381944440305233, 0.7078993320465088, 0.0345052070915699, 0.1019965261220932, 0.0403645820915699, 0.95703125, 0.0802951380610466, 0.064453125, 0.0353732630610466, 0.9997829794883728, 0.2033420205116272, 0.0494791679084301, 0.1820746511220932, 0.0381944440305233, 0.0555555559694767, 0.0572916679084301, 0.0559895820915699, 0.0514322929084301, 0.0509982630610466, 0.0876736119389534, 0.0555555559694767, 0.5978732705116272, 0.5442708134651184, 0.0603298619389534, 0.0460069440305233, 0.0394965298473835, 0.0319010429084301, 0.0744357630610466, 0.1076388880610466, 0.0720486119389534, 0.0, 0.0993923619389534, 0.0616319440305233, 0.7651909589767456, 0.0366753488779068, 0.112196184694767, 0.0, 0.0203993059694767, 0.1723090261220932, 0.0, 0.02191840298473835, 0.0772569477558136, 0.052734375, 0.1030815988779068, 0.083984375, 0.037109375, 0.0427517369389534, 0.0993923619389534, 0.0542534738779068, 0.0627170130610466, 0.1032986119389534, 0.8450520634651184, 0.9144965410232544, 0.0974392369389534, 0.138671875, 0.8472222089767456, 0.0481770820915699, 0.8025173544883728, 0.0321180559694767, 0.3209635317325592, 0.1781684011220932, 0.0668402761220932, 0.0364583320915699, 0.8723958134651184, 0.23046875, 0.9296875, 0.02105034701526165, 0.0345052070915699, 0.9997829794883728, 0.6200087070465088, 0.2428385466337204, 0.7126736044883728, 0.0412326380610466, 0.2513020932674408, 0.1174045130610466, 0.0483940988779068, 0.0377604179084301, 0.9997829794883728, 0.012369791977107525, 0.8628472089767456, 0.09765625, 0.02191840298473835, 0.0340711809694767, 0.0694444477558136, 0.853515625, 0.5444878339767456, 0.0368923619389534, 0.768663227558136, 0.1098090261220932, 0.0518663190305233, 0.0325520820915699, 0.875, 0.0616319440305233, 0.02864583395421505, 0.01584201492369175, 0.0384114570915699, 0.0464409738779068, 0.046875, 0.0381944440305233, 0.9919704794883728, 0.0425347238779068, 0.0282118059694767, 0.0924479141831398, 0.0416666679084301, 0.017578125, 0.0475260429084301, 0.0709635391831398, 0.0939670130610466, 0.296440988779068, 0.0557725690305233, 0.1825086772441864, 0.930772602558136, 0.0282118059694767, 0.02886284701526165, 0.02669270895421505, 0.0490451380610466, 0.01909722201526165, 0.0614149309694767, 0.134765625, 0.03515625, 0.0891927108168602, 0.11328125, 0.7124565839767456, 0.6408420205116272, 0.0442708320915699, 0.0421006940305233, 0.8515625, 0.02083333395421505, 0.0622829869389534, 0.1859809011220932, 0.8493923544883728, 0.0303819440305233, 0.0412326380610466, 0.1783854216337204, 0.8667534589767456, 0.2180989533662796, 0.6805555820465088, 0.0457899309694767, 0.6981337070465088, 0.0447048619389534, 0.0668402761220932, 0.0501302070915699, 0.6197916865348816, 0.0460069440305233, 0.3891059160232544, 0.0991753488779068, 0.0364583320915699, 0.02495659701526165, 0.8615451455116272, 0.8324652910232544, 0.2588975727558136, 0.0737847238779068, 0.0457899309694767, 0.6108940839767456, 0.0553385429084301, 0.072265625, 0.908203125, 0.7311198115348816, 0.0078125, 0.388671875, 0.9299045205116272, 0.0416666679084301, 0.6616753339767456, 0.0416666679084301, 0.0542534738779068, 0.098524309694767, 0.0340711809694767, 0.0776909738779068, 0.00390625, 0.5327690839767456, 0.92578125, 0.04296875, 0.0642361119389534, 0.5687934160232544, 0.8791232705116272, 0.02669270895421505, 0.883897602558136, 0.0503472238779068, 0.0473090298473835, 0.048828125, 0.0989583358168602, 0.0818142369389534, 0.0477430559694767, 0.0427517369389534, 0.119140625, 0.0423177070915699, 0.0609809048473835, 0.0989583358168602, 0.2630208432674408, 0.0501302070915699, 0.02387152798473835, 0.4867621660232544, 0.2191840261220932, 0.02387152798473835, 0.0319010429084301, 0.1150173619389534, 0.0609809048473835, 0.6510416865348816, 0.0857204869389534, 0.1655815988779068, 0.013020833022892475, 0.0796440988779068, 0.0412326380610466, 0.0401475690305233, 0.108289934694767, 0.5902777910232544, 0.4967447817325592, 0.0, 0.0763888880610466, 0.0896267369389534, 0.0911458358168602, 0.0646701380610466, 0.655381977558136, 0.2046440988779068, 0.02387152798473835, 0.010850694961845875, 0.6480034589767456, 0.0145399309694767, 0.5818142294883728, 0.0345052070915699, 0.02408854104578495, 0.00021701389050576836, 0.9025607705116272, 0.03081597201526165, 0.068359375, 0.060546875, 0.0603298619389534, 0.078125, 0.0, 0.0861545130610466, 0.9997829794883728, 0.0679253488779068, 0.091796875, 0.7057291865348816, 0.0763888880610466, 0.1440972238779068, 0.01953125, 0.0842013880610466, 0.037109375, 0.9993489384651184, 0.0338541679084301, 0.2035590261220932, 0.9997829794883728, 0.02105034701526165, 0.1052517369389534, 0.1134982630610466, 0.7615017294883728, 0.068359375, 0.0, 0.03515625, 0.0496961809694767, 0.0737847238779068, 0.0607638880610466, 0.0327690988779068, 0.03125, 0.1187065988779068, 0.2176649272441864, 0.758897602558136, 0.0193142369389534, 0.0360243059694767, 0.0622829869389534, 0.0763888880610466, 0.6482204794883728, 0.0514322929084301, 0.0492621548473835, 0.5455729365348816, 0.0709635391831398, 0.9743923544883728, 0.1178385391831398, 0.0928819477558136, 0.946397602558136, 0.713975727558136, 0.136501744389534, 0.078125, 0.8483073115348816, 0.0967881977558136, 0.218532994389534, 0.0303819440305233, 0.0616319440305233, 0.0792100727558136, 0.0243055559694767, 0.9242621660232544, 0.0401475690305233, 0.0913628488779068, 0.0611979179084301, 0.0494791679084301, 0.0, 0.0581597238779068, 0.9997829794883728, 0.148220494389534, 0.02951388992369175, 0.0, 0.179470494389534, 0.0455729179084301, 0.0796440988779068, 0.0668402761220932, 0.02973090298473835, 0.0659722238779068, 0.0549045130610466, 0.029296875, 0.014756944961845875, 0.02213541604578495, 0.0559895820915699, 0.0403645820915699, 0.8977864384651184, 0.012803819961845875, 0.1195746511220932, 0.0826822891831398, 0.0457899309694767, 0.7400173544883728, 0.0631510391831398, 0.048828125, 0.0, 0.02408854104578495, 0.0503472238779068, 0.071180559694767, 0.0453559048473835, 0.0716145858168602, 0.1115451380610466, 0.9778645634651184, 0.1080729141831398, 0.1799045205116272, 0.8697916865348816, 0.1536458283662796, 0.532335102558136, 0.7376301884651184, 0.0774739608168602, 0.0859375, 0.099609375, 0.232421875, 0.9283854365348816, 0.096571184694767, 0.1247829869389534, 0.4138454794883728, 0.0805121511220932, 0.0516493059694767, 0.0451388880610466, 0.0746527761220932, 0.0776909738779068, 0.0340711809694767, 0.5887587070465088, 0.0509982630610466, 0.0941840261220932, 0.9168837070465088, 0.0572916679084301, 0.0718315988779068, 0.3274739682674408, 0.104383684694767, 0.8773871660232544, 0.6078559160232544, 0.0824652761220932, 0.0232204869389534, 0.0186631940305233, 0.02495659701526165, 0.08203125, 0.02669270895421505, 0.0427517369389534, 0.0796440988779068, 0.5705295205116272, 0.835069477558136, 0.2521701455116272, 0.3109809160232544, 0.0264756940305233, 0.046875, 0.9997829794883728, 0.009548611007630825, 0.0301649309694767, 0.0347222238779068, 0.0503472238779068, 0.5503472089767456, 0.0616319440305233, 0.7914496660232544, 0.0425347238779068, 0.1529947966337204, 0.7849392294883728, 0.0375434048473835, 0.1588541716337204, 0.0536024309694767, 0.0802951380610466, 0.173611119389534, 0.0, 0.288628488779068, 0.8621962070465088, 0.0, 0.094618059694767, 0.01323784701526165, 0.8200954794883728, 0.0520833320915699, 0.1143663227558136, 0.0457899309694767, 0.1208767369389534, 0.0592447929084301, 0.5685763955116272, 0.0466579869389534, 0.0425347238779068, 0.0392795130610466, 0.0720486119389534, 0.1812065988779068, 0.014756944961845875, 0.0776909738779068, 0.5314670205116272, 0.0753038227558136, 0.3051215410232544, 0.6317274570465088, 0.0679253488779068, 0.0386284738779068, 0.6842448115348816, 0.0338541679084301, 0.0223524309694767, 0.1119791641831398, 0.0564236119389534, 0.1193576380610466, 0.8715277910232544, 0.4379340410232544, 0.0802951380610466, 0.7398003339767456, 0.084852434694767, 0.1011284738779068, 0.1078559011220932, 0.0264756940305233, 0.0674913227558136, 0.0464409738779068, 0.6981337070465088, 0.082899309694767, 0.7020399570465088, 0.5240885615348816, 0.0679253488779068, 0.1351996511220932, 0.2549913227558136, 0.0503472238779068, 0.1226128488779068, 0.2180989533662796, 0.125, 0.5609809160232544, 0.9819878339767456, 0.9364149570465088, 0.02951388992369175, 0.0, 0.0486111119389534, 0.0321180559694767, 0.7547743320465088, 0.0, 0.0466579869389534, 0.9845920205116272, 0.7165798544883728, 0.0540364570915699, 0.048828125, 0.0779079869389534, 0.0842013880610466, 0.041015625, 0.0516493059694767, 0.0399305559694767, 0.0666232630610466, 0.01605902798473835, 0.1310763955116272, 0.6703559160232544, 0.1232638880610466, 0.0184461809694767, 0.0381944440305233, 0.02886284701526165, 0.0499131940305233, 0.01605902798473835, 0.1571180522441864, 0.6575520634651184, 0.0648871511220932, 0.076171875, 0.0477430559694767, 0.1890190988779068, 0.5991753339767456, 0.5006510615348816, 0.7601996660232544, 0.0939670130610466, 0.0904947891831398]

 sparsity of   [0.0234375, 0.1662326455116272, 0.0826822891831398, 0.0846354141831398, 0.0857204869389534, 0.3190104067325592, 0.4848090410232544, 0.1994357705116272, 0.0753038227558136, 0.0863715261220932, 0.5486111044883728, 0.559678852558136, 0.0340711809694767, 0.2428385466337204, 0.9220920205116272, 0.126736119389534, 0.2326388955116272, 0.6078559160232544, 0.0824652761220932, 0.1807725727558136, 0.6384548544883728, 0.0881076380610466, 0.1213107630610466, 0.2287326455116272, 0.0859375, 0.0915798619389534, 0.10546875, 0.3426649272441864, 0.1017795130610466, 0.0, 0.0, 0.1822916716337204, 0.1213107630610466, 0.02170138992369175, 0.1840277761220932, 0.1733940988779068, 0.0403645820915699, 0.0, 0.177517369389534, 0.4730902910232544, 0.0813802108168602, 0.071180559694767, 0.0872395858168602, 0.3033854067325592, 0.1727430522441864, 0.01171875, 0.1979166716337204, 0.2623697817325592, 0.0462239570915699, 0.0577256940305233, 0.2747395932674408, 0.0776909738779068, 0.103515625, 0.3049045205116272, 0.073133684694767, 0.1538628488779068, 0.1553819477558136, 0.02690972201526165, 0.333984375, 0.0655381977558136, 0.0787760391831398, 0.1067708358168602, 0.4713541567325592, 0.099609375, 0.1846788227558136, 0.1842447966337204, 0.3891059160232544, 0.0724826380610466, 0.0805121511220932, 0.1299913227558136, 0.0303819440305233, 0.1130642369389534, 0.134548619389534, 0.0457899309694767, 0.3185763955116272, 0.087890625, 0.1009114608168602, 0.0729166641831398, 0.1694878488779068, 0.0904947891831398, 0.01714409701526165, 0.0894097238779068, 0.1026475727558136, 0.366753488779068, 0.1048177108168602, 0.0646701380610466, 0.3036024272441864, 0.1184895858168602, 0.1592881977558136, 0.2434895783662796, 0.1375868022441864, 0.0271267369389534, 0.16015625, 0.154296875, 0.1189236119389534, 0.0794270858168602, 0.3519965410232544, 0.6126301884651184, 0.3591579794883728, 0.120008684694767, 0.6091579794883728, 0.007378472480922937, 0.161892369389534, 0.0822482630610466, 0.0, 0.0733506977558136, 0.0900607630610466, 0.083984375, 0.1401909738779068, 0.3930121660232544, 0.1013454869389534, 0.0815972238779068, 0.9997829794883728, 0.4954427182674408, 0.4561631977558136, 0.179470494389534, 0.2756076455116272, 0.3465711772441864, 0.0861545130610466, 0.0933159738779068, 0.0935329869389534, 0.2018229216337204, 0.0950520858168602, 0.3732638955116272, 0.078993059694767, 0.552734375, 0.0, 0.0520833320915699, 0.4720052182674408, 0.0822482630610466, 0.3207465410232544, 0.2465277761220932, 0.2022569477558136, 0.075086809694767, 0.090711809694767, 0.100477434694767, 0.0794270858168602, 0.3428819477558136, 0.2745225727558136, 0.1514756977558136, 0.2105034738779068, 0.0581597238779068, 0.0785590261220932, 0.408203125, 0.0763888880610466, 0.0941840261220932, 0.3517795205116272, 0.3704427182674408, 0.2152777761220932, 0.0726996511220932, 0.893663227558136, 0.1006944477558136, 0.0798611119389534, 0.02408854104578495, 0.286675363779068, 0.0592447929084301, 0.329644113779068, 0.0440538190305233, 0.0805121511220932, 0.1334635466337204, 0.10546875, 0.2170138955116272, 0.01801215298473835, 0.0212673619389534, 0.0972222238779068, 0.0234375, 0.0833333358168602, 0.4396701455116272, 0.1019965261220932, 0.224392369389534, 0.9071180820465088, 0.4503038227558136, 0.0792100727558136, 0.0833333358168602, 0.02387152798473835, 0.3812934160232544, 0.0579427070915699, 0.745225727558136, 0.1169704869389534, 0.0818142369389534, 0.02018229104578495, 0.0909288227558136, 0.1280381977558136, 0.0661892369389534, 0.2300347238779068, 0.080078125, 0.1614583283662796, 0.0625, 0.216796875, 0.2296006977558136, 0.0486111119389534, 0.1273871511220932, 0.1573350727558136, 0.0913628488779068, 0.1497395783662796, 0.0831163227558136, 0.126736119389534, 0.1838107705116272, 0.1130642369389534, 0.159939244389534, 0.077039934694767, 0.118055559694767, 0.0451388880610466, 0.1458333283662796, 0.9993489384651184, 0.3602430522441864, 0.5158420205116272, 0.2269965261220932, 0.5983073115348816, 0.087890625, 0.2313368022441864, 0.1976996511220932, 0.25390625, 0.1879340261220932, 0.0490451380610466, 0.2417534738779068, 0.1128472238779068, 0.591796875, 0.5891926884651184, 0.0705295130610466, 0.0831163227558136, 0.1069878488779068, 0.2272135466337204, 0.0757378488779068, 0.0902777761220932, 0.0846354141831398, 0.0509982630610466, 0.1156684011220932, 0.0815972238779068, 0.197265625, 0.0900607630610466, 0.3090277910232544, 0.0835503488779068, 0.7717013955116272, 0.0358072929084301, 0.083984375, 0.1453993022441864, 0.136501744389534, 0.1184895858168602, 0.2163628488779068, 0.1206597238779068, 0.0, 0.2016059011220932, 0.2328559011220932, 0.1584201455116272, 0.0911458358168602, 0.2947048544883728, 0.0431857630610466, 0.083984375, 0.0234375, 0.0987413227558136, 0.1360677033662796, 0.0203993059694767, 0.0920138880610466, 0.1106770858168602, 0.0473090298473835, 0.0004340277810115367, 0.1000434011220932, 0.0746527761220932, 0.0852864608168602, 0.0941840261220932, 0.0987413227558136, 0.0379774309694767, 0.107421875, 0.0724826380610466, 0.29296875, 0.0631510391831398, 0.1395399272441864, 0.1098090261220932, 0.0, 0.0913628488779068, 0.077039934694767, 0.214626744389534, 0.2254774272441864, 0.1725260466337204, 0.0681423619389534, 0.1050347238779068, 0.1723090261220932, 0.3203125, 0.1019965261220932, 0.1770833283662796, 0.1354166716337204, 0.071180559694767, 0.01584201492369175, 0.3363715410232544, 0.0674913227558136, 0.0716145858168602, 0.1189236119389534, 0.1065538227558136, 0.0900607630610466, 0.0928819477558136, 0.1558159738779068, 0.0753038227558136, 0.0987413227558136, 0.078993059694767, 0.1291232705116272, 0.1050347238779068, 0.4125434160232544, 0.0948350727558136, 0.100477434694767, 0.2235243022441864, 0.259331613779068, 0.078125, 0.0944010391831398, 0.1338975727558136, 0.2823350727558136, 0.086805559694767, 0.0601128488779068, 0.1315104216337204, 0.1202256977558136, 0.1373697966337204, 0.922960102558136, 0.548828125, 0.5086805820465088, 0.2415364533662796, 0.0950520858168602, 0.1382378488779068, 0.0941840261220932, 0.05859375, 0.0776909738779068, 0.5453559160232544, 0.0857204869389534, 0.0486111119389534, 0.0588107630610466, 0.0768229141831398, 0.3157552182674408, 0.712022602558136, 0.2567274272441864, 0.9997829794883728, 0.1204427108168602, 0.6284722089767456, 0.2196180522441864, 0.0627170130610466, 0.3654513955116272, 0.1048177108168602, 0.2680121660232544, 0.010850694961845875, 0.0698784738779068, 0.0802951380610466, 0.4670138955116272, 0.261284738779068, 0.1134982630610466, 0.1046006977558136, 0.106336809694767, 0.3489583432674408, 0.0746527761220932, 0.1002604141831398, 0.2630208432674408, 0.0837673619389534, 0.080078125, 0.3776041567325592, 0.0941840261220932, 0.0915798619389534, 0.0794270858168602, 0.1408420205116272, 0.0729166641831398, 0.5308159589767456, 0.0991753488779068, 0.3142361044883728, 0.0944010391831398, 0.090711809694767, 0.0, 0.7749565839767456, 0.0753038227558136, 0.228515625, 0.0709635391831398, 0.0744357630610466, 0.4255642294883728, 0.1154513880610466, 0.2120225727558136, 0.083984375, 0.1608072966337204, 0.4457465410232544, 0.1868489533662796, 0.086805559694767, 0.1506076455116272, 0.1247829869389534, 0.6252170205116272, 0.1421440988779068, 0.0811631977558136, 0.0716145858168602, 0.2940538227558136, 0.0609809048473835, 0.2660590410232544, 0.08203125, 0.1143663227558136, 0.0978732630610466, 0.0705295130610466, 0.1293402761220932, 0.0846354141831398, 0.1937934011220932, 0.4173177182674408, 0.0720486119389534, 0.0883246511220932, 0.102430559694767, 0.1184895858168602, 0.1078559011220932, 0.12890625, 0.05859375, 0.0, 0.4149305522441864, 0.0772569477558136, 0.118055559694767, 0.3359375, 0.1375868022441864, 0.0655381977558136, 0.1089409738779068, 0.0818142369389534, 0.13671875, 0.2042100727558136, 0.094618059694767, 0.1453993022441864, 0.0874565988779068, 0.1050347238779068, 0.02734375, 0.10546875, 0.9997829794883728, 0.1720920205116272, 0.0703125, 0.1788194477558136, 0.1508246511220932, 0.1037326380610466, 0.0846354141831398, 0.1809895783662796, 0.1354166716337204, 0.0579427070915699, 0.2065972238779068, 0.0833333358168602, 0.02560763992369175, 0.08203125, 0.120008684694767, 0.0698784738779068, 0.046875, 0.48828125, 0.0980902761220932, 0.2829861044883728, 0.3387586772441864, 0.0703125, 0.072265625, 0.0846354141831398, 0.4774305522441864, 0.4435763955116272, 0.1764322966337204, 0.9997829794883728, 0.0859375, 0.07421875, 0.3018663227558136, 0.1506076455116272, 0.0473090298473835, 0.372612863779068, 0.140407994389534, 0.0, 0.1688368022441864, 0.6736111044883728, 0.185546875, 0.125, 0.6380208134651184, 0.9997829794883728, 0.0928819477558136, 0.0, 0.279296875, 0.3509114682674408, 0.3713107705116272, 0.0520833320915699, 0.1373697966337204, 0.7660590410232544, 0.0891927108168602, 0.0735677108168602, 0.1612413227558136, 0.0234375, 0.1006944477558136, 0.0891927108168602, 0.318359375, 0.1623263955116272, 0.041015625, 0.2712673544883728, 0.092664934694767, 0.3736979067325592, 0.1762152761220932, 0.1881510466337204, 0.2152777761220932, 0.0970052108168602, 0.1028645858168602, 0.191189244389534, 0.4151475727558136, 0.1592881977558136, 0.1085069477558136, 0.048828125, 0.0902777761220932, 0.3142361044883728, 0.1590711772441864, 0.096571184694767, 0.130859375, 0.4626736044883728, 0.2513020932674408, 0.4542100727558136, 0.0251736119389534, 0.1163194477558136, 0.576171875, 0.3578559160232544, 0.146484375, 0.1204427108168602, 0.1228298619389534, 0.3576388955116272, 0.0733506977558136, 0.0833333358168602, 0.1341145783662796, 0.2113715261220932, 0.4919704794883728, 0.0813802108168602, 0.0, 0.3211805522441864]

 sparsity of   [0.0844184011220932, 0.1317274272441864, 0.0032552082557231188, 0.060546875, 0.0651041641831398, 0.0, 0.0622829869389534, 0.0, 0.0362413190305233, 0.0473090298473835, 0.0555555559694767, 0.0496961809694767, 0.0470920130610466, 0.0325520820915699, 0.033203125, 0.0952690988779068, 0.017578125, 0.02387152798473835, 0.0499131940305233, 0.0481770820915699, 0.0713975727558136, 0.2686631977558136, 0.0983072891831398, 0.0857204869389534, 0.1258680522441864, 0.04296875, 0.02690972201526165, 0.0397135429084301, 0.0575086809694767, 0.0915798619389534, 0.0423177070915699, 0.1128472238779068, 0.290581613779068, 0.0972222238779068, 0.0598958320915699, 0.0464409738779068, 0.037109375, 0.0546875, 0.116102434694767, 0.1271701455116272, 0.0928819477558136, 0.0822482630610466, 0.008029513992369175, 0.0536024309694767, 0.0913628488779068, 0.0876736119389534, 0.2044270783662796, 0.1725260466337204, 0.025390625, 0.3615451455116272, 0.337456613779068, 0.2180989533662796, 0.0774739608168602, 0.0763888880610466, 0.1265190988779068, 0.0570746548473835, 0.9997829794883728, 0.0598958320915699, 0.02105034701526165, 0.0549045130610466, 0.1759982705116272, 0.6206597089767456, 0.1204427108168602, 0.067274309694767, 0.0, 0.2074652761220932, 0.0700954869389534, 0.1321614533662796, 0.0646701380610466, 0.803819477558136, 0.130642369389534, 0.0776909738779068, 0.0631510391831398, 0.2697482705116272, 0.02300347201526165, 0.0397135429084301, 0.1629774272441864, 0.0850694477558136, 0.1037326380610466, 0.0824652761220932, 0.1080729141831398, 0.01888020895421505, 0.0763888880610466, 0.1341145783662796, 0.1197916641831398, 0.0440538190305233, 0.9995659589767456, 0.1493055522441864, 0.052734375, 0.0668402761220932, 0.2552083432674408, 0.0412326380610466, 0.0503472238779068, 0.0603298619389534, 0.0575086809694767, 0.0523003488779068, 0.068359375, 0.1126302108168602, 0.339409738779068, 0.1516927033662796, 0.339409738779068, 0.132595494389534, 0.2819010317325592, 0.4724392294883728, 0.126953125, 0.0811631977558136, 0.0646701380610466, 0.1265190988779068, 0.0783420130610466, 0.6015625, 0.1208767369389534, 0.0855034738779068, 0.0842013880610466, 0.0766059011220932, 0.0, 0.1488715261220932, 0.1453993022441864, 0.1937934011220932, 0.0438368059694767, 0.0859375, 0.0397135429084301, 0.069227434694767, 0.0922309011220932, 0.16015625, 0.0629340261220932, 0.0460069440305233, 0.0757378488779068, 0.084852434694767, 0.072265625, 0.0885416641831398, 0.3878038227558136, 0.0846354141831398, 0.01909722201526165, 0.3509114682674408, 0.1176215261220932, 0.0503472238779068, 0.6443142294883728, 0.1460503488779068, 0.1226128488779068, 0.0, 0.010850694961845875, 0.169704869389534, 0.084852434694767, 0.673828125, 0.1996527761220932, 0.0492621548473835, 0.0989583358168602, 0.1890190988779068, 0.10546875, 0.1208767369389534, 0.0479600690305233, 0.07421875, 0.0388454869389534, 0.0596788190305233, 0.0496961809694767, 0.078125, 0.0603298619389534, 0.0, 0.0559895820915699, 0.532335102558136, 0.5529513955116272, 0.0284288190305233, 0.216579869389534, 0.0939670130610466, 0.2545572817325592, 0.2860243022441864, 0.1189236119389534, 0.2417534738779068, 0.3157552182674408, 0.1864149272441864, 0.0855034738779068, 0.0362413190305233, 0.0570746548473835, 0.0904947891831398, 0.0941840261220932, 0.3229166567325592, 0.0234375, 0.0503472238779068, 0.4594184160232544, 0.0978732630610466, 0.0679253488779068, 0.2571614682674408, 0.0425347238779068, 0.0436197929084301, 0.1983506977558136, 0.0753038227558136, 0.0700954869389534, 0.1048177108168602, 0.01410590298473835, 0.1727430522441864, 0.0735677108168602, 0.0631510391831398, 0.046875, 0.9982638955116272, 0.0366753488779068, 0.1510416716337204, 0.104383684694767, 0.0863715261220932, 0.2682291567325592, 0.0616319440305233, 0.0490451380610466, 0.0559895820915699, 0.1647135466337204, 0.0603298619389534, 0.5394965410232544, 0.657335102558136, 0.0690104141831398, 0.9995659589767456, 0.1258680522441864, 0.0748697891831398, 0.0418836809694767, 0.02777777798473835, 0.0473090298473835, 0.0635850727558136, 0.02018229104578495, 0.0668402761220932, 0.0909288227558136, 0.1840277761220932, 0.0737847238779068, 0.0542534738779068, 0.0575086809694767, 0.0494791679084301, 0.0512152798473835, 0.1731770783662796, 0.603515625, 0.0425347238779068, 0.454644113779068, 0.1634114533662796, 0.1317274272441864, 0.7797309160232544, 0.0685763880610466, 0.2586805522441864, 0.0952690988779068, 0.1022135391831398, 0.234157994389534, 0.09765625, 0.0, 0.2779947817325592, 0.01323784701526165, 0.0520833320915699, 0.083984375, 0.077039934694767, 0.015407986007630825, 0.2799479067325592, 0.0850694477558136, 0.05078125, 0.0933159738779068, 0.0622829869389534, 0.25390625, 0.2133246511220932, 0.1922743022441864, 0.0364583320915699, 0.0499131940305233, 0.0473090298473835, 0.1046006977558136, 0.0700954869389534, 0.0336371548473835, 0.3780381977558136, 0.0705295130610466, 0.0462239570915699, 0.0696614608168602, 0.2513020932674408, 0.0193142369389534, 0.084852434694767, 0.0047743055038154125, 0.0234375, 0.0271267369389534, 0.0737847238779068, 0.1859809011220932, 0.0451388880610466, 0.02994791604578495, 0.0008680555620230734, 0.0555555559694767, 0.1145833358168602, 0.08203125, 0.713975727558136, 0.169704869389534, 0.0746527761220932, 0.0327690988779068, 0.065321184694767, 0.0579427070915699, 0.0173611119389534, 0.0392795130610466, 0.0835503488779068, 0.0447048619389534, 0.0629340261220932, 0.0184461809694767, 0.0618489570915699, 0.0542534738779068, 0.0401475690305233, 0.0377604179084301, 0.150173619389534, 0.2934027910232544, 0.02495659701526165, 0.0549045130610466, 0.0564236119389534, 0.0544704869389534, 0.0316840298473835, 0.2404513955116272, 0.0592447929084301, 0.0494791679084301, 0.1030815988779068, 0.0364583320915699, 0.064453125, 0.0740017369389534, 0.0004340277810115367, 0.0696614608168602, 0.2829861044883728, 0.00021701389050576836, 0.1187065988779068, 0.0924479141831398, 0.0212673619389534, 0.0499131940305233, 0.0865885391831398, 0.025390625, 0.095703125, 0.065321184694767, 0.0505642369389534, 0.0844184011220932, 0.0635850727558136, 0.1050347238779068, 0.5859375, 0.0555555559694767, 0.094618059694767, 0.5358073115348816, 0.1284722238779068, 0.033203125, 0.0479600690305233, 0.001953125, 0.0462239570915699, 0.0394965298473835, 0.118055559694767, 0.0753038227558136, 0.069227434694767, 0.7986111044883728, 0.082899309694767, 0.2328559011220932, 0.1608072966337204, 0.0922309011220932, 0.0670572891831398, 0.4375, 0.0503472238779068, 0.02604166604578495, 0.0225694440305233, 0.0733506977558136, 0.013671875, 0.0375434048473835, 0.0655381977558136, 0.0403645820915699, 0.1046006977558136, 0.05859375, 0.0436197929084301, 0.1072048619389534, 0.0473090298473835, 0.1206597238779068, 0.306206613779068, 0.1373697966337204, 0.0392795130610466, 0.0807291641831398, 0.0598958320915699, 0.06640625, 0.1987847238779068, 0.0805121511220932, 0.0, 0.1263020783662796, 0.0362413190305233, 0.008897569961845875, 0.1623263955116272, 0.046875, 0.9995659589767456, 0.0525173619389534, 0.0972222238779068, 0.107421875, 0.0, 0.755859375, 0.0904947891831398, 0.0833333358168602, 0.314453125, 0.0681423619389534, 0.0030381944961845875, 0.142361119389534, 0.0705295130610466, 0.073133684694767, 0.0904947891831398, 0.490234375, 0.4578993022441864, 0.4520399272441864, 0.173828125, 0.1037326380610466, 0.0850694477558136, 0.1085069477558136, 0.041015625, 0.2013888955116272, 0.6471354365348816, 0.5095486044883728, 0.169921875, 0.1408420205116272, 0.010416666977107525, 0.177517369389534, 0.3090277910232544, 0.041015625, 0.0746527761220932, 0.0496961809694767, 0.0930989608168602, 0.5314670205116272, 0.49609375, 0.2367621511220932, 0.0837673619389534, 0.0685763880610466, 0.181423619389534, 0.01692708395421505, 0.0716145858168602, 0.01714409701526165, 0.01909722201526165, 0.0453559048473835, 0.03081597201526165, 0.0831163227558136, 0.1247829869389534, 0.083984375, 0.2560763955116272, 0.088758684694767, 0.0603298619389534, 0.0924479141831398, 0.6121962070465088, 0.4516059160232544, 0.03081597201526165, 0.0282118059694767, 0.0575086809694767, 0.1809895783662796, 0.1532118022441864, 0.0861545130610466, 0.02799479104578495, 0.060546875, 0.0787760391831398, 0.1608072966337204, 0.1631944477558136, 0.0928819477558136, 0.1358506977558136, 0.041015625, 0.0922309011220932, 0.0347222238779068, 0.010850694961845875, 0.082899309694767, 0.1907552033662796, 0.150173619389534, 0.6059027910232544, 0.1195746511220932, 0.0861545130610466, 0.0991753488779068, 0.0455729179084301, 0.0412326380610466, 0.0421006940305233, 0.0607638880610466, 0.014973958022892475, 0.0928819477558136, 0.0436197929084301, 0.0536024309694767, 0.0607638880610466, 0.167751744389534, 0.1781684011220932, 0.758897602558136, 0.1171875, 0.087890625, 0.0384114570915699, 0.092664934694767, 0.2315538227558136, 0.1983506977558136, 0.9997829794883728, 0.0844184011220932, 0.0889756977558136, 0.02170138992369175, 0.0032552082557231188, 0.0457899309694767, 0.05078125, 0.0388454869389534, 0.2406684011220932, 0.0245225690305233, 0.05859375, 0.0596788190305233, 0.2350260466337204, 0.0980902761220932, 0.1818576455116272, 0.0, 0.0, 0.0329861119389534, 0.2582465410232544, 0.0694444477558136, 0.041015625, 0.0913628488779068, 0.0559895820915699, 0.9995659589767456, 0.1091579869389534, 0.0, 0.0, 0.1022135391831398, 0.1864149272441864, 0.0846354141831398, 0.0032552082557231188, 0.0700954869389534, 0.2450086772441864, 0.7658420205116272, 0.0553385429084301, 0.02083333395421505, 0.2170138955116272, 0.8786892294883728, 0.0726996511220932, 0.0338541679084301, 0.0564236119389534, 0.0948350727558136, 0.7115885615348816, 0.108289934694767, 0.0787760391831398]

 sparsity of   [0.025390625, 0.0245225690305233, 0.0, 0.002170138992369175, 0.0523003488779068, 0.0, 0.0, 0.0, 0.0, 0.00434027798473835, 0.0, 0.0, 0.0, 0.0, 0.013671875, 0.0, 0.0, 0.0193142369389534, 0.0006510416860692203, 0.0071614584885537624, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.014973958022892475, 0.0, 0.0, 0.00629340298473835, 0.01605902798473835, 0.005859375, 0.0052083334885537624, 0.0, 0.0, 0.0206163190305233, 0.01323784701526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0349392369389534, 0.0, 0.0, 0.0, 0.0, 0.0251736119389534, 0.01019965298473835, 0.0, 0.01779513992369175, 0.0, 0.0, 0.0023871527519077063, 0.0347222238779068, 0.01714409701526165, 0.0, 0.02408854104578495, 0.0, 0.0, 0.0067274305038154125, 0.0, 0.0, 0.0, 0.009114583022892475, 0.0, 0.0364583320915699, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.009982638992369175, 0.0, 0.02994791604578495, 0.0, 0.0, 0.0008680555620230734, 0.0, 0.0212673619389534, 0.0, 0.01410590298473835, 0.0290798619389534, 0.0, 0.006076388992369175, 0.0, 0.0008680555620230734, 0.011067708022892475, 0.006076388992369175, 0.01909722201526165, 0.0, 0.0, 0.9418402910232544, 0.0, 0.1473524272441864, 0.0, 0.0303819440305233, 0.0, 0.03125, 0.9409722089767456, 0.00434027798473835, 0.0, 0.0047743055038154125, 0.01692708395421505, 0.008897569961845875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008680555620230734, 0.1098090261220932, 0.0015190972480922937, 0.0, 0.0008680555620230734, 0.0013020833721384406, 0.0, 0.0, 0.0, 0.0, 0.0577256940305233, 0.00629340298473835, 0.02495659701526165, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.00434027798473835, 0.0045572915114462376, 0.009114583022892475, 0.0, 0.0, 0.9997829794883728, 0.0, 0.0, 0.0, 0.0, 0.02669270895421505, 0.0, 0.02994791604578495, 0.01584201492369175, 0.0193142369389534, 0.01888020895421505, 0.005642361007630825, 0.0, 0.012369791977107525, 0.0, 0.0, 0.0, 0.037109375, 0.4118923544883728, 0.00021701389050576836, 0.017578125, 0.0015190972480922937, 0.0, 0.0442708320915699, 0.00021701389050576836, 0.0, 0.02105034701526165, 0.0687934011220932, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.014322916977107525, 0.01801215298473835, 0.0225694440305233, 0.0, 0.0203993059694767, 0.0, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.01519097201526165, 0.0, 0.007378472480922937, 0.0687934011220932, 0.00021701389050576836, 0.0, 0.0, 0.005642361007630825, 0.0, 0.0, 0.013671875, 0.9993489384651184, 0.025390625, 0.0, 0.0045572915114462376, 0.02756076492369175, 0.0284288190305233, 0.0, 0.0, 0.009982638992369175, 0.02669270895421505, 0.0008680555620230734, 0.0, 0.0078125, 0.0, 0.0193142369389534, 0.0, 0.0, 0.0366753488779068, 0.011935763992369175, 0.0, 0.00021701389050576836, 0.0023871527519077063, 0.0, 0.015407986007630825, 0.0, 0.0106336809694767, 0.0375434048473835, 0.01822916604578495, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.0145399309694767, 0.0, 0.0, 0.0, 0.01605902798473835, 0.0125868059694767, 0.0, 0.0, 0.0, 0.0, 0.0010850694961845875, 0.0, 0.0, 0.0251736119389534, 0.0, 0.0, 0.0, 0.0193142369389534, 0.0010850694961845875, 0.009114583022892475, 0.0, 0.7853732705116272, 0.0, 0.0, 0.0, 0.0184461809694767, 0.02170138992369175, 0.012803819961845875, 0.00021701389050576836, 0.0, 0.1471354216337204, 0.0010850694961845875, 0.0125868059694767, 0.0, 0.012803819961845875, 0.0314670130610466, 0.0, 0.0, 0.009114583022892475, 0.9997829794883728, 0.0, 0.0006510416860692203, 0.014322916977107525, 0.9995659589767456, 0.0, 0.0026041667442768812, 0.0, 0.009765625, 0.0, 0.0010850694961845875, 0.00629340298473835, 0.01584201492369175, 0.0, 0.0325520820915699, 0.0, 0.0, 0.01627604104578495, 0.00434027798473835, 0.010416666977107525, 0.3357204794883728, 0.017578125, 0.00629340298473835, 0.0, 0.012369791977107525, 0.0, 0.01627604104578495, 0.009765625, 0.0006510416860692203, 0.0, 0.00021701389050576836, 0.0, 0.0668402761220932, 0.0, 0.0388454869389534, 0.0, 0.4405381977558136, 0.0193142369389534, 0.0, 0.1143663227558136, 0.0271267369389534, 0.025390625, 0.0, 0.0, 0.008029513992369175, 0.02473958395421505, 0.007378472480922937, 0.0017361111240461469, 0.0, 0.0, 0.0262586809694767, 0.0, 0.0, 0.0030381944961845875, 0.02864583395421505, 0.0, 0.0, 0.0071614584885537624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1803385466337204, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.0, 0.009548611007630825, 0.0015190972480922937, 0.0, 0.02799479104578495, 0.0, 0.0321180559694767, 0.9995659589767456, 0.0, 0.0023871527519077063, 0.0, 0.0, 0.0067274305038154125, 0.384331613779068, 0.0323350690305233, 0.0, 0.0045572915114462376, 0.0, 0.0, 0.0, 0.006076388992369175, 0.0, 0.0978732630610466, 0.0, 0.0, 0.0, 0.0251736119389534, 0.0008680555620230734, 0.0, 0.0, 0.0, 0.0067274305038154125, 0.0049913194961845875, 0.4055989682674408, 0.0, 0.0, 0.0423177070915699, 0.0067274305038154125, 0.0, 0.002170138992369175, 0.0032552082557231188, 0.009982638992369175, 0.0, 0.0, 0.0036892362404614687, 0.0, 0.0325520820915699, 0.011067708022892475, 0.0, 0.01605902798473835, 0.0, 0.0010850694961845875, 0.0, 0.0164930559694767, 0.0184461809694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013020833721384406, 0.0, 0.0203993059694767, 0.0, 0.0, 0.01215277798473835, 0.0006510416860692203, 0.0, 0.004123263992369175, 0.0078125, 0.44921875, 0.0, 0.0010850694961845875, 0.015407986007630825, 0.01519097201526165, 0.01215277798473835, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0067274305038154125, 0.0, 0.0, 0.00021701389050576836, 0.0026041667442768812, 0.9997829794883728, 0.0, 0.0, 0.0, 0.0392795130610466, 0.0, 0.0, 0.0026041667442768812, 0.0813802108168602, 0.0, 0.014322916977107525, 0.0030381944961845875, 0.0, 0.01410590298473835, 0.0, 0.0006510416860692203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01779513992369175, 0.01888020895421505, 0.0, 0.0282118059694767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0030381944961845875, 0.0071614584885537624, 0.010850694961845875, 0.0, 0.0, 0.0004340277810115367, 0.0, 0.02799479104578495, 0.0, 0.0290798619389534, 0.0, 0.01692708395421505, 0.2074652761220932, 0.0, 0.0245225690305233, 0.0, 0.02734375, 0.8441840410232544, 0.0065104165114462376, 0.0, 0.0659722238779068, 0.0757378488779068, 0.0, 0.0234375, 0.0, 0.0264756940305233, 0.01909722201526165, 0.0, 0.0, 0.0, 0.0, 0.0922309011220932, 0.0, 0.0, 0.0125868059694767, 0.0, 0.0, 0.0, 0.0, 0.0030381944961845875, 0.008897569961845875, 0.0, 0.0, 0.0015190972480922937, 0.008897569961845875, 0.0032552082557231188, 0.0251736119389534, 0.013454861007630825, 0.0251736119389534, 0.008463541977107525, 0.0030381944961845875, 0.0, 0.01627604104578495, 0.02300347201526165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01801215298473835, 0.0030381944961845875, 0.0]

 sparsity of   [0.00021701389050576836, 0.9997829794883728, 0.2319878488779068, 0.0, 0.9989149570465088, 0.00434027798473835, 0.0, 0.1944444477558136, 0.0, 0.0447048619389534, 0.9997829794883728, 0.0592447929084301, 0.01323784701526165, 0.0, 0.9995659589767456, 0.0, 0.02604166604578495, 0.0, 0.0, 0.001953125, 0.0, 0.02604166604578495, 0.0028211805038154125, 0.00021701389050576836, 0.00021701389050576836, 0.009982638992369175, 0.0026041667442768812, 0.0052083334885537624, 0.0, 0.6569010615348816, 0.0, 0.9995659589767456, 0.02278645895421505, 0.002170138992369175, 0.0, 0.01410590298473835, 0.0, 0.0234375, 0.1213107630610466, 0.9993489384651184, 0.0577256940305233, 0.010416666977107525, 0.0, 0.0212673619389534, 0.0, 0.0013020833721384406, 0.0, 0.011935763992369175, 0.0, 0.0, 0.0069444444961845875, 0.0368923619389534, 0.0, 0.0013020833721384406, 0.0, 0.9993489384651184, 0.0, 0.1436631977558136, 0.581163227558136, 0.008029513992369175, 0.0184461809694767, 0.00021701389050576836, 0.0360243059694767, 0.0069444444961845875, 0.0, 0.0, 0.0, 0.005859375, 0.630859375, 0.0, 0.00824652798473835, 0.0, 0.0, 0.00021701389050576836, 0.0310329869389534, 0.00629340298473835, 0.0036892362404614687, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.009765625, 0.0045572915114462376, 0.0, 0.009765625, 0.0010850694961845875, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.00021701389050576836, 0.0, 0.0, 0.00021701389050576836, 0.1124131977558136, 0.008029513992369175, 0.999131977558136, 0.0049913194961845875, 0.0251736119389534, 0.0026041667442768812, 0.0, 0.0, 0.0, 0.0251736119389534, 0.0, 0.00021701389050576836, 0.056640625, 0.0, 0.0, 0.0473090298473835, 0.0184461809694767, 0.0, 0.00021701389050576836, 0.0, 0.010416666977107525, 0.0, 0.0206163190305233, 0.9995659589767456, 0.005425347480922937, 0.0, 0.00824652798473835, 0.002170138992369175, 0.0010850694961845875, 0.0, 0.291015625, 0.0069444444961845875, 0.0, 0.009982638992369175, 0.0, 0.0167100690305233, 0.0, 0.0164930559694767, 0.0, 0.0078125, 0.999131977558136, 0.0381944440305233, 0.011067708022892475, 0.014756944961845875, 0.021484375, 0.0, 0.01019965298473835, 0.0503472238779068, 0.0004340277810115367, 0.0562065988779068, 0.011067708022892475, 0.0, 0.0, 0.0538194440305233, 0.0, 0.9997829794883728, 0.02300347201526165, 0.0008680555620230734, 0.0069444444961845875, 0.0, 0.0, 0.01996527798473835, 0.0, 0.0, 0.0323350690305233, 0.00021701389050576836, 0.02387152798473835, 0.0, 0.0015190972480922937, 0.0, 0.0, 0.0323350690305233, 0.0, 0.0, 0.00021701389050576836, 0.064453125, 0.0173611119389534, 0.0, 0.0, 0.0, 0.0193142369389534, 0.0017361111240461469, 0.0518663190305233, 0.6740451455116272, 0.0, 0.0032552082557231188, 0.01019965298473835, 0.1134982630610466, 0.0, 0.0, 0.0013020833721384406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0397135429084301, 0.0, 0.010416666977107525, 0.0, 0.02864583395421505, 0.0349392369389534, 0.1323784738779068, 0.0, 0.00021701389050576836, 0.03515625, 0.00021701389050576836, 0.00390625, 0.0310329869389534, 0.0483940988779068, 0.02734375, 0.0540364570915699, 0.0959201380610466, 0.0, 0.02105034701526165, 0.0, 0.0, 0.0, 0.892578125, 0.0, 0.0023871527519077063, 0.0, 0.0, 0.012369791977107525, 0.0, 0.0, 0.0, 0.0, 0.0006510416860692203, 0.0, 0.0, 0.0, 0.0, 0.0028211805038154125, 0.0, 0.1649305522441864, 0.0, 0.0, 0.0425347238779068, 0.05078125, 0.0, 0.013888888992369175, 0.0, 0.0004340277810115367, 0.0, 0.0225694440305233, 0.0017361111240461469, 0.9989149570465088, 0.0, 0.0, 0.0173611119389534, 0.0384114570915699, 0.0355902798473835, 0.0, 0.0559895820915699, 0.0, 0.0, 0.00021701389050576836, 0.0347222238779068, 0.01627604104578495, 0.0, 0.0384114570915699, 0.0, 0.02278645895421505, 0.0013020833721384406, 0.7983940839767456, 0.0184461809694767, 0.0, 0.0, 0.0, 0.009114583022892475, 0.0, 0.0, 0.0069444444961845875, 0.001953125, 0.0, 0.0349392369389534, 0.0, 0.0, 0.009765625, 0.9995659589767456, 0.0013020833721384406, 0.01822916604578495, 0.2141927033662796, 0.0, 0.00021701389050576836, 0.9950087070465088, 0.0, 0.0, 0.088758684694767, 0.0, 0.0004340277810115367, 0.0362413190305233, 0.0509982630610466, 0.0, 0.0, 0.9995659589767456, 0.0, 0.0, 0.0323350690305233, 0.02278645895421505, 0.0032552082557231188, 0.012369791977107525, 0.0, 0.094618059694767, 0.0, 0.0264756940305233, 0.9900173544883728, 0.0, 0.00434027798473835, 0.014322916977107525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032552082557231188, 0.0, 0.014973958022892475, 0.012803819961845875, 0.0, 0.0, 0.0, 0.010416666977107525, 0.0032552082557231188, 0.0425347238779068, 0.0, 0.9850260615348816, 0.01801215298473835, 0.0, 0.8843315839767456, 0.9401041865348816, 0.0674913227558136, 0.0262586809694767, 0.0, 0.008029513992369175, 0.0, 0.008463541977107525, 0.0017361111240461469, 0.0, 0.0, 0.008897569961845875, 0.02387152798473835, 0.9984809160232544, 0.0010850694961845875, 0.0377604179084301, 0.0, 0.0, 0.00824652798473835, 0.0, 0.014322916977107525, 0.001953125, 0.0, 0.0, 0.0473090298473835, 0.0, 0.0, 0.01801215298473835, 0.0, 0.02278645895421505, 0.0034722222480922937, 0.0032552082557231188, 0.0, 0.2328559011220932, 0.00021701389050576836, 0.0, 0.0004340277810115367, 0.0, 0.0, 0.0, 0.091796875, 0.0358072929084301, 0.0325520820915699, 0.0, 0.594835102558136, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0501302070915699, 0.0319010429084301, 0.0036892362404614687, 0.0052083334885537624, 0.0, 0.00021701389050576836, 0.0032552082557231188, 0.0, 0.0, 0.351128488779068, 0.0, 0.02734375, 0.0, 0.005642361007630825, 0.0, 0.0, 0.0, 0.0414496548473835, 0.0, 0.0067274305038154125, 0.0, 0.0030381944961845875, 0.011067708022892475, 0.015625, 0.0, 0.02734375, 0.00021701389050576836, 0.4945746660232544, 0.01627604104578495, 0.0006510416860692203, 0.0, 0.0416666679084301, 0.0034722222480922937, 0.0028211805038154125, 0.0032552082557231188, 0.136501744389534, 0.007378472480922937, 0.0, 0.0, 0.0145399309694767, 0.0885416641831398, 0.0, 0.0010850694961845875, 0.0, 0.02213541604578495, 0.0086805559694767, 0.0071614584885537624, 0.0, 0.0, 0.0, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0004340277810115367, 0.0473090298473835, 0.0, 0.2150607705116272, 0.0, 0.0, 0.0592447929084301, 0.013020833022892475, 0.013020833022892475, 0.0, 0.0, 0.0023871527519077063, 0.0342881940305233, 0.0470920130610466, 0.0381944440305233, 0.009765625, 0.02864583395421505, 0.00021701389050576836, 0.0, 0.0, 0.0, 0.0666232630610466, 0.0, 0.0013020833721384406, 0.04296875, 0.0, 0.0, 0.0, 0.009982638992369175, 0.00933159701526165, 0.02777777798473835, 0.0, 0.9995659589767456, 0.0, 0.0, 0.007378472480922937, 0.0, 0.9442274570465088, 0.0, 0.0, 0.0067274305038154125, 0.0, 0.00021701389050576836, 0.0483940988779068, 0.0455729179084301, 0.0314670130610466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0290798619389534, 0.0983072891831398, 0.0145399309694767, 0.0, 0.0, 0.0, 0.0336371548473835, 0.0, 0.9997829794883728, 0.0, 0.0, 0.0, 0.0, 0.9368489384651184, 0.0, 0.0, 0.012369791977107525, 0.0520833320915699, 0.0, 0.0065104165114462376, 0.0, 0.00021701389050576836, 0.00434027798473835, 0.0086805559694767, 0.01410590298473835, 0.007595486007630825, 0.0, 0.014322916977107525]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 2888340.0372540504 (unstructured) 0 (structured)

max weight is  tensor([1.2807e-10, 9.4040e-11, 5.3426e-01, 1.7359e-01, 6.2486e-11, 1.0424e-01,
        1.8647e-11, 3.0415e-02, 7.7936e-01, 1.9014e-10, 9.6187e-02, 5.4191e-01,
        4.4781e-11, 2.3776e-01, 6.3613e-03, 1.1092e-01, 1.8502e-01, 2.6773e-01,
        5.2177e-01, 3.4921e-01, 2.0826e-11, 2.2426e-02, 1.4053e-10, 1.3760e-01,
        9.8070e-03, 1.7354e-12, 5.7582e-11, 1.2107e-10, 6.4282e-11, 6.2481e-11,
        9.6149e-02, 1.5284e-11, 1.2985e-10, 2.6114e-11, 1.4883e-01, 2.6759e-01,
        1.7504e-10, 4.5192e-02, 2.3849e-01, 2.5651e-01, 5.8983e-01, 2.3141e-11,
        2.2697e-01, 3.8201e-11, 8.1891e-04, 2.6263e-11, 1.3094e-01, 6.3868e-03,
        8.0705e-11, 5.0112e-10, 1.6696e-11, 2.8308e-01, 3.7430e-01, 7.9843e-03,
        2.6306e-01, 1.1516e-01, 1.9645e-01, 2.3024e-11, 3.6454e-01, 2.5505e-11,
        2.6401e-01, 3.7358e-01, 9.0739e-11, 2.7434e-03], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1514, 0.0994, 0.2148, 0.1746, 0.1790, 0.2043, 0.2265, 0.1914, 0.1426,
        0.2029, 0.2095, 0.2458, 0.1376, 0.1449, 0.2826, 0.1656, 0.2286, 0.1525,
        0.2738, 0.1283, 0.1354, 0.1554, 0.2225, 0.1967, 0.1959, 0.1581, 0.1958,
        0.1565, 0.2258, 0.2299, 0.1934, 0.1199, 0.2397, 0.1394, 0.1993, 0.2394,
        0.1886, 0.2433, 0.1851, 0.2327, 0.1150, 0.2170, 0.1942, 0.1722, 0.1491,
        0.1973, 0.1084, 0.2025, 0.1726, 0.1752, 0.1888, 0.1439, 0.1808, 0.1964,
        0.1729, 0.1543, 0.1637, 0.2140, 0.1648, 0.1883, 0.2392, 0.1778, 0.2273,
        0.2205], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.1237, 0.1020, 0.1041, 0.0922, 0.0906, 0.1470, 0.1310, 0.1097, 0.1272,
        0.1147, 0.1027, 0.1445, 0.0819, 0.1209, 0.0952, 0.1011, 0.0268, 0.1075,
        0.1010, 0.0777, 0.1208, 0.1280, 0.1055, 0.1179, 0.0832, 0.1212, 0.0718,
        0.0878, 0.1220, 0.1184, 0.1056, 0.1380, 0.1184, 0.0857, 0.0857, 0.1135,
        0.0886, 0.0922, 0.0931, 0.1205, 0.1301, 0.1114, 0.1180, 0.0777, 0.1034,
        0.0958, 0.1261, 0.1129, 0.0810, 0.1027, 0.0835, 0.1161, 0.1139, 0.1353,
        0.0945, 0.1077, 0.1641, 0.1000, 0.1066, 0.1085, 0.0964, 0.0951, 0.0544,
        0.0894, 0.0993, 0.1193, 0.1088, 0.1053, 0.0873, 0.1034, 0.1063, 0.0880,
        0.1205, 0.1173, 0.1194, 0.0895, 0.1070, 0.1465, 0.1153, 0.1096, 0.1279,
        0.0797, 0.0943, 0.0944, 0.1366, 0.0712, 0.1038, 0.0813, 0.0951, 0.0804,
        0.0850, 0.1228, 0.0889, 0.0982, 0.1035, 0.1206, 0.1179, 0.0979, 0.1247,
        0.1280, 0.1336, 0.1229, 0.1125, 0.1264, 0.0739, 0.0831, 0.0866, 0.1128,
        0.1152, 0.0915, 0.1103, 0.0929, 0.1013, 0.1013, 0.1113, 0.1282, 0.0902,
        0.1246, 0.1129, 0.0667, 0.0605, 0.1223, 0.1259, 0.1212, 0.0872, 0.0974,
        0.1338, 0.1146], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1831e-01, 1.0764e-01, 1.0286e-01, 1.1664e-01, 1.0783e-01, 1.1025e-01,
        1.1837e-01, 1.0978e-01, 1.2034e-01, 1.1361e-01, 1.1258e-01, 1.1659e-01,
        1.0516e-01, 1.0683e-01, 1.1024e-01, 1.1924e-01, 1.1199e-01, 1.0493e-01,
        1.1751e-01, 1.0900e-01, 1.0976e-01, 1.2493e-01, 1.3332e-01, 1.0704e-01,
        1.1896e-01, 1.2353e-01, 9.8912e-02, 1.0628e-01, 1.1775e-01, 9.4833e-02,
        1.0618e-01, 1.1318e-01, 1.1085e-01, 1.2866e-01, 1.0672e-01, 1.1374e-01,
        1.2613e-01, 1.1533e-01, 1.0840e-01, 1.1542e-01, 1.0059e-01, 1.0907e-01,
        1.2344e-01, 1.0807e-01, 1.0823e-01, 1.1359e-01, 1.0210e-01, 1.1251e-01,
        1.1806e-01, 1.2001e-01, 1.2096e-01, 1.0917e-01, 1.2987e-01, 1.0152e-01,
        1.2985e-01, 1.1187e-01, 1.1628e-01, 1.1174e-01, 1.2051e-01, 1.1654e-01,
        1.1096e-01, 1.0439e-01, 1.0874e-01, 1.0721e-01, 1.0632e-01, 1.1101e-01,
        1.1320e-01, 1.5091e-08, 1.1212e-01, 1.1377e-01, 1.0973e-01, 9.7411e-02,
        1.2170e-01, 1.2221e-01, 1.1006e-01, 1.1858e-01, 1.2182e-01, 1.1718e-01,
        1.1773e-01, 1.0525e-01, 1.0011e-01, 9.6986e-02, 1.0782e-01, 1.1883e-01,
        1.0622e-01, 1.0278e-01, 1.2445e-01, 1.2462e-01, 9.7820e-02, 1.1742e-01,
        1.1786e-01, 1.1687e-01, 1.0431e-01, 1.0944e-01, 9.3244e-02, 1.6686e-01,
        1.2552e-01, 1.1038e-01, 1.1762e-01, 1.0570e-01, 1.0596e-01, 8.9608e-02,
        1.1790e-01, 1.1212e-01, 1.0698e-01, 1.2356e-01, 1.0517e-01, 1.0303e-01,
        1.0339e-01, 1.3326e-01, 1.1526e-01, 1.1214e-01, 1.0956e-01, 1.0485e-01,
        1.0784e-01, 1.1648e-01, 1.1002e-01, 1.1820e-01, 1.2384e-01, 1.2208e-01,
        1.1874e-01, 1.2265e-01, 1.1146e-01, 1.1086e-01, 1.2611e-01, 1.0533e-01,
        1.2117e-01, 1.2215e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2711e-01, 1.3541e-01, 1.5201e-01, 1.1508e-01, 1.3809e-01, 1.4134e-01,
        1.4299e-01, 1.4391e-01, 1.5217e-01, 1.5778e-01, 1.3425e-01, 1.3590e-01,
        1.5438e-01, 1.3130e-01, 1.4390e-01, 1.5733e-01, 1.0029e-01, 1.4955e-01,
        1.3547e-01, 2.1517e-08, 7.1095e-02, 1.4030e-01, 1.4847e-01, 1.2960e-01,
        1.0900e-01, 1.5154e-01, 1.3532e-01, 1.5792e-01, 1.3447e-01, 1.6115e-01,
        1.5593e-01, 1.3503e-01, 1.5591e-01, 1.2554e-01, 1.6058e-01, 1.5961e-01,
        1.1972e-01, 1.6901e-01, 1.1054e-01, 1.4533e-01, 1.1861e-01, 1.4805e-01,
        1.5133e-01, 1.6153e-01, 1.4426e-01, 1.3426e-01, 1.5163e-01, 1.4001e-01,
        1.4249e-01, 1.5711e-01, 1.4790e-01, 1.5051e-01, 1.5179e-01, 1.4123e-01,
        1.4178e-01, 1.5583e-01, 1.4675e-01, 1.5050e-01, 1.2858e-01, 1.9610e-01,
        1.4553e-01, 3.0401e-08, 1.2982e-01, 1.4519e-01, 1.3965e-01, 1.2473e-01,
        1.3062e-01, 1.5706e-01, 1.3172e-01, 1.5866e-01, 1.5914e-01, 1.4583e-01,
        1.2859e-08, 1.3764e-01, 1.2275e-01, 1.5716e-01, 1.4322e-01, 1.3084e-01,
        1.2600e-01, 1.3478e-01, 1.3582e-01, 1.4271e-01, 1.3754e-01, 1.4147e-08,
        1.3620e-01, 1.3779e-01, 1.4625e-01, 1.5293e-01, 1.4047e-01, 1.4897e-01,
        1.3875e-01, 1.4535e-01, 1.4379e-01, 1.6540e-01, 1.5207e-01, 1.7254e-01,
        1.3305e-01, 1.5019e-01, 1.3266e-01, 1.2960e-01, 1.6360e-01, 1.4408e-01,
        1.5002e-01, 1.7153e-01, 1.5599e-01, 1.4701e-01, 1.4742e-01, 1.3335e-01,
        1.3521e-01, 1.6937e-01, 1.4342e-01, 1.5795e-01, 1.2662e-01, 1.7029e-01,
        1.5299e-01, 1.7448e-01, 1.7047e-01, 1.4626e-01, 1.6451e-01, 1.3878e-01,
        1.5200e-01, 1.5346e-01, 1.4732e-01, 1.1200e-01, 1.3491e-01, 3.3493e-08,
        1.4290e-01, 1.1631e-01, 1.5750e-01, 1.0327e-01, 1.4214e-01, 1.7557e-01,
        1.5637e-01, 1.2631e-01, 5.4900e-08, 1.1519e-01, 1.4306e-01, 1.1670e-01,
        1.5423e-01, 1.4507e-01, 1.6290e-01, 1.3926e-01, 1.5248e-01, 1.4055e-01,
        1.8660e-01, 1.3141e-01, 1.4517e-01, 1.4465e-01, 1.3331e-01, 1.3784e-01,
        1.6568e-01, 1.4624e-01, 1.6180e-01, 1.4730e-01, 1.4179e-01, 1.6009e-01,
        9.4222e-10, 1.2636e-01, 1.3368e-01, 1.4658e-01, 1.5610e-01, 1.1506e-01,
        1.5775e-01, 1.2096e-01, 1.4322e-01, 1.7230e-01, 1.3709e-01, 1.2674e-01,
        1.3409e-01, 1.6270e-01, 1.3709e-01, 1.4576e-01, 1.5142e-01, 1.1443e-08,
        1.4631e-01, 1.4866e-01, 1.3957e-01, 1.4344e-01, 1.4823e-01, 1.3952e-01,
        7.8053e-09, 1.5915e-01, 1.6778e-01, 1.6658e-01, 1.4712e-01, 1.5545e-01,
        1.0784e-01, 1.4292e-01, 1.2311e-01, 1.5070e-01, 1.2996e-01, 1.5046e-01,
        1.4715e-01, 1.3834e-01, 1.0895e-01, 1.3759e-01, 1.4288e-01, 1.4452e-01,
        1.5667e-01, 1.4528e-01, 1.5256e-01, 1.4765e-01, 1.5132e-01, 1.4448e-01,
        1.4964e-01, 1.3647e-01, 1.4351e-01, 1.4216e-01, 1.4825e-01, 1.2717e-01,
        1.4784e-01, 1.9146e-08, 1.6087e-01, 1.4628e-01, 1.4130e-01, 1.3467e-01,
        1.5171e-01, 1.6559e-01, 1.4852e-01, 1.4982e-01, 1.4876e-01, 1.5386e-01,
        1.2055e-01, 1.5562e-01, 1.5754e-01, 1.5584e-01, 1.3938e-01, 1.3192e-01,
        1.3690e-01, 1.5407e-01, 1.3906e-01, 1.4420e-01, 3.2818e-08, 3.8949e-08,
        1.5000e-01, 1.6888e-01, 1.3050e-01, 1.2974e-01, 1.2989e-01, 7.4812e-08,
        1.4504e-01, 1.4893e-01, 1.5293e-01, 1.3607e-01, 1.5670e-01, 1.2365e-01,
        1.5485e-01, 1.3199e-01, 1.3337e-01, 1.3611e-01, 1.5621e-01, 1.3460e-01,
        1.5349e-01, 1.4161e-01, 1.2930e-01, 1.4475e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([5.9104e-08, 2.4267e-07, 1.1835e-07, 8.7890e-02, 1.2480e-07, 7.5465e-02,
        2.4267e-07, 5.6126e-02, 7.5678e-02, 6.8448e-02, 5.3739e-02, 7.2956e-08,
        1.4249e-08, 8.7058e-02, 9.0168e-02, 8.1487e-02, 4.8422e-02, 8.4630e-02,
        9.8447e-02, 6.1624e-08, 6.5719e-02, 6.1624e-08, 9.8690e-02, 9.4991e-02,
        4.2290e-02, 1.0023e-07, 4.6095e-02, 5.5487e-02, 8.3604e-02, 8.2685e-08,
        1.4253e-07, 3.4927e-08, 1.2822e-07, 6.3214e-08, 8.2951e-02, 8.0138e-08,
        8.6717e-02, 6.1497e-02, 1.2069e-07, 5.8703e-02, 9.5097e-02, 1.9941e-07,
        6.9964e-02, 9.5306e-08, 3.4927e-08, 5.0483e-02, 2.0963e-07, 9.3415e-02,
        1.0237e-01, 7.8550e-02, 7.2710e-02, 9.5554e-02, 7.2347e-02, 1.0023e-07,
        1.9694e-07, 2.2203e-07, 8.2821e-02, 1.3583e-07, 6.1624e-08, 6.1624e-08,
        1.8967e-07, 2.2203e-07, 5.5576e-08, 7.1007e-02, 6.1784e-02, 2.2203e-07,
        8.2916e-02, 3.4927e-08, 5.4787e-02, 2.8484e-07, 1.5827e-07, 3.9597e-08,
        8.6429e-08, 3.2753e-08, 8.7014e-02, 7.4204e-02, 8.5549e-08, 2.8014e-08,
        3.9597e-08, 6.0865e-02, 7.4918e-02, 8.0138e-08, 2.4267e-07, 7.6419e-02,
        1.4220e-07, 1.9754e-07, 9.3757e-08, 1.9694e-07, 8.6435e-02, 5.1040e-02,
        8.6429e-08, 6.1624e-08, 1.9754e-07, 6.4641e-02, 1.9941e-07, 1.1885e-07,
        6.2464e-02, 8.7057e-02, 7.3584e-02, 1.1048e-07, 9.3483e-02, 7.5872e-02,
        6.6063e-02, 1.9754e-07, 6.7982e-02, 1.3345e-07, 1.3987e-07, 4.4495e-08,
        1.3769e-07, 8.2684e-08, 3.6944e-02, 7.1514e-02, 1.9941e-07, 7.7311e-02,
        7.0793e-02, 8.8274e-02, 1.4633e-07, 1.9754e-07, 6.4728e-02, 4.4772e-02,
        8.8873e-02, 7.8857e-02, 9.3757e-08, 1.9808e-07, 3.4893e-08, 7.0556e-02,
        1.6160e-07, 9.9736e-02, 1.0532e-01, 8.8816e-02, 7.3142e-02, 2.3270e-08,
        1.9754e-07, 1.6248e-07, 1.6248e-07, 1.3458e-07, 6.1636e-08, 4.8217e-02,
        2.3270e-08, 1.5267e-07, 6.4640e-02, 6.3507e-02, 9.5275e-02, 1.3987e-07,
        8.4849e-02, 7.6839e-02, 3.5469e-08, 6.4326e-02, 7.5322e-02, 7.9706e-02,
        7.2956e-08, 1.4140e-08, 2.3270e-08, 9.4008e-02, 2.8484e-07, 1.1637e-07,
        2.4267e-07, 2.9839e-07, 1.2069e-07, 1.0023e-07, 9.5845e-02, 7.7645e-02,
        4.9980e-08, 8.9777e-02, 6.1624e-08, 7.5314e-02, 7.0130e-02, 7.6366e-02,
        1.0023e-07, 6.1624e-08, 7.2552e-02, 8.3610e-02, 7.9858e-02, 9.6002e-02,
        9.0967e-02, 7.1902e-02, 8.7667e-02, 8.0138e-08, 7.4608e-02, 8.0120e-02,
        7.4165e-02, 3.9597e-08, 1.2613e-07, 8.6814e-02, 1.5204e-07, 6.7317e-02,
        8.1457e-02, 6.0012e-02, 8.3731e-02, 7.0336e-02, 4.8484e-02, 7.2596e-02,
        7.2124e-02, 5.7953e-02, 9.2290e-08, 6.4576e-02, 2.0963e-07, 8.2467e-02,
        7.3605e-02, 2.0068e-02, 6.8380e-02, 7.3746e-02, 7.8590e-02, 9.1351e-02,
        7.2501e-02, 8.6661e-02, 4.2021e-02, 5.9420e-02, 8.8922e-02, 6.4971e-02,
        7.9033e-02, 8.6142e-02, 6.8728e-02, 9.1021e-08, 6.0341e-02, 7.9469e-02,
        7.6862e-02, 8.6429e-08, 9.2566e-02, 6.1624e-08, 6.0524e-02, 2.0665e-02,
        6.6174e-02, 2.0963e-07, 6.4214e-02, 1.5267e-07, 8.4587e-02, 6.7829e-02,
        7.4696e-02, 7.5942e-02, 1.1835e-07, 2.4267e-07, 6.9625e-02, 5.7543e-02,
        1.1958e-01, 2.0963e-07, 3.9597e-08, 7.1402e-02, 8.4614e-02, 7.0048e-02,
        4.8943e-07, 6.9264e-02, 1.3987e-07, 8.2684e-08, 9.5306e-08, 6.2578e-02,
        4.9980e-08, 7.1179e-02, 6.1624e-08, 2.8805e-08, 5.1418e-02, 6.3748e-02,
        8.0460e-02, 1.3987e-07, 8.1956e-02, 7.4318e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.9615e-08, 4.4398e-08, 6.7723e-08, 7.1535e-08, 4.9739e-08, 1.6939e-07,
        6.9535e-08, 9.1465e-08, 2.0963e-07, 7.7695e-08, 1.4684e-07, 4.3209e-02,
        9.0897e-08, 8.9616e-08, 4.5632e-08, 4.2534e-02, 5.8649e-08, 1.9594e-02,
        2.6950e-07, 6.2239e-02, 6.8249e-08, 1.5173e-07, 6.3131e-02, 4.5133e-02,
        4.5227e-02, 1.4684e-07, 4.9090e-02, 4.9423e-02, 4.5660e-02, 5.0701e-02,
        5.9745e-02, 9.1465e-08, 1.6611e-07, 8.1158e-08, 4.1999e-02, 4.8603e-02,
        1.0123e-07, 6.8249e-08, 3.8094e-02, 2.6950e-07, 5.7909e-02, 1.7713e-07,
        1.7713e-07, 2.8716e-02, 4.8933e-02, 1.0835e-07, 6.8641e-08, 5.3815e-02,
        5.3846e-02, 5.1698e-02, 5.4184e-02, 2.2497e-07, 5.8225e-02, 1.0123e-07,
        7.8618e-08, 8.8368e-08, 3.4600e-02, 6.4385e-10, 1.6611e-07, 4.5632e-08,
        7.9568e-08, 3.3476e-08, 7.7756e-08, 1.5173e-07, 5.2048e-08, 2.0528e-07,
        1.5173e-07, 7.8582e-08, 9.1465e-08, 5.1450e-02, 1.8788e-07, 5.4145e-02,
        5.9025e-02, 1.8469e-08, 4.5632e-08, 1.2512e-07, 5.7889e-02, 6.5204e-08,
        4.7371e-02, 4.2403e-02, 9.4556e-08, 6.7723e-08, 8.9616e-08, 1.4684e-07,
        1.0164e-07, 1.4053e-07, 3.3353e-02, 2.0528e-07, 9.1465e-08, 9.7261e-08,
        4.9739e-08, 4.9250e-02, 6.3019e-02, 6.1288e-08, 2.0528e-07, 8.9616e-08,
        5.2452e-08, 4.9739e-08, 2.4012e-07, 1.5173e-07, 5.1291e-02, 5.4395e-02,
        1.1233e-07, 5.7648e-08, 2.9562e-08, 5.7203e-02, 4.9633e-02, 2.5229e-07,
        1.6611e-07, 1.5173e-07, 1.2044e-07, 1.4875e-07, 1.6755e-07, 9.7261e-08,
        3.5176e-08, 8.9615e-08, 6.1068e-02, 1.0123e-07, 6.5235e-02, 2.0633e-07,
        4.5632e-08, 1.3796e-07, 1.1984e-07, 6.2384e-02, 7.1535e-08, 4.8551e-08,
        1.6755e-07, 5.5804e-02, 4.0037e-02, 2.1759e-07, 6.9535e-08, 4.6792e-02,
        8.9615e-08, 1.0835e-07, 1.0164e-07, 5.3667e-02, 3.4275e-02, 2.5229e-07,
        4.9739e-08, 4.3919e-02, 1.1984e-07, 8.9615e-08, 1.1214e-07, 2.0633e-07,
        1.1233e-07, 4.4522e-08, 1.1233e-07, 1.4356e-07, 5.4059e-02, 8.8371e-08,
        2.1759e-07, 6.0987e-02, 5.0961e-02, 2.6950e-07, 1.4054e-07, 8.9615e-08,
        1.4875e-07, 6.8215e-08, 6.7895e-02, 1.5429e-07, 2.0633e-07, 1.6258e-07,
        2.1759e-07, 5.4448e-02, 1.4875e-07, 9.7261e-08, 6.9114e-08, 3.1278e-02,
        4.5968e-02, 1.0123e-07, 3.2606e-08, 3.4924e-02, 8.6462e-08, 5.4059e-02,
        5.2791e-02, 2.6950e-07, 5.7228e-02, 4.8161e-02, 1.7713e-07, 5.0711e-02,
        3.5622e-08, 1.0123e-07, 6.5204e-08, 2.5229e-07, 1.2512e-07, 1.4875e-07,
        5.0773e-02, 3.3476e-08, 1.5173e-07, 9.8975e-08, 3.7592e-02, 7.1535e-08,
        5.1444e-02, 1.6939e-07, 5.4642e-02, 4.6288e-02, 5.4609e-02, 1.4086e-07,
        5.4607e-02, 5.5707e-02, 1.5278e-07, 6.2030e-02, 2.0689e-07, 5.1988e-02,
        4.2659e-02, 6.3217e-02, 6.1727e-02, 5.5551e-02, 2.1759e-07, 1.4875e-07,
        1.0164e-07, 5.0445e-02, 9.8975e-08, 3.5299e-08, 1.5173e-07, 1.1426e-07,
        2.0963e-07, 5.5095e-02, 3.9080e-02, 5.2866e-02, 2.6950e-07, 1.4344e-07,
        1.3219e-07, 1.8467e-08, 4.9574e-02, 4.8231e-02, 1.4875e-07, 5.5650e-08,
        4.5396e-02, 4.9927e-02, 3.3476e-08, 5.9032e-02, 1.4686e-07, 4.6709e-02,
        5.5524e-02, 1.0813e-02, 6.4027e-02, 6.0618e-02, 4.8551e-08, 1.2680e-07,
        6.5271e-02, 4.9739e-08, 8.6462e-08, 1.7713e-07, 5.0782e-02, 1.4875e-07,
        7.9484e-08, 4.7427e-02, 5.8095e-02, 1.1233e-07, 5.7665e-02, 1.9321e-07,
        1.7713e-07, 5.9315e-02, 6.1581e-02, 6.9114e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.3295e-07, 9.1004e-08, 1.0544e-07, 4.1964e-02, 1.6856e-07, 1.6856e-07,
        7.6213e-08, 2.5682e-07, 6.5103e-08, 2.8919e-02, 2.7350e-03, 2.1886e-07,
        5.6902e-09, 4.4724e-08, 7.0220e-08, 1.1407e-07, 1.6480e-07, 1.2361e-07,
        1.4271e-07, 3.4119e-02, 2.0410e-07, 4.5922e-08, 4.4833e-02, 6.2108e-08,
        3.5599e-02, 4.2008e-02, 9.1900e-08, 1.0171e-08, 3.3261e-07, 6.2298e-08,
        1.3751e-07, 1.6856e-07, 6.5541e-08, 4.2093e-02, 1.0544e-07, 5.6195e-08,
        1.2109e-07, 1.0235e-07, 2.6275e-08, 1.4155e-07, 3.8777e-07, 8.5926e-08,
        2.8576e-07, 9.5244e-08, 2.0640e-07, 3.8777e-07, 9.5244e-08, 2.0410e-07,
        2.8576e-07, 2.6275e-08, 4.1556e-02, 6.1307e-08, 1.5523e-07, 1.7396e-07,
        2.9603e-07, 9.4966e-09, 1.3472e-07, 3.9727e-02, 2.5682e-07, 2.5682e-07,
        1.3460e-07, 1.4155e-07, 1.2109e-07, 1.3472e-07, 1.6795e-07, 2.1886e-07,
        6.0504e-07, 1.3472e-07, 1.2089e-07, 2.3176e-07, 1.6856e-07, 1.4155e-07,
        2.8576e-07, 1.1950e-07, 1.1436e-07, 2.4846e-07, 1.0544e-07, 1.4155e-07,
        3.9948e-08, 2.2338e-08, 1.6070e-07, 2.0600e-08, 1.2816e-07, 1.4155e-07,
        4.0340e-02, 1.0303e-07, 7.0220e-08, 7.3436e-08, 2.6089e-07, 2.9603e-07,
        2.5381e-07, 1.0303e-07, 2.0410e-07, 9.5244e-08, 6.9094e-08, 1.4267e-07,
        1.2009e-07, 2.9603e-07, 6.5103e-08, 3.6026e-08, 1.2361e-07, 1.3472e-07,
        6.5103e-08, 1.6856e-07, 4.8518e-08, 9.1900e-08, 1.0437e-07, 1.4271e-07,
        1.0477e-07, 1.1407e-07, 1.6414e-07, 8.4809e-08, 1.6856e-07, 9.5244e-08,
        1.3751e-07, 1.6795e-07, 1.5972e-07, 1.0303e-07, 2.0410e-07, 7.3315e-08,
        9.6381e-08, 5.9661e-08, 2.0410e-07, 1.0078e-07, 1.4155e-07, 8.0622e-08,
        1.4155e-07, 1.6856e-07, 9.5244e-08, 1.1527e-07, 2.3176e-07, 2.8576e-07,
        1.0235e-07, 1.0544e-07, 1.6856e-07, 1.8906e-07, 1.8152e-07, 3.9951e-08,
        6.2298e-08, 1.3604e-07, 1.6856e-07, 2.3358e-07, 2.0000e-07, 6.3379e-08,
        4.4224e-02, 3.4312e-02, 2.8576e-07, 2.1886e-07, 9.1900e-08, 7.5595e-08,
        9.5244e-08, 1.1323e-07, 1.0078e-07, 8.4809e-08, 2.0410e-07, 1.1822e-07,
        2.5381e-07, 2.4628e-02, 1.3295e-07, 1.4155e-07, 8.0262e-08, 3.5122e-02,
        1.0170e-08, 1.3295e-07, 1.2109e-07, 9.5244e-08, 6.7792e-08, 1.0256e-07,
        5.9816e-08, 3.1301e-02, 8.0262e-08, 1.3604e-07, 5.1306e-08, 3.9951e-08,
        2.9222e-07, 2.3176e-07, 2.9603e-07, 2.9603e-07, 1.4271e-07, 2.0000e-07,
        1.6194e-07, 1.3472e-07, 2.9603e-07, 4.5926e-08, 1.8526e-07, 8.5079e-08,
        2.5682e-07, 2.9603e-07, 2.5682e-07, 5.5078e-02, 1.3472e-07, 6.2298e-08,
        8.0262e-08, 1.3526e-07, 1.0544e-07, 3.2551e-02, 1.5523e-07, 2.9603e-07,
        1.9887e-08, 1.6856e-07, 1.3295e-07, 2.9949e-03, 2.0410e-07, 2.8576e-07,
        4.3255e-02, 4.1147e-02, 2.1886e-07, 3.3679e-07, 2.1721e-07, 1.3472e-07,
        2.0818e-08, 5.7168e-08, 2.9603e-07, 1.5498e-07, 4.8623e-08, 1.4155e-07,
        4.8025e-02, 2.3358e-07, 1.3472e-07, 2.0410e-07, 6.9094e-08, 1.0303e-07,
        1.3751e-07, 1.0477e-07, 1.3063e-07, 3.3118e-07, 4.8518e-08, 2.0410e-07,
        1.0078e-07, 2.6928e-07, 7.3315e-08, 8.5079e-08, 2.0410e-07, 1.3751e-07,
        2.0000e-07, 3.3125e-02, 1.2109e-07, 5.6195e-08, 1.2109e-07, 1.2109e-07,
        1.1323e-07, 2.0000e-07, 6.1307e-08, 3.4574e-07, 2.9603e-07, 7.3436e-08,
        1.6795e-07, 6.3379e-08, 1.7351e-07, 1.1950e-07, 2.0410e-07, 1.9158e-07,
        8.4809e-08, 1.6856e-07, 9.1767e-08, 4.4851e-02, 7.3315e-08, 1.3165e-07,
        8.7194e-08, 1.0544e-07, 1.2109e-07, 1.3063e-07, 1.6856e-07, 1.3472e-07,
        2.1985e-07, 1.3472e-07, 4.3805e-02, 7.3436e-08, 1.1407e-07, 2.9603e-07,
        6.9094e-08, 6.1860e-08, 2.8576e-07, 4.1125e-03, 1.4310e-07, 4.7220e-02,
        8.0262e-08, 1.2109e-07, 1.1822e-07, 1.6856e-07, 1.3118e-07, 3.4574e-07,
        1.1140e-08, 7.3315e-08, 2.0410e-07, 6.2298e-08, 1.3472e-07, 3.8777e-07,
        1.3751e-07, 2.6020e-02, 1.6856e-07, 1.0361e-07, 4.8921e-02, 6.2298e-08,
        1.8906e-07, 4.5075e-02, 2.9603e-07, 3.7911e-08, 3.8973e-02, 2.8576e-07,
        3.0214e-08, 1.3603e-07, 4.8518e-08, 1.6856e-07, 1.9497e-07, 8.4809e-08,
        1.8326e-07, 1.5499e-07, 3.4574e-07, 1.8906e-07, 7.3436e-08, 6.5103e-08,
        2.3314e-07, 1.1436e-07, 1.8143e-07, 1.3472e-07, 1.3354e-07, 1.3295e-07,
        1.4155e-07, 1.6126e-07, 1.0758e-07, 9.5244e-08, 8.8351e-08, 6.7792e-08,
        9.1900e-08, 8.8351e-08, 4.5100e-08, 1.0477e-07, 6.9271e-08, 1.0303e-07,
        1.4802e-07, 1.4155e-07, 1.8906e-07, 9.1901e-08, 6.2298e-08, 1.4155e-07,
        1.6194e-07, 2.9603e-07, 1.1318e-07, 3.3446e-07, 4.8518e-08, 1.0303e-07,
        2.5381e-07, 8.8351e-08, 1.4310e-07, 1.1407e-07, 6.5103e-08, 2.3358e-07,
        5.2283e-08, 1.6795e-07, 1.0437e-07, 2.8576e-07, 6.2108e-08, 1.6795e-07,
        1.4271e-07, 9.5244e-08, 8.8351e-08, 8.7369e-08, 1.6856e-07, 1.3295e-07,
        1.3295e-07, 1.5307e-07, 1.6803e-07, 6.2298e-08, 4.8518e-08, 2.3358e-07,
        8.0262e-08, 2.3358e-07, 2.9603e-07, 1.4155e-07, 6.1307e-08, 7.3315e-08,
        8.8351e-08, 2.1865e-07, 1.6856e-07, 1.8906e-07, 2.2025e-07, 8.4809e-08,
        4.4724e-08, 1.6856e-07, 8.8037e-08, 6.5103e-08, 2.6372e-07, 1.0078e-07,
        5.3571e-08, 2.9603e-07, 2.8576e-07, 2.0410e-07, 4.0122e-02, 1.3472e-07,
        2.0410e-07, 2.0410e-07, 5.9845e-08, 7.7696e-08, 2.1886e-07, 1.4155e-07,
        8.0262e-08, 2.1871e-07, 6.9271e-08, 7.7696e-08, 1.1407e-07, 1.3472e-07,
        4.8261e-02, 6.5103e-08, 3.0794e-02, 1.4155e-07, 1.3165e-07, 9.5244e-08,
        1.3295e-07, 4.5922e-08, 4.8288e-02, 3.7911e-08, 1.4310e-07, 1.6856e-07,
        1.0477e-07, 1.3604e-07, 2.9603e-07, 1.6795e-07, 1.0303e-07, 1.0758e-07,
        1.1407e-07, 2.5381e-07, 1.0477e-07, 2.0638e-08, 6.2298e-08, 1.2109e-07,
        7.0209e-02, 2.6304e-02, 1.7806e-07, 1.2109e-07, 1.1527e-07, 1.3603e-07,
        2.1886e-07, 1.3913e-07, 1.3472e-07, 2.5381e-07, 2.8576e-07, 1.3165e-07,
        1.1822e-07, 1.6856e-07, 1.8906e-07, 1.1407e-07, 3.3118e-07, 3.3118e-07,
        1.2757e-08, 3.3118e-07, 1.1407e-07, 1.4155e-07, 9.5244e-08, 1.3472e-07,
        1.1140e-08, 5.9816e-08, 2.5381e-07, 1.0303e-07, 2.3176e-07, 1.2109e-07,
        2.1886e-07, 1.2837e-07, 2.3176e-07, 1.3165e-07, 7.6213e-08, 2.2326e-07,
        5.6195e-08, 1.2816e-07, 6.2298e-08, 3.8777e-07, 9.5244e-08, 1.0477e-07,
        1.0758e-07, 1.3664e-07, 2.8043e-02, 1.6980e-07, 3.8693e-02, 1.6856e-07,
        9.5244e-08, 2.5681e-07, 2.1985e-07, 3.8777e-07, 1.4271e-07, 2.5381e-07,
        2.2025e-07, 1.0235e-07, 2.2025e-07, 1.1822e-07, 2.3358e-07, 1.1323e-07,
        1.6856e-07, 1.6126e-07, 2.7191e-07, 7.4172e-08, 6.7183e-08, 2.1975e-07,
        2.1985e-07, 4.3272e-02, 1.8929e-07, 2.5381e-07, 1.1407e-07, 1.8907e-07,
        1.7396e-07, 5.9563e-08, 4.8518e-08, 1.2806e-08, 3.3956e-02, 3.9950e-08,
        2.0410e-07, 1.0544e-07, 1.0437e-07, 1.0233e-01, 4.2476e-02, 9.1900e-08,
        5.5057e-08, 2.1886e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.8310e-07, 2.3207e-07, 1.0892e-06, 2.9994e-07, 3.5709e-07, 2.9994e-07,
        5.0394e-07, 4.9555e-07, 2.7895e-07, 3.7335e-07, 1.2844e-06, 1.0656e-07,
        3.7597e-07, 1.9215e-07, 1.1535e-06, 8.9196e-07, 5.3504e-07, 3.6360e-07,
        5.5953e-07, 2.5574e-07, 6.6939e-07, 1.3410e-06, 9.7036e-07, 2.9757e-07,
        1.0135e-06, 3.8338e-07, 4.6417e-07, 7.2138e-07, 3.3293e-07, 2.4876e-07,
        3.8411e-07, 1.0988e-06, 6.2146e-07, 1.0015e-07, 2.8113e-07, 3.6817e-07,
        6.5648e-07, 5.4545e-07, 2.8787e-08, 9.4497e-07, 2.9819e-07, 1.9999e-07,
        1.0892e-06, 6.3080e-07, 8.2239e-07, 7.1972e-07, 4.0226e-07, 2.2588e-07,
        1.4360e-07, 5.6139e-07, 8.9196e-07, 8.4787e-07, 2.9994e-07, 3.6041e-07,
        1.2263e-06, 5.5477e-07, 4.8439e-07, 3.7553e-07, 6.4793e-07, 2.9819e-07,
        3.6360e-07, 1.8412e-07, 6.8549e-07, 7.4742e-07, 4.9659e-07, 4.7564e-07,
        2.3207e-07, 3.9318e-07, 2.0423e-07, 7.8986e-02, 6.6939e-07, 3.5537e-07,
        1.9891e-07, 2.3207e-07, 2.0257e-07, 1.7308e-02, 5.4194e-07, 4.9811e-07,
        7.4741e-02, 2.5574e-07, 9.2088e-07, 4.6757e-07, 3.8983e-07, 1.2930e-07,
        3.6554e-07, 8.2239e-07, 2.5700e-07, 1.0956e-06, 2.8113e-07, 3.8902e-07,
        3.7259e-07, 8.9321e-07, 3.7335e-07, 8.2239e-07, 7.0409e-07, 5.9254e-07,
        2.8647e-07, 4.2403e-07, 9.5791e-07, 8.0522e-07, 2.8113e-07, 9.0106e-07,
        4.9555e-07, 2.0257e-07, 2.6609e-07, 1.9369e-07, 3.6360e-07, 4.9071e-08,
        6.2217e-07, 2.5687e-07, 1.6516e-07, 4.8316e-07, 6.2146e-07, 8.4787e-07,
        4.0481e-07, 2.1720e-07, 6.4898e-07, 9.7116e-07, 1.2844e-06, 6.7586e-07,
        1.4623e-07, 6.6504e-07, 3.6360e-07, 3.4235e-07, 3.1334e-07, 1.7738e-07,
        3.7866e-07, 4.0002e-07, 1.4428e-07, 9.9747e-07, 8.9196e-07, 2.7681e-07,
        9.2145e-07, 2.3195e-07, 7.1972e-07, 2.1492e-07, 6.4793e-07, 2.2588e-07,
        4.5012e-07, 3.2414e-07, 3.5709e-07, 1.2930e-07, 9.0849e-07, 9.2145e-07,
        1.7738e-07, 9.7036e-07, 2.9994e-07, 4.5332e-07, 1.4856e-07, 3.3293e-07,
        8.0522e-07, 3.9837e-07, 4.9811e-07, 9.2145e-07, 1.4856e-07, 9.2088e-07,
        4.8513e-07, 7.4734e-07, 8.9196e-07, 3.4922e-07, 4.7564e-07, 2.8871e-07,
        2.8871e-07, 5.0626e-07, 2.3195e-07, 3.7259e-07, 2.2588e-07, 9.0106e-07,
        2.1463e-07, 3.6041e-07, 8.7095e-07, 4.0791e-07, 3.2140e-07, 8.0522e-07,
        4.5012e-07, 9.1034e-07, 4.2633e-07, 2.5883e-07, 5.9530e-07, 8.2239e-07,
        8.6556e-07, 1.8412e-07, 2.3580e-07, 2.4651e-07, 2.7829e-07, 2.8122e-07,
        9.8998e-07, 4.4990e-07, 2.2182e-07, 6.2146e-07, 8.2239e-07, 3.6360e-07,
        5.7866e-07, 6.5002e-07, 9.0106e-07, 2.0011e-07, 1.6516e-07, 9.0106e-07,
        1.9288e-07, 6.7397e-07, 4.8439e-07, 8.6785e-07, 2.9994e-07, 4.5012e-07,
        8.1220e-07, 1.2263e-06, 8.2471e-07, 4.1369e-07, 1.7462e-07, 8.2239e-07,
        9.4497e-07, 4.7564e-07, 6.4514e-07, 1.0892e-06, 1.9297e-07, 9.5791e-07,
        3.0511e-07, 5.5774e-07, 4.8439e-07, 4.7633e-07, 5.0583e-07, 4.8439e-07,
        5.5318e-07, 8.4787e-07, 3.3293e-07, 4.3871e-07, 3.5709e-07, 3.0656e-07,
        4.9811e-07, 1.1443e-06, 2.6002e-07, 5.1830e-07, 7.1972e-07, 8.2239e-07,
        2.5574e-07, 2.6374e-07, 2.2588e-07, 2.5645e-07, 2.5655e-07, 5.9117e-07,
        1.0309e-06, 7.5423e-07, 3.7983e-07, 4.5012e-07, 2.2182e-07, 3.6622e-07,
        6.6062e-02, 5.0469e-07, 4.2104e-07, 1.3653e-07, 1.0892e-06, 1.6628e-07,
        5.0583e-07, 4.8439e-07, 5.4194e-07, 3.2400e-07, 2.2925e-07, 2.8787e-08,
        3.2140e-07, 4.7564e-07, 6.8669e-07, 8.0522e-07, 3.6554e-07, 2.5655e-07,
        1.6763e-07, 2.9994e-07, 5.6602e-07, 6.1947e-02, 1.0309e-06, 6.2593e-07,
        5.4435e-07, 2.3794e-07, 3.7553e-07, 6.3531e-07, 2.0257e-07, 5.3504e-07,
        2.0359e-07, 5.8632e-07, 1.3486e-07, 1.0363e-06, 5.0082e-07, 4.9298e-08,
        7.3060e-07, 4.4990e-07, 4.7850e-07, 2.9819e-07, 4.2633e-07, 7.6035e-02,
        1.7738e-07, 5.4783e-07, 6.3531e-07, 2.5645e-07, 4.5012e-07, 5.3504e-07,
        1.8412e-07, 1.0892e-06, 7.3770e-07, 9.4497e-07, 1.7738e-07, 5.1092e-07,
        2.8077e-08, 3.7866e-07, 2.8113e-07, 9.5726e-07, 3.2365e-07, 8.2239e-07,
        2.4011e-07, 2.7895e-07, 1.4856e-07, 2.9757e-07, 3.7866e-07, 1.8412e-07,
        1.0041e-06, 3.6041e-07, 2.5289e-07, 4.2403e-07, 2.9650e-07, 2.3580e-07,
        2.3195e-07, 6.6141e-07, 4.7646e-07, 1.8232e-07, 1.0363e-06, 6.6939e-07,
        6.6141e-07, 5.4126e-02, 2.7895e-07, 6.2593e-07, 1.0355e-06, 2.4651e-07,
        2.3557e-02, 4.2403e-07, 1.0306e-06, 4.6757e-07, 3.7983e-07, 9.2089e-07,
        3.6360e-07, 5.9254e-07, 7.4734e-07, 3.7866e-07, 7.5288e-07, 3.8576e-07,
        3.5709e-07, 9.8799e-08, 5.4194e-07, 6.6313e-07, 5.4482e-07, 2.8113e-07,
        8.2239e-07, 4.6264e-07, 8.0089e-07, 6.0625e-02, 7.1972e-07, 9.2354e-07,
        5.4783e-07, 6.5648e-07, 5.5953e-07, 9.9133e-07, 1.2844e-06, 1.0892e-06,
        4.5936e-07, 5.8863e-08, 1.2312e-07, 2.8113e-07, 1.2996e-07, 6.1706e-07,
        3.9318e-07, 1.0041e-06, 1.8080e-07, 2.9757e-07, 2.5645e-07, 2.8908e-07,
        2.8787e-08, 4.3535e-07, 4.3117e-07, 7.1273e-07, 5.5953e-07, 5.0430e-07,
        5.3504e-07, 2.1498e-07, 7.4734e-07, 1.0309e-06, 1.2844e-06, 6.1596e-07,
        7.4734e-07, 6.2146e-07, 5.3504e-07, 3.7758e-07, 3.3338e-07, 9.7036e-07,
        1.4428e-07, 5.1657e-07, 1.0306e-06, 1.0309e-06, 3.5709e-07, 9.7116e-07,
        4.5012e-07, 5.2075e-07, 2.5687e-07, 6.2146e-07, 1.3410e-06, 2.5574e-07,
        3.7597e-07, 4.6562e-07, 1.4623e-07, 3.7553e-07, 6.6748e-07, 2.0799e-07,
        2.2588e-07, 4.3871e-07, 9.8798e-08, 4.0356e-07, 2.2588e-07, 7.4734e-07,
        6.6939e-07, 5.6658e-07, 1.4856e-07, 3.9528e-07, 5.6602e-07, 1.3234e-02,
        9.6324e-07, 4.5012e-07, 7.3463e-02, 3.7022e-07, 5.4194e-07, 8.2239e-07,
        1.8232e-07, 2.8113e-07, 4.7720e-07, 5.6139e-07, 8.2239e-07, 7.8083e-07,
        5.4650e-07, 4.3374e-07, 3.7597e-07, 3.1344e-07, 4.9555e-07, 3.5888e-07,
        1.3486e-07, 3.8576e-07, 4.5936e-07, 6.2146e-07, 6.2217e-07, 4.3374e-07,
        4.9811e-07, 2.1492e-07, 9.1034e-07, 2.2588e-07, 5.1092e-07, 8.2239e-07,
        5.4435e-07, 4.5012e-07, 1.7374e-07, 3.9456e-07, 4.3374e-07, 3.9047e-07,
        1.2263e-06, 2.3195e-07, 9.0849e-07, 1.8888e-07, 4.6156e-07, 1.5813e-07,
        1.3655e-06, 4.5012e-07, 1.8412e-07, 5.9254e-07, 3.4631e-07, 6.2146e-07,
        4.3871e-07, 3.7335e-07, 5.3625e-07, 1.2996e-07, 9.7036e-07, 1.2844e-06,
        6.2217e-07, 9.2145e-07, 3.4548e-02, 6.8361e-07, 5.4194e-07, 7.3770e-07,
        6.6650e-02, 6.2915e-07, 1.2844e-06, 2.5883e-07, 7.4734e-07, 5.4193e-07,
        1.3655e-06, 2.8109e-07, 5.5864e-07, 2.6258e-07, 2.9994e-07, 4.5012e-07,
        5.4194e-07, 1.6763e-07, 8.0089e-07, 5.1092e-07, 2.3195e-07, 5.4194e-07,
        4.3212e-07, 4.5936e-07, 2.3207e-07, 4.9811e-07, 4.3212e-07, 5.2630e-07,
        4.6417e-07, 5.5416e-07, 1.2996e-07, 3.6360e-07, 5.7856e-07, 2.4011e-07,
        2.9569e-07, 1.2263e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.7693e-07, 1.1587e-06, 7.9555e-07, 2.0774e-07, 3.3877e-07, 4.2789e-07,
        8.5701e-07, 3.2525e-07, 4.7854e-07, 4.9498e-07, 4.3060e-07, 2.9641e-07,
        2.3224e-07, 5.7381e-07, 4.7854e-07, 2.9181e-07, 2.6705e-07, 4.7537e-07,
        6.4418e-07, 1.0306e-06, 4.8046e-07, 1.1655e-06, 7.7415e-07, 5.3019e-07,
        4.4382e-07, 7.8249e-07, 1.0532e-06, 2.5088e-07, 2.9181e-07, 1.1040e-01,
        1.1724e-01, 1.1587e-06, 7.2188e-07, 8.8326e-07, 6.8377e-07, 2.0397e-07,
        6.0948e-07, 7.2967e-02, 4.4832e-07, 2.7192e-07, 6.8163e-07, 4.4897e-07,
        2.0814e-07, 1.8420e-07, 6.3753e-07, 6.8377e-07, 7.2964e-07, 4.0242e-07,
        1.4775e-07, 4.9741e-07, 4.0578e-07, 2.6705e-07, 2.0776e-07, 1.1587e-06,
        8.2421e-07, 1.7561e-07, 6.8163e-07, 2.9940e-07, 1.9373e-07, 8.8326e-07,
        2.4667e-07, 4.4382e-07, 3.0664e-07, 1.1016e-06, 1.3684e-07, 2.2704e-07,
        5.6180e-07, 9.8689e-07, 2.9181e-07, 4.2840e-07, 2.4234e-07, 6.8721e-07,
        3.6892e-07, 6.8163e-07, 8.2421e-07, 9.6037e-07, 4.1445e-07, 5.5261e-07,
        7.1276e-07, 2.4667e-07, 2.5088e-07, 6.9056e-07, 7.9494e-07, 2.8544e-07,
        2.9834e-07, 5.3019e-07, 8.4264e-07, 6.8163e-07, 2.5088e-07, 2.4510e-07,
        2.9874e-07, 1.1399e-06, 2.4672e-07, 1.1137e-06, 5.3525e-07, 1.7853e-07,
        2.7258e-07, 5.9682e-07, 3.2009e-07, 2.6567e-07, 2.7192e-07, 3.7693e-07,
        5.9714e-07, 5.8895e-07, 9.6515e-02, 8.7491e-07, 3.0525e-07, 3.1901e-07,
        2.9181e-07, 4.7536e-07, 5.5966e-07, 6.8163e-07, 1.6329e-07, 5.5143e-07,
        7.6698e-07, 3.9335e-07, 5.7896e-07, 4.8046e-07, 1.1293e-06, 7.2663e-07,
        7.2414e-07, 7.9494e-07, 4.5002e-07, 8.2421e-07, 9.6520e-07, 4.9673e-07,
        7.3826e-02, 3.6086e-07, 1.1718e-07, 6.8887e-07, 5.0546e-07, 8.7170e-07,
        2.7192e-07, 4.5840e-07, 6.8721e-07, 8.3092e-07, 5.8682e-07, 5.9682e-07,
        9.2595e-07, 1.7561e-07, 2.0789e-07, 5.7886e-07, 7.2964e-07, 1.4775e-07,
        7.6402e-07, 9.0147e-07, 4.7537e-07, 5.0546e-07, 3.1511e-07, 1.4775e-07,
        9.4467e-07, 5.7381e-07, 7.0307e-07, 2.7280e-07, 4.0578e-07, 2.4833e-07,
        6.4418e-07, 1.5709e-07, 1.0666e-06, 2.5777e-07, 3.2360e-07, 9.2513e-07,
        4.5920e-07, 1.1587e-06, 1.4096e-06, 7.6698e-07, 2.4667e-07, 7.5416e-07,
        1.7805e-07, 6.6136e-07, 8.4861e-07, 6.7041e-07, 7.6698e-07, 5.3808e-07,
        1.0306e-06, 2.1641e-07, 2.3454e-07, 2.5088e-07, 4.5713e-07, 5.3664e-07,
        5.4081e-07, 2.6567e-07, 5.0651e-07, 5.0546e-07, 5.2948e-07, 4.3041e-07,
        3.2487e-07, 6.6133e-07, 1.7561e-07, 2.1982e-07, 3.7693e-07, 4.9821e-07,
        5.9714e-07, 4.4820e-07, 3.4152e-07, 3.0958e-07, 5.2830e-07, 6.8752e-07,
        4.5455e-07, 2.9181e-07, 5.3808e-07, 6.1447e-07, 1.7619e-07, 2.1982e-07,
        9.8514e-08, 3.4474e-07, 4.7854e-07, 3.9627e-07, 4.5309e-07, 7.3340e-07,
        5.8823e-07, 2.4672e-07, 3.4474e-07, 2.8737e-07, 8.2421e-07, 2.1982e-07,
        2.5932e-07, 4.0349e-07, 4.7536e-07, 4.1632e-07, 9.8144e-07, 5.4821e-07,
        5.2948e-07, 5.5261e-07, 1.1016e-06, 3.3877e-07, 5.8378e-07, 2.3895e-07,
        1.1238e-06, 2.8737e-07, 7.8727e-07, 2.4370e-07, 9.8043e-07, 5.2427e-07,
        5.5966e-07, 1.7805e-07, 8.3092e-07, 4.7910e-07, 2.9874e-07, 6.0791e-07,
        1.0183e-06, 1.1853e-01, 4.5309e-07, 3.9627e-07, 1.0600e-06, 6.7455e-07,
        2.7717e-07, 5.0734e-07, 4.7910e-07, 2.5090e-07, 4.6389e-07, 3.8877e-07,
        1.0306e-06, 2.4672e-07, 6.7041e-07, 4.4351e-07, 2.4672e-07, 5.7116e-07,
        5.2241e-07, 2.9874e-07, 5.0517e-07, 1.1016e-06, 2.5088e-07, 1.2551e-06,
        6.9056e-07, 5.1556e-07, 3.6391e-07, 9.8514e-08, 1.1569e-06, 9.6155e-02,
        3.0525e-07, 1.1925e-06, 5.0751e-07, 2.0789e-07, 4.0085e-07, 1.4205e-07,
        4.3618e-07, 9.8514e-08, 1.3664e-06, 2.4370e-07, 1.0614e-06, 9.8514e-08,
        6.2364e-08, 8.7368e-07, 8.2421e-07, 7.2963e-07, 7.0216e-07, 1.5709e-07,
        3.2360e-07, 2.3955e-07, 1.6005e-07, 3.4152e-07, 1.9374e-07, 3.6086e-07,
        7.2142e-07, 1.7620e-07, 1.7805e-07, 5.7896e-07, 5.7381e-07, 5.7896e-07,
        2.5088e-07, 4.5102e-07, 2.3132e-07, 2.7297e-07, 4.2840e-07, 7.0803e-07,
        7.7433e-07, 2.7192e-07, 5.0517e-07, 1.1587e-06, 1.0083e-06, 6.1329e-07,
        2.7192e-07, 1.4775e-07, 5.6525e-07, 8.2421e-07, 4.7910e-07, 1.7805e-07,
        2.6295e-07, 3.4165e-07, 3.4474e-07, 1.0666e-06, 1.1718e-07, 9.3642e-07,
        8.8326e-07, 4.0578e-07, 2.7611e-07, 6.8752e-07, 1.7863e-07, 5.6525e-07,
        1.0511e-07, 6.0791e-07, 7.2142e-07, 1.1587e-06, 1.3028e-06, 2.7258e-07,
        6.8163e-07, 1.1416e-06, 5.2736e-07, 1.1587e-06, 4.5920e-07, 8.6211e-07,
        3.0817e-07, 2.9874e-07, 1.4457e-07, 2.4672e-07, 8.3092e-07, 2.4667e-07,
        5.6818e-07, 7.2990e-07, 6.1273e-08, 5.3019e-07, 1.0666e-06, 1.0532e-06,
        2.4672e-07, 7.2142e-07, 3.7763e-07, 2.7192e-07, 4.6871e-07, 1.7620e-07,
        1.0532e-06, 1.1243e-01, 3.4474e-07, 1.0479e-06, 1.6826e-07, 1.0627e-07,
        2.9328e-07, 4.2840e-07, 5.5966e-07, 1.8420e-07, 7.8231e-07, 5.7886e-07,
        1.4775e-07, 1.1293e-06, 8.0220e-07, 2.5088e-07, 6.7041e-07, 2.8051e-08,
        2.3269e-07, 7.0216e-07, 4.1445e-07, 4.7170e-07, 2.6705e-07, 2.7258e-07,
        3.2553e-07, 5.0751e-07, 7.4290e-07, 5.6315e-07, 3.5419e-07, 2.5777e-07,
        3.1511e-07, 4.6852e-07, 9.8043e-07, 6.2364e-08, 5.5966e-07, 3.8877e-07,
        5.0751e-07, 5.6138e-07, 1.7805e-07, 8.7546e-02, 5.7001e-07, 1.7805e-07,
        4.7170e-07, 4.2840e-07, 3.1410e-07, 3.0525e-07, 2.7192e-07, 5.2241e-07,
        4.6437e-07, 3.1901e-07, 5.3752e-07, 9.8043e-07, 1.1016e-06, 9.0135e-07,
        5.0145e-07, 6.6133e-07, 5.7889e-07, 4.1445e-07, 1.4775e-07, 1.6188e-06,
        5.7886e-07, 2.9874e-07, 2.9181e-07, 1.4750e-06, 9.2513e-07, 5.8378e-07,
        3.1632e-07, 1.7853e-07, 3.3443e-07, 2.7422e-07, 3.8748e-07, 9.5600e-07,
        3.5129e-07, 7.0450e-07, 5.1556e-07, 5.7896e-07, 6.2814e-07, 6.8533e-07,
        2.7659e-07, 2.5058e-07, 1.3664e-06, 2.7258e-07, 1.1587e-06, 2.3214e-06,
        1.1569e-06, 8.2421e-07, 8.2421e-07, 1.1016e-06, 6.8720e-07, 1.8068e-07,
        4.0855e-07, 1.1370e-01, 3.5989e-07, 8.5605e-07, 2.8661e-07, 7.9494e-07,
        1.6753e-07, 5.1260e-07, 7.2663e-07, 1.7070e-01, 6.7041e-07, 5.0546e-07,
        2.4265e-07, 3.2553e-07, 5.2508e-07, 5.5151e-07, 8.3026e-07, 5.1401e-07,
        1.4750e-06, 1.1016e-06, 6.8533e-07, 9.2184e-07, 4.2789e-07, 6.7041e-07,
        4.1445e-07, 5.7885e-07, 3.5419e-07, 2.4265e-07, 1.1942e-06, 3.1747e-07,
        5.7381e-07, 5.6315e-07, 5.5966e-07, 2.3269e-07, 1.6826e-07, 2.4370e-07,
        1.0306e-06, 2.5576e-08, 6.4802e-08, 4.5920e-07, 2.5777e-07, 3.0817e-07,
        7.6029e-07, 8.0379e-07, 2.4510e-07, 1.0898e-07, 5.8170e-07, 8.6211e-07,
        2.9641e-07, 7.2963e-07, 3.9985e-07, 1.1016e-06, 3.9388e-07, 3.5129e-07,
        2.2650e-07, 4.7170e-07, 5.5151e-07, 3.4978e-07, 5.1340e-08, 5.7381e-07,
        9.5954e-02, 3.9335e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.8284e-07, 5.7970e-07, 9.8940e-07, 5.7280e-07, 5.8117e-07, 1.0231e-01,
        1.1223e-06, 1.3917e-01, 3.5720e-07, 4.4193e-07, 9.3455e-07, 5.2951e-07,
        3.2674e-07, 4.9854e-07, 3.7211e-07, 6.5497e-07, 1.1129e-06, 4.1665e-07,
        9.5309e-07, 1.0936e-06, 2.6256e-07, 4.2008e-07, 4.1174e-07, 6.7954e-07,
        8.4052e-07, 3.8385e-07, 8.1265e-07, 5.4519e-07, 6.8313e-07, 6.5497e-07,
        4.6571e-07, 3.9487e-07, 6.7449e-07, 4.0275e-07, 5.1025e-07, 7.2869e-07,
        8.0485e-07, 3.5780e-07, 5.2318e-07, 3.5494e-07, 3.5234e-07, 6.2026e-07,
        1.5121e-07, 1.4593e-06, 3.7211e-07, 8.7182e-07, 4.6163e-07, 2.4056e-07,
        8.5819e-07, 6.6336e-07, 4.7968e-07, 6.7804e-07, 2.3604e-07, 4.8284e-07,
        5.2023e-07, 9.5339e-07, 8.9133e-07, 2.9682e-07, 5.6709e-07, 1.6934e-07,
        5.2318e-07, 6.7449e-07, 4.6571e-07, 6.0916e-07, 1.1538e-06, 3.6614e-07,
        4.2178e-07, 2.6533e-07, 4.8733e-07, 3.9904e-07, 9.3588e-07, 2.2322e-07,
        3.7119e-07, 4.7968e-07, 5.6560e-07, 5.2318e-07, 3.5172e-07, 4.6794e-07,
        8.9410e-07, 6.4912e-07, 9.1755e-08, 1.1119e-06, 9.7045e-07, 9.3588e-07,
        8.3610e-07, 2.2322e-07, 3.3055e-07, 4.7968e-07, 1.0944e-06, 1.0394e-06,
        4.7968e-07, 9.7045e-07, 5.7027e-07, 4.3005e-07, 5.7280e-07, 3.7119e-07,
        4.8733e-07, 6.5717e-07, 3.7495e-07, 6.1645e-07, 8.9285e-07, 5.7560e-07,
        4.7968e-07, 6.8068e-07, 4.3136e-07, 9.6650e-07, 1.0936e-06, 6.5497e-07,
        3.3440e-07, 6.4337e-07, 4.3758e-07, 6.5717e-07, 4.3136e-07, 6.4571e-07,
        6.9312e-02, 7.7862e-07, 9.3191e-07, 2.3359e-07, 2.7606e-07, 9.4432e-07,
        1.2789e-06, 4.6844e-07, 4.3557e-07, 2.3359e-07, 7.2939e-07, 3.4037e-07,
        8.7182e-07, 8.8818e-07, 4.2178e-07, 9.3604e-07, 2.1049e-07, 6.2026e-07,
        1.2789e-06, 2.7883e-07, 6.4314e-07, 6.4570e-07, 1.4156e-06, 6.4762e-07,
        6.9757e-07, 3.8959e-07, 8.0464e-07, 6.6009e-07, 9.3167e-07, 5.1580e-07,
        3.6259e-07, 8.0485e-07, 2.3604e-07, 9.4525e-07, 5.4736e-07, 3.6614e-07,
        3.8340e-07, 4.2178e-07, 1.3105e-07, 3.5720e-07, 5.2023e-07, 1.6114e-06,
        3.8113e-07, 1.2575e-01, 6.7737e-07, 4.7968e-07, 4.7968e-07, 9.8989e-07,
        2.6533e-07, 1.1263e-06, 6.6449e-07, 9.1755e-08, 9.3604e-07, 6.8341e-07,
        4.7968e-07, 9.4525e-07, 8.0464e-07, 3.0100e-07, 3.8385e-07, 6.6009e-07,
        2.2040e-07, 4.7968e-07, 1.0559e-06, 8.0720e-07, 6.2371e-07, 1.6950e-07,
        6.8341e-07, 6.8341e-07, 8.3035e-07, 6.8313e-07, 1.1310e-06, 1.0532e-06,
        9.3050e-07, 6.1897e-07, 7.5295e-07, 7.6258e-07, 3.9904e-07, 2.7204e-07,
        3.0100e-07, 9.4525e-07, 8.0485e-07, 6.3809e-07, 6.3196e-07, 8.6346e-07,
        6.1741e-07, 7.6996e-07, 9.3588e-07, 4.1665e-07, 4.0275e-07, 5.1033e-07,
        6.6844e-07, 1.8061e-07, 3.3196e-07, 5.5970e-07, 3.9632e-07, 1.4544e-06,
        4.3005e-07, 1.1119e-06, 7.5295e-07, 6.0425e-07, 4.2178e-07, 1.1263e-06,
        9.3604e-07, 6.0746e-07, 2.0087e-07, 5.0982e-07, 8.3012e-07, 4.9427e-07,
        5.5970e-07, 9.5192e-07, 6.6009e-07, 2.0537e-07, 6.2371e-07, 6.4571e-07,
        9.3116e-07, 2.3442e-07, 9.3588e-07, 1.3415e-06, 9.5029e-07, 5.6709e-07,
        8.4793e-07, 6.1897e-07, 1.0995e-01, 4.7968e-07, 3.5720e-07, 9.5339e-07,
        5.7280e-07, 2.2322e-07, 1.0759e-06, 5.8378e-07, 4.6571e-07, 5.8378e-07,
        1.2795e-06, 1.6934e-07, 1.0099e-06, 6.7323e-08, 5.2438e-07, 2.9184e-07,
        1.1517e-06, 2.4965e-07, 1.4156e-06, 8.2567e-07, 2.9184e-07, 4.1665e-07,
        2.8614e-07, 3.9632e-07, 5.0561e-07, 5.8378e-07, 3.6955e-07, 8.0464e-07,
        5.5238e-07, 6.1855e-07, 3.4943e-07, 9.7435e-07, 3.2634e-07, 1.3089e-06,
        3.3463e-07, 5.4482e-07, 2.5950e-07, 8.3035e-07, 3.9904e-07, 6.8341e-07,
        1.9274e-07, 8.0476e-07, 3.8537e-07, 8.9046e-07, 5.6709e-07, 5.3585e-07,
        3.5720e-07, 5.8993e-07, 6.6009e-07, 3.6408e-07, 9.0161e-07, 1.0339e-06,
        2.2040e-07, 2.5430e-07, 1.2789e-06, 1.1453e-06, 6.1855e-07, 8.8621e-07,
        3.9243e-07, 3.0941e-07, 4.7502e-07, 1.0999e-06, 5.8378e-07, 4.1665e-07,
        6.9196e-07, 7.9471e-08, 1.3583e-06, 2.0087e-07, 1.3434e-06, 3.5764e-07,
        7.1068e-07, 9.1755e-08, 5.5970e-07, 9.5339e-07, 5.3585e-07, 2.9682e-07,
        5.4736e-07, 5.4062e-07, 1.2308e-06, 8.4052e-07, 5.1353e-07, 4.2045e-07,
        2.4018e-07, 4.7478e-07, 4.6396e-07, 4.8733e-07, 6.1571e-07, 1.1899e-06,
        8.4793e-07, 7.5295e-07, 1.3417e-06, 2.9724e-07, 3.9043e-07, 8.9410e-07,
        3.5720e-07, 6.7792e-07, 5.4848e-07, 1.0061e-06, 3.6400e-08, 1.7411e-06,
        8.4239e-07, 1.1517e-06, 1.2543e-06, 5.9590e-07, 9.5542e-07, 6.0548e-07,
        6.8605e-07, 5.4691e-07, 4.1665e-07, 4.8284e-07, 1.2789e-06, 1.0530e-06,
        1.2308e-06, 3.2654e-07, 2.9184e-07, 1.0576e-06, 9.5339e-07, 2.9716e-07,
        1.2217e-06, 3.6167e-07, 9.6947e-07, 5.5970e-07, 5.5880e-07, 9.6947e-07,
        5.1270e-07, 7.5404e-07, 7.6996e-07, 2.1671e-07, 2.2040e-07, 1.1743e-06,
        4.3557e-07, 6.0746e-07, 2.5285e-07, 7.6674e-07, 5.0013e-07, 7.0556e-07,
        9.5339e-07, 1.2308e-01, 2.8148e-07, 9.7045e-07, 8.5837e-07, 9.3004e-07,
        3.3673e-07, 4.3136e-07, 7.2869e-07, 2.1835e-07, 8.0345e-07, 1.3583e-06,
        3.6167e-07, 1.4593e-06, 6.2371e-07, 1.3973e-06, 4.3136e-07, 5.9043e-07,
        1.0691e-06, 3.9632e-07, 7.5295e-07, 5.1580e-07, 1.4499e-07, 7.5295e-07,
        6.1645e-07, 5.0561e-07, 9.4525e-07, 4.5102e-07, 6.1571e-07, 8.4659e-07,
        8.4511e-07, 8.0978e-07, 4.5369e-07, 6.2371e-07, 2.5643e-07, 2.2040e-07,
        4.9427e-07, 3.4943e-07, 9.5403e-07, 9.0161e-07, 4.8389e-07, 3.4943e-07,
        8.8621e-07, 5.4482e-07, 3.4943e-07, 6.4218e-07, 6.2562e-07, 1.0212e-06,
        7.8057e-07, 6.8605e-07, 6.4912e-07, 9.8989e-07, 7.5404e-07, 1.0301e-06,
        1.8765e-07, 8.6569e-07, 9.4525e-07, 6.9517e-07, 6.5497e-07, 5.6709e-07,
        9.4835e-07, 1.2308e-06, 2.7204e-07, 1.0532e-06, 1.3025e-07, 1.4544e-06,
        1.1743e-06, 5.6586e-07, 6.3809e-07, 3.5494e-07, 4.7912e-07, 3.0100e-07,
        8.4033e-08, 6.4337e-07, 1.0959e-06, 7.6885e-07, 4.0275e-07, 4.8733e-07,
        2.5430e-07, 8.6569e-07, 1.0141e-06, 1.0312e-06, 4.6654e-07, 4.6571e-07,
        1.0193e-06, 5.5970e-07, 6.3809e-07, 5.2318e-07, 4.7968e-07, 6.1645e-07,
        4.3557e-07, 2.1835e-07, 1.4801e-06, 8.4511e-07, 1.1115e-06, 5.4227e-08,
        3.6402e-07, 6.3196e-07, 4.3005e-07, 1.3890e-07, 8.0464e-07, 4.3554e-07,
        3.1676e-07, 6.8313e-07, 3.5494e-07, 2.3808e-07, 5.4736e-07, 1.4512e-07,
        6.8341e-07, 1.5587e-06, 7.0562e-02, 1.8408e-01, 2.6654e-07, 1.9096e-06,
        2.0934e-07, 3.4037e-07, 6.9196e-07, 1.7176e-06, 5.5970e-07, 3.5657e-07,
        1.7395e-01, 1.4879e-01, 1.3434e-06, 1.4241e-06, 9.7045e-07, 1.0193e-06,
        1.4593e-06, 2.1049e-07, 6.1855e-07, 5.3733e-07, 8.0345e-07, 3.7211e-07,
        6.7640e-07, 1.1743e-06, 4.1665e-07, 2.4018e-07, 7.2552e-07, 4.1320e-07,
        3.5325e-07, 1.0759e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.1525e-07, 7.2483e-07, 2.1525e-07, 1.6041e-06, 6.4731e-07, 7.4709e-07,
        6.4307e-07, 6.7728e-07, 2.6380e-07, 5.8413e-07, 1.8193e-06, 1.7650e-06,
        1.3247e-06, 1.6774e-06, 1.4019e-06, 3.3171e-07, 4.4827e-07, 6.3249e-07,
        1.6120e-06, 1.3682e-06, 2.7526e-06, 8.8565e-07, 7.5705e-07, 3.9226e-07,
        8.7355e-07, 2.8293e-07, 9.9560e-07, 5.3669e-07, 1.4554e-06, 4.4850e-07,
        7.3493e-07, 1.3333e-06, 1.7650e-06, 1.8654e-06, 8.5880e-07, 1.0172e-01,
        3.9817e-07, 2.7705e-07, 2.2441e-06, 7.4198e-07, 3.3171e-07, 5.5407e-07,
        9.0810e-08, 6.4594e-07, 5.1148e-07, 2.3111e-07, 1.0558e-06, 1.1392e-06,
        1.6037e-06, 1.8414e-06, 8.8179e-07, 4.9216e-07, 1.3527e-06, 1.1518e-06,
        4.8216e-07, 6.4322e-07, 1.8654e-06, 5.7387e-07, 3.9453e-07, 3.2371e-07,
        1.4505e-06, 9.3503e-07, 5.1148e-07, 9.0558e-07, 7.8671e-07, 5.1359e-07,
        7.9740e-07, 4.7289e-07, 9.2184e-07, 1.0405e-06, 1.6084e-06, 6.9136e-07,
        2.1619e-06, 3.7114e-07, 1.4192e-01, 1.8696e-07, 1.9520e-06, 1.8089e-01,
        5.1359e-07, 1.1404e-06, 3.7435e-07, 9.6953e-07, 8.4213e-07, 6.9413e-07,
        9.8602e-07, 9.5236e-07, 8.5131e-07, 1.7650e-06, 1.3247e-06, 5.5886e-07,
        1.5879e-07, 9.9467e-07, 7.2345e-02, 1.0075e-06, 6.1203e-07, 1.9154e-07,
        5.5679e-07, 5.4444e-07, 1.3589e-06, 4.9871e-07, 3.4218e-07, 1.3023e-06,
        1.0753e-06, 1.7755e-06, 1.3333e-06, 1.8654e-06, 4.8320e-02, 1.1744e-06,
        4.2316e-07, 6.9413e-07, 1.2608e-06, 1.6041e-06, 2.1235e-06, 1.9104e-06,
        2.3111e-07, 4.8991e-07, 1.0387e-06, 6.3178e-07, 5.3302e-02, 5.2473e-07,
        1.0938e-06, 5.8413e-07, 1.7303e-06, 6.4377e-02, 5.8102e-07, 2.8740e-07,
        1.3023e-06, 8.4702e-07, 7.6373e-07, 1.1550e-07, 3.8464e-07, 7.9150e-07,
        1.1980e-06, 5.2582e-07, 1.4607e-06, 9.6658e-07, 6.9413e-07, 4.8263e-07,
        5.9945e-02, 8.0726e-07, 8.8565e-02, 7.9759e-07, 1.5879e-07, 6.9774e-07,
        9.4388e-07, 4.5535e-07, 1.1908e-06, 8.7140e-07, 1.0067e-06, 7.7658e-07,
        5.3695e-07, 3.0202e-07, 6.8937e-07, 1.6041e-06, 4.7289e-07, 8.0382e-07,
        2.0357e-06, 1.2906e-06, 1.2571e-06, 3.3503e-07, 1.7197e-06, 4.5431e-07,
        4.9348e-02, 9.0843e-07, 1.3022e-06, 1.7650e-06, 8.2205e-07, 6.9595e-07,
        5.0310e-07, 6.2086e-07, 3.2508e-07, 2.8293e-07, 1.5871e-06, 1.0956e-06,
        2.2430e-06, 1.1260e-06, 8.4565e-07, 5.8173e-07, 5.1630e-07, 1.4692e-06,
        1.9547e-07, 4.2493e-07, 4.1025e-07, 7.9858e-07, 2.4767e-07, 1.6612e-06,
        1.0630e-06, 9.5236e-07, 7.0359e-07, 1.7406e-06, 2.0110e-06, 1.3023e-06,
        5.9253e-07, 8.7182e-07, 9.2184e-07, 5.4489e-07, 1.5879e-07, 1.2849e-06,
        5.5407e-07, 5.3695e-07, 1.4655e-06, 8.7457e-07, 9.0258e-07, 7.3571e-07,
        3.3260e-07, 1.4557e-06, 1.6038e-06, 7.8257e-07, 1.6285e-06, 8.9610e-07,
        9.4831e-07, 1.2606e-06, 3.7726e-07, 4.5123e-07, 1.6765e-06, 1.4690e-06,
        1.6041e-06, 7.6024e-07, 1.0908e-06, 7.9942e-07, 1.4361e-06, 1.9203e-06,
        1.4380e-06, 9.4388e-07, 4.5321e-07, 5.8413e-07, 1.6830e-06, 3.7435e-07,
        1.1147e-06, 1.1980e-06, 1.6057e-06, 1.6474e-06, 7.9150e-07, 1.0168e-06,
        6.7668e-07, 1.4221e-06, 1.3851e-06, 9.8602e-07, 1.3440e-06, 6.3554e-02,
        3.7727e-07, 3.7841e-07, 1.3682e-06, 1.6084e-06, 2.1161e-06, 1.3249e-06,
        2.6380e-07, 4.6472e-07, 1.0908e-06, 2.9761e-06, 1.4581e-06, 8.3806e-07,
        1.2526e-06, 1.3589e-06, 8.9610e-07, 1.1716e-06, 8.4214e-02, 5.7998e-07,
        1.1772e-06, 5.6722e-07, 1.7650e-06, 1.7723e-06, 2.7247e-06, 1.5879e-07,
        1.1406e-06, 2.7039e-07, 3.2371e-07, 7.3571e-07, 4.3612e-07, 7.7402e-07,
        8.0038e-07, 2.1235e-06, 2.0357e-06, 1.8746e-06, 3.7469e-07, 2.2441e-06,
        2.0110e-06, 1.5081e-06, 4.9271e-07, 3.7435e-07, 9.0922e-07, 1.1600e-06,
        3.9378e-07, 1.1436e-06, 2.4054e-07, 1.3514e-06, 1.2910e-06, 1.5878e-06,
        1.0217e-06, 1.0236e-06, 6.4731e-07, 3.9453e-07, 1.0405e-06, 8.0137e-07,
        1.8696e-07, 1.3247e-06, 7.9170e-07, 8.3074e-07, 4.4679e-07, 1.0275e-06,
        7.8672e-07, 7.0272e-07, 1.1550e-07, 3.4196e-07, 2.9027e-07, 9.6527e-07,
        9.4388e-07, 8.0725e-07, 7.1634e-07, 1.8339e-06, 5.1174e-07, 3.9445e-07,
        6.9354e-07, 2.9027e-07, 7.4709e-07, 1.2091e-06, 1.0395e-06, 6.2047e-07,
        6.4731e-07, 1.0829e-06, 2.2441e-06, 6.9413e-07, 8.8585e-07, 2.1235e-06,
        9.5064e-07, 2.1324e-07, 1.0286e-06, 1.4678e-06, 1.6612e-06, 9.3615e-06,
        3.2625e-07, 7.4662e-07, 1.4560e-06, 1.7650e-06, 1.0817e-06, 8.4730e-07,
        1.2507e-06, 2.9027e-07, 5.7608e-07, 1.4406e-06, 6.4162e-07, 9.5762e-07,
        1.2849e-06, 3.3025e-07, 9.4831e-07, 7.4709e-07, 9.6135e-07, 1.9659e-06,
        5.6454e-07, 1.0984e-06, 1.1690e-06, 2.0068e-06, 3.0686e-07, 5.5049e-07,
        1.1744e-06, 1.9203e-06, 2.0100e-06, 8.0725e-07, 8.8871e-07, 1.2835e-06,
        1.1436e-06, 1.4380e-06, 8.7504e-07, 7.0675e-07, 8.9683e-07, 2.7706e-07,
        3.7841e-07, 1.7650e-06, 1.0126e-06, 6.4983e-07, 1.3527e-06, 9.6659e-07,
        8.4243e-02, 5.7989e-07, 6.1648e-07, 2.7706e-07, 1.1175e-06, 7.7182e-02,
        3.7841e-07, 3.6555e-07, 8.2416e-07, 1.0846e-06, 1.7650e-06, 6.2383e-07,
        5.8413e-07, 6.2383e-07, 1.7650e-06, 1.7650e-06, 1.8654e-06, 1.5912e-06,
        7.7466e-07, 1.0534e-06, 5.1148e-07, 2.2092e-06, 5.5663e-07, 2.2574e-06,
        4.5181e-08, 8.7140e-07, 1.2507e-06, 7.3571e-07, 9.4388e-07, 7.0675e-07,
        9.0737e-07, 8.7457e-07, 9.6659e-07, 4.4679e-07, 6.4307e-07, 8.3348e-07,
        6.1226e-07, 6.1226e-07, 1.0908e-06, 2.8307e-07, 1.0236e-06, 4.9746e-07,
        1.5248e-07, 1.0129e-06, 9.6659e-07, 6.8901e-07, 8.5880e-07, 1.7406e-06,
        3.0464e-06, 2.0587e-06, 4.2316e-07, 1.4179e-06, 3.0686e-07, 1.6202e-07,
        1.2229e-01, 7.5294e-07, 6.7085e-07, 8.4565e-07, 4.9746e-07, 1.0534e-06,
        1.6474e-06, 2.0062e-06, 5.3131e-07, 4.1257e-07, 1.9203e-06, 3.9129e-07,
        4.1994e-07, 6.4983e-07, 2.1124e-01, 1.2123e-06, 9.6335e-07, 5.1630e-07,
        4.2101e-07, 6.8281e-07, 1.2424e-06, 5.4005e-07, 2.0225e-07, 1.2190e-06,
        6.9354e-07, 5.1989e-07, 1.3023e-06, 1.0817e-06, 1.0276e-06, 9.3813e-07,
        1.3209e-01, 8.3900e-07, 1.1450e-06, 1.5879e-07, 7.5429e-07, 1.1600e-06,
        6.2960e-07, 7.9150e-07, 2.1525e-07, 1.1608e-06, 1.3682e-06, 1.5190e-06,
        1.7208e-07, 5.7608e-07, 1.6612e-06, 7.9858e-07, 6.9413e-07, 1.6474e-06,
        3.7089e-07, 2.8293e-07, 1.4019e-06, 5.9387e-07, 1.2986e-06, 1.2849e-06,
        6.7668e-07, 3.7841e-07, 2.6380e-07, 2.5840e-06, 5.3669e-07, 5.4443e-07,
        7.7843e-07, 4.9854e-07, 1.2401e-06, 2.2321e-07, 5.8385e-07, 1.5768e-06,
        9.6135e-07, 1.2712e-06, 1.5879e-07, 1.6041e-06, 1.0159e-06, 6.1226e-07,
        9.0123e-07, 5.3695e-07, 8.1908e-07, 2.4224e-07, 1.8973e-06, 6.6927e-07,
        1.0993e-06, 8.7140e-07, 1.7389e-06, 7.4709e-07, 1.1878e-07, 5.4444e-07,
        2.4224e-07, 6.9136e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.5940e-07, 1.4593e-06, 1.3131e-06, 1.2285e-01, 1.5109e-06, 7.7743e-07,
        7.6188e-08, 9.5937e-07, 5.2020e-07, 6.3127e-07, 3.6375e-06, 1.7245e-06,
        1.5665e-06, 4.5148e-07, 1.6744e-06, 3.1522e-07, 3.1818e-07, 1.3581e-06,
        4.7886e-07, 3.0061e-07, 1.2415e-06, 7.5470e-07, 6.6548e-07, 6.2614e-07,
        5.8338e-07, 7.4851e-07, 7.2521e-07, 8.8799e-07, 5.3526e-07, 3.2704e-07,
        1.3036e-06, 1.0333e-06, 5.1868e-07, 8.4584e-07, 1.8043e-01, 6.6548e-07,
        7.5470e-07, 5.5367e-07, 1.1211e-06, 7.2521e-07, 5.3836e-07, 3.0647e-07,
        8.4897e-07, 8.5515e-07, 7.0144e-07, 1.3036e-06, 1.2940e-06, 5.3527e-07,
        8.2357e-07, 8.3029e-07, 6.8830e-07, 7.8528e-07, 4.6935e-07, 8.1546e-07,
        3.2409e-07, 2.3018e-07, 6.7904e-07, 3.9118e-07, 8.5067e-07, 1.0754e-06,
        8.3870e-07, 5.3491e-07, 8.7837e-07, 5.0670e-07, 1.1052e-06, 6.7513e-07,
        9.0194e-02, 6.6548e-07, 8.6391e-07, 6.6658e-07, 9.8366e-07, 5.2563e-07,
        7.9589e-02, 1.1504e-06, 3.3672e-07, 8.9037e-07, 8.8898e-07, 1.6627e-06,
        8.0385e-02, 1.3689e-06, 1.3131e-06, 9.9517e-07, 5.7971e-07, 2.0072e-07,
        8.7274e-07, 9.8843e-07, 7.1417e-07, 3.5838e-07, 3.6857e-07, 3.0061e-07,
        7.5566e-07, 1.4927e-06, 1.4222e-06, 7.1225e-07, 1.7231e-06, 4.4871e-07,
        1.0549e-06, 5.2099e-07, 6.5645e-07, 1.4521e-06, 8.1005e-07, 8.9635e-07,
        1.0407e-06, 5.1328e-07, 5.1970e-07, 1.8341e-07, 4.7886e-07, 7.0389e-07,
        4.7523e-07, 7.1529e-07, 1.2543e-06, 1.8341e-07, 5.7216e-07, 3.0061e-07,
        1.7231e-06, 3.2409e-07, 7.7496e-07, 5.1479e-07, 1.5383e-06, 2.8571e-07,
        1.2569e-06, 3.2704e-07, 1.0320e-06, 7.9858e-07, 8.8910e-07, 1.9670e-07,
        5.0115e-07, 3.9839e-07, 1.0047e-06, 2.6581e-07, 4.6769e-07, 7.0890e-07,
        9.6748e-07, 7.5034e-07, 9.2203e-07, 1.2980e-06, 7.9858e-07, 5.2099e-07,
        8.9474e-07, 8.4584e-07, 9.8843e-07, 7.5470e-07, 2.7384e-07, 1.1384e-06,
        8.7802e-07, 5.1868e-07, 5.3366e-07, 1.0265e-06, 1.0557e-06, 8.7837e-07,
        1.8341e-07, 7.0566e-07, 6.3106e-07, 1.4476e-06, 2.8511e-07, 1.3036e-06,
        1.0320e-06, 5.8742e-07, 1.8341e-07, 8.9870e-07, 1.8835e-07, 5.4360e-07,
        5.4408e-07, 3.1100e-07, 7.2373e-07, 1.8673e-06, 7.0216e-07, 1.2563e-07,
        7.1225e-07, 6.6478e-07, 1.1211e-06, 1.5433e-06, 7.5035e-07, 9.9500e-07,
        1.2031e-06, 3.6480e-07, 1.8668e-06, 5.2333e-07, 8.8910e-07, 8.1582e-07,
        5.0413e-07, 1.0889e-06, 7.2749e-07, 8.7920e-07, 8.4584e-07, 4.6033e-07,
        1.2184e-06, 5.5445e-07, 3.1129e-07, 6.8017e-02, 5.9190e-07, 6.2465e-07,
        1.2204e-06, 4.3794e-07, 2.6301e-07, 5.4129e-07, 2.8511e-07, 9.8843e-07,
        1.5489e-06, 1.3760e-06, 1.0320e-06, 8.1546e-07, 9.7040e-07, 1.5201e-06,
        1.1539e-07, 7.2749e-07, 5.6257e-08, 4.6742e-07, 1.1211e-06, 1.0889e-06,
        8.4584e-07, 6.0099e-07, 1.6627e-06, 6.9328e-07, 8.1404e-02, 4.5209e-07,
        1.4906e-06, 1.1711e-06, 5.4360e-07, 1.0754e-06, 1.1499e-06, 6.3037e-07,
        6.0640e-07, 8.9653e-02, 6.5606e-07, 7.3523e-07, 6.2449e-07, 4.4316e-07,
        1.1106e-01, 7.2749e-07, 1.4400e-01, 6.5556e-07, 1.1826e-06, 4.9694e-02,
        5.8963e-07, 1.4661e-06, 1.1959e-06, 7.5034e-07, 3.6437e-07, 7.6285e-02,
        8.7941e-07, 7.5945e-07, 7.7743e-07, 9.4167e-07, 1.1513e-06, 8.1546e-07,
        9.5188e-07, 8.1323e-02, 5.9190e-07, 9.9959e-07, 3.0061e-07, 7.7743e-07,
        1.9685e-06, 8.4584e-07, 6.3355e-07, 1.7261e-06, 2.1058e-07, 1.7206e-06,
        7.9231e-07, 7.4228e-02, 4.5470e-07, 6.5606e-07, 1.1513e-06, 6.1886e-07,
        1.6594e-06, 6.6548e-07, 1.9903e-01, 1.8250e-06, 1.0861e-06, 4.1026e-07,
        4.2700e-07, 9.8423e-07, 1.7231e-06, 1.1646e-06, 1.1384e-06, 8.4584e-07,
        5.0574e-07, 1.1315e-06, 1.0320e-06, 6.9675e-07, 5.0115e-07, 6.7514e-07,
        6.2614e-07, 2.9080e-07, 5.3563e-07, 8.5067e-07, 5.3535e-07, 5.0115e-07,
        1.3490e-06, 1.1497e-06, 6.8418e-07, 3.0380e-07, 1.7231e-06, 9.4566e-07,
        1.0823e-06, 1.6764e-06, 7.5034e-07, 7.9406e-07, 8.4584e-07, 1.0320e-06,
        5.5367e-07, 7.0664e-07, 9.9060e-07, 5.9827e-07, 8.7018e-07, 2.0309e-07,
        9.9185e-07, 1.1797e-06, 1.0320e-06, 2.7798e-07, 4.7859e-07, 1.7207e-01,
        1.1440e-06, 1.9151e-01, 8.5830e-07, 2.1168e-07, 8.7846e-07, 1.5418e-06,
        7.5830e-07, 7.0956e-07, 1.3278e-01, 7.5035e-07, 7.6888e-07, 1.3392e-06,
        4.2583e-07, 9.4167e-07, 3.4861e-07, 8.6313e-07, 8.4568e-07, 4.5987e-07,
        1.4495e-06, 1.4411e-06, 2.0309e-07, 6.3106e-07, 5.5445e-07, 4.2583e-07,
        4.9744e-07, 1.8216e-06, 9.8423e-07, 6.0099e-07, 1.6468e-06, 7.5034e-07,
        1.9670e-07, 6.3355e-07, 1.0120e-06, 1.2154e-06, 1.8341e-07, 6.9328e-07,
        5.7216e-07, 1.3248e-06, 9.1274e-07, 1.4391e-06, 1.2184e-06, 7.4576e-07,
        1.8250e-06, 1.1389e-06, 8.1639e-07, 1.1304e-06, 1.3090e-06, 1.1380e-06,
        7.9566e-07, 1.5224e-06, 8.7920e-07, 1.1805e-06, 7.8576e-07, 1.2129e-06,
        5.5127e-07, 1.8250e-06, 3.2989e-07, 1.1211e-06, 2.2406e-06, 1.2940e-06,
        7.0389e-07, 4.9275e-07, 2.7206e-07, 1.3036e-06, 1.8250e-06, 6.8428e-07,
        1.2543e-06, 1.0236e-06, 8.7679e-07, 5.0694e-07, 6.8889e-07, 4.2951e-07,
        1.6468e-06, 1.2424e-06, 9.7071e-07, 7.7435e-07, 4.3794e-07, 6.3476e-07,
        9.8843e-07, 3.9515e-07, 5.6832e-07, 1.3760e-06, 4.6125e-07, 1.2543e-06,
        4.0018e-07, 1.0496e-06, 6.6658e-07, 1.6003e-06, 3.9924e-07, 1.3760e-06,
        5.9050e-07, 1.3787e-06, 1.6125e-07, 5.8025e-07, 5.4022e-07, 1.4138e-06,
        9.8843e-07, 8.4584e-07, 4.5850e-07, 6.6658e-07, 1.2489e-06, 4.7739e-07,
        6.6548e-07, 1.4700e-06, 6.3355e-07, 6.9328e-07, 1.0664e-06, 7.5566e-07,
        9.6516e-07, 1.1211e-06, 8.7941e-07, 5.3548e-07, 4.7523e-07, 2.1058e-07,
        2.0982e-07, 6.5606e-07, 8.4584e-07, 1.6468e-06, 5.7216e-07, 9.9003e-07,
        9.1104e-07, 7.9669e-07, 9.3572e-07, 3.7756e-07, 5.2251e-07, 7.9626e-07,
        9.4449e-07, 3.6480e-07, 4.5854e-07, 5.0115e-07, 1.8341e-07, 7.1047e-07,
        1.1826e-06, 6.0099e-07, 1.0754e-06, 1.0392e-06, 1.8619e-06, 9.8843e-07,
        7.1529e-07, 2.4207e-07, 3.0774e-07, 6.2241e-07, 7.7743e-07, 7.1860e-07,
        1.2031e-06, 6.6658e-07, 4.5934e-07, 1.3447e-06, 3.0380e-07, 1.8964e-06,
        7.6615e-07, 3.3551e-07, 6.3341e-07, 1.5766e-01, 6.6551e-07, 2.3844e-07,
        4.6125e-07, 6.8759e-07, 1.1826e-06, 1.5489e-06, 6.5969e-07, 2.7798e-07,
        3.8551e-07, 4.4316e-07, 3.0305e-07, 6.6548e-07, 1.2277e-01, 7.4851e-07,
        1.5489e-06, 8.4584e-07, 8.4162e-07, 1.3534e-06, 3.3374e-07, 7.5035e-07,
        6.0724e-07, 7.2998e-07, 9.0607e-07, 8.8745e-07, 5.3348e-07, 5.1119e-07,
        5.7634e-07, 3.2409e-07, 2.4454e-07, 1.0889e-06, 1.2678e-06, 1.1687e-06,
        1.4066e-06, 4.6125e-07, 1.7261e-06, 8.3366e-07, 5.6690e-07, 6.7135e-08,
        1.4257e-06, 6.4984e-07, 8.5913e-07, 8.7802e-07, 4.7523e-07, 1.8964e-06,
        1.1409e-01, 8.7802e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.6823, 0.9759, 0.7951, 0.8000, 0.7828, 0.7493, 1.0520, 0.8027, 0.8363,
        0.8934], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1111111119389534, 1.0, 0.0, 0.03703703731298447, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.07407407462596893]

 sparsity of   [0.4114583432674408, 0.4357638955116272, 0.4236111044883728, 0.4236111044883728, 0.4357638955116272, 0.4149305522441864, 0.425347238779068, 0.4184027910232544, 0.4322916567325592, 0.4184027910232544, 0.4236111044883728, 0.4305555522441864, 0.4288194477558136, 0.4131944477558136, 0.4201388955116272, 0.4270833432674408, 0.421875, 0.4288194477558136, 0.4305555522441864, 0.4079861044883728, 0.421875, 0.425347238779068, 0.4305555522441864, 0.4236111044883728, 0.4184027910232544, 0.4236111044883728, 0.4270833432674408, 0.4392361044883728, 0.4149305522441864, 0.4357638955116272, 0.4166666567325592, 0.4427083432674408, 0.4375, 0.4340277910232544, 0.4166666567325592, 0.4270833432674408, 0.4236111044883728, 0.4236111044883728, 0.4236111044883728, 0.4184027910232544, 0.4496527910232544, 0.4305555522441864, 0.4288194477558136, 0.4322916567325592, 0.4288194477558136, 0.4166666567325592, 0.4322916567325592, 0.4340277910232544, 0.4357638955116272, 0.4322916567325592, 0.425347238779068, 0.4322916567325592, 0.425347238779068, 0.4305555522441864, 0.4322916567325592, 0.425347238779068, 0.4166666567325592, 0.425347238779068, 0.409722238779068, 0.4236111044883728, 0.4270833432674408, 0.4305555522441864, 0.4201388955116272, 0.421875]

 sparsity of   [0.0034722222480922937, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0, 0.0017361111240461469, 0.0086805559694767, 0.0017361111240461469, 0.0, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.02083333395421505, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0, 0.0, 0.0, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0, 0.0, 0.0069444444961845875, 0.0, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.010416666977107525, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0]

 sparsity of   [0.0069444444961845875, 0.00434027798473835, 0.0069444444961845875, 0.00434027798473835, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0017361111240461469, 0.00434027798473835, 0.0052083334885537624, 0.006076388992369175, 0.0017361111240461469, 0.0017361111240461469, 0.0078125, 0.0026041667442768812, 0.006076388992369175, 0.0069444444961845875, 0.0026041667442768812, 0.0026041667442768812, 0.0034722222480922937, 0.0026041667442768812, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.00434027798473835, 0.0078125, 0.0034722222480922937, 0.0069444444961845875, 0.0078125, 0.0034722222480922937, 0.00434027798473835, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.006076388992369175, 0.0034722222480922937, 0.0026041667442768812, 0.0078125, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0078125, 0.0052083334885537624, 0.0052083334885537624, 0.0086805559694767, 0.0026041667442768812, 0.0052083334885537624, 0.0034722222480922937, 0.0078125, 0.0017361111240461469, 0.0052083334885537624, 0.00434027798473835, 0.0034722222480922937, 0.00434027798473835, 0.0026041667442768812, 0.0034722222480922937, 0.009548611007630825, 0.006076388992369175, 1.0, 0.0026041667442768812, 0.0052083334885537624, 0.009548611007630825, 0.0052083334885537624, 0.0026041667442768812, 0.0026041667442768812, 0.006076388992369175, 0.0026041667442768812, 0.006076388992369175, 0.00434027798473835, 0.0034722222480922937, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0052083334885537624, 0.0069444444961845875, 0.0034722222480922937, 0.0026041667442768812, 0.0017361111240461469, 0.0052083334885537624, 0.0069444444961845875, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.006076388992369175, 0.0008680555620230734, 0.0052083334885537624, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0026041667442768812, 0.006076388992369175, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.00434027798473835, 0.0069444444961845875, 0.0034722222480922937, 0.006076388992369175, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.0017361111240461469, 0.00434027798473835, 0.0026041667442768812, 0.009548611007630825, 0.0017361111240461469, 0.00434027798473835, 0.0052083334885537624, 0.0026041667442768812]

 sparsity of   [0.01215277798473835, 0.013888888992369175, 0.009548611007630825, 0.01128472201526165, 0.009548611007630825, 0.014756944961845875, 0.009548611007630825, 0.01215277798473835, 0.013020833022892475, 0.010416666977107525, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.013020833022892475, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 1.0, 0.01215277798473835, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 0.01128472201526165, 0.015625, 0.01128472201526165, 0.01128472201526165, 0.013888888992369175, 0.0086805559694767, 0.01215277798473835, 0.013020833022892475, 0.010416666977107525, 0.013888888992369175, 0.01215277798473835, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.01215277798473835, 0.009548611007630825, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.009548611007630825, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.01128472201526165, 0.01215277798473835, 0.014756944961845875, 0.010416666977107525, 0.009548611007630825, 0.013020833022892475, 0.0164930559694767, 0.01128472201526165, 0.01128472201526165, 1.0, 0.014756944961845875, 0.013020833022892475, 0.013888888992369175, 0.01215277798473835, 0.01128472201526165, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.013020833022892475, 1.0, 0.010416666977107525, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.015625, 0.01128472201526165, 0.01215277798473835, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.009548611007630825, 0.013020833022892475, 0.0086805559694767, 0.013888888992369175, 0.014756944961845875, 0.01215277798473835, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.014756944961845875, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.009548611007630825, 0.014756944961845875, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.01215277798473835, 0.009548611007630825, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.014756944961845875, 0.0086805559694767, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.0086805559694767, 0.01215277798473835, 0.010416666977107525, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.010416666977107525, 0.01128472201526165, 0.01128472201526165, 1.0, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.015625, 1.0, 0.013020833022892475, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.009548611007630825, 0.009548611007630825, 0.009548611007630825, 0.01128472201526165, 0.01215277798473835, 0.01128472201526165, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.009548611007630825, 1.0, 0.01215277798473835, 0.0086805559694767, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.01215277798473835, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.0086805559694767, 0.010416666977107525, 0.013888888992369175, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 1.0, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.0173611119389534, 0.013020833022892475, 0.0086805559694767, 1.0, 0.01215277798473835, 0.015625, 0.01215277798473835, 0.01215277798473835, 0.01128472201526165, 0.01822916604578495, 0.013020833022892475, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013888888992369175, 0.015625, 0.010416666977107525, 0.01215277798473835, 0.013020833022892475, 0.014756944961845875, 0.014756944961845875, 0.014756944961845875, 0.010416666977107525, 0.013888888992369175, 0.010416666977107525, 0.009548611007630825, 0.009548611007630825, 0.01215277798473835, 0.01215277798473835, 0.014756944961845875, 0.0164930559694767, 0.01128472201526165, 1.0, 0.01215277798473835, 0.010416666977107525, 0.010416666977107525, 0.010416666977107525, 0.01128472201526165, 0.013020833022892475, 0.009548611007630825, 0.015625, 0.013020833022892475, 0.009548611007630825, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01822916604578495, 0.01128472201526165, 0.010416666977107525, 0.013888888992369175, 1.0, 1.0, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.013020833022892475, 0.009548611007630825, 1.0, 0.013020833022892475, 0.01215277798473835, 0.013020833022892475, 0.01909722201526165, 0.01215277798473835, 0.009548611007630825, 0.01215277798473835, 0.013888888992369175, 0.013020833022892475, 0.01128472201526165, 0.01215277798473835, 0.01128472201526165, 0.01215277798473835, 0.014756944961845875, 0.01215277798473835, 0.014756944961845875]

 sparsity of   [1.0, 1.0, 1.0, 0.0568576380610466, 1.0, 0.0529513880610466, 1.0, 0.0568576380610466, 0.0572916679084301, 0.0611979179084301, 0.0581597238779068, 1.0, 1.0, 0.0555555559694767, 0.0542534738779068, 0.0559895820915699, 0.0607638880610466, 0.0555555559694767, 0.0559895820915699, 1.0, 0.0603298619389534, 1.0, 0.0533854179084301, 0.0555555559694767, 0.0594618059694767, 1.0, 0.0620659738779068, 0.05859375, 0.0564236119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0559895820915699, 1.0, 0.0564236119389534, 0.0577256940305233, 1.0, 0.05859375, 0.0555555559694767, 1.0, 0.0564236119389534, 1.0, 1.0, 0.0611979179084301, 1.0, 0.0542534738779068, 0.0551215298473835, 0.0546875, 0.0572916679084301, 0.0568576380610466, 0.0542534738779068, 1.0, 1.0, 1.0, 0.0577256940305233, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0538194440305233, 0.0581597238779068, 1.0, 0.0555555559694767, 1.0, 0.0598958320915699, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0568576380610466, 0.0564236119389534, 1.0, 1.0, 1.0, 0.0577256940305233, 0.0564236119389534, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.0555555559694767, 0.0572916679084301, 1.0, 1.0, 1.0, 0.0572916679084301, 1.0, 1.0, 0.0577256940305233, 0.0577256940305233, 0.0542534738779068, 1.0, 0.0551215298473835, 0.05859375, 0.0598958320915699, 1.0, 0.0581597238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0598958320915699, 0.0564236119389534, 1.0, 0.0555555559694767, 0.0581597238779068, 0.0559895820915699, 1.0, 1.0, 0.0590277798473835, 0.0625, 0.0542534738779068, 0.0555555559694767, 1.0, 1.0, 1.0, 0.0577256940305233, 1.0, 0.0551215298473835, 0.0559895820915699, 0.0581597238779068, 0.0564236119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0598958320915699, 1.0, 1.0, 0.0603298619389534, 0.0603298619389534, 0.05859375, 1.0, 0.0542534738779068, 0.0568576380610466, 1.0, 0.0581597238779068, 0.0564236119389534, 0.0568576380610466, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0555555559694767, 0.0559895820915699, 1.0, 0.0533854179084301, 1.0, 0.0546875, 0.0625, 0.0546875, 1.0, 1.0, 0.0559895820915699, 0.0577256940305233, 0.0572916679084301, 0.0533854179084301, 0.0559895820915699, 0.0559895820915699, 0.0542534738779068, 1.0, 0.0546875, 0.0555555559694767, 0.0551215298473835, 1.0, 1.0, 0.0546875, 1.0, 0.0611979179084301, 0.0564236119389534, 0.0577256940305233, 0.0542534738779068, 0.0572916679084301, 0.0620659738779068, 0.0590277798473835, 0.0572916679084301, 0.0581597238779068, 1.0, 0.0594618059694767, 1.0, 0.0572916679084301, 0.0577256940305233, 0.0776909738779068, 0.0559895820915699, 0.05859375, 0.0572916679084301, 0.0559895820915699, 0.0577256940305233, 0.0559895820915699, 0.0616319440305233, 0.0572916679084301, 0.0533854179084301, 0.0603298619389534, 0.0538194440305233, 0.0551215298473835, 0.0572916679084301, 1.0, 0.0594618059694767, 0.0538194440305233, 0.0577256940305233, 1.0, 0.0546875, 1.0, 0.0559895820915699, 0.0772569477558136, 0.0572916679084301, 1.0, 0.0568576380610466, 1.0, 0.0598958320915699, 0.0577256940305233, 0.0568576380610466, 0.0568576380610466, 1.0, 1.0, 0.0564236119389534, 0.0603298619389534, 0.0546875, 1.0, 1.0, 0.0581597238779068, 0.0577256940305233, 0.0577256940305233, 1.0, 0.0594618059694767, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.0590277798473835, 0.0572916679084301, 0.0568576380610466, 1.0, 0.0555555559694767, 0.0577256940305233]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4353298544883728, 1.0, 0.440972238779068, 1.0, 0.4327256977558136, 1.0, 1.0, 0.4344618022441864, 0.4344618022441864, 0.4348958432674408, 1.0, 0.4348958432674408, 0.4357638955116272, 0.4327256977558136, 0.433159738779068, 0.43359375, 1.0, 1.0, 1.0, 0.4353298544883728, 0.4327256977558136, 1.0, 1.0, 0.4379340410232544, 1.0, 0.4344618022441864, 1.0, 1.0, 0.4379340410232544, 0.4340277910232544, 1.0, 1.0, 0.4318576455116272, 0.4348958432674408, 0.4357638955116272, 0.4344618022441864, 1.0, 0.433159738779068, 1.0, 1.0, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4344618022441864, 1.0, 0.4344618022441864, 0.4314236044883728, 1.0, 1.0, 1.0, 0.4348958432674408, 1.0, 0.4353298544883728, 0.4383680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 0.433159738779068, 0.4318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4344618022441864, 1.0, 1.0, 1.0, 0.4318576455116272, 0.4340277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4309895932674408, 1.0, 0.43359375, 1.0, 1.0, 1.0, 1.0, 0.4305555522441864, 1.0, 1.0, 1.0, 0.43359375, 0.4327256977558136, 1.0, 1.0, 0.4348958432674408, 1.0, 1.0, 1.0, 0.4314236044883728, 0.4344618022441864, 1.0, 1.0, 0.4344618022441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 0.4327256977558136, 0.4348958432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4318576455116272, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 1.0, 0.4353298544883728, 1.0, 0.43359375, 0.4340277910232544, 1.0, 0.4327256977558136, 0.43359375, 1.0, 0.43359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4344618022441864, 1.0, 1.0, 1.0, 0.437065988779068, 1.0, 0.433159738779068, 1.0, 0.4327256977558136, 0.4348958432674408, 0.4357638955116272, 1.0, 0.4327256977558136, 0.43359375, 1.0, 0.4322916567325592, 1.0, 0.4340277910232544, 0.4361979067325592, 0.4344618022441864, 0.433159738779068, 0.433159738779068, 1.0, 1.0, 1.0, 0.4327256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.437065988779068, 0.4357638955116272, 0.43359375, 1.0, 1.0, 1.0, 1.0, 0.4348958432674408, 0.4348958432674408, 1.0, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 0.4344618022441864, 1.0, 0.4340277910232544, 0.4353298544883728, 0.4479166567325592, 0.43359375, 0.433159738779068, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 0.4357638955116272, 0.4348958432674408, 1.0, 0.4322916567325592, 1.0, 1.0, 0.43359375, 0.4318576455116272, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 0.6345486044883728, 1.0, 0.6362847089767456, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6332465410232544, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 1.0, 1.0, 1.0, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6297743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6697048544883728, 1.0, 1.0, 0.635850727558136, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 0.631944477558136, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62890625, 1.0, 0.6375868320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 0.6332465410232544, 0.6336805820465088, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9086371660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9084201455116272, 1.0, 1.0, 0.9049479365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.907335102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9069010615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.908203125, 1.0, 1.0, 1.0, 1.0, 0.908203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.907335102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9114583134651184, 1.0, 1.0, 0.9086371660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090712070465088, 1.0, 1.0, 1.0, 0.9084201455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9752604365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9661458134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9652777910232544, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 0.9652777910232544, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9654948115348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9652777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0]

 sparsity of   [0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375]

Total parameter pruned: 13711299.00331086 (unstructured) 13186494 (structured)

Test: [0/79]	Time 0.181 (0.181)	Loss 0.3987 (0.3987) ([0.307]+[0.092])	Prec@1 93.750 (93.750)
 * Prec@1 93.520

 Total elapsed time  0:54:07.428671 
 FINETUNING


 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1111111119389534, 1.0, 0.0, 0.03703703731298447, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.07407407462596893]

 sparsity of   [0.4114583432674408, 0.4357638955116272, 0.4236111044883728, 0.4236111044883728, 0.4357638955116272, 0.4149305522441864, 0.425347238779068, 0.4184027910232544, 0.4322916567325592, 0.4184027910232544, 0.4236111044883728, 0.4305555522441864, 0.4288194477558136, 0.4131944477558136, 0.4201388955116272, 0.4270833432674408, 0.421875, 0.4288194477558136, 0.4305555522441864, 0.4079861044883728, 0.421875, 0.425347238779068, 0.4305555522441864, 0.4236111044883728, 0.4184027910232544, 0.4236111044883728, 0.4270833432674408, 0.4392361044883728, 0.4149305522441864, 0.4357638955116272, 0.4166666567325592, 0.4427083432674408, 0.4375, 0.4340277910232544, 0.4166666567325592, 0.4270833432674408, 0.4236111044883728, 0.4236111044883728, 0.4236111044883728, 0.4184027910232544, 0.4496527910232544, 0.4305555522441864, 0.4288194477558136, 0.4322916567325592, 0.4288194477558136, 0.4166666567325592, 0.4322916567325592, 0.4340277910232544, 0.4357638955116272, 0.4322916567325592, 0.425347238779068, 0.4322916567325592, 0.425347238779068, 0.4305555522441864, 0.4322916567325592, 0.425347238779068, 0.4166666567325592, 0.425347238779068, 0.409722238779068, 0.4236111044883728, 0.4270833432674408, 0.4305555522441864, 0.4201388955116272, 0.421875]

 sparsity of   [0.0034722222480922937, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0, 0.0017361111240461469, 0.0086805559694767, 0.0017361111240461469, 0.0, 0.0, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.02083333395421505, 0.0052083334885537624, 0.0017361111240461469, 0.0069444444961845875, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0017361111240461469, 0.0, 0.0, 0.0, 0.0034722222480922937, 0.0086805559694767, 0.0034722222480922937, 0.0, 0.0069444444961845875, 0.0069444444961845875, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0, 0.0, 0.0069444444961845875, 0.0, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0052083334885537624, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.010416666977107525, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0034722222480922937, 0.0, 0.0034722222480922937, 0.0, 0.0, 0.0034722222480922937, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.0017361111240461469, 0.0, 0.0017361111240461469, 0.0017361111240461469, 0.0034722222480922937, 0.0034722222480922937, 0.0052083334885537624, 0.0069444444961845875, 0.0017361111240461469, 0.0034722222480922937, 0.0069444444961845875, 0.0069444444961845875, 0.010416666977107525, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0017361111240461469, 0.0069444444961845875, 0.0034722222480922937, 0.0]

 sparsity of   [0.0069444444961845875, 0.00434027798473835, 0.0069444444961845875, 0.00434027798473835, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.0017361111240461469, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0017361111240461469, 0.00434027798473835, 0.0052083334885537624, 0.006076388992369175, 0.0017361111240461469, 0.0017361111240461469, 0.0078125, 0.0026041667442768812, 0.006076388992369175, 0.0069444444961845875, 0.0026041667442768812, 0.0026041667442768812, 0.0034722222480922937, 0.0026041667442768812, 0.00434027798473835, 0.0052083334885537624, 0.00434027798473835, 0.00434027798473835, 0.0078125, 0.0034722222480922937, 0.0069444444961845875, 0.0078125, 0.0034722222480922937, 0.00434027798473835, 0.0069444444961845875, 0.0034722222480922937, 0.0017361111240461469, 0.006076388992369175, 0.0034722222480922937, 0.0026041667442768812, 0.0078125, 0.00434027798473835, 0.00434027798473835, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.0078125, 0.0052083334885537624, 0.0052083334885537624, 0.0086805559694767, 0.0026041667442768812, 0.0052083334885537624, 0.0034722222480922937, 0.0078125, 0.0017361111240461469, 0.0052083334885537624, 0.00434027798473835, 0.0034722222480922937, 0.00434027798473835, 0.0026041667442768812, 0.0034722222480922937, 0.009548611007630825, 0.006076388992369175, 1.0, 0.0026041667442768812, 0.0052083334885537624, 0.009548611007630825, 0.0052083334885537624, 0.0026041667442768812, 0.0026041667442768812, 0.006076388992369175, 0.0026041667442768812, 0.006076388992369175, 0.00434027798473835, 0.0034722222480922937, 0.0026041667442768812, 0.00434027798473835, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.00434027798473835, 0.0052083334885537624, 0.0069444444961845875, 0.0034722222480922937, 0.0026041667442768812, 0.0017361111240461469, 0.0052083334885537624, 0.0069444444961845875, 0.0052083334885537624, 0.0017361111240461469, 0.0034722222480922937, 0.006076388992369175, 0.0008680555620230734, 0.0052083334885537624, 0.006076388992369175, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0034722222480922937, 0.0069444444961845875, 0.0034722222480922937, 0.0052083334885537624, 0.0052083334885537624, 0.0017361111240461469, 0.0026041667442768812, 0.006076388992369175, 0.0017361111240461469, 0.0052083334885537624, 0.0034722222480922937, 0.00434027798473835, 0.0069444444961845875, 0.0034722222480922937, 0.006076388992369175, 0.0026041667442768812, 0.0034722222480922937, 0.006076388992369175, 0.0017361111240461469, 0.00434027798473835, 0.0026041667442768812, 0.009548611007630825, 0.0017361111240461469, 0.00434027798473835, 0.0052083334885537624, 0.0026041667442768812]

 sparsity of   [0.01215277798473835, 0.013888888992369175, 0.009548611007630825, 0.01128472201526165, 0.009548611007630825, 0.014756944961845875, 0.009548611007630825, 0.01215277798473835, 0.013020833022892475, 0.010416666977107525, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.013020833022892475, 0.013888888992369175, 0.01128472201526165, 0.01128472201526165, 0.010416666977107525, 0.010416666977107525, 1.0, 0.01215277798473835, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 0.01128472201526165, 0.015625, 0.01128472201526165, 0.01128472201526165, 0.013888888992369175, 0.0086805559694767, 0.01215277798473835, 0.013020833022892475, 0.010416666977107525, 0.013888888992369175, 0.01215277798473835, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.01215277798473835, 0.009548611007630825, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.009548611007630825, 0.013020833022892475, 0.01215277798473835, 0.015625, 0.01128472201526165, 0.01215277798473835, 0.014756944961845875, 0.010416666977107525, 0.009548611007630825, 0.013020833022892475, 0.0164930559694767, 0.01128472201526165, 0.01128472201526165, 1.0, 0.014756944961845875, 0.013020833022892475, 0.013888888992369175, 0.01215277798473835, 0.01128472201526165, 0.013020833022892475, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.013020833022892475, 1.0, 0.010416666977107525, 0.01128472201526165, 0.0086805559694767, 0.009548611007630825, 0.015625, 0.01128472201526165, 0.01215277798473835, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 1.0, 0.009548611007630825, 0.013020833022892475, 0.0086805559694767, 0.013888888992369175, 0.014756944961845875, 0.01215277798473835, 0.01215277798473835, 0.01215277798473835, 0.010416666977107525, 0.014756944961845875, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.009548611007630825, 0.014756944961845875, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.01215277798473835, 0.009548611007630825, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01215277798473835, 0.014756944961845875, 0.0086805559694767, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.0086805559694767, 0.01215277798473835, 0.010416666977107525, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.010416666977107525, 0.01128472201526165, 0.01128472201526165, 1.0, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.013888888992369175, 0.013020833022892475, 0.01215277798473835, 0.01215277798473835, 0.015625, 1.0, 0.013020833022892475, 0.01128472201526165, 0.01128472201526165, 0.01128472201526165, 0.013888888992369175, 0.010416666977107525, 0.01215277798473835, 0.013888888992369175, 0.01128472201526165, 0.009548611007630825, 0.009548611007630825, 0.009548611007630825, 0.01128472201526165, 0.01215277798473835, 0.01128472201526165, 0.010416666977107525, 0.013888888992369175, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.009548611007630825, 1.0, 0.01215277798473835, 0.0086805559694767, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.01215277798473835, 0.013020833022892475, 0.01128472201526165, 0.010416666977107525, 0.014756944961845875, 0.0086805559694767, 0.010416666977107525, 0.013888888992369175, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 1.0, 0.01215277798473835, 0.01215277798473835, 0.013020833022892475, 0.0173611119389534, 0.013020833022892475, 0.0086805559694767, 1.0, 0.01215277798473835, 0.015625, 0.01215277798473835, 0.01215277798473835, 0.01128472201526165, 0.01822916604578495, 0.013020833022892475, 0.010416666977107525, 0.013020833022892475, 0.013020833022892475, 0.01128472201526165, 0.013020833022892475, 0.013888888992369175, 0.015625, 0.010416666977107525, 0.01215277798473835, 0.013020833022892475, 0.014756944961845875, 0.014756944961845875, 0.014756944961845875, 0.010416666977107525, 0.013888888992369175, 0.010416666977107525, 0.009548611007630825, 0.009548611007630825, 0.01215277798473835, 0.01215277798473835, 0.014756944961845875, 0.0164930559694767, 0.01128472201526165, 1.0, 0.01215277798473835, 0.010416666977107525, 0.010416666977107525, 0.010416666977107525, 0.01128472201526165, 0.013020833022892475, 0.009548611007630825, 0.015625, 0.013020833022892475, 0.009548611007630825, 0.014756944961845875, 0.01215277798473835, 0.01128472201526165, 0.009548611007630825, 0.01128472201526165, 0.013020833022892475, 0.01822916604578495, 0.01128472201526165, 0.010416666977107525, 0.013888888992369175, 1.0, 1.0, 0.01128472201526165, 0.01215277798473835, 0.010416666977107525, 0.013020833022892475, 0.009548611007630825, 1.0, 0.013020833022892475, 0.01215277798473835, 0.013020833022892475, 0.01909722201526165, 0.01215277798473835, 0.009548611007630825, 0.01215277798473835, 0.013888888992369175, 0.013020833022892475, 0.01128472201526165, 0.01215277798473835, 0.01128472201526165, 0.01215277798473835, 0.014756944961845875, 0.01215277798473835, 0.014756944961845875]

 sparsity of   [1.0, 1.0, 1.0, 0.0568576380610466, 1.0, 0.0529513880610466, 1.0, 0.0568576380610466, 0.0572916679084301, 0.0611979179084301, 0.0581597238779068, 1.0, 1.0, 0.0555555559694767, 0.0542534738779068, 0.0559895820915699, 0.0607638880610466, 0.0555555559694767, 0.0559895820915699, 1.0, 0.0603298619389534, 1.0, 0.0533854179084301, 0.0555555559694767, 0.0594618059694767, 1.0, 0.0620659738779068, 0.05859375, 0.0564236119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0559895820915699, 1.0, 0.0564236119389534, 0.0577256940305233, 1.0, 0.05859375, 0.0555555559694767, 1.0, 0.0564236119389534, 1.0, 1.0, 0.0611979179084301, 1.0, 0.0542534738779068, 0.0551215298473835, 0.0546875, 0.0572916679084301, 0.0568576380610466, 0.0542534738779068, 1.0, 1.0, 1.0, 0.0577256940305233, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0538194440305233, 0.0581597238779068, 1.0, 0.0555555559694767, 1.0, 0.0598958320915699, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0568576380610466, 0.0564236119389534, 1.0, 1.0, 1.0, 0.0577256940305233, 0.0564236119389534, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.0555555559694767, 0.0572916679084301, 1.0, 1.0, 1.0, 0.0572916679084301, 1.0, 1.0, 0.0577256940305233, 0.0577256940305233, 0.0542534738779068, 1.0, 0.0551215298473835, 0.05859375, 0.0598958320915699, 1.0, 0.0581597238779068, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0598958320915699, 0.0564236119389534, 1.0, 0.0555555559694767, 0.0581597238779068, 0.0559895820915699, 1.0, 1.0, 0.0590277798473835, 0.0625, 0.0542534738779068, 0.0555555559694767, 1.0, 1.0, 1.0, 0.0577256940305233, 1.0, 0.0551215298473835, 0.0559895820915699, 0.0581597238779068, 0.0564236119389534, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0598958320915699, 1.0, 1.0, 0.0603298619389534, 0.0603298619389534, 0.05859375, 1.0, 0.0542534738779068, 0.0568576380610466, 1.0, 0.0581597238779068, 0.0564236119389534, 0.0568576380610466, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0555555559694767, 0.0559895820915699, 1.0, 0.0533854179084301, 1.0, 0.0546875, 0.0625, 0.0546875, 1.0, 1.0, 0.0559895820915699, 0.0577256940305233, 0.0572916679084301, 0.0533854179084301, 0.0559895820915699, 0.0559895820915699, 0.0542534738779068, 1.0, 0.0546875, 0.0555555559694767, 0.0551215298473835, 1.0, 1.0, 0.0546875, 1.0, 0.0611979179084301, 0.0564236119389534, 0.0577256940305233, 0.0542534738779068, 0.0572916679084301, 0.0620659738779068, 0.0590277798473835, 0.0572916679084301, 0.0581597238779068, 1.0, 0.0594618059694767, 1.0, 0.0572916679084301, 0.0577256940305233, 0.0776909738779068, 0.0559895820915699, 0.05859375, 0.0572916679084301, 0.0559895820915699, 0.0577256940305233, 0.0559895820915699, 0.0616319440305233, 0.0572916679084301, 0.0533854179084301, 0.0603298619389534, 0.0538194440305233, 0.0551215298473835, 0.0572916679084301, 1.0, 0.0594618059694767, 0.0538194440305233, 0.0577256940305233, 1.0, 0.0546875, 1.0, 0.0559895820915699, 0.0772569477558136, 0.0572916679084301, 1.0, 0.0568576380610466, 1.0, 0.0598958320915699, 0.0577256940305233, 0.0568576380610466, 0.0568576380610466, 1.0, 1.0, 0.0564236119389534, 0.0603298619389534, 0.0546875, 1.0, 1.0, 0.0581597238779068, 0.0577256940305233, 0.0577256940305233, 1.0, 0.0594618059694767, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.0590277798473835, 0.0572916679084301, 0.0568576380610466, 1.0, 0.0555555559694767, 0.0577256940305233]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4353298544883728, 1.0, 0.440972238779068, 1.0, 0.4327256977558136, 1.0, 1.0, 0.4344618022441864, 0.4344618022441864, 0.4348958432674408, 1.0, 0.4348958432674408, 0.4357638955116272, 0.4327256977558136, 0.433159738779068, 0.43359375, 1.0, 1.0, 1.0, 0.4353298544883728, 0.4327256977558136, 1.0, 1.0, 0.4379340410232544, 1.0, 0.4344618022441864, 1.0, 1.0, 0.4379340410232544, 0.4340277910232544, 1.0, 1.0, 0.4318576455116272, 0.4348958432674408, 0.4357638955116272, 0.4344618022441864, 1.0, 0.433159738779068, 1.0, 1.0, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4344618022441864, 1.0, 0.4344618022441864, 0.4314236044883728, 1.0, 1.0, 1.0, 0.4348958432674408, 1.0, 0.4353298544883728, 0.4383680522441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4366319477558136, 1.0, 1.0, 1.0, 1.0, 0.433159738779068, 0.4318576455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4344618022441864, 1.0, 1.0, 1.0, 0.4318576455116272, 0.4340277910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4309895932674408, 1.0, 0.43359375, 1.0, 1.0, 1.0, 1.0, 0.4305555522441864, 1.0, 1.0, 1.0, 0.43359375, 0.4327256977558136, 1.0, 1.0, 0.4348958432674408, 1.0, 1.0, 1.0, 0.4314236044883728, 0.4344618022441864, 1.0, 1.0, 0.4344618022441864, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 0.4327256977558136, 0.4348958432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4318576455116272, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 1.0, 0.4353298544883728, 1.0, 0.43359375, 0.4340277910232544, 1.0, 0.4327256977558136, 0.43359375, 1.0, 0.43359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4344618022441864, 1.0, 1.0, 1.0, 0.437065988779068, 1.0, 0.433159738779068, 1.0, 0.4327256977558136, 0.4348958432674408, 0.4357638955116272, 1.0, 0.4327256977558136, 0.43359375, 1.0, 0.4322916567325592, 1.0, 0.4340277910232544, 0.4361979067325592, 0.4344618022441864, 0.433159738779068, 0.433159738779068, 1.0, 1.0, 1.0, 0.4327256977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.437065988779068, 0.4357638955116272, 0.43359375, 1.0, 1.0, 1.0, 1.0, 0.4348958432674408, 0.4348958432674408, 1.0, 1.0, 0.4357638955116272, 0.4340277910232544, 1.0, 0.4344618022441864, 1.0, 0.4340277910232544, 0.4353298544883728, 0.4479166567325592, 0.43359375, 0.433159738779068, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 1.0, 0.4340277910232544, 1.0, 1.0, 0.4357638955116272, 0.4348958432674408, 1.0, 0.4322916567325592, 1.0, 1.0, 0.43359375, 0.4318576455116272, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 0.6762152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 0.6345486044883728, 1.0, 0.6362847089767456, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6332465410232544, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 1.0, 1.0, 1.0, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6297743320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6697048544883728, 1.0, 1.0, 0.635850727558136, 0.6349826455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6310763955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 1.0, 0.63671875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.635850727558136, 1.0, 1.0, 0.6362847089767456, 1.0, 1.0, 0.631944477558136, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62890625, 1.0, 0.6375868320465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 0.6388888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.63671875, 1.0, 0.6341145634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6323784589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6345486044883728, 1.0, 1.0, 1.0, 1.0, 0.6332465410232544, 0.6336805820465088, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9086371660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9084201455116272, 1.0, 1.0, 0.9049479365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.907335102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9069010615348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9079861044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.908203125, 1.0, 1.0, 1.0, 1.0, 0.908203125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.907335102558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9114583134651184, 1.0, 1.0, 0.9086371660232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090712070465088, 1.0, 1.0, 1.0, 0.9084201455116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9752604365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.974609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9748263955116272, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9769965410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 0.9767795205116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9767795205116272, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 0.9806857705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9661458134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9652777910232544, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 0.9652777910232544, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9654948115348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9652777910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9650607705116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0]

 sparsity of   [0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375]

Total parameter pruned: 13711299.00331086 (unstructured) 13186494 (structured)

Test: [0/79]	Time 0.168 (0.168)	Loss 2.4657 (2.4657) ([2.388]+[0.078])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0114, device='cuda:0')
Epoch: [300][0/391]	Time 0.196 (0.196)	Data 0.178 (0.178)	Loss 2.3017 (2.3017) ([2.302]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [300][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3067 (2.3032) ([2.307]+[0.000])	Prec@1 5.469 (10.203)
Epoch: [300][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3045 (2.3035) ([2.305]+[0.000])	Prec@1 11.719 (10.106)
Epoch: [300][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3100 (2.3038) ([2.310]+[0.000])	Prec@1 9.375 (9.943)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3045 (2.3045) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0204, device='cuda:0')
Epoch: [301][0/391]	Time 0.196 (0.196)	Data 0.175 (0.175)	Loss 2.3016 (2.3016) ([2.302]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [301][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.2970 (2.3038) ([2.297]+[0.000])	Prec@1 13.281 (9.506)
Epoch: [301][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3052 (2.3034) ([2.305]+[0.000])	Prec@1 9.375 (9.853)
Epoch: [301][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3054 (2.3033) ([2.305]+[0.000])	Prec@1 10.156 (9.967)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3041 (2.3041) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0298, device='cuda:0')
Epoch: [302][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3086 (2.3086) ([2.309]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [302][100/391]	Time 0.013 (0.014)	Data 0.000 (0.002)	Loss 2.3010 (2.3030) ([2.301]+[0.000])	Prec@1 11.719 (10.249)
Epoch: [302][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.2986 (2.3030) ([2.299]+[0.000])	Prec@1 10.938 (10.067)
Epoch: [302][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3012 (2.3031) ([2.301]+[0.000])	Prec@1 7.812 (10.055)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3038 (2.3038) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0290, device='cuda:0')
Epoch: [303][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3068 (2.3068) ([2.307]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [303][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.2990 (2.3028) ([2.299]+[0.000])	Prec@1 11.719 (9.816)
Epoch: [303][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3031 (2.3031) ([2.303]+[0.000])	Prec@1 12.500 (9.736)
Epoch: [303][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3030) ([2.303]+[0.000])	Prec@1 9.375 (9.995)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3036 (2.3036) ([2.304]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0314, device='cuda:0')
Epoch: [304][0/391]	Time 0.245 (0.245)	Data 0.225 (0.225)	Loss 2.3016 (2.3016) ([2.302]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [304][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3031 (2.3030) ([2.303]+[0.000])	Prec@1 10.156 (9.592)
Epoch: [304][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3082 (2.3030) ([2.308]+[0.000])	Prec@1 3.906 (9.896)
Epoch: [304][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3039 (2.3030) ([2.304]+[0.000])	Prec@1 9.375 (9.904)
Test: [0/79]	Time 0.202 (0.202)	Loss 2.3034 (2.3034) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0598, device='cuda:0')
Epoch: [305][0/391]	Time 0.219 (0.219)	Data 0.200 (0.200)	Loss 2.3060 (2.3060) ([2.306]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [305][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3041 (2.3032) ([2.304]+[0.000])	Prec@1 9.375 (9.715)
Epoch: [305][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3013 (2.3030) ([2.301]+[0.000])	Prec@1 8.594 (9.869)
Epoch: [305][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3035 (2.3029) ([2.303]+[0.000])	Prec@1 10.156 (9.990)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3033 (2.3033) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0343, device='cuda:0')
Epoch: [306][0/391]	Time 0.250 (0.250)	Data 0.230 (0.230)	Loss 2.3015 (2.3015) ([2.301]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [306][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3032 (2.3029) ([2.303]+[0.000])	Prec@1 10.156 (9.870)
Epoch: [306][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3021 (2.3027) ([2.302]+[0.000])	Prec@1 10.156 (10.012)
Epoch: [306][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3035 (2.3027) ([2.304]+[0.000])	Prec@1 12.500 (10.045)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3032 (2.3032) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0443, device='cuda:0')
Epoch: [307][0/391]	Time 0.215 (0.215)	Data 0.196 (0.196)	Loss 2.3039 (2.3039) ([2.304]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [307][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3019 (2.3029) ([2.302]+[0.000])	Prec@1 10.938 (9.700)
Epoch: [307][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3020 (2.3027) ([2.302]+[0.000])	Prec@1 11.719 (10.040)
Epoch: [307][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3021 (2.3027) ([2.302]+[0.000])	Prec@1 10.156 (10.029)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3031 (2.3031) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0158, device='cuda:0')
Epoch: [308][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3019 (2.3019) ([2.302]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [308][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3030 (2.3027) ([2.303]+[0.000])	Prec@1 6.250 (9.576)
Epoch: [308][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3028) ([2.303]+[0.000])	Prec@1 6.250 (9.752)
Epoch: [308][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3016 (2.3027) ([2.302]+[0.000])	Prec@1 12.500 (9.915)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3030 (2.3030) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0284, device='cuda:0')
Epoch: [309][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3038 (2.3038) ([2.304]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [309][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3029 (2.3024) ([2.303]+[0.000])	Prec@1 10.156 (10.690)
Epoch: [309][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3027) ([2.303]+[0.000])	Prec@1 7.812 (10.005)
Epoch: [309][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3027) ([2.303]+[0.000])	Prec@1 8.594 (9.902)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0291, device='cuda:0')
Epoch: [310][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3042 (2.3042) ([2.304]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [310][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3023 (2.3027) ([2.302]+[0.000])	Prec@1 7.812 (9.661)
Epoch: [310][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.834)
Epoch: [310][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.910)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [311][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3023 (2.3023) ([2.302]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [311][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3030 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.963)
Epoch: [311][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.907)
Epoch: [311][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.985)
Test: [0/79]	Time 0.169 (0.169)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0244, device='cuda:0')
Epoch: [312][0/391]	Time 0.217 (0.217)	Data 0.199 (0.199)	Loss 2.3032 (2.3032) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [312][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3017 (2.3027) ([2.302]+[0.000])	Prec@1 14.844 (9.994)
Epoch: [312][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3030 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.020)
Epoch: [312][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3028 (2.3027) ([2.303]+[0.000])	Prec@1 10.938 (9.858)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0232, device='cuda:0')
Epoch: [313][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3019 (2.3019) ([2.302]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [313][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.032)
Epoch: [313][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.981)
Epoch: [313][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3022 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.058)
Test: [0/79]	Time 0.207 (0.207)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0343, device='cuda:0')
Epoch: [314][0/391]	Time 0.234 (0.234)	Data 0.211 (0.211)	Loss 2.3035 (2.3035) ([2.304]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [314][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.442)
Epoch: [314][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (10.160)
Epoch: [314][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3033 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.094)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0077, device='cuda:0')
Epoch: [315][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [315][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3021 (2.3025) ([2.302]+[0.000])	Prec@1 11.719 (10.295)
Epoch: [315][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.993)
Epoch: [315][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.985)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0257, device='cuda:0')
Epoch: [316][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [316][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (10.350)
Epoch: [316][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3030 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.129)
Epoch: [316][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (9.954)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0367, device='cuda:0')
Epoch: [317][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3029 (2.3029) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [317][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3022 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (10.071)
Epoch: [317][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.129)
Epoch: [317][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.042)
Test: [0/79]	Time 0.206 (0.206)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0207, device='cuda:0')
Epoch: [318][0/391]	Time 0.192 (0.192)	Data 0.174 (0.174)	Loss 2.3025 (2.3025) ([2.302]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [318][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.940)
Epoch: [318][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.028)
Epoch: [318][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3020 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (10.135)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0508, device='cuda:0')
Epoch: [319][0/391]	Time 0.226 (0.226)	Data 0.207 (0.207)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [319][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.381)
Epoch: [319][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3021 (2.3026) ([2.302]+[0.000])	Prec@1 17.188 (9.989)
Epoch: [319][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.956)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0081, device='cuda:0')
Epoch: [320][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [320][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.607)
Epoch: [320][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (9.682)
Epoch: [320][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.879)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0424, device='cuda:0')
Epoch: [321][0/391]	Time 0.210 (0.210)	Data 0.191 (0.191)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [321][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.087)
Epoch: [321][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.985)
Epoch: [321][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.032)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0272, device='cuda:0')
Epoch: [322][0/391]	Time 0.212 (0.212)	Data 0.194 (0.194)	Loss 2.3028 (2.3028) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [322][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.272)
Epoch: [322][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.125)
Epoch: [322][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.037)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0386, device='cuda:0')
Epoch: [323][0/391]	Time 0.211 (0.211)	Data 0.193 (0.193)	Loss 2.3022 (2.3022) ([2.302]+[0.000])	Prec@1 17.969 (17.969)
Epoch: [323][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3029 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.087)
Epoch: [323][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.047)
Epoch: [323][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 7.812 (10.115)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0245, device='cuda:0')
Epoch: [324][0/391]	Time 0.204 (0.204)	Data 0.184 (0.184)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [324][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.419)
Epoch: [324][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.164)
Epoch: [324][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.060)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [325][0/391]	Time 0.220 (0.220)	Data 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [325][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.412)
Epoch: [325][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.977)
Epoch: [325][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 17.188 (9.811)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0272, device='cuda:0')
Epoch: [326][0/391]	Time 0.246 (0.246)	Data 0.226 (0.226)	Loss 2.3024 (2.3024) ([2.302]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [326][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.203)
Epoch: [326][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.974)
Epoch: [326][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.148)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0259, device='cuda:0')
Epoch: [327][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [327][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.071)
Epoch: [327][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.818)
Epoch: [327][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.710)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0155, device='cuda:0')
Epoch: [328][0/391]	Time 0.229 (0.229)	Data 0.212 (0.212)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [328][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (10.125)
Epoch: [328][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.997)
Epoch: [328][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.003)
Test: [0/79]	Time 0.222 (0.222)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0073, device='cuda:0')
Epoch: [329][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [329][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.947)
Epoch: [329][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.098)
Epoch: [329][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.925)
Test: [0/79]	Time 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0370, device='cuda:0')
Epoch: [330][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [330][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.955)
Epoch: [330][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.795)
Epoch: [330][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.866)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0458, device='cuda:0')
Epoch: [331][0/391]	Time 0.202 (0.202)	Data 0.185 (0.185)	Loss 2.3027 (2.3027) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [331][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (10.172)
Epoch: [331][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.145)
Epoch: [331][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (10.068)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [332][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [332][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.669)
Epoch: [332][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.721)
Epoch: [332][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.736)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0262, device='cuda:0')
Epoch: [333][0/391]	Time 0.190 (0.190)	Data 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [333][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.110)
Epoch: [333][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.044)
Epoch: [333][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.063)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0276, device='cuda:0')
Epoch: [334][0/391]	Time 0.205 (0.205)	Data 0.186 (0.186)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [334][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.793)
Epoch: [334][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.865)
Epoch: [334][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.832)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0315, device='cuda:0')
Epoch: [335][0/391]	Time 0.199 (0.199)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [335][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.133)
Epoch: [335][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.896)
Epoch: [335][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.811)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0095, device='cuda:0')
Epoch: [336][0/391]	Time 0.204 (0.204)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [336][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (10.087)
Epoch: [336][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.071)
Epoch: [336][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.985)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0173, device='cuda:0')
Epoch: [337][0/391]	Time 0.196 (0.196)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [337][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (10.056)
Epoch: [337][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.900)
Epoch: [337][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.982)
Test: [0/79]	Time 0.196 (0.196)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0412, device='cuda:0')
Epoch: [338][0/391]	Time 0.196 (0.196)	Data 0.178 (0.178)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 17.188 (17.188)
Epoch: [338][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.218)
Epoch: [338][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.110)
Epoch: [338][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.917)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0230, device='cuda:0')
Epoch: [339][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [339][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.568)
Epoch: [339][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (9.787)
Epoch: [339][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.775)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0239, device='cuda:0')
Epoch: [340][0/391]	Time 0.257 (0.257)	Data 0.236 (0.236)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [340][100/391]	Time 0.014 (0.017)	Data 0.000 (0.003)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.878)
Epoch: [340][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.814)
Epoch: [340][300/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.855)
Test: [0/79]	Time 0.210 (0.210)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0113, device='cuda:0')
Epoch: [341][0/391]	Time 0.235 (0.235)	Data 0.216 (0.216)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [341][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.692)
Epoch: [341][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.775)
Epoch: [341][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.725)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0385, device='cuda:0')
Epoch: [342][0/391]	Time 0.210 (0.210)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [342][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.800)
Epoch: [342][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.810)
Epoch: [342][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.889)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0636, device='cuda:0')
Epoch: [343][0/391]	Time 0.235 (0.235)	Data 0.215 (0.215)	Loss 2.3025 (2.3025) ([2.302]+[0.000])	Prec@1 17.188 (17.188)
Epoch: [343][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.187)
Epoch: [343][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.861)
Epoch: [343][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.899)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0250, device='cuda:0')
Epoch: [344][0/391]	Time 0.223 (0.223)	Data 0.203 (0.203)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [344][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.739)
Epoch: [344][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.783)
Epoch: [344][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.767)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0115, device='cuda:0')
Epoch: [345][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [345][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.746)
Epoch: [345][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.016)
Epoch: [345][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.847)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0215, device='cuda:0')
Epoch: [346][0/391]	Time 0.197 (0.197)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [346][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.156)
Epoch: [346][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.966)
Epoch: [346][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.764)
Test: [0/79]	Time 0.214 (0.214)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0205, device='cuda:0')
Epoch: [347][0/391]	Time 0.222 (0.222)	Data 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [347][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.574)
Epoch: [347][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.405)
Epoch: [347][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.086)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0327, device='cuda:0')
Epoch: [348][0/391]	Time 0.218 (0.218)	Data 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [348][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.350)
Epoch: [348][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.071)
Epoch: [348][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.904)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0373, device='cuda:0')
Epoch: [349][0/391]	Time 0.212 (0.212)	Data 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [349][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.986)
Epoch: [349][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.841)
Epoch: [349][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.840)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [350][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [350][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.383)
Epoch: [350][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.667)
Epoch: [350][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.624)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0236, device='cuda:0')
Epoch: [351][0/391]	Time 0.208 (0.208)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [351][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.901)
Epoch: [351][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.814)
Epoch: [351][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.819)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0467, device='cuda:0')
Epoch: [352][0/391]	Time 0.198 (0.198)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [352][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.118)
Epoch: [352][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.907)
Epoch: [352][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.829)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0276, device='cuda:0')
Epoch: [353][0/391]	Time 0.199 (0.199)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [353][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.048)
Epoch: [353][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.752)
Epoch: [353][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.801)
Test: [0/79]	Time 0.203 (0.203)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0252, device='cuda:0')
Epoch: [354][0/391]	Time 0.194 (0.194)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [354][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.032)
Epoch: [354][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.970)
Epoch: [354][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.923)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [355][0/391]	Time 0.242 (0.242)	Data 0.222 (0.222)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [355][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.187)
Epoch: [355][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.016)
Epoch: [355][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.923)
Test: [0/79]	Time 0.167 (0.167)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0130, device='cuda:0')
Epoch: [356][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [356][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.032)
Epoch: [356][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.814)
Epoch: [356][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.686)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0390, device='cuda:0')
Epoch: [357][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [357][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.435)
Epoch: [357][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.106)
Epoch: [357][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.889)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0131, device='cuda:0')
Epoch: [358][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [358][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.151)
Epoch: [358][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.527)
Epoch: [358][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.562)
Test: [0/79]	Time 0.196 (0.196)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0169, device='cuda:0')
Epoch: [359][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [359][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.412)
Epoch: [359][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.207)
Epoch: [359][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (10.107)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [360][0/391]	Time 0.200 (0.200)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [360][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.677)
Epoch: [360][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.919)
Epoch: [360][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.879)
Test: [0/79]	Time 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0111, device='cuda:0')
Epoch: [361][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [361][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.071)
Epoch: [361][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.040)
Epoch: [361][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.855)
Test: [0/79]	Time 0.196 (0.196)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0262, device='cuda:0')
Epoch: [362][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [362][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.808)
Epoch: [362][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.012)
Epoch: [362][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.946)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0600, device='cuda:0')
Epoch: [363][0/391]	Time 0.204 (0.204)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [363][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.723)
Epoch: [363][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.841)
Epoch: [363][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.824)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0411, device='cuda:0')
Epoch: [364][0/391]	Time 0.220 (0.220)	Data 0.201 (0.201)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [364][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 7.812 (10.009)
Epoch: [364][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.962)
Epoch: [364][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.923)
Test: [0/79]	Time 0.203 (0.203)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [365][0/391]	Time 0.217 (0.217)	Data 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [365][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.909)
Epoch: [365][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.791)
Epoch: [365][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.814)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [366][0/391]	Time 0.202 (0.202)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [366][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.932)
Epoch: [366][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.020)
Epoch: [366][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.912)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0553, device='cuda:0')
Epoch: [367][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [367][100/391]	Time 0.015 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.940)
Epoch: [367][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.989)
Epoch: [367][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.078)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [368][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [368][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 17.188 (9.986)
Epoch: [368][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.009)
Epoch: [368][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.816)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0468, device='cuda:0')
Epoch: [369][0/391]	Time 0.191 (0.191)	Data 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [369][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 16.406 (10.388)
Epoch: [369][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.129)
Epoch: [369][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.055)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0181, device='cuda:0')
Epoch: [370][0/391]	Time 0.252 (0.252)	Data 0.231 (0.231)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [370][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.940)
Epoch: [370][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.830)
Epoch: [370][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.863)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0326, device='cuda:0')
Epoch: [371][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [371][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (10.009)
Epoch: [371][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.841)
Epoch: [371][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.777)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0284, device='cuda:0')
Epoch: [372][0/391]	Time 0.215 (0.215)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [372][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.793)
Epoch: [372][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.690)
Epoch: [372][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.736)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0154, device='cuda:0')
Epoch: [373][0/391]	Time 0.201 (0.201)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [373][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.506)
Epoch: [373][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.729)
Epoch: [373][300/391]	Time 0.016 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.661)
Test: [0/79]	Time 0.203 (0.203)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0267, device='cuda:0')
Epoch: [374][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [374][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.040)
Epoch: [374][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.919)
Epoch: [374][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.912)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0232, device='cuda:0')
Epoch: [375][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [375][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.669)
Epoch: [375][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.803)
Epoch: [375][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.640)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0207, device='cuda:0')
Epoch: [376][0/391]	Time 0.220 (0.220)	Data 0.200 (0.200)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [376][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.978)
Epoch: [376][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.958)
Epoch: [376][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.798)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0227, device='cuda:0')
Epoch: [377][0/391]	Time 0.192 (0.192)	Data 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [377][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.265)
Epoch: [377][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.012)
Epoch: [377][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.975)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0786, device='cuda:0')
Epoch: [378][0/391]	Time 0.254 (0.254)	Data 0.233 (0.233)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [378][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.592)
Epoch: [378][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.756)
Epoch: [378][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.728)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0111, device='cuda:0')
Epoch: [379][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [379][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.468)
Epoch: [379][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.705)
Epoch: [379][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.777)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0329, device='cuda:0')
Epoch: [380][0/391]	Time 0.224 (0.224)	Data 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [380][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.777)
Epoch: [380][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (10.067)
Epoch: [380][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.814)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0213, device='cuda:0')
Epoch: [381][0/391]	Time 0.203 (0.203)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [381][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.677)
Epoch: [381][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.725)
Epoch: [381][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.808)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0315, device='cuda:0')
Epoch: [382][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [382][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (10.613)
Epoch: [382][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.292)
Epoch: [382][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.078)
Test: [0/79]	Time 0.191 (0.191)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [383][0/391]	Time 0.244 (0.244)	Data 0.224 (0.224)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [383][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.824)
Epoch: [383][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.927)
Epoch: [383][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.847)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0224, device='cuda:0')
Epoch: [384][0/391]	Time 0.211 (0.211)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [384][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.334)
Epoch: [384][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 17.969 (10.230)
Epoch: [384][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.003)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [385][0/391]	Time 0.198 (0.198)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [385][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.924)
Epoch: [385][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.106)
Epoch: [385][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.008)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0308, device='cuda:0')
Epoch: [386][0/391]	Time 0.207 (0.207)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [386][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.156)
Epoch: [386][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.032)
Epoch: [386][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.821)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0248, device='cuda:0')
Epoch: [387][0/391]	Time 0.206 (0.206)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [387][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (10.265)
Epoch: [387][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.129)
Epoch: [387][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.047)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0217, device='cuda:0')
Epoch: [388][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [388][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.568)
Epoch: [388][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.803)
Epoch: [388][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.764)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0154, device='cuda:0')
Epoch: [389][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [389][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.009)
Epoch: [389][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.016)
Epoch: [389][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.904)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0177, device='cuda:0')
Epoch: [390][0/391]	Time 0.214 (0.214)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [390][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.700)
Epoch: [390][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.705)
Epoch: [390][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.658)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0267, device='cuda:0')
Epoch: [391][0/391]	Time 0.206 (0.206)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [391][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.816)
Epoch: [391][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.036)
Epoch: [391][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.855)
Test: [0/79]	Time 0.200 (0.200)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0221, device='cuda:0')
Epoch: [392][0/391]	Time 0.205 (0.205)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [392][100/391]	Time 0.016 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.893)
Epoch: [392][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.997)
Epoch: [392][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.899)
Test: [0/79]	Time 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0166, device='cuda:0')
Epoch: [393][0/391]	Time 0.208 (0.208)	Data 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [393][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.048)
Epoch: [393][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.900)
Epoch: [393][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.808)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0364, device='cuda:0')
Epoch: [394][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [394][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.048)
Epoch: [394][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.071)
Epoch: [394][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.951)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0359, device='cuda:0')
Epoch: [395][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [395][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.514)
Epoch: [395][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.853)
Epoch: [395][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.842)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0121, device='cuda:0')
Epoch: [396][0/391]	Time 0.187 (0.187)	Data 0.168 (0.168)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [396][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.940)
Epoch: [396][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.861)
Epoch: [396][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.751)
Test: [0/79]	Time 0.171 (0.171)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0450, device='cuda:0')
Epoch: [397][0/391]	Time 0.230 (0.230)	Data 0.210 (0.210)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [397][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.924)
Epoch: [397][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.946)
Epoch: [397][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (9.910)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0525, device='cuda:0')
Epoch: [398][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [398][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.087)
Epoch: [398][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.771)
Epoch: [398][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.725)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0094, device='cuda:0')
Epoch: [399][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [399][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.164)
Epoch: [399][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.744)
Epoch: [399][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.738)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0175, device='cuda:0')
Epoch: [400][0/391]	Time 0.207 (0.207)	Data 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [400][100/391]	Time 0.013 (0.017)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.924)
Epoch: [400][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.721)
Epoch: [400][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.767)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [401][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [401][100/391]	Time 0.012 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.669)
Epoch: [401][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.729)
Epoch: [401][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.635)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0138, device='cuda:0')
Epoch: [402][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [402][100/391]	Time 0.015 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.777)
Epoch: [402][200/391]	Time 0.012 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.686)
Epoch: [402][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.622)
Test: [0/79]	Time 0.164 (0.164)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0392, device='cuda:0')
Epoch: [403][0/391]	Time 0.225 (0.225)	Data 0.204 (0.204)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [403][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.754)
Epoch: [403][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.717)
Epoch: [403][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.728)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0119, device='cuda:0')
Epoch: [404][0/391]	Time 0.222 (0.222)	Data 0.203 (0.203)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [404][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.777)
Epoch: [404][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.896)
Epoch: [404][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.907)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0261, device='cuda:0')
Epoch: [405][0/391]	Time 0.246 (0.246)	Data 0.226 (0.226)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [405][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (10.087)
Epoch: [405][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.822)
Epoch: [405][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.889)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0324, device='cuda:0')
Epoch: [406][0/391]	Time 0.197 (0.197)	Data 0.177 (0.177)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [406][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.156)
Epoch: [406][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (10.005)
Epoch: [406][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.923)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0419, device='cuda:0')
Epoch: [407][0/391]	Time 0.193 (0.193)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (3.906)
Epoch: [407][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.584)
Epoch: [407][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.873)
Epoch: [407][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (9.920)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0134, device='cuda:0')
Epoch: [408][0/391]	Time 0.245 (0.245)	Data 0.221 (0.221)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [408][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.094)
Epoch: [408][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.919)
Epoch: [408][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.824)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0269, device='cuda:0')
Epoch: [409][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [409][100/391]	Time 0.015 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.535)
Epoch: [409][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.277)
Epoch: [409][300/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.050)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0122, device='cuda:0')
Epoch: [410][0/391]	Time 0.209 (0.209)	Data 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [410][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.824)
Epoch: [410][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 2.344 (9.876)
Epoch: [410][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.692)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [411][0/391]	Time 0.190 (0.190)	Data 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [411][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.017)
Epoch: [411][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.787)
Epoch: [411][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.673)
Test: [0/79]	Time 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0277, device='cuda:0')
Epoch: [412][0/391]	Time 0.196 (0.196)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [412][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.715)
Epoch: [412][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.806)
Epoch: [412][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.757)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0202, device='cuda:0')
Epoch: [413][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [413][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.839)
Epoch: [413][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.752)
Epoch: [413][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.738)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0095, device='cuda:0')
Epoch: [414][0/391]	Time 0.251 (0.251)	Data 0.231 (0.231)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [414][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (10.226)
Epoch: [414][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.826)
Epoch: [414][300/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.676)
Test: [0/79]	Time 0.201 (0.201)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0252, device='cuda:0')
Epoch: [415][0/391]	Time 0.201 (0.201)	Data 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [415][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.824)
Epoch: [415][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (10.012)
Epoch: [415][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.923)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [416][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [416][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.288)
Epoch: [416][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.958)
Epoch: [416][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.798)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [417][0/391]	Time 0.218 (0.218)	Data 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [417][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 17.969 (10.040)
Epoch: [417][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.040)
Epoch: [417][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.881)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0337, device='cuda:0')
Epoch: [418][0/391]	Time 0.196 (0.196)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [418][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (10.373)
Epoch: [418][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.841)
Epoch: [418][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 15.625 (9.827)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0330, device='cuda:0')
Epoch: [419][0/391]	Time 0.223 (0.223)	Data 0.202 (0.202)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [419][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.195)
Epoch: [419][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.942)
Epoch: [419][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.819)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0158, device='cuda:0')
Epoch: [420][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [420][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.878)
Epoch: [420][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.795)
Epoch: [420][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.772)
Test: [0/79]	Time 0.202 (0.202)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0180, device='cuda:0')
Epoch: [421][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [421][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.769)
Epoch: [421][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.748)
Epoch: [421][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.751)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0439, device='cuda:0')
Epoch: [422][0/391]	Time 0.217 (0.217)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [422][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.986)
Epoch: [422][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.919)
Epoch: [422][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.801)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [423][0/391]	Time 0.213 (0.213)	Data 0.196 (0.196)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [423][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (9.978)
Epoch: [423][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.857)
Epoch: [423][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.736)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0215, device='cuda:0')
Epoch: [424][0/391]	Time 0.218 (0.218)	Data 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [424][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.412)
Epoch: [424][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (10.226)
Epoch: [424][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.042)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0595, device='cuda:0')
Epoch: [425][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [425][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.048)
Epoch: [425][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.180)
Epoch: [425][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.084)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0178, device='cuda:0')
Epoch: [426][0/391]	Time 0.203 (0.203)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [426][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.793)
Epoch: [426][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.931)
Epoch: [426][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.798)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0165, device='cuda:0')
Epoch: [427][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [427][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.916)
Epoch: [427][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.938 (9.818)
Epoch: [427][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.808)
Test: [0/79]	Time 0.170 (0.170)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0246, device='cuda:0')
Epoch: [428][0/391]	Time 0.196 (0.196)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (15.625)
Epoch: [428][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.311)
Epoch: [428][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.024)
Epoch: [428][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.894)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0323, device='cuda:0')
Epoch: [429][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
Epoch: [429][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 16.406 (9.793)
Epoch: [429][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3023 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.989)
Epoch: [429][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.837)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0337, device='cuda:0')
Epoch: [430][0/391]	Time 0.208 (0.208)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [430][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.429)
Epoch: [430][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.799)
Epoch: [430][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.889)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0249, device='cuda:0')
Epoch: [431][0/391]	Time 0.192 (0.192)	Data 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [431][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.816)
Epoch: [431][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.698)
Epoch: [431][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.824)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0302, device='cuda:0')
Epoch: [432][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [432][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.094)
Epoch: [432][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.040)
Epoch: [432][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.876)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0308, device='cuda:0')
Epoch: [433][0/391]	Time 0.221 (0.221)	Data 0.204 (0.204)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [433][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.040)
Epoch: [433][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.764)
Epoch: [433][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.785)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0316, device='cuda:0')
Epoch: [434][0/391]	Time 0.197 (0.197)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [434][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.218)
Epoch: [434][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.946)
Epoch: [434][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.832)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0156, device='cuda:0')
Epoch: [435][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [435][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.071)
Epoch: [435][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.826)
Epoch: [435][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.886)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0256, device='cuda:0')
Epoch: [436][0/391]	Time 0.197 (0.197)	Data 0.179 (0.179)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [436][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.623)
Epoch: [436][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 6.250 (9.775)
Epoch: [436][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.712)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0153, device='cuda:0')
Epoch: [437][0/391]	Time 0.201 (0.201)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [437][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.257)
Epoch: [437][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.253)
Epoch: [437][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.076)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0449, device='cuda:0')
Epoch: [438][0/391]	Time 0.201 (0.201)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [438][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.319)
Epoch: [438][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.028)
Epoch: [438][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.912)
Test: [0/79]	Time 0.173 (0.173)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0270, device='cuda:0')
Epoch: [439][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [439][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.491)
Epoch: [439][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.581)
Epoch: [439][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.645)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0268, device='cuda:0')
Epoch: [440][0/391]	Time 0.202 (0.202)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [440][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.125)
Epoch: [440][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.931)
Epoch: [440][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.972)
Test: [0/79]	Time 0.212 (0.212)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0283, device='cuda:0')
Epoch: [441][0/391]	Time 0.233 (0.233)	Data 0.215 (0.215)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [441][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.893)
Epoch: [441][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.028)
Epoch: [441][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.827)
Test: [0/79]	Time 0.169 (0.169)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0242, device='cuda:0')
Epoch: [442][0/391]	Time 0.203 (0.203)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [442][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.715)
Epoch: [442][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.966)
Epoch: [442][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.962)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0477, device='cuda:0')
Epoch: [443][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [443][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.141)
Epoch: [443][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (10.075)
Epoch: [443][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.798)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0371, device='cuda:0')
Epoch: [444][0/391]	Time 0.201 (0.201)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [444][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.785)
Epoch: [444][200/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.873)
Epoch: [444][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.889)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0089, device='cuda:0')
Epoch: [445][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [445][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 3.125 (9.785)
Epoch: [445][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.942)
Epoch: [445][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.801)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0219, device='cuda:0')
Epoch: [446][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [446][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.528)
Epoch: [446][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (10.288)
Epoch: [446][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.094)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0096, device='cuda:0')
Epoch: [447][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [447][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.009)
Epoch: [447][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.896)
Epoch: [447][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.889)
Test: [0/79]	Time 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0373, device='cuda:0')
Epoch: [448][0/391]	Time 0.220 (0.220)	Data 0.200 (0.200)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [448][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.800)
Epoch: [448][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.896)
Epoch: [448][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.879)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0196, device='cuda:0')
Epoch: [449][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [449][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.816)
Epoch: [449][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.869)
Epoch: [449][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.842)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0215, device='cuda:0')
Epoch: [450][0/391]	Time 0.200 (0.200)	Data 0.179 (0.179)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [450][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.210)
Epoch: [450][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.098)
Epoch: [450][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.977)
Test: [0/79]	Time 0.220 (0.220)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [451][0/391]	Time 0.248 (0.248)	Data 0.228 (0.228)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [451][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.607)
Epoch: [451][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.608)
Epoch: [451][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.596)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [452][0/391]	Time 0.196 (0.196)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [452][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (9.808)
Epoch: [452][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.752)
Epoch: [452][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 17.188 (9.790)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0213, device='cuda:0')
Epoch: [453][0/391]	Time 0.231 (0.231)	Data 0.211 (0.211)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 16.406 (16.406)
Epoch: [453][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.048)
Epoch: [453][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.884)
Epoch: [453][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.915)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0191, device='cuda:0')
Epoch: [454][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [454][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.762)
Epoch: [454][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.919)
Epoch: [454][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.879)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [455][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [455][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.357)
Epoch: [455][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.071)
Epoch: [455][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.959)
Test: [0/79]	Time 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0263, device='cuda:0')
Epoch: [456][0/391]	Time 0.232 (0.232)	Data 0.213 (0.213)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [456][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.110)
Epoch: [456][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.970)
Epoch: [456][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.824)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0686, device='cuda:0')
Epoch: [457][0/391]	Time 0.195 (0.195)	Data 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [457][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.476)
Epoch: [457][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.511)
Epoch: [457][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.712)
Test: [0/79]	Time 0.194 (0.194)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0197, device='cuda:0')
Epoch: [458][0/391]	Time 0.216 (0.216)	Data 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [458][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.893)
Epoch: [458][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.896)
Epoch: [458][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.938)
Test: [0/79]	Time 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0106, device='cuda:0')
Epoch: [459][0/391]	Time 0.207 (0.207)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [459][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.056)
Epoch: [459][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.125)
Epoch: [459][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.894)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0523, device='cuda:0')
Epoch: [460][0/391]	Time 0.196 (0.196)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.125 (3.125)
Epoch: [460][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.723)
Epoch: [460][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 8.594 (9.923)
Epoch: [460][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 16.406 (9.902)
Test: [0/79]	Time 0.200 (0.200)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0374, device='cuda:0')
Epoch: [461][0/391]	Time 0.218 (0.218)	Data 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [461][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.272)
Epoch: [461][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.981)
Epoch: [461][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.892)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0276, device='cuda:0')
Epoch: [462][0/391]	Time 0.190 (0.190)	Data 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [462][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (10.071)
Epoch: [462][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.106)
Epoch: [462][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.873)
Test: [0/79]	Time 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0718, device='cuda:0')
Epoch: [463][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (3.906)
Epoch: [463][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.762)
Epoch: [463][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.028)
Epoch: [463][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.904)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0061, device='cuda:0')
Epoch: [464][0/391]	Time 0.193 (0.193)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [464][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (9.963)
Epoch: [464][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.989)
Epoch: [464][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.866)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0146, device='cuda:0')
Epoch: [465][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [465][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.257)
Epoch: [465][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.931)
Epoch: [465][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.764)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0241, device='cuda:0')
Epoch: [466][0/391]	Time 0.209 (0.209)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [466][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.646)
Epoch: [466][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.717)
Epoch: [466][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.707)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0169, device='cuda:0')
Epoch: [467][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [467][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.630)
Epoch: [467][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 3.906 (9.659)
Epoch: [467][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.723)
Test: [0/79]	Time 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0431, device='cuda:0')
Epoch: [468][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [468][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.421)
Epoch: [468][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (9.639)
Epoch: [468][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.746)
Test: [0/79]	Time 0.215 (0.215)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0164, device='cuda:0')
Epoch: [469][0/391]	Time 0.207 (0.207)	Data 0.189 (0.189)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
Epoch: [469][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (9.963)
Epoch: [469][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.830)
Epoch: [469][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.845)
Test: [0/79]	Time 0.189 (0.189)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0125, device='cuda:0')
Epoch: [470][0/391]	Time 0.203 (0.203)	Data 0.183 (0.183)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [470][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.504)
Epoch: [470][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.844 (10.168)
Epoch: [470][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (10.042)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0199, device='cuda:0')
Epoch: [471][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [471][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.963)
Epoch: [471][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.946)
Epoch: [471][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.712)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0128, device='cuda:0')
Epoch: [472][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [472][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.048)
Epoch: [472][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 10.156 (10.024)
Epoch: [472][300/391]	Time 0.015 (0.014)	Data 0.000 (0.001)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 16.406 (10.021)
Test: [0/79]	Time 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0073, device='cuda:0')
Epoch: [473][0/391]	Time 0.191 (0.191)	Data 0.172 (0.172)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [473][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (10.272)
Epoch: [473][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.989)
Epoch: [473][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.884)
Test: [0/79]	Time 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0240, device='cuda:0')
Epoch: [474][0/391]	Time 0.206 (0.206)	Data 0.187 (0.187)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 10.938 (10.938)
Epoch: [474][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.746)
Epoch: [474][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.849)
Epoch: [474][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.806)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0390, device='cuda:0')
Epoch: [475][0/391]	Time 0.197 (0.197)	Data 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (6.250)
Epoch: [475][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (10.203)
Epoch: [475][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (10.044)
Epoch: [475][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.982)
Test: [0/79]	Time 0.197 (0.197)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0411, device='cuda:0')
Epoch: [476][0/391]	Time 0.195 (0.195)	Data 0.176 (0.176)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [476][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.568)
Epoch: [476][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.705)
Epoch: [476][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.679)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (7.031)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0370, device='cuda:0')
Epoch: [477][0/391]	Time 0.205 (0.205)	Data 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [477][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.847)
Epoch: [477][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.946)
Epoch: [477][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.814)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0286, device='cuda:0')
Epoch: [478][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [478][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.831)
Epoch: [478][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.620)
Epoch: [478][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (9.606)
Test: [0/79]	Time 0.199 (0.199)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0391, device='cuda:0')
Epoch: [479][0/391]	Time 0.209 (0.209)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [479][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.901)
Epoch: [479][200/391]	Time 0.016 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.892)
Epoch: [479][300/391]	Time 0.016 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.886)
Test: [0/79]	Time 0.188 (0.188)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0120, device='cuda:0')
Epoch: [480][0/391]	Time 0.211 (0.211)	Data 0.192 (0.192)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
Epoch: [480][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (10.203)
Epoch: [480][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 7.812 (10.036)
Epoch: [480][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 15.625 (9.920)
Test: [0/79]	Time 0.227 (0.227)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [481][0/391]	Time 0.232 (0.232)	Data 0.212 (0.212)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (12.500)
Epoch: [481][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.878)
Epoch: [481][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.016)
Epoch: [481][300/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.925)
Test: [0/79]	Time 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0067, device='cuda:0')
Epoch: [482][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [482][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.723)
Epoch: [482][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.853)
Epoch: [482][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.754)
Test: [0/79]	Time 0.174 (0.174)	Loss 2.3025 (2.3025) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0349, device='cuda:0')
Epoch: [483][0/391]	Time 0.200 (0.200)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [483][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.110)
Epoch: [483][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (10.113)
Epoch: [483][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.904)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0238, device='cuda:0')
Epoch: [484][0/391]	Time 0.200 (0.200)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 4.688 (4.688)
Epoch: [484][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (10.141)
Epoch: [484][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.160)
Epoch: [484][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 11.719 (9.967)
Test: [0/79]	Time 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0240, device='cuda:0')
Epoch: [485][0/391]	Time 0.210 (0.210)	Data 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [485][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 14.062 (10.102)
Epoch: [485][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.892)
Epoch: [485][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 16.406 (10.019)
Test: [0/79]	Time 0.193 (0.193)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0335, device='cuda:0')
Epoch: [486][0/391]	Time 0.223 (0.223)	Data 0.205 (0.205)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [486][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (10.381)
Epoch: [486][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.168)
Epoch: [486][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.964)
Test: [0/79]	Time 0.178 (0.178)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0159, device='cuda:0')
Epoch: [487][0/391]	Time 0.209 (0.209)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [487][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.886)
Epoch: [487][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.849)
Epoch: [487][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 6.250 (9.741)
Test: [0/79]	Time 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0283, device='cuda:0')
Epoch: [488][0/391]	Time 0.198 (0.198)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [488][100/391]	Time 0.016 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.087)
Epoch: [488][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (9.744)
Epoch: [488][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.842)
Test: [0/79]	Time 0.165 (0.165)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (8.594)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [489][0/391]	Time 0.208 (0.208)	Data 0.190 (0.190)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (14.062)
Epoch: [489][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.847)
Epoch: [489][200/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.966)
Epoch: [489][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.946)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0229, device='cuda:0')
Epoch: [490][0/391]	Time 0.199 (0.199)	Data 0.180 (0.180)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
Epoch: [490][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.282)
Epoch: [490][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.721)
Epoch: [490][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 13.281 (9.772)
Test: [0/79]	Time 0.165 (0.165)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0200, device='cuda:0')
Epoch: [491][0/391]	Time 0.199 (0.199)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [491][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.638)
Epoch: [491][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (9.795)
Epoch: [491][300/391]	Time 0.013 (0.013)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.671)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0221, device='cuda:0')
Epoch: [492][0/391]	Time 0.195 (0.195)	Data 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 13.281 (13.281)
Epoch: [492][100/391]	Time 0.014 (0.015)	Data 0.000 (0.002)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 14.062 (10.149)
Epoch: [492][200/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.911)
Epoch: [492][300/391]	Time 0.016 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.705)
Test: [0/79]	Time 0.184 (0.184)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0157, device='cuda:0')
Epoch: [493][0/391]	Time 0.205 (0.205)	Data 0.187 (0.187)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 5.469 (5.469)
Epoch: [493][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.094)
Epoch: [493][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (10.020)
Epoch: [493][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.990)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0271, device='cuda:0')
Epoch: [494][0/391]	Time 0.214 (0.214)	Data 0.195 (0.195)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 14.844 (14.844)
Epoch: [494][100/391]	Time 0.014 (0.016)	Data 0.000 (0.002)	Loss 2.3024 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.831)
Epoch: [494][200/391]	Time 0.014 (0.015)	Data 0.000 (0.001)	Loss 2.3028 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.834)
Epoch: [494][300/391]	Time 0.014 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 12.500 (9.772)
Test: [0/79]	Time 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0597, device='cuda:0')
Epoch: [495][0/391]	Time 0.201 (0.201)	Data 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [495][100/391]	Time 0.016 (0.016)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.808)
Epoch: [495][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.814)
Epoch: [495][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.031 (9.697)
Test: [0/79]	Time 0.177 (0.177)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (7.812)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0161, device='cuda:0')
Epoch: [496][0/391]	Time 0.193 (0.193)	Data 0.174 (0.174)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [496][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (10.141)
Epoch: [496][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.938 (9.954)
Epoch: [496][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.824)
Test: [0/79]	Time 0.181 (0.181)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0227, device='cuda:0')
Epoch: [497][0/391]	Time 0.203 (0.203)	Data 0.186 (0.186)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [497][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.800)
Epoch: [497][200/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 12.500 (9.946)
Epoch: [497][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.666)
Test: [0/79]	Time 0.198 (0.198)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0050, device='cuda:0')
Epoch: [498][0/391]	Time 0.206 (0.206)	Data 0.188 (0.188)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 9.375 (9.375)
Epoch: [498][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.955)
Epoch: [498][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 8.594 (9.841)
Epoch: [498][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (9.741)
Test: [0/79]	Time 0.175 (0.175)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
current lr 1.00000e-04
Grad=  tensor(0.0360, device='cuda:0')
Epoch: [499][0/391]	Time 0.203 (0.203)	Data 0.185 (0.185)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 10.156 (10.156)
Epoch: [499][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (9.947)
Epoch: [499][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3027 (2.3026) ([2.303]+[0.000])	Prec@1 7.812 (9.900)
Epoch: [499][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 2.3025 (2.3026) ([2.302]+[0.000])	Prec@1 9.375 (9.814)
Test: [0/79]	Time 0.182 (0.182)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000

 Elapsed time for training  1:14:52.073891

 sparsity of   [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375, 0.958984375]
Total parameter pruned: 13467884.0 (unstructured) 13462974 (structured)
Test: [0/79]	Time 0.159 (0.159)	Loss 2.3026 (2.3026) ([2.303]+[0.000])	Prec@1 11.719 (11.719)
 * Prec@1 10.000
Best accuracy:  10.0
