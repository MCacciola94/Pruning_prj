V1.1.0-_resnet20_Cifar10_lr0.1_l10.0_a0.1_e3+1_bs128_t0.05_tstr0.05_m0.9_wd0.0005_mlstemp3_Mscl1.0_structconvs_and_batchnorm
Files already downloaded and verified
M values:
 {0: 1.6352988481521606, 1: 1.169867753982544, 2: 0.9106662273406982, 3: 1.2466709613800049, 4: 0.6928866505622864, 5: 0.9905681610107422, 6: 0.9305789470672607, 7: 0.9705817699432373, 8: 0.895604133605957, 9: 0.8712202906608582, 10: 0.8085764646530151, 11: 0.942848265171051, 12: 0.7068805694580078, 13: 0.8874967098236084, 14: 1.2306768894195557, 15: 0.8342021703720093, 16: 1.0810145139694214, 17: 0.9741626381874084, 18: 2.1248533725738525}
current lr 1.00000e-01
Grad=  tensor(56.1955, device='cuda:0')
Epoch: [0][0/391]	Time 1.757 (1.757)	Data 0.170 (0.170)	Loss 39.7018 (39.7018) ([4.269]+[35.433])	Prec@1 7.031 (7.031)
Epoch: [0][100/391]	Time 0.032 (0.047)	Data 0.000 (0.002)	Loss 3.9950 (14.0854) ([1.822]+[2.173])	Prec@1 24.219 (24.722)
Epoch: [0][200/391]	Time 0.032 (0.040)	Data 0.000 (0.001)	Loss 2.9105 (8.6523) ([1.790]+[1.121])	Prec@1 32.031 (28.001)
Epoch: [0][300/391]	Time 0.033 (0.037)	Data 0.000 (0.001)	Loss 2.3972 (6.6103) ([1.834]+[0.563])	Prec@1 25.000 (28.343)
Test: [0/79]	Time 0.206 (0.206)	Loss 3.0758 (3.0758) ([2.478]+[0.598])	Prec@1 23.438 (23.438)
 * Prec@1 23.070
current lr 1.00000e-01
Grad=  tensor(2.2458, device='cuda:0')
Epoch: [1][0/391]	Time 0.270 (0.270)	Data 0.228 (0.228)	Loss 2.4171 (2.4171) ([1.819]+[0.598])	Prec@1 32.812 (32.812)
Epoch: [1][100/391]	Time 0.029 (0.034)	Data 0.000 (0.002)	Loss 2.5527 (2.3385) ([1.997]+[0.556])	Prec@1 22.656 (30.801)
Epoch: [1][200/391]	Time 0.029 (0.032)	Data 0.000 (0.001)	Loss 2.4092 (2.2917) ([1.950]+[0.459])	Prec@1 19.531 (30.857)
Epoch: [1][300/391]	Time 0.029 (0.031)	Data 0.000 (0.001)	Loss 2.2105 (2.2645) ([1.751]+[0.460])	Prec@1 35.156 (30.931)
Test: [0/79]	Time 0.249 (0.249)	Loss 2.1793 (2.1793) ([1.721]+[0.459])	Prec@1 30.469 (30.469)
 * Prec@1 33.450
current lr 1.00000e-01
Grad=  tensor(1.8862, device='cuda:0')
Epoch: [2][0/391]	Time 0.286 (0.286)	Data 0.245 (0.245)	Loss 2.1516 (2.1516) ([1.693]+[0.459])	Prec@1 36.719 (36.719)
Epoch: [2][100/391]	Time 0.032 (0.034)	Data 0.000 (0.003)	Loss 2.2359 (2.2245) ([1.771]+[0.465])	Prec@1 29.688 (30.941)
Epoch: [2][200/391]	Time 0.033 (0.033)	Data 0.000 (0.001)	Loss 2.1786 (2.2275) ([1.745]+[0.433])	Prec@1 28.125 (31.301)
Epoch: [2][300/391]	Time 0.031 (0.032)	Data 0.000 (0.001)	Loss 2.2660 (2.2297) ([1.788]+[0.478])	Prec@1 25.781 (31.170)
Test: [0/79]	Time 0.249 (0.249)	Loss 2.0963 (2.0963) ([1.658]+[0.438])	Prec@1 38.281 (38.281)
 * Prec@1 34.300

 Elapsed time for training  0:00:42.636619
Total parameter pruned: 0.0 (unstructured) 0 (structured)
Test: [0/391]	Time 0.251 (0.251)	Loss 1.7994 (1.7994) ([1.799]+[0.000])	Prec@1 27.344 (27.344)
Test: [100/391]	Time 0.019 (0.010)	Loss 1.7247 (1.7195) ([1.725]+[0.000])	Prec@1 28.906 (33.888)
Test: [200/391]	Time 0.020 (0.009)	Loss 1.7369 (1.7202) ([1.737]+[0.000])	Prec@1 39.844 (34.204)
Test: [300/391]	Time 0.012 (0.008)	Loss 1.6191 (1.7262) ([1.619]+[0.000])	Prec@1 39.844 (34.032)
 * Prec@1 33.880
Test: [0/391]	Time 0.252 (0.252)	Loss 1.7840 (1.7840) ([1.784]+[0.000])	Prec@1 32.031 (32.031)
Test: [100/391]	Time 0.019 (0.010)	Loss 1.7895 (1.7331) ([1.790]+[0.000])	Prec@1 35.938 (34.120)
Test: [200/391]	Time 0.017 (0.009)	Loss 1.7285 (1.7278) ([1.728]+[0.000])	Prec@1 30.469 (34.014)
Test: [300/391]	Time 0.016 (0.009)	Loss 1.6918 (1.7288) ([1.692]+[0.000])	Prec@1 42.188 (34.064)
 * Prec@1 33.992
Test: [0/391]	Time 0.248 (0.248)	Loss 1.8553 (1.8553) ([1.855]+[0.000])	Prec@1 34.375 (34.375)
Test: [100/391]	Time 0.008 (0.010)	Loss 1.9367 (1.8405) ([1.937]+[0.000])	Prec@1 28.125 (30.894)
Test: [200/391]	Time 0.004 (0.009)	Loss 1.7622 (1.8386) ([1.762]+[0.000])	Prec@1 35.156 (30.889)
Test: [300/391]	Time 0.004 (0.008)	Loss 1.8082 (1.8388) ([1.808]+[0.000])	Prec@1 26.562 (30.793)
 * Prec@1 30.732
Test: [0/391]	Time 0.250 (0.250)	Loss 1.8279 (1.8279) ([1.828]+[0.000])	Prec@1 24.219 (24.219)
Test: [100/391]	Time 0.015 (0.010)	Loss 2.0036 (1.8441) ([2.004]+[0.000])	Prec@1 26.562 (30.670)
Test: [200/391]	Time 0.013 (0.009)	Loss 1.7456 (1.8396) ([1.746]+[0.000])	Prec@1 31.250 (30.881)
Test: [300/391]	Time 0.011 (0.008)	Loss 1.9233 (1.8425) ([1.923]+[0.000])	Prec@1 23.438 (30.861)
 * Prec@1 30.858
Test: [0/391]	Time 0.248 (0.248)	Loss 1.8574 (1.8574) ([1.857]+[0.000])	Prec@1 31.250 (31.250)
Test: [100/391]	Time 0.017 (0.010)	Loss 1.8575 (1.8234) ([1.857]+[0.000])	Prec@1 24.219 (32.054)
Test: [200/391]	Time 0.017 (0.009)	Loss 1.8917 (1.8336) ([1.892]+[0.000])	Prec@1 31.250 (31.542)
Test: [300/391]	Time 0.005 (0.008)	Loss 1.8267 (1.8354) ([1.827]+[0.000])	Prec@1 28.906 (31.232)
 * Prec@1 31.222
Test: [0/391]	Time 0.245 (0.245)	Loss 1.9361 (1.9361) ([1.936]+[0.000])	Prec@1 28.906 (28.906)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.7884 (1.8413) ([1.788]+[0.000])	Prec@1 35.938 (31.134)
Test: [200/391]	Time 0.014 (0.009)	Loss 1.9727 (1.8421) ([1.973]+[0.000])	Prec@1 28.906 (30.752)
Test: [300/391]	Time 0.016 (0.008)	Loss 1.8268 (1.8414) ([1.827]+[0.000])	Prec@1 25.781 (30.926)
 * Prec@1 30.916
Test: [0/391]	Time 0.244 (0.244)	Loss 1.8659 (1.8659) ([1.866]+[0.000])	Prec@1 32.031 (32.031)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.7321 (1.8203) ([1.732]+[0.000])	Prec@1 32.812 (31.250)
Test: [200/391]	Time 0.017 (0.009)	Loss 1.7328 (1.8334) ([1.733]+[0.000])	Prec@1 34.375 (31.095)
Test: [300/391]	Time 0.017 (0.008)	Loss 1.9129 (1.8360) ([1.913]+[0.000])	Prec@1 28.125 (31.037)
 * Prec@1 30.818
Test: [0/391]	Time 0.247 (0.247)	Loss 2.0380 (2.0380) ([2.038]+[0.000])	Prec@1 27.344 (27.344)
Test: [100/391]	Time 0.017 (0.010)	Loss 1.8511 (1.8312) ([1.851]+[0.000])	Prec@1 27.344 (30.724)
Test: [200/391]	Time 0.016 (0.009)	Loss 1.7677 (1.8365) ([1.768]+[0.000])	Prec@1 28.125 (30.900)
Test: [300/391]	Time 0.017 (0.008)	Loss 1.7721 (1.8382) ([1.772]+[0.000])	Prec@1 37.500 (30.907)
 * Prec@1 30.854
Test: [0/391]	Time 0.249 (0.249)	Loss 1.8457 (1.8457) ([1.846]+[0.000])	Prec@1 26.562 (26.562)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.7902 (1.8515) ([1.790]+[0.000])	Prec@1 27.344 (30.531)
Test: [200/391]	Time 0.012 (0.009)	Loss 1.7171 (1.8448) ([1.717]+[0.000])	Prec@1 30.469 (31.067)
Test: [300/391]	Time 0.015 (0.008)	Loss 1.7180 (1.8415) ([1.718]+[0.000])	Prec@1 33.594 (31.001)
 * Prec@1 31.066
Test: [0/391]	Time 0.250 (0.250)	Loss 1.8563 (1.8563) ([1.856]+[0.000])	Prec@1 26.562 (26.562)
Test: [100/391]	Time 0.017 (0.010)	Loss 1.8710 (1.8375) ([1.871]+[0.000])	Prec@1 28.906 (30.933)
Test: [200/391]	Time 0.014 (0.009)	Loss 1.8053 (1.8396) ([1.805]+[0.000])	Prec@1 27.344 (31.040)
Test: [300/391]	Time 0.015 (0.008)	Loss 1.7885 (1.8420) ([1.788]+[0.000])	Prec@1 37.500 (30.791)
 * Prec@1 30.846
Test: [0/391]	Time 0.253 (0.253)	Loss 1.9624 (1.9624) ([1.962]+[0.000])	Prec@1 29.688 (29.688)
Test: [100/391]	Time 0.016 (0.011)	Loss 1.6893 (1.8496) ([1.689]+[0.000])	Prec@1 32.812 (30.090)
Test: [200/391]	Time 0.017 (0.009)	Loss 1.7859 (1.8472) ([1.786]+[0.000])	Prec@1 32.812 (30.550)
Test: [300/391]	Time 0.017 (0.009)	Loss 1.8941 (1.8430) ([1.894]+[0.000])	Prec@1 28.125 (30.801)
 * Prec@1 31.064
Test: [0/391]	Time 0.249 (0.249)	Loss 1.8151 (1.8151) ([1.815]+[0.000])	Prec@1 31.250 (31.250)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.8016 (1.8395) ([1.802]+[0.000])	Prec@1 29.688 (30.531)
Test: [200/391]	Time 0.013 (0.009)	Loss 1.6802 (1.8351) ([1.680]+[0.000])	Prec@1 30.469 (30.749)
Test: [300/391]	Time 0.017 (0.009)	Loss 1.8738 (1.8358) ([1.874]+[0.000])	Prec@1 27.344 (30.827)
 * Prec@1 30.670

Total parameter pruned: 266932.000023745 (unstructured) 262971 (structured)

Test: [0/79]	Time 0.242 (0.242)	Loss 2.0157 (2.0157) ([1.771]+[0.245])	Prec@1 33.594 (33.594)
 * Prec@1 31.270

 Total elapsed time  0:01:24.010304 
 FINETUNING

Test: [0/391]	Time 0.243 (0.243)	Loss 1.9175 (1.9175) ([1.918]+[0.000])	Prec@1 28.125 (28.125)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.7497 (1.8404) ([1.750]+[0.000])	Prec@1 35.938 (31.745)
Test: [200/391]	Time 0.008 (0.009)	Loss 1.8774 (1.8389) ([1.877]+[0.000])	Prec@1 30.469 (31.437)
Test: [300/391]	Time 0.005 (0.009)	Loss 1.6947 (1.8416) ([1.695]+[0.000])	Prec@1 31.250 (31.164)
 * Prec@1 31.038
Test: [0/391]	Time 0.251 (0.251)	Loss 1.6461 (1.6461) ([1.646]+[0.000])	Prec@1 35.156 (35.156)
Test: [100/391]	Time 0.014 (0.010)	Loss 1.7962 (1.8404) ([1.796]+[0.000])	Prec@1 28.906 (30.902)
Test: [200/391]	Time 0.014 (0.009)	Loss 1.8075 (1.8350) ([1.808]+[0.000])	Prec@1 31.250 (31.176)
Test: [300/391]	Time 0.014 (0.009)	Loss 1.8464 (1.8411) ([1.846]+[0.000])	Prec@1 29.688 (30.848)
 * Prec@1 30.886
Test: [0/391]	Time 0.287 (0.287)	Loss 2.0331 (2.0331) ([2.033]+[0.000])	Prec@1 22.656 (22.656)
Test: [100/391]	Time 0.004 (0.010)	Loss 2.0359 (2.0160) ([2.036]+[0.000])	Prec@1 18.750 (25.696)
Test: [200/391]	Time 0.004 (0.009)	Loss 1.9915 (2.0153) ([1.991]+[0.000])	Prec@1 23.438 (26.092)
Test: [300/391]	Time 0.005 (0.008)	Loss 1.9774 (2.0187) ([1.977]+[0.000])	Prec@1 29.688 (25.864)
 * Prec@1 25.972
Test: [0/391]	Time 0.250 (0.250)	Loss 1.9283 (1.9283) ([1.928]+[0.000])	Prec@1 28.125 (28.125)
Test: [100/391]	Time 0.015 (0.010)	Loss 1.9950 (2.0145) ([1.995]+[0.000])	Prec@1 28.125 (25.851)
Test: [200/391]	Time 0.009 (0.009)	Loss 1.9986 (2.0122) ([1.999]+[0.000])	Prec@1 24.219 (26.081)
Test: [300/391]	Time 0.008 (0.008)	Loss 2.1036 (2.0132) ([2.104]+[0.000])	Prec@1 21.875 (26.067)
 * Prec@1 25.990
Test: [0/391]	Time 0.244 (0.244)	Loss 2.0562 (2.0562) ([2.056]+[0.000])	Prec@1 26.562 (26.562)
Test: [100/391]	Time 0.015 (0.010)	Loss 2.0054 (2.0118) ([2.005]+[0.000])	Prec@1 22.656 (26.307)
Test: [200/391]	Time 0.016 (0.009)	Loss 2.0317 (2.0187) ([2.032]+[0.000])	Prec@1 27.344 (25.894)
Test: [300/391]	Time 0.018 (0.008)	Loss 2.0070 (2.0169) ([2.007]+[0.000])	Prec@1 25.000 (25.984)
 * Prec@1 26.080
Test: [0/391]	Time 0.255 (0.255)	Loss 1.9996 (1.9996) ([2.000]+[0.000])	Prec@1 25.781 (25.781)
Test: [100/391]	Time 0.017 (0.010)	Loss 2.0751 (2.0164) ([2.075]+[0.000])	Prec@1 25.000 (26.238)
Test: [200/391]	Time 0.016 (0.009)	Loss 2.0059 (2.0134) ([2.006]+[0.000])	Prec@1 36.719 (26.322)
Test: [300/391]	Time 0.023 (0.008)	Loss 1.9454 (2.0184) ([1.945]+[0.000])	Prec@1 23.438 (26.111)
 * Prec@1 26.100
Test: [0/391]	Time 0.253 (0.253)	Loss 2.0594 (2.0594) ([2.059]+[0.000])	Prec@1 18.750 (18.750)
Test: [100/391]	Time 0.014 (0.010)	Loss 2.0163 (2.0078) ([2.016]+[0.000])	Prec@1 27.344 (26.431)
Test: [200/391]	Time 0.008 (0.009)	Loss 2.0494 (2.0182) ([2.049]+[0.000])	Prec@1 25.000 (26.255)
Test: [300/391]	Time 0.004 (0.008)	Loss 1.9464 (2.0181) ([1.946]+[0.000])	Prec@1 27.344 (26.111)
 * Prec@1 26.078
Test: [0/391]	Time 0.252 (0.252)	Loss 2.0710 (2.0710) ([2.071]+[0.000])	Prec@1 24.219 (24.219)
Test: [100/391]	Time 0.016 (0.010)	Loss 2.0112 (2.0065) ([2.011]+[0.000])	Prec@1 25.000 (26.462)
Test: [200/391]	Time 0.016 (0.009)	Loss 1.9755 (2.0128) ([1.975]+[0.000])	Prec@1 31.250 (26.232)
Test: [300/391]	Time 0.016 (0.008)	Loss 2.0642 (2.0152) ([2.064]+[0.000])	Prec@1 24.219 (26.090)
 * Prec@1 26.164
Test: [0/391]	Time 0.254 (0.254)	Loss 1.9781 (1.9781) ([1.978]+[0.000])	Prec@1 27.344 (27.344)
Test: [100/391]	Time 0.016 (0.010)	Loss 1.9786 (2.0103) ([1.979]+[0.000])	Prec@1 30.469 (26.238)
Test: [200/391]	Time 0.015 (0.009)	Loss 1.8754 (2.0156) ([1.875]+[0.000])	Prec@1 31.250 (26.263)
Test: [300/391]	Time 0.016 (0.008)	Loss 1.9420 (2.0177) ([1.942]+[0.000])	Prec@1 26.562 (26.075)
 * Prec@1 26.090
Test: [0/391]	Time 0.249 (0.249)	Loss 1.9549 (1.9549) ([1.955]+[0.000])	Prec@1 21.094 (21.094)
Test: [100/391]	Time 0.013 (0.010)	Loss 1.9855 (2.0261) ([1.985]+[0.000])	Prec@1 32.031 (26.354)
Test: [200/391]	Time 0.011 (0.009)	Loss 2.0010 (2.0224) ([2.001]+[0.000])	Prec@1 24.219 (26.189)
Test: [300/391]	Time 0.005 (0.008)	Loss 2.0714 (2.0173) ([2.071]+[0.000])	Prec@1 30.469 (26.259)
 * Prec@1 26.112
Test: [0/391]	Time 0.248 (0.248)	Loss 2.0250 (2.0250) ([2.025]+[0.000])	Prec@1 29.688 (29.688)
Test: [100/391]	Time 0.017 (0.010)	Loss 2.0544 (2.0234) ([2.054]+[0.000])	Prec@1 25.000 (25.874)
Test: [200/391]	Time 0.011 (0.009)	Loss 2.1089 (2.0213) ([2.109]+[0.000])	Prec@1 24.219 (26.158)
Test: [300/391]	Time 0.005 (0.008)	Loss 1.8912 (2.0167) ([1.891]+[0.000])	Prec@1 24.219 (26.241)
 * Prec@1 26.258
Test: [0/391]	Time 0.250 (0.250)	Loss 2.0802 (2.0802) ([2.080]+[0.000])	Prec@1 19.531 (19.531)
Test: [100/391]	Time 0.016 (0.010)	Loss 2.0223 (2.0290) ([2.022]+[0.000])	Prec@1 22.656 (25.472)
Test: [200/391]	Time 0.016 (0.009)	Loss 1.9674 (2.0212) ([1.967]+[0.000])	Prec@1 31.250 (25.999)
Test: [300/391]	Time 0.016 (0.008)	Loss 2.0058 (2.0200) ([2.006]+[0.000])	Prec@1 23.438 (26.163)
 * Prec@1 26.322

Total parameter pruned: 266932.000023745 (unstructured) 262971 (structured)

Test: [0/79]	Time 0.249 (0.249)	Loss 2.1632 (2.1632) ([1.993]+[0.171])	Prec@1 32.031 (32.031)
 * Prec@1 26.630
current lr 1.00000e-01
Grad=  tensor(1.5726, device='cuda:0')
Epoch: [3][0/391]	Time 0.260 (0.260)	Data 0.238 (0.238)	Loss 1.8671 (1.8671) ([1.867]+[0.000])	Prec@1 37.500 (37.500)
Epoch: [3][100/391]	Time 0.013 (0.016)	Data 0.000 (0.002)	Loss 1.7184 (1.7177) ([1.718]+[0.000])	Prec@1 40.625 (33.888)
Epoch: [3][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 1.6358 (1.7082) ([1.636]+[0.000])	Prec@1 33.594 (34.083)
Epoch: [3][300/391]	Time 0.012 (0.014)	Data 0.000 (0.001)	Loss 1.6913 (1.6943) ([1.691]+[0.000])	Prec@1 32.812 (34.551)
Test: [0/79]	Time 0.240 (0.240)	Loss 1.6166 (1.6166) ([1.617]+[0.000])	Prec@1 38.281 (38.281)
 * Prec@1 36.810

 Elapsed time for training  0:02:12.036756
Total parameter pruned: 265388.0000003055 (unstructured) 265131 (structured)
Test: [0/79]	Time 0.235 (0.235)	Loss 1.6166 (1.6166) ([1.617]+[0.000])	Prec@1 38.281 (38.281)
 * Prec@1 36.810
Best accuracy:  36.81
Traceback (most recent call last):
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 182, in <module>
    main()
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 179, in main
    grid_search.run()
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 160, in run
    trainer_pr.validate(reg_on = False)
  File "/local1/caccmatt/Pruning_prj/trainer.py", line 266, in validate
    output = self.model(input_var)
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/local1/caccmatt/Pruning_prj/resnet_pruned.py", line 161, in forward
    out = self.layer3(out)
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/x86_64-unknown-linux_ol8-gnu/anaconda-2022.10/envs/pytorch-1.12.1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/local1/caccmatt/Pruning_prj/resnet_pruned.py", line 90, in forward
    out=self.incompatible_sum(skip_x,out)
  File "/local1/caccmatt/Pruning_prj/resnet_pruned.py", line 127, in incompatible_sum
    t_aux[:,final_idxs_skip_x,:]=skip_x[:,shortcut_only_unpruned,:]
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
