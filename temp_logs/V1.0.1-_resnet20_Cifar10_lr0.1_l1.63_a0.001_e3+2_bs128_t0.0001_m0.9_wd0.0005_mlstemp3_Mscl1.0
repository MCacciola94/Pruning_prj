V1.0.1-_resnet20_Cifar10_lr0.1_l1.63_a0.001_e3+2_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 1.6352988481521606, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 1.169867753982544, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.9106662273406982, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.7931119203567505, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.654869556427002, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.9789378643035889, Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.7008146047592163, Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.6066990494728088, Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.6057915091514587, Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.6954296231269836, Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.40856727957725525, Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.7257518768310547, Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5365347862243652, Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.48648884892463684, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5309917330741882, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.39136967062950134, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.41638830304145813, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.43563324213027954, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.300857812166214, Linear(in_features=64, out_features=10, bias=True): 1.863915205001831}
current lr 1.00000e-01
Grad=  tensor(36.7976, device='cuda:0')
Epoch: [0][0/391]	Time 1.710 (1.710)	Data 0.147 (0.147)	Loss 4.4349 (4.4349) ([3.479]+[0.956])	Prec@1 9.375 (9.375)
Epoch: [0][100/391]	Time 0.032 (0.047)	Data 0.000 (0.002)	Loss 2.2400 (2.6933) ([1.641]+[0.599])	Prec@1 38.281 (29.038)
Epoch: [0][200/391]	Time 0.032 (0.039)	Data 0.000 (0.001)	Loss 2.0180 (2.4124) ([1.495]+[0.523])	Prec@1 43.750 (35.230)
Epoch: [0][300/391]	Time 0.031 (0.037)	Data 0.000 (0.001)	Loss 1.7736 (2.2365) ([1.299]+[0.475])	Prec@1 49.219 (39.815)
Test: [0/79]	Time 0.121 (0.121)	Loss 1.9135 (1.9135) ([1.473]+[0.441])	Prec@1 48.438 (48.438)
 * Prec@1 49.400
current lr 1.00000e-01
Grad=  tensor(1.0710, device='cuda:0')
Epoch: [1][0/391]	Time 0.156 (0.156)	Data 0.115 (0.115)	Loss 1.6738 (1.6738) ([1.233]+[0.441])	Prec@1 54.688 (54.688)
Epoch: [1][100/391]	Time 0.028 (0.032)	Data 0.000 (0.001)	Loss 1.5419 (1.5886) ([1.133]+[0.409])	Prec@1 60.938 (58.161)
Epoch: [1][200/391]	Time 0.029 (0.030)	Data 0.000 (0.001)	Loss 1.5275 (1.5259) ([1.145]+[0.382])	Prec@1 55.469 (59.993)
Epoch: [1][300/391]	Time 0.032 (0.030)	Data 0.000 (0.001)	Loss 1.2417 (1.4842) ([0.881]+[0.360])	Prec@1 69.531 (61.150)
Test: [0/79]	Time 0.153 (0.153)	Loss 1.5358 (1.5358) ([1.191]+[0.345])	Prec@1 57.812 (57.812)
 * Prec@1 60.460
current lr 1.00000e-01
Grad=  tensor(1.8061, device='cuda:0')
Epoch: [2][0/391]	Time 0.170 (0.170)	Data 0.131 (0.131)	Loss 1.5814 (1.5814) ([1.237]+[0.345])	Prec@1 57.031 (57.031)
Epoch: [2][100/391]	Time 0.032 (0.033)	Data 0.000 (0.001)	Loss 1.3752 (1.2621) ([1.042]+[0.333])	Prec@1 60.938 (67.141)
Epoch: [2][200/391]	Time 0.029 (0.032)	Data 0.000 (0.001)	Loss 1.2541 (1.2454) ([0.932]+[0.322])	Prec@1 68.750 (67.669)
Epoch: [2][300/391]	Time 0.029 (0.031)	Data 0.000 (0.001)	Loss 1.2594 (1.2272) ([0.945]+[0.314])	Prec@1 67.188 (68.265)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.4135 (1.4135) ([1.106]+[0.307])	Prec@1 60.156 (60.156)
 * Prec@1 60.100

 Elapsed time for training  0:00:40.797107
Total parameter pruned: 0.0 (unstructured) 0 (structured)

max weight is  tensor([0.7663, 0.8526, 1.0277, 0.7678, 0.5163, 0.4229, 0.9806, 1.4204, 0.9992,
        0.2600, 0.6346, 0.4949, 0.4472, 1.0321, 0.7617, 0.9217],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.5077, 0.4659, 0.6253, 0.2083, 0.6112, 0.7560, 0.2407, 0.5127, 0.5545,
        0.6861, 0.6540, 0.5862, 0.6026, 0.5075, 0.1991, 0.6354],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.2013, 0.5247, 0.3283, 0.5202, 0.6175, 0.4481, 0.4286, 0.7627, 0.4695,
        0.4287, 0.2902, 0.5296, 0.2703, 0.4223, 0.5697, 0.3607],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.4802, 0.4939, 0.3367, 0.3592, 0.6088, 0.4752, 0.5128, 0.4042, 0.5226,
        0.6196, 0.5491, 0.3717, 0.3339, 0.5200, 0.4460, 0.1728],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.2126, 0.4428, 0.4700, 0.4590, 0.4290, 0.2278, 0.2516, 0.4602, 0.4279,
        0.4168, 0.4835, 0.4176, 0.2220, 0.2365, 0.4931, 0.5156],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.4206, 0.3893, 0.4242, 0.4123, 0.6309, 0.5139, 0.3881, 0.5337, 0.4515,
        0.5745, 0.5461, 0.5051, 0.4304, 0.4731, 0.2652, 0.5643],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.3862, 0.4604, 0.5030, 0.2445, 0.1821, 0.3705, 0.3698, 0.4679, 0.2641,
        0.4363, 0.2518, 0.5184, 0.5116, 0.3723, 0.1841, 0.2735],
       device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.4648, 0.3720, 0.4036, 0.5551, 0.1963, 0.4448, 0.3195, 0.3830, 0.2304,
        0.2193, 0.4584, 0.3738, 0.2498, 0.1863, 0.1806, 0.4224, 0.2330, 0.3868,
        0.5506, 0.6038, 0.4074, 0.4671, 0.2965, 0.1994, 0.3583, 0.5160, 0.3612,
        0.2845, 0.3962, 0.2643, 0.3816, 0.1837], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.4807, 0.3839, 0.3790, 0.1444, 0.1760, 0.5055, 0.1774, 0.5940, 0.1480,
        0.4617, 0.3467, 0.1862, 0.4529, 0.1954, 0.3730, 0.4257, 0.4362, 0.1737,
        0.4709, 0.4803, 0.4371, 0.1760, 0.4244, 0.1899, 0.1536, 0.2287, 0.4712,
        0.1241, 0.1881, 0.4326, 0.4563, 0.3722], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.4494, 0.6094, 0.3731, 0.5057, 0.3438, 0.4943, 0.1967, 0.3598, 0.3257,
        0.2105, 0.1651, 0.2858, 0.3247, 0.4985, 0.5421, 0.4944, 0.2051, 0.3540,
        0.5160, 0.3002, 0.3372, 0.5799, 0.1588, 0.1887, 0.2160, 0.3508, 0.2356,
        0.5288, 0.1768, 0.4333, 0.1185, 0.4532], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1225, 0.1382, 0.2825, 0.3770, 0.1892, 0.3194, 0.1410, 0.2754, 0.1645,
        0.1911, 0.3282, 0.3296, 0.3497, 0.3147, 0.1411, 0.2303, 0.3184, 0.3456,
        0.1805, 0.3382, 0.3603, 0.3019, 0.1586, 0.2055, 0.3086, 0.3188, 0.3354,
        0.1604, 0.3415, 0.3187, 0.1960, 0.2965], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1780, 0.4787, 0.5568, 0.4019, 0.4431, 0.6295, 0.5074, 0.3017, 0.4544,
        0.1858, 0.4509, 0.3915, 0.3473, 0.3750, 0.1525, 0.4086, 0.4399, 0.1690,
        0.1510, 0.3119, 0.4946, 0.1375, 0.1267, 0.3868, 0.1278, 0.2882, 0.3643,
        0.3251, 0.3738, 0.5773, 0.4900, 0.4246], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.1425, 0.3735, 0.1600, 0.4322, 0.3962, 0.4158, 0.4044, 0.3782, 0.1471,
        0.2037, 0.3667, 0.3021, 0.3832, 0.4289, 0.3861, 0.1929, 0.3232, 0.1457,
        0.3320, 0.3377, 0.1437, 0.3646, 0.1310, 0.1313, 0.3159, 0.1081, 0.1634,
        0.3752, 0.1625, 0.3796, 0.3327, 0.1340], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([0.2182, 0.1066, 0.2459, 0.2622, 0.1071, 0.1524, 0.1719, 0.2951, 0.2071,
        0.1272, 0.2636, 0.1281, 0.2060, 0.1227, 0.2905, 0.2471, 0.2670, 0.1465,
        0.1215, 0.1345, 0.2987, 0.1007, 0.1355, 0.3080, 0.3467, 0.1201, 0.3239,
        0.3411, 0.0970, 0.1216, 0.2857, 0.1281, 0.2296, 0.1043, 0.1155, 0.3351,
        0.1397, 0.2656, 0.1115, 0.1674, 0.3175, 0.2955, 0.2652, 0.2541, 0.1311,
        0.2544, 0.3294, 0.2493, 0.2588, 0.1233, 0.1251, 0.2852, 0.2164, 0.1842,
        0.2347, 0.2392, 0.3356, 0.3600, 0.3258, 0.3057, 0.3445, 0.1057, 0.2225,
        0.1244], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.2807, 0.0975, 0.3399, 0.3328, 0.3996, 0.2782, 0.3728, 0.0719, 0.3519,
        0.2482, 0.0976, 0.2896, 0.0815, 0.1172, 0.3220, 0.3256, 0.0840, 0.2960,
        0.0903, 0.2854, 0.1128, 0.0744, 0.1270, 0.3566, 0.3865, 0.1348, 0.0700,
        0.3081, 0.1121, 0.1113, 0.2990, 0.3545, 0.0988, 0.3368, 0.1116, 0.3083,
        0.3039, 0.3568, 0.3680, 0.3556, 0.1661, 0.3686, 0.1127, 0.3256, 0.3301,
        0.0996, 0.3163, 0.3138, 0.0837, 0.1230, 0.1282, 0.0763, 0.1279, 0.2735,
        0.3750, 0.2203, 0.3184, 0.2765, 0.1115, 0.2035, 0.2799, 0.3225, 0.2525,
        0.1213], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.1817, 0.1516, 0.3378, 0.3766, 0.2559, 0.0767, 0.1020, 0.3435, 0.1095,
        0.1050, 0.2705, 0.1051, 0.0907, 0.1576, 0.2817, 0.1064, 0.3014, 0.2491,
        0.2056, 0.2972, 0.1776, 0.3471, 0.3797, 0.1679, 0.3306, 0.0841, 0.2925,
        0.3795, 0.3653, 0.3314, 0.1079, 0.1486, 0.2815, 0.1793, 0.0974, 0.1383,
        0.0923, 0.1443, 0.1218, 0.1472, 0.1484, 0.1479, 0.1523, 0.1923, 0.3471,
        0.2102, 0.1836, 0.1090, 0.2397, 0.1906, 0.0657, 0.1069, 0.2538, 0.2084,
        0.1326, 0.3030, 0.1106, 0.1029, 0.2478, 0.0754, 0.2715, 0.1181, 0.1201,
        0.2281], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.2170, 0.2423, 0.1973, 0.0977, 0.0761, 0.3061, 0.1190, 0.2485, 0.2325,
        0.0671, 0.3333, 0.1709, 0.0715, 0.2477, 0.3030, 0.2253, 0.2290, 0.0724,
        0.0803, 0.2560, 0.0693, 0.2660, 0.2032, 0.2078, 0.2552, 0.2262, 0.2149,
        0.0826, 0.2003, 0.1753, 0.1958, 0.2167, 0.2244, 0.0850, 0.2458, 0.2688,
        0.0909, 0.2183, 0.2503, 0.2258, 0.1032, 0.2110, 0.0703, 0.2330, 0.1238,
        0.1673, 0.1434, 0.1942, 0.2253, 0.2275, 0.1999, 0.2307, 0.2277, 0.2259,
        0.0615, 0.1169, 0.2254, 0.3101, 0.0618, 0.3461, 0.2828, 0.2231, 0.2151,
        0.2390], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.3069, 0.1866, 0.2158, 0.3625, 0.2522, 0.2398, 0.2407, 0.2102, 0.3170,
        0.1155, 0.3011, 0.0976, 0.0870, 0.2071, 0.2677, 0.1920, 0.1653, 0.0636,
        0.1623, 0.1229, 0.0736, 0.2408, 0.1033, 0.2997, 0.2200, 0.1270, 0.1349,
        0.2316, 0.2508, 0.2889, 0.0853, 0.1144, 0.2516, 0.3463, 0.1865, 0.2384,
        0.0943, 0.2156, 0.0851, 0.2497, 0.2758, 0.1907, 0.1381, 0.1811, 0.1502,
        0.2549, 0.3286, 0.2500, 0.2420, 0.1141, 0.1175, 0.0714, 0.1031, 0.2576,
        0.2708, 0.0873, 0.0831, 0.0674, 0.2794, 0.1316, 0.2560, 0.0963, 0.2375,
        0.2367], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.2174, 0.0546, 0.1649, 0.0845, 0.1293, 0.1785, 0.0775, 0.1458, 0.1818,
        0.1130, 0.0744, 0.1189, 0.0718, 0.1544, 0.1074, 0.0935, 0.0748, 0.1397,
        0.1551, 0.1736, 0.1591, 0.0864, 0.1504, 0.1738, 0.0662, 0.0560, 0.0559,
        0.1013, 0.2019, 0.1483, 0.1524, 0.1918, 0.1030, 0.0447, 0.1238, 0.1575,
        0.1394, 0.0743, 0.1302, 0.1928, 0.1073, 0.0600, 0.1056, 0.0887, 0.0601,
        0.0623, 0.1415, 0.0704, 0.1748, 0.1300, 0.1461, 0.0529, 0.1258, 0.0569,
        0.0734, 0.1285, 0.0714, 0.0611, 0.0999, 0.1092, 0.1574, 0.1048, 0.1748,
        0.1611], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([0.7226, 0.8365, 0.6644, 0.6286, 0.8405, 1.1614, 0.9227, 0.9761, 0.9852,
        0.8863], device='cuda:0', grad_fn=<NormBackward1>)

Total parameter pruned: 785.0000147670507 (unstructured) 0 (structured)

Test: [0/79]	Time 0.159 (0.159)	Loss 1.4135 (1.4135) ([1.106]+[0.307])	Prec@1 60.156 (60.156)
 * Prec@1 60.110

 Total elapsed time  0:00:42.151830 
 FINETUNING


Total parameter pruned: 785.0000147670507 (unstructured) 0 (structured)

Test: [0/79]	Time 0.137 (0.137)	Loss 1.4135 (1.4135) ([1.106]+[0.307])	Prec@1 60.156 (60.156)
 * Prec@1 60.100
current lr 1.00000e-01
Grad=  tensor(1.7487, device='cuda:0')
Epoch: [3][0/391]	Time 0.146 (0.146)	Data 0.125 (0.125)	Loss 0.8735 (0.8735) ([0.873]+[0.000])	Prec@1 72.656 (72.656)
Epoch: [3][100/391]	Time 0.013 (0.015)	Data 0.000 (0.001)	Loss 0.7530 (0.8174) ([0.753]+[0.000])	Prec@1 73.438 (72.146)
Epoch: [3][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 0.9971 (0.7967) ([0.997]+[0.000])	Prec@1 64.062 (72.540)
Epoch: [3][300/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 0.9384 (0.7821) ([0.938]+[0.000])	Prec@1 65.625 (72.934)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.8935 (0.8935) ([0.894]+[0.000])	Prec@1 67.969 (67.969)
 * Prec@1 70.470
current lr 1.00000e-01
Grad=  tensor(1.4230, device='cuda:0')
Epoch: [4][0/391]	Time 0.176 (0.176)	Data 0.153 (0.153)	Loss 0.7741 (0.7741) ([0.774]+[0.000])	Prec@1 74.219 (74.219)
Epoch: [4][100/391]	Time 0.013 (0.015)	Data 0.000 (0.002)	Loss 0.6986 (0.6949) ([0.699]+[0.000])	Prec@1 77.344 (76.338)
Epoch: [4][200/391]	Time 0.013 (0.014)	Data 0.000 (0.001)	Loss 0.6848 (0.6899) ([0.685]+[0.000])	Prec@1 75.781 (76.178)
Epoch: [4][300/391]	Time 0.012 (0.013)	Data 0.000 (0.001)	Loss 0.8800 (0.6862) ([0.880]+[0.000])	Prec@1 75.000 (76.238)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.7086 (0.7086) ([0.709]+[0.000])	Prec@1 71.875 (71.875)
 * Prec@1 74.840

 Elapsed time for training  0:00:55.109859
Total parameter pruned: 0.0 (unstructured) 0 (structured)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.7086 (0.7086) ([0.709]+[0.000])	Prec@1 71.875 (71.875)
 * Prec@1 74.840
Best accuracy:  74.84
Test: [0/79]	Time 0.152 (0.152)	Loss 0.7698 (0.7698) ([0.770]+[0.000])	Prec@1 71.094 (71.094)
 * Prec@1 73.850
Traceback (most recent call last):
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 178, in <module>
    main()
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 175, in main
    grid_search.run()
  File "/local1/caccmatt/Pruning_prj/grid_search.py", line 158, in run
    print(' Real pruned parameter ',en.par_count(model)-en.par_count(model_pruned))
  File "/local1/caccmatt/Pruning_prj/eval_net.py", line 44, in par_count
    res+=par_count_module(m, bias=(bias_conv or all_modules))
  File "/local1/caccmatt/Pruning_prj/eval_net.py", line 58, in par_count_module
    res+= p.numel()
NameError: name 'p' is not defined
