V0.0.1-_resnet50_Cifar10_lr0.1_l2.5_a0.3_e300+200_bs128_t0.0001_m0.9_wd0.0005_mlstemp3_Mscl1.0
Files already downloaded and verified
M values:
 {Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.5254763960838318, Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.24196940660476685, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.15759095549583435, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.13501641154289246, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.3461485505104065, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.17473100125789642, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.21617008745670319, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.14946585893630981, Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09060623496770859, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.08498729020357132, Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11497705429792404, Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11804789304733276, Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.08379501849412918, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1424030363559723, Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.19753389060497284, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.16684924066066742, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.22829987108707428, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12801074981689453, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09603530913591385, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06901206821203232, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12272872775793076, Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0835055485367775, Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.07954221963882446, Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12703275680541992, Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.19747452437877655, Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.15407174825668335, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1602816879749298, Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.10645194351673126, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10600411146879196, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.13483507931232452, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.21709460020065308, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.11353497207164764, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.0660422295331955, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.10686782747507095, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06808818876743317, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.05323619768023491, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.08759226649999619, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07965513318777084, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06643471866846085, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12164679169654846, Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.09185265004634857, Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.06330689787864685, Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.1110600158572197, Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.12582343816757202, Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 0.09035182744264603, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.07410024106502533, Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False): 0.07018566876649857, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.0687103122472763, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.04065759852528572, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.03755198046565056, Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.04725675657391548, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 0.045549724251031876, Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False): 0.06192586570978165, Linear(in_features=2048, out_features=100, bias=True): 0.44340774416923523}
current lr 1.00000e-01
Grad=  tensor(5798.1572, device='cuda:0')
Epoch: [0][0/391]	Time 1.358 (1.358)	Data 0.166 (0.166)	Loss 5.9619 (5.9619) ([4.539]+[1.423])	Prec@1 0.000 (0.000)
Epoch: [0][100/391]	Time 0.112 (0.125)	Data 0.000 (0.002)	Loss 5.5045 (7.4563) ([2.326]+[3.178])	Prec@1 9.375 (10.094)
Epoch: [0][200/391]	Time 0.112 (0.118)	Data 0.000 (0.001)	Loss 5.1737 (6.4037) ([2.363]+[2.810])	Prec@1 11.719 (10.506)
Epoch: [0][300/391]	Time 0.112 (0.116)	Data 0.000 (0.001)	Loss 4.4740 (5.8656) ([1.991]+[2.483])	Prec@1 21.094 (12.752)
Test: [0/79]	Time 0.183 (0.183)	Loss 4.1346 (4.1346) ([1.925]+[2.209])	Prec@1 19.531 (19.531)
 * Prec@1 22.870
current lr 1.00000e-01
Grad=  tensor(0.2721, device='cuda:0')
Epoch: [1][0/391]	Time 0.266 (0.266)	Data 0.138 (0.138)	Loss 4.0684 (4.0684) ([1.859]+[2.209])	Prec@1 25.781 (25.781)
Epoch: [1][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 3.8734 (4.0427) ([1.934]+[1.939])	Prec@1 25.000 (23.693)
Epoch: [1][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 3.6209 (3.8921) ([1.923]+[1.698])	Prec@1 27.344 (24.541)
Epoch: [1][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 3.3141 (3.7575) ([1.783]+[1.531])	Prec@1 32.031 (25.812)
Test: [0/79]	Time 0.176 (0.176)	Loss 3.1146 (3.1146) ([1.759]+[1.356])	Prec@1 37.500 (37.500)
 * Prec@1 32.640
current lr 1.00000e-01
Grad=  tensor(0.4925, device='cuda:0')
Epoch: [2][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 3.0402 (3.0402) ([1.684]+[1.356])	Prec@1 40.625 (40.625)
Epoch: [2][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 2.8869 (3.0118) ([1.694]+[1.193])	Prec@1 38.281 (34.653)
Epoch: [2][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 2.8005 (2.9379) ([1.719]+[1.081])	Prec@1 31.250 (35.098)
Epoch: [2][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 2.6167 (2.8630) ([1.656]+[0.961])	Prec@1 39.062 (35.631)
Test: [0/79]	Time 0.179 (0.179)	Loss 2.4724 (2.4724) ([1.589]+[0.884])	Prec@1 38.281 (38.281)
 * Prec@1 36.010
current lr 1.00000e-01
Grad=  tensor(0.6923, device='cuda:0')
Epoch: [3][0/391]	Time 0.272 (0.272)	Data 0.149 (0.149)	Loss 2.6006 (2.6006) ([1.717]+[0.884])	Prec@1 35.156 (35.156)
Epoch: [3][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 2.5479 (2.4444) ([1.761]+[0.787])	Prec@1 38.281 (40.122)
Epoch: [3][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 2.2483 (2.3890) ([1.520]+[0.728])	Prec@1 45.312 (41.041)
Epoch: [3][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 2.0569 (2.3261) ([1.389]+[0.668])	Prec@1 48.438 (42.175)
Test: [0/79]	Time 0.179 (0.179)	Loss 1.9127 (1.9127) ([1.295]+[0.618])	Prec@1 50.000 (50.000)
 * Prec@1 48.040
current lr 1.00000e-01
Grad=  tensor(1.2189, device='cuda:0')
Epoch: [4][0/391]	Time 0.269 (0.269)	Data 0.145 (0.145)	Loss 2.0524 (2.0524) ([1.434]+[0.618])	Prec@1 50.000 (50.000)
Epoch: [4][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 2.0211 (2.0381) ([1.442]+[0.579])	Prec@1 47.656 (47.935)
Epoch: [4][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.8045 (1.9954) ([1.250]+[0.555])	Prec@1 54.688 (48.612)
Epoch: [4][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.8416 (1.9552) ([1.314]+[0.527])	Prec@1 50.781 (49.598)
Test: [0/79]	Time 0.172 (0.172)	Loss 2.0118 (2.0118) ([1.524]+[0.488])	Prec@1 48.438 (48.438)
 * Prec@1 46.880
current lr 1.00000e-01
Grad=  tensor(1.7072, device='cuda:0')
Epoch: [5][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 1.9178 (1.9178) ([1.430]+[0.488])	Prec@1 50.781 (50.781)
Epoch: [5][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.6550 (1.7809) ([1.156]+[0.499])	Prec@1 55.469 (53.048)
Epoch: [5][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.6804 (1.7415) ([1.230]+[0.450])	Prec@1 54.688 (54.497)
Epoch: [5][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.5897 (1.7121) ([1.167]+[0.423])	Prec@1 57.812 (54.986)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.6759 (1.6759) ([1.253]+[0.423])	Prec@1 54.688 (54.688)
 * Prec@1 52.500
current lr 1.00000e-01
Grad=  tensor(1.6112, device='cuda:0')
Epoch: [6][0/391]	Time 0.265 (0.265)	Data 0.138 (0.138)	Loss 1.5578 (1.5578) ([1.135]+[0.423])	Prec@1 57.812 (57.812)
Epoch: [6][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 1.5433 (1.5635) ([1.127]+[0.416])	Prec@1 59.375 (59.073)
Epoch: [6][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 1.6819 (1.5472) ([1.296]+[0.386])	Prec@1 51.562 (59.223)
Epoch: [6][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 1.6122 (1.5202) ([1.232]+[0.380])	Prec@1 56.250 (60.006)
Test: [0/79]	Time 0.174 (0.174)	Loss 1.3719 (1.3719) ([1.004]+[0.368])	Prec@1 60.938 (60.938)
 * Prec@1 61.660
current lr 1.00000e-01
Grad=  tensor(1.6894, device='cuda:0')
Epoch: [7][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 1.3408 (1.3408) ([0.973]+[0.368])	Prec@1 66.406 (66.406)
Epoch: [7][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.6129 (1.3767) ([1.254]+[0.359])	Prec@1 56.250 (63.683)
Epoch: [7][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.3215 (1.3712) ([0.965]+[0.357])	Prec@1 67.188 (64.101)
Epoch: [7][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.2519 (1.3481) ([0.899]+[0.353])	Prec@1 64.844 (64.659)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.6009 (1.6009) ([1.251]+[0.350])	Prec@1 59.375 (59.375)
 * Prec@1 56.350
current lr 1.00000e-01
Grad=  tensor(1.8340, device='cuda:0')
Epoch: [8][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 1.3482 (1.3482) ([0.998]+[0.350])	Prec@1 62.500 (62.500)
Epoch: [8][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.3121 (1.2738) ([0.972]+[0.340])	Prec@1 66.406 (67.536)
Epoch: [8][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.3906 (1.2599) ([1.038]+[0.352])	Prec@1 63.281 (67.821)
Epoch: [8][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.2761 (1.2516) ([0.933]+[0.343])	Prec@1 66.406 (68.132)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.8493 (1.8493) ([1.512]+[0.338])	Prec@1 54.688 (54.688)
 * Prec@1 56.390
current lr 1.00000e-01
Grad=  tensor(2.7572, device='cuda:0')
Epoch: [9][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 1.2429 (1.2429) ([0.905]+[0.338])	Prec@1 64.062 (64.062)
Epoch: [9][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 1.3515 (1.1743) ([1.022]+[0.329])	Prec@1 66.406 (70.630)
Epoch: [9][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.0195 (1.1662) ([0.695]+[0.325])	Prec@1 72.656 (70.631)
Epoch: [9][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 1.0735 (1.1538) ([0.756]+[0.318])	Prec@1 77.344 (71.039)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.3402 (1.3402) ([1.024]+[0.316])	Prec@1 64.844 (64.844)
 * Prec@1 61.680
current lr 1.00000e-01
Grad=  tensor(2.0689, device='cuda:0')
Epoch: [10][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 1.1056 (1.1056) ([0.789]+[0.316])	Prec@1 68.750 (68.750)
Epoch: [10][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 1.0391 (1.0923) ([0.727]+[0.312])	Prec@1 73.438 (73.144)
Epoch: [10][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 1.0903 (1.0829) ([0.782]+[0.308])	Prec@1 70.312 (73.274)
Epoch: [10][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.9538 (1.0763) ([0.644]+[0.310])	Prec@1 76.562 (73.248)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2878 (1.2878) ([0.975]+[0.313])	Prec@1 63.281 (63.281)
 * Prec@1 65.740
current lr 1.00000e-01
Grad=  tensor(1.8145, device='cuda:0')
Epoch: [11][0/391]	Time 0.260 (0.260)	Data 0.141 (0.141)	Loss 1.0457 (1.0457) ([0.733]+[0.313])	Prec@1 75.000 (75.000)
Epoch: [11][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9869 (1.0511) ([0.680]+[0.307])	Prec@1 82.031 (74.134)
Epoch: [11][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 1.0574 (1.0386) ([0.753]+[0.305])	Prec@1 70.312 (74.518)
Epoch: [11][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 1.0904 (1.0353) ([0.794]+[0.296])	Prec@1 71.875 (74.618)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.2063 (1.2063) ([0.902]+[0.304])	Prec@1 70.312 (70.312)
 * Prec@1 67.320
current lr 1.00000e-01
Grad=  tensor(1.7962, device='cuda:0')
Epoch: [12][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 1.0851 (1.0851) ([0.781]+[0.304])	Prec@1 72.656 (72.656)
Epoch: [12][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9865 (1.0026) ([0.692]+[0.295])	Prec@1 76.562 (75.897)
Epoch: [12][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 1.0519 (1.0040) ([0.756]+[0.296])	Prec@1 74.219 (75.416)
Epoch: [12][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 1.0911 (0.9953) ([0.803]+[0.288])	Prec@1 71.094 (75.657)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.1318 (1.1318) ([0.844]+[0.288])	Prec@1 67.969 (67.969)
 * Prec@1 68.500
current lr 1.00000e-01
Grad=  tensor(2.2945, device='cuda:0')
Epoch: [13][0/391]	Time 0.263 (0.263)	Data 0.140 (0.140)	Loss 1.0472 (1.0472) ([0.759]+[0.288])	Prec@1 74.219 (74.219)
Epoch: [13][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.9144 (0.9794) ([0.630]+[0.284])	Prec@1 79.688 (76.029)
Epoch: [13][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8874 (0.9808) ([0.603]+[0.284])	Prec@1 78.906 (75.847)
Epoch: [13][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9568 (0.9753) ([0.672]+[0.284])	Prec@1 78.125 (75.942)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.2431 (1.2431) ([0.959]+[0.284])	Prec@1 67.969 (67.969)
 * Prec@1 65.970
current lr 1.00000e-01
Grad=  tensor(1.7821, device='cuda:0')
Epoch: [14][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.9124 (0.9124) ([0.628]+[0.284])	Prec@1 74.219 (74.219)
Epoch: [14][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8848 (0.9776) ([0.604]+[0.281])	Prec@1 82.031 (75.774)
Epoch: [14][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.9756 (0.9643) ([0.699]+[0.277])	Prec@1 71.875 (76.458)
Epoch: [14][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.9764 (0.9568) ([0.700]+[0.276])	Prec@1 77.344 (76.692)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.1072 (1.1072) ([0.835]+[0.273])	Prec@1 70.312 (70.312)
 * Prec@1 75.200
current lr 1.00000e-01
Grad=  tensor(2.5368, device='cuda:0')
Epoch: [15][0/391]	Time 0.261 (0.261)	Data 0.139 (0.139)	Loss 0.9455 (0.9455) ([0.673]+[0.273])	Prec@1 77.344 (77.344)
Epoch: [15][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 1.0063 (0.9356) ([0.735]+[0.272])	Prec@1 74.219 (77.220)
Epoch: [15][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9128 (0.9229) ([0.644]+[0.269])	Prec@1 73.438 (77.375)
Epoch: [15][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.9410 (0.9247) ([0.673]+[0.268])	Prec@1 76.562 (77.396)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.1242 (1.1242) ([0.858]+[0.266])	Prec@1 65.625 (65.625)
 * Prec@1 70.680
current lr 1.00000e-01
Grad=  tensor(2.4216, device='cuda:0')
Epoch: [16][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.8713 (0.8713) ([0.605]+[0.266])	Prec@1 75.781 (75.781)
Epoch: [16][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.8217 (0.9280) ([0.556]+[0.266])	Prec@1 81.250 (77.321)
Epoch: [16][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9158 (0.9213) ([0.652]+[0.263])	Prec@1 79.688 (77.542)
Epoch: [16][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7876 (0.9110) ([0.526]+[0.261])	Prec@1 82.031 (77.897)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.1197 (1.1197) ([0.858]+[0.261])	Prec@1 73.438 (73.438)
 * Prec@1 71.590
current lr 1.00000e-01
Grad=  tensor(2.5009, device='cuda:0')
Epoch: [17][0/391]	Time 0.258 (0.258)	Data 0.137 (0.137)	Loss 1.0168 (1.0168) ([0.755]+[0.261])	Prec@1 75.000 (75.000)
Epoch: [17][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 1.0514 (0.8896) ([0.790]+[0.262])	Prec@1 74.219 (78.110)
Epoch: [17][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9966 (0.8901) ([0.735]+[0.261])	Prec@1 75.781 (78.187)
Epoch: [17][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9521 (0.8899) ([0.692]+[0.260])	Prec@1 76.562 (78.224)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.1281 (1.1281) ([0.868]+[0.260])	Prec@1 67.969 (67.969)
 * Prec@1 71.390
current lr 1.00000e-01
Grad=  tensor(1.3154, device='cuda:0')
Epoch: [18][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.8025 (0.8025) ([0.542]+[0.260])	Prec@1 78.906 (78.906)
Epoch: [18][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7329 (0.8627) ([0.473]+[0.260])	Prec@1 82.812 (79.285)
Epoch: [18][200/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8557 (0.8723) ([0.597]+[0.259])	Prec@1 75.781 (78.949)
Epoch: [18][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9024 (0.8817) ([0.642]+[0.260])	Prec@1 80.469 (78.673)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.2395 (1.2395) ([0.981]+[0.258])	Prec@1 64.844 (64.844)
 * Prec@1 70.800
current lr 1.00000e-01
Grad=  tensor(1.6331, device='cuda:0')
Epoch: [19][0/391]	Time 0.262 (0.262)	Data 0.139 (0.139)	Loss 0.7986 (0.7986) ([0.540]+[0.258])	Prec@1 80.469 (80.469)
Epoch: [19][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8093 (0.8619) ([0.550]+[0.259])	Prec@1 82.812 (79.100)
Epoch: [19][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8119 (0.8613) ([0.553]+[0.259])	Prec@1 78.906 (79.299)
Epoch: [19][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9068 (0.8668) ([0.648]+[0.259])	Prec@1 77.344 (79.091)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.2878 (1.2878) ([1.030]+[0.258])	Prec@1 65.625 (65.625)
 * Prec@1 67.860
current lr 1.00000e-01
Grad=  tensor(1.8358, device='cuda:0')
Epoch: [20][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.7272 (0.7272) ([0.469]+[0.258])	Prec@1 84.375 (84.375)
Epoch: [20][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9949 (0.8558) ([0.738]+[0.257])	Prec@1 78.125 (79.463)
Epoch: [20][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8595 (0.8610) ([0.603]+[0.257])	Prec@1 79.688 (79.248)
Epoch: [20][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8561 (0.8574) ([0.600]+[0.256])	Prec@1 85.156 (79.537)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.0670 (1.0670) ([0.811]+[0.256])	Prec@1 73.438 (73.438)
 * Prec@1 72.450
current lr 1.00000e-01
Grad=  tensor(2.5263, device='cuda:0')
Epoch: [21][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.9625 (0.9625) ([0.706]+[0.256])	Prec@1 75.000 (75.000)
Epoch: [21][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8295 (0.8494) ([0.573]+[0.256])	Prec@1 78.125 (79.471)
Epoch: [21][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6530 (0.8541) ([0.396]+[0.257])	Prec@1 89.844 (79.447)
Epoch: [21][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7662 (0.8549) ([0.510]+[0.256])	Prec@1 87.500 (79.651)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0024 (1.0024) ([0.747]+[0.256])	Prec@1 75.000 (75.000)
 * Prec@1 76.140
current lr 1.00000e-01
Grad=  tensor(1.8468, device='cuda:0')
Epoch: [22][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.8992 (0.8992) ([0.644]+[0.256])	Prec@1 75.781 (75.781)
Epoch: [22][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7499 (0.8451) ([0.495]+[0.255])	Prec@1 84.375 (79.827)
Epoch: [22][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6614 (0.8443) ([0.406]+[0.255])	Prec@1 90.625 (79.804)
Epoch: [22][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8782 (0.8467) ([0.624]+[0.255])	Prec@1 77.344 (79.781)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.1847 (1.1847) ([0.931]+[0.253])	Prec@1 68.750 (68.750)
 * Prec@1 66.780
current lr 1.00000e-01
Grad=  tensor(1.4282, device='cuda:0')
Epoch: [23][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.6600 (0.6600) ([0.407]+[0.253])	Prec@1 85.156 (85.156)
Epoch: [23][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8205 (0.8434) ([0.567]+[0.254])	Prec@1 78.125 (79.711)
Epoch: [23][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9378 (0.8454) ([0.684]+[0.254])	Prec@1 76.562 (79.586)
Epoch: [23][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8084 (0.8452) ([0.555]+[0.254])	Prec@1 84.375 (79.786)
Test: [0/79]	Time 0.171 (0.171)	Loss 1.5193 (1.5193) ([1.267]+[0.253])	Prec@1 65.625 (65.625)
 * Prec@1 67.850
current lr 1.00000e-01
Grad=  tensor(2.0106, device='cuda:0')
Epoch: [24][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.7554 (0.7554) ([0.503]+[0.253])	Prec@1 85.156 (85.156)
Epoch: [24][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8455 (0.8249) ([0.592]+[0.253])	Prec@1 80.469 (80.314)
Epoch: [24][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8779 (0.8289) ([0.625]+[0.253])	Prec@1 75.781 (80.267)
Epoch: [24][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9550 (0.8285) ([0.702]+[0.253])	Prec@1 75.781 (80.274)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.8927 (0.8927) ([0.640]+[0.252])	Prec@1 79.688 (79.688)
 * Prec@1 77.750
current lr 1.00000e-01
Grad=  tensor(1.8209, device='cuda:0')
Epoch: [25][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.7629 (0.7629) ([0.511]+[0.252])	Prec@1 82.031 (82.031)
Epoch: [25][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8195 (0.8243) ([0.568]+[0.251])	Prec@1 81.250 (80.422)
Epoch: [25][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7458 (0.8337) ([0.494]+[0.252])	Prec@1 83.594 (80.119)
Epoch: [25][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7529 (0.8312) ([0.502]+[0.251])	Prec@1 82.812 (80.277)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9774 (0.9774) ([0.728]+[0.249])	Prec@1 75.781 (75.781)
 * Prec@1 75.830
current lr 1.00000e-01
Grad=  tensor(1.3096, device='cuda:0')
Epoch: [26][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.6309 (0.6309) ([0.382]+[0.249])	Prec@1 85.156 (85.156)
Epoch: [26][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7636 (0.7949) ([0.514]+[0.250])	Prec@1 82.031 (80.848)
Epoch: [26][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7576 (0.8149) ([0.507]+[0.251])	Prec@1 84.375 (80.372)
Epoch: [26][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8977 (0.8156) ([0.647]+[0.250])	Prec@1 78.906 (80.419)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.4675 (1.4675) ([1.218]+[0.250])	Prec@1 66.406 (66.406)
 * Prec@1 68.160
current lr 1.00000e-01
Grad=  tensor(1.6434, device='cuda:0')
Epoch: [27][0/391]	Time 0.253 (0.253)	Data 0.130 (0.130)	Loss 0.8032 (0.8032) ([0.553]+[0.250])	Prec@1 83.594 (83.594)
Epoch: [27][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7731 (0.8115) ([0.523]+[0.250])	Prec@1 82.812 (80.902)
Epoch: [27][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7522 (0.8162) ([0.503]+[0.249])	Prec@1 83.594 (80.846)
Epoch: [27][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7763 (0.8150) ([0.527]+[0.249])	Prec@1 76.562 (80.809)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9517 (0.9517) ([0.702]+[0.249])	Prec@1 77.344 (77.344)
 * Prec@1 74.360
current lr 1.00000e-01
Grad=  tensor(1.9310, device='cuda:0')
Epoch: [28][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.8220 (0.8220) ([0.573]+[0.249])	Prec@1 81.250 (81.250)
Epoch: [28][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7892 (0.8126) ([0.540]+[0.249])	Prec@1 82.812 (80.886)
Epoch: [28][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7812 (0.8125) ([0.532]+[0.249])	Prec@1 78.906 (80.616)
Epoch: [28][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7452 (0.8086) ([0.497]+[0.248])	Prec@1 79.688 (80.754)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.0734 (1.0734) ([0.826]+[0.247])	Prec@1 74.219 (74.219)
 * Prec@1 73.920
current lr 1.00000e-01
Grad=  tensor(1.5271, device='cuda:0')
Epoch: [29][0/391]	Time 0.267 (0.267)	Data 0.144 (0.144)	Loss 0.6665 (0.6665) ([0.419]+[0.247])	Prec@1 85.156 (85.156)
Epoch: [29][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 1.1567 (0.7993) ([0.910]+[0.247])	Prec@1 71.094 (81.227)
Epoch: [29][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6740 (0.8009) ([0.427]+[0.247])	Prec@1 84.375 (81.164)
Epoch: [29][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8422 (0.8055) ([0.596]+[0.247])	Prec@1 78.125 (80.954)
Test: [0/79]	Time 0.178 (0.178)	Loss 1.2510 (1.2510) ([1.004]+[0.247])	Prec@1 67.188 (67.188)
 * Prec@1 70.800
current lr 1.00000e-01
Grad=  tensor(1.5842, device='cuda:0')
Epoch: [30][0/391]	Time 0.271 (0.271)	Data 0.148 (0.148)	Loss 0.7501 (0.7501) ([0.503]+[0.247])	Prec@1 87.500 (87.500)
Epoch: [30][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.8078 (0.7967) ([0.562]+[0.246])	Prec@1 81.250 (81.033)
Epoch: [30][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7064 (0.7889) ([0.461]+[0.246])	Prec@1 84.375 (81.370)
Epoch: [30][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7186 (0.7996) ([0.472]+[0.246])	Prec@1 84.375 (81.019)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.9416 (0.9416) ([0.696]+[0.246])	Prec@1 78.906 (78.906)
 * Prec@1 75.980
current lr 1.00000e-01
Grad=  tensor(2.2753, device='cuda:0')
Epoch: [31][0/391]	Time 0.269 (0.269)	Data 0.147 (0.147)	Loss 0.7601 (0.7601) ([0.514]+[0.246])	Prec@1 80.469 (80.469)
Epoch: [31][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.8136 (0.7760) ([0.569]+[0.244])	Prec@1 78.906 (81.815)
Epoch: [31][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8145 (0.7909) ([0.570]+[0.245])	Prec@1 82.812 (81.328)
Epoch: [31][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8990 (0.7935) ([0.655]+[0.244])	Prec@1 77.344 (81.172)
Test: [0/79]	Time 0.178 (0.178)	Loss 1.0511 (1.0511) ([0.806]+[0.245])	Prec@1 70.312 (70.312)
 * Prec@1 75.160
current lr 1.00000e-01
Grad=  tensor(2.4472, device='cuda:0')
Epoch: [32][0/391]	Time 0.267 (0.267)	Data 0.144 (0.144)	Loss 0.8314 (0.8314) ([0.586]+[0.245])	Prec@1 82.031 (82.031)
Epoch: [32][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.8536 (0.7933) ([0.609]+[0.245])	Prec@1 77.344 (81.072)
Epoch: [32][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 1.0034 (0.7874) ([0.759]+[0.244])	Prec@1 68.750 (81.363)
Epoch: [32][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6894 (0.7892) ([0.445]+[0.244])	Prec@1 85.938 (81.289)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.9966 (0.9966) ([0.752]+[0.245])	Prec@1 76.562 (76.562)
 * Prec@1 74.970
current lr 1.00000e-01
Grad=  tensor(1.9271, device='cuda:0')
Epoch: [33][0/391]	Time 0.263 (0.263)	Data 0.140 (0.140)	Loss 0.8251 (0.8251) ([0.580]+[0.245])	Prec@1 78.125 (78.125)
Epoch: [33][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7920 (0.7904) ([0.548]+[0.244])	Prec@1 84.375 (81.420)
Epoch: [33][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7612 (0.7980) ([0.516]+[0.245])	Prec@1 85.938 (81.025)
Epoch: [33][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7983 (0.7932) ([0.554]+[0.244])	Prec@1 81.250 (81.263)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.9523 (0.9523) ([0.709]+[0.244])	Prec@1 81.250 (81.250)
 * Prec@1 74.540
current lr 1.00000e-01
Grad=  tensor(1.7653, device='cuda:0')
Epoch: [34][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.8279 (0.8279) ([0.584]+[0.244])	Prec@1 76.562 (76.562)
Epoch: [34][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6688 (0.7848) ([0.425]+[0.243])	Prec@1 83.594 (81.327)
Epoch: [34][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7944 (0.7872) ([0.551]+[0.244])	Prec@1 80.469 (81.304)
Epoch: [34][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8584 (0.7918) ([0.615]+[0.244])	Prec@1 82.031 (81.253)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.1127 (1.1127) ([0.869]+[0.243])	Prec@1 73.438 (73.438)
 * Prec@1 73.210
current lr 1.00000e-01
Grad=  tensor(1.9858, device='cuda:0')
Epoch: [35][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.7572 (0.7572) ([0.514]+[0.243])	Prec@1 82.812 (82.812)
Epoch: [35][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8443 (0.7788) ([0.600]+[0.244])	Prec@1 78.125 (81.668)
Epoch: [35][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8427 (0.7770) ([0.599]+[0.243])	Prec@1 85.938 (81.891)
Epoch: [35][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8631 (0.7863) ([0.620]+[0.244])	Prec@1 76.562 (81.489)
Test: [0/79]	Time 0.177 (0.177)	Loss 1.7998 (1.7998) ([1.556]+[0.244])	Prec@1 60.938 (60.938)
 * Prec@1 62.150
current lr 1.00000e-01
Grad=  tensor(2.1914, device='cuda:0')
Epoch: [36][0/391]	Time 0.263 (0.263)	Data 0.140 (0.140)	Loss 0.7930 (0.7930) ([0.550]+[0.244])	Prec@1 82.812 (82.812)
Epoch: [36][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7451 (0.7833) ([0.502]+[0.244])	Prec@1 83.594 (81.598)
Epoch: [36][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7273 (0.7803) ([0.484]+[0.243])	Prec@1 82.812 (81.701)
Epoch: [36][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7356 (0.7831) ([0.493]+[0.243])	Prec@1 82.812 (81.520)
Test: [0/79]	Time 0.173 (0.173)	Loss 1.1649 (1.1649) ([0.923]+[0.242])	Prec@1 63.281 (63.281)
 * Prec@1 73.230
current lr 1.00000e-01
Grad=  tensor(1.9670, device='cuda:0')
Epoch: [37][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.7482 (0.7482) ([0.506]+[0.242])	Prec@1 79.688 (79.688)
Epoch: [37][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.6818 (0.7598) ([0.440]+[0.242])	Prec@1 89.062 (82.457)
Epoch: [37][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.7670 (0.7733) ([0.525]+[0.242])	Prec@1 80.469 (81.954)
Epoch: [37][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.7663 (0.7775) ([0.524]+[0.242])	Prec@1 77.344 (81.686)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2065 (1.2065) ([0.965]+[0.241])	Prec@1 67.188 (67.188)
 * Prec@1 70.750
current lr 1.00000e-01
Grad=  tensor(1.6883, device='cuda:0')
Epoch: [38][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.6667 (0.6667) ([0.425]+[0.241])	Prec@1 84.375 (84.375)
Epoch: [38][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.8430 (0.7745) ([0.602]+[0.241])	Prec@1 78.906 (82.000)
Epoch: [38][200/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.8229 (0.7810) ([0.582]+[0.241])	Prec@1 80.469 (81.720)
Epoch: [38][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.7481 (0.7832) ([0.506]+[0.242])	Prec@1 82.031 (81.481)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0044 (1.0044) ([0.764]+[0.241])	Prec@1 74.219 (74.219)
 * Prec@1 72.140
current lr 1.00000e-01
Grad=  tensor(2.0750, device='cuda:0')
Epoch: [39][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.8237 (0.8237) ([0.583]+[0.241])	Prec@1 77.344 (77.344)
Epoch: [39][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8408 (0.7758) ([0.601]+[0.240])	Prec@1 79.688 (82.186)
Epoch: [39][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7982 (0.7797) ([0.558]+[0.241])	Prec@1 82.031 (81.817)
Epoch: [39][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8565 (0.7771) ([0.617]+[0.240])	Prec@1 79.688 (81.855)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.0207 (1.0207) ([0.782]+[0.239])	Prec@1 71.094 (71.094)
 * Prec@1 75.210
current lr 1.00000e-01
Grad=  tensor(1.2618, device='cuda:0')
Epoch: [40][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.6556 (0.6556) ([0.417]+[0.239])	Prec@1 85.156 (85.156)
Epoch: [40][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8054 (0.7736) ([0.566]+[0.239])	Prec@1 77.344 (81.513)
Epoch: [40][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8422 (0.7738) ([0.603]+[0.239])	Prec@1 80.469 (81.678)
Epoch: [40][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8215 (0.7764) ([0.582]+[0.239])	Prec@1 80.469 (81.582)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.9090 (0.9090) ([0.670]+[0.239])	Prec@1 79.688 (79.688)
 * Prec@1 76.490
current lr 1.00000e-01
Grad=  tensor(1.6984, device='cuda:0')
Epoch: [41][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.7719 (0.7719) ([0.533]+[0.239])	Prec@1 83.594 (83.594)
Epoch: [41][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6825 (0.7514) ([0.444]+[0.239])	Prec@1 84.375 (82.519)
Epoch: [41][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6558 (0.7636) ([0.417]+[0.238])	Prec@1 85.938 (81.954)
Epoch: [41][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7764 (0.7688) ([0.538]+[0.239])	Prec@1 80.469 (81.834)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.9671 (0.9671) ([0.728]+[0.239])	Prec@1 72.656 (72.656)
 * Prec@1 73.650
current lr 1.00000e-01
Grad=  tensor(1.7319, device='cuda:0')
Epoch: [42][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.7211 (0.7211) ([0.482]+[0.239])	Prec@1 87.500 (87.500)
Epoch: [42][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8949 (0.7621) ([0.657]+[0.238])	Prec@1 74.219 (81.900)
Epoch: [42][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6909 (0.7738) ([0.452]+[0.239])	Prec@1 85.938 (81.576)
Epoch: [42][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8154 (0.7713) ([0.577]+[0.238])	Prec@1 81.250 (81.751)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9977 (0.9977) ([0.760]+[0.238])	Prec@1 70.312 (70.312)
 * Prec@1 74.690
current lr 1.00000e-01
Grad=  tensor(2.2324, device='cuda:0')
Epoch: [43][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.9349 (0.9349) ([0.697]+[0.238])	Prec@1 75.000 (75.000)
Epoch: [43][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8234 (0.7666) ([0.585]+[0.238])	Prec@1 79.688 (82.232)
Epoch: [43][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7719 (0.7682) ([0.534]+[0.238])	Prec@1 82.031 (81.899)
Epoch: [43][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7889 (0.7727) ([0.551]+[0.238])	Prec@1 80.469 (81.774)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2097 (1.2097) ([0.973]+[0.237])	Prec@1 72.656 (72.656)
 * Prec@1 73.300
current lr 1.00000e-01
Grad=  tensor(1.2041, device='cuda:0')
Epoch: [44][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.5683 (0.5683) ([0.331]+[0.237])	Prec@1 89.844 (89.844)
Epoch: [44][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8224 (0.7446) ([0.586]+[0.236])	Prec@1 80.469 (82.627)
Epoch: [44][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6834 (0.7523) ([0.447]+[0.236])	Prec@1 85.156 (82.498)
Epoch: [44][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6812 (0.7588) ([0.444]+[0.237])	Prec@1 85.938 (82.073)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9932 (0.9932) ([0.757]+[0.237])	Prec@1 73.438 (73.438)
 * Prec@1 76.150
current lr 1.00000e-01
Grad=  tensor(0.8994, device='cuda:0')
Epoch: [45][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.5135 (0.5135) ([0.277]+[0.237])	Prec@1 92.969 (92.969)
Epoch: [45][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9065 (0.7741) ([0.670]+[0.237])	Prec@1 79.688 (81.907)
Epoch: [45][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7033 (0.7705) ([0.466]+[0.237])	Prec@1 81.250 (81.930)
Epoch: [45][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7011 (0.7723) ([0.464]+[0.237])	Prec@1 85.156 (81.774)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.9913 (0.9913) ([0.754]+[0.237])	Prec@1 75.781 (75.781)
 * Prec@1 76.370
current lr 1.00000e-01
Grad=  tensor(1.8613, device='cuda:0')
Epoch: [46][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.6555 (0.6555) ([0.418]+[0.237])	Prec@1 84.375 (84.375)
Epoch: [46][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8416 (0.7565) ([0.605]+[0.237])	Prec@1 81.250 (82.364)
Epoch: [46][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6349 (0.7575) ([0.398]+[0.237])	Prec@1 84.375 (82.323)
Epoch: [46][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7705 (0.7618) ([0.534]+[0.236])	Prec@1 81.250 (82.179)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.8799 (0.8799) ([0.645]+[0.235])	Prec@1 78.125 (78.125)
 * Prec@1 76.370
current lr 1.00000e-01
Grad=  tensor(2.1718, device='cuda:0')
Epoch: [47][0/391]	Time 0.260 (0.260)	Data 0.136 (0.136)	Loss 0.8078 (0.8078) ([0.572]+[0.235])	Prec@1 79.688 (79.688)
Epoch: [47][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8936 (0.7502) ([0.658]+[0.235])	Prec@1 76.562 (82.364)
Epoch: [47][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6699 (0.7583) ([0.435]+[0.235])	Prec@1 85.156 (82.229)
Epoch: [47][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7595 (0.7557) ([0.524]+[0.236])	Prec@1 82.031 (82.252)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.5003 (1.5003) ([1.264]+[0.236])	Prec@1 67.969 (67.969)
 * Prec@1 68.310
current lr 1.00000e-01
Grad=  tensor(2.3256, device='cuda:0')
Epoch: [48][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.8428 (0.8428) ([0.607]+[0.236])	Prec@1 78.125 (78.125)
Epoch: [48][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8584 (0.7619) ([0.623]+[0.235])	Prec@1 78.906 (81.691)
Epoch: [48][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7494 (0.7649) ([0.514]+[0.236])	Prec@1 80.469 (81.650)
Epoch: [48][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8159 (0.7669) ([0.580]+[0.236])	Prec@1 79.688 (81.691)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.8878 (0.8878) ([0.653]+[0.235])	Prec@1 75.000 (75.000)
 * Prec@1 73.210
current lr 1.00000e-01
Grad=  tensor(1.8341, device='cuda:0')
Epoch: [49][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.7225 (0.7225) ([0.487]+[0.235])	Prec@1 80.469 (80.469)
Epoch: [49][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9275 (0.7423) ([0.693]+[0.234])	Prec@1 79.688 (82.488)
Epoch: [49][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6305 (0.7431) ([0.397]+[0.233])	Prec@1 86.719 (82.540)
Epoch: [49][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9410 (0.7526) ([0.707]+[0.234])	Prec@1 76.562 (82.247)
Test: [0/79]	Time 0.216 (0.216)	Loss 0.7043 (0.7043) ([0.470]+[0.234])	Prec@1 84.375 (84.375)
 * Prec@1 77.830
current lr 1.00000e-01
Grad=  tensor(1.8152, device='cuda:0')
Epoch: [50][0/391]	Time 0.261 (0.261)	Data 0.136 (0.136)	Loss 0.6548 (0.6548) ([0.420]+[0.234])	Prec@1 84.375 (84.375)
Epoch: [50][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7529 (0.7396) ([0.519]+[0.234])	Prec@1 82.812 (82.627)
Epoch: [50][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7779 (0.7518) ([0.544]+[0.234])	Prec@1 81.250 (82.377)
Epoch: [50][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7142 (0.7487) ([0.481]+[0.233])	Prec@1 79.688 (82.426)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2538 (1.2538) ([1.020]+[0.234])	Prec@1 75.000 (75.000)
 * Prec@1 71.540
current lr 1.00000e-01
Grad=  tensor(1.8120, device='cuda:0')
Epoch: [51][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.8338 (0.8338) ([0.600]+[0.234])	Prec@1 81.250 (81.250)
Epoch: [51][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9477 (0.7662) ([0.713]+[0.235])	Prec@1 74.219 (81.753)
Epoch: [51][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8851 (0.7686) ([0.651]+[0.234])	Prec@1 75.781 (81.806)
Epoch: [51][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6615 (0.7630) ([0.428]+[0.233])	Prec@1 85.938 (82.068)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.7966 (0.7966) ([0.564]+[0.232])	Prec@1 80.469 (80.469)
 * Prec@1 78.640
current lr 1.00000e-01
Grad=  tensor(2.3204, device='cuda:0')
Epoch: [52][0/391]	Time 0.266 (0.266)	Data 0.142 (0.142)	Loss 0.8171 (0.8171) ([0.585]+[0.232])	Prec@1 77.344 (77.344)
Epoch: [52][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8139 (0.7415) ([0.582]+[0.232])	Prec@1 80.469 (82.565)
Epoch: [52][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8588 (0.7397) ([0.627]+[0.232])	Prec@1 79.688 (82.591)
Epoch: [52][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6778 (0.7435) ([0.446]+[0.232])	Prec@1 81.250 (82.410)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.8560 (0.8560) ([0.625]+[0.231])	Prec@1 78.906 (78.906)
 * Prec@1 75.760
current lr 1.00000e-01
Grad=  tensor(1.6307, device='cuda:0')
Epoch: [53][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.6961 (0.6961) ([0.465]+[0.231])	Prec@1 86.719 (86.719)
Epoch: [53][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6494 (0.7314) ([0.418]+[0.232])	Prec@1 85.938 (82.843)
Epoch: [53][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7858 (0.7486) ([0.554]+[0.232])	Prec@1 83.594 (82.307)
Epoch: [53][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9422 (0.7541) ([0.710]+[0.232])	Prec@1 77.344 (82.221)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.2584 (1.2584) ([1.027]+[0.232])	Prec@1 70.312 (70.312)
 * Prec@1 65.740
current lr 1.00000e-01
Grad=  tensor(2.4945, device='cuda:0')
Epoch: [54][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.8355 (0.8355) ([0.604]+[0.232])	Prec@1 75.781 (75.781)
Epoch: [54][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8561 (0.7484) ([0.624]+[0.232])	Prec@1 81.250 (82.155)
Epoch: [54][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6745 (0.7595) ([0.441]+[0.233])	Prec@1 85.156 (81.709)
Epoch: [54][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8642 (0.7619) ([0.632]+[0.233])	Prec@1 75.781 (81.728)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.3349 (1.3349) ([1.103]+[0.231])	Prec@1 69.531 (69.531)
 * Prec@1 67.140
current lr 1.00000e-01
Grad=  tensor(2.1174, device='cuda:0')
Epoch: [55][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.6881 (0.6881) ([0.457]+[0.231])	Prec@1 82.812 (82.812)
Epoch: [55][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6349 (0.7536) ([0.403]+[0.232])	Prec@1 89.062 (82.054)
Epoch: [55][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7564 (0.7482) ([0.524]+[0.232])	Prec@1 80.469 (82.156)
Epoch: [55][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8645 (0.7496) ([0.633]+[0.231])	Prec@1 78.906 (82.187)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.8324 (0.8324) ([0.602]+[0.231])	Prec@1 77.344 (77.344)
 * Prec@1 79.250
current lr 1.00000e-01
Grad=  tensor(2.0750, device='cuda:0')
Epoch: [56][0/391]	Time 0.267 (0.267)	Data 0.144 (0.144)	Loss 0.7380 (0.7380) ([0.507]+[0.231])	Prec@1 84.375 (84.375)
Epoch: [56][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.7774 (0.7271) ([0.547]+[0.230])	Prec@1 79.688 (83.184)
Epoch: [56][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7486 (0.7429) ([0.518]+[0.231])	Prec@1 82.031 (82.540)
Epoch: [56][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7068 (0.7490) ([0.476]+[0.231])	Prec@1 82.031 (82.369)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.1471 (1.1471) ([0.918]+[0.229])	Prec@1 74.219 (74.219)
 * Prec@1 71.870
current lr 1.00000e-01
Grad=  tensor(1.4453, device='cuda:0')
Epoch: [57][0/391]	Time 0.270 (0.270)	Data 0.146 (0.146)	Loss 0.6526 (0.6526) ([0.423]+[0.229])	Prec@1 85.156 (85.156)
Epoch: [57][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.7033 (0.7199) ([0.474]+[0.229])	Prec@1 82.031 (83.099)
Epoch: [57][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8453 (0.7315) ([0.616]+[0.230])	Prec@1 75.781 (82.696)
Epoch: [57][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6753 (0.7365) ([0.445]+[0.230])	Prec@1 85.938 (82.620)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.3472 (1.3472) ([1.118]+[0.229])	Prec@1 69.531 (69.531)
 * Prec@1 71.070
current lr 1.00000e-01
Grad=  tensor(1.7664, device='cuda:0')
Epoch: [58][0/391]	Time 0.270 (0.270)	Data 0.146 (0.146)	Loss 0.6738 (0.6738) ([0.444]+[0.229])	Prec@1 85.156 (85.156)
Epoch: [58][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.7482 (0.7366) ([0.518]+[0.230])	Prec@1 80.469 (82.635)
Epoch: [58][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.5855 (0.7475) ([0.355]+[0.230])	Prec@1 87.500 (82.556)
Epoch: [58][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7870 (0.7505) ([0.557]+[0.230])	Prec@1 80.469 (82.397)
Test: [0/79]	Time 0.183 (0.183)	Loss 1.2027 (1.2027) ([0.973]+[0.230])	Prec@1 71.094 (71.094)
 * Prec@1 70.620
current lr 1.00000e-01
Grad=  tensor(1.9554, device='cuda:0')
Epoch: [59][0/391]	Time 0.266 (0.266)	Data 0.143 (0.143)	Loss 0.7746 (0.7746) ([0.544]+[0.230])	Prec@1 85.156 (85.156)
Epoch: [59][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.7644 (0.7229) ([0.534]+[0.230])	Prec@1 82.812 (83.083)
Epoch: [59][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7879 (0.7449) ([0.557]+[0.230])	Prec@1 78.906 (82.253)
Epoch: [59][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6831 (0.7419) ([0.453]+[0.230])	Prec@1 85.938 (82.353)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.9789 (0.9789) ([0.750]+[0.229])	Prec@1 72.656 (72.656)
 * Prec@1 72.970
current lr 1.00000e-01
Grad=  tensor(1.8522, device='cuda:0')
Epoch: [60][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.7448 (0.7448) ([0.516]+[0.229])	Prec@1 85.938 (85.938)
Epoch: [60][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9202 (0.7357) ([0.691]+[0.229])	Prec@1 76.562 (83.014)
Epoch: [60][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7004 (0.7437) ([0.471]+[0.229])	Prec@1 82.031 (82.568)
Epoch: [60][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8302 (0.7462) ([0.601]+[0.229])	Prec@1 77.344 (82.470)
Test: [0/79]	Time 0.181 (0.181)	Loss 1.0311 (1.0311) ([0.802]+[0.229])	Prec@1 75.781 (75.781)
 * Prec@1 72.950
current lr 1.00000e-01
Grad=  tensor(1.7790, device='cuda:0')
Epoch: [61][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.6614 (0.6614) ([0.433]+[0.229])	Prec@1 85.938 (85.938)
Epoch: [61][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6706 (0.7577) ([0.441]+[0.229])	Prec@1 88.281 (81.900)
Epoch: [61][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6315 (0.7507) ([0.403]+[0.228])	Prec@1 85.938 (82.264)
Epoch: [61][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7206 (0.7462) ([0.493]+[0.228])	Prec@1 84.375 (82.501)
Test: [0/79]	Time 0.176 (0.176)	Loss 1.0912 (1.0912) ([0.863]+[0.228])	Prec@1 72.656 (72.656)
 * Prec@1 73.660
current lr 1.00000e-01
Grad=  tensor(1.5293, device='cuda:0')
Epoch: [62][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.6231 (0.6231) ([0.395]+[0.228])	Prec@1 88.281 (88.281)
Epoch: [62][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.5653 (0.7404) ([0.337]+[0.228])	Prec@1 91.406 (82.464)
Epoch: [62][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7436 (0.7404) ([0.516]+[0.228])	Prec@1 79.688 (82.482)
Epoch: [62][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7703 (0.7411) ([0.543]+[0.227])	Prec@1 81.250 (82.493)
Test: [0/79]	Time 0.182 (0.182)	Loss 0.7043 (0.7043) ([0.477]+[0.227])	Prec@1 83.594 (83.594)
 * Prec@1 79.840
current lr 1.00000e-01
Grad=  tensor(3.3674, device='cuda:0')
Epoch: [63][0/391]	Time 0.274 (0.274)	Data 0.151 (0.151)	Loss 0.9410 (0.9410) ([0.714]+[0.227])	Prec@1 76.562 (76.562)
Epoch: [63][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.6665 (0.7325) ([0.439]+[0.227])	Prec@1 86.719 (82.820)
Epoch: [63][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.9125 (0.7385) ([0.685]+[0.227])	Prec@1 75.781 (82.665)
Epoch: [63][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7684 (0.7377) ([0.542]+[0.227])	Prec@1 79.688 (82.620)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.8966 (0.8966) ([0.670]+[0.227])	Prec@1 77.344 (77.344)
 * Prec@1 76.940
current lr 1.00000e-01
Grad=  tensor(2.5925, device='cuda:0')
Epoch: [64][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.7374 (0.7374) ([0.510]+[0.227])	Prec@1 82.031 (82.031)
Epoch: [64][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9027 (0.7371) ([0.676]+[0.227])	Prec@1 82.031 (82.635)
Epoch: [64][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7164 (0.7472) ([0.489]+[0.227])	Prec@1 83.594 (82.214)
Epoch: [64][300/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.5627 (0.7476) ([0.336]+[0.227])	Prec@1 88.281 (82.166)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.8791 (0.8791) ([0.652]+[0.227])	Prec@1 76.562 (76.562)
 * Prec@1 74.210
current lr 1.00000e-01
Grad=  tensor(1.9203, device='cuda:0')
Epoch: [65][0/391]	Time 0.261 (0.261)	Data 0.140 (0.140)	Loss 0.6554 (0.6554) ([0.429]+[0.227])	Prec@1 85.938 (85.938)
Epoch: [65][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.6453 (0.7189) ([0.419]+[0.227])	Prec@1 85.156 (83.277)
Epoch: [65][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8150 (0.7307) ([0.588]+[0.227])	Prec@1 79.688 (82.812)
Epoch: [65][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7578 (0.7382) ([0.530]+[0.228])	Prec@1 78.125 (82.457)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.7598 (0.7598) ([0.534]+[0.226])	Prec@1 78.906 (78.906)
 * Prec@1 78.450
current lr 1.00000e-01
Grad=  tensor(1.9784, device='cuda:0')
Epoch: [66][0/391]	Time 0.264 (0.264)	Data 0.143 (0.143)	Loss 0.6773 (0.6773) ([0.451]+[0.226])	Prec@1 87.500 (87.500)
Epoch: [66][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.7597 (0.7421) ([0.533]+[0.226])	Prec@1 81.250 (82.472)
Epoch: [66][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.6326 (0.7446) ([0.406]+[0.227])	Prec@1 89.844 (82.257)
Epoch: [66][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.7306 (0.7413) ([0.504]+[0.227])	Prec@1 81.250 (82.428)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.8206 (0.8206) ([0.593]+[0.227])	Prec@1 79.688 (79.688)
 * Prec@1 78.740
current lr 1.00000e-01
Grad=  tensor(1.6176, device='cuda:0')
Epoch: [67][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.6556 (0.6556) ([0.428]+[0.227])	Prec@1 85.156 (85.156)
Epoch: [67][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.8259 (0.7333) ([0.599]+[0.227])	Prec@1 80.469 (82.735)
Epoch: [67][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6439 (0.7273) ([0.417]+[0.226])	Prec@1 86.719 (82.960)
Epoch: [67][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6494 (0.7334) ([0.422]+[0.227])	Prec@1 89.062 (82.802)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.2322 (1.2322) ([1.005]+[0.227])	Prec@1 67.969 (67.969)
 * Prec@1 69.880
current lr 1.00000e-01
Grad=  tensor(1.9129, device='cuda:0')
Epoch: [68][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.7382 (0.7382) ([0.511]+[0.227])	Prec@1 81.250 (81.250)
Epoch: [68][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7731 (0.7305) ([0.546]+[0.227])	Prec@1 84.375 (82.867)
Epoch: [68][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.7583 (0.7299) ([0.533]+[0.226])	Prec@1 78.906 (82.863)
Epoch: [68][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.7807 (0.7343) ([0.556]+[0.225])	Prec@1 79.688 (82.784)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.8642 (1.8642) ([1.640]+[0.224])	Prec@1 59.375 (59.375)
 * Prec@1 60.320
current lr 1.00000e-01
Grad=  tensor(2.2045, device='cuda:0')
Epoch: [69][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.8449 (0.8449) ([0.621]+[0.224])	Prec@1 78.906 (78.906)
Epoch: [69][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.8069 (0.7227) ([0.582]+[0.224])	Prec@1 84.375 (83.037)
Epoch: [69][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.5819 (0.7356) ([0.356]+[0.225])	Prec@1 89.844 (82.665)
Epoch: [69][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.8363 (0.7379) ([0.611]+[0.226])	Prec@1 80.469 (82.519)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.7912 (0.7912) ([0.566]+[0.225])	Prec@1 80.469 (80.469)
 * Prec@1 77.020
current lr 1.00000e-01
Grad=  tensor(1.6964, device='cuda:0')
Epoch: [70][0/391]	Time 0.256 (0.256)	Data 0.135 (0.135)	Loss 0.7311 (0.7311) ([0.506]+[0.225])	Prec@1 81.250 (81.250)
Epoch: [70][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6854 (0.7351) ([0.460]+[0.225])	Prec@1 83.594 (82.952)
Epoch: [70][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6857 (0.7372) ([0.460]+[0.226])	Prec@1 82.812 (82.548)
Epoch: [70][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7261 (0.7373) ([0.501]+[0.225])	Prec@1 82.812 (82.623)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.2911 (1.2911) ([1.066]+[0.225])	Prec@1 71.875 (71.875)
 * Prec@1 68.570
current lr 1.00000e-01
Grad=  tensor(1.8441, device='cuda:0')
Epoch: [71][0/391]	Time 0.264 (0.264)	Data 0.138 (0.138)	Loss 0.6360 (0.6360) ([0.411]+[0.225])	Prec@1 85.156 (85.156)
Epoch: [71][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7952 (0.7351) ([0.570]+[0.225])	Prec@1 82.031 (82.650)
Epoch: [71][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6108 (0.7338) ([0.386]+[0.225])	Prec@1 82.812 (82.696)
Epoch: [71][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6886 (0.7374) ([0.464]+[0.225])	Prec@1 87.500 (82.597)
Test: [0/79]	Time 0.178 (0.178)	Loss 1.3779 (1.3779) ([1.153]+[0.225])	Prec@1 65.625 (65.625)
 * Prec@1 67.160
current lr 1.00000e-01
Grad=  tensor(1.6108, device='cuda:0')
Epoch: [72][0/391]	Time 0.268 (0.268)	Data 0.145 (0.145)	Loss 0.7075 (0.7075) ([0.483]+[0.225])	Prec@1 84.375 (84.375)
Epoch: [72][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.8164 (0.7117) ([0.592]+[0.224])	Prec@1 79.688 (83.470)
Epoch: [72][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8340 (0.7262) ([0.610]+[0.224])	Prec@1 77.344 (82.844)
Epoch: [72][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8231 (0.7358) ([0.599]+[0.224])	Prec@1 78.906 (82.428)
Test: [0/79]	Time 0.176 (0.176)	Loss 1.2631 (1.2631) ([1.040]+[0.223])	Prec@1 68.750 (68.750)
 * Prec@1 67.980
current lr 1.00000e-01
Grad=  tensor(1.9939, device='cuda:0')
Epoch: [73][0/391]	Time 0.266 (0.266)	Data 0.143 (0.143)	Loss 0.7231 (0.7231) ([0.500]+[0.223])	Prec@1 82.812 (82.812)
Epoch: [73][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.7771 (0.7438) ([0.553]+[0.224])	Prec@1 80.469 (82.093)
Epoch: [73][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7152 (0.7309) ([0.492]+[0.224])	Prec@1 88.281 (82.540)
Epoch: [73][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7180 (0.7325) ([0.494]+[0.224])	Prec@1 82.031 (82.517)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.9261 (0.9261) ([0.702]+[0.224])	Prec@1 72.656 (72.656)
 * Prec@1 76.710
current lr 1.00000e-01
Grad=  tensor(2.4231, device='cuda:0')
Epoch: [74][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.7678 (0.7678) ([0.544]+[0.224])	Prec@1 81.250 (81.250)
Epoch: [74][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6391 (0.7413) ([0.415]+[0.224])	Prec@1 88.281 (82.310)
Epoch: [74][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8390 (0.7258) ([0.616]+[0.223])	Prec@1 83.594 (82.925)
Epoch: [74][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9551 (0.7271) ([0.732]+[0.223])	Prec@1 79.688 (82.893)
Test: [0/79]	Time 0.169 (0.169)	Loss 1.0697 (1.0697) ([0.847]+[0.222])	Prec@1 71.094 (71.094)
 * Prec@1 71.170
current lr 1.00000e-01
Grad=  tensor(1.2132, device='cuda:0')
Epoch: [75][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.5317 (0.5317) ([0.309]+[0.222])	Prec@1 88.281 (88.281)
Epoch: [75][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.6397 (0.7175) ([0.417]+[0.223])	Prec@1 85.938 (83.207)
Epoch: [75][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8498 (0.7289) ([0.627]+[0.223])	Prec@1 81.250 (82.898)
Epoch: [75][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8079 (0.7294) ([0.586]+[0.222])	Prec@1 78.906 (82.797)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.8804 (0.8804) ([0.659]+[0.222])	Prec@1 78.125 (78.125)
 * Prec@1 76.930
current lr 1.00000e-01
Grad=  tensor(3.8432, device='cuda:0')
Epoch: [76][0/391]	Time 0.262 (0.262)	Data 0.139 (0.139)	Loss 0.8779 (0.8779) ([0.656]+[0.222])	Prec@1 75.000 (75.000)
Epoch: [76][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6039 (0.7229) ([0.382]+[0.222])	Prec@1 87.500 (82.874)
Epoch: [76][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.5694 (0.7295) ([0.347]+[0.223])	Prec@1 90.625 (82.653)
Epoch: [76][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6986 (0.7331) ([0.475]+[0.223])	Prec@1 83.594 (82.441)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.8812 (0.8812) ([0.658]+[0.223])	Prec@1 75.000 (75.000)
 * Prec@1 78.360
current lr 1.00000e-01
Grad=  tensor(1.6445, device='cuda:0')
Epoch: [77][0/391]	Time 0.263 (0.263)	Data 0.140 (0.140)	Loss 0.7231 (0.7231) ([0.500]+[0.223])	Prec@1 85.156 (85.156)
Epoch: [77][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6330 (0.7126) ([0.411]+[0.222])	Prec@1 85.156 (83.300)
Epoch: [77][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8606 (0.7233) ([0.637]+[0.224])	Prec@1 78.906 (82.746)
Epoch: [77][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7236 (0.7313) ([0.500]+[0.223])	Prec@1 78.125 (82.460)
Test: [0/79]	Time 0.170 (0.170)	Loss 1.1115 (1.1115) ([0.889]+[0.223])	Prec@1 74.219 (74.219)
 * Prec@1 73.020
current lr 1.00000e-01
Grad=  tensor(2.3268, device='cuda:0')
Epoch: [78][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.7406 (0.7406) ([0.518]+[0.223])	Prec@1 82.031 (82.031)
Epoch: [78][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7746 (0.7230) ([0.551]+[0.223])	Prec@1 83.594 (82.990)
Epoch: [78][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7823 (0.7304) ([0.559]+[0.224])	Prec@1 82.031 (82.704)
Epoch: [78][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.9106 (0.7314) ([0.688]+[0.223])	Prec@1 76.562 (82.594)
Test: [0/79]	Time 0.165 (0.165)	Loss 1.0712 (1.0712) ([0.849]+[0.223])	Prec@1 72.656 (72.656)
 * Prec@1 72.370
current lr 1.00000e-01
Grad=  tensor(1.7710, device='cuda:0')
Epoch: [79][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.7129 (0.7129) ([0.490]+[0.223])	Prec@1 85.156 (85.156)
Epoch: [79][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7919 (0.7147) ([0.569]+[0.223])	Prec@1 84.375 (83.277)
Epoch: [79][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7582 (0.7265) ([0.535]+[0.223])	Prec@1 78.906 (82.696)
Epoch: [79][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6492 (0.7251) ([0.427]+[0.223])	Prec@1 82.031 (82.838)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.9481 (0.9481) ([0.726]+[0.222])	Prec@1 78.906 (78.906)
 * Prec@1 78.000
current lr 1.00000e-01
Grad=  tensor(2.0108, device='cuda:0')
Epoch: [80][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.7156 (0.7156) ([0.494]+[0.222])	Prec@1 85.938 (85.938)
Epoch: [80][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.9277 (0.7263) ([0.706]+[0.222])	Prec@1 75.000 (83.060)
Epoch: [80][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.9074 (0.7397) ([0.685]+[0.222])	Prec@1 75.000 (82.338)
Epoch: [80][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7387 (0.7348) ([0.516]+[0.223])	Prec@1 84.375 (82.465)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0577 (1.0577) ([0.836]+[0.222])	Prec@1 71.875 (71.875)
 * Prec@1 72.940
current lr 1.00000e-01
Grad=  tensor(1.5023, device='cuda:0')
Epoch: [81][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.6257 (0.6257) ([0.404]+[0.222])	Prec@1 86.719 (86.719)
Epoch: [81][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7163 (0.7040) ([0.495]+[0.221])	Prec@1 82.812 (83.230)
Epoch: [81][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.8382 (0.7115) ([0.617]+[0.222])	Prec@1 78.906 (83.151)
Epoch: [81][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6796 (0.7249) ([0.458]+[0.222])	Prec@1 80.469 (82.758)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.1971 (1.1971) ([0.975]+[0.222])	Prec@1 67.188 (67.188)
 * Prec@1 67.690
current lr 1.00000e-01
Grad=  tensor(2.3780, device='cuda:0')
Epoch: [82][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.7577 (0.7577) ([0.536]+[0.222])	Prec@1 83.594 (83.594)
Epoch: [82][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7740 (0.7311) ([0.551]+[0.223])	Prec@1 79.688 (82.410)
Epoch: [82][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7998 (0.7208) ([0.578]+[0.222])	Prec@1 82.812 (83.034)
Epoch: [82][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8299 (0.7240) ([0.609]+[0.221])	Prec@1 78.125 (82.896)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.8405 (0.8405) ([0.620]+[0.221])	Prec@1 78.906 (78.906)
 * Prec@1 74.950
current lr 1.00000e-01
Grad=  tensor(1.9530, device='cuda:0')
Epoch: [83][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.6382 (0.6382) ([0.418]+[0.221])	Prec@1 83.594 (83.594)
Epoch: [83][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7151 (0.7369) ([0.493]+[0.222])	Prec@1 83.594 (82.464)
Epoch: [83][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7082 (0.7321) ([0.487]+[0.222])	Prec@1 85.156 (82.673)
Epoch: [83][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6117 (0.7220) ([0.391]+[0.220])	Prec@1 85.156 (82.958)
Test: [0/79]	Time 0.166 (0.166)	Loss 1.4280 (1.4280) ([1.207]+[0.221])	Prec@1 64.844 (64.844)
 * Prec@1 62.960
current lr 1.00000e-01
Grad=  tensor(1.6885, device='cuda:0')
Epoch: [84][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.6518 (0.6518) ([0.430]+[0.221])	Prec@1 87.500 (87.500)
Epoch: [84][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.9177 (0.7256) ([0.696]+[0.222])	Prec@1 79.688 (82.495)
Epoch: [84][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6693 (0.7345) ([0.447]+[0.222])	Prec@1 85.156 (82.342)
Epoch: [84][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6163 (0.7371) ([0.395]+[0.221])	Prec@1 86.719 (82.325)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.0593 (1.0593) ([0.839]+[0.220])	Prec@1 76.562 (76.562)
 * Prec@1 74.560
current lr 1.00000e-01
Grad=  tensor(2.0339, device='cuda:0')
Epoch: [85][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.7018 (0.7018) ([0.481]+[0.220])	Prec@1 82.031 (82.031)
Epoch: [85][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6798 (0.7147) ([0.459]+[0.221])	Prec@1 81.250 (83.114)
Epoch: [85][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8107 (0.7169) ([0.590]+[0.220])	Prec@1 84.375 (83.139)
Epoch: [85][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.7071 (0.7189) ([0.487]+[0.220])	Prec@1 85.156 (83.127)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.0481 (1.0481) ([0.828]+[0.220])	Prec@1 75.781 (75.781)
 * Prec@1 76.190
current lr 1.00000e-01
Grad=  tensor(1.7595, device='cuda:0')
Epoch: [86][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.6839 (0.6839) ([0.464]+[0.220])	Prec@1 84.375 (84.375)
Epoch: [86][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.6040 (0.7146) ([0.383]+[0.221])	Prec@1 86.719 (82.998)
Epoch: [86][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.7540 (0.7312) ([0.533]+[0.221])	Prec@1 82.812 (82.432)
Epoch: [86][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.5383 (0.7343) ([0.317]+[0.221])	Prec@1 91.406 (82.371)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.8200 (0.8200) ([0.600]+[0.220])	Prec@1 81.250 (81.250)
 * Prec@1 79.200
current lr 1.00000e-01
Grad=  tensor(1.6265, device='cuda:0')
Epoch: [87][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.5897 (0.5897) ([0.369]+[0.220])	Prec@1 87.500 (87.500)
Epoch: [87][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8191 (0.7192) ([0.598]+[0.221])	Prec@1 82.031 (82.990)
Epoch: [87][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.9218 (0.7211) ([0.701]+[0.220])	Prec@1 74.219 (82.984)
Epoch: [87][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6826 (0.7189) ([0.463]+[0.220])	Prec@1 83.594 (83.002)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9607 (0.9607) ([0.740]+[0.220])	Prec@1 74.219 (74.219)
 * Prec@1 73.630
current lr 1.00000e-01
Grad=  tensor(1.8579, device='cuda:0')
Epoch: [88][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.6790 (0.6790) ([0.459]+[0.220])	Prec@1 84.375 (84.375)
Epoch: [88][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7712 (0.7274) ([0.551]+[0.221])	Prec@1 82.812 (82.410)
Epoch: [88][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.6793 (0.7351) ([0.458]+[0.221])	Prec@1 85.938 (82.191)
Epoch: [88][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7412 (0.7341) ([0.520]+[0.221])	Prec@1 84.375 (82.319)
Test: [0/79]	Time 0.215 (0.215)	Loss 0.9108 (0.9108) ([0.690]+[0.221])	Prec@1 78.125 (78.125)
 * Prec@1 72.820
current lr 1.00000e-01
Grad=  tensor(2.4897, device='cuda:0')
Epoch: [89][0/391]	Time 0.348 (0.348)	Data 0.182 (0.182)	Loss 0.8055 (0.8055) ([0.585]+[0.221])	Prec@1 81.250 (81.250)
Epoch: [89][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.6607 (0.7013) ([0.441]+[0.220])	Prec@1 83.594 (83.338)
Epoch: [89][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6655 (0.7139) ([0.446]+[0.219])	Prec@1 85.938 (83.092)
Epoch: [89][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7915 (0.7160) ([0.572]+[0.219])	Prec@1 78.125 (82.818)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.7939 (0.7939) ([0.575]+[0.219])	Prec@1 78.125 (78.125)
 * Prec@1 74.900
current lr 1.00000e-01
Grad=  tensor(1.9115, device='cuda:0')
Epoch: [90][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.7036 (0.7036) ([0.484]+[0.219])	Prec@1 84.375 (84.375)
Epoch: [90][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8507 (0.7220) ([0.630]+[0.220])	Prec@1 78.906 (82.611)
Epoch: [90][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7895 (0.7259) ([0.569]+[0.220])	Prec@1 82.812 (82.544)
Epoch: [90][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6415 (0.7233) ([0.421]+[0.220])	Prec@1 85.938 (82.742)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9317 (0.9317) ([0.711]+[0.220])	Prec@1 75.781 (75.781)
 * Prec@1 75.740
current lr 1.00000e-01
Grad=  tensor(2.0472, device='cuda:0')
Epoch: [91][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.6859 (0.6859) ([0.466]+[0.220])	Prec@1 80.469 (80.469)
Epoch: [91][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7307 (0.7151) ([0.510]+[0.220])	Prec@1 79.688 (82.874)
Epoch: [91][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6989 (0.7227) ([0.478]+[0.220])	Prec@1 83.594 (82.645)
Epoch: [91][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6257 (0.7255) ([0.405]+[0.220])	Prec@1 85.938 (82.605)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.9427 (0.9427) ([0.723]+[0.220])	Prec@1 81.250 (81.250)
 * Prec@1 73.310
current lr 1.00000e-01
Grad=  tensor(2.4762, device='cuda:0')
Epoch: [92][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.7902 (0.7902) ([0.571]+[0.220])	Prec@1 80.469 (80.469)
Epoch: [92][100/391]	Time 0.110 (0.113)	Data 0.000 (0.002)	Loss 0.7323 (0.7025) ([0.513]+[0.219])	Prec@1 84.375 (83.710)
Epoch: [92][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.5925 (0.7182) ([0.373]+[0.220])	Prec@1 87.500 (83.073)
Epoch: [92][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.6696 (0.7192) ([0.450]+[0.219])	Prec@1 85.156 (82.960)
Test: [0/79]	Time 0.167 (0.167)	Loss 1.0779 (1.0779) ([0.859]+[0.219])	Prec@1 78.125 (78.125)
 * Prec@1 71.920
current lr 1.00000e-01
Grad=  tensor(1.5589, device='cuda:0')
Epoch: [93][0/391]	Time 0.262 (0.262)	Data 0.139 (0.139)	Loss 0.6533 (0.6533) ([0.434]+[0.219])	Prec@1 85.156 (85.156)
Epoch: [93][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.8524 (0.7080) ([0.633]+[0.219])	Prec@1 77.344 (83.238)
Epoch: [93][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7581 (0.7145) ([0.539]+[0.219])	Prec@1 79.688 (82.875)
Epoch: [93][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.7214 (0.7101) ([0.503]+[0.219])	Prec@1 80.469 (83.064)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.8624 (0.8624) ([0.643]+[0.219])	Prec@1 78.125 (78.125)
 * Prec@1 79.050
current lr 1.00000e-01
Grad=  tensor(1.8980, device='cuda:0')
Epoch: [94][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.7284 (0.7284) ([0.509]+[0.219])	Prec@1 82.812 (82.812)
Epoch: [94][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.7170 (0.7101) ([0.498]+[0.219])	Prec@1 80.469 (83.037)
Epoch: [94][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6986 (0.7065) ([0.480]+[0.219])	Prec@1 85.156 (83.294)
Epoch: [94][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7810 (0.7152) ([0.562]+[0.219])	Prec@1 78.125 (82.973)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.8664 (0.8664) ([0.647]+[0.219])	Prec@1 75.000 (75.000)
 * Prec@1 73.990
current lr 1.00000e-01
Grad=  tensor(2.2547, device='cuda:0')
Epoch: [95][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.7671 (0.7671) ([0.548]+[0.219])	Prec@1 82.031 (82.031)
Epoch: [95][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.6102 (0.7204) ([0.391]+[0.219])	Prec@1 85.156 (82.782)
Epoch: [95][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8583 (0.7228) ([0.639]+[0.219])	Prec@1 78.906 (82.894)
Epoch: [95][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6670 (0.7191) ([0.448]+[0.219])	Prec@1 81.250 (83.046)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.8346 (0.8346) ([0.616]+[0.218])	Prec@1 80.469 (80.469)
 * Prec@1 75.380
current lr 1.00000e-01
Grad=  tensor(1.4559, device='cuda:0')
Epoch: [96][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.5529 (0.5529) ([0.335]+[0.218])	Prec@1 86.719 (86.719)
Epoch: [96][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.8991 (0.7193) ([0.680]+[0.219])	Prec@1 78.906 (82.859)
Epoch: [96][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6668 (0.7216) ([0.448]+[0.219])	Prec@1 85.156 (82.844)
Epoch: [96][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7804 (0.7257) ([0.561]+[0.219])	Prec@1 78.906 (82.729)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.8111 (0.8111) ([0.593]+[0.218])	Prec@1 78.906 (78.906)
 * Prec@1 75.160
current lr 1.00000e-01
Grad=  tensor(2.2064, device='cuda:0')
Epoch: [97][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.9391 (0.9391) ([0.721]+[0.218])	Prec@1 75.781 (75.781)
Epoch: [97][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.7067 (0.7069) ([0.489]+[0.217])	Prec@1 81.250 (83.377)
Epoch: [97][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7328 (0.7158) ([0.516]+[0.217])	Prec@1 82.031 (83.050)
Epoch: [97][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.7224 (0.7158) ([0.505]+[0.217])	Prec@1 81.250 (83.067)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.9258 (0.9258) ([0.708]+[0.218])	Prec@1 81.250 (81.250)
 * Prec@1 76.160
current lr 1.00000e-01
Grad=  tensor(2.3525, device='cuda:0')
Epoch: [98][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.8238 (0.8238) ([0.606]+[0.218])	Prec@1 82.031 (82.031)
Epoch: [98][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.7488 (0.7297) ([0.530]+[0.219])	Prec@1 82.031 (82.503)
Epoch: [98][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6496 (0.7179) ([0.431]+[0.218])	Prec@1 85.156 (82.914)
Epoch: [98][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6011 (0.7203) ([0.383]+[0.218])	Prec@1 85.156 (82.859)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.9609 (0.9609) ([0.744]+[0.217])	Prec@1 71.875 (71.875)
 * Prec@1 76.240
current lr 1.00000e-01
Grad=  tensor(1.4698, device='cuda:0')
Epoch: [99][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.5840 (0.5840) ([0.367]+[0.217])	Prec@1 87.500 (87.500)
Epoch: [99][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.8964 (0.7006) ([0.680]+[0.217])	Prec@1 78.125 (83.439)
Epoch: [99][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6480 (0.7036) ([0.431]+[0.217])	Prec@1 82.812 (83.302)
Epoch: [99][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.6001 (0.7151) ([0.383]+[0.217])	Prec@1 86.719 (82.929)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.9870 (0.9870) ([0.769]+[0.218])	Prec@1 73.438 (73.438)
 * Prec@1 77.950
current lr 1.00000e-02
Grad=  tensor(1.9144, device='cuda:0')
Epoch: [100][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.7197 (0.7197) ([0.502]+[0.218])	Prec@1 79.688 (79.688)
Epoch: [100][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.5057 (0.5648) ([0.301]+[0.205])	Prec@1 88.281 (87.825)
Epoch: [100][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.6165 (0.5414) ([0.414]+[0.203])	Prec@1 82.812 (88.592)
Epoch: [100][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.4764 (0.5253) ([0.275]+[0.201])	Prec@1 89.844 (89.091)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4202 (0.4202) ([0.220]+[0.200])	Prec@1 92.969 (92.969)
 * Prec@1 89.610
current lr 1.00000e-02
Grad=  tensor(1.3239, device='cuda:0')
Epoch: [101][0/391]	Time 0.258 (0.258)	Data 0.137 (0.137)	Loss 0.4411 (0.4411) ([0.241]+[0.200])	Prec@1 91.406 (91.406)
Epoch: [101][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4224 (0.4570) ([0.224]+[0.198])	Prec@1 92.969 (90.958)
Epoch: [101][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4910 (0.4547) ([0.294]+[0.197])	Prec@1 89.062 (91.021)
Epoch: [101][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4055 (0.4538) ([0.210]+[0.195])	Prec@1 91.406 (91.129)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.3738 (0.3738) ([0.180]+[0.194])	Prec@1 96.094 (96.094)
 * Prec@1 90.360
current lr 1.00000e-02
Grad=  tensor(1.3284, device='cuda:0')
Epoch: [102][0/391]	Time 0.267 (0.267)	Data 0.145 (0.145)	Loss 0.3779 (0.3779) ([0.184]+[0.194])	Prec@1 93.750 (93.750)
Epoch: [102][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.4820 (0.4228) ([0.290]+[0.192])	Prec@1 92.188 (92.172)
Epoch: [102][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3517 (0.4188) ([0.161]+[0.191])	Prec@1 94.531 (92.257)
Epoch: [102][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4703 (0.4187) ([0.281]+[0.190])	Prec@1 90.625 (92.291)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4006 (0.4006) ([0.212]+[0.188])	Prec@1 91.406 (91.406)
 * Prec@1 90.510
current lr 1.00000e-02
Grad=  tensor(1.8170, device='cuda:0')
Epoch: [103][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.3965 (0.3965) ([0.208]+[0.188])	Prec@1 95.312 (95.312)
Epoch: [103][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.4928 (0.4062) ([0.306]+[0.187])	Prec@1 89.062 (92.551)
Epoch: [103][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4628 (0.4012) ([0.277]+[0.186])	Prec@1 89.062 (92.658)
Epoch: [103][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.4811 (0.4001) ([0.297]+[0.184])	Prec@1 89.844 (92.647)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3770 (0.3770) ([0.194]+[0.183])	Prec@1 92.969 (92.969)
 * Prec@1 90.770
current lr 1.00000e-02
Grad=  tensor(2.1157, device='cuda:0')
Epoch: [104][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.3723 (0.3723) ([0.189]+[0.183])	Prec@1 92.188 (92.188)
Epoch: [104][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3889 (0.3833) ([0.207]+[0.182])	Prec@1 92.188 (93.270)
Epoch: [104][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3164 (0.3852) ([0.136]+[0.181])	Prec@1 94.531 (93.066)
Epoch: [104][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4873 (0.3859) ([0.308]+[0.180])	Prec@1 89.062 (93.054)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3730 (0.3730) ([0.194]+[0.179])	Prec@1 93.750 (93.750)
 * Prec@1 90.820
current lr 1.00000e-02
Grad=  tensor(2.0962, device='cuda:0')
Epoch: [105][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.3782 (0.3782) ([0.200]+[0.179])	Prec@1 92.969 (92.969)
Epoch: [105][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3471 (0.3710) ([0.170]+[0.177])	Prec@1 92.969 (93.286)
Epoch: [105][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3216 (0.3718) ([0.145]+[0.176])	Prec@1 95.312 (93.354)
Epoch: [105][300/391]	Time 0.111 (0.110)	Data 0.000 (0.001)	Loss 0.4175 (0.3723) ([0.242]+[0.175])	Prec@1 92.188 (93.330)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.3977 (0.3977) ([0.223]+[0.174])	Prec@1 90.625 (90.625)
 * Prec@1 91.160
current lr 1.00000e-02
Grad=  tensor(2.1450, device='cuda:0')
Epoch: [106][0/391]	Time 0.260 (0.260)	Data 0.139 (0.139)	Loss 0.3699 (0.3699) ([0.196]+[0.174])	Prec@1 91.406 (91.406)
Epoch: [106][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3400 (0.3516) ([0.167]+[0.173])	Prec@1 93.750 (93.843)
Epoch: [106][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3408 (0.3569) ([0.169]+[0.172])	Prec@1 92.969 (93.668)
Epoch: [106][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3201 (0.3555) ([0.149]+[0.171])	Prec@1 95.312 (93.690)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3911 (0.3911) ([0.221]+[0.170])	Prec@1 92.188 (92.188)
 * Prec@1 90.670
current lr 1.00000e-02
Grad=  tensor(2.9533, device='cuda:0')
Epoch: [107][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.3538 (0.3538) ([0.184]+[0.170])	Prec@1 92.188 (92.188)
Epoch: [107][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3227 (0.3424) ([0.154]+[0.169])	Prec@1 94.531 (94.044)
Epoch: [107][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3241 (0.3459) ([0.156]+[0.168])	Prec@1 95.312 (93.878)
Epoch: [107][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3085 (0.3467) ([0.141]+[0.167])	Prec@1 92.969 (93.869)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4024 (0.4024) ([0.236]+[0.166])	Prec@1 92.188 (92.188)
 * Prec@1 90.390
current lr 1.00000e-02
Grad=  tensor(1.7061, device='cuda:0')
Epoch: [108][0/391]	Time 0.254 (0.254)	Data 0.133 (0.133)	Loss 0.2701 (0.2701) ([0.104]+[0.166])	Prec@1 95.312 (95.312)
Epoch: [108][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3803 (0.3332) ([0.215]+[0.165])	Prec@1 92.188 (94.330)
Epoch: [108][200/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.4721 (0.3303) ([0.308]+[0.164])	Prec@1 85.938 (94.395)
Epoch: [108][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3922 (0.3340) ([0.229]+[0.164])	Prec@1 92.969 (94.331)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3986 (0.3986) ([0.236]+[0.163])	Prec@1 91.406 (91.406)
 * Prec@1 90.740
current lr 1.00000e-02
Grad=  tensor(1.5784, device='cuda:0')
Epoch: [109][0/391]	Time 0.277 (0.277)	Data 0.154 (0.154)	Loss 0.2710 (0.2710) ([0.108]+[0.163])	Prec@1 98.438 (98.438)
Epoch: [109][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.3616 (0.3199) ([0.200]+[0.162])	Prec@1 92.969 (94.732)
Epoch: [109][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3457 (0.3230) ([0.185]+[0.161])	Prec@1 93.750 (94.496)
Epoch: [109][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3047 (0.3284) ([0.144]+[0.160])	Prec@1 95.312 (94.199)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4122 (0.4122) ([0.253]+[0.160])	Prec@1 91.406 (91.406)
 * Prec@1 91.020
current lr 1.00000e-02
Grad=  tensor(1.9608, device='cuda:0')
Epoch: [110][0/391]	Time 0.256 (0.256)	Data 0.135 (0.135)	Loss 0.2655 (0.2655) ([0.106]+[0.160])	Prec@1 96.094 (96.094)
Epoch: [110][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2859 (0.3099) ([0.127]+[0.159])	Prec@1 96.875 (94.794)
Epoch: [110][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2306 (0.3156) ([0.073]+[0.158])	Prec@1 98.438 (94.539)
Epoch: [110][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3762 (0.3200) ([0.219]+[0.157])	Prec@1 93.750 (94.344)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4175 (0.4175) ([0.261]+[0.157])	Prec@1 92.188 (92.188)
 * Prec@1 90.310
current lr 1.00000e-02
Grad=  tensor(2.2811, device='cuda:0')
Epoch: [111][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 0.2559 (0.2559) ([0.099]+[0.157])	Prec@1 96.094 (96.094)
Epoch: [111][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3310 (0.3116) ([0.175]+[0.156])	Prec@1 92.969 (94.748)
Epoch: [111][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2997 (0.3117) ([0.145]+[0.155])	Prec@1 95.312 (94.706)
Epoch: [111][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3412 (0.3150) ([0.187]+[0.154])	Prec@1 94.531 (94.643)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3617 (0.3617) ([0.208]+[0.154])	Prec@1 90.625 (90.625)
 * Prec@1 89.980
current lr 1.00000e-02
Grad=  tensor(4.8484, device='cuda:0')
Epoch: [112][0/391]	Time 0.256 (0.256)	Data 0.135 (0.135)	Loss 0.3176 (0.3176) ([0.164]+[0.154])	Prec@1 94.531 (94.531)
Epoch: [112][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2965 (0.2997) ([0.143]+[0.153])	Prec@1 92.969 (95.444)
Epoch: [112][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3175 (0.3082) ([0.165]+[0.152])	Prec@1 94.531 (94.920)
Epoch: [112][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3627 (0.3080) ([0.211]+[0.152])	Prec@1 92.969 (94.822)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3988 (0.3988) ([0.248]+[0.151])	Prec@1 89.844 (89.844)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(2.4029, device='cuda:0')
Epoch: [113][0/391]	Time 0.254 (0.254)	Data 0.133 (0.133)	Loss 0.2448 (0.2448) ([0.094]+[0.151])	Prec@1 96.875 (96.875)
Epoch: [113][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2815 (0.2987) ([0.131]+[0.151])	Prec@1 94.531 (94.964)
Epoch: [113][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3149 (0.3043) ([0.165]+[0.150])	Prec@1 96.094 (94.799)
Epoch: [113][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3101 (0.3056) ([0.161]+[0.149])	Prec@1 93.750 (94.747)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4017 (0.4017) ([0.253]+[0.149])	Prec@1 89.844 (89.844)
 * Prec@1 90.520
current lr 1.00000e-02
Grad=  tensor(3.8630, device='cuda:0')
Epoch: [114][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.3160 (0.3160) ([0.167]+[0.149])	Prec@1 93.750 (93.750)
Epoch: [114][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2674 (0.2986) ([0.119]+[0.148])	Prec@1 94.531 (94.957)
Epoch: [114][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2844 (0.3000) ([0.137]+[0.148])	Prec@1 96.094 (94.889)
Epoch: [114][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2658 (0.2987) ([0.119]+[0.147])	Prec@1 96.094 (94.884)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4537 (0.4537) ([0.307]+[0.147])	Prec@1 89.062 (89.062)
 * Prec@1 89.900
current lr 1.00000e-02
Grad=  tensor(5.3456, device='cuda:0')
Epoch: [115][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.3036 (0.3036) ([0.157]+[0.147])	Prec@1 95.312 (95.312)
Epoch: [115][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3288 (0.2862) ([0.183]+[0.146])	Prec@1 91.406 (95.320)
Epoch: [115][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2639 (0.2930) ([0.118]+[0.146])	Prec@1 96.094 (94.967)
Epoch: [115][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3619 (0.3001) ([0.217]+[0.145])	Prec@1 92.188 (94.729)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4134 (0.4134) ([0.268]+[0.145])	Prec@1 90.625 (90.625)
 * Prec@1 90.100
current lr 1.00000e-02
Grad=  tensor(4.2665, device='cuda:0')
Epoch: [116][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.3235 (0.3235) ([0.179]+[0.145])	Prec@1 93.750 (93.750)
Epoch: [116][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2546 (0.2837) ([0.110]+[0.144])	Prec@1 97.656 (95.305)
Epoch: [116][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3417 (0.2872) ([0.198]+[0.144])	Prec@1 92.188 (95.153)
Epoch: [116][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2923 (0.2940) ([0.149]+[0.144])	Prec@1 94.531 (94.915)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3566 (0.3566) ([0.213]+[0.143])	Prec@1 92.969 (92.969)
 * Prec@1 90.070
current lr 1.00000e-02
Grad=  tensor(7.1879, device='cuda:0')
Epoch: [117][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.3408 (0.3408) ([0.198]+[0.143])	Prec@1 93.750 (93.750)
Epoch: [117][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2553 (0.2851) ([0.113]+[0.143])	Prec@1 95.312 (95.227)
Epoch: [117][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2835 (0.2907) ([0.141]+[0.142])	Prec@1 94.531 (95.025)
Epoch: [117][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2672 (0.2909) ([0.125]+[0.142])	Prec@1 95.312 (94.939)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.5324 (0.5324) ([0.391]+[0.142])	Prec@1 89.844 (89.844)
 * Prec@1 89.570
current lr 1.00000e-02
Grad=  tensor(5.0661, device='cuda:0')
Epoch: [118][0/391]	Time 0.254 (0.254)	Data 0.133 (0.133)	Loss 0.2965 (0.2965) ([0.155]+[0.142])	Prec@1 94.531 (94.531)
Epoch: [118][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3273 (0.2914) ([0.186]+[0.141])	Prec@1 92.969 (94.748)
Epoch: [118][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2292 (0.2909) ([0.088]+[0.141])	Prec@1 96.875 (94.850)
Epoch: [118][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2509 (0.2924) ([0.110]+[0.141])	Prec@1 95.312 (94.703)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3941 (0.3941) ([0.254]+[0.140])	Prec@1 89.844 (89.844)
 * Prec@1 90.340
current lr 1.00000e-02
Grad=  tensor(6.5202, device='cuda:0')
Epoch: [119][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.3186 (0.3186) ([0.178]+[0.140])	Prec@1 92.188 (92.188)
Epoch: [119][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2155 (0.2720) ([0.076]+[0.140])	Prec@1 97.656 (95.374)
Epoch: [119][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2605 (0.2800) ([0.121]+[0.140])	Prec@1 96.094 (95.114)
Epoch: [119][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.3513 (0.2847) ([0.212]+[0.139])	Prec@1 94.531 (94.892)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4272 (0.4272) ([0.288]+[0.139])	Prec@1 92.969 (92.969)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(5.4262, device='cuda:0')
Epoch: [120][0/391]	Time 0.257 (0.257)	Data 0.137 (0.137)	Loss 0.2733 (0.2733) ([0.134]+[0.139])	Prec@1 96.094 (96.094)
Epoch: [120][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2861 (0.2829) ([0.147]+[0.139])	Prec@1 94.531 (95.111)
Epoch: [120][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3421 (0.2893) ([0.204]+[0.139])	Prec@1 92.188 (94.908)
Epoch: [120][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2611 (0.2920) ([0.123]+[0.138])	Prec@1 95.312 (94.791)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4101 (0.4101) ([0.272]+[0.138])	Prec@1 91.406 (91.406)
 * Prec@1 89.600
current lr 1.00000e-02
Grad=  tensor(7.4152, device='cuda:0')
Epoch: [121][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.3339 (0.3339) ([0.196]+[0.138])	Prec@1 91.406 (91.406)
Epoch: [121][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2523 (0.2771) ([0.115]+[0.138])	Prec@1 96.875 (95.096)
Epoch: [121][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2355 (0.2852) ([0.098]+[0.137])	Prec@1 96.875 (94.963)
Epoch: [121][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2460 (0.2869) ([0.109]+[0.137])	Prec@1 96.875 (94.858)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3161 (0.3161) ([0.179]+[0.137])	Prec@1 96.094 (96.094)
 * Prec@1 90.520
current lr 1.00000e-02
Grad=  tensor(6.2126, device='cuda:0')
Epoch: [122][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2945 (0.2945) ([0.157]+[0.137])	Prec@1 92.969 (92.969)
Epoch: [122][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2368 (0.2826) ([0.100]+[0.137])	Prec@1 97.656 (94.895)
Epoch: [122][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2667 (0.2865) ([0.130]+[0.137])	Prec@1 95.312 (94.745)
Epoch: [122][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2134 (0.2890) ([0.077]+[0.136])	Prec@1 98.438 (94.708)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4360 (0.4360) ([0.300]+[0.136])	Prec@1 92.188 (92.188)
 * Prec@1 90.090
current lr 1.00000e-02
Grad=  tensor(3.2152, device='cuda:0')
Epoch: [123][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2414 (0.2414) ([0.105]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [123][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2844 (0.2753) ([0.148]+[0.136])	Prec@1 94.531 (95.266)
Epoch: [123][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2758 (0.2784) ([0.140]+[0.136])	Prec@1 94.531 (95.141)
Epoch: [123][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3281 (0.2847) ([0.192]+[0.136])	Prec@1 91.406 (94.830)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3694 (0.3694) ([0.234]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 89.510
current lr 1.00000e-02
Grad=  tensor(4.9927, device='cuda:0')
Epoch: [124][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2792 (0.2792) ([0.144]+[0.136])	Prec@1 93.750 (93.750)
Epoch: [124][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3077 (0.2846) ([0.172]+[0.135])	Prec@1 93.750 (94.926)
Epoch: [124][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3688 (0.2888) ([0.234]+[0.135])	Prec@1 92.969 (94.694)
Epoch: [124][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2607 (0.2902) ([0.126]+[0.135])	Prec@1 95.312 (94.622)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3334 (0.3334) ([0.198]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(7.0508, device='cuda:0')
Epoch: [125][0/391]	Time 0.260 (0.260)	Data 0.132 (0.132)	Loss 0.3238 (0.3238) ([0.189]+[0.135])	Prec@1 91.406 (91.406)
Epoch: [125][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3667 (0.2723) ([0.232]+[0.135])	Prec@1 92.188 (95.142)
Epoch: [125][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3218 (0.2738) ([0.187]+[0.134])	Prec@1 92.188 (95.270)
Epoch: [125][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2769 (0.2807) ([0.143]+[0.134])	Prec@1 94.531 (95.001)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3557 (0.3557) ([0.221]+[0.134])	Prec@1 92.969 (92.969)
 * Prec@1 90.110
current lr 1.00000e-02
Grad=  tensor(5.6811, device='cuda:0')
Epoch: [126][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2787 (0.2787) ([0.144]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [126][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3191 (0.2786) ([0.185]+[0.134])	Prec@1 92.188 (95.034)
Epoch: [126][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3342 (0.2796) ([0.200]+[0.134])	Prec@1 91.406 (94.955)
Epoch: [126][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3290 (0.2851) ([0.195]+[0.134])	Prec@1 93.750 (94.729)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3152 (0.3152) ([0.181]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 90.400
current lr 1.00000e-02
Grad=  tensor(5.5757, device='cuda:0')
Epoch: [127][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2530 (0.2530) ([0.119]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [127][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2681 (0.2806) ([0.134]+[0.134])	Prec@1 94.531 (94.763)
Epoch: [127][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3252 (0.2834) ([0.191]+[0.134])	Prec@1 93.750 (94.679)
Epoch: [127][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1863 (0.2871) ([0.053]+[0.134])	Prec@1 100.000 (94.651)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3957 (0.3957) ([0.262]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 89.780
current lr 1.00000e-02
Grad=  tensor(7.7188, device='cuda:0')
Epoch: [128][0/391]	Time 0.272 (0.272)	Data 0.149 (0.149)	Loss 0.3802 (0.3802) ([0.247]+[0.134])	Prec@1 92.969 (92.969)
Epoch: [128][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.3208 (0.2834) ([0.187]+[0.134])	Prec@1 92.969 (94.895)
Epoch: [128][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2799 (0.2792) ([0.147]+[0.133])	Prec@1 93.750 (95.025)
Epoch: [128][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2695 (0.2827) ([0.136]+[0.133])	Prec@1 96.875 (94.900)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.2998 (0.2998) ([0.167]+[0.133])	Prec@1 92.969 (92.969)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(3.2860, device='cuda:0')
Epoch: [129][0/391]	Time 0.262 (0.262)	Data 0.139 (0.139)	Loss 0.2123 (0.2123) ([0.079]+[0.133])	Prec@1 96.094 (96.094)
Epoch: [129][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2523 (0.2733) ([0.119]+[0.133])	Prec@1 98.438 (95.196)
Epoch: [129][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3459 (0.2756) ([0.213]+[0.133])	Prec@1 92.188 (95.025)
Epoch: [129][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2248 (0.2776) ([0.092]+[0.133])	Prec@1 97.656 (94.949)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4009 (0.4009) ([0.268]+[0.133])	Prec@1 89.844 (89.844)
 * Prec@1 90.150
current lr 1.00000e-02
Grad=  tensor(2.9940, device='cuda:0')
Epoch: [130][0/391]	Time 0.266 (0.266)	Data 0.143 (0.143)	Loss 0.2186 (0.2186) ([0.086]+[0.133])	Prec@1 96.875 (96.875)
Epoch: [130][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2790 (0.2756) ([0.146]+[0.133])	Prec@1 96.094 (94.980)
Epoch: [130][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2784 (0.2833) ([0.146]+[0.133])	Prec@1 96.875 (94.764)
Epoch: [130][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2412 (0.2868) ([0.108]+[0.133])	Prec@1 93.750 (94.661)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.3677 (0.3677) ([0.235]+[0.133])	Prec@1 93.750 (93.750)
 * Prec@1 90.150
current lr 1.00000e-02
Grad=  tensor(7.6523, device='cuda:0')
Epoch: [131][0/391]	Time 0.266 (0.266)	Data 0.143 (0.143)	Loss 0.2954 (0.2954) ([0.163]+[0.133])	Prec@1 94.531 (94.531)
Epoch: [131][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.3076 (0.2723) ([0.175]+[0.132])	Prec@1 93.750 (95.374)
Epoch: [131][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2542 (0.2777) ([0.122]+[0.132])	Prec@1 95.312 (95.215)
Epoch: [131][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2928 (0.2824) ([0.160]+[0.132])	Prec@1 95.312 (95.006)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.4038 (0.4038) ([0.271]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 90.640
current lr 1.00000e-02
Grad=  tensor(4.4655, device='cuda:0')
Epoch: [132][0/391]	Time 0.266 (0.266)	Data 0.142 (0.142)	Loss 0.2271 (0.2271) ([0.095]+[0.132])	Prec@1 97.656 (97.656)
Epoch: [132][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3192 (0.2691) ([0.187]+[0.132])	Prec@1 92.188 (95.289)
Epoch: [132][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2742 (0.2775) ([0.142]+[0.132])	Prec@1 95.312 (95.044)
Epoch: [132][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2406 (0.2799) ([0.108]+[0.132])	Prec@1 96.094 (94.960)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3554 (0.3554) ([0.223]+[0.132])	Prec@1 92.969 (92.969)
 * Prec@1 89.840
current lr 1.00000e-02
Grad=  tensor(6.1295, device='cuda:0')
Epoch: [133][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.3053 (0.3053) ([0.173]+[0.132])	Prec@1 92.969 (92.969)
Epoch: [133][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.3668 (0.2698) ([0.235]+[0.132])	Prec@1 92.188 (95.367)
Epoch: [133][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3593 (0.2784) ([0.227]+[0.132])	Prec@1 91.406 (94.963)
Epoch: [133][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3003 (0.2827) ([0.168]+[0.132])	Prec@1 92.969 (94.804)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.3832 (0.3832) ([0.251]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 90.040
current lr 1.00000e-02
Grad=  tensor(8.4709, device='cuda:0')
Epoch: [134][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.3155 (0.3155) ([0.183]+[0.132])	Prec@1 94.531 (94.531)
Epoch: [134][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2745 (0.2708) ([0.142]+[0.132])	Prec@1 97.656 (95.274)
Epoch: [134][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2982 (0.2770) ([0.166]+[0.132])	Prec@1 95.312 (94.982)
Epoch: [134][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2072 (0.2834) ([0.075]+[0.132])	Prec@1 97.656 (94.747)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.4078 (0.4078) ([0.276]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 90.580
current lr 1.00000e-02
Grad=  tensor(8.1418, device='cuda:0')
Epoch: [135][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2656 (0.2656) ([0.133]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [135][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3465 (0.2797) ([0.214]+[0.132])	Prec@1 94.531 (94.941)
Epoch: [135][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2680 (0.2779) ([0.136]+[0.132])	Prec@1 94.531 (94.943)
Epoch: [135][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2390 (0.2791) ([0.107]+[0.132])	Prec@1 96.875 (94.889)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4059 (0.4059) ([0.274]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 89.680
current lr 1.00000e-02
Grad=  tensor(4.7368, device='cuda:0')
Epoch: [136][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2092 (0.2092) ([0.077]+[0.132])	Prec@1 97.656 (97.656)
Epoch: [136][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3800 (0.2738) ([0.248]+[0.132])	Prec@1 90.625 (95.026)
Epoch: [136][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2333 (0.2780) ([0.101]+[0.132])	Prec@1 96.094 (94.959)
Epoch: [136][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3291 (0.2810) ([0.197]+[0.132])	Prec@1 93.750 (94.835)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.5284 (0.5284) ([0.396]+[0.132])	Prec@1 86.719 (86.719)
 * Prec@1 87.130
current lr 1.00000e-02
Grad=  tensor(7.3516, device='cuda:0')
Epoch: [137][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2633 (0.2633) ([0.131]+[0.132])	Prec@1 93.750 (93.750)
Epoch: [137][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2650 (0.2870) ([0.133]+[0.132])	Prec@1 92.969 (94.632)
Epoch: [137][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2244 (0.2852) ([0.092]+[0.132])	Prec@1 95.312 (94.757)
Epoch: [137][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.3279 (0.2857) ([0.196]+[0.132])	Prec@1 92.188 (94.731)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4153 (0.4153) ([0.283]+[0.132])	Prec@1 93.750 (93.750)
 * Prec@1 90.510
current lr 1.00000e-02
Grad=  tensor(6.6297, device='cuda:0')
Epoch: [138][0/391]	Time 0.263 (0.263)	Data 0.140 (0.140)	Loss 0.2463 (0.2463) ([0.114]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [138][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2949 (0.2756) ([0.163]+[0.132])	Prec@1 94.531 (94.980)
Epoch: [138][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2703 (0.2772) ([0.138]+[0.132])	Prec@1 93.750 (95.005)
Epoch: [138][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2394 (0.2786) ([0.107]+[0.132])	Prec@1 97.656 (94.913)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4672 (0.4672) ([0.335]+[0.132])	Prec@1 88.281 (88.281)
 * Prec@1 89.350
current lr 1.00000e-02
Grad=  tensor(6.5719, device='cuda:0')
Epoch: [139][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2552 (0.2552) ([0.123]+[0.132])	Prec@1 96.094 (96.094)
Epoch: [139][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2485 (0.2755) ([0.116]+[0.132])	Prec@1 95.312 (94.964)
Epoch: [139][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2420 (0.2787) ([0.110]+[0.132])	Prec@1 96.094 (94.893)
Epoch: [139][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3153 (0.2808) ([0.183]+[0.132])	Prec@1 94.531 (94.866)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3639 (0.3639) ([0.232]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(9.2329, device='cuda:0')
Epoch: [140][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2845 (0.2845) ([0.152]+[0.132])	Prec@1 92.969 (92.969)
Epoch: [140][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2186 (0.2750) ([0.086]+[0.132])	Prec@1 96.875 (94.957)
Epoch: [140][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2159 (0.2773) ([0.084]+[0.132])	Prec@1 97.656 (94.897)
Epoch: [140][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2228 (0.2762) ([0.091]+[0.132])	Prec@1 98.438 (94.972)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4011 (0.4011) ([0.269]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 88.980
current lr 1.00000e-02
Grad=  tensor(9.2977, device='cuda:0')
Epoch: [141][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2731 (0.2731) ([0.141]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [141][100/391]	Time 0.115 (0.114)	Data 0.000 (0.001)	Loss 0.2162 (0.2767) ([0.084]+[0.132])	Prec@1 96.094 (95.042)
Epoch: [141][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2331 (0.2765) ([0.101]+[0.132])	Prec@1 96.875 (95.103)
Epoch: [141][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2888 (0.2802) ([0.157]+[0.132])	Prec@1 94.531 (94.975)
Test: [0/79]	Time 0.175 (0.175)	Loss 0.4420 (0.4420) ([0.310]+[0.132])	Prec@1 87.500 (87.500)
 * Prec@1 89.200
current lr 1.00000e-02
Grad=  tensor(4.6103, device='cuda:0')
Epoch: [142][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2474 (0.2474) ([0.115]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [142][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2715 (0.2760) ([0.139]+[0.132])	Prec@1 94.531 (95.119)
Epoch: [142][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2326 (0.2767) ([0.100]+[0.132])	Prec@1 97.656 (94.939)
Epoch: [142][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2601 (0.2814) ([0.128]+[0.132])	Prec@1 94.531 (94.726)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4075 (0.4075) ([0.275]+[0.132])	Prec@1 90.625 (90.625)
 * Prec@1 89.880
current lr 1.00000e-02
Grad=  tensor(5.1454, device='cuda:0')
Epoch: [143][0/391]	Time 0.252 (0.252)	Data 0.129 (0.129)	Loss 0.2210 (0.2210) ([0.089]+[0.132])	Prec@1 97.656 (97.656)
Epoch: [143][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2497 (0.2644) ([0.118]+[0.132])	Prec@1 95.312 (95.506)
Epoch: [143][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2766 (0.2656) ([0.145]+[0.132])	Prec@1 92.969 (95.538)
Epoch: [143][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2785 (0.2729) ([0.146]+[0.132])	Prec@1 95.312 (95.214)
Test: [0/79]	Time 0.189 (0.189)	Loss 0.6501 (0.6501) ([0.518]+[0.132])	Prec@1 82.812 (82.812)
 * Prec@1 87.770
current lr 1.00000e-02
Grad=  tensor(6.6386, device='cuda:0')
Epoch: [144][0/391]	Time 0.268 (0.268)	Data 0.144 (0.144)	Loss 0.2448 (0.2448) ([0.113]+[0.132])	Prec@1 96.875 (96.875)
Epoch: [144][100/391]	Time 0.112 (0.113)	Data 0.000 (0.002)	Loss 0.2799 (0.2698) ([0.148]+[0.132])	Prec@1 94.531 (95.057)
Epoch: [144][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3528 (0.2755) ([0.221]+[0.132])	Prec@1 93.750 (94.932)
Epoch: [144][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3544 (0.2799) ([0.222]+[0.132])	Prec@1 91.406 (94.783)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4074 (0.4074) ([0.275]+[0.132])	Prec@1 92.188 (92.188)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(6.4463, device='cuda:0')
Epoch: [145][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2424 (0.2424) ([0.110]+[0.132])	Prec@1 95.312 (95.312)
Epoch: [145][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2944 (0.2786) ([0.162]+[0.132])	Prec@1 94.531 (95.026)
Epoch: [145][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2628 (0.2743) ([0.131]+[0.132])	Prec@1 93.750 (95.103)
Epoch: [145][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.3526 (0.2811) ([0.220]+[0.132])	Prec@1 92.188 (94.760)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4369 (0.4369) ([0.304]+[0.132])	Prec@1 88.281 (88.281)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(8.5094, device='cuda:0')
Epoch: [146][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2902 (0.2902) ([0.158]+[0.132])	Prec@1 93.750 (93.750)
Epoch: [146][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2430 (0.2767) ([0.110]+[0.132])	Prec@1 95.312 (95.050)
Epoch: [146][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2856 (0.2786) ([0.153]+[0.132])	Prec@1 94.531 (94.908)
Epoch: [146][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2399 (0.2790) ([0.107]+[0.132])	Prec@1 97.656 (94.918)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3590 (0.3590) ([0.226]+[0.133])	Prec@1 93.750 (93.750)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(4.3503, device='cuda:0')
Epoch: [147][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2226 (0.2226) ([0.090]+[0.133])	Prec@1 97.656 (97.656)
Epoch: [147][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2069 (0.2714) ([0.074]+[0.132])	Prec@1 98.438 (95.119)
Epoch: [147][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2871 (0.2777) ([0.155]+[0.133])	Prec@1 94.531 (94.978)
Epoch: [147][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2506 (0.2793) ([0.118]+[0.133])	Prec@1 96.875 (94.991)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3775 (0.3775) ([0.245]+[0.133])	Prec@1 91.406 (91.406)
 * Prec@1 89.750
current lr 1.00000e-02
Grad=  tensor(9.8041, device='cuda:0')
Epoch: [148][0/391]	Time 0.252 (0.252)	Data 0.129 (0.129)	Loss 0.3093 (0.3093) ([0.177]+[0.133])	Prec@1 94.531 (94.531)
Epoch: [148][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3809 (0.2671) ([0.248]+[0.133])	Prec@1 91.406 (95.560)
Epoch: [148][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2238 (0.2707) ([0.091]+[0.132])	Prec@1 96.094 (95.347)
Epoch: [148][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2618 (0.2760) ([0.129]+[0.133])	Prec@1 96.875 (95.066)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4101 (0.4101) ([0.277]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 88.460
current lr 1.00000e-02
Grad=  tensor(8.2782, device='cuda:0')
Epoch: [149][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.2665 (0.2665) ([0.134]+[0.133])	Prec@1 94.531 (94.531)
Epoch: [149][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2596 (0.2695) ([0.127]+[0.133])	Prec@1 95.312 (95.289)
Epoch: [149][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3176 (0.2742) ([0.185]+[0.133])	Prec@1 93.750 (95.157)
Epoch: [149][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2999 (0.2749) ([0.167]+[0.133])	Prec@1 92.969 (95.110)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.3697 (0.3697) ([0.237]+[0.133])	Prec@1 92.188 (92.188)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(12.0066, device='cuda:0')
Epoch: [150][0/391]	Time 0.252 (0.252)	Data 0.130 (0.130)	Loss 0.3113 (0.3113) ([0.178]+[0.133])	Prec@1 93.750 (93.750)
Epoch: [150][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2048 (0.2708) ([0.072]+[0.133])	Prec@1 98.438 (95.398)
Epoch: [150][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2952 (0.2720) ([0.162]+[0.133])	Prec@1 93.750 (95.297)
Epoch: [150][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2392 (0.2758) ([0.106]+[0.133])	Prec@1 96.094 (95.196)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3148 (0.3148) ([0.182]+[0.133])	Prec@1 92.969 (92.969)
 * Prec@1 89.750
current lr 1.00000e-02
Grad=  tensor(10.3109, device='cuda:0')
Epoch: [151][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.3364 (0.3364) ([0.203]+[0.133])	Prec@1 92.969 (92.969)
Epoch: [151][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3279 (0.2768) ([0.195]+[0.133])	Prec@1 93.750 (94.964)
Epoch: [151][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2260 (0.2752) ([0.093]+[0.133])	Prec@1 97.656 (94.998)
Epoch: [151][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2711 (0.2779) ([0.138]+[0.133])	Prec@1 96.875 (94.921)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3882 (0.3882) ([0.255]+[0.133])	Prec@1 91.406 (91.406)
 * Prec@1 88.080
current lr 1.00000e-02
Grad=  tensor(4.3019, device='cuda:0')
Epoch: [152][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2295 (0.2295) ([0.097]+[0.133])	Prec@1 96.094 (96.094)
Epoch: [152][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2075 (0.2630) ([0.075]+[0.133])	Prec@1 99.219 (95.746)
Epoch: [152][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3166 (0.2681) ([0.184]+[0.133])	Prec@1 94.531 (95.445)
Epoch: [152][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3664 (0.2700) ([0.234]+[0.133])	Prec@1 92.969 (95.351)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4695 (0.4695) ([0.337]+[0.133])	Prec@1 88.281 (88.281)
 * Prec@1 90.190
current lr 1.00000e-02
Grad=  tensor(13.3769, device='cuda:0')
Epoch: [153][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.3167 (0.3167) ([0.184]+[0.133])	Prec@1 92.969 (92.969)
Epoch: [153][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2190 (0.2764) ([0.086]+[0.133])	Prec@1 96.094 (94.995)
Epoch: [153][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2876 (0.2744) ([0.154]+[0.133])	Prec@1 96.094 (95.165)
Epoch: [153][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2800 (0.2770) ([0.147]+[0.133])	Prec@1 92.969 (95.027)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4352 (0.4352) ([0.302]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(5.6277, device='cuda:0')
Epoch: [154][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2699 (0.2699) ([0.136]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [154][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2402 (0.2705) ([0.107]+[0.133])	Prec@1 97.656 (95.258)
Epoch: [154][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2360 (0.2741) ([0.102]+[0.133])	Prec@1 96.094 (95.025)
Epoch: [154][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2621 (0.2788) ([0.129]+[0.134])	Prec@1 95.312 (94.934)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.6161 (0.6161) ([0.482]+[0.134])	Prec@1 89.062 (89.062)
 * Prec@1 87.050
current lr 1.00000e-02
Grad=  tensor(10.0228, device='cuda:0')
Epoch: [155][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.3524 (0.3524) ([0.219]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [155][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2127 (0.2678) ([0.079]+[0.134])	Prec@1 96.094 (95.359)
Epoch: [155][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2138 (0.2723) ([0.080]+[0.134])	Prec@1 97.656 (95.274)
Epoch: [155][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3488 (0.2755) ([0.215]+[0.134])	Prec@1 89.844 (95.084)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4530 (0.4530) ([0.319]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 89.590
current lr 1.00000e-02
Grad=  tensor(2.0160, device='cuda:0')
Epoch: [156][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.1997 (0.1997) ([0.066]+[0.134])	Prec@1 98.438 (98.438)
Epoch: [156][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2935 (0.2713) ([0.160]+[0.134])	Prec@1 96.094 (95.382)
Epoch: [156][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2405 (0.2721) ([0.107]+[0.134])	Prec@1 95.312 (95.367)
Epoch: [156][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2816 (0.2717) ([0.148]+[0.134])	Prec@1 92.188 (95.346)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.5283 (0.5283) ([0.395]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 89.520
current lr 1.00000e-02
Grad=  tensor(6.9343, device='cuda:0')
Epoch: [157][0/391]	Time 0.268 (0.268)	Data 0.145 (0.145)	Loss 0.2590 (0.2590) ([0.125]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [157][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2868 (0.2727) ([0.153]+[0.134])	Prec@1 96.094 (95.181)
Epoch: [157][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2876 (0.2742) ([0.154]+[0.134])	Prec@1 94.531 (95.153)
Epoch: [157][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2920 (0.2775) ([0.158]+[0.134])	Prec@1 94.531 (95.050)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4241 (0.4241) ([0.290]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 90.680
current lr 1.00000e-02
Grad=  tensor(13.6289, device='cuda:0')
Epoch: [158][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.2759 (0.2759) ([0.142]+[0.134])	Prec@1 93.750 (93.750)
Epoch: [158][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2264 (0.2737) ([0.093]+[0.134])	Prec@1 97.656 (95.212)
Epoch: [158][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3061 (0.2771) ([0.172]+[0.134])	Prec@1 92.969 (95.037)
Epoch: [158][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2733 (0.2783) ([0.140]+[0.134])	Prec@1 92.188 (95.094)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.3612 (0.3612) ([0.227]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 89.420
current lr 1.00000e-02
Grad=  tensor(10.4967, device='cuda:0')
Epoch: [159][0/391]	Time 0.268 (0.268)	Data 0.144 (0.144)	Loss 0.2812 (0.2812) ([0.147]+[0.134])	Prec@1 95.312 (95.312)
Epoch: [159][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2888 (0.2625) ([0.155]+[0.134])	Prec@1 93.750 (95.490)
Epoch: [159][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2883 (0.2725) ([0.154]+[0.134])	Prec@1 94.531 (95.188)
Epoch: [159][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2352 (0.2795) ([0.101]+[0.134])	Prec@1 96.094 (94.884)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.4022 (0.4022) ([0.268]+[0.134])	Prec@1 91.406 (91.406)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(12.0718, device='cuda:0')
Epoch: [160][0/391]	Time 0.266 (0.266)	Data 0.143 (0.143)	Loss 0.3403 (0.3403) ([0.206]+[0.134])	Prec@1 91.406 (91.406)
Epoch: [160][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2114 (0.2568) ([0.077]+[0.134])	Prec@1 96.875 (95.955)
Epoch: [160][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2354 (0.2613) ([0.101]+[0.134])	Prec@1 98.438 (95.701)
Epoch: [160][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2355 (0.2675) ([0.102]+[0.134])	Prec@1 96.094 (95.486)
Test: [0/79]	Time 0.179 (0.179)	Loss 0.4236 (0.4236) ([0.290]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 88.300
current lr 1.00000e-02
Grad=  tensor(11.2223, device='cuda:0')
Epoch: [161][0/391]	Time 0.267 (0.267)	Data 0.143 (0.143)	Loss 0.3408 (0.3408) ([0.207]+[0.134])	Prec@1 91.406 (91.406)
Epoch: [161][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2621 (0.2638) ([0.128]+[0.134])	Prec@1 96.094 (95.645)
Epoch: [161][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2796 (0.2675) ([0.146]+[0.134])	Prec@1 96.875 (95.371)
Epoch: [161][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2166 (0.2711) ([0.083]+[0.134])	Prec@1 98.438 (95.198)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3107 (0.3107) ([0.177]+[0.134])	Prec@1 96.094 (96.094)
 * Prec@1 89.850
current lr 1.00000e-02
Grad=  tensor(9.4830, device='cuda:0')
Epoch: [162][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2736 (0.2736) ([0.140]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [162][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3058 (0.2729) ([0.172]+[0.134])	Prec@1 92.969 (95.320)
Epoch: [162][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2806 (0.2801) ([0.146]+[0.134])	Prec@1 96.875 (94.990)
Epoch: [162][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3102 (0.2780) ([0.176]+[0.134])	Prec@1 92.188 (95.032)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.4282 (0.4282) ([0.294]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 90.470
current lr 1.00000e-02
Grad=  tensor(8.6361, device='cuda:0')
Epoch: [163][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.2829 (0.2829) ([0.149]+[0.134])	Prec@1 94.531 (94.531)
Epoch: [163][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2391 (0.2544) ([0.105]+[0.134])	Prec@1 95.312 (95.877)
Epoch: [163][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2428 (0.2560) ([0.109]+[0.134])	Prec@1 97.656 (95.767)
Epoch: [163][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2458 (0.2622) ([0.112]+[0.134])	Prec@1 95.312 (95.580)
Test: [0/79]	Time 0.173 (0.173)	Loss 0.4847 (0.4847) ([0.350]+[0.134])	Prec@1 92.188 (92.188)
 * Prec@1 89.620
current lr 1.00000e-02
Grad=  tensor(9.0942, device='cuda:0')
Epoch: [164][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2649 (0.2649) ([0.131]+[0.134])	Prec@1 96.094 (96.094)
Epoch: [164][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2858 (0.2672) ([0.152]+[0.134])	Prec@1 96.094 (95.429)
Epoch: [164][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2537 (0.2633) ([0.120]+[0.134])	Prec@1 97.656 (95.565)
Epoch: [164][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2610 (0.2673) ([0.127]+[0.134])	Prec@1 95.312 (95.424)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4994 (0.4994) ([0.365]+[0.134])	Prec@1 89.062 (89.062)
 * Prec@1 89.370
current lr 1.00000e-02
Grad=  tensor(6.3845, device='cuda:0')
Epoch: [165][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2664 (0.2664) ([0.132]+[0.134])	Prec@1 96.875 (96.875)
Epoch: [165][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2603 (0.2580) ([0.126]+[0.134])	Prec@1 95.312 (95.761)
Epoch: [165][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2437 (0.2603) ([0.110]+[0.134])	Prec@1 94.531 (95.581)
Epoch: [165][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2072 (0.2601) ([0.073]+[0.134])	Prec@1 97.656 (95.676)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3750 (0.3750) ([0.241]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(6.0780, device='cuda:0')
Epoch: [166][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2323 (0.2323) ([0.098]+[0.134])	Prec@1 98.438 (98.438)
Epoch: [166][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3538 (0.2539) ([0.220]+[0.134])	Prec@1 95.312 (95.985)
Epoch: [166][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2413 (0.2626) ([0.107]+[0.134])	Prec@1 97.656 (95.666)
Epoch: [166][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3060 (0.2666) ([0.172]+[0.134])	Prec@1 93.750 (95.559)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4652 (0.4652) ([0.331]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 88.950
current lr 1.00000e-02
Grad=  tensor(5.3338, device='cuda:0')
Epoch: [167][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2288 (0.2288) ([0.095]+[0.134])	Prec@1 98.438 (98.438)
Epoch: [167][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2761 (0.2583) ([0.142]+[0.134])	Prec@1 92.969 (95.823)
Epoch: [167][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2538 (0.2657) ([0.120]+[0.134])	Prec@1 96.875 (95.557)
Epoch: [167][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2805 (0.2706) ([0.146]+[0.134])	Prec@1 93.750 (95.349)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3771 (0.3771) ([0.243]+[0.134])	Prec@1 89.844 (89.844)
 * Prec@1 90.240
current lr 1.00000e-02
Grad=  tensor(5.1015, device='cuda:0')
Epoch: [168][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2245 (0.2245) ([0.090]+[0.134])	Prec@1 97.656 (97.656)
Epoch: [168][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2306 (0.2584) ([0.096]+[0.134])	Prec@1 96.875 (95.893)
Epoch: [168][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2404 (0.2609) ([0.106]+[0.134])	Prec@1 96.094 (95.651)
Epoch: [168][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3158 (0.2648) ([0.182]+[0.134])	Prec@1 93.750 (95.489)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4011 (0.4011) ([0.267]+[0.134])	Prec@1 90.625 (90.625)
 * Prec@1 90.620
current lr 1.00000e-02
Grad=  tensor(8.1272, device='cuda:0')
Epoch: [169][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2869 (0.2869) ([0.153]+[0.134])	Prec@1 93.750 (93.750)
Epoch: [169][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2494 (0.2607) ([0.115]+[0.134])	Prec@1 97.656 (95.444)
Epoch: [169][200/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2815 (0.2608) ([0.147]+[0.134])	Prec@1 96.875 (95.639)
Epoch: [169][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2738 (0.2662) ([0.139]+[0.134])	Prec@1 95.312 (95.455)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4021 (0.4021) ([0.268]+[0.135])	Prec@1 89.062 (89.062)
 * Prec@1 89.340
current lr 1.00000e-02
Grad=  tensor(7.6118, device='cuda:0')
Epoch: [170][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.2811 (0.2811) ([0.147]+[0.135])	Prec@1 95.312 (95.312)
Epoch: [170][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2238 (0.2584) ([0.089]+[0.134])	Prec@1 98.438 (95.692)
Epoch: [170][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2719 (0.2635) ([0.137]+[0.134])	Prec@1 95.312 (95.530)
Epoch: [170][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3281 (0.2633) ([0.194]+[0.134])	Prec@1 93.750 (95.551)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4533 (0.4533) ([0.319]+[0.135])	Prec@1 87.500 (87.500)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(8.1943, device='cuda:0')
Epoch: [171][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.2478 (0.2478) ([0.113]+[0.135])	Prec@1 96.094 (96.094)
Epoch: [171][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3156 (0.2707) ([0.181]+[0.135])	Prec@1 94.531 (95.305)
Epoch: [171][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2857 (0.2718) ([0.151]+[0.135])	Prec@1 93.750 (95.215)
Epoch: [171][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3336 (0.2726) ([0.199]+[0.135])	Prec@1 91.406 (95.258)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4191 (0.4191) ([0.284]+[0.135])	Prec@1 90.625 (90.625)
 * Prec@1 90.610
current lr 1.00000e-02
Grad=  tensor(3.3718, device='cuda:0')
Epoch: [172][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.2035 (0.2035) ([0.069]+[0.135])	Prec@1 97.656 (97.656)
Epoch: [172][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2152 (0.2579) ([0.081]+[0.135])	Prec@1 97.656 (95.738)
Epoch: [172][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2949 (0.2605) ([0.160]+[0.134])	Prec@1 93.750 (95.717)
Epoch: [172][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2859 (0.2656) ([0.151]+[0.135])	Prec@1 93.750 (95.502)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.5451 (0.5451) ([0.410]+[0.135])	Prec@1 87.500 (87.500)
 * Prec@1 88.590
current lr 1.00000e-02
Grad=  tensor(3.5073, device='cuda:0')
Epoch: [173][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.2150 (0.2150) ([0.080]+[0.135])	Prec@1 96.094 (96.094)
Epoch: [173][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3070 (0.2691) ([0.172]+[0.135])	Prec@1 92.969 (95.374)
Epoch: [173][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2699 (0.2642) ([0.135]+[0.135])	Prec@1 94.531 (95.596)
Epoch: [173][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2341 (0.2650) ([0.099]+[0.135])	Prec@1 96.094 (95.479)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4160 (0.4160) ([0.281]+[0.135])	Prec@1 92.969 (92.969)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(6.9452, device='cuda:0')
Epoch: [174][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2430 (0.2430) ([0.108]+[0.135])	Prec@1 96.094 (96.094)
Epoch: [174][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2364 (0.2607) ([0.101]+[0.135])	Prec@1 96.875 (95.661)
Epoch: [174][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3368 (0.2669) ([0.202]+[0.135])	Prec@1 94.531 (95.480)
Epoch: [174][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2483 (0.2675) ([0.113]+[0.135])	Prec@1 96.094 (95.463)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.3846 (0.3846) ([0.250]+[0.135])	Prec@1 92.969 (92.969)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(10.1849, device='cuda:0')
Epoch: [175][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.2332 (0.2332) ([0.098]+[0.135])	Prec@1 96.094 (96.094)
Epoch: [175][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2789 (0.2458) ([0.144]+[0.135])	Prec@1 96.094 (96.218)
Epoch: [175][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3493 (0.2507) ([0.215]+[0.135])	Prec@1 89.844 (96.063)
Epoch: [175][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2850 (0.2560) ([0.150]+[0.135])	Prec@1 95.312 (95.889)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3639 (0.3639) ([0.229]+[0.135])	Prec@1 92.188 (92.188)
 * Prec@1 89.450
current lr 1.00000e-02
Grad=  tensor(13.3746, device='cuda:0')
Epoch: [176][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.3171 (0.3171) ([0.182]+[0.135])	Prec@1 94.531 (94.531)
Epoch: [176][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2202 (0.2683) ([0.085]+[0.135])	Prec@1 96.875 (95.475)
Epoch: [176][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3272 (0.2693) ([0.192]+[0.135])	Prec@1 92.188 (95.433)
Epoch: [176][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2792 (0.2719) ([0.144]+[0.135])	Prec@1 96.094 (95.315)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5197 (0.5197) ([0.385]+[0.135])	Prec@1 89.062 (89.062)
 * Prec@1 86.970
current lr 1.00000e-02
Grad=  tensor(7.1246, device='cuda:0')
Epoch: [177][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.2542 (0.2542) ([0.119]+[0.135])	Prec@1 95.312 (95.312)
Epoch: [177][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2270 (0.2651) ([0.092]+[0.135])	Prec@1 96.875 (95.676)
Epoch: [177][200/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.2540 (0.2635) ([0.119]+[0.135])	Prec@1 96.875 (95.740)
Epoch: [177][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2812 (0.2639) ([0.146]+[0.135])	Prec@1 96.094 (95.616)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4249 (0.4249) ([0.290]+[0.135])	Prec@1 89.062 (89.062)
 * Prec@1 89.520
current lr 1.00000e-02
Grad=  tensor(2.8071, device='cuda:0')
Epoch: [178][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.1958 (0.1958) ([0.061]+[0.135])	Prec@1 98.438 (98.438)
Epoch: [178][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2333 (0.2587) ([0.098]+[0.135])	Prec@1 97.656 (95.846)
Epoch: [178][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2917 (0.2650) ([0.157]+[0.135])	Prec@1 92.969 (95.538)
Epoch: [178][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2802 (0.2684) ([0.145]+[0.135])	Prec@1 96.094 (95.416)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4075 (0.4075) ([0.272]+[0.135])	Prec@1 91.406 (91.406)
 * Prec@1 88.610
current lr 1.00000e-02
Grad=  tensor(5.7893, device='cuda:0')
Epoch: [179][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2089 (0.2089) ([0.074]+[0.135])	Prec@1 96.875 (96.875)
Epoch: [179][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2016 (0.2619) ([0.066]+[0.135])	Prec@1 97.656 (95.715)
Epoch: [179][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3797 (0.2616) ([0.245]+[0.135])	Prec@1 92.969 (95.616)
Epoch: [179][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2665 (0.2650) ([0.131]+[0.135])	Prec@1 95.312 (95.476)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4463 (0.4463) ([0.311]+[0.135])	Prec@1 89.844 (89.844)
 * Prec@1 89.640
current lr 1.00000e-02
Grad=  tensor(7.0575, device='cuda:0')
Epoch: [180][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2581 (0.2581) ([0.123]+[0.135])	Prec@1 94.531 (94.531)
Epoch: [180][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3129 (0.2549) ([0.178]+[0.135])	Prec@1 92.969 (95.978)
Epoch: [180][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3356 (0.2591) ([0.200]+[0.135])	Prec@1 92.188 (95.876)
Epoch: [180][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2098 (0.2603) ([0.075]+[0.135])	Prec@1 96.875 (95.787)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3679 (0.3679) ([0.233]+[0.135])	Prec@1 93.750 (93.750)
 * Prec@1 89.610
current lr 1.00000e-02
Grad=  tensor(9.0324, device='cuda:0')
Epoch: [181][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2574 (0.2574) ([0.122]+[0.135])	Prec@1 93.750 (93.750)
Epoch: [181][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2782 (0.2573) ([0.143]+[0.135])	Prec@1 95.312 (96.094)
Epoch: [181][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2769 (0.2609) ([0.142]+[0.135])	Prec@1 96.875 (95.931)
Epoch: [181][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2465 (0.2642) ([0.111]+[0.135])	Prec@1 94.531 (95.746)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5201 (0.5201) ([0.385]+[0.135])	Prec@1 89.062 (89.062)
 * Prec@1 88.740
current lr 1.00000e-02
Grad=  tensor(11.6269, device='cuda:0')
Epoch: [182][0/391]	Time 0.265 (0.265)	Data 0.138 (0.138)	Loss 0.3098 (0.3098) ([0.174]+[0.135])	Prec@1 94.531 (94.531)
Epoch: [182][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3174 (0.2670) ([0.182]+[0.135])	Prec@1 96.094 (95.622)
Epoch: [182][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2473 (0.2648) ([0.112]+[0.135])	Prec@1 96.875 (95.620)
Epoch: [182][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2424 (0.2645) ([0.107]+[0.135])	Prec@1 97.656 (95.629)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4013 (0.4013) ([0.266]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 90.090
current lr 1.00000e-02
Grad=  tensor(10.9722, device='cuda:0')
Epoch: [183][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2770 (0.2770) ([0.141]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [183][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2512 (0.2667) ([0.116]+[0.136])	Prec@1 96.094 (95.521)
Epoch: [183][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2988 (0.2672) ([0.163]+[0.136])	Prec@1 95.312 (95.639)
Epoch: [183][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2316 (0.2699) ([0.096]+[0.136])	Prec@1 96.875 (95.507)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4376 (0.4376) ([0.302]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 88.610
current lr 1.00000e-02
Grad=  tensor(11.8528, device='cuda:0')
Epoch: [184][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.3424 (0.3424) ([0.207]+[0.136])	Prec@1 91.406 (91.406)
Epoch: [184][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2876 (0.2696) ([0.152]+[0.136])	Prec@1 94.531 (95.251)
Epoch: [184][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2161 (0.2630) ([0.081]+[0.136])	Prec@1 96.875 (95.620)
Epoch: [184][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2638 (0.2621) ([0.128]+[0.136])	Prec@1 96.094 (95.660)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5097 (0.5097) ([0.374]+[0.136])	Prec@1 89.062 (89.062)
 * Prec@1 89.060
current lr 1.00000e-02
Grad=  tensor(4.9270, device='cuda:0')
Epoch: [185][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2498 (0.2498) ([0.114]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [185][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2158 (0.2557) ([0.080]+[0.136])	Prec@1 97.656 (95.854)
Epoch: [185][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2107 (0.2593) ([0.075]+[0.136])	Prec@1 96.875 (95.725)
Epoch: [185][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2511 (0.2625) ([0.115]+[0.136])	Prec@1 96.875 (95.665)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.3428 (0.3428) ([0.207]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 90.070
current lr 1.00000e-02
Grad=  tensor(8.9022, device='cuda:0')
Epoch: [186][0/391]	Time 0.253 (0.253)	Data 0.132 (0.132)	Loss 0.2591 (0.2591) ([0.123]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [186][100/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2449 (0.2498) ([0.109]+[0.136])	Prec@1 96.875 (96.241)
Epoch: [186][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2197 (0.2498) ([0.084]+[0.136])	Prec@1 94.531 (96.210)
Epoch: [186][300/391]	Time 0.110 (0.110)	Data 0.000 (0.001)	Loss 0.2574 (0.2562) ([0.122]+[0.136])	Prec@1 96.875 (95.925)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4912 (0.4912) ([0.355]+[0.136])	Prec@1 89.062 (89.062)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(13.6064, device='cuda:0')
Epoch: [187][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.3733 (0.3733) ([0.238]+[0.136])	Prec@1 92.969 (92.969)
Epoch: [187][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2169 (0.2620) ([0.081]+[0.136])	Prec@1 96.875 (95.831)
Epoch: [187][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2624 (0.2611) ([0.127]+[0.136])	Prec@1 96.875 (95.868)
Epoch: [187][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.3058 (0.2635) ([0.170]+[0.136])	Prec@1 93.750 (95.769)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4895 (0.4895) ([0.354]+[0.136])	Prec@1 85.938 (85.938)
 * Prec@1 88.600
current lr 1.00000e-02
Grad=  tensor(13.8061, device='cuda:0')
Epoch: [188][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.3345 (0.3345) ([0.199]+[0.136])	Prec@1 92.969 (92.969)
Epoch: [188][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2544 (0.2562) ([0.119]+[0.136])	Prec@1 96.094 (95.970)
Epoch: [188][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2648 (0.2618) ([0.129]+[0.136])	Prec@1 95.312 (95.771)
Epoch: [188][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3374 (0.2706) ([0.201]+[0.136])	Prec@1 92.969 (95.432)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3603 (0.3603) ([0.224]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 90.590
current lr 1.00000e-02
Grad=  tensor(6.8047, device='cuda:0')
Epoch: [189][0/391]	Time 0.259 (0.259)	Data 0.136 (0.136)	Loss 0.2527 (0.2527) ([0.117]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [189][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3314 (0.2592) ([0.196]+[0.136])	Prec@1 92.969 (95.730)
Epoch: [189][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2263 (0.2600) ([0.090]+[0.136])	Prec@1 96.094 (95.787)
Epoch: [189][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2690 (0.2623) ([0.133]+[0.136])	Prec@1 94.531 (95.697)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.3797 (0.3797) ([0.244]+[0.136])	Prec@1 92.188 (92.188)
 * Prec@1 90.400
current lr 1.00000e-02
Grad=  tensor(4.6336, device='cuda:0')
Epoch: [190][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2374 (0.2374) ([0.101]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [190][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2629 (0.2553) ([0.127]+[0.136])	Prec@1 95.312 (95.823)
Epoch: [190][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1810 (0.2598) ([0.045]+[0.136])	Prec@1 99.219 (95.763)
Epoch: [190][300/391]	Time 0.112 (0.111)	Data 0.000 (0.001)	Loss 0.2323 (0.2582) ([0.096]+[0.136])	Prec@1 96.094 (95.785)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.3686 (0.3686) ([0.233]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 89.760
current lr 1.00000e-02
Grad=  tensor(5.5164, device='cuda:0')
Epoch: [191][0/391]	Time 0.262 (0.262)	Data 0.140 (0.140)	Loss 0.2322 (0.2322) ([0.096]+[0.136])	Prec@1 98.438 (98.438)
Epoch: [191][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1888 (0.2491) ([0.053]+[0.136])	Prec@1 98.438 (96.125)
Epoch: [191][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3540 (0.2599) ([0.218]+[0.136])	Prec@1 92.188 (95.775)
Epoch: [191][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2358 (0.2632) ([0.100]+[0.136])	Prec@1 96.875 (95.588)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4056 (0.4056) ([0.270]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 89.890
current lr 1.00000e-02
Grad=  tensor(5.0601, device='cuda:0')
Epoch: [192][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2263 (0.2263) ([0.090]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [192][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2550 (0.2585) ([0.119]+[0.136])	Prec@1 95.312 (95.753)
Epoch: [192][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2311 (0.2582) ([0.095]+[0.136])	Prec@1 95.312 (95.767)
Epoch: [192][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2923 (0.2610) ([0.156]+[0.136])	Prec@1 92.188 (95.665)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3461 (0.3461) ([0.210]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 89.630
current lr 1.00000e-02
Grad=  tensor(3.3491, device='cuda:0')
Epoch: [193][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.1971 (0.1971) ([0.061]+[0.136])	Prec@1 98.438 (98.438)
Epoch: [193][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2830 (0.2432) ([0.147]+[0.136])	Prec@1 93.750 (96.287)
Epoch: [193][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2319 (0.2533) ([0.096]+[0.136])	Prec@1 97.656 (95.962)
Epoch: [193][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2784 (0.2576) ([0.142]+[0.136])	Prec@1 93.750 (95.775)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4471 (0.4471) ([0.311]+[0.136])	Prec@1 89.062 (89.062)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(8.2586, device='cuda:0')
Epoch: [194][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.3015 (0.3015) ([0.165]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [194][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2628 (0.2571) ([0.127]+[0.136])	Prec@1 93.750 (95.869)
Epoch: [194][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2404 (0.2532) ([0.104]+[0.136])	Prec@1 97.656 (95.989)
Epoch: [194][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2701 (0.2574) ([0.134]+[0.136])	Prec@1 95.312 (95.868)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4268 (0.4268) ([0.291]+[0.136])	Prec@1 89.062 (89.062)
 * Prec@1 90.600
current lr 1.00000e-02
Grad=  tensor(6.2224, device='cuda:0')
Epoch: [195][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.2614 (0.2614) ([0.125]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [195][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2151 (0.2504) ([0.079]+[0.136])	Prec@1 97.656 (96.032)
Epoch: [195][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3781 (0.2528) ([0.242]+[0.136])	Prec@1 90.625 (95.950)
Epoch: [195][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2608 (0.2561) ([0.125]+[0.136])	Prec@1 96.094 (95.826)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4470 (0.4470) ([0.311]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 89.460
current lr 1.00000e-02
Grad=  tensor(5.4479, device='cuda:0')
Epoch: [196][0/391]	Time 0.266 (0.266)	Data 0.142 (0.142)	Loss 0.2490 (0.2490) ([0.113]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [196][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2514 (0.2502) ([0.115]+[0.136])	Prec@1 96.094 (96.132)
Epoch: [196][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2005 (0.2560) ([0.064]+[0.136])	Prec@1 97.656 (95.903)
Epoch: [196][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2838 (0.2599) ([0.148]+[0.136])	Prec@1 93.750 (95.754)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3453 (0.3453) ([0.209]+[0.136])	Prec@1 92.969 (92.969)
 * Prec@1 90.360
current lr 1.00000e-02
Grad=  tensor(9.2277, device='cuda:0')
Epoch: [197][0/391]	Time 0.264 (0.264)	Data 0.141 (0.141)	Loss 0.2990 (0.2990) ([0.163]+[0.136])	Prec@1 95.312 (95.312)
Epoch: [197][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2109 (0.2642) ([0.075]+[0.136])	Prec@1 97.656 (95.738)
Epoch: [197][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2142 (0.2564) ([0.078]+[0.136])	Prec@1 96.875 (95.985)
Epoch: [197][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1890 (0.2627) ([0.053]+[0.136])	Prec@1 97.656 (95.689)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.4121 (0.4121) ([0.276]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 90.090
current lr 1.00000e-02
Grad=  tensor(7.9041, device='cuda:0')
Epoch: [198][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2526 (0.2526) ([0.116]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [198][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2852 (0.2442) ([0.149]+[0.136])	Prec@1 94.531 (96.372)
Epoch: [198][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2405 (0.2490) ([0.104]+[0.136])	Prec@1 97.656 (96.210)
Epoch: [198][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2749 (0.2544) ([0.139]+[0.136])	Prec@1 95.312 (95.985)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3686 (0.3686) ([0.232]+[0.136])	Prec@1 92.188 (92.188)
 * Prec@1 90.150
current lr 1.00000e-02
Grad=  tensor(2.8575, device='cuda:0')
Epoch: [199][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1868 (0.1868) ([0.051]+[0.136])	Prec@1 98.438 (98.438)
Epoch: [199][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2244 (0.2456) ([0.088]+[0.136])	Prec@1 96.875 (96.341)
Epoch: [199][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2929 (0.2519) ([0.157]+[0.136])	Prec@1 92.969 (96.024)
Epoch: [199][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3533 (0.2569) ([0.217]+[0.136])	Prec@1 94.531 (95.824)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3962 (0.3962) ([0.260]+[0.136])	Prec@1 90.625 (90.625)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(9.2910, device='cuda:0')
Epoch: [200][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.3008 (0.3008) ([0.165]+[0.136])	Prec@1 95.312 (95.312)
Epoch: [200][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2129 (0.2422) ([0.077]+[0.136])	Prec@1 96.094 (96.349)
Epoch: [200][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.4375 (0.2481) ([0.302]+[0.136])	Prec@1 89.844 (96.039)
Epoch: [200][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2144 (0.2574) ([0.078]+[0.136])	Prec@1 97.656 (95.782)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3860 (0.3860) ([0.250]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 90.750
current lr 1.00000e-02
Grad=  tensor(8.8623, device='cuda:0')
Epoch: [201][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2569 (0.2569) ([0.121]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [201][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2499 (0.2572) ([0.114]+[0.136])	Prec@1 95.312 (95.931)
Epoch: [201][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2481 (0.2618) ([0.112]+[0.136])	Prec@1 96.875 (95.701)
Epoch: [201][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2426 (0.2662) ([0.106]+[0.136])	Prec@1 98.438 (95.507)
Test: [0/79]	Time 0.193 (0.193)	Loss 0.4635 (0.4635) ([0.327]+[0.136])	Prec@1 88.281 (88.281)
 * Prec@1 89.650
current lr 1.00000e-02
Grad=  tensor(8.2466, device='cuda:0')
Epoch: [202][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.2555 (0.2555) ([0.119]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [202][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2328 (0.2376) ([0.097]+[0.136])	Prec@1 98.438 (96.635)
Epoch: [202][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3184 (0.2470) ([0.182]+[0.136])	Prec@1 91.406 (96.222)
Epoch: [202][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2633 (0.2501) ([0.127]+[0.136])	Prec@1 92.969 (96.104)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4784 (0.4784) ([0.342]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 89.740
current lr 1.00000e-02
Grad=  tensor(5.9135, device='cuda:0')
Epoch: [203][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2355 (0.2355) ([0.099]+[0.136])	Prec@1 95.312 (95.312)
Epoch: [203][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2859 (0.2565) ([0.150]+[0.136])	Prec@1 94.531 (95.808)
Epoch: [203][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2904 (0.2578) ([0.154]+[0.136])	Prec@1 94.531 (95.802)
Epoch: [203][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2294 (0.2612) ([0.093]+[0.136])	Prec@1 98.438 (95.702)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.5663 (0.5663) ([0.430]+[0.136])	Prec@1 89.062 (89.062)
 * Prec@1 89.110
current lr 1.00000e-02
Grad=  tensor(7.3934, device='cuda:0')
Epoch: [204][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.2315 (0.2315) ([0.095]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [204][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3284 (0.2520) ([0.192]+[0.136])	Prec@1 92.969 (95.916)
Epoch: [204][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2825 (0.2539) ([0.146]+[0.136])	Prec@1 95.312 (95.915)
Epoch: [204][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2963 (0.2552) ([0.160]+[0.136])	Prec@1 94.531 (95.881)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.5753 (0.5753) ([0.439]+[0.136])	Prec@1 87.500 (87.500)
 * Prec@1 89.280
current lr 1.00000e-02
Grad=  tensor(10.7903, device='cuda:0')
Epoch: [205][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.2956 (0.2956) ([0.159]+[0.136])	Prec@1 94.531 (94.531)
Epoch: [205][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2413 (0.2447) ([0.105]+[0.136])	Prec@1 95.312 (96.357)
Epoch: [205][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2614 (0.2479) ([0.125]+[0.136])	Prec@1 96.875 (96.094)
Epoch: [205][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2099 (0.2564) ([0.074]+[0.136])	Prec@1 96.875 (95.845)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4325 (0.4325) ([0.296]+[0.136])	Prec@1 90.625 (90.625)
 * Prec@1 88.230
current lr 1.00000e-02
Grad=  tensor(6.1696, device='cuda:0')
Epoch: [206][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.2514 (0.2514) ([0.115]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [206][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2568 (0.2438) ([0.120]+[0.136])	Prec@1 96.094 (96.395)
Epoch: [206][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2422 (0.2464) ([0.106]+[0.136])	Prec@1 95.312 (96.284)
Epoch: [206][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2507 (0.2517) ([0.115]+[0.136])	Prec@1 96.094 (96.122)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.6482 (0.6482) ([0.512]+[0.136])	Prec@1 85.938 (85.938)
 * Prec@1 88.850
current lr 1.00000e-02
Grad=  tensor(3.9022, device='cuda:0')
Epoch: [207][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2127 (0.2127) ([0.076]+[0.136])	Prec@1 96.875 (96.875)
Epoch: [207][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2912 (0.2485) ([0.155]+[0.136])	Prec@1 93.750 (96.086)
Epoch: [207][200/391]	Time 0.114 (0.114)	Data 0.000 (0.001)	Loss 0.2827 (0.2517) ([0.147]+[0.136])	Prec@1 94.531 (96.032)
Epoch: [207][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2467 (0.2575) ([0.110]+[0.136])	Prec@1 96.875 (95.816)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4224 (0.4224) ([0.286]+[0.136])	Prec@1 92.188 (92.188)
 * Prec@1 89.740
current lr 1.00000e-02
Grad=  tensor(8.0879, device='cuda:0')
Epoch: [208][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2296 (0.2296) ([0.093]+[0.136])	Prec@1 96.094 (96.094)
Epoch: [208][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2493 (0.2460) ([0.113]+[0.136])	Prec@1 95.312 (96.303)
Epoch: [208][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3475 (0.2491) ([0.211]+[0.136])	Prec@1 93.750 (96.140)
Epoch: [208][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2590 (0.2536) ([0.123]+[0.136])	Prec@1 93.750 (96.018)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4299 (0.4299) ([0.294]+[0.136])	Prec@1 88.281 (88.281)
 * Prec@1 89.990
current lr 1.00000e-02
Grad=  tensor(9.4895, device='cuda:0')
Epoch: [209][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2925 (0.2925) ([0.156]+[0.136])	Prec@1 93.750 (93.750)
Epoch: [209][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.2469 (0.2481) ([0.111]+[0.136])	Prec@1 95.312 (96.132)
Epoch: [209][200/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2426 (0.2543) ([0.106]+[0.136])	Prec@1 94.531 (95.880)
Epoch: [209][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2713 (0.2581) ([0.135]+[0.136])	Prec@1 95.312 (95.754)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4903 (0.4903) ([0.354]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 89.000
current lr 1.00000e-02
Grad=  tensor(11.2962, device='cuda:0')
Epoch: [210][0/391]	Time 0.254 (0.254)	Data 0.135 (0.135)	Loss 0.2625 (0.2625) ([0.126]+[0.137])	Prec@1 94.531 (94.531)
Epoch: [210][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2748 (0.2566) ([0.138]+[0.137])	Prec@1 96.094 (95.908)
Epoch: [210][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2149 (0.2566) ([0.078]+[0.137])	Prec@1 98.438 (95.853)
Epoch: [210][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2226 (0.2553) ([0.086]+[0.137])	Prec@1 96.094 (95.938)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4683 (0.4683) ([0.332]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 88.970
current lr 1.00000e-02
Grad=  tensor(5.6804, device='cuda:0')
Epoch: [211][0/391]	Time 0.253 (0.253)	Data 0.132 (0.132)	Loss 0.2334 (0.2334) ([0.097]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [211][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3003 (0.2461) ([0.164]+[0.137])	Prec@1 92.969 (96.194)
Epoch: [211][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2892 (0.2448) ([0.153]+[0.136])	Prec@1 95.312 (96.335)
Epoch: [211][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2931 (0.2473) ([0.157]+[0.136])	Prec@1 95.312 (96.252)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4058 (0.4058) ([0.269]+[0.136])	Prec@1 91.406 (91.406)
 * Prec@1 89.020
current lr 1.00000e-02
Grad=  tensor(4.0920, device='cuda:0')
Epoch: [212][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.1998 (0.1998) ([0.063]+[0.136])	Prec@1 97.656 (97.656)
Epoch: [212][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2723 (0.2522) ([0.136]+[0.136])	Prec@1 96.875 (95.993)
Epoch: [212][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2187 (0.2526) ([0.082]+[0.136])	Prec@1 96.094 (96.020)
Epoch: [212][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2263 (0.2569) ([0.090]+[0.136])	Prec@1 97.656 (95.865)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.5177 (0.5177) ([0.381]+[0.136])	Prec@1 89.844 (89.844)
 * Prec@1 90.160
current lr 1.00000e-02
Grad=  tensor(9.3130, device='cuda:0')
Epoch: [213][0/391]	Time 0.254 (0.254)	Data 0.132 (0.132)	Loss 0.2815 (0.2815) ([0.145]+[0.136])	Prec@1 95.312 (95.312)
Epoch: [213][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2888 (0.2580) ([0.152]+[0.137])	Prec@1 96.094 (95.746)
Epoch: [213][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3360 (0.2636) ([0.199]+[0.137])	Prec@1 94.531 (95.581)
Epoch: [213][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3178 (0.2623) ([0.181]+[0.137])	Prec@1 92.969 (95.624)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.3559 (0.3559) ([0.219]+[0.137])	Prec@1 92.188 (92.188)
 * Prec@1 88.600
current lr 1.00000e-02
Grad=  tensor(8.0446, device='cuda:0')
Epoch: [214][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.2494 (0.2494) ([0.113]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [214][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1991 (0.2507) ([0.062]+[0.137])	Prec@1 97.656 (96.202)
Epoch: [214][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3084 (0.2516) ([0.172]+[0.136])	Prec@1 93.750 (96.191)
Epoch: [214][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2371 (0.2536) ([0.101]+[0.137])	Prec@1 97.656 (96.078)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4536 (0.4536) ([0.317]+[0.137])	Prec@1 88.281 (88.281)
 * Prec@1 88.710
current lr 1.00000e-02
Grad=  tensor(5.7025, device='cuda:0')
Epoch: [215][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.2071 (0.2071) ([0.071]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [215][100/391]	Time 0.110 (0.113)	Data 0.000 (0.001)	Loss 0.2754 (0.2645) ([0.139]+[0.137])	Prec@1 96.875 (95.506)
Epoch: [215][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3560 (0.2648) ([0.219]+[0.137])	Prec@1 92.969 (95.546)
Epoch: [215][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2632 (0.2672) ([0.126]+[0.137])	Prec@1 95.312 (95.476)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.4162 (0.4162) ([0.279]+[0.137])	Prec@1 92.969 (92.969)
 * Prec@1 89.080
current lr 1.00000e-02
Grad=  tensor(2.3124, device='cuda:0')
Epoch: [216][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.1846 (0.1846) ([0.048]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [216][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2546 (0.2543) ([0.118]+[0.137])	Prec@1 96.094 (96.024)
Epoch: [216][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2394 (0.2560) ([0.103]+[0.137])	Prec@1 96.094 (95.915)
Epoch: [216][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2347 (0.2569) ([0.098]+[0.137])	Prec@1 98.438 (95.912)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4620 (0.4620) ([0.325]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 89.790
current lr 1.00000e-02
Grad=  tensor(5.6644, device='cuda:0')
Epoch: [217][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2098 (0.2098) ([0.073]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [217][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2201 (0.2597) ([0.083]+[0.137])	Prec@1 96.875 (95.699)
Epoch: [217][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3542 (0.2567) ([0.217]+[0.137])	Prec@1 93.750 (95.931)
Epoch: [217][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2958 (0.2602) ([0.159]+[0.137])	Prec@1 92.188 (95.795)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.5952 (0.5952) ([0.458]+[0.137])	Prec@1 87.500 (87.500)
 * Prec@1 87.730
current lr 1.00000e-02
Grad=  tensor(4.5228, device='cuda:0')
Epoch: [218][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2180 (0.2180) ([0.081]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [218][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2076 (0.2439) ([0.071]+[0.137])	Prec@1 96.875 (96.395)
Epoch: [218][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2592 (0.2535) ([0.122]+[0.137])	Prec@1 96.094 (96.004)
Epoch: [218][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2198 (0.2554) ([0.083]+[0.137])	Prec@1 98.438 (95.990)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3702 (0.3702) ([0.233]+[0.137])	Prec@1 92.969 (92.969)
 * Prec@1 89.920
current lr 1.00000e-02
Grad=  tensor(8.9949, device='cuda:0')
Epoch: [219][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.2591 (0.2591) ([0.122]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [219][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2326 (0.2481) ([0.096]+[0.137])	Prec@1 97.656 (96.233)
Epoch: [219][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2474 (0.2498) ([0.110]+[0.137])	Prec@1 95.312 (96.218)
Epoch: [219][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2682 (0.2521) ([0.131]+[0.137])	Prec@1 96.094 (96.161)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4789 (0.4789) ([0.342]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 89.970
current lr 1.00000e-02
Grad=  tensor(9.5350, device='cuda:0')
Epoch: [220][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.2749 (0.2749) ([0.138]+[0.137])	Prec@1 94.531 (94.531)
Epoch: [220][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.3483 (0.2579) ([0.211]+[0.137])	Prec@1 92.188 (95.862)
Epoch: [220][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1983 (0.2539) ([0.061]+[0.137])	Prec@1 98.438 (96.039)
Epoch: [220][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3018 (0.2576) ([0.165]+[0.137])	Prec@1 95.312 (95.876)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.5350 (0.5350) ([0.398]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 90.410
current lr 1.00000e-02
Grad=  tensor(8.5021, device='cuda:0')
Epoch: [221][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.3008 (0.3008) ([0.164]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [221][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2080 (0.2375) ([0.071]+[0.137])	Prec@1 97.656 (96.643)
Epoch: [221][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2788 (0.2453) ([0.142]+[0.137])	Prec@1 95.312 (96.343)
Epoch: [221][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2292 (0.2501) ([0.092]+[0.137])	Prec@1 96.094 (96.146)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.4463 (0.4463) ([0.309]+[0.137])	Prec@1 89.062 (89.062)
 * Prec@1 89.480
current lr 1.00000e-02
Grad=  tensor(6.0146, device='cuda:0')
Epoch: [222][0/391]	Time 0.258 (0.258)	Data 0.135 (0.135)	Loss 0.2294 (0.2294) ([0.093]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [222][100/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.3215 (0.2469) ([0.185]+[0.137])	Prec@1 92.188 (96.156)
Epoch: [222][200/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.2184 (0.2511) ([0.081]+[0.137])	Prec@1 97.656 (96.059)
Epoch: [222][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2324 (0.2528) ([0.096]+[0.137])	Prec@1 96.875 (96.042)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.6279 (0.6279) ([0.491]+[0.137])	Prec@1 85.938 (85.938)
 * Prec@1 87.270
current lr 1.00000e-02
Grad=  tensor(2.1836, device='cuda:0')
Epoch: [223][0/391]	Time 0.258 (0.258)	Data 0.136 (0.136)	Loss 0.1955 (0.1955) ([0.058]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [223][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2318 (0.2491) ([0.095]+[0.137])	Prec@1 94.531 (96.194)
Epoch: [223][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2325 (0.2503) ([0.096]+[0.137])	Prec@1 95.312 (96.102)
Epoch: [223][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1838 (0.2567) ([0.047]+[0.137])	Prec@1 100.000 (95.837)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4611 (0.4611) ([0.324]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 90.320
current lr 1.00000e-02
Grad=  tensor(13.6432, device='cuda:0')
Epoch: [224][0/391]	Time 0.257 (0.257)	Data 0.136 (0.136)	Loss 0.3693 (0.3693) ([0.232]+[0.137])	Prec@1 93.750 (93.750)
Epoch: [224][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2165 (0.2494) ([0.080]+[0.137])	Prec@1 98.438 (96.140)
Epoch: [224][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2232 (0.2530) ([0.086]+[0.137])	Prec@1 96.875 (96.000)
Epoch: [224][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.2314 (0.2542) ([0.095]+[0.137])	Prec@1 97.656 (95.993)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3926 (0.3926) ([0.256]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 89.990
current lr 1.00000e-02
Grad=  tensor(5.0507, device='cuda:0')
Epoch: [225][0/391]	Time 0.253 (0.253)	Data 0.131 (0.131)	Loss 0.2196 (0.2196) ([0.083]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [225][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2491 (0.2353) ([0.112]+[0.137])	Prec@1 96.094 (96.744)
Epoch: [225][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2288 (0.2432) ([0.092]+[0.137])	Prec@1 96.875 (96.475)
Epoch: [225][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3235 (0.2520) ([0.187]+[0.137])	Prec@1 92.188 (96.148)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.4368 (0.4368) ([0.300]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 89.460
current lr 1.00000e-02
Grad=  tensor(6.6465, device='cuda:0')
Epoch: [226][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.2254 (0.2254) ([0.088]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [226][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2456 (0.2457) ([0.109]+[0.137])	Prec@1 97.656 (96.426)
Epoch: [226][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2079 (0.2480) ([0.071]+[0.137])	Prec@1 96.875 (96.366)
Epoch: [226][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2771 (0.2500) ([0.140]+[0.137])	Prec@1 93.750 (96.249)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4028 (0.4028) ([0.266]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 90.010
current lr 1.00000e-02
Grad=  tensor(8.3983, device='cuda:0')
Epoch: [227][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2374 (0.2374) ([0.101]+[0.137])	Prec@1 96.094 (96.094)
Epoch: [227][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2497 (0.2411) ([0.113]+[0.137])	Prec@1 97.656 (96.473)
Epoch: [227][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2688 (0.2433) ([0.132]+[0.137])	Prec@1 95.312 (96.370)
Epoch: [227][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2626 (0.2491) ([0.126]+[0.137])	Prec@1 96.875 (96.185)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3809 (0.3809) ([0.244]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 89.990
current lr 1.00000e-02
Grad=  tensor(9.6240, device='cuda:0')
Epoch: [228][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2189 (0.2189) ([0.082]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [228][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2280 (0.2438) ([0.091]+[0.137])	Prec@1 96.094 (96.403)
Epoch: [228][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2564 (0.2523) ([0.120]+[0.137])	Prec@1 97.656 (96.074)
Epoch: [228][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2155 (0.2578) ([0.078]+[0.137])	Prec@1 98.438 (95.860)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.4952 (0.4952) ([0.358]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 90.990
current lr 1.00000e-02
Grad=  tensor(6.8859, device='cuda:0')
Epoch: [229][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2533 (0.2533) ([0.116]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [229][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.3513 (0.2457) ([0.214]+[0.137])	Prec@1 92.969 (96.419)
Epoch: [229][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3207 (0.2490) ([0.184]+[0.137])	Prec@1 93.750 (96.261)
Epoch: [229][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1885 (0.2478) ([0.052]+[0.137])	Prec@1 99.219 (96.265)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4435 (0.4435) ([0.306]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 89.580
current lr 1.00000e-02
Grad=  tensor(5.3039, device='cuda:0')
Epoch: [230][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2149 (0.2149) ([0.078]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [230][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.3739 (0.2490) ([0.237]+[0.137])	Prec@1 92.969 (96.148)
Epoch: [230][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3631 (0.2506) ([0.226]+[0.137])	Prec@1 92.969 (96.067)
Epoch: [230][300/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.2647 (0.2535) ([0.128]+[0.137])	Prec@1 96.875 (95.935)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.5193 (0.5193) ([0.382]+[0.137])	Prec@1 88.281 (88.281)
 * Prec@1 90.210
current lr 1.00000e-02
Grad=  tensor(4.4688, device='cuda:0')
Epoch: [231][0/391]	Time 0.256 (0.256)	Data 0.134 (0.134)	Loss 0.2208 (0.2208) ([0.084]+[0.137])	Prec@1 98.438 (98.438)
Epoch: [231][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.2751 (0.2438) ([0.138]+[0.137])	Prec@1 96.094 (96.341)
Epoch: [231][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.2953 (0.2470) ([0.158]+[0.137])	Prec@1 95.312 (96.269)
Epoch: [231][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.3024 (0.2522) ([0.165]+[0.137])	Prec@1 94.531 (96.052)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.7352 (0.7352) ([0.598]+[0.137])	Prec@1 85.938 (85.938)
 * Prec@1 88.080
current lr 1.00000e-02
Grad=  tensor(5.1670, device='cuda:0')
Epoch: [232][0/391]	Time 0.279 (0.279)	Data 0.156 (0.156)	Loss 0.2072 (0.2072) ([0.070]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [232][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2305 (0.2410) ([0.094]+[0.137])	Prec@1 96.094 (96.442)
Epoch: [232][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3512 (0.2494) ([0.214]+[0.137])	Prec@1 93.750 (96.249)
Epoch: [232][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2724 (0.2495) ([0.135]+[0.137])	Prec@1 96.094 (96.221)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4822 (0.4822) ([0.345]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 89.960
current lr 1.00000e-02
Grad=  tensor(7.6396, device='cuda:0')
Epoch: [233][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2637 (0.2637) ([0.127]+[0.137])	Prec@1 94.531 (94.531)
Epoch: [233][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2385 (0.2475) ([0.102]+[0.137])	Prec@1 97.656 (96.194)
Epoch: [233][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2239 (0.2467) ([0.087]+[0.137])	Prec@1 96.875 (96.276)
Epoch: [233][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2011 (0.2504) ([0.064]+[0.137])	Prec@1 98.438 (96.127)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.5635 (0.5635) ([0.427]+[0.137])	Prec@1 86.719 (86.719)
 * Prec@1 89.240
current lr 1.00000e-02
Grad=  tensor(7.1761, device='cuda:0')
Epoch: [234][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.2454 (0.2454) ([0.108]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [234][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1752 (0.2367) ([0.038]+[0.137])	Prec@1 100.000 (96.821)
Epoch: [234][200/391]	Time 0.114 (0.113)	Data 0.000 (0.001)	Loss 0.2744 (0.2424) ([0.138]+[0.137])	Prec@1 96.094 (96.642)
Epoch: [234][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2993 (0.2499) ([0.162]+[0.137])	Prec@1 92.969 (96.348)
Test: [0/79]	Time 0.162 (0.162)	Loss 0.4394 (0.4394) ([0.302]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 89.550
current lr 1.00000e-02
Grad=  tensor(8.7588, device='cuda:0')
Epoch: [235][0/391]	Time 0.260 (0.260)	Data 0.137 (0.137)	Loss 0.2577 (0.2577) ([0.121]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [235][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2232 (0.2464) ([0.086]+[0.137])	Prec@1 98.438 (96.473)
Epoch: [235][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2625 (0.2481) ([0.126]+[0.137])	Prec@1 92.969 (96.292)
Epoch: [235][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1725 (0.2482) ([0.036]+[0.137])	Prec@1 99.219 (96.273)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4661 (0.4661) ([0.329]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 90.350
current lr 1.00000e-02
Grad=  tensor(10.6833, device='cuda:0')
Epoch: [236][0/391]	Time 0.268 (0.268)	Data 0.144 (0.144)	Loss 0.2872 (0.2872) ([0.150]+[0.137])	Prec@1 96.094 (96.094)
Epoch: [236][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.2050 (0.2505) ([0.068]+[0.137])	Prec@1 98.438 (96.202)
Epoch: [236][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2778 (0.2490) ([0.141]+[0.137])	Prec@1 96.094 (96.319)
Epoch: [236][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2337 (0.2534) ([0.097]+[0.137])	Prec@1 96.875 (96.034)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.3665 (0.3665) ([0.230]+[0.137])	Prec@1 92.969 (92.969)
 * Prec@1 90.130
current lr 1.00000e-02
Grad=  tensor(10.5256, device='cuda:0')
Epoch: [237][0/391]	Time 0.261 (0.261)	Data 0.138 (0.138)	Loss 0.2814 (0.2814) ([0.144]+[0.137])	Prec@1 93.750 (93.750)
Epoch: [237][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1979 (0.2588) ([0.061]+[0.137])	Prec@1 99.219 (96.055)
Epoch: [237][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2120 (0.2529) ([0.075]+[0.137])	Prec@1 98.438 (96.168)
Epoch: [237][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2861 (0.2541) ([0.149]+[0.137])	Prec@1 95.312 (96.133)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.4032 (0.4032) ([0.266]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 89.910
current lr 1.00000e-02
Grad=  tensor(9.1664, device='cuda:0')
Epoch: [238][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2286 (0.2286) ([0.092]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [238][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2008 (0.2379) ([0.064]+[0.137])	Prec@1 98.438 (96.434)
Epoch: [238][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2908 (0.2390) ([0.154]+[0.137])	Prec@1 94.531 (96.444)
Epoch: [238][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2647 (0.2427) ([0.128]+[0.137])	Prec@1 96.875 (96.358)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.4938 (0.4938) ([0.357]+[0.137])	Prec@1 89.062 (89.062)
 * Prec@1 89.990
current lr 1.00000e-02
Grad=  tensor(12.4591, device='cuda:0')
Epoch: [239][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2897 (0.2897) ([0.153]+[0.137])	Prec@1 93.750 (93.750)
Epoch: [239][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2092 (0.2395) ([0.073]+[0.137])	Prec@1 98.438 (96.713)
Epoch: [239][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3070 (0.2477) ([0.170]+[0.137])	Prec@1 93.750 (96.292)
Epoch: [239][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3091 (0.2570) ([0.172]+[0.137])	Prec@1 93.750 (95.884)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3301 (0.3301) ([0.193]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 90.230
current lr 1.00000e-02
Grad=  tensor(6.0835, device='cuda:0')
Epoch: [240][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.2324 (0.2324) ([0.095]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [240][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1975 (0.2450) ([0.061]+[0.137])	Prec@1 97.656 (96.426)
Epoch: [240][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2852 (0.2462) ([0.148]+[0.137])	Prec@1 95.312 (96.374)
Epoch: [240][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2271 (0.2517) ([0.090]+[0.137])	Prec@1 97.656 (96.185)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.4600 (0.4600) ([0.323]+[0.137])	Prec@1 91.406 (91.406)
 * Prec@1 90.560
current lr 1.00000e-02
Grad=  tensor(5.4728, device='cuda:0')
Epoch: [241][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.2313 (0.2313) ([0.094]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [241][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1972 (0.2454) ([0.060]+[0.137])	Prec@1 97.656 (96.241)
Epoch: [241][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2074 (0.2461) ([0.071]+[0.137])	Prec@1 96.875 (96.238)
Epoch: [241][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2795 (0.2509) ([0.143]+[0.137])	Prec@1 96.094 (96.091)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.3678 (0.3678) ([0.231]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 89.930
current lr 1.00000e-02
Grad=  tensor(4.7198, device='cuda:0')
Epoch: [242][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.1941 (0.1941) ([0.057]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [242][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2602 (0.2393) ([0.123]+[0.137])	Prec@1 97.656 (96.535)
Epoch: [242][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2312 (0.2386) ([0.094]+[0.137])	Prec@1 96.875 (96.541)
Epoch: [242][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2613 (0.2447) ([0.125]+[0.137])	Prec@1 95.312 (96.257)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.4495 (0.4495) ([0.313]+[0.137])	Prec@1 89.844 (89.844)
 * Prec@1 89.860
current lr 1.00000e-02
Grad=  tensor(6.7725, device='cuda:0')
Epoch: [243][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.2156 (0.2156) ([0.079]+[0.137])	Prec@1 97.656 (97.656)
Epoch: [243][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2976 (0.2411) ([0.161]+[0.137])	Prec@1 93.750 (96.620)
Epoch: [243][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2942 (0.2460) ([0.157]+[0.137])	Prec@1 93.750 (96.428)
Epoch: [243][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1720 (0.2492) ([0.035]+[0.137])	Prec@1 100.000 (96.265)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.4236 (0.4236) ([0.287]+[0.137])	Prec@1 93.750 (93.750)
 * Prec@1 88.910
current lr 1.00000e-02
Grad=  tensor(8.9924, device='cuda:0')
Epoch: [244][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.2679 (0.2679) ([0.131]+[0.137])	Prec@1 96.094 (96.094)
Epoch: [244][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2786 (0.2450) ([0.142]+[0.137])	Prec@1 95.312 (96.426)
Epoch: [244][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2452 (0.2437) ([0.108]+[0.137])	Prec@1 96.094 (96.432)
Epoch: [244][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2278 (0.2518) ([0.091]+[0.137])	Prec@1 95.312 (96.257)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.5807 (0.5807) ([0.444]+[0.137])	Prec@1 87.500 (87.500)
 * Prec@1 89.530
current lr 1.00000e-02
Grad=  tensor(20.5022, device='cuda:0')
Epoch: [245][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.3305 (0.3305) ([0.194]+[0.137])	Prec@1 92.188 (92.188)
Epoch: [245][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2320 (0.2427) ([0.095]+[0.137])	Prec@1 96.094 (96.504)
Epoch: [245][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2617 (0.2513) ([0.125]+[0.137])	Prec@1 96.875 (96.195)
Epoch: [245][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2755 (0.2522) ([0.139]+[0.137])	Prec@1 93.750 (96.138)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5086 (0.5086) ([0.372]+[0.137])	Prec@1 89.062 (89.062)
 * Prec@1 89.950
current lr 1.00000e-02
Grad=  tensor(7.5046, device='cuda:0')
Epoch: [246][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.2164 (0.2164) ([0.079]+[0.137])	Prec@1 96.875 (96.875)
Epoch: [246][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2609 (0.2393) ([0.124]+[0.137])	Prec@1 96.875 (96.535)
Epoch: [246][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.3049 (0.2422) ([0.168]+[0.137])	Prec@1 92.969 (96.471)
Epoch: [246][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1877 (0.2499) ([0.051]+[0.137])	Prec@1 99.219 (96.205)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.5623 (0.5623) ([0.425]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 89.880
current lr 1.00000e-02
Grad=  tensor(7.3191, device='cuda:0')
Epoch: [247][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2479 (0.2479) ([0.111]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [247][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.2209 (0.2446) ([0.084]+[0.137])	Prec@1 96.875 (96.395)
Epoch: [247][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2997 (0.2464) ([0.163]+[0.137])	Prec@1 96.094 (96.331)
Epoch: [247][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.2311 (0.2477) ([0.094]+[0.137])	Prec@1 97.656 (96.234)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4002 (0.4002) ([0.263]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 90.280
current lr 1.00000e-02
Grad=  tensor(8.8968, device='cuda:0')
Epoch: [248][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.2459 (0.2459) ([0.109]+[0.137])	Prec@1 94.531 (94.531)
Epoch: [248][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1960 (0.2397) ([0.059]+[0.137])	Prec@1 98.438 (96.504)
Epoch: [248][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2361 (0.2445) ([0.099]+[0.137])	Prec@1 96.094 (96.280)
Epoch: [248][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.3464 (0.2482) ([0.209]+[0.137])	Prec@1 92.969 (96.125)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.4138 (0.4138) ([0.277]+[0.137])	Prec@1 90.625 (90.625)
 * Prec@1 89.140
current lr 1.00000e-02
Grad=  tensor(3.0022, device='cuda:0')
Epoch: [249][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.2016 (0.2016) ([0.064]+[0.137])	Prec@1 99.219 (99.219)
Epoch: [249][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2359 (0.2455) ([0.099]+[0.137])	Prec@1 96.875 (96.380)
Epoch: [249][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2158 (0.2479) ([0.079]+[0.137])	Prec@1 97.656 (96.265)
Epoch: [249][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.2897 (0.2501) ([0.153]+[0.137])	Prec@1 94.531 (96.203)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.5218 (0.5218) ([0.385]+[0.137])	Prec@1 88.281 (88.281)
 * Prec@1 91.240
current lr 1.00000e-03
Grad=  tensor(11.4933, device='cuda:0')
Epoch: [250][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.2706 (0.2706) ([0.133]+[0.137])	Prec@1 95.312 (95.312)
Epoch: [250][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.2018 (0.2140) ([0.066]+[0.136])	Prec@1 97.656 (97.471)
Epoch: [250][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1712 (0.2015) ([0.036]+[0.136])	Prec@1 99.219 (97.944)
Epoch: [250][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1997 (0.1964) ([0.064]+[0.136])	Prec@1 98.438 (98.144)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.3250 (0.3250) ([0.190]+[0.135])	Prec@1 94.531 (94.531)
 * Prec@1 93.300
current lr 1.00000e-03
Grad=  tensor(1.6057, device='cuda:0')
Epoch: [251][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.1673 (0.1673) ([0.032]+[0.135])	Prec@1 100.000 (100.000)
Epoch: [251][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.2025 (0.1734) ([0.067]+[0.135])	Prec@1 97.656 (99.033)
Epoch: [251][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1552 (0.1730) ([0.020]+[0.135])	Prec@1 100.000 (99.044)
Epoch: [251][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1544 (0.1727) ([0.019]+[0.135])	Prec@1 100.000 (99.019)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.3160 (0.3160) ([0.181]+[0.135])	Prec@1 94.531 (94.531)
 * Prec@1 93.470
current lr 1.00000e-03
Grad=  tensor(2.1451, device='cuda:0')
Epoch: [252][0/391]	Time 0.262 (0.262)	Data 0.138 (0.138)	Loss 0.1683 (0.1683) ([0.033]+[0.135])	Prec@1 99.219 (99.219)
Epoch: [252][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1515 (0.1671) ([0.017]+[0.135])	Prec@1 100.000 (99.196)
Epoch: [252][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1728 (0.1673) ([0.038]+[0.135])	Prec@1 99.219 (99.137)
Epoch: [252][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1639 (0.1671) ([0.029]+[0.134])	Prec@1 100.000 (99.141)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3048 (0.3048) ([0.170]+[0.134])	Prec@1 93.750 (93.750)
 * Prec@1 93.700
current lr 1.00000e-03
Grad=  tensor(1.7012, device='cuda:0')
Epoch: [253][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1595 (0.1595) ([0.025]+[0.134])	Prec@1 99.219 (99.219)
Epoch: [253][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1647 (0.1628) ([0.030]+[0.134])	Prec@1 99.219 (99.312)
Epoch: [253][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1504 (0.1611) ([0.016]+[0.134])	Prec@1 100.000 (99.386)
Epoch: [253][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1562 (0.1618) ([0.022]+[0.134])	Prec@1 100.000 (99.336)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.2863 (0.2863) ([0.152]+[0.134])	Prec@1 95.312 (95.312)
 * Prec@1 93.490
current lr 1.00000e-03
Grad=  tensor(0.3440, device='cuda:0')
Epoch: [254][0/391]	Time 0.263 (0.263)	Data 0.141 (0.141)	Loss 0.1433 (0.1433) ([0.010]+[0.134])	Prec@1 100.000 (100.000)
Epoch: [254][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1584 (0.1557) ([0.025]+[0.134])	Prec@1 100.000 (99.551)
Epoch: [254][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1475 (0.1563) ([0.014]+[0.134])	Prec@1 100.000 (99.534)
Epoch: [254][300/391]	Time 0.111 (0.113)	Data 0.000 (0.001)	Loss 0.1745 (0.1565) ([0.041]+[0.133])	Prec@1 98.438 (99.538)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2901 (0.2901) ([0.157]+[0.133])	Prec@1 96.094 (96.094)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(1.1183, device='cuda:0')
Epoch: [255][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 0.1547 (0.1547) ([0.021]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [255][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1462 (0.1559) ([0.013]+[0.133])	Prec@1 100.000 (99.497)
Epoch: [255][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1447 (0.1560) ([0.012]+[0.133])	Prec@1 100.000 (99.475)
Epoch: [255][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1714 (0.1562) ([0.038]+[0.133])	Prec@1 98.438 (99.473)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2891 (0.2891) ([0.156]+[0.133])	Prec@1 96.094 (96.094)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.4708, device='cuda:0')
Epoch: [256][0/391]	Time 0.260 (0.260)	Data 0.138 (0.138)	Loss 0.1440 (0.1440) ([0.011]+[0.133])	Prec@1 100.000 (100.000)
Epoch: [256][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1412 (0.1522) ([0.008]+[0.133])	Prec@1 100.000 (99.613)
Epoch: [256][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1369 (0.1531) ([0.004]+[0.133])	Prec@1 100.000 (99.572)
Epoch: [256][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1611 (0.1533) ([0.029]+[0.132])	Prec@1 98.438 (99.538)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2883 (0.2883) ([0.156]+[0.132])	Prec@1 96.094 (96.094)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(2.0469, device='cuda:0')
Epoch: [257][0/391]	Time 0.255 (0.255)	Data 0.133 (0.133)	Loss 0.1593 (0.1593) ([0.027]+[0.132])	Prec@1 99.219 (99.219)
Epoch: [257][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1454 (0.1511) ([0.013]+[0.132])	Prec@1 100.000 (99.590)
Epoch: [257][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1518 (0.1505) ([0.020]+[0.132])	Prec@1 99.219 (99.619)
Epoch: [257][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1546 (0.1508) ([0.023]+[0.132])	Prec@1 99.219 (99.621)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2903 (0.2903) ([0.158]+[0.132])	Prec@1 95.312 (95.312)
 * Prec@1 93.820
current lr 1.00000e-03
Grad=  tensor(0.7512, device='cuda:0')
Epoch: [258][0/391]	Time 0.259 (0.259)	Data 0.138 (0.138)	Loss 0.1455 (0.1455) ([0.014]+[0.132])	Prec@1 100.000 (100.000)
Epoch: [258][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1468 (0.1473) ([0.015]+[0.132])	Prec@1 100.000 (99.776)
Epoch: [258][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1476 (0.1485) ([0.016]+[0.132])	Prec@1 99.219 (99.708)
Epoch: [258][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1582 (0.1496) ([0.027]+[0.132])	Prec@1 99.219 (99.668)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2783 (0.2783) ([0.147]+[0.131])	Prec@1 96.094 (96.094)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(2.0029, device='cuda:0')
Epoch: [259][0/391]	Time 0.260 (0.260)	Data 0.139 (0.139)	Loss 0.1549 (0.1549) ([0.024]+[0.131])	Prec@1 99.219 (99.219)
Epoch: [259][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1388 (0.1480) ([0.008]+[0.131])	Prec@1 100.000 (99.706)
Epoch: [259][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1396 (0.1481) ([0.008]+[0.131])	Prec@1 100.000 (99.666)
Epoch: [259][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1414 (0.1482) ([0.010]+[0.131])	Prec@1 100.000 (99.668)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.2701 (0.2701) ([0.139]+[0.131])	Prec@1 95.312 (95.312)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.3885, device='cuda:0')
Epoch: [260][0/391]	Time 0.255 (0.255)	Data 0.134 (0.134)	Loss 0.1381 (0.1381) ([0.007]+[0.131])	Prec@1 100.000 (100.000)
Epoch: [260][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1418 (0.1463) ([0.011]+[0.131])	Prec@1 100.000 (99.714)
Epoch: [260][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1387 (0.1466) ([0.008]+[0.131])	Prec@1 100.000 (99.701)
Epoch: [260][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1470 (0.1468) ([0.016]+[0.131])	Prec@1 100.000 (99.696)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.3084 (0.3084) ([0.178]+[0.130])	Prec@1 95.312 (95.312)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(1.9883, device='cuda:0')
Epoch: [261][0/391]	Time 0.260 (0.260)	Data 0.139 (0.139)	Loss 0.1511 (0.1511) ([0.021]+[0.130])	Prec@1 99.219 (99.219)
Epoch: [261][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1378 (0.1468) ([0.007]+[0.130])	Prec@1 100.000 (99.714)
Epoch: [261][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1409 (0.1459) ([0.011]+[0.130])	Prec@1 100.000 (99.708)
Epoch: [261][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1423 (0.1459) ([0.012]+[0.130])	Prec@1 100.000 (99.707)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2736 (0.2736) ([0.144]+[0.130])	Prec@1 96.094 (96.094)
 * Prec@1 94.000
current lr 1.00000e-03
Grad=  tensor(3.8784, device='cuda:0')
Epoch: [262][0/391]	Time 0.261 (0.261)	Data 0.140 (0.140)	Loss 0.1634 (0.1634) ([0.033]+[0.130])	Prec@1 99.219 (99.219)
Epoch: [262][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1398 (0.1450) ([0.010]+[0.130])	Prec@1 100.000 (99.783)
Epoch: [262][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1453 (0.1459) ([0.016]+[0.130])	Prec@1 100.000 (99.701)
Epoch: [262][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1458 (0.1453) ([0.016]+[0.130])	Prec@1 99.219 (99.707)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2765 (0.2765) ([0.147]+[0.130])	Prec@1 95.312 (95.312)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(0.1573, device='cuda:0')
Epoch: [263][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.1348 (0.1348) ([0.005]+[0.130])	Prec@1 100.000 (100.000)
Epoch: [263][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1396 (0.1435) ([0.010]+[0.129])	Prec@1 100.000 (99.729)
Epoch: [263][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1403 (0.1435) ([0.011]+[0.129])	Prec@1 100.000 (99.759)
Epoch: [263][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1378 (0.1435) ([0.009]+[0.129])	Prec@1 100.000 (99.769)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2662 (0.2662) ([0.137]+[0.129])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.8603, device='cuda:0')
Epoch: [264][0/391]	Time 0.259 (0.259)	Data 0.137 (0.137)	Loss 0.1420 (0.1420) ([0.013]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [264][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1434 (0.1428) ([0.014]+[0.129])	Prec@1 100.000 (99.807)
Epoch: [264][200/391]	Time 0.112 (0.111)	Data 0.000 (0.001)	Loss 0.1442 (0.1425) ([0.015]+[0.129])	Prec@1 100.000 (99.790)
Epoch: [264][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.1381 (0.1420) ([0.009]+[0.129])	Prec@1 100.000 (99.785)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2354 (0.2354) ([0.107]+[0.129])	Prec@1 96.875 (96.875)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.8803, device='cuda:0')
Epoch: [265][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.1405 (0.1405) ([0.012]+[0.129])	Prec@1 100.000 (100.000)
Epoch: [265][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1484 (0.1415) ([0.020]+[0.128])	Prec@1 100.000 (99.807)
Epoch: [265][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1696 (0.1410) ([0.041]+[0.128])	Prec@1 98.438 (99.852)
Epoch: [265][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1401 (0.1415) ([0.012]+[0.128])	Prec@1 100.000 (99.821)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2440 (0.2440) ([0.116]+[0.128])	Prec@1 96.094 (96.094)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.1557, device='cuda:0')
Epoch: [266][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.1325 (0.1325) ([0.004]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [266][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1400 (0.1405) ([0.012]+[0.128])	Prec@1 99.219 (99.776)
Epoch: [266][200/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1329 (0.1412) ([0.005]+[0.128])	Prec@1 100.000 (99.751)
Epoch: [266][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1474 (0.1412) ([0.020]+[0.128])	Prec@1 99.219 (99.746)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.2791 (0.2791) ([0.151]+[0.128])	Prec@1 94.531 (94.531)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.4054, device='cuda:0')
Epoch: [267][0/391]	Time 0.270 (0.270)	Data 0.146 (0.146)	Loss 0.1345 (0.1345) ([0.007]+[0.128])	Prec@1 100.000 (100.000)
Epoch: [267][100/391]	Time 0.113 (0.114)	Data 0.000 (0.002)	Loss 0.1357 (0.1410) ([0.008]+[0.128])	Prec@1 100.000 (99.706)
Epoch: [267][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1365 (0.1405) ([0.009]+[0.127])	Prec@1 100.000 (99.771)
Epoch: [267][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1387 (0.1403) ([0.011]+[0.127])	Prec@1 100.000 (99.772)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.2639 (0.2639) ([0.137]+[0.127])	Prec@1 95.312 (95.312)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.5089, device='cuda:0')
Epoch: [268][0/391]	Time 0.264 (0.264)	Data 0.140 (0.140)	Loss 0.1383 (0.1383) ([0.011]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [268][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1427 (0.1382) ([0.016]+[0.127])	Prec@1 99.219 (99.830)
Epoch: [268][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1378 (0.1386) ([0.011]+[0.127])	Prec@1 100.000 (99.813)
Epoch: [268][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1335 (0.1389) ([0.007]+[0.127])	Prec@1 100.000 (99.798)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.2485 (0.2485) ([0.122]+[0.127])	Prec@1 95.312 (95.312)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.4755, device='cuda:0')
Epoch: [269][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.1364 (0.1364) ([0.010]+[0.127])	Prec@1 100.000 (100.000)
Epoch: [269][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1397 (0.1388) ([0.013]+[0.127])	Prec@1 100.000 (99.807)
Epoch: [269][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1345 (0.1390) ([0.008]+[0.127])	Prec@1 100.000 (99.782)
Epoch: [269][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1301 (0.1391) ([0.004]+[0.126])	Prec@1 100.000 (99.782)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2663 (0.2663) ([0.140]+[0.126])	Prec@1 95.312 (95.312)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.2909, device='cuda:0')
Epoch: [270][0/391]	Time 0.270 (0.270)	Data 0.147 (0.147)	Loss 0.1335 (0.1335) ([0.007]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [270][100/391]	Time 0.112 (0.114)	Data 0.000 (0.002)	Loss 0.1342 (0.1382) ([0.008]+[0.126])	Prec@1 100.000 (99.807)
Epoch: [270][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1476 (0.1381) ([0.021]+[0.126])	Prec@1 99.219 (99.806)
Epoch: [270][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1387 (0.1381) ([0.013]+[0.126])	Prec@1 100.000 (99.800)
Test: [0/79]	Time 0.174 (0.174)	Loss 0.2766 (0.2766) ([0.151]+[0.126])	Prec@1 94.531 (94.531)
 * Prec@1 93.690
current lr 1.00000e-03
Grad=  tensor(0.5130, device='cuda:0')
Epoch: [271][0/391]	Time 0.265 (0.265)	Data 0.141 (0.141)	Loss 0.1330 (0.1330) ([0.007]+[0.126])	Prec@1 100.000 (100.000)
Epoch: [271][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1285 (0.1382) ([0.003]+[0.126])	Prec@1 100.000 (99.745)
Epoch: [271][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1462 (0.1367) ([0.021]+[0.126])	Prec@1 99.219 (99.817)
Epoch: [271][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1343 (0.1362) ([0.009]+[0.126])	Prec@1 100.000 (99.839)
Test: [0/79]	Time 0.177 (0.177)	Loss 0.2607 (0.2607) ([0.135]+[0.125])	Prec@1 95.312 (95.312)
 * Prec@1 93.660
current lr 1.00000e-03
Grad=  tensor(0.4313, device='cuda:0')
Epoch: [272][0/391]	Time 0.259 (0.259)	Data 0.138 (0.138)	Loss 0.1333 (0.1333) ([0.008]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [272][100/391]	Time 0.111 (0.112)	Data 0.000 (0.001)	Loss 0.1338 (0.1362) ([0.009]+[0.125])	Prec@1 100.000 (99.869)
Epoch: [272][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1485 (0.1360) ([0.023]+[0.125])	Prec@1 99.219 (99.852)
Epoch: [272][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1354 (0.1363) ([0.010]+[0.125])	Prec@1 100.000 (99.829)
Test: [0/79]	Time 0.178 (0.178)	Loss 0.2590 (0.2590) ([0.134]+[0.125])	Prec@1 94.531 (94.531)
 * Prec@1 93.730
current lr 1.00000e-03
Grad=  tensor(1.4664, device='cuda:0')
Epoch: [273][0/391]	Time 0.265 (0.265)	Data 0.142 (0.142)	Loss 0.1401 (0.1401) ([0.015]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [273][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.1322 (0.1359) ([0.007]+[0.125])	Prec@1 100.000 (99.814)
Epoch: [273][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1329 (0.1359) ([0.008]+[0.125])	Prec@1 100.000 (99.802)
Epoch: [273][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1298 (0.1357) ([0.005]+[0.125])	Prec@1 100.000 (99.816)
Test: [0/79]	Time 0.181 (0.181)	Loss 0.2606 (0.2606) ([0.136]+[0.125])	Prec@1 96.094 (96.094)
 * Prec@1 93.770
current lr 1.00000e-03
Grad=  tensor(0.6297, device='cuda:0')
Epoch: [274][0/391]	Time 0.267 (0.267)	Data 0.145 (0.145)	Loss 0.1336 (0.1336) ([0.009]+[0.125])	Prec@1 100.000 (100.000)
Epoch: [274][100/391]	Time 0.111 (0.112)	Data 0.000 (0.002)	Loss 0.1326 (0.1341) ([0.008]+[0.124])	Prec@1 100.000 (99.869)
Epoch: [274][200/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1389 (0.1342) ([0.015]+[0.124])	Prec@1 100.000 (99.856)
Epoch: [274][300/391]	Time 0.111 (0.111)	Data 0.000 (0.001)	Loss 0.1297 (0.1344) ([0.005]+[0.124])	Prec@1 100.000 (99.865)
Test: [0/79]	Time 0.176 (0.176)	Loss 0.2486 (0.2486) ([0.125]+[0.124])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.3232, device='cuda:0')
Epoch: [275][0/391]	Time 0.264 (0.264)	Data 0.143 (0.143)	Loss 0.1330 (0.1330) ([0.009]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [275][100/391]	Time 0.110 (0.112)	Data 0.000 (0.002)	Loss 0.1289 (0.1343) ([0.005]+[0.124])	Prec@1 100.000 (99.853)
Epoch: [275][200/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.1281 (0.1342) ([0.004]+[0.124])	Prec@1 100.000 (99.852)
Epoch: [275][300/391]	Time 0.113 (0.112)	Data 0.000 (0.001)	Loss 0.1319 (0.1338) ([0.008]+[0.124])	Prec@1 100.000 (99.860)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.2415 (0.2415) ([0.118]+[0.124])	Prec@1 96.094 (96.094)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(0.7378, device='cuda:0')
Epoch: [276][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.1334 (0.1334) ([0.010]+[0.124])	Prec@1 100.000 (100.000)
Epoch: [276][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1435 (0.1334) ([0.020]+[0.124])	Prec@1 99.219 (99.822)
Epoch: [276][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1262 (0.1334) ([0.003]+[0.123])	Prec@1 100.000 (99.841)
Epoch: [276][300/391]	Time 0.112 (0.112)	Data 0.000 (0.001)	Loss 0.1381 (0.1334) ([0.015]+[0.123])	Prec@1 100.000 (99.842)
Test: [0/79]	Time 0.170 (0.170)	Loss 0.2487 (0.2487) ([0.125]+[0.123])	Prec@1 96.094 (96.094)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(0.1509, device='cuda:0')
Epoch: [277][0/391]	Time 0.257 (0.257)	Data 0.133 (0.133)	Loss 0.1271 (0.1271) ([0.004]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [277][100/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1335 (0.1334) ([0.010]+[0.123])	Prec@1 100.000 (99.853)
Epoch: [277][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1280 (0.1333) ([0.005]+[0.123])	Prec@1 100.000 (99.837)
Epoch: [277][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1269 (0.1332) ([0.004]+[0.123])	Prec@1 100.000 (99.842)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.2348 (0.2348) ([0.112]+[0.123])	Prec@1 95.312 (95.312)
 * Prec@1 93.740
current lr 1.00000e-03
Grad=  tensor(0.3074, device='cuda:0')
Epoch: [278][0/391]	Time 0.259 (0.259)	Data 0.134 (0.134)	Loss 0.1262 (0.1262) ([0.003]+[0.123])	Prec@1 100.000 (100.000)
Epoch: [278][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1297 (0.1324) ([0.007]+[0.123])	Prec@1 100.000 (99.876)
Epoch: [278][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1265 (0.1329) ([0.004]+[0.123])	Prec@1 100.000 (99.841)
Epoch: [278][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1312 (0.1326) ([0.009]+[0.122])	Prec@1 100.000 (99.847)
Test: [0/79]	Time 0.171 (0.171)	Loss 0.2631 (0.2631) ([0.141]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 93.710
current lr 1.00000e-03
Grad=  tensor(0.6872, device='cuda:0')
Epoch: [279][0/391]	Time 0.258 (0.258)	Data 0.134 (0.134)	Loss 0.1299 (0.1299) ([0.008]+[0.122])	Prec@1 100.000 (100.000)
Epoch: [279][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1340 (0.1307) ([0.012]+[0.122])	Prec@1 100.000 (99.915)
Epoch: [279][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1304 (0.1310) ([0.008]+[0.122])	Prec@1 100.000 (99.903)
Epoch: [279][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1375 (0.1311) ([0.016]+[0.122])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2489 (0.2489) ([0.127]+[0.122])	Prec@1 94.531 (94.531)
 * Prec@1 93.820
current lr 1.00000e-03
Grad=  tensor(3.1563, device='cuda:0')
Epoch: [280][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.1446 (0.1446) ([0.023]+[0.122])	Prec@1 99.219 (99.219)
Epoch: [280][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1246 (0.1306) ([0.003]+[0.122])	Prec@1 100.000 (99.884)
Epoch: [280][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1262 (0.1312) ([0.004]+[0.122])	Prec@1 100.000 (99.852)
Epoch: [280][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1262 (0.1310) ([0.005]+[0.122])	Prec@1 100.000 (99.862)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.2515 (0.2515) ([0.130]+[0.121])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(1.6364, device='cuda:0')
Epoch: [281][0/391]	Time 0.263 (0.263)	Data 0.139 (0.139)	Loss 0.1368 (0.1368) ([0.015]+[0.121])	Prec@1 99.219 (99.219)
Epoch: [281][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1341 (0.1305) ([0.013]+[0.121])	Prec@1 100.000 (99.869)
Epoch: [281][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1285 (0.1301) ([0.007]+[0.121])	Prec@1 100.000 (99.899)
Epoch: [281][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1291 (0.1302) ([0.008]+[0.121])	Prec@1 100.000 (99.883)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2506 (0.2506) ([0.130]+[0.121])	Prec@1 95.312 (95.312)
 * Prec@1 93.640
current lr 1.00000e-03
Grad=  tensor(0.4073, device='cuda:0')
Epoch: [282][0/391]	Time 0.253 (0.253)	Data 0.130 (0.130)	Loss 0.1282 (0.1282) ([0.007]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [282][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1258 (0.1298) ([0.005]+[0.121])	Prec@1 100.000 (99.884)
Epoch: [282][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1257 (0.1297) ([0.005]+[0.121])	Prec@1 100.000 (99.907)
Epoch: [282][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1307 (0.1295) ([0.010]+[0.121])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2559 (0.2559) ([0.135]+[0.121])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.7627, device='cuda:0')
Epoch: [283][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.1291 (0.1291) ([0.009]+[0.121])	Prec@1 100.000 (100.000)
Epoch: [283][100/391]	Time 0.113 (0.114)	Data 0.000 (0.001)	Loss 0.1353 (0.1292) ([0.015]+[0.120])	Prec@1 99.219 (99.923)
Epoch: [283][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1259 (0.1285) ([0.006]+[0.120])	Prec@1 100.000 (99.911)
Epoch: [283][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1247 (0.1285) ([0.004]+[0.120])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.168 (0.168)	Loss 0.2689 (0.2689) ([0.149]+[0.120])	Prec@1 95.312 (95.312)
 * Prec@1 93.930
current lr 1.00000e-03
Grad=  tensor(2.8696, device='cuda:0')
Epoch: [284][0/391]	Time 0.255 (0.255)	Data 0.131 (0.131)	Loss 0.1390 (0.1390) ([0.019]+[0.120])	Prec@1 99.219 (99.219)
Epoch: [284][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1285 (0.1282) ([0.008]+[0.120])	Prec@1 100.000 (99.869)
Epoch: [284][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1254 (0.1281) ([0.005]+[0.120])	Prec@1 100.000 (99.891)
Epoch: [284][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1249 (0.1279) ([0.005]+[0.120])	Prec@1 100.000 (99.899)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2654 (0.2654) ([0.146]+[0.120])	Prec@1 94.531 (94.531)
 * Prec@1 93.780
current lr 1.00000e-03
Grad=  tensor(0.3932, device='cuda:0')
Epoch: [285][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.1257 (0.1257) ([0.006]+[0.120])	Prec@1 100.000 (100.000)
Epoch: [285][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1256 (0.1270) ([0.006]+[0.120])	Prec@1 100.000 (99.946)
Epoch: [285][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1293 (0.1274) ([0.010]+[0.120])	Prec@1 100.000 (99.946)
Epoch: [285][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1304 (0.1278) ([0.011]+[0.119])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2412 (0.2412) ([0.122]+[0.119])	Prec@1 95.312 (95.312)
 * Prec@1 93.970
current lr 1.00000e-03
Grad=  tensor(0.1299, device='cuda:0')
Epoch: [286][0/391]	Time 0.254 (0.254)	Data 0.131 (0.131)	Loss 0.1219 (0.1219) ([0.003]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [286][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1289 (0.1274) ([0.010]+[0.119])	Prec@1 100.000 (99.907)
Epoch: [286][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1393 (0.1277) ([0.020]+[0.119])	Prec@1 99.219 (99.872)
Epoch: [286][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1317 (0.1275) ([0.013]+[0.119])	Prec@1 99.219 (99.891)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.2536 (0.2536) ([0.135]+[0.119])	Prec@1 93.750 (93.750)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.4792, device='cuda:0')
Epoch: [287][0/391]	Time 0.261 (0.261)	Data 0.137 (0.137)	Loss 0.1241 (0.1241) ([0.005]+[0.119])	Prec@1 100.000 (100.000)
Epoch: [287][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1266 (0.1268) ([0.008]+[0.119])	Prec@1 100.000 (99.892)
Epoch: [287][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1275 (0.1273) ([0.009]+[0.119])	Prec@1 100.000 (99.864)
Epoch: [287][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1510 (0.1271) ([0.032]+[0.119])	Prec@1 99.219 (99.881)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.2590 (0.2590) ([0.141]+[0.118])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-03
Grad=  tensor(0.1321, device='cuda:0')
Epoch: [288][0/391]	Time 0.259 (0.259)	Data 0.135 (0.135)	Loss 0.1203 (0.1203) ([0.002]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [288][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1213 (0.1264) ([0.003]+[0.118])	Prec@1 100.000 (99.892)
Epoch: [288][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1245 (0.1263) ([0.006]+[0.118])	Prec@1 100.000 (99.895)
Epoch: [288][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1303 (0.1262) ([0.012]+[0.118])	Prec@1 99.219 (99.907)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2392 (0.2392) ([0.121]+[0.118])	Prec@1 95.312 (95.312)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.2685, device='cuda:0')
Epoch: [289][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.1222 (0.1222) ([0.004]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [289][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1255 (0.1252) ([0.008]+[0.118])	Prec@1 100.000 (99.915)
Epoch: [289][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1242 (0.1255) ([0.006]+[0.118])	Prec@1 100.000 (99.918)
Epoch: [289][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1279 (0.1258) ([0.010]+[0.118])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2350 (0.2350) ([0.117]+[0.118])	Prec@1 95.312 (95.312)
 * Prec@1 93.850
current lr 1.00000e-03
Grad=  tensor(0.1663, device='cuda:0')
Epoch: [290][0/391]	Time 0.256 (0.256)	Data 0.132 (0.132)	Loss 0.1216 (0.1216) ([0.004]+[0.118])	Prec@1 100.000 (100.000)
Epoch: [290][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1261 (0.1247) ([0.009]+[0.117])	Prec@1 100.000 (99.930)
Epoch: [290][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1342 (0.1246) ([0.017]+[0.117])	Prec@1 99.219 (99.942)
Epoch: [290][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1207 (0.1246) ([0.003]+[0.117])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2604 (0.2604) ([0.143]+[0.117])	Prec@1 94.531 (94.531)
 * Prec@1 93.670
current lr 1.00000e-03
Grad=  tensor(0.1418, device='cuda:0')
Epoch: [291][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.1200 (0.1200) ([0.003]+[0.117])	Prec@1 100.000 (100.000)
Epoch: [291][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1230 (0.1248) ([0.006]+[0.117])	Prec@1 100.000 (99.884)
Epoch: [291][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1308 (0.1242) ([0.014]+[0.117])	Prec@1 99.219 (99.911)
Epoch: [291][300/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1204 (0.1243) ([0.004]+[0.117])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.163 (0.163)	Loss 0.2572 (0.2572) ([0.140]+[0.117])	Prec@1 94.531 (94.531)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(5.4394, device='cuda:0')
Epoch: [292][0/391]	Time 0.257 (0.257)	Data 0.134 (0.134)	Loss 0.1449 (0.1449) ([0.028]+[0.117])	Prec@1 99.219 (99.219)
Epoch: [292][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1283 (0.1241) ([0.012]+[0.117])	Prec@1 100.000 (99.923)
Epoch: [292][200/391]	Time 0.113 (0.113)	Data 0.000 (0.001)	Loss 0.1203 (0.1239) ([0.004]+[0.117])	Prec@1 100.000 (99.918)
Epoch: [292][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1223 (0.1240) ([0.006]+[0.116])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.167 (0.167)	Loss 0.2595 (0.2595) ([0.143]+[0.116])	Prec@1 93.750 (93.750)
 * Prec@1 93.930
current lr 1.00000e-03
Grad=  tensor(0.2036, device='cuda:0')
Epoch: [293][0/391]	Time 0.256 (0.256)	Data 0.133 (0.133)	Loss 0.1201 (0.1201) ([0.004]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [293][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1291 (0.1237) ([0.013]+[0.116])	Prec@1 100.000 (99.892)
Epoch: [293][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1262 (0.1234) ([0.010]+[0.116])	Prec@1 100.000 (99.899)
Epoch: [293][300/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1316 (0.1233) ([0.016]+[0.116])	Prec@1 99.219 (99.899)
Test: [0/79]	Time 0.169 (0.169)	Loss 0.2294 (0.2294) ([0.113]+[0.116])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-03
Grad=  tensor(0.7304, device='cuda:0')
Epoch: [294][0/391]	Time 0.255 (0.255)	Data 0.132 (0.132)	Loss 0.1216 (0.1216) ([0.006]+[0.116])	Prec@1 100.000 (100.000)
Epoch: [294][100/391]	Time 0.112 (0.114)	Data 0.000 (0.001)	Loss 0.1210 (0.1225) ([0.005]+[0.116])	Prec@1 100.000 (99.946)
Epoch: [294][200/391]	Time 0.112 (0.113)	Data 0.000 (0.001)	Loss 0.1192 (0.1227) ([0.004]+[0.116])	Prec@1 100.000 (99.934)
Epoch: [294][300/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1216 (0.1230) ([0.006]+[0.116])	Prec@1 100.000 (99.917)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2293 (0.2293) ([0.114]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.1855, device='cuda:0')
Epoch: [295][0/391]	Time 0.260 (0.260)	Data 0.139 (0.139)	Loss 0.1194 (0.1194) ([0.004]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [295][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1217 (0.1221) ([0.006]+[0.115])	Prec@1 100.000 (99.954)
Epoch: [295][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1192 (0.1223) ([0.004]+[0.115])	Prec@1 100.000 (99.938)
Epoch: [295][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1173 (0.1223) ([0.002]+[0.115])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.165 (0.165)	Loss 0.2561 (0.2561) ([0.141]+[0.115])	Prec@1 95.312 (95.312)
 * Prec@1 93.810
current lr 1.00000e-03
Grad=  tensor(0.2961, device='cuda:0')
Epoch: [296][0/391]	Time 0.257 (0.257)	Data 0.137 (0.137)	Loss 0.1196 (0.1196) ([0.005]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [296][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1299 (0.1221) ([0.015]+[0.115])	Prec@1 100.000 (99.938)
Epoch: [296][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1184 (0.1217) ([0.004]+[0.115])	Prec@1 100.000 (99.938)
Epoch: [296][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1299 (0.1215) ([0.015]+[0.115])	Prec@1 99.219 (99.938)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2584 (0.2584) ([0.144]+[0.115])	Prec@1 94.531 (94.531)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(0.8396, device='cuda:0')
Epoch: [297][0/391]	Time 0.257 (0.257)	Data 0.135 (0.135)	Loss 0.1268 (0.1268) ([0.012]+[0.115])	Prec@1 100.000 (100.000)
Epoch: [297][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1298 (0.1217) ([0.015]+[0.115])	Prec@1 99.219 (99.915)
Epoch: [297][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1198 (0.1215) ([0.005]+[0.114])	Prec@1 100.000 (99.914)
Epoch: [297][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1182 (0.1216) ([0.004]+[0.114])	Prec@1 100.000 (99.907)
Test: [0/79]	Time 0.166 (0.166)	Loss 0.2618 (0.2618) ([0.148]+[0.114])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.1772, device='cuda:0')
Epoch: [298][0/391]	Time 0.258 (0.258)	Data 0.137 (0.137)	Loss 0.1180 (0.1180) ([0.004]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [298][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1196 (0.1204) ([0.006]+[0.114])	Prec@1 100.000 (99.954)
Epoch: [298][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1293 (0.1216) ([0.015]+[0.114])	Prec@1 99.219 (99.903)
Epoch: [298][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1159 (0.1213) ([0.002]+[0.114])	Prec@1 100.000 (99.914)
Test: [0/79]	Time 0.172 (0.172)	Loss 0.2618 (0.2618) ([0.148]+[0.114])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.3508, device='cuda:0')
Epoch: [299][0/391]	Time 0.262 (0.262)	Data 0.140 (0.140)	Loss 0.1191 (0.1191) ([0.005]+[0.114])	Prec@1 100.000 (100.000)
Epoch: [299][100/391]	Time 0.110 (0.112)	Data 0.000 (0.001)	Loss 0.1206 (0.1214) ([0.007]+[0.114])	Prec@1 100.000 (99.884)
Epoch: [299][200/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1233 (0.1209) ([0.010]+[0.114])	Prec@1 100.000 (99.907)
Epoch: [299][300/391]	Time 0.110 (0.111)	Data 0.000 (0.001)	Loss 0.1187 (0.1208) ([0.005]+[0.113])	Prec@1 100.000 (99.901)
Test: [0/79]	Time 0.164 (0.164)	Loss 0.2490 (0.2490) ([0.136]+[0.113])	Prec@1 94.531 (94.531)
 * Prec@1 93.830

 Elapsed time for training  3:56:21.497055

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.9629629850387573, 0.7407407760620117, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.8888888955116272, 0.9259259104728699, 0.8888888955116272, 0.0, 0.0, 0.9629629850387573, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666865348816, 0.0, 0.9259259104728699, 0.0, 0.0, 0.9629629850387573, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.984375, 0.984375, 0.984375, 0.984375, 0.34375, 0.984375, 0.96875, 0.0, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.96875, 0.984375, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.984375, 0.0, 0.96875, 0.984375, 0.0]

 sparsity of   [0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.9965277910232544, 0.9982638955116272, 0.9982638955116272, 0.0]

 sparsity of   [0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.953125, 0.71875, 0.0, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.96875, 0.0, 0.71875, 0.96875, 0.0, 0.0, 0.71875, 0.984375, 0.953125, 0.96875, 0.0, 0.265625, 0.0, 0.0, 0.0, 0.71875, 0.0, 0.71875, 0.984375, 0.0, 0.71875, 0.0, 0.71875, 0.96875, 0.0, 0.96875, 0.71875, 0.96875, 0.71875, 0.0, 0.71875, 0.96875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71875, 0.71875, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.71875, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.71875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.71875, 0.71875, 0.96875, 0.71875, 0.71875, 0.71875, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.71875, 0.71875, 0.953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71875, 0.0, 0.96875, 0.984375, 0.984375, 0.984375, 0.0, 0.96875, 0.96875, 0.96875, 0.71875, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.859375, 0.0, 0.0, 0.71875, 0.71875, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.0, 0.96875, 0.71875, 0.71875, 0.96875, 0.96875, 0.71875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.71875, 0.0, 0.9375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.953125, 0.96875, 0.0, 0.9375, 0.0, 0.0, 0.96875, 0.71875, 0.0, 0.71875, 0.71875, 0.96875, 0.0, 0.96875, 0.0, 0.71875, 0.984375, 0.0, 0.0, 0.96875, 0.0, 0.96875, 0.96875, 0.0, 0.71875, 0.0, 0.71875, 0.96875, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.96875, 0.96875, 0.96875, 0.71875, 0.0, 0.0, 0.71875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.953125, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.71875, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.96875, 0.0]

 sparsity of   [0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.953125, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.34375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.953125, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.984375, 0.984375, 0.96875, 0.984375, 0.34375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.96875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0]

 sparsity of   [0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.3515625, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.3515625, 0.3515625, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.3515625]

 sparsity of   [0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9965277910232544, 0.484375, 0.9965277910232544, 0.9982638955116272, 0.9982638955116272, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9982638955116272, 0.6215277910232544, 0.9965277910232544, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9965277910232544, 0.0, 0.484375, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9965277910232544, 0.9965277910232544]

 sparsity of   [0.0, 0.96875, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.96875, 0.0, 0.578125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.96875, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.96875, 0.578125, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.96875, 0.578125, 0.96875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.578125, 0.578125, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.578125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.96875, 0.0, 0.953125, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.984375, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.578125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.96875, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.96875, 0.0, 0.96875, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.96875, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.0, 0.578125, 0.96875, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1640625, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.08203125, 0.0, 0.1328125, 0.0, 0.0, 0.9921875, 0.6328125, 0.0, 0.98828125, 0.0, 0.0, 0.9921875, 0.15234375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15234375, 0.0, 0.13671875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.37890625, 0.03125, 0.16015625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78515625, 0.9921875, 0.0390625, 0.0, 0.02734375, 0.08984375, 0.234375, 0.99609375, 0.0, 0.0]

 sparsity of   [0.0572916679084301, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.4895833432674408, 0.265625, 0.0, 0.0, 0.0, 0.220486119389534, 0.9965277910232544, 0.0, 0.0, 0.0347222238779068, 0.071180559694767, 0.9965277910232544, 0.9965277910232544, 0.0, 0.0538194440305233, 0.0729166641831398, 0.0, 0.0, 0.0607638880610466, 0.0, 0.0, 0.0, 0.9965277910232544, 0.0, 0.0, 0.0, 0.9947916865348816, 0.9965277910232544, 0.0, 0.0, 0.9965277910232544, 0.102430559694767, 0.8767361044883728, 0.0, 0.0, 0.0763888880610466, 0.9965277910232544, 0.0520833320915699, 0.086805559694767, 0.0, 0.0, 0.0, 0.8697916865348816, 0.0, 0.9253472089767456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.390625, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.96875, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.390625, 0.390625, 0.390625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.390625, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.390625, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.96875, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.390625, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 0.99609375, 0.0, 0.0078125, 0.0078125, 0.0078125, 0.0, 0.0, 0.0078125, 0.9921875, 0.9921875, 0.0078125, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0078125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.51953125, 0.0, 0.0078125, 0.0, 0.0, 0.0078125, 0.0, 0.46875, 0.0, 0.0078125, 0.0, 0.9921875, 0.9921875, 0.0078125, 0.99609375, 0.9921875, 0.00390625, 0.0, 0.0, 0.0078125, 0.9921875, 0.0, 0.0078125, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.421875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0078125, 0.4765625, 0.00390625, 0.0, 0.0, 0.98828125, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0078125, 0.9921875, 0.0078125, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0078125, 0.0, 0.9921875, 0.0, 0.0, 0.1171875, 0.9921875, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875]

 sparsity of   [0.0, 0.0, 0.0, 0.2378472238779068, 0.094618059694767, 0.9973958134651184, 0.2248263955116272, 0.0703125, 0.0503472238779068, 0.0, 0.1215277761220932, 0.0911458358168602, 0.796875, 0.0434027798473835, 0.0, 0.078125, 0.071180559694767, 0.0, 0.5190972089767456, 0.0321180559694767, 0.3046875, 0.126736119389534, 0.9982638955116272, 0.0, 0.9973958134651184, 0.2586805522441864, 0.1336805522441864, 0.0, 0.9982638955116272, 0.0, 0.9973958134651184, 0.0546875, 0.2239583283662796, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.1588541716337204, 0.078993059694767, 0.0, 0.0, 0.9435763955116272, 0.0, 0.1232638880610466, 0.1831597238779068, 0.4939236044883728, 0.0, 0.0, 0.1519097238779068, 0.0, 0.9982638955116272, 0.9973958134651184, 0.0911458358168602, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0677083358168602, 0.0, 0.999131977558136, 0.0651041641831398, 0.0512152798473835, 0.9982638955116272, 0.1276041716337204, 0.9982638955116272, 0.0824652761220932, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0763888880610466, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0807291641831398, 0.0494791679084301, 0.0833333358168602, 0.0, 0.0, 0.0, 0.0329861119389534, 0.0, 0.0364583320915699, 0.1215277761220932, 0.0, 0.0850694477558136, 0.063368059694767, 0.0, 0.9982638955116272, 0.2057291716337204, 0.0, 0.9973958134651184, 0.1284722238779068, 0.269097238779068, 0.0512152798473835, 0.0, 0.3046875, 0.0416666679084301, 0.2664930522441864, 0.0555555559694767, 0.0425347238779068, 0.0590277798473835, 0.9982638955116272, 0.0, 0.0, 0.03125, 0.0, 0.0902777761220932, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0763888880610466, 0.0, 0.0, 0.0746527761220932, 0.9982638955116272, 0.0451388880610466, 0.2074652761220932, 0.0, 0.3046875, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0564236119389534, 0.9973958134651184, 0.9982638955116272]

 sparsity of   [0.0, 0.984375, 0.9921875, 0.0, 0.9921875, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.6015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6015625, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.6015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.6015625, 0.9921875, 0.0, 0.0, 0.984375, 0.6015625, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.6015625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.6015625, 0.0, 0.6015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6015625, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.6015625, 0.9921875, 0.0, 0.9921875, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.484375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.6015625, 0.0, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.9765625, 0.984375, 0.6015625, 0.984375, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.9921875, 0.984375, 0.6015625, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.6015625, 0.0, 0.9921875, 0.984375, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.6015625, 0.9921875, 0.0, 0.984375, 0.984375, 0.6015625, 0.9921875, 0.0, 0.0, 0.0, 0.984375, 0.9765625, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.6015625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.6015625, 0.6015625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.6015625, 0.0, 0.6015625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.6015625, 0.0, 0.0, 0.6015625, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.6015625, 0.96875, 0.6015625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.6015625, 0.0, 0.9921875, 0.984375, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.6015625, 0.0, 0.6015625, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.984375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.6015625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9609375]

 sparsity of   [0.0, 0.9921875, 0.9921875, 0.0078125, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0078125, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0078125, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0078125, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0078125, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0078125, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0078125, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0078125, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.9921875, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0078125, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0078125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0078125, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0078125, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0078125, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0078125, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0078125, 0.0, 0.0, 0.99609375, 0.0, 0.0078125, 0.99609375]

 sparsity of   [0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.34765625, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.34765625, 0.0, 0.34765625, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.34765625, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.34765625, 0.34765625, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.34765625, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.34765625, 0.0, 0.0, 0.34765625, 0.34765625, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.34765625, 0.34765625, 0.0, 0.0, 0.0, 0.34765625, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.34765625, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.34765625]

 sparsity of   [0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.0, 0.9982638955116272, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.9982638955116272, 0.999131977558136, 0.9982638955116272, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.999131977558136, 0.0, 0.9982638955116272, 0.999131977558136, 0.0, 0.9982638955116272, 0.0, 0.999131977558136, 0.9982638955116272, 0.9982638955116272, 0.0, 0.9982638955116272, 0.999131977558136]

 sparsity of   [0.0, 0.0, 0.296875, 0.5625, 0.0, 0.0, 0.6796875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.4765625, 0.984375, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.9921875, 0.984375, 0.5625, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.9921875, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.5625, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.5625, 0.3203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5859375, 0.5625, 0.0, 0.0, 0.0, 0.984375, 0.5625, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.15625, 0.0, 0.5625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.9921875, 0.984375, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.5625, 0.0, 0.0, 0.9921875, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.984375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.515625, 0.0, 0.0, 0.5625, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.0, 0.46875, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.5625, 0.5625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.5625, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 0.984375, 0.984375, 0.984375, 0.0, 0.984375, 0.0, 0.5625, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.984375, 0.0, 0.9921875, 0.53125, 0.9921875, 0.0, 0.0078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.984375, 0.0, 0.9765625, 0.0, 0.0, 0.984375, 0.0, 0.5625, 0.9921875]

 sparsity of   [0.044921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.080078125, 0.99609375, 0.99609375, 0.19140625, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.11328125, 0.0, 0.99609375, 0.177734375, 0.916015625, 0.998046875, 0.99609375, 0.0, 0.0, 0.998046875, 0.994140625, 0.0, 0.087890625, 0.99609375, 0.0, 0.994140625, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.994140625, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.306640625, 0.998046875, 0.48046875, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.99609375, 0.12109375, 0.0, 0.021484375, 0.99609375, 0.0, 0.19140625, 0.37109375, 0.998046875, 0.837890625, 0.3203125, 0.0, 0.076171875, 0.0, 0.994140625, 0.99609375, 0.0, 0.111328125, 0.99609375, 0.119140625, 0.0, 0.99609375, 0.0, 0.99609375, 0.203125, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.443359375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.369140625, 0.0, 0.994140625, 0.0, 0.998046875, 0.189453125, 0.998046875, 0.0390625, 0.19140625, 0.99609375, 0.0, 0.173828125, 0.328125, 0.0]

 sparsity of   [0.0, 0.9982638955116272, 0.0, 0.0, 0.0329861119389534, 0.0486111119389534, 0.0894097238779068, 0.02690972201526165, 0.0598958320915699, 0.0460069440305233, 0.00434027798473835, 0.02951388992369175, 0.0, 0.02951388992369175, 0.0329861119389534, 0.9071180820465088, 0.3177083432674408, 0.0, 0.0, 0.0651041641831398, 0.0, 0.01909722201526165, 0.0381944440305233, 0.0, 0.0321180559694767, 0.0581597238779068, 0.7126736044883728, 0.9982638955116272, 0.0, 0.9982638955116272, 0.1085069477558136, 0.0, 0.9513888955116272, 0.999131977558136, 0.0026041667442768812, 0.0, 0.1032986119389534, 0.6762152910232544, 0.013020833022892475, 0.01128472201526165, 0.0, 0.803819477558136, 0.0928819477558136, 0.0364583320915699, 0.0, 0.0, 0.0338541679084301, 0.0, 0.086805559694767, 0.0, 0.015625, 0.0651041641831398, 0.0, 0.0, 0.0807291641831398, 0.0, 0.9982638955116272, 0.967881977558136, 0.0225694440305233, 0.01215277798473835, 0.02170138992369175, 0.0, 0.0225694440305233, 0.9166666865348816, 0.02604166604578495, 0.02604166604578495, 0.9973958134651184, 0.0, 0.0, 0.0494791679084301, 0.0, 0.0, 0.0598958320915699, 0.0607638880610466, 0.983506977558136, 0.0, 0.0, 0.0390625, 0.0, 0.0, 0.0364583320915699, 0.0564236119389534, 0.0, 0.9982638955116272, 0.0243055559694767, 0.0668402761220932, 0.9973958134651184, 0.0017361111240461469, 0.014756944961845875, 0.0, 0.0, 0.126736119389534, 0.9704861044883728, 0.0460069440305233, 0.0416666679084301, 0.0, 0.0, 0.1796875, 0.0, 0.9982638955116272, 0.8680555820465088, 0.0, 0.2135416716337204, 0.0, 0.0, 0.2777777910232544, 0.9973958134651184, 0.0017361111240461469, 0.1510416716337204, 0.046875, 0.0, 0.2638888955116272, 0.9765625, 0.0572916679084301, 0.0, 0.0, 0.0, 0.0, 0.0798611119389534, 0.014756944961845875, 0.02864583395421505, 0.0381944440305233, 0.0052083334885537624, 0.046875, 0.0, 0.1388888955116272, 0.9470486044883728, 0.0]

 sparsity of   [0.0, 0.0, 0.65625, 0.65625, 0.0, 0.0, 0.8046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9765625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.65625, 0.984375, 0.0, 0.0, 0.984375, 0.65625, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.65625, 0.0, 0.4296875, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.6328125, 0.0, 0.65625, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.65625, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.65625, 0.0, 0.0, 0.65625, 0.65625, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.6484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4765625, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.9921875, 0.7890625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.65625, 0.0, 0.65625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.65625, 0.9921875, 0.0, 0.8203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.65625, 0.0]

 sparsity of   [0.0, 0.05078125, 0.0, 0.0, 0.154296875, 0.0, 0.890625, 0.0, 0.0, 0.994140625, 0.0, 0.99609375, 0.994140625, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.091796875, 0.00390625, 0.0, 0.0, 0.0, 0.04296875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.953125, 0.57421875, 0.0, 0.173828125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.474609375, 0.068359375, 0.0703125, 0.99609375, 0.09375, 0.0, 0.044921875, 0.0, 0.0, 0.0, 0.05859375, 0.994140625, 0.0, 0.0, 0.998046875, 0.08984375, 0.0, 0.0703125, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.048828125, 0.0, 0.0, 0.0, 0.0, 0.0703125, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.04296875, 0.890625, 0.091796875, 0.0, 0.0, 0.99609375, 0.998046875, 0.0703125, 0.0, 0.076171875, 0.99609375, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0]

 sparsity of   [0.0, 0.9982638955116272, 0.1840277761220932, 0.9982638955116272, 0.0, 0.0546875, 0.0, 0.0520833320915699, 0.1388888955116272, 0.9982638955116272, 0.0980902761220932, 0.9982638955116272, 0.0303819440305233, 0.9982638955116272, 0.157986119389534, 0.0, 0.0373263880610466, 0.086805559694767, 0.1701388955116272, 0.9453125, 0.1953125, 0.0, 0.0, 0.0842013880610466, 0.0, 0.0, 0.0, 0.0234375, 0.0, 0.7986111044883728, 0.0, 0.0, 0.0364583320915699, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.8480902910232544, 0.0503472238779068, 0.0321180559694767, 0.0, 0.0546875, 0.8637152910232544, 0.0, 0.1041666641831398, 0.9982638955116272, 0.1284722238779068, 0.0, 0.1006944477558136, 0.0, 0.0, 0.02604166604578495, 0.0364583320915699, 0.046875, 0.8871527910232544, 0.0572916679084301, 0.0, 0.0, 0.0442708320915699, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.01909722201526165, 0.0, 0.0, 0.9982638955116272, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9982638955116272, 0.0, 0.0, 0.0, 0.0373263880610466, 0.9982638955116272, 0.9314236044883728, 0.9982638955116272, 0.0668402761220932, 0.0, 0.0, 0.9973958134651184, 0.9262152910232544, 0.0989583358168602, 0.0, 0.0, 0.0, 0.0390625, 0.999131977558136, 0.0, 0.0685763880610466, 0.1163194477558136, 0.9982638955116272, 0.0720486119389534, 0.0, 0.9982638955116272, 0.0, 0.1901041716337204, 0.071180559694767, 0.9965277910232544, 0.999131977558136, 0.0, 0.0, 0.1354166716337204, 0.0, 0.140625, 0.1059027761220932, 0.0, 0.0, 0.0, 0.0902777761220932, 0.9982638955116272, 0.0, 0.0, 0.0772569477558136, 0.9618055820465088]

 sparsity of   [0.0, 0.5390625, 0.5390625, 0.0, 0.5390625, 0.0, 0.984375, 0.5390625, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.9765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.5390625, 0.9921875, 0.0, 0.0, 0.984375, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.53125, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.5390625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.5390625, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.5390625, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.984375, 0.0, 0.984375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.0, 0.0, 0.0, 0.0, 0.5390625, 0.5390625]

 sparsity of   [0.0, 0.0, 0.0, 0.998046875, 0.02734375, 0.998046875, 0.0, 0.0, 0.02734375, 0.998046875, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.0, 0.99609375, 0.02734375, 0.0, 0.998046875, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.02734375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.02734375, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.99609375, 0.02734375, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.02734375, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.99609375, 0.02734375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.02734375, 0.99609375, 0.0, 0.02734375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.02734375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.02734375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.02734375, 0.02734375, 0.99609375, 0.0, 0.0, 0.02734375, 0.0, 0.02734375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.02734375, 0.0, 0.99609375, 0.998046875, 0.0, 0.0, 0.02734375, 0.99609375, 0.998046875, 0.99609375, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.02734375, 0.0, 0.02734375, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.0]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9982638955116272, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.3984375, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456]

 sparsity of   [0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.7421875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.7421875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.7421875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.94921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9453125, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7421875, 0.9921875, 0.0, 0.0, 0.9921875, 0.7421875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.7421875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.7421875, 0.9921875, 0.9921875, 0.7421875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.7421875, 0.9921875, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.7421875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.7421875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.7421875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.7421875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.7421875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.7421875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.7421875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.7421875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.7421875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.73828125, 0.9921875, 0.0, 0.0, 0.98828125, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.7421875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.7421875, 0.9921875, 0.0, 0.98828125, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.7421875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.7421875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.7421875, 0.99609375, 0.9921875, 0.0, 0.7421875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.7421875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.7421875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.7421875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.7421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.7421875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.7421875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875]

 sparsity of   [0.19140625, 0.0, 0.02734375, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.021484375, 0.0, 0.998046875, 0.0, 0.99609375, 0.8203125, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0078125, 0.0, 0.998046875, 0.998046875, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.02734375, 0.0, 0.068359375, 0.994140625, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.998046875, 0.0234375, 0.0, 0.998046875, 0.99609375, 0.994140625, 0.0, 0.0, 0.998046875, 0.0, 0.99609375, 0.994140625, 0.99609375, 0.02734375, 0.435546875, 0.99609375, 0.998046875, 0.091796875, 0.39453125, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.025390625, 0.998046875, 0.998046875, 0.994140625, 0.0, 0.998046875, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.0, 0.09765625, 0.99609375, 0.998046875, 0.1171875, 0.998046875, 0.0, 0.0, 0.99609375, 0.9921875, 0.998046875, 0.0, 0.08984375, 0.99609375, 0.99609375, 0.998046875, 0.41015625, 0.998046875, 0.220703125, 0.0, 0.458984375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.22265625, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.482421875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.0234375, 0.99609375, 0.998046875, 0.998046875, 0.837890625, 0.0, 0.02734375, 0.994140625, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.025390625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.134765625, 0.998046875, 0.99609375, 0.0, 0.119140625, 0.99609375, 0.0, 0.02734375, 0.0, 0.0, 0.025390625, 0.0, 0.998046875, 0.99609375, 0.111328125, 0.0, 0.998046875, 0.0, 0.0, 0.5625, 0.021484375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.0, 0.140625, 0.998046875, 0.0, 0.0, 0.99609375, 0.109375, 0.99609375, 0.0, 0.998046875, 0.12109375, 0.994140625, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.994140625, 0.22265625, 0.0234375, 0.99609375, 0.0, 0.0, 0.0, 0.998046875, 0.958984375, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.994140625, 0.0, 0.99609375, 0.994140625, 0.017578125, 0.998046875, 0.45703125, 0.02734375, 0.0, 0.994140625, 0.0, 0.0, 0.0, 0.015625, 0.109375, 0.0, 0.0, 0.025390625, 0.0, 0.0, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.11328125, 0.998046875, 0.08203125, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.072265625, 0.37109375, 0.0, 0.015625, 0.0, 0.53125, 0.99609375, 0.99609375, 0.021484375, 0.10546875, 0.9921875, 0.0546875, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.994140625, 0.02734375, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.99609375, 0.0, 0.013671875, 0.0, 0.994140625, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.994140625, 0.021484375, 0.01953125, 0.0, 0.02734375, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.990234375, 0.162109375, 0.998046875, 0.4375, 0.99609375, 0.99609375, 0.990234375, 0.99609375, 0.9921875, 0.998046875, 0.18359375, 0.998046875, 0.99609375, 0.998046875, 0.0, 0.0, 0.99609375, 0.02734375, 0.017578125, 0.0, 0.994140625, 0.998046875, 0.99609375, 0.552734375, 0.0, 0.99609375, 0.998046875, 0.994140625, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.99609375, 0.8203125, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.994140625, 0.998046875, 0.0, 0.484375, 0.220703125, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.013671875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.994140625, 0.068359375, 0.998046875, 0.99609375, 0.0, 0.021484375, 0.998046875, 0.998046875, 0.99609375, 0.0078125, 0.99609375, 0.384765625, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.017578125, 0.99609375, 0.0, 0.853515625, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.150390625, 0.0, 0.998046875, 0.0, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.02734375, 0.99609375, 0.99609375, 0.0, 0.0, 0.005859375, 0.072265625, 0.02734375, 0.357421875, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.994140625, 0.0, 0.0, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.0, 0.02734375, 0.998046875, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.111328125, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.515625, 0.0, 0.150390625, 0.994140625, 0.0, 0.99609375, 0.392578125, 0.990234375, 0.0, 0.99609375, 0.99609375, 0.0859375, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.373046875, 0.99609375, 0.0, 0.99609375, 0.994140625, 0.0, 0.998046875, 0.0, 0.9921875, 0.0234375, 0.99609375, 0.02734375, 0.994140625, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.404296875, 0.02734375, 0.99609375, 0.009765625, 0.99609375, 0.99609375, 0.99609375, 0.09375, 0.0, 0.998046875, 0.421875, 0.02734375, 0.99609375, 0.0, 0.0, 0.994140625, 0.994140625, 0.09375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.11328125, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.4140625, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.99609375, 0.0, 0.998046875, 0.0, 0.994140625, 0.0, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.55078125, 0.99609375, 0.99609375, 0.380859375, 0.0, 0.583984375, 0.998046875, 0.123046875, 0.998046875, 0.0, 0.4453125, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.998046875, 0.861328125, 0.990234375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.02734375, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.001953125, 0.99609375, 0.455078125, 0.0, 0.419921875, 0.02734375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12109375, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.994140625, 0.017578125, 0.998046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.169921875, 0.99609375, 0.998046875, 0.998046875, 0.083984375, 0.0, 0.01171875, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.015625, 0.99609375, 0.0, 0.0, 0.99609375, 0.09375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.02734375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.134765625, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.07421875, 0.1328125, 0.0, 0.0, 0.0, 0.99609375, 0.02734375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.994140625, 0.998046875, 0.99609375, 0.013671875, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.087890625, 0.99609375, 0.0, 0.99609375, 0.02734375, 0.990234375, 0.998046875, 0.0, 0.02734375, 0.0, 0.0, 0.998046875, 0.0, 0.119140625, 0.99609375, 0.998046875, 0.03515625, 0.998046875, 0.373046875, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.1484375, 0.359375, 0.99609375, 0.126953125, 0.857421875, 0.0, 0.021484375, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.005859375, 0.99609375, 0.998046875, 0.998046875, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.5234375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.068359375, 0.998046875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.0, 0.169921875, 0.0, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0, 0.01953125, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.99609375, 0.140625, 0.99609375, 0.0, 0.08984375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.99609375, 0.0, 0.0, 0.0, 0.009765625, 0.99609375, 0.466796875, 0.021484375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.087890625, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.998046875, 0.0234375, 0.0, 0.998046875, 0.158203125, 0.0, 0.998046875, 0.0, 0.994140625, 0.99609375, 0.21875, 0.99609375, 0.0, 0.0, 0.99609375, 0.998046875, 0.998046875, 0.0, 0.0, 0.02734375, 0.025390625, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.302734375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.005859375, 0.99609375, 0.017578125, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.1171875, 0.0, 0.0, 0.1796875, 0.0, 0.99609375, 0.109375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.099609375, 0.998046875, 0.998046875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.0, 0.025390625, 0.994140625, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.212890625, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.998046875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.2734375, 0.994140625, 0.802734375, 0.994140625, 0.994140625, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.02734375, 0.99609375, 0.99609375, 0.080078125, 0.99609375, 0.998046875, 0.99609375, 0.09765625, 0.99609375, 0.146484375, 0.01953125, 0.998046875, 0.83203125, 0.0, 0.0, 0.99609375]

 sparsity of   [0.998046875, 0.234375, 0.9970703125, 0.587890625, 0.9970703125, 0.9990234375, 0.587890625, 0.0, 0.9990234375, 0.5869140625, 0.0, 0.4365234375, 0.0712890625, 0.9990234375, 0.703125, 0.0, 0.0, 0.5869140625, 0.998046875, 0.0, 0.998046875, 0.0, 0.103515625, 0.0, 0.0, 0.8291015625, 0.998046875, 0.4775390625, 0.998046875, 0.564453125, 0.9970703125, 0.2373046875, 0.998046875, 0.0, 0.9990234375, 0.9990234375, 0.1806640625, 0.0, 0.587890625, 0.0, 0.56640625, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.587890625, 0.998046875, 0.9970703125, 0.994140625, 0.0, 0.0, 0.9970703125, 0.998046875, 0.5693359375, 0.9970703125, 0.998046875, 0.216796875, 0.998046875, 0.998046875, 0.3720703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.5869140625, 0.421875, 0.8740234375, 0.103515625, 0.0, 0.5869140625, 0.0, 0.9990234375, 0.5869140625, 0.0, 0.0, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.587890625, 0.564453125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.9970703125, 0.998046875, 0.9990234375, 0.587890625, 0.17578125, 0.9990234375, 0.5869140625, 0.3916015625, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.08203125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.5693359375, 0.998046875, 0.0, 0.5048828125, 0.9990234375, 0.587890625, 0.509765625, 0.0, 0.0, 0.0, 0.9970703125, 0.2587890625, 0.587890625, 0.0, 0.0, 0.998046875, 0.0, 0.587890625, 0.998046875, 0.236328125, 0.0, 0.998046875, 0.107421875, 0.0, 0.0966796875, 0.998046875, 0.0, 0.998046875, 0.9404296875, 0.0, 0.9990234375, 0.998046875, 0.0, 0.9990234375, 0.3642578125, 0.998046875, 0.9970703125, 0.9970703125, 0.5869140625, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.15234375, 0.9990234375, 0.587890625, 0.998046875, 0.9990234375, 0.998046875, 0.564453125, 0.8125, 0.998046875, 0.998046875, 0.5869140625, 0.9970703125, 0.9970703125, 0.587890625, 0.998046875, 0.998046875, 0.0, 0.0, 0.998046875, 0.587890625, 0.4111328125, 0.484375, 0.9970703125, 0.0, 0.0, 0.8662109375, 0.998046875, 0.998046875, 0.587890625, 0.587890625, 0.998046875, 0.0, 0.998046875, 0.2763671875, 0.0, 0.0, 0.998046875, 0.587890625, 0.5869140625, 0.0, 0.9970703125, 0.998046875, 0.1328125, 0.998046875, 0.0927734375, 0.998046875, 0.87109375, 0.40625, 0.998046875, 0.3984375, 0.0, 0.0, 0.0, 0.125, 0.5869140625, 0.134765625, 0.0, 0.587890625, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.587890625, 0.5859375, 0.5712890625, 0.998046875, 0.611328125, 0.998046875, 0.5693359375, 0.998046875, 0.998046875, 0.9990234375, 0.0, 0.998046875, 0.0, 0.4072265625, 0.9970703125, 0.5859375, 0.0, 0.0, 0.0, 0.998046875, 0.0, 0.0, 0.5869140625, 0.998046875, 0.0, 0.998046875, 0.0, 0.203125, 0.08984375, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.3173828125, 0.861328125, 0.0, 0.998046875, 0.0, 0.998046875, 0.1044921875, 0.998046875, 0.998046875]

 sparsity of   [0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.9995659589767456, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9995659589767456, 0.0, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9986979365348816, 0.999131977558136, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.0, 0.9986979365348816, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.9986979365348816, 0.0, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.0, 0.0, 0.0, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.9986979365348816, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136, 0.999131977558136, 0.0, 0.0, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.9995659589767456, 0.999131977558136, 0.999131977558136, 0.999131977558136, 0.0, 0.999131977558136]

 sparsity of   [0.99609375, 0.0, 0.75, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.75, 0.0, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.75, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.75, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.75, 0.9921875, 0.99609375, 0.0, 0.75, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.75, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.75, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.75, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.75, 0.9921875, 0.9921875, 0.0, 0.75, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.75, 0.75, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.75, 0.0, 0.75, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.75, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.75, 0.75, 0.0, 0.0, 0.75, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.75, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.75, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.75, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.75, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.75, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.75, 0.75, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.75, 0.75, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.75, 0.75, 0.0, 0.9921875, 0.0, 0.99609375, 0.75, 0.0, 0.9921875, 0.9921875, 0.75, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.75, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.9296875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.75, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.75, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.75, 0.99609375, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.99609375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.75, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.75, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.75, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.75, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.75, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.75, 0.99609375, 0.75, 0.99609375, 0.75, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.8828125, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.75, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.75, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.75, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.75, 0.9921875, 0.9296875, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9296875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.75, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.75, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.75, 0.0, 0.75, 0.0, 0.99609375, 0.0, 0.9921875, 0.75, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.75, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.96875, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.75, 0.0, 0.75, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.99609375, 0.0, 0.0, 0.9921875, 0.75, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.99609375, 0.75, 0.0, 0.0, 0.75, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.0, 0.99609375, 0.75, 0.0, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.75, 0.9921875, 0.99609375, 0.99609375, 0.75, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.75, 0.9921875, 0.75, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.75, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.75, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.75, 0.9921875, 0.99609375, 0.75, 0.9921875, 0.9921875, 0.0, 0.75, 0.99609375, 0.0, 0.0, 0.8359375, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.75, 0.9921875, 0.75, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.0, 0.75, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.75, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.75, 0.75, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.75, 0.9921875, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.9921875, 0.75, 0.0, 0.0, 0.9921875, 0.75, 0.0, 0.0, 0.99609375, 0.75, 0.75, 0.99609375, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.99609375, 0.75, 0.9921875, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.99609375, 0.9921875, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.75, 0.75, 0.0, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.99609375, 0.75, 0.0, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.0, 0.75, 0.9921875, 0.9921875, 0.9921875, 0.75, 0.9921875, 0.99609375, 0.99609375, 0.0, 0.99609375, 0.0, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.75, 0.9921875, 0.99609375, 0.0, 0.0, 0.99609375, 0.99609375, 0.75, 0.0, 0.9921875, 0.99609375, 0.99609375, 0.99609375, 0.9921875, 0.75, 0.99609375, 0.9921875, 0.0, 0.0, 0.9921875]

 sparsity of   [0.998046875, 0.0, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.998046875, 0.0, 0.998046875, 0.9970703125, 0.36328125, 0.9990234375, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.0, 0.99609375, 0.0, 0.998046875, 0.0, 0.0, 0.9990234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.0, 0.998046875, 0.0, 0.541015625, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.0, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.998046875, 0.3505859375, 0.9990234375, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.0, 0.109375, 0.0, 0.9990234375, 0.998046875, 0.99609375, 0.0, 0.0, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.0, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.0, 0.998046875, 0.0, 0.080078125, 0.9970703125, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.5419921875, 0.0, 0.998046875, 0.9990234375, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.0, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.9990234375, 0.998046875, 0.0, 0.0, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.0, 0.998046875, 0.0, 0.0, 0.9990234375, 0.998046875, 0.5126953125, 0.998046875, 0.0, 0.998046875, 0.9990234375, 0.0, 0.0, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.359375, 0.998046875, 0.0, 0.0, 0.998046875, 0.9990234375, 0.0, 0.998046875, 0.5166015625, 0.9970703125, 0.9990234375, 0.5419921875, 0.998046875, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.5419921875, 0.998046875, 0.5185546875, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.0, 0.9990234375, 0.998046875, 0.5419921875, 0.1650390625, 0.654296875, 0.9990234375, 0.0, 0.0, 0.9990234375, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.998046875, 0.5419921875, 0.998046875, 0.0, 0.998046875, 0.0, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.318359375, 0.9970703125, 0.9990234375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9990234375, 0.998046875, 0.0, 0.0, 0.9990234375, 0.0, 0.998046875, 0.998046875, 0.0, 0.9990234375, 0.9970703125, 0.998046875, 0.5419921875, 0.0, 0.0, 0.0, 0.146484375, 0.0, 0.998046875, 0.51171875, 0.9990234375, 0.19140625, 0.0, 0.9990234375, 0.9990234375, 0.998046875, 0.5419921875, 0.0966796875, 0.998046875, 0.9990234375, 0.9970703125, 0.998046875, 0.0, 0.9970703125, 0.998046875, 0.0, 0.998046875, 0.0, 0.9990234375, 0.0, 0.0, 0.9990234375, 0.0, 0.076171875]

 sparsity of   [0.8359375, 0.8728298544883728, 0.8806423544883728, 0.0, 0.0364583320915699, 0.03125, 0.0338541679084301, 0.7009548544883728, 0.0360243059694767, 0.3736979067325592, 0.0342881940305233, 0.0572916679084301, 0.0, 0.999131977558136, 0.0, 0.0941840261220932, 0.3038194477558136, 0.9730902910232544, 0.9986979365348816, 0.0, 0.0, 0.1866319477558136, 0.0, 0.9348958134651184, 0.0, 0.0, 0.0, 0.01779513992369175, 0.9652777910232544, 0.9982638955116272, 0.3949652910232544, 0.014322916977107525, 0.0, 0.0455729179084301, 0.1037326380610466, 0.0, 0.9427083134651184, 0.0407986119389534, 0.0069444444961845875, 0.02300347201526165, 0.078125, 0.0, 0.0, 0.823350727558136, 0.0, 0.0, 0.1011284738779068, 0.0381944440305233, 0.0377604179084301, 0.0798611119389534, 0.0, 0.0811631977558136, 0.0490451380610466, 0.999131977558136, 0.03515625, 0.9986979365348816, 0.9986979365348816, 0.0, 0.8337673544883728, 0.0243055559694767, 0.0, 0.0373263880610466, 0.0642361119389534, 0.010416666977107525, 0.3914930522441864, 0.3845486044883728, 0.0733506977558136, 0.999131977558136, 0.0, 0.094618059694767, 0.0290798619389534, 0.0, 0.02604166604578495, 0.9986979365348816, 0.2269965261220932, 0.8841145634651184, 0.0, 0.1019965261220932, 0.1111111119389534, 0.01692708395421505, 0.0555555559694767, 0.0802951380610466, 0.0889756977558136, 0.0525173619389534, 0.04296875, 0.8146701455116272, 0.0303819440305233, 0.9184027910232544, 0.013888888992369175, 0.0225694440305233, 0.999131977558136, 0.0, 0.0694444477558136, 0.0, 0.0407986119389534, 0.02300347201526165, 0.0863715261220932, 0.999131977558136, 0.0529513880610466, 0.0520833320915699, 0.02994791604578495, 0.0290798619389534, 0.999131977558136, 0.0681423619389534, 0.0321180559694767, 0.9431423544883728, 0.0347222238779068, 0.0546875, 0.0347222238779068, 0.009982638992369175, 0.9986979365348816, 0.0, 0.0342881940305233, 0.7816840410232544, 0.0, 0.01692708395421505, 0.9006076455116272, 0.0, 0.0, 0.0, 0.0, 0.009114583022892475, 0.999131977558136, 0.6896701455116272, 0.6796875, 0.9548611044883728, 0.0555555559694767, 0.0798611119389534, 0.0, 0.0, 0.1927083283662796, 0.0, 0.9114583134651184, 0.999131977558136, 0.0, 0.0598958320915699, 0.0551215298473835, 0.0, 0.02300347201526165, 0.0607638880610466, 0.0, 0.90625, 0.0, 0.094618059694767, 0.9535590410232544, 0.0, 0.0503472238779068, 0.0321180559694767, 0.0, 0.0321180559694767, 0.08984375, 0.0282118059694767, 0.6154513955116272, 0.0234375, 0.999131977558136, 0.7795138955116272, 0.02560763992369175, 0.9518229365348816, 0.0464409738779068, 0.0451388880610466, 0.0212673619389534, 0.9605034589767456, 0.0, 0.03081597201526165, 0.0503472238779068, 0.0, 0.0325520820915699, 0.02604166604578495, 0.9075520634651184, 0.9214409589767456, 0.0, 0.9552951455116272, 0.9995659589767456, 0.0, 0.9986979365348816, 0.0, 0.02387152798473835, 0.0, 0.1124131977558136, 0.0, 0.0, 0.0386284738779068, 0.9995659589767456, 0.01822916604578495, 0.228298619389534, 0.0364583320915699, 0.1940104216337204, 0.0, 0.9184027910232544, 0.0, 0.0, 0.03081597201526165, 0.0, 0.0703125, 0.01822916604578495, 0.0855034738779068, 0.9353298544883728, 0.0, 0.0, 0.1176215261220932, 0.0, 0.0572916679084301, 0.9079861044883728, 0.0998263880610466, 0.0842013880610466, 0.2183159738779068, 0.0, 0.0707465261220932, 0.9995659589767456, 0.999131977558136, 0.0, 0.0316840298473835, 0.0, 0.7673611044883728, 0.1414930522441864, 0.01779513992369175, 0.0455729179084301, 0.999131977558136, 0.0, 0.4249131977558136, 0.01779513992369175, 0.2521701455116272, 0.0360243059694767, 0.0416666679084301, 0.0, 0.0802951380610466, 0.014756944961845875, 0.01128472201526165, 0.02560763992369175, 0.02560763992369175, 0.0837673619389534, 0.0, 0.0850694477558136, 0.9986979365348816, 0.0, 0.0421006940305233, 0.0, 0.0399305559694767, 0.999131977558136, 0.02560763992369175, 0.02083333395421505, 0.999131977558136, 0.02734375, 0.1059027761220932, 0.063368059694767, 0.0282118059694767, 0.0486111119389534, 0.0, 0.0, 0.0, 0.0, 0.9657118320465088, 0.0, 0.0243055559694767, 0.9422743320465088, 0.999131977558136]

 sparsity of   [0.99609375, 0.734375, 0.734375, 0.0, 0.0, 0.99609375, 0.9921875, 0.1953125, 0.0, 0.0, 0.7109375, 0.9921875, 0.734375, 0.734375, 0.9921875, 0.0, 0.99609375, 0.71875, 0.0, 0.9921875, 0.734375, 0.9921875, 0.9921875, 0.0, 0.734375, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.8359375, 0.0, 0.9921875, 0.0, 0.0, 0.140625, 0.73046875, 0.0234375, 0.984375, 0.24609375, 0.734375, 0.0, 0.71484375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.734375, 0.0, 0.734375, 0.9921875, 0.99609375, 0.734375, 0.0, 0.734375, 0.9921875, 0.734375, 0.71484375, 0.0, 0.9921875, 0.6875, 0.9921875, 0.0, 0.0, 0.7265625, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.99609375, 0.9921875, 0.9921875, 0.0703125, 0.9921875, 0.9921875, 0.0, 0.0, 0.734375, 0.9921875, 0.34375, 0.734375, 0.9921875, 0.0, 0.0, 0.734375, 0.98828125, 0.7265625, 0.734375, 0.734375, 0.734375, 0.7265625, 0.0, 0.734375, 0.734375, 0.9921875, 0.99609375, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.734375, 0.9140625, 0.9921875, 0.98828125, 0.9921875, 0.734375, 0.0, 0.0, 0.9921875, 0.7578125, 0.11328125, 0.734375, 0.99609375, 0.9921875, 0.734375, 0.98828125, 0.9921875, 0.734375, 0.0, 0.0, 0.9921875, 0.734375, 0.34765625, 0.0, 0.0, 0.0, 0.99609375, 0.9921875, 0.9921875, 0.0, 0.99609375, 0.734375, 0.99609375, 0.84375, 0.99609375, 0.0, 0.0, 0.0, 0.74609375, 0.0, 0.08984375, 0.0, 0.72265625, 0.703125, 0.11328125, 0.9921875, 0.98828125, 0.734375, 0.99609375, 0.734375, 0.0, 0.98828125, 0.7265625, 0.0, 0.734375, 0.0, 0.98828125, 0.734375, 0.08984375, 0.734375, 0.7265625, 0.734375, 0.0, 0.734375, 0.9921875, 0.734375, 0.0, 0.734375, 0.7265625, 0.734375, 0.9453125, 0.734375, 0.734375, 0.9921875, 0.0, 0.9921875, 0.0, 0.72265625, 0.734375, 0.0, 0.98828125, 0.7421875, 0.734375, 0.734375, 0.734375, 0.0, 0.0, 0.734375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.7265625, 0.9921875, 0.9921875, 0.734375, 0.0, 0.9921875, 0.9921875, 0.734375, 0.0, 0.71875, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.71484375, 0.73046875, 0.7265625, 0.734375, 0.98828125, 0.9921875, 0.72265625, 0.99609375, 0.0, 0.0, 0.734375, 0.9921875, 0.734375, 0.9921875, 0.734375, 0.98828125, 0.0, 0.9921875, 0.734375, 0.6171875, 0.99609375, 0.734375, 0.98828125, 0.734375, 0.9921875, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.734375, 0.58984375, 0.9921875, 0.734375, 0.99609375, 0.0, 0.71875, 0.0, 0.1875, 0.73046875, 0.0, 0.703125, 0.734375, 0.0, 0.0, 0.734375, 0.9921875, 0.734375, 0.0, 0.72265625, 0.9921875, 0.734375, 0.0, 0.99609375, 0.0, 0.98828125, 0.60546875, 0.7265625, 0.9921875, 0.734375, 0.734375, 0.0, 0.0, 0.10546875, 0.734375, 0.99609375, 0.9921875, 0.03515625, 0.9921875, 0.9921875, 0.9921875, 0.09375, 0.0, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.7265625, 0.734375, 0.87890625, 0.73046875, 0.9921875, 0.234375, 0.7265625, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.734375, 0.390625, 0.99609375, 0.7265625, 0.73046875, 0.9921875, 0.734375, 0.734375, 0.9921875, 0.9921875, 0.734375, 0.734375, 0.734375, 0.0, 0.72265625, 0.0, 0.734375, 0.08203125, 0.734375, 0.0, 0.73046875, 0.9921875, 0.0, 0.99609375, 0.6796875, 0.0, 0.734375, 0.734375, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.9375, 0.9921875, 0.99609375, 0.734375, 0.9921875, 0.9921875, 0.1171875, 0.99609375, 0.734375, 0.9921875, 0.9921875, 0.734375, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.98828125, 0.73046875, 0.734375, 0.734375, 0.99609375, 0.9921875, 0.98828125, 0.99609375, 0.71875, 0.734375, 0.12890625, 0.99609375, 0.24609375, 0.0, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.99609375, 0.734375, 0.734375, 0.734375, 0.0, 0.55859375, 0.9921875, 0.72265625, 0.9921875, 0.9921875, 0.01953125, 0.7265625, 0.05859375, 0.9921875, 0.0, 0.93359375, 0.734375, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.72265625, 0.9921875, 0.0, 0.0, 0.98828125, 0.734375, 0.60546875, 0.0, 0.734375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.23046875, 0.0, 0.0625, 0.11328125, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.99609375, 0.0, 0.9921875, 0.734375, 0.734375, 0.734375, 0.62890625, 0.0, 0.0, 0.9921875, 0.34375, 0.99609375, 0.3203125, 0.69921875, 0.0, 0.734375, 0.99609375, 0.0, 0.72265625, 0.9921875, 0.0, 0.9921875, 0.0, 0.734375, 0.0, 0.7265625, 0.0, 0.734375, 0.9921875, 0.0, 0.73046875, 0.99609375, 0.734375, 0.99609375, 0.9921875, 0.99609375, 0.0, 0.0, 0.31640625, 0.9140625, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.734375, 0.0, 0.99609375, 0.95703125, 0.984375, 0.734375, 0.92578125, 0.9921875, 0.7265625, 0.9921875, 0.9921875, 0.98828125, 0.296875, 0.0, 0.734375, 0.9921875, 0.734375, 0.58984375, 0.0, 0.0, 0.9921875, 0.734375, 0.734375, 0.98828125, 0.99609375, 0.0, 0.0, 0.734375, 0.734375, 0.0, 0.99609375, 0.98828125, 0.72265625, 0.0, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.99609375, 0.0, 0.0, 0.734375, 0.1015625, 0.09375, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.734375, 0.0, 0.9921875, 0.0, 0.0, 0.9921875, 0.0, 0.734375, 0.0, 0.9921875, 0.9921875, 0.734375, 0.41015625, 0.72265625, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.9921875, 0.9921875, 0.734375, 0.734375, 0.734375, 0.16796875, 0.7265625, 0.9921875, 0.734375, 0.9921875, 0.734375, 0.0, 0.0, 0.0, 0.99609375, 0.0, 0.734375, 0.734375, 0.45703125, 0.0, 0.9921875, 0.98828125, 0.9921875, 0.734375, 0.98828125, 0.99609375, 0.99609375, 0.9921875, 0.0, 0.73046875, 0.83203125, 0.9921875, 0.99609375, 0.0, 0.0, 0.7265625, 0.9921875, 0.9921875, 0.140625, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.734375, 0.734375, 0.9296875, 0.45703125, 0.734375, 0.0, 0.4375, 0.734375, 0.99609375, 0.0, 0.734375, 0.9921875, 0.0, 0.8203125, 0.0, 0.98828125, 0.7265625, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.734375, 0.734375, 0.3515625, 0.0, 0.98828125, 0.42578125, 0.9921875, 0.0, 0.9921875, 0.70703125, 0.62890625, 0.9921875, 0.0, 0.734375, 0.9921875, 0.734375, 0.7265625, 0.99609375, 0.0, 0.16015625, 0.67578125, 0.734375, 0.734375, 0.9921875, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.0, 0.9921875, 0.734375, 0.9921875, 0.71875, 0.9921875, 0.1015625, 0.0, 0.9921875, 0.734375, 0.0, 0.0, 0.10546875, 0.734375, 0.0, 0.0, 0.0, 0.734375, 0.62109375, 0.70703125, 0.9921875, 0.2890625, 0.3125, 0.9921875, 0.0, 0.1328125, 0.0, 0.71484375, 0.734375, 0.734375, 0.0, 0.9921875, 0.734375, 0.7421875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.734375, 0.734375, 0.72265625, 0.0, 0.67578125, 0.98828125, 0.734375, 0.0, 0.734375, 0.52734375, 0.734375, 0.02734375, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.0, 0.71484375, 0.734375, 0.0, 0.0, 0.9921875, 0.99609375, 0.0, 0.56640625, 0.01953125, 0.734375, 0.734375, 0.734375, 0.0, 0.0, 0.0, 0.12890625, 0.0, 0.0, 0.9921875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.9921875, 0.734375, 0.05859375, 0.72265625, 0.9921875, 0.0, 0.0, 0.125, 0.73046875, 0.9921875, 0.734375, 0.734375, 0.734375, 0.0, 0.0, 0.9921875, 0.9921875, 0.99609375, 0.0, 0.0, 0.734375, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.7265625, 0.70703125, 0.0, 0.9921875, 0.7265625, 0.5625, 0.0, 0.0, 0.33984375, 0.9921875, 0.0, 0.0, 0.734375, 0.734375, 0.9921875, 0.0, 0.9921875, 0.734375, 0.99609375, 0.9921875, 0.0, 0.9921875, 0.0, 0.0, 0.734375, 0.0, 0.734375, 0.7890625, 0.9921875, 0.9921875, 0.734375, 0.734375, 0.734375, 0.72265625, 0.9921875, 0.0, 0.98828125, 0.0, 0.2890625, 0.99609375, 0.9921875, 0.0, 0.734375, 0.734375, 0.0, 0.9921875, 0.0, 0.0, 0.734375, 0.9921875, 0.9921875, 0.99609375, 0.9921875, 0.734375, 0.9921875, 0.7265625, 0.8671875, 0.9921875, 0.734375, 0.99609375, 0.9921875, 0.734375, 0.7109375, 0.9921875, 0.734375, 0.734375, 0.99609375, 0.734375, 0.98828125, 0.0625, 0.99609375, 0.69140625, 0.72265625, 0.9921875, 0.1796875, 0.0, 0.0, 0.734375, 0.9921875, 0.9921875, 0.0, 0.6875, 0.6953125, 0.0, 0.9921875, 0.734375, 0.0, 0.0, 0.68359375, 0.9921875, 0.9921875, 0.0, 0.0, 0.9921875, 0.9921875, 0.9921875, 0.734375, 0.734375, 0.734375, 0.83984375, 0.734375, 0.0, 0.9921875, 0.73046875, 0.734375, 0.734375, 0.0, 0.99609375, 0.046875, 0.09765625, 0.0, 0.9921875, 0.99609375, 0.0, 0.734375, 0.734375, 0.99609375, 0.0, 0.734375, 0.0, 0.734375, 0.9921875, 0.9921875, 0.734375, 0.9921875, 0.0, 0.72265625, 0.0, 0.734375, 0.0, 0.734375, 0.9921875, 0.734375, 0.0, 0.9921875, 0.109375, 0.734375, 0.0, 0.0, 0.734375, 0.9921875, 0.9921875, 0.734375, 0.3828125, 0.734375, 0.0, 0.09765625, 0.2578125, 0.734375, 0.9921875, 0.0, 0.734375, 0.11328125, 0.0, 0.046875, 0.9921875, 0.0, 0.0, 0.1328125, 0.9921875, 0.734375, 0.9921875, 0.17578125, 0.0, 0.0, 0.9921875, 0.9921875, 0.0, 0.72265625, 0.734375, 0.71875, 0.99609375, 0.99609375, 0.99609375, 0.0, 0.0, 0.9921875, 0.9921875, 0.72265625, 0.0, 0.734375, 0.9921875, 0.734375, 0.734375, 0.0, 0.09765625, 0.59375, 0.72265625, 0.734375, 0.0, 0.98828125, 0.0, 0.0, 0.734375, 0.07421875, 0.9921875, 0.99609375, 0.734375, 0.734375, 0.734375, 0.4609375, 0.734375, 0.0, 0.734375, 0.734375, 0.31640625, 0.0, 0.0, 0.99609375, 0.671875, 0.734375, 0.98828125, 0.734375, 0.734375, 0.0, 0.41796875, 0.03515625, 0.9921875, 0.9921875, 0.0, 0.0, 0.0, 0.9921875, 0.9921875, 0.8046875, 0.734375, 0.23046875, 0.734375, 0.99609375, 0.9375, 0.9921875, 0.0, 0.9921875, 0.9921875, 0.734375, 0.72265625, 0.0, 0.0, 0.99609375, 0.99609375, 0.0, 0.734375, 0.734375, 0.1171875, 0.30859375, 0.9921875, 0.734375, 0.9921875, 0.99609375, 0.93359375, 0.734375, 0.9921875, 0.0, 0.9921875, 0.0, 0.99609375, 0.7265625, 0.734375, 0.9921875, 0.0390625, 0.0, 0.0, 0.734375, 0.9921875, 0.7265625, 0.0, 0.9921875, 0.43359375, 0.9921875, 0.9921875, 0.98828125, 0.734375, 0.9921875, 0.73046875, 0.0, 0.734375, 0.7265625]

 sparsity of   [0.0263671875, 0.0205078125, 0.0478515625, 0.998046875, 0.09375, 0.0390625, 0.998046875, 0.05859375, 0.0361328125, 0.111328125, 0.0263671875, 0.0361328125, 0.078125, 0.1494140625, 0.9365234375, 0.9306640625, 0.9169921875, 0.0283203125, 0.015625, 0.166015625, 0.025390625, 0.998046875, 0.048828125, 0.58203125, 0.037109375, 0.0537109375, 0.0810546875, 0.033203125, 0.0322265625, 0.2353515625, 0.0322265625, 0.998046875, 0.0439453125, 0.7900390625, 0.021484375, 0.044921875, 0.033203125, 0.0087890625, 0.0439453125, 0.998046875, 0.9013671875, 0.0478515625, 0.044921875, 0.048828125, 0.05078125, 0.14453125, 0.7783203125, 0.08984375, 0.9990234375, 0.095703125, 0.42578125, 0.0546875, 0.0849609375, 0.0302734375, 0.01953125, 0.017578125, 0.0244140625, 0.0263671875, 0.029296875, 0.03125, 0.103515625, 0.017578125, 0.025390625, 0.126953125, 0.046875, 0.060546875, 0.1650390625, 0.9990234375, 0.0283203125, 0.037109375, 0.1630859375, 0.0517578125, 0.9970703125, 0.875, 0.1103515625, 0.03515625, 0.0283203125, 0.1171875, 0.0654296875, 0.0625, 0.998046875, 0.845703125, 0.97265625, 0.890625, 0.021484375, 0.0390625, 0.998046875, 0.0927734375, 0.8427734375, 0.0400390625, 0.048828125, 0.0205078125, 0.9345703125, 0.0732421875, 0.115234375, 0.0087890625, 0.998046875, 0.021484375, 0.0458984375, 0.5048828125, 0.8759765625, 0.9560546875, 0.875, 0.0283203125, 0.029296875, 0.0263671875, 0.0302734375, 0.9248046875, 0.998046875, 0.021484375, 0.0908203125, 0.998046875, 0.1904296875, 0.0400390625, 0.0458984375, 0.943359375, 0.798828125, 0.931640625, 0.0185546875, 0.998046875, 0.0185546875, 0.09375, 0.0712890625, 0.802734375, 0.9013671875, 0.8701171875, 0.0263671875, 0.84375, 0.0283203125, 0.0224609375, 0.05078125, 0.0205078125, 0.8955078125, 0.998046875, 0.9306640625, 0.0390625, 0.03515625, 0.013671875, 0.0546875, 0.091796875, 0.080078125, 0.736328125, 0.0517578125, 0.03515625, 0.0322265625, 0.046875, 0.9970703125, 0.59375, 0.9990234375, 0.0419921875, 0.65625, 0.029296875, 0.1220703125, 0.056640625, 0.0693359375, 0.0087890625, 0.025390625, 0.0478515625, 0.9541015625, 0.025390625, 0.0205078125, 0.0732421875, 0.998046875, 0.8603515625, 0.998046875, 0.998046875, 0.078125, 0.7080078125, 0.998046875, 0.9970703125, 0.0283203125, 0.0341796875, 0.041015625, 0.0361328125, 0.037109375, 0.0205078125, 0.0693359375, 0.916015625, 0.017578125, 0.033203125, 0.0595703125, 0.048828125, 0.0380859375, 0.0537109375, 0.9970703125, 0.869140625, 0.037109375, 0.998046875, 0.0791015625, 0.8916015625, 0.1357421875, 0.0810546875, 0.1826171875, 0.8408203125, 0.9365234375, 0.02734375, 0.998046875, 0.0205078125, 0.05078125, 0.998046875, 0.0732421875, 0.048828125, 0.0498046875, 0.234375, 0.009765625, 0.0107421875, 0.919921875, 0.876953125, 0.0390625, 0.05859375, 0.8330078125, 0.025390625, 0.01953125, 0.0244140625, 0.0908203125, 0.0576171875, 0.0205078125, 0.0400390625, 0.0126953125, 0.015625, 0.91015625, 0.3388671875, 0.068359375, 0.068359375, 0.998046875, 0.0146484375, 0.025390625, 0.9970703125, 0.03515625, 0.998046875, 0.9990234375, 0.998046875, 0.1123046875, 0.0595703125, 0.025390625, 0.07421875, 0.044921875, 0.919921875, 0.0419921875, 0.046875, 0.9970703125, 0.9013671875, 0.0986328125, 0.353515625, 0.0419921875, 0.9970703125, 0.998046875, 0.970703125, 0.033203125, 0.228515625, 0.81640625, 0.0205078125, 0.681640625, 0.03515625, 0.017578125, 0.0078125]

 sparsity of   [0.0360243059694767, 0.1436631977558136, 0.005642361007630825, 0.0546875, 0.1779513955116272, 0.0303819440305233, 0.0642361119389534, 0.0403645820915699, 0.0355902798473835, 0.010850694961845875, 0.01953125, 0.014756944961845875, 0.05078125, 0.0347222238779068, 0.01779513992369175, 0.5685763955116272, 0.9986979365348816, 0.0321180559694767, 0.0368923619389534, 0.0542534738779068, 0.01909722201526165, 0.02387152798473835, 0.0763888880610466, 0.3454861044883728, 0.0503472238779068, 0.6072048544883728, 0.0243055559694767, 0.02864583395421505, 0.01909722201526165, 0.0512152798473835, 0.0737847238779068, 0.134548619389534, 0.01779513992369175, 0.009548611007630825, 0.0334201380610466, 0.1297743022441864, 0.0568576380610466, 0.0368923619389534, 0.0533854179084301, 0.8033854365348816, 0.9995659589767456, 0.0638020858168602, 0.0264756940305233, 0.7799479365348816, 0.0815972238779068, 0.999131977558136, 0.0421006940305233, 0.0802951380610466, 0.0516493059694767, 0.01692708395421505, 0.1050347238779068, 0.013454861007630825, 0.1189236119389534, 0.0872395858168602, 0.03515625, 0.0425347238779068, 0.0972222238779068, 0.0859375, 0.1953125, 0.02994791604578495, 0.0325520820915699, 0.0525173619389534, 0.7174479365348816, 0.0173611119389534, 0.0347222238779068, 0.0646701380610466, 0.1393229216337204, 0.0203993059694767, 0.0872395858168602, 0.0303819440305233, 0.1089409738779068, 0.0607638880610466, 0.01605902798473835, 0.0464409738779068, 0.1002604141831398, 0.9431423544883728, 0.9986979365348816, 0.2230902761220932, 0.0186631940305233, 0.013888888992369175, 0.0494791679084301, 0.046875, 0.0460069440305233, 0.1484375, 0.01605902798473835, 0.0894097238779068, 0.02864583395421505, 0.671875, 0.1015625, 0.1510416716337204, 0.06640625, 0.013020833022892475, 0.0989583358168602, 0.0086805559694767, 0.05859375, 0.9010416865348816, 0.6432291865348816, 0.1684027761220932, 0.2209201455116272, 0.009982638992369175, 0.0386284738779068, 0.0490451380610466, 0.01779513992369175, 0.013020833022892475, 0.063368059694767, 0.0525173619389534, 0.0381944440305233, 0.0425347238779068, 0.005642361007630825, 0.0390625, 0.0303819440305233, 0.2543402910232544, 0.3793402910232544, 0.01605902798473835, 0.0438368059694767, 0.046875, 0.0703125, 0.0203993059694767, 0.0902777761220932, 0.0755208358168602, 0.02083333395421505, 0.0390625, 0.0225694440305233, 0.0668402761220932, 0.0555555559694767, 0.0772569477558136, 0.0338541679084301, 0.02994791604578495, 0.0125868059694767, 0.005642361007630825, 0.1158854141831398, 0.0980902761220932, 0.0520833320915699, 0.1362847238779068, 0.0503472238779068, 0.999131977558136, 0.0360243059694767, 0.067274309694767, 0.0872395858168602, 0.0125868059694767, 0.0490451380610466, 0.1106770858168602, 0.0911458358168602, 0.01909722201526165, 0.02170138992369175, 0.9986979365348816, 0.00824652798473835, 0.015625, 0.02300347201526165, 0.046875, 0.0078125, 0.11328125, 0.0225694440305233, 0.0885416641831398, 0.01171875, 0.0512152798473835, 0.0, 0.010416666977107525, 0.0078125, 0.0069444444961845875, 0.05078125, 0.01605902798473835, 0.01779513992369175, 0.0394965298473835, 0.0434027798473835, 0.009114583022892475, 0.0425347238779068, 0.013888888992369175, 0.0316840298473835, 0.7730034589767456, 0.015625, 0.0967881977558136, 0.0052083334885537624, 0.121961809694767, 0.01215277798473835, 0.0251736119389534, 0.0677083358168602, 0.0438368059694767, 0.0677083358168602, 0.7630208134651184, 0.999131977558136, 0.0516493059694767, 0.0086805559694767, 0.0581597238779068, 0.9995659589767456, 0.6801215410232544, 0.0355902798473835, 0.9140625, 0.0377604179084301, 0.0607638880610466, 0.03515625, 0.013020833022892475, 0.010850694961845875, 0.9986979365348816, 0.07421875, 0.1258680522441864, 0.02864583395421505, 0.1801215261220932, 0.0008680555620230734, 0.999131977558136, 0.0525173619389534, 0.01171875, 0.0347222238779068, 0.0421006940305233, 0.1080729141831398, 0.02473958395421505, 0.0611979179084301, 0.015625, 0.0360243059694767, 0.0698784738779068, 0.93359375, 0.0325520820915699, 0.010850694961845875, 0.014756944961845875, 0.02387152798473835, 0.04296875, 0.010416666977107525, 0.0451388880610466, 0.014322916977107525, 0.02560763992369175, 0.1276041716337204, 0.1671006977558136, 0.0833333358168602, 0.0282118059694767, 0.0442708320915699, 0.01996527798473835, 0.7578125, 0.1297743022441864, 0.0720486119389534, 0.9301215410232544, 0.9986979365348816, 0.02387152798473835, 0.013454861007630825, 0.02170138992369175, 0.6501736044883728, 0.0993923619389534, 0.0069444444961845875, 0.05859375, 0.0460069440305233, 0.0325520820915699, 0.00824652798473835, 0.9231770634651184, 0.6701388955116272, 0.0776909738779068, 0.02083333395421505, 0.1375868022441864, 0.0243055559694767, 0.9995659589767456, 0.8250868320465088, 0.0703125, 0.01822916604578495, 0.08984375, 0.0164930559694767, 0.02387152798473835, 0.1484375, 0.0685763880610466]

 sparsity of   [0.9921875, 0.04296875, 0.20703125, 0.98828125, 0.98828125, 0.98828125, 0.140625, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.015625, 0.9921875, 0.078125, 0.9921875, 0.85546875, 0.06640625, 0.08984375, 0.98828125, 0.06640625, 0.9921875, 0.09375, 0.98828125, 0.1796875, 0.984375, 0.08203125, 0.0859375, 0.984375, 0.1171875, 0.9921875, 0.16015625, 0.98828125, 0.02734375, 0.04296875, 0.078125, 0.984375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.125, 0.953125, 0.99609375, 0.11328125, 0.0859375, 0.1640625, 0.88671875, 0.98828125, 0.20703125, 0.33203125, 0.98828125, 0.9921875, 0.98828125, 0.01171875, 0.06640625, 0.12109375, 0.984375, 0.98828125, 0.9921875, 0.0625, 0.98828125, 0.98828125, 0.0078125, 0.98828125, 0.21484375, 0.98828125, 0.125, 0.00390625, 0.98828125, 0.0703125, 0.98828125, 0.09375, 0.03125, 0.0625, 0.125, 0.984375, 0.984375, 0.16015625, 0.98828125, 0.9921875, 0.04296875, 0.04296875, 0.98828125, 0.17578125, 0.09765625, 0.98828125, 0.9921875, 0.01953125, 0.16015625, 0.09765625, 0.15625, 0.08984375, 0.03515625, 0.0390625, 0.98828125, 0.9921875, 0.10546875, 0.9921875, 0.08203125, 0.0390625, 0.984375, 0.0625, 0.0546875, 0.046875, 0.12109375, 0.984375, 0.16796875, 0.0625, 0.9921875, 0.09765625, 0.86328125, 0.34765625, 0.9921875, 0.05859375, 0.1953125, 0.98828125, 0.0859375, 0.01171875, 0.98828125, 0.98828125, 0.98828125, 0.2890625, 0.03515625, 0.00390625, 0.078125, 0.04296875, 0.02734375, 0.16015625, 0.9921875, 0.0078125, 0.99609375, 0.98828125, 0.9921875, 0.0078125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.03515625, 0.12109375, 0.98828125, 0.875, 0.046875, 0.9921875, 0.8125, 0.9921875, 0.9921875, 0.00390625, 0.98828125, 0.00390625, 0.1171875, 0.98828125, 0.01171875, 0.98828125, 0.98828125, 0.203125, 0.98828125, 0.15625, 0.98828125, 0.98828125, 0.9921875, 0.08984375, 0.9921875, 0.98828125, 0.1484375, 0.984375, 0.984375, 0.11328125, 0.015625, 0.06640625, 0.0546875, 0.0390625, 0.0703125, 0.08984375, 0.05078125, 0.140625, 0.98828125, 0.01953125, 0.9921875, 0.02734375, 0.13671875, 0.984375, 0.0703125, 0.98828125, 0.98828125, 0.0703125, 0.1328125, 0.98828125, 0.06640625, 0.03515625, 0.11328125, 0.0546875, 0.01171875, 0.98828125, 0.99609375, 0.02734375, 0.12890625, 0.09765625, 0.9921875, 0.98828125, 0.04296875, 0.00390625, 0.98828125, 0.1484375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.03125, 0.984375, 0.9921875, 0.98828125, 0.0078125, 0.359375, 0.06640625, 0.98828125, 0.984375, 0.09765625, 0.98828125, 0.0703125, 0.0234375, 0.98828125, 0.98828125, 0.140625, 0.03515625, 0.08984375, 0.98828125, 0.03515625, 0.98828125, 0.0546875, 0.015625, 0.01953125, 0.99609375, 0.06640625, 0.98828125, 0.98828125, 0.9296875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.1171875, 0.9921875, 0.9921875, 0.07421875, 0.9921875, 0.9921875, 0.07421875, 0.9140625, 0.90625, 0.02734375, 0.1015625, 0.04296875, 0.98828125, 0.98828125, 0.07421875, 0.75, 0.0859375, 0.9921875, 0.03125, 0.98828125, 0.25, 0.1328125, 0.12890625, 0.21875, 0.0234375, 0.78515625, 0.07421875, 0.98828125, 0.98828125, 0.84765625, 0.10546875, 0.9921875, 0.98828125, 0.03515625, 0.12890625, 0.0078125, 0.3046875, 0.9921875, 0.1484375, 0.08203125, 0.08984375, 0.18359375, 0.09375, 0.98828125, 0.984375, 0.99609375, 0.10546875, 0.07421875, 0.98828125, 0.98828125, 0.13671875, 0.9921875, 0.26953125, 0.0078125, 0.98828125, 0.98828125, 0.98828125, 0.09765625, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.13671875, 0.98828125, 0.0390625, 0.10546875, 0.98828125, 0.98828125, 0.00390625, 0.1015625, 0.98828125, 0.25390625, 0.921875, 0.98828125, 0.98828125, 0.08984375, 0.98828125, 0.0859375, 0.16015625, 0.98828125, 0.10546875, 0.9921875, 0.0703125, 0.73828125, 0.0546875, 0.109375, 0.31640625, 0.99609375, 0.1796875, 0.76171875, 0.01171875, 0.015625, 0.9921875, 0.9921875, 0.984375, 0.0, 0.9921875, 0.8671875, 0.98828125, 0.0859375, 0.9921875, 0.04296875, 0.98828125, 0.0, 0.98828125, 0.98828125, 0.10546875, 0.12109375, 0.13671875, 0.1640625, 0.9921875, 0.0625, 0.0625, 0.03125, 0.17578125, 0.69921875, 0.98828125, 0.16015625, 0.98828125, 0.015625, 0.9296875, 0.09375, 0.171875, 0.12890625, 0.0234375, 0.046875, 0.0546875, 0.99609375, 0.109375, 0.12890625, 0.98828125, 0.29296875, 0.98828125, 0.98828125, 0.98828125, 0.09375, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.03125, 0.01953125, 0.98828125, 0.02734375, 0.0390625, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.01171875, 0.1171875, 0.01171875, 0.04296875, 0.13671875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.1015625, 0.9921875, 0.9921875, 0.23828125, 0.03125, 0.9921875, 0.99609375, 0.95703125, 0.9921875, 0.01171875, 0.9921875, 0.0234375, 0.21875, 0.984375, 0.98828125, 0.05078125, 0.0390625, 0.984375, 0.07421875, 0.12109375, 0.9921875, 0.078125, 0.19140625, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.07421875, 0.984375, 0.9921875, 0.15625, 0.98828125, 0.98828125, 0.05859375, 0.99609375, 0.98828125, 0.984375, 0.6875, 0.0546875, 0.984375, 0.98828125, 0.12109375, 0.98828125, 0.0078125, 0.9921875, 0.31640625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.10546875, 0.10546875, 0.05078125, 0.0625, 0.234375, 0.9921875, 0.98828125, 0.984375, 0.10546875, 0.98828125, 0.98828125, 0.0703125, 0.76953125, 0.91015625, 0.78515625, 0.99609375, 0.0625, 0.9921875, 0.98828125, 0.98828125, 0.99609375, 0.03125, 0.9921875, 0.85546875, 0.98828125, 0.9921875, 0.99609375, 0.2109375, 0.0078125, 0.9921875, 0.05078125, 0.7734375, 0.9921875, 0.98828125, 0.9921875, 0.98046875, 0.98828125, 0.140625, 0.98828125, 0.9921875, 0.98828125, 0.0625, 0.04296875, 0.29296875, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.84375, 0.98828125, 0.98828125, 0.890625, 0.98828125, 0.2109375, 0.15234375, 0.0859375, 0.05859375, 0.98828125, 0.0390625, 0.37109375, 0.98828125, 0.98828125, 0.0625, 0.984375, 0.0625, 0.0859375, 0.09765625, 0.9921875, 0.98828125, 0.9921875, 0.0703125, 0.98828125, 0.89453125, 0.07421875, 0.98828125, 0.98828125, 0.03515625, 0.0390625, 0.98828125, 0.12890625, 0.11328125, 0.0546875, 0.140625, 0.0, 0.98828125, 0.98828125, 0.07421875, 0.02734375, 0.0390625, 0.9921875, 0.171875, 0.09765625, 0.078125, 0.890625, 0.9921875, 0.27734375, 0.98828125, 0.98828125, 0.98828125, 0.3203125, 0.9921875, 0.0234375, 0.03125, 0.98828125, 0.0546875, 0.140625, 0.9921875, 0.12109375, 0.12890625, 0.984375, 0.9921875, 0.9921875, 0.125, 0.98828125, 0.9921875, 0.98828125, 0.88671875, 0.9921875, 0.98828125, 0.08203125, 0.9921875, 0.98828125, 0.0703125, 0.140625, 0.05078125, 0.1171875, 0.90234375, 0.98828125, 0.98828125, 0.06640625, 0.1015625, 0.8046875, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.08984375, 0.17578125, 0.10546875, 0.98828125, 0.9921875, 0.0625, 0.98828125, 0.15625, 0.02734375, 0.6015625, 0.99609375, 0.12109375, 0.984375, 0.71484375, 0.98828125, 0.921875, 0.9921875, 0.12109375, 0.09765625, 0.01171875, 0.953125, 0.27734375, 0.98828125, 0.0859375, 0.9921875, 0.09375, 0.03125, 0.98828125, 0.01953125, 0.984375, 0.1796875, 0.98828125, 0.98828125, 0.83984375, 0.09375, 0.90234375, 0.9921875, 0.98828125, 0.0078125, 0.1171875, 0.0703125, 0.1640625, 0.984375, 0.9921875, 0.12890625, 0.0078125, 0.0546875, 0.98828125, 0.9921875, 0.0703125, 0.98828125, 0.0625, 0.1640625, 0.98828125, 0.89453125, 0.98828125, 0.0234375, 0.15625, 0.98828125, 0.9921875, 0.98828125, 0.06640625, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.046875, 0.9921875, 0.98828125, 0.98828125, 0.77734375, 0.98828125, 0.98828125, 0.01953125, 0.08984375, 0.171875, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.0390625, 0.1171875, 0.984375, 0.03125, 0.9921875, 0.16796875, 0.984375, 0.88671875, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.0546875, 0.11328125, 0.03125, 0.07421875, 0.0078125, 0.98828125, 0.87890625, 0.046875, 0.98828125, 0.08203125, 0.06640625, 0.015625, 0.13671875, 0.0078125, 0.984375, 0.98828125, 0.98828125, 0.03125, 0.33984375, 0.08984375, 0.9921875, 0.98828125, 0.984375, 0.01171875, 0.0546875, 0.98828125, 0.078125, 0.98828125, 0.1640625, 0.9921875, 0.9921875, 0.05078125, 0.98828125, 0.12109375, 0.01171875, 0.125, 0.98828125, 0.984375, 0.9921875, 0.9921875, 0.0546875, 0.09375, 0.046875, 0.078125, 0.828125, 0.98828125, 0.98828125, 0.05859375, 0.98828125, 0.98828125, 0.09765625, 0.98828125, 0.02734375, 0.796875, 0.0390625, 0.98828125, 0.98828125, 0.98828125, 0.28125, 0.98828125, 0.07421875, 0.04296875, 0.98828125, 0.8828125, 0.98828125, 0.171875, 0.9921875, 0.9921875, 0.98828125, 0.9921875, 0.8671875, 0.3125, 0.0625, 0.0859375, 0.9921875, 0.98828125, 0.13671875, 0.1015625, 0.03515625, 0.9921875, 0.08984375, 0.984375, 0.0546875, 0.98828125, 0.0078125, 0.9921875, 0.04296875, 0.98828125, 0.0546875, 0.09375, 0.98828125, 0.03515625, 0.9921875, 0.03125, 0.984375, 0.109375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.14453125, 0.98828125, 0.0, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.05078125, 0.015625, 0.0234375, 0.04296875, 0.98828125, 0.98828125, 0.984375, 0.99609375, 0.140625, 0.12109375, 0.08984375, 0.109375, 0.0546875, 0.98828125, 0.9921875, 0.10546875, 0.9921875, 0.05859375, 0.03125, 0.046875, 0.08984375, 0.13671875, 0.03515625, 0.03125, 0.046875, 0.21875, 0.15625, 0.17578125, 0.08984375, 0.109375, 0.9921875, 0.07421875, 0.1796875, 0.00390625, 0.05859375, 0.03125, 0.09375, 0.04296875, 0.98828125, 0.984375, 0.13671875, 0.98828125, 0.11328125, 0.984375, 0.0625, 0.3046875, 0.1171875, 0.9921875, 0.98828125, 0.98828125, 0.0859375, 0.99609375, 0.02734375, 0.109375, 0.98828125, 0.08984375, 0.09375, 0.98828125, 0.98828125, 0.03125, 0.22265625, 0.98828125, 0.98828125, 0.98828125, 0.06640625, 0.98828125, 0.08984375, 0.98828125, 0.99609375, 0.9921875, 0.14453125, 0.08984375, 0.0625, 0.07421875, 0.109375, 0.265625, 0.98828125, 0.01953125, 0.98828125, 0.078125, 0.265625, 0.9921875, 0.984375, 0.03515625, 0.12890625, 0.98828125, 0.9921875, 0.03125, 0.015625, 0.98828125, 0.98828125, 0.0546875, 0.02734375, 0.98828125, 0.98828125, 0.125, 0.22265625, 0.0390625, 0.0546875, 0.17578125, 0.9921875, 0.98828125, 0.984375, 0.9921875, 0.984375, 0.98828125, 0.08203125, 0.09375, 0.984375, 0.25390625, 0.83203125, 0.2890625, 0.09765625, 0.02734375, 0.1328125, 0.98828125, 0.98828125, 0.98828125, 0.05859375, 0.08984375, 0.98828125, 0.4296875, 0.08203125, 0.39453125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.296875, 0.9921875, 0.11328125, 0.77734375, 0.9921875, 0.98828125, 0.98828125, 0.03125, 0.0859375, 0.12890625, 0.02734375, 0.984375, 0.12109375, 0.703125, 0.98828125, 0.98828125, 0.98828125, 0.9140625, 0.98828125, 0.046875, 0.98828125, 0.0859375, 0.77734375, 0.984375, 0.98828125, 0.98828125, 0.11328125, 0.015625, 0.98828125, 0.0078125, 0.98828125, 0.98828125, 0.24609375, 0.12890625, 0.0234375, 0.14453125, 0.98828125, 0.98828125, 0.0078125, 0.0703125, 0.9921875, 0.0703125, 0.9921875, 0.078125, 0.10546875, 0.0390625, 0.09765625, 0.0859375, 0.984375, 0.984375, 0.98828125, 0.9921875, 0.0703125, 0.9921875, 0.125, 0.0390625, 0.98828125, 0.98828125, 0.08203125, 0.12109375, 0.984375, 0.0546875, 0.1328125, 0.19921875, 0.078125, 0.984375, 0.98828125, 0.9921875, 0.0625, 0.296875, 0.98828125, 0.02734375, 0.9921875, 0.9921875, 0.9921875, 0.109375, 0.078125, 0.09375, 0.16796875, 0.98828125, 0.9921875, 0.140625, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.1015625, 0.0703125, 0.0546875, 0.984375, 0.13671875, 0.078125, 0.98828125, 0.98828125, 0.046875, 0.9921875, 0.27734375, 0.1171875, 0.33203125, 0.0234375, 0.140625, 0.98828125, 0.98828125, 0.98828125, 0.12890625, 0.125]

 sparsity of   [0.1025390625, 0.998046875, 0.9970703125, 0.0595703125, 0.802734375, 0.0341796875, 0.998046875, 0.998046875, 0.0380859375, 0.998046875, 0.171875, 0.1728515625, 0.998046875, 0.0390625, 0.998046875, 0.998046875, 0.8662109375, 0.0205078125, 0.224609375, 0.7626953125, 0.1123046875, 0.0361328125, 0.1484375, 0.9990234375, 0.0791015625, 0.1025390625, 0.998046875, 0.1533203125, 0.0673828125, 0.0087890625, 0.119140625, 0.998046875, 0.87109375, 0.9638671875, 0.998046875, 0.0810546875, 0.0, 0.99609375, 0.0751953125, 0.998046875, 0.0400390625, 0.998046875, 0.1484375, 0.02734375, 0.814453125, 0.0244140625, 0.1650390625, 0.40625, 0.18359375, 0.0771484375, 0.08984375, 0.0869140625, 0.998046875, 0.0927734375, 0.9990234375, 0.8984375, 0.998046875, 0.998046875, 0.998046875, 0.03125, 0.998046875, 0.998046875, 0.033203125, 0.0390625, 0.998046875, 0.0224609375, 0.07421875, 0.029296875, 0.0302734375, 0.0517578125, 0.9990234375, 0.2724609375, 0.048828125, 0.998046875, 0.091796875, 0.0224609375, 0.0283203125, 0.1025390625, 0.0537109375, 0.0498046875, 0.0859375, 0.998046875, 0.998046875, 0.0, 0.05859375, 0.0498046875, 0.2275390625, 0.0732421875, 0.8583984375, 0.09375, 0.1591796875, 0.0986328125, 0.998046875, 0.1796875, 0.107421875, 0.9453125, 0.07421875, 0.173828125, 0.998046875, 0.009765625, 0.044921875, 0.126953125, 0.9970703125, 0.0625, 0.9482421875, 0.1015625, 0.0908203125, 0.162109375, 0.79296875, 0.0302734375, 0.1728515625, 0.0341796875, 0.998046875, 0.134765625, 0.1044921875, 0.998046875, 0.998046875, 0.0322265625, 0.169921875, 0.0283203125, 0.0927734375, 0.4130859375, 0.869140625, 0.998046875, 0.998046875, 0.998046875, 0.9228515625, 0.0927734375, 0.998046875, 0.060546875, 0.998046875, 0.8798828125, 0.0107421875, 0.0224609375, 0.998046875, 0.998046875, 0.998046875, 0.0244140625, 0.046875, 0.0556640625, 0.0224609375, 0.19140625, 0.998046875, 0.9990234375, 0.998046875, 0.0341796875, 0.9970703125, 0.1796875, 0.068359375, 0.955078125, 0.0458984375, 0.998046875, 0.048828125, 0.0224609375, 0.0830078125, 0.01171875, 0.02734375, 0.029296875, 0.12109375, 0.9970703125, 0.89453125, 0.0517578125, 0.0771484375, 0.0791015625, 0.0712890625, 0.9990234375, 0.177734375, 0.9404296875, 0.998046875, 0.9990234375, 0.9072265625, 0.9951171875, 0.998046875, 0.998046875, 0.998046875, 0.072265625, 0.0302734375, 0.0673828125, 0.1123046875, 0.0341796875, 0.0791015625, 0.1044921875, 0.9990234375, 0.05078125, 0.1298828125, 0.998046875, 0.0498046875, 0.0234375, 0.998046875, 0.998046875, 0.0478515625, 0.9580078125, 0.021484375, 0.1455078125, 0.044921875, 0.068359375, 0.998046875, 0.44921875, 0.0654296875, 0.998046875, 0.0, 0.998046875, 0.796875, 0.87890625, 0.07421875, 0.998046875, 0.998046875, 0.158203125, 0.9970703125, 0.998046875, 0.998046875, 0.1162109375, 0.998046875, 0.0703125, 0.9990234375, 0.0927734375, 0.265625, 0.1591796875, 0.998046875, 0.9267578125, 0.03515625, 0.0517578125, 0.044921875, 0.25, 0.998046875, 0.8447265625, 0.9970703125, 0.1181640625, 0.998046875, 0.1650390625, 0.0478515625, 0.9970703125, 0.0078125, 0.0771484375, 0.1611328125, 0.0009765625, 0.921875, 0.998046875, 0.0390625, 0.0390625, 0.072265625, 0.1064453125, 0.12890625, 0.037109375, 0.0234375, 0.9990234375, 0.998046875, 0.998046875, 0.00390625, 0.0234375, 0.0693359375, 0.998046875, 0.91015625, 0.044921875, 0.1259765625, 0.99609375]

 sparsity of   [0.62109375, 0.0559895820915699, 0.0438368059694767, 0.02083333395421505, 0.0603298619389534, 0.0529513880610466, 0.8528645634651184, 0.8884548544883728, 0.146267369389534, 0.0329861119389534, 0.9270833134651184, 0.05859375, 0.1805555522441864, 0.0321180559694767, 0.0212673619389534, 0.02473958395421505, 0.02777777798473835, 0.02473958395421505, 0.0373263880610466, 0.9383680820465088, 0.9986979365348816, 0.0755208358168602, 0.0490451380610466, 0.9986979365348816, 0.6584201455116272, 0.01779513992369175, 0.0967881977558136, 0.0173611119389534, 0.197048619389534, 0.999131977558136, 0.0212673619389534, 0.8259548544883728, 0.9040798544883728, 0.5711805820465088, 0.01128472201526165, 0.9322916865348816, 0.009982638992369175, 0.075086809694767, 0.999131977558136, 0.9609375, 0.9513888955116272, 0.9986979365348816, 0.03515625, 0.0264756940305233, 0.7951388955116272, 0.0577256940305233, 0.015625, 0.9270833134651184, 0.0290798619389534, 0.0655381977558136, 0.02560763992369175, 0.0473090298473835, 0.0989583358168602, 0.0290798619389534, 0.936631977558136, 0.807725727558136, 0.0850694477558136, 0.014756944961845875, 0.0212673619389534, 0.2604166567325592, 0.01909722201526165, 0.9986979365348816, 0.0993923619389534, 0.0290798619389534, 0.999131977558136, 0.1067708358168602, 0.0364583320915699, 0.8654513955116272, 0.090711809694767, 0.0251736119389534, 0.9995659589767456, 0.0360243059694767, 0.8246527910232544, 0.7387152910232544, 0.9140625, 0.013454861007630825, 0.0889756977558136, 0.3519965410232544, 0.0477430559694767, 0.02994791604578495, 0.9427083134651184, 0.0243055559694767, 0.8138020634651184, 0.0876736119389534, 0.9995659589767456, 0.9995659589767456, 0.1484375, 0.013020833022892475, 0.01692708395421505, 0.8480902910232544, 0.6714409589767456, 0.9986979365348816, 0.0590277798473835, 0.0481770820915699, 0.04296875, 0.0516493059694767, 0.999131977558136, 0.0225694440305233, 0.6536458134651184, 0.0303819440305233, 0.03081597201526165, 0.0581597238779068, 0.9557291865348816, 0.02387152798473835, 0.013454861007630825, 0.02690972201526165, 0.8949652910232544, 0.390190988779068, 0.0551215298473835, 0.00824652798473835, 0.909288227558136, 0.0303819440305233, 0.014322916977107525, 0.0477430559694767, 0.0390625, 0.9574652910232544, 0.0811631977558136, 0.8984375, 0.009982638992369175, 0.0486111119389534, 0.0590277798473835, 0.0559895820915699, 0.0494791679084301, 0.0164930559694767, 0.0212673619389534, 0.0824652761220932, 0.999131977558136, 0.0933159738779068, 0.0516493059694767, 0.02083333395421505, 0.9609375, 0.01996527798473835, 0.9986979365348816, 0.8446180820465088, 0.0538194440305233, 0.1241319477558136, 0.0282118059694767, 0.1449652761220932, 0.0, 0.03081597201526165, 0.0264756940305233, 0.01519097201526165, 0.03081597201526165, 0.999131977558136, 0.9105902910232544, 0.0321180559694767, 0.0364583320915699, 0.0412326380610466, 0.901475727558136, 0.0360243059694767, 0.10546875, 0.0881076380610466, 0.328125, 0.9995659589767456, 0.01909722201526165, 0.02604166604578495, 0.106336809694767, 0.03515625, 0.009982638992369175, 0.1488715261220932, 0.999131977558136, 0.764756977558136, 0.9995659589767456, 0.0065104165114462376, 0.9995659589767456, 0.009982638992369175, 0.999131977558136, 0.9986979365348816, 0.1271701455116272, 0.0572916679084301, 0.0490451380610466, 0.0399305559694767, 0.9513888955116272, 0.0125868059694767, 0.0355902798473835, 0.4279513955116272, 0.02951388992369175, 0.0316840298473835, 0.9986979365348816, 0.9986979365348816, 0.01519097201526165, 0.0559895820915699, 0.0321180559694767, 0.0438368059694767, 0.0855034738779068, 0.15234375, 0.999131977558136, 0.0607638880610466, 0.0282118059694767, 0.792100727558136, 0.1671006977558136, 0.0164930559694767, 0.7252604365348816, 0.9995659589767456, 0.0842013880610466, 0.08984375, 0.014322916977107525, 0.9388020634651184, 0.7400173544883728, 0.0164930559694767, 0.01779513992369175, 0.0164930559694767, 0.00824652798473835, 0.01953125, 0.5234375, 0.0164930559694767, 0.2591145932674408, 0.0555555559694767, 0.0364583320915699, 0.999131977558136, 0.0251736119389534, 0.01779513992369175, 0.013888888992369175, 0.0403645820915699, 0.9995659589767456, 0.2608506977558136, 0.9986979365348816, 0.02300347201526165, 0.9995659589767456, 0.9986979365348816, 0.999131977558136, 0.9769965410232544, 0.0460069440305233, 0.02734375, 0.01953125, 0.999131977558136, 0.01779513992369175, 0.9344618320465088, 0.0980902761220932, 0.9986979365348816, 0.0572916679084301, 0.0251736119389534, 0.02170138992369175, 0.0355902798473835, 0.0186631940305233, 0.7981770634651184, 0.0225694440305233, 0.1245659738779068, 0.9986979365348816, 0.0282118059694767, 0.02560763992369175, 0.788194477558136, 0.0, 0.999131977558136, 0.9986979365348816, 0.2647569477558136, 0.0325520820915699, 0.0577256940305233, 0.0464409738779068, 0.0989583358168602, 0.9578993320465088, 0.0282118059694767, 0.9995659589767456, 0.0225694440305233, 0.9787326455116272, 0.0494791679084301]

 sparsity of   [0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.99609375, 0.97265625, 0.97265625, 0.99609375, 0.99609375, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.96484375, 0.96875, 0.99609375, 0.9921875, 0.97265625, 0.99609375, 0.99609375, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.89453125, 0.97265625, 0.90625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.95703125, 0.97265625, 0.99609375, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.9921875, 0.95703125, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9609375, 0.9921875, 0.9921875, 0.95703125, 0.9921875, 0.9921875, 0.96875, 0.953125, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.96484375, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.953125, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.0, 0.96484375, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9609375, 0.9609375, 0.97265625, 0.9609375, 0.95703125, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.953125, 0.97265625, 0.97265625, 0.99609375, 0.9609375, 0.9921875, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.98828125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.97265625, 0.97265625, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9296875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.99609375, 0.97265625, 0.99609375, 0.99609375, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.96484375, 0.95703125, 0.97265625, 0.9921875, 0.9921875, 0.96875, 0.9921875, 0.9921875, 0.9609375, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.953125, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.99609375, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.99609375, 0.99609375, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.9921875, 0.96875, 0.97265625, 0.0, 0.97265625, 0.98046875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.9609375, 0.96875, 0.91796875, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.96875, 0.96875, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.99609375, 0.99609375, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.96875, 0.9921875, 0.99609375, 0.97265625, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.953125, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.96875, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.97265625, 0.98828125, 0.97265625, 0.34765625, 0.97265625, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.0, 0.99609375, 0.99609375, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.90625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.96875, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.99609375, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.96875, 0.99609375, 0.9921875, 0.97265625, 0.96484375, 0.0, 0.9921875, 0.96875, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.99609375, 0.97265625, 0.9921875, 0.9609375, 0.97265625, 0.99609375, 0.9921875, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.95703125, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.9921875, 0.0, 0.9921875, 0.97265625, 0.97265625, 0.99609375, 0.9453125, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.96484375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.96875, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.90625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9140625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.0, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.0, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.95703125, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.953125, 0.95703125, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.0, 0.97265625, 0.97265625, 0.94921875, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.953125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.9921875, 0.99609375, 0.9609375, 0.9921875, 0.96875, 0.97265625, 0.99609375, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.9921875, 0.953125, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.984375, 0.96484375, 0.97265625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.9609375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.94921875, 0.97265625, 0.97265625, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.95703125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.0, 0.98828125, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9609375, 0.9921875, 0.9921875, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.96875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.953125, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.97265625, 0.95703125, 0.99609375, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.96484375, 0.97265625, 0.97265625, 0.91796875, 0.9609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.95703125, 0.97265625, 0.9921875, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.99609375, 0.953125, 0.99609375, 0.97265625, 0.97265625, 0.99609375, 0.99609375, 0.9921875, 0.9921875, 0.97265625, 0.9609375, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.96875, 0.97265625, 0.97265625, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.97265625, 0.99609375, 0.97265625, 0.97265625, 0.97265625, 0.9921875, 0.99609375, 0.96875, 0.97265625, 0.97265625, 0.9921875, 0.953125, 0.97265625, 0.97265625, 0.97265625]

 sparsity of   [0.998046875, 0.998046875, 0.998046875, 0.376953125, 0.998046875, 0.998046875, 0.5830078125, 0.375, 0.998046875, 0.8505859375, 0.0595703125, 0.072265625, 0.9990234375, 0.4638671875, 0.998046875, 0.998046875, 0.9970703125, 0.0693359375, 0.998046875, 0.1953125, 0.0693359375, 0.1494140625, 0.09375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.1083984375, 0.9990234375, 0.33203125, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.875, 0.998046875, 0.12109375, 0.0654296875, 0.99609375, 0.998046875, 0.1328125, 0.078125, 0.0517578125, 0.1103515625, 0.998046875, 0.052734375, 0.9990234375, 0.087890625, 0.1171875, 0.998046875, 0.2021484375, 0.998046875, 0.998046875, 0.0869140625, 0.9990234375, 0.998046875, 0.998046875, 0.0908203125, 0.068359375, 0.2021484375, 0.06640625, 0.998046875, 0.998046875, 0.41015625, 0.0615234375, 0.509765625, 0.998046875, 0.833984375, 0.107421875, 0.203125, 0.998046875, 0.998046875, 0.998046875, 0.08984375, 0.998046875, 0.0595703125, 0.291015625, 0.060546875, 0.0869140625, 0.076171875, 0.0595703125, 0.998046875, 0.0390625, 0.998046875, 0.380859375, 0.1220703125, 0.998046875, 0.998046875, 0.048828125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.1123046875, 0.1572265625, 0.15625, 0.998046875, 0.1220703125, 0.9990234375, 0.998046875, 0.458984375, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.4453125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.0400390625, 0.998046875, 0.9970703125, 0.0439453125, 0.9970703125, 0.998046875, 0.3544921875, 0.9990234375, 0.0625, 0.0615234375, 0.998046875, 0.99609375, 0.8701171875, 0.9970703125, 0.11328125, 0.998046875, 0.0390625, 0.15234375, 0.998046875, 0.99609375, 0.998046875, 0.998046875, 0.9970703125, 0.421875, 0.107421875, 0.998046875, 0.998046875, 0.087890625, 0.998046875, 0.1005859375, 0.4833984375, 0.998046875, 0.0576171875, 0.1015625, 0.7421875, 0.1171875, 0.9990234375, 0.904296875, 0.12109375, 0.07421875, 0.998046875, 0.1650390625, 0.998046875, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.9970703125, 0.2021484375, 0.0810546875, 0.998046875, 0.998046875, 0.048828125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.0, 0.138671875, 0.998046875, 0.9970703125, 0.0380859375, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.0703125, 0.998046875, 0.0859375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.109375, 0.998046875, 0.2021484375, 0.0556640625, 0.9970703125, 0.998046875, 0.9990234375, 0.9970703125, 0.9990234375, 0.1416015625, 0.998046875, 0.9970703125, 0.0703125, 0.9970703125, 0.998046875, 0.0126953125, 0.998046875, 0.998046875, 0.998046875, 0.033203125, 0.998046875, 0.0712890625, 0.142578125, 0.998046875, 0.8251953125, 0.07421875, 0.998046875, 0.0654296875, 0.998046875, 0.1591796875, 0.9990234375, 0.998046875, 0.998046875, 0.138671875, 0.2021484375, 0.2021484375, 0.9990234375, 0.998046875, 0.072265625, 0.9990234375, 0.9970703125, 0.9990234375, 0.9990234375, 0.0576171875, 0.998046875, 0.8125, 0.4423828125, 0.998046875, 0.1240234375, 0.998046875, 0.998046875, 0.03515625, 0.4052734375, 0.998046875, 0.8408203125, 0.08203125, 0.4052734375, 0.056640625, 0.9970703125, 0.0390625, 0.060546875, 0.1142578125]

 sparsity of   [0.067274309694767, 0.967881977558136, 0.2330729216337204, 0.118055559694767, 0.1119791641831398, 0.1145833358168602, 0.9518229365348816, 0.1232638880610466, 0.9253472089767456, 0.0811631977558136, 0.0421006940305233, 0.01822916604578495, 0.0034722222480922937, 0.02387152798473835, 0.0542534738779068, 0.0303819440305233, 0.9322916865348816, 0.999131977558136, 0.0607638880610466, 0.0872395858168602, 0.0338541679084301, 0.0455729179084301, 0.0334201380610466, 0.0525173619389534, 0.0434027798473835, 0.1080729141831398, 0.00824652798473835, 0.0212673619389534, 0.9192708134651184, 0.4418402910232544, 0.007378472480922937, 0.014756944961845875, 0.7955729365348816, 0.999131977558136, 0.0846354141831398, 0.0447048619389534, 0.086805559694767, 0.01605902798473835, 0.02994791604578495, 0.9986979365348816, 0.0342881940305233, 0.0347222238779068, 0.0282118059694767, 0.01909722201526165, 0.0598958320915699, 0.0212673619389534, 0.0264756940305233, 0.0394965298473835, 0.0438368059694767, 0.0360243059694767, 0.015625, 0.0164930559694767, 0.02170138992369175, 0.0325520820915699, 0.013454861007630825, 0.1362847238779068, 0.0755208358168602, 0.0490451380610466, 0.999131977558136, 0.0464409738779068, 0.00824652798473835, 0.0460069440305233, 0.0681423619389534, 0.0, 0.1293402761220932, 0.999131977558136, 0.01996527798473835, 0.07421875, 0.8363715410232544, 0.0638020858168602, 0.0234375, 0.03081597201526165, 0.01822916604578495, 0.02777777798473835, 0.0282118059694767, 0.02777777798473835, 0.094618059694767, 0.0407986119389534, 0.9774305820465088, 0.9231770634651184, 0.01953125, 0.0334201380610466, 0.01779513992369175, 0.0225694440305233, 0.0594618059694767, 0.02604166604578495, 0.1623263955116272, 0.063368059694767, 0.9118923544883728, 0.0425347238779068, 0.0451388880610466, 0.01909722201526165, 0.02213541604578495, 0.9296875, 0.0325520820915699, 0.02473958395421505, 0.2473958283662796, 0.01519097201526165, 0.999131977558136, 0.6948784589767456, 0.03515625, 0.69140625, 0.9431423544883728, 0.0282118059694767, 0.0763888880610466, 0.0707465261220932, 0.009114583022892475, 0.9253472089767456, 0.0243055559694767, 0.0911458358168602, 0.0321180559694767, 0.2595486044883728, 0.01128472201526165, 0.1393229216337204, 0.2608506977558136, 0.0325520820915699, 0.0486111119389534, 0.02734375, 0.0503472238779068, 0.0321180559694767, 0.03081597201526165, 0.02473958395421505, 0.2842881977558136, 0.0325520820915699, 0.0473090298473835, 0.5980902910232544, 0.005642361007630825, 0.014322916977107525, 0.0993923619389534, 0.01215277798473835, 0.1197916641831398, 0.19140625, 0.0759548619389534, 0.0477430559694767, 0.0377604179084301, 0.999131977558136, 0.2339409738779068, 0.0347222238779068, 0.0538194440305233, 0.90625, 0.02604166604578495, 0.0703125, 0.0499131940305233, 0.0225694440305233, 0.0677083358168602, 0.0577256940305233, 0.0403645820915699, 0.0438368059694767, 0.01692708395421505, 0.0763888880610466, 0.9995659589767456, 0.0451388880610466, 0.0598958320915699, 0.01605902798473835, 0.01605902798473835, 0.01953125, 0.0546875, 0.071180559694767, 0.0503472238779068, 0.02734375, 0.0629340261220932, 0.9800347089767456, 0.0, 0.0768229141831398, 0.1206597238779068, 0.0690104141831398, 0.7964409589767456, 0.0342881940305233, 0.0399305559694767, 0.0438368059694767, 0.0212673619389534, 0.0173611119389534, 0.02690972201526165, 0.0503472238779068, 0.0173611119389534, 0.0264756940305233, 0.02734375, 0.9006076455116272, 0.0464409738779068, 0.9375, 0.1032986119389534, 0.0703125, 0.02300347201526165, 0.9157986044883728, 0.0438368059694767, 0.8836805820465088, 0.1193576380610466, 0.01692708395421505, 0.03515625, 0.1497395783662796, 0.0520833320915699, 0.00390625, 0.0434027798473835, 0.0438368059694767, 0.0972222238779068, 0.0386284738779068, 0.0342881940305233, 0.1245659738779068, 0.05078125, 0.0447048619389534, 0.850694477558136, 0.1137152761220932, 0.807725727558136, 0.999131977558136, 0.1119791641831398, 0.0481770820915699, 0.7738715410232544, 0.1193576380610466, 0.0559895820915699, 0.6953125, 0.007378472480922937, 0.0303819440305233, 0.2938368022441864, 0.0360243059694767, 0.01128472201526165, 0.0373263880610466, 0.0581597238779068, 0.0425347238779068, 0.002170138992369175, 0.9461805820465088, 0.999131977558136, 0.0316840298473835, 0.9296875, 0.999131977558136, 0.0334201380610466, 0.0464409738779068, 0.0794270858168602, 0.014322916977107525, 0.0403645820915699, 0.2200520783662796, 0.9986979365348816, 0.0282118059694767, 0.0377604179084301, 0.243923619389534, 0.9986979365348816, 0.109375, 0.009114583022892475, 0.893663227558136, 0.8693576455116272, 0.0264756940305233, 0.9223090410232544, 0.0234375, 0.01996527798473835, 0.03125, 0.014756944961845875, 0.1072048619389534, 0.03125, 0.0533854179084301, 0.885850727558136, 0.0716145858168602, 0.09375, 0.0303819440305233, 0.0703125, 0.0464409738779068, 0.02473958395421505, 0.114149309694767]

 sparsity of   [0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.0, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.984375, 0.9765625, 0.99609375, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98046875, 0.98828125, 0.9765625, 0.9921875, 0.9921875, 0.97265625, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.99609375, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.96875, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.96875, 0.99609375, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.98046875, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.859375, 0.98046875, 0.98828125, 0.9921875, 0.9765625, 0.98828125, 0.98828125, 0.99609375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.97265625, 0.9765625, 0.9921875, 0.98046875, 0.9921875, 0.98828125, 0.9921875, 0.96484375, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9765625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.21484375, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98046875, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.96875, 0.99609375, 0.98828125, 0.98046875, 0.7109375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.96484375, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.9765625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98046875, 0.98046875, 0.98828125, 0.98046875, 0.98828125, 0.9765625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.99609375, 0.9921875, 0.96484375, 0.98828125, 0.98046875, 0.98046875, 0.98828125, 0.98828125, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.0, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.97265625, 0.98046875, 0.99609375, 0.984375, 0.98046875, 0.9765625, 0.99609375, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.9765625, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.97265625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.125, 0.984375, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.96484375, 0.9609375, 0.98828125, 0.984375, 0.98046875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98046875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.06640625, 0.98828125, 0.98828125, 0.99609375, 0.98046875, 0.98046875, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.99609375, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9765625, 0.984375, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.98046875, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.98828125, 0.98046875, 0.98828125, 0.9609375, 0.98828125, 0.98828125, 0.9921875, 0.0, 0.97265625, 0.98828125, 0.98828125, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.9921875, 0.98828125, 0.984375, 0.98046875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98046875, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.9921875, 0.95703125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.96875, 0.98828125, 0.99609375, 0.98828125, 0.98046875, 0.99609375, 0.984375, 0.9921875, 0.9921875, 0.9921875, 0.97265625, 0.9765625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.96484375, 0.98828125, 0.984375, 0.98828125, 0.9765625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.0, 0.98828125, 0.98828125, 0.97265625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9765625, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.984375, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.9765625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.984375, 0.98828125, 0.9765625, 0.9765625, 0.9921875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.96484375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98046875, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.96875, 0.98828125, 0.984375, 0.99609375, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.42578125, 0.98828125, 0.98828125, 0.96875, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.99609375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98046875, 0.0, 0.99609375, 0.98828125, 0.98046875, 0.98828125, 0.99609375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.0, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.97265625, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.98828125, 0.97265625, 0.99609375, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.99609375, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.96875, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.0, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.96484375, 0.98828125, 0.9921875, 0.98828125, 0.96875, 0.98046875, 0.984375, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.96484375, 0.984375, 0.98046875, 0.984375, 0.98828125, 0.98046875, 0.984375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.9921875, 0.99609375, 0.984375, 0.9921875, 0.96875, 0.99609375, 0.96875, 0.984375, 0.9921875, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.96875, 0.98828125, 0.98828125, 0.96875, 0.9921875, 0.98828125, 0.98828125, 0.16796875, 0.9765625, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.96875, 0.96875, 0.98828125, 0.98828125, 0.9609375, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98046875, 0.98828125, 0.96484375, 0.984375, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.96484375, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.99609375, 0.98828125, 0.96875, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.9609375, 0.9921875, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9609375, 0.0, 0.98046875, 0.98828125, 0.98046875, 0.98828125, 0.98046875, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.9921875, 0.96875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9609375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.14453125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.984375, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.9921875, 0.98828125, 0.9609375, 0.98828125, 0.9921875, 0.9921875, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.99609375, 0.99609375, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98046875, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.9921875, 0.98828125, 0.9765625, 0.98828125, 0.984375, 0.98828125, 0.9921875, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.9921875, 0.984375, 0.98046875, 0.98828125, 0.96484375, 0.99609375, 0.984375, 0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.984375, 0.98828125, 0.98828125, 0.98828125]

 sparsity of   [0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.439453125, 0.998046875, 0.9990234375, 0.9990234375, 0.9970703125, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.2177734375, 0.998046875, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.2451171875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.197265625, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.2177734375, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.21875, 0.9970703125, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.4072265625, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9970703125, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.23828125, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.41015625, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.2265625, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.99609375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.9990234375, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.4501953125, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.99609375, 0.99609375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.9990234375, 0.998046875, 0.998046875, 0.99609375, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.9990234375, 0.9970703125, 0.998046875, 0.998046875, 0.998046875, 0.9990234375, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875, 0.998046875]

 sparsity of   [0.9995659589767456, 0.2313368022441864, 0.9997829794883728, 0.9995659589767456, 0.0588107630610466, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3630642294883728, 0.2094184011220932, 0.9995659589767456, 0.9993489384651184, 0.0555555559694767, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.9997829794883728, 0.138454869389534, 0.9995659589767456, 0.9995659589767456, 0.1707899272441864, 0.1753472238779068, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.4626736044883728, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.169921875, 0.2584635317325592, 0.177517369389534, 0.9993489384651184, 0.175564244389534, 0.9997829794883728, 0.7892795205116272, 0.9997829794883728, 0.9995659589767456, 0.0930989608168602, 0.9997829794883728, 0.1729600727558136, 0.9995659589767456, 0.9997829794883728, 0.1460503488779068, 0.0904947891831398, 0.1458333283662796, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.8873698115348816, 0.323784738779068, 0.9995659589767456, 0.9993489384651184, 0.9993489384651184, 0.9995659589767456, 0.0974392369389534, 0.9995659589767456, 0.2280815988779068, 0.2367621511220932, 0.1174045130610466, 0.8493923544883728, 0.9995659589767456, 0.094618059694767, 0.0486111119389534, 0.9997829794883728, 0.3331163227558136, 0.8103298544883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3187934160232544, 0.9995659589767456, 0.2311197966337204, 0.9995659589767456, 0.9995659589767456, 0.1842447966337204, 0.440972238779068, 0.9993489384651184, 0.9995659589767456, 0.1380208283662796, 0.0792100727558136, 0.999131977558136, 0.1731770783662796, 0.8684895634651184, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.1072048619389534, 0.9995659589767456, 0.9995659589767456, 0.1768663227558136, 0.0629340261220932, 0.0870225727558136, 0.1245659738779068, 0.9997829794883728, 0.3005642294883728, 0.9997829794883728, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.1026475727558136, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.0748697891831398, 0.9993489384651184, 0.9995659589767456, 0.0588107630610466, 0.0865885391831398, 0.115234375, 0.9995659589767456, 0.9997829794883728, 0.1295572966337204, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.0837673619389534, 0.073133684694767, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.2591145932674408, 0.9995659589767456, 0.108289934694767, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.1130642369389534, 0.1864149272441864, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.3092447817325592, 0.1271701455116272, 0.8151041865348816, 0.1104600727558136, 0.9995659589767456, 0.9995659589767456, 0.0813802108168602, 0.2189670205116272, 0.4431423544883728, 0.9997829794883728, 0.3181423544883728, 0.9995659589767456, 0.1419270783662796, 0.9997829794883728, 0.0334201380610466, 0.9997829794883728, 0.8170573115348816, 0.483940988779068, 0.1226128488779068, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.2502170205116272, 0.9997829794883728, 0.9995659589767456, 0.3567708432674408, 0.9997829794883728, 0.7684462070465088, 0.9995659589767456, 0.1134982630610466, 0.8211805820465088, 0.9995659589767456, 0.2506510317325592, 0.0392795130610466, 0.9995659589767456, 0.9995659589767456, 0.7875434160232544, 0.072265625, 0.9995659589767456, 0.9995659589767456, 0.8185763955116272, 0.9997829794883728, 0.9995659589767456, 0.1469184011220932, 0.9995659589767456, 0.9997829794883728, 0.0470920130610466, 0.2335069477558136, 0.9997829794883728, 0.1451822966337204, 0.2328559011220932, 0.046875, 0.9993489384651184, 0.115234375, 0.9993489384651184, 0.9995659589767456, 0.1846788227558136, 0.1080729141831398, 0.2942708432674408, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.076171875, 0.39453125, 0.9995659589767456, 0.3988715410232544, 0.9997829794883728, 0.476128488779068, 0.1158854141831398, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.169704869389534, 0.9993489384651184, 0.2552083432674408, 0.8263888955116272, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.7795138955116272, 0.2337239533662796, 0.1076388880610466, 0.9995659589767456, 0.9997829794883728, 0.140625, 0.9993489384651184, 0.9995659589767456, 0.9993489384651184, 0.163845494389534, 0.1056857630610466, 0.078993059694767, 0.0622829869389534, 0.0772569477558136, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.9993489384651184, 0.1792534738779068, 0.9995659589767456, 0.9997829794883728, 0.2434895783662796, 0.9995659589767456, 0.9995659589767456, 0.4522569477558136, 0.2443576455116272, 0.9993489384651184, 0.9995659589767456, 0.1653645783662796, 0.9995659589767456, 0.9995659589767456, 0.1525607705116272, 0.9997829794883728, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.2447916716337204, 0.0583767369389534, 0.3294270932674408, 0.1245659738779068, 0.9997829794883728, 0.9995659589767456, 0.2354600727558136, 0.1870659738779068, 0.8426649570465088, 0.0570746548473835, 0.9997829794883728, 0.9995659589767456, 0.0568576380610466, 0.9995659589767456, 0.0494791679084301, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.2513020932674408, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.9997829794883728, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.3192274272441864, 0.9993489384651184, 0.9993489384651184, 0.9997829794883728, 0.0483940988779068, 0.9995659589767456, 0.9997829794883728, 0.9993489384651184, 0.9997829794883728, 0.1145833358168602, 0.2319878488779068, 0.9993489384651184, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.0358072929084301, 0.9997829794883728, 0.9995659589767456, 0.9997829794883728, 0.0531684048473835, 0.9995659589767456, 0.9995659589767456, 0.2102864533662796, 0.9993489384651184, 0.0872395858168602, 0.9995659589767456, 0.2452256977558136, 0.2489149272441864, 0.0961371511220932, 0.9995659589767456, 0.0774739608168602, 0.0596788190305233, 0.0646701380610466, 0.9995659589767456, 0.0464409738779068, 0.9995659589767456, 0.999131977558136, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.1436631977558136, 0.8359375, 0.029296875, 0.9997829794883728, 0.3289930522441864, 0.9995659589767456, 0.9993489384651184, 0.9997829794883728, 0.2417534738779068, 0.9995659589767456, 0.0842013880610466, 0.1039496511220932, 0.9997829794883728, 0.9995659589767456, 0.241970494389534, 0.9995659589767456, 0.2727864682674408, 0.0679253488779068, 0.0670572891831398, 0.9995659589767456, 0.9995659589767456, 0.3166232705116272, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.9997829794883728, 0.9997829794883728, 0.9995659589767456, 0.0948350727558136, 0.9995659589767456, 0.0687934011220932, 0.071180559694767, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.1095920130610466, 0.9997829794883728, 0.2628038227558136, 0.8309462070465088, 0.1017795130610466, 0.2081163227558136, 0.3337673544883728, 0.0857204869389534, 0.7944878339767456, 0.9997829794883728, 0.9997829794883728, 0.1089409738779068, 0.3463541567325592, 0.0475260429084301, 0.4286024272441864, 0.9995659589767456, 0.1334635466337204, 0.9993489384651184, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.1258680522441864, 0.9995659589767456, 0.1805555522441864, 0.1069878488779068, 0.9997829794883728, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.2443576455116272, 0.9995659589767456, 0.102430559694767, 0.9995659589767456, 0.1338975727558136, 0.9995659589767456, 0.134765625, 0.9993489384651184, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9997829794883728, 0.9997829794883728, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.9995659589767456, 0.3587239682674408, 0.9997829794883728, 0.1647135466337204, 0.1215277761220932, 0.9993489384651184, 0.9997829794883728, 0.9995659589767456, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.0998263880610466, 0.1861979216337204, 0.0811631977558136, 0.1586371511220932, 0.9995659589767456, 0.3899739682674408, 0.7936198115348816, 0.1603732705116272, 0.1399739533662796, 0.206814244389534, 0.173611119389534, 0.9997829794883728, 0.2682291567325592, 0.9997829794883728, 0.9995659589767456, 0.08203125, 0.0431857630610466, 0.041015625, 0.2430555522441864, 0.3118489682674408, 0.2371961772441864, 0.8044704794883728, 0.9995659589767456, 0.9995659589767456, 0.243923619389534, 0.0407986119389534, 0.9995659589767456, 0.1360677033662796, 0.9997829794883728, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.9993489384651184, 0.1241319477558136, 0.9995659589767456, 0.9997829794883728, 0.9993489384651184, 0.9995659589767456, 0.1631944477558136, 0.3020833432674408, 0.9995659589767456, 0.9997829794883728, 0.1006944477558136, 0.9995659589767456, 0.9995659589767456, 0.9993489384651184, 0.1182725727558136, 0.9997829794883728, 0.9995659589767456, 0.9993489384651184, 0.9995659589767456, 0.9997829794883728, 0.1623263955116272, 0.1825086772441864, 0.1273871511220932, 0.7994791865348816, 0.3322482705116272, 0.9995659589767456, 0.9997829794883728, 0.0729166641831398, 0.9995659589767456, 0.9995659589767456, 0.0837673619389534, 0.9995659589767456, 0.2667100727558136, 0.0379774309694767, 0.9993489384651184, 0.189453125, 0.9995659589767456, 0.0486111119389534, 0.9995659589767456, 0.0427517369389534, 0.9997829794883728, 0.3289930522441864, 0.9993489384651184, 0.8200954794883728]

 sparsity of   [0.99609375, 0.021484375, 0.04296875, 0.033203125, 0.994140625, 0.109375, 0.994140625, 0.03515625, 0.99609375, 0.16015625, 0.02734375, 0.9375, 0.39453125, 0.0078125, 0.99609375, 0.0078125, 0.828125, 0.048828125, 0.99609375, 0.99609375, 0.908203125, 0.02734375, 0.998046875, 0.0390625, 0.029296875, 0.103515625, 0.0390625, 0.90234375, 0.037109375, 0.025390625, 0.1171875, 0.076171875, 0.056640625, 0.0546875, 0.994140625, 0.015625, 0.1484375, 0.015625, 0.048828125, 0.044921875, 0.0546875, 0.017578125, 0.99609375, 0.033203125, 0.107421875, 0.04296875, 0.025390625, 0.99609375, 0.99609375, 0.994140625, 0.99609375, 0.998046875, 0.99609375, 0.025390625, 0.037109375, 0.994140625, 0.99609375, 0.99609375, 0.046875, 0.029296875, 0.99609375, 0.958984375, 0.05859375, 0.02734375, 0.029296875, 0.029296875, 0.095703125, 0.99609375, 0.9296875, 0.02734375, 0.078125, 0.048828125, 0.052734375, 0.046875, 0.99609375, 0.8125, 0.0390625, 0.99609375, 0.99609375, 0.015625, 0.033203125, 0.046875, 0.333984375, 0.0390625, 0.103515625, 0.029296875, 0.01953125, 0.857421875, 0.994140625, 0.158203125, 0.8046875, 0.04296875, 0.84375, 0.01171875, 0.044921875, 0.037109375, 0.015625, 0.03515625, 0.03515625, 0.99609375, 0.03125, 0.076171875, 0.994140625, 0.107421875, 0.068359375, 0.109375, 0.03125, 0.01171875, 0.02734375, 0.994140625, 0.84765625, 0.8359375, 0.99609375, 0.927734375, 0.095703125, 0.048828125, 0.8046875, 0.994140625, 0.017578125, 0.99609375, 0.99609375, 0.994140625, 0.052734375, 0.09765625, 0.99609375, 0.029296875, 0.037109375, 0.15625, 0.94921875, 0.953125, 0.99609375, 0.0546875, 0.154296875, 0.99609375, 0.02734375, 0.029296875, 0.033203125, 0.185546875, 0.078125, 0.111328125, 0.994140625, 0.068359375, 0.015625, 0.994140625, 0.056640625, 0.99609375, 0.99609375, 0.03515625, 0.033203125, 0.62109375, 0.99609375, 0.068359375, 0.044921875, 0.087890625, 0.060546875, 0.048828125, 0.8515625, 0.044921875, 0.79296875, 0.99609375, 0.03125, 0.005859375, 0.01953125, 0.06640625, 0.048828125, 0.99609375, 0.048828125, 0.0390625, 0.666015625, 0.09375, 0.994140625, 0.03125, 0.048828125, 0.080078125, 0.85546875, 0.095703125, 0.0703125, 0.99609375, 0.12890625, 0.99609375, 0.994140625, 0.0390625, 0.134765625, 0.890625, 0.060546875, 0.041015625, 0.009765625, 0.99609375, 0.091796875, 0.99609375, 0.060546875, 0.095703125, 0.078125, 0.83984375, 0.931640625, 0.994140625, 0.99609375, 0.08203125, 0.07421875, 0.11328125, 0.994140625, 0.052734375, 0.029296875, 0.0390625, 0.99609375, 0.056640625, 0.0234375, 0.013671875, 0.052734375, 0.0234375, 0.103515625, 0.955078125, 0.826171875, 0.05859375, 0.99609375, 0.158203125, 0.99609375, 0.048828125, 0.884765625, 0.025390625, 0.125, 0.078125, 0.03125, 0.017578125, 0.830078125, 0.087890625, 0.02734375, 0.103515625, 0.017578125, 0.994140625, 0.103515625, 0.99609375, 0.044921875, 0.052734375, 0.880859375, 0.05078125, 0.017578125, 0.728515625, 0.833984375, 0.99609375, 0.0390625, 0.00390625, 0.05078125, 0.994140625, 0.99609375, 0.0234375, 0.0078125, 0.99609375, 0.044921875, 0.994140625, 0.033203125, 0.00390625, 0.021484375, 0.99609375, 0.025390625, 0.99609375, 0.056640625, 0.994140625, 0.994140625, 0.03125, 0.27734375, 0.11328125, 0.146484375, 0.072265625, 0.060546875, 0.99609375, 0.99609375, 0.029296875, 0.99609375, 0.083984375, 0.994140625, 0.056640625, 0.99609375, 0.99609375, 0.033203125, 0.0234375, 0.958984375, 0.021484375, 0.99609375, 0.99609375, 0.822265625, 0.05078125, 0.99609375, 0.857421875, 0.99609375, 0.037109375, 0.125, 0.025390625, 0.017578125, 0.953125, 0.99609375, 0.994140625, 0.99609375, 0.994140625, 0.99609375, 0.876953125, 0.03125, 0.029296875, 0.99609375, 0.041015625, 0.08203125, 0.2578125, 0.056640625, 0.037109375, 0.115234375, 0.99609375, 0.99609375, 0.046875, 0.82421875, 0.998046875, 0.19140625, 0.09765625, 0.185546875, 0.033203125, 0.052734375, 0.025390625, 0.830078125, 0.890625, 0.05078125, 0.033203125, 0.0234375, 0.03125, 0.087890625, 0.994140625, 0.99609375, 0.994140625, 0.025390625, 0.146484375, 0.029296875, 0.087890625, 0.99609375, 0.09375, 0.994140625, 0.02734375, 0.0390625, 0.060546875, 0.0703125, 0.0, 0.041015625, 0.0390625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.044921875, 0.091796875, 0.99609375, 0.205078125, 0.994140625, 0.021484375, 0.947265625, 0.025390625, 0.99609375, 0.0234375, 0.970703125, 0.0078125, 0.025390625, 0.146484375, 0.0390625, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.8515625, 0.955078125, 0.04296875, 0.1328125, 0.99609375, 0.087890625, 0.99609375, 0.99609375, 0.025390625, 0.115234375, 0.025390625, 0.869140625, 0.07421875, 0.99609375, 0.076171875, 0.041015625, 0.908203125, 0.99609375, 0.033203125, 0.0625, 0.140625, 0.759765625, 0.8203125, 0.091796875, 0.056640625, 0.822265625, 0.9921875, 0.091796875, 0.025390625, 0.025390625, 0.99609375, 0.06640625, 0.001953125, 0.0390625, 0.99609375, 0.068359375, 0.51171875, 0.99609375, 0.04296875, 0.99609375, 0.04296875, 0.046875, 0.99609375, 0.021484375, 0.140625, 0.046875, 0.99609375, 0.029296875, 0.806640625, 0.99609375, 0.056640625, 0.99609375, 0.02734375, 0.025390625, 0.03515625, 0.99609375, 0.13671875, 0.072265625, 0.791015625, 0.994140625, 0.0546875, 0.021484375, 0.935546875, 0.06640625, 0.083984375, 0.99609375, 0.0390625, 0.232421875, 0.99609375, 0.9921875, 0.03515625, 0.1171875, 0.12109375, 0.99609375, 0.044921875, 0.99609375, 0.05078125, 0.052734375, 0.9609375, 0.99609375, 0.091796875, 0.99609375, 0.015625, 0.037109375, 0.998046875, 0.99609375, 0.181640625, 0.99609375, 0.99609375, 0.177734375, 0.966796875, 0.994140625, 0.8828125, 0.033203125, 0.994140625, 0.037109375, 0.138671875, 0.0625, 0.10546875, 0.02734375, 0.052734375, 0.99609375, 0.041015625, 0.041015625, 0.99609375, 0.94921875, 0.05078125, 0.001953125, 0.09375, 0.013671875, 0.998046875, 0.05078125, 0.017578125, 0.99609375, 0.0625, 0.09375, 0.69921875, 0.060546875, 0.9296875, 0.02734375, 0.048828125, 0.931640625, 0.99609375, 0.994140625, 0.994140625, 0.126953125, 0.03515625, 0.994140625, 0.234375, 0.068359375, 0.99609375, 0.99609375, 0.125, 0.994140625, 0.99609375, 0.05078125, 0.083984375, 0.99609375, 0.087890625, 0.044921875, 0.052734375, 0.994140625, 0.05859375, 0.033203125, 0.828125, 0.994140625, 0.05859375, 0.97265625, 0.99609375, 0.994140625, 0.03515625, 0.01953125, 0.0703125, 0.06640625, 0.890625, 0.3046875, 0.994140625, 0.896484375, 0.99609375, 0.99609375, 0.02734375, 0.033203125, 0.025390625, 0.134765625, 0.99609375, 0.060546875, 0.03125, 0.99609375, 0.99609375, 0.150390625, 0.994140625, 0.10546875, 0.99609375, 0.009765625, 0.99609375, 0.060546875, 0.01953125, 0.8828125, 0.99609375, 0.99609375, 0.90625, 0.0234375, 0.025390625, 0.048828125, 0.103515625, 0.845703125, 0.09765625, 0.046875, 0.03515625, 0.017578125, 0.029296875, 0.048828125, 0.041015625, 0.99609375, 0.99609375, 0.017578125, 0.09765625, 0.994140625, 0.046875, 0.994140625, 0.931640625, 0.90234375, 0.0234375, 0.208984375, 0.076171875, 0.90234375, 0.994140625, 0.994140625, 0.966796875, 0.99609375, 0.048828125, 0.03515625, 0.080078125, 0.103515625, 0.0234375, 0.99609375, 0.05078125, 0.158203125, 0.04296875, 0.033203125, 0.109375, 0.0234375, 0.021484375, 0.99609375, 0.091796875, 0.015625, 0.080078125, 0.091796875, 0.994140625, 0.861328125, 0.072265625, 0.99609375, 0.03515625, 0.0390625, 0.029296875, 0.125, 0.0234375, 0.01953125, 0.99609375, 0.095703125, 0.3125, 0.9375, 0.99609375, 0.03125, 0.05859375, 0.93359375, 0.99609375, 0.087890625, 0.99609375, 0.02734375, 0.17578125, 0.03515625, 0.99609375, 0.99609375, 0.912109375, 0.0390625, 0.033203125, 0.99609375, 0.99609375, 0.99609375, 0.03125, 0.125, 0.037109375, 0.875, 0.99609375, 0.12890625, 0.931640625, 0.033203125, 0.046875, 0.0703125, 0.947265625, 0.08984375, 0.0625, 0.8203125, 0.09375, 0.994140625, 0.01953125, 0.1015625, 0.1015625, 0.99609375, 0.814453125, 0.794921875, 0.208984375, 0.07421875, 0.111328125, 0.99609375, 0.02734375, 0.216796875, 0.087890625, 0.03515625, 0.119140625, 0.103515625, 0.99609375, 0.056640625, 0.037109375, 0.087890625, 0.189453125, 0.06640625, 0.029296875, 0.994140625, 0.99609375, 0.13671875, 0.99609375, 0.0390625, 0.02734375, 0.021484375, 0.99609375, 0.93359375, 0.916015625, 0.744140625, 0.998046875, 0.2109375, 0.083984375, 0.994140625, 0.05078125, 0.029296875, 0.03125, 0.04296875, 0.8984375, 0.994140625, 0.015625, 0.99609375, 0.99609375, 0.0078125, 0.076171875, 0.0390625, 0.05859375, 0.02734375, 0.99609375, 0.046875, 0.080078125, 0.029296875, 0.99609375, 0.197265625, 0.99609375, 0.99609375, 0.029296875, 0.73046875, 0.025390625, 0.99609375, 0.9296875, 0.99609375, 0.994140625, 0.806640625, 0.01953125, 0.021484375, 0.021484375, 0.9375, 0.029296875, 0.0234375, 0.048828125, 0.078125, 0.1171875, 0.015625, 0.11328125, 0.99609375, 0.99609375, 0.904296875, 0.08984375, 0.02734375, 0.91015625, 0.90625, 0.83984375, 0.107421875, 0.052734375, 0.99609375, 0.958984375, 0.08984375, 0.99609375, 0.05078125, 0.99609375, 0.08984375, 0.994140625, 0.068359375, 0.037109375, 0.994140625, 0.025390625, 0.99609375, 0.052734375, 0.05078125, 0.013671875, 0.087890625, 0.99609375, 0.017578125, 0.0390625, 0.99609375, 0.998046875, 0.99609375, 0.037109375, 0.01171875, 0.01171875, 0.994140625, 0.01171875, 0.052734375, 0.060546875, 0.99609375, 0.021484375, 0.947265625, 0.1015625, 0.9609375, 0.037109375, 0.95703125, 0.04296875, 0.05078125, 0.99609375, 0.029296875, 0.025390625, 0.994140625, 0.044921875, 0.99609375, 0.087890625, 0.03125, 0.052734375, 0.99609375, 0.99609375, 0.80859375, 0.01171875, 0.02734375, 0.955078125, 0.818359375, 0.99609375, 0.99609375, 0.111328125, 0.029296875, 0.99609375, 0.99609375, 0.025390625, 0.99609375, 0.03125, 0.189453125, 0.99609375, 0.0234375, 0.072265625, 0.998046875, 0.94140625, 0.083984375, 0.037109375, 0.99609375, 0.046875, 0.99609375, 0.99609375, 0.994140625, 0.91796875, 0.103515625, 0.087890625, 0.125, 0.033203125, 0.037109375, 0.953125, 0.99609375, 0.99609375, 0.994140625, 0.03515625, 0.150390625, 0.037109375, 0.044921875, 0.123046875, 0.080078125, 0.99609375, 0.8515625, 0.888671875, 0.041015625, 0.076171875, 0.0234375, 0.994140625, 0.99609375, 0.0859375, 0.07421875, 0.99609375, 0.005859375, 0.99609375, 0.02734375, 0.994140625, 0.99609375, 0.060546875, 0.99609375, 0.048828125, 0.99609375, 0.123046875, 0.234375, 0.080078125, 0.99609375, 0.04296875, 0.021484375, 0.99609375, 0.044921875, 0.994140625, 0.99609375, 0.994140625, 0.99609375, 0.99609375, 0.06640625, 0.083984375, 0.087890625, 0.845703125, 0.056640625, 0.07421875, 0.052734375, 0.087890625, 0.033203125, 0.044921875, 0.994140625, 0.99609375, 0.0390625, 0.994140625, 0.04296875, 0.037109375, 0.03515625, 0.078125, 0.021484375, 0.017578125, 0.994140625, 0.99609375, 0.134765625, 0.998046875, 0.1484375, 0.09765625, 0.994140625, 0.03515625, 0.99609375, 0.064453125, 0.240234375, 0.017578125, 0.046875, 0.068359375, 0.998046875, 0.009765625, 0.99609375, 0.033203125, 0.09765625, 0.03515625, 0.935546875, 0.06640625, 0.033203125, 0.03125, 0.025390625, 0.12109375, 0.9453125, 0.99609375, 0.025390625, 0.11328125, 0.994140625, 0.87109375, 0.130859375, 0.99609375, 0.087890625, 0.255859375, 0.12890625, 0.056640625, 0.076171875, 0.029296875, 0.078125, 0.994140625, 0.0234375, 0.015625, 0.048828125, 0.994140625, 0.095703125, 0.615234375, 0.064453125, 0.04296875, 0.046875, 0.1171875, 0.99609375, 0.259765625, 0.99609375, 0.99609375, 0.080078125, 0.08203125, 0.02734375, 0.9296875, 0.935546875, 0.10546875, 0.029296875, 0.958984375, 0.99609375, 0.837890625, 0.9921875, 0.111328125, 0.0859375, 0.99609375, 0.99609375, 0.03125, 0.998046875, 0.994140625, 0.0234375, 0.833984375, 0.994140625, 0.798828125, 0.99609375, 0.99609375, 0.99609375, 0.076171875, 0.0625, 0.046875, 0.029296875, 0.048828125, 0.033203125, 0.99609375, 0.046875, 0.025390625, 0.99609375, 0.029296875, 0.10546875, 0.99609375, 0.033203125, 0.044921875, 0.068359375, 0.951171875, 0.994140625, 0.025390625, 0.0234375, 0.166015625, 0.99609375, 0.994140625, 0.068359375, 0.046875, 0.99609375, 0.021484375, 0.15234375, 0.99609375, 0.099609375, 0.095703125, 0.044921875, 0.99609375, 0.99609375, 0.99609375, 0.046875, 0.07421875, 0.99609375, 0.859375, 0.08203125, 0.06640625, 0.078125, 0.044921875, 0.01953125, 0.998046875, 0.99609375, 0.146484375, 0.0234375, 0.03125, 0.03515625, 0.046875, 0.99609375, 0.9140625, 0.0703125, 0.99609375, 0.052734375, 0.09765625, 0.810546875, 0.02734375, 0.99609375, 0.08984375, 0.275390625, 0.99609375, 0.994140625, 0.025390625, 0.06640625, 0.046875, 0.0625, 0.111328125, 0.07421875, 0.10546875, 0.962890625, 0.12109375, 0.05078125, 0.052734375, 0.853515625, 0.78125, 0.99609375, 0.09375, 0.923828125, 0.134765625, 0.052734375, 0.99609375, 0.01953125, 0.02734375, 0.99609375, 0.080078125, 0.048828125, 0.99609375, 0.80859375, 0.919921875, 0.02734375, 0.998046875, 0.072265625, 0.111328125, 0.99609375, 0.99609375, 0.998046875, 0.794921875, 0.0859375, 0.03515625, 0.029296875, 0.0078125, 0.763671875, 0.0859375, 0.0703125, 0.015625, 0.99609375, 0.951171875, 0.02734375, 0.060546875, 0.99609375, 0.99609375, 0.162109375, 0.02734375, 0.115234375, 0.99609375, 0.056640625, 0.06640625, 0.810546875, 0.072265625, 0.0546875, 0.087890625, 0.99609375, 0.017578125, 0.998046875, 0.0234375, 0.99609375, 0.021484375, 0.177734375, 0.982421875, 0.03125, 0.0390625, 0.99609375, 0.03515625, 0.9453125, 0.998046875, 0.99609375, 0.85546875, 0.03125, 0.068359375, 0.994140625, 0.04296875, 0.947265625, 0.013671875, 0.005859375, 0.99609375, 0.99609375, 0.99609375, 0.0390625, 0.99609375, 0.9296875, 0.99609375, 0.03515625, 0.99609375, 0.080078125, 0.140625, 0.123046875, 0.021484375, 0.021484375, 0.99609375, 0.0546875, 0.955078125, 0.99609375, 0.037109375, 0.994140625, 0.99609375, 0.015625, 0.916015625, 0.052734375, 0.99609375, 0.052734375, 0.953125, 0.99609375, 0.041015625, 0.05859375, 0.08203125, 0.99609375, 0.056640625, 0.056640625, 0.03515625, 0.029296875, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.041015625, 0.99609375, 0.119140625, 0.126953125, 0.994140625, 0.99609375, 0.99609375, 0.04296875, 0.0390625, 0.08203125, 0.287109375, 0.095703125, 0.03125, 0.078125, 0.076171875, 0.99609375, 0.015625, 0.03125, 0.015625, 0.091796875, 0.998046875, 0.08203125, 0.99609375, 0.111328125, 0.0078125, 0.0625, 0.021484375, 0.06640625, 0.0625, 0.99609375, 0.060546875, 0.025390625, 0.99609375, 0.029296875, 0.0234375, 0.02734375, 0.025390625, 0.140625, 0.99609375, 0.041015625, 0.99609375, 0.09765625, 0.03515625, 0.865234375, 0.994140625, 0.021484375, 0.96484375, 0.029296875, 0.126953125, 0.99609375, 0.9453125, 0.99609375, 0.01953125, 0.03515625, 0.060546875, 0.99609375, 0.08984375, 0.865234375, 0.99609375, 0.998046875, 0.029296875, 0.880859375, 0.99609375, 0.046875, 0.119140625, 0.958984375, 0.99609375, 0.0859375, 0.1171875, 0.23046875, 0.99609375, 0.015625, 0.099609375, 0.05859375, 0.01953125, 0.400390625, 0.037109375, 0.99609375, 0.99609375, 0.08984375, 0.994140625, 0.03125, 0.99609375, 0.017578125, 0.0078125, 0.021484375, 0.93359375, 0.005859375, 0.017578125, 0.890625, 0.962890625, 0.90234375, 0.0234375, 0.009765625, 0.919921875, 0.95703125, 0.04296875, 0.828125, 0.99609375, 0.033203125, 0.025390625, 0.9140625, 0.111328125, 0.052734375, 0.041015625, 0.015625, 0.740234375, 0.994140625, 0.99609375, 0.134765625, 0.99609375, 0.99609375, 0.005859375, 0.095703125, 0.111328125, 0.03125, 0.0390625, 0.1328125, 0.005859375, 0.99609375, 0.99609375, 0.99609375, 0.05078125, 0.35546875, 0.05859375, 0.916015625, 0.95703125, 0.99609375, 0.0234375, 0.791015625, 0.0234375, 0.99609375, 0.03515625, 0.99609375, 0.994140625, 0.1328125, 0.1328125, 0.994140625, 0.078125, 0.99609375, 0.99609375, 0.99609375, 0.0703125, 0.900390625, 0.068359375, 0.248046875, 0.0390625, 0.05859375, 0.99609375, 0.755859375, 0.05859375, 0.0859375, 0.017578125, 0.99609375, 0.84765625, 0.99609375, 0.0390625, 0.015625, 0.99609375, 0.025390625, 0.087890625, 0.994140625, 0.99609375, 0.833984375, 0.009765625, 0.99609375, 0.177734375, 0.009765625, 0.021484375, 0.115234375, 0.796875, 0.99609375, 0.041015625, 0.01953125, 0.994140625, 0.033203125, 0.125, 0.041015625, 0.021484375, 0.03515625, 0.99609375, 0.060546875, 0.998046875, 0.05078125, 0.99609375, 0.044921875, 0.0390625, 0.056640625, 0.06640625, 0.044921875, 0.03125, 0.046875, 0.037109375, 0.99609375, 0.99609375, 0.994140625, 0.99609375, 0.068359375, 0.08203125, 0.025390625, 0.99609375, 0.99609375, 0.03125, 0.033203125, 0.01953125, 0.99609375, 0.03125, 0.025390625, 0.01953125, 0.146484375, 0.0234375, 0.021484375, 0.99609375, 0.908203125, 0.048828125, 0.0390625, 0.01953125, 0.994140625, 0.99609375, 0.060546875, 0.046875, 0.99609375, 0.017578125, 0.05859375, 0.08203125, 0.92578125, 0.025390625, 0.03125, 0.150390625, 0.99609375, 0.849609375, 0.025390625, 0.1640625, 0.025390625, 0.0703125, 0.994140625, 0.99609375, 0.119140625, 0.0625, 0.99609375, 0.0546875, 0.0234375, 0.021484375, 0.083984375, 0.99609375, 0.8125, 0.091796875, 0.017578125, 0.146484375, 0.03515625, 0.017578125, 0.09375, 0.77734375, 0.28125, 0.125, 0.037109375, 0.068359375, 0.0390625, 0.146484375, 0.99609375, 0.03515625, 0.130859375, 0.994140625, 0.07421875, 0.119140625, 0.05859375, 0.99609375, 0.9921875, 0.904296875, 0.107421875, 0.0625, 0.99609375, 0.048828125, 0.033203125, 0.96484375, 0.130859375, 0.052734375, 0.046875, 0.0390625, 0.048828125, 0.994140625, 0.052734375, 0.029296875, 0.8671875, 0.994140625, 0.12109375, 0.0234375, 0.03515625, 0.8515625, 0.037109375, 0.01171875, 0.044921875, 0.02734375, 0.169921875, 0.041015625, 0.130859375, 0.115234375, 0.009765625, 0.994140625, 0.310546875, 0.994140625, 0.025390625, 0.046875, 0.99609375, 0.056640625, 0.025390625, 0.99609375, 0.880859375, 0.103515625, 0.046875, 0.939453125, 0.01953125, 0.0625, 0.99609375, 0.107421875, 0.04296875, 0.99609375, 0.068359375, 0.01953125, 0.890625, 0.99609375, 0.99609375, 0.99609375, 0.994140625, 0.08203125, 0.1015625, 0.015625, 0.99609375, 0.99609375, 0.99609375, 0.02734375, 0.99609375, 0.150390625, 0.01171875, 0.09375, 0.94921875, 0.03125, 0.025390625, 0.08203125, 0.029296875, 0.10546875, 0.99609375, 0.0078125, 0.99609375, 0.994140625, 0.03515625, 0.0, 0.99609375, 0.052734375, 0.017578125, 0.99609375, 0.99609375, 0.0234375, 0.99609375, 0.05859375, 0.99609375, 0.087890625, 0.998046875, 0.04296875, 0.033203125, 0.99609375, 0.76953125, 0.064453125, 0.99609375, 0.052734375, 0.0234375, 0.01953125, 0.10546875, 0.99609375, 0.009765625, 0.99609375, 0.01171875, 0.99609375, 0.015625, 0.056640625, 0.013671875, 0.271484375, 0.16796875, 0.072265625, 0.99609375, 0.029296875, 0.140625, 0.052734375, 0.01953125, 0.8203125, 0.994140625, 0.02734375, 0.03125, 0.052734375, 0.99609375, 0.99609375, 0.048828125, 0.017578125, 0.080078125, 0.025390625, 0.0234375, 0.046875, 0.99609375, 0.056640625, 0.876953125, 0.02734375, 0.099609375, 0.015625, 0.037109375, 0.1484375, 0.01171875, 0.99609375, 0.04296875, 0.056640625, 0.99609375, 0.06640625, 0.98046875, 0.953125, 0.99609375, 0.056640625, 0.99609375, 0.99609375, 0.0546875, 0.99609375, 0.99609375, 0.994140625, 0.025390625, 0.0625, 0.0859375, 0.080078125, 0.025390625, 0.99609375, 0.0625, 0.109375, 0.99609375, 0.99609375, 0.09765625, 0.94921875, 0.060546875, 0.064453125, 0.052734375, 0.02734375, 0.021484375, 0.07421875, 0.037109375, 0.029296875, 0.994140625, 0.015625, 0.99609375, 0.041015625, 0.833984375, 0.9140625, 0.99609375, 0.04296875, 0.10546875, 0.03125, 0.8359375, 0.03515625, 0.0546875, 0.046875, 0.15234375, 0.99609375, 0.99609375, 0.013671875, 0.99609375, 0.056640625, 0.994140625, 0.994140625, 0.068359375, 0.931640625, 0.03125, 0.99609375, 0.056640625, 0.99609375, 0.17578125, 0.048828125, 0.044921875, 0.107421875, 0.822265625, 0.99609375, 0.09375, 0.99609375, 0.0390625, 0.99609375, 0.177734375, 0.017578125, 0.1796875, 0.015625, 0.994140625, 0.033203125, 0.994140625, 0.029296875, 0.07421875, 0.953125, 0.029296875, 0.99609375, 0.134765625, 0.044921875, 0.01171875, 0.994140625, 0.994140625, 0.021484375, 0.99609375, 0.99609375, 0.123046875, 0.126953125, 0.1484375, 0.927734375, 0.04296875, 0.99609375, 0.08984375, 0.1015625, 0.99609375, 0.892578125, 0.99609375, 0.994140625, 0.994140625, 0.900390625, 0.99609375, 0.02734375, 0.0625, 0.99609375, 0.04296875, 0.05078125, 0.029296875, 0.02734375, 0.99609375, 0.111328125, 0.87109375, 0.994140625, 0.083984375, 0.1015625, 0.99609375, 0.02734375, 0.8828125, 0.048828125, 0.78125, 0.017578125, 0.99609375, 0.05859375, 0.99609375, 0.912109375, 0.99609375, 0.99609375, 0.083984375, 0.025390625, 0.99609375, 0.0234375, 0.9765625, 0.01953125, 0.099609375, 0.994140625, 0.150390625, 0.99609375, 0.04296875, 0.033203125, 0.01171875, 0.00390625, 0.0859375, 0.083984375, 0.99609375, 0.0859375, 0.80078125, 0.99609375, 0.09375, 0.0234375, 0.025390625, 0.994140625, 0.99609375, 0.025390625, 0.99609375, 0.994140625, 0.080078125, 0.99609375, 0.99609375, 0.03515625, 0.05078125, 0.025390625, 0.107421875, 0.20703125, 0.109375, 0.99609375, 0.041015625, 0.076171875, 0.87109375, 0.994140625, 0.060546875, 0.998046875, 0.044921875, 0.015625, 0.041015625, 0.99609375, 0.044921875, 0.12109375, 0.99609375, 0.99609375, 0.013671875, 0.0234375, 0.99609375, 0.99609375, 0.994140625, 0.107421875, 0.99609375, 0.99609375, 0.080078125, 0.99609375, 0.0390625, 0.056640625, 0.076171875, 0.994140625, 0.041015625, 0.078125, 0.0234375, 0.015625, 0.99609375, 0.142578125, 0.052734375, 0.994140625, 0.8671875, 0.0546875, 0.99609375, 0.99609375, 0.0390625, 0.044921875, 0.99609375, 0.99609375, 0.04296875, 0.01953125, 0.005859375, 0.99609375, 0.99609375, 0.033203125, 0.99609375, 0.99609375, 0.01953125, 0.115234375, 0.99609375, 0.99609375, 0.994140625, 0.033203125, 0.041015625, 0.99609375, 0.998046875, 0.017578125, 0.166015625, 0.970703125, 0.0390625, 0.01171875, 0.037109375, 0.05078125, 0.8203125, 0.123046875, 0.01953125, 0.80078125, 0.994140625, 0.99609375, 0.994140625, 0.82421875, 0.99609375, 0.01953125, 0.01171875, 0.01953125, 0.103515625, 0.015625, 0.037109375, 0.9609375, 0.99609375, 0.99609375, 0.111328125, 0.072265625, 0.056640625, 0.015625, 0.021484375, 0.015625, 0.99609375, 0.88671875, 0.01953125, 0.046875, 0.99609375, 0.10546875, 0.09375, 0.10546875, 0.013671875, 0.02734375, 0.017578125, 0.169921875, 0.013671875, 0.01953125, 0.03515625, 0.0078125, 0.083984375, 0.03515625, 0.837890625, 0.03125, 0.99609375, 0.99609375, 0.09375, 0.99609375, 0.099609375, 0.0390625, 0.888671875, 0.994140625, 0.99609375, 0.994140625, 0.99609375, 0.060546875, 0.203125, 0.033203125, 0.99609375, 0.927734375, 0.994140625, 0.99609375, 0.0234375, 0.033203125, 0.046875, 0.13671875, 0.041015625, 0.99609375, 0.013671875, 0.99609375, 0.080078125, 0.994140625, 0.99609375, 0.28125, 0.060546875, 0.994140625, 0.783203125, 0.154296875, 0.994140625, 0.04296875, 0.0234375, 0.060546875, 0.083984375, 0.078125, 0.025390625, 0.1015625, 0.99609375, 0.99609375, 0.431640625, 0.072265625, 0.01953125, 0.1640625, 0.994140625, 0.994140625, 0.95703125, 0.94921875, 0.99609375, 0.8984375, 0.99609375, 0.677734375, 0.99609375, 0.025390625, 0.009765625, 0.99609375, 0.046875, 0.99609375, 0.171875, 0.08984375, 0.0703125, 0.130859375, 0.99609375, 0.037109375, 0.865234375, 0.171875, 0.943359375, 0.009765625, 0.005859375, 0.994140625, 0.99609375, 0.01953125, 0.013671875, 0.884765625, 0.0078125, 0.09375, 0.99609375, 0.99609375, 0.99609375, 0.99609375, 0.037109375, 0.04296875, 0.99609375, 0.021484375, 0.99609375, 0.99609375, 0.033203125, 0.810546875, 0.99609375, 0.99609375, 0.03515625, 0.037109375, 0.0234375, 0.99609375, 0.01953125, 0.994140625, 0.013671875, 0.99609375, 0.0390625, 0.041015625, 0.234375, 0.044921875, 0.056640625, 0.0390625, 0.037109375, 0.99609375, 0.046875, 0.0703125, 0.998046875, 0.029296875, 0.044921875, 0.177734375, 0.087890625, 0.05078125, 0.99609375, 0.861328125, 0.03125, 0.095703125, 0.876953125, 0.99609375, 0.017578125, 0.03515625, 0.0546875, 0.99609375, 0.09375, 0.99609375, 0.09765625, 0.951171875, 0.037109375, 0.013671875, 0.6015625, 0.99609375, 0.99609375, 0.041015625, 0.03515625, 0.044921875, 0.892578125, 0.994140625, 0.99609375, 0.88671875, 0.99609375, 0.99609375, 0.99609375, 0.015625, 0.99609375, 0.23828125, 0.017578125, 0.013671875, 0.0390625, 0.994140625, 0.103515625, 0.87109375, 0.025390625, 0.927734375, 0.09765625, 0.052734375, 0.076171875, 0.890625, 0.953125, 0.0390625, 0.02734375, 0.99609375, 0.044921875, 0.068359375, 0.009765625, 0.99609375, 0.99609375, 0.908203125, 0.83203125, 0.921875, 0.99609375, 0.99609375, 0.046875, 0.078125, 0.044921875, 0.015625, 0.99609375, 0.994140625, 0.99609375, 0.015625, 0.109375, 0.845703125, 0.083984375, 0.087890625, 0.142578125, 0.99609375, 0.771484375, 0.009765625, 0.99609375, 0.083984375, 0.02734375, 0.130859375, 0.083984375, 0.99609375, 0.017578125, 0.013671875, 0.029296875, 0.99609375, 0.05859375, 0.134765625, 0.03125, 0.994140625, 0.994140625, 0.99609375, 0.890625, 0.103515625, 0.02734375, 0.080078125, 0.99609375, 0.01953125, 0.03515625, 0.943359375, 0.994140625]

 sparsity of   [0.267578125, 0.0595703125, 0.0146484375, 0.1201171875, 0.0498046875, 0.0263671875, 0.90234375, 0.05078125, 0.9501953125, 0.0166015625, 0.9267578125, 0.869140625, 0.0830078125, 0.9169921875, 0.064453125, 0.91015625, 0.0185546875, 0.0341796875, 0.943359375, 0.1162109375, 0.9462890625, 0.0048828125, 0.0859375, 0.9970703125, 0.0263671875, 0.1142578125, 0.1572265625, 0.0166015625, 0.033203125, 0.06640625, 0.060546875, 0.0419921875, 0.099609375, 0.06640625, 0.02734375, 0.015625, 0.109375, 0.0048828125, 0.9990234375, 0.939453125, 0.0751953125, 0.880859375, 0.998046875, 0.916015625, 0.05859375, 0.5849609375, 0.12890625, 0.0380859375, 0.10546875, 0.0595703125, 0.119140625, 0.0068359375, 0.0, 0.0302734375, 0.052734375, 0.041015625, 0.0185546875, 0.0146484375, 0.1005859375, 0.26171875, 0.0283203125, 0.8466796875, 0.01171875, 0.8935546875, 0.013671875, 0.9970703125, 0.9462890625, 0.126953125, 0.1484375, 0.1435546875, 0.0595703125, 0.955078125, 0.65625, 0.0234375, 0.607421875, 0.1396484375, 0.3349609375, 0.0166015625, 0.2548828125, 0.03515625, 0.0615234375, 0.90234375, 0.0302734375, 0.998046875, 0.927734375, 0.017578125, 0.0361328125, 0.05859375, 0.0126953125, 0.12890625, 0.85546875, 0.041015625, 0.0205078125, 0.7568359375, 0.998046875, 0.033203125, 0.0234375, 0.037109375, 0.998046875, 0.998046875, 0.9970703125, 0.056640625, 0.0, 0.03515625, 0.796875, 0.041015625, 0.0380859375, 0.03125, 0.0322265625, 0.0703125, 0.0654296875, 0.931640625, 0.0, 0.99609375, 0.0361328125, 0.048828125, 0.1142578125, 0.056640625, 0.0205078125, 0.0322265625, 0.1142578125, 0.8369140625, 0.1279296875, 0.0673828125, 0.7998046875, 0.017578125, 0.005859375, 0.05859375, 0.0361328125, 0.0166015625, 0.203125, 0.072265625, 0.0439453125, 0.03125, 0.1142578125, 0.04296875, 0.912109375, 0.1484375, 0.1396484375, 0.0166015625, 0.8896484375, 0.1240234375, 0.8916015625, 0.0283203125, 0.017578125, 0.166015625, 0.0, 0.01953125, 0.048828125, 0.19140625, 0.689453125, 0.0419921875, 0.025390625, 0.0439453125, 0.9140625, 0.916015625, 0.06640625, 0.873046875, 0.90234375, 0.0, 0.0400390625, 0.0771484375, 0.0625, 0.060546875, 0.0205078125, 0.1083984375, 0.1240234375, 0.03125, 0.1337890625, 0.0458984375, 0.2802734375, 0.03515625, 0.033203125, 0.0205078125, 0.0390625, 0.0126953125, 0.08203125, 0.05859375, 0.0693359375, 0.0615234375, 0.140625, 0.0322265625, 0.8544921875, 0.0205078125, 0.248046875, 0.998046875, 0.8251953125, 0.072265625, 0.0498046875, 0.1318359375, 0.0205078125, 0.876953125, 0.1435546875, 0.0458984375, 0.998046875, 0.0556640625, 0.8623046875, 0.3623046875, 0.033203125, 0.037109375, 0.0419921875, 0.0302734375, 0.2119140625, 0.056640625, 0.0244140625, 0.0283203125, 0.0439453125, 0.0390625, 0.03125, 0.08984375, 0.0244140625, 0.0859375, 0.0, 0.0849609375, 0.9404296875, 0.0146484375, 0.12109375, 0.009765625, 0.0341796875, 0.998046875, 0.046875, 0.0107421875, 0.0205078125, 0.021484375, 0.0908203125, 0.01953125, 0.283203125, 0.033203125, 0.0361328125, 0.0400390625, 0.0078125, 0.998046875, 0.0654296875, 0.056640625, 0.119140625, 0.103515625, 0.998046875, 0.9990234375, 0.12109375, 0.0400390625, 0.0146484375, 0.998046875, 0.080078125, 0.0009765625, 0.0078125, 0.9052734375, 0.0400390625, 0.1328125, 0.9189453125, 0.931640625, 0.9443359375, 0.044921875, 0.0341796875, 0.099609375, 0.0234375, 0.021484375, 0.14453125, 0.0390625, 0.3125, 0.0478515625, 0.0244140625, 0.685546875, 0.1083984375, 0.0263671875, 0.044921875, 0.6083984375, 0.0146484375, 0.0224609375, 0.0400390625, 0.3896484375, 0.1455078125, 0.2421875, 0.037109375, 0.0751953125, 0.998046875, 0.0263671875, 0.1328125, 0.8681640625, 0.060546875, 0.15234375, 0.0849609375, 0.8515625, 0.0322265625, 0.998046875, 0.1181640625, 0.052734375, 0.10546875, 0.140625, 0.044921875, 0.0478515625, 0.150390625, 0.052734375, 0.6806640625, 0.01171875, 0.0322265625, 0.021484375, 0.0458984375, 0.0458984375, 0.015625, 0.0302734375, 0.03125, 0.0458984375, 0.0732421875, 0.9970703125, 0.6435546875, 0.8017578125, 0.0546875, 0.9990234375, 0.9248046875, 0.017578125, 0.935546875, 0.076171875, 0.998046875, 0.9013671875, 0.8701171875, 0.998046875, 0.076171875, 0.0478515625, 0.037109375, 0.9619140625, 0.0400390625, 0.025390625, 0.0224609375, 0.025390625, 0.033203125, 0.0322265625, 0.0712890625, 0.9970703125, 0.998046875, 0.033203125, 0.05078125, 0.0126953125, 0.1171875, 0.998046875, 0.04296875, 0.1806640625, 0.0400390625, 0.0, 0.8017578125, 0.033203125, 0.998046875, 0.0078125, 0.005859375, 0.09375, 0.9970703125, 0.0498046875, 0.11328125, 0.091796875, 0.109375, 0.0, 0.01171875, 0.0234375, 0.0263671875, 0.0263671875, 0.3740234375, 0.1044921875, 0.0283203125, 0.009765625, 0.1337890625, 0.04296875, 0.6064453125, 0.0166015625, 0.0185546875, 0.060546875, 0.85546875, 0.0478515625, 0.0458984375, 0.0, 0.0224609375, 0.0322265625, 0.0634765625, 0.0244140625, 0.0166015625, 0.044921875, 0.0341796875, 0.998046875, 0.9404296875, 0.8798828125, 0.0361328125, 0.041015625, 0.9970703125, 0.88671875, 0.083984375, 0.9296875, 0.0283203125, 0.1474609375, 0.119140625, 0.998046875, 0.9970703125, 0.0, 0.318359375, 0.03125, 0.046875, 0.6767578125, 0.017578125, 0.0546875, 0.998046875, 0.017578125, 0.0380859375, 0.958984375, 0.0419921875, 0.8486328125, 0.0234375, 0.0166015625, 0.0380859375, 0.1748046875, 0.8740234375, 0.0888671875, 0.1220703125, 0.029296875, 0.0126953125, 0.0341796875, 0.9599609375, 0.01171875, 0.8310546875, 0.9130859375, 0.0146484375, 0.7021484375, 0.142578125, 0.1416015625, 0.0751953125, 0.1484375, 0.1396484375, 0.1904296875, 0.0478515625, 0.041015625, 0.91796875, 0.9345703125, 0.5888671875, 0.1201171875, 0.84765625, 0.94140625, 0.15625, 0.755859375, 0.8447265625, 0.0380859375, 0.0263671875, 0.857421875, 0.9970703125, 0.27734375, 0.9990234375, 0.998046875, 0.0205078125, 0.03515625, 0.9970703125, 0.0, 0.056640625, 0.9970703125, 0.033203125, 0.16796875, 0.1357421875, 0.998046875, 0.1220703125, 0.029296875, 0.029296875, 0.0146484375, 0.8857421875, 0.1484375, 0.111328125, 0.083984375, 0.037109375, 0.078125, 0.0625, 0.8583984375, 0.04296875, 0.6884765625, 0.056640625, 0.25, 0.0322265625, 0.9169921875, 0.158203125, 0.033203125, 0.8828125, 0.05859375, 0.037109375, 0.0537109375, 0.8232421875, 0.1123046875, 0.0908203125, 0.10546875, 0.931640625, 0.013671875, 0.0537109375, 0.865234375, 0.880859375, 0.0048828125, 0.1298828125, 0.0205078125, 0.138671875, 0.07421875, 0.0400390625, 0.0400390625, 0.07421875, 0.029296875, 0.7587890625, 0.3046875, 0.0302734375, 0.01171875, 0.9970703125, 0.115234375, 0.0498046875, 0.998046875, 0.0693359375, 0.0361328125, 0.0, 0.802734375, 0.0625, 0.1005859375, 0.998046875, 0.1845703125, 0.998046875, 0.826171875, 0.80078125, 0.013671875, 0.8017578125, 0.9970703125, 0.1435546875, 0.998046875, 0.939453125, 0.0634765625, 0.9970703125, 0.150390625, 0.1484375, 0.076171875, 0.9111328125, 0.12890625, 0.978515625, 0.0078125, 0.9970703125, 0.02734375, 0.859375, 0.033203125, 0.0224609375, 0.921875, 0.8369140625, 0.021484375, 0.9111328125, 0.9990234375, 0.3095703125, 0.9091796875, 0.001953125, 0.2275390625, 0.140625, 0.248046875, 0.0341796875, 0.0, 0.8115234375, 0.998046875, 0.91015625, 0.0400390625, 0.12890625, 0.0458984375, 0.10546875, 0.013671875, 0.84375, 0.998046875, 0.033203125, 0.0126953125, 0.94921875, 0.0380859375, 0.0302734375, 0.041015625, 0.0068359375, 0.02734375, 0.9970703125, 0.0224609375, 0.0419921875, 0.1357421875, 0.0107421875, 0.029296875, 0.015625, 0.9404296875, 0.0439453125, 0.04296875, 0.921875, 0.8994140625, 0.0458984375, 0.0, 0.828125, 0.00390625, 0.02734375, 0.9990234375, 0.025390625, 0.1279296875, 0.046875, 0.0302734375, 0.7685546875, 0.05078125, 0.998046875, 0.23046875, 0.0947265625, 0.0625, 0.9296875, 0.0576171875, 0.0166015625, 0.998046875, 0.0, 0.0830078125, 0.0546875, 0.998046875, 0.0322265625, 0.1455078125, 0.0224609375, 0.0166015625, 0.005859375, 0.01953125, 0.03515625, 0.02734375, 0.1015625, 0.08203125, 0.9970703125, 0.1298828125, 0.01171875, 0.1484375, 0.9970703125, 0.1484375, 0.9375, 0.05078125, 0.0341796875, 0.1279296875, 0.0283203125, 0.0224609375, 0.7666015625, 0.0224609375, 0.03125, 0.9404296875, 0.0244140625, 0.0703125, 0.0185546875, 0.03515625, 0.9970703125, 0.0166015625, 0.9384765625, 0.03515625, 0.05859375, 0.00390625, 0.0341796875, 0.025390625, 0.998046875, 0.0, 0.0009765625, 0.1064453125, 0.06640625, 0.0419921875, 0.021484375, 0.017578125, 0.0107421875, 0.0625, 0.111328125, 0.0234375, 0.935546875, 0.1435546875, 0.0107421875, 0.099609375, 0.900390625, 0.0498046875, 0.083984375, 0.9970703125, 0.0029296875, 0.0947265625, 0.0029296875, 0.92578125, 0.134765625, 0.009765625, 0.0322265625, 0.1064453125, 0.03515625, 0.78125, 0.9970703125, 0.9091796875, 0.1328125, 0.998046875, 0.0390625, 0.0341796875, 0.0390625, 0.998046875, 0.15625, 0.115234375, 0.181640625, 0.96875, 0.908203125, 0.0498046875, 0.0400390625, 0.0244140625, 0.0341796875, 0.064453125, 0.0791015625, 0.91015625, 0.0693359375, 0.0380859375, 0.015625, 0.021484375, 0.0283203125, 0.9990234375, 0.1572265625, 0.017578125, 0.009765625, 0.0595703125, 0.7568359375, 0.0634765625, 0.01953125, 0.001953125, 0.005859375, 0.1484375, 0.0439453125, 0.0205078125, 0.037109375, 0.7939453125, 0.037109375, 0.94140625, 0.1611328125, 0.8486328125, 0.89453125, 0.041015625, 0.044921875, 0.072265625, 0.0322265625, 0.9091796875, 0.1484375, 0.171875, 0.322265625, 0.02734375, 0.0, 0.0546875, 0.01953125, 0.1572265625, 0.138671875, 0.0341796875, 0.025390625, 0.189453125, 0.0400390625, 0.025390625, 0.998046875, 0.037109375, 0.927734375, 0.998046875, 0.9970703125, 0.0458984375, 0.0, 0.0654296875, 0.044921875, 0.0087890625, 0.0537109375, 0.1298828125, 0.0185546875, 0.0234375, 0.1220703125, 0.021484375, 0.0185546875, 0.013671875, 0.8857421875, 0.078125, 0.0888671875, 0.0048828125, 0.0458984375, 0.02734375, 0.1015625, 0.0830078125, 0.0478515625, 0.1142578125, 0.0068359375, 0.9970703125, 0.9404296875, 0.1220703125, 0.0361328125, 0.060546875, 0.9990234375, 0.0576171875, 0.0654296875, 0.9990234375, 0.0185546875, 0.1904296875, 0.021484375, 0.0830078125, 0.0263671875, 0.7021484375, 0.1181640625, 0.072265625, 0.017578125, 0.0908203125, 0.0322265625, 0.85546875, 0.03125, 0.109375, 0.009765625, 0.1484375, 0.9169921875, 0.17578125, 0.0517578125, 0.83203125, 0.9482421875, 0.9990234375, 0.880859375, 0.0, 0.0791015625, 0.8701171875, 0.05859375, 0.767578125, 0.0107421875, 0.0048828125, 0.0244140625, 0.5048828125, 0.998046875, 0.0048828125, 0.0439453125, 0.0146484375, 0.849609375, 0.0390625, 0.033203125, 0.0341796875, 0.02734375, 0.0537109375, 0.0732421875, 0.0224609375, 0.0322265625, 0.998046875, 0.00390625, 0.9541015625, 0.044921875, 0.06640625, 0.0078125, 0.0703125, 0.0361328125, 0.9384765625, 0.068359375, 0.0478515625, 0.021484375, 0.1416015625, 0.9970703125, 0.0791015625, 0.0390625, 0.0419921875, 0.1201171875, 0.998046875, 0.998046875, 0.9970703125, 0.9560546875, 0.1484375, 0.9228515625, 0.15625, 0.083984375, 0.033203125, 0.0400390625, 0.95703125, 0.998046875, 0.951171875, 0.7587890625, 0.0322265625, 0.0732421875, 0.9970703125, 0.890625, 0.0283203125, 0.0380859375, 0.017578125, 0.0986328125, 0.04296875, 0.8310546875, 0.0302734375, 0.0703125, 0.09375, 0.955078125, 0.1484375, 0.1162109375, 0.9169921875, 0.998046875, 0.0712890625, 0.0, 0.18359375, 0.9970703125, 0.02734375, 0.927734375, 0.65234375, 0.0693359375, 0.0400390625, 0.048828125, 0.0234375, 0.537109375, 0.126953125, 0.0234375, 0.998046875, 0.0830078125, 0.0, 0.0087890625, 0.916015625, 0.0546875, 0.998046875, 0.0087890625, 0.0341796875, 0.998046875, 0.0615234375, 0.052734375, 0.1513671875, 0.05078125, 0.041015625, 0.1484375, 0.041015625, 0.0576171875, 0.017578125, 0.998046875, 0.1474609375, 0.0, 0.943359375, 0.0625, 0.8740234375, 0.0380859375, 0.0341796875, 0.91015625, 0.0546875, 0.8271484375, 0.166015625, 0.1044921875, 0.923828125, 0.0390625, 0.0322265625, 0.0341796875, 0.01953125, 0.0576171875, 0.044921875, 0.0400390625, 0.1484375, 0.0478515625, 0.060546875, 0.8134765625, 0.1123046875, 0.0322265625, 0.048828125, 0.0107421875, 0.779296875, 0.1474609375, 0.9521484375, 0.01171875, 0.9716796875, 0.0126953125, 0.0634765625, 0.22265625, 0.93359375, 0.0078125, 0.0, 0.658203125, 0.9970703125, 0.1025390625, 0.9990234375, 0.044921875, 0.0126953125, 0.033203125, 0.0703125, 0.0, 0.05078125, 0.0166015625, 0.109375, 0.9599609375, 0.0546875, 0.865234375, 0.9404296875, 0.0439453125, 0.8818359375, 0.033203125, 0.9228515625, 0.0634765625, 0.056640625, 0.9658203125, 0.9140625, 0.0478515625, 0.041015625, 0.146484375, 0.876953125, 0.0, 0.0654296875, 0.12109375, 0.888671875, 0.0068359375, 0.1240234375, 0.1484375, 0.03125, 0.2529296875, 0.0615234375, 0.83984375, 0.05078125, 0.0263671875, 0.033203125, 0.01171875, 0.0390625, 0.0654296875, 0.8291015625, 0.9990234375, 0.7998046875, 0.9990234375, 0.046875, 0.0, 0.0732421875, 0.1484375, 0.046875, 0.046875, 0.0458984375, 0.1162109375, 0.0078125, 0.0009765625, 0.0302734375, 0.0146484375, 0.0400390625, 0.0068359375, 0.1123046875, 0.9599609375, 0.998046875, 0.203125, 0.041015625, 0.998046875, 0.1123046875, 0.9228515625, 0.8818359375, 0.9375, 0.8310546875, 0.03515625, 0.095703125, 0.7119140625, 0.04296875, 0.0283203125, 0.0654296875, 0.142578125, 0.1484375, 0.1064453125, 0.0126953125, 0.0380859375, 0.0185546875, 0.0322265625, 0.0, 0.9296875, 0.138671875, 0.919921875, 0.9970703125, 0.0234375, 0.0234375, 0.9990234375, 0.0048828125, 0.9287109375, 0.037109375, 0.998046875, 0.08203125, 0.794921875, 0.107421875, 0.0087890625, 0.140625, 0.0107421875, 0.328125, 0.017578125, 0.0458984375, 0.0361328125, 0.322265625, 0.0478515625, 0.939453125, 0.0283203125, 0.025390625, 0.998046875, 0.9970703125, 0.998046875, 0.0263671875, 0.1279296875, 0.0048828125, 0.0244140625, 0.1328125, 0.0615234375, 0.9970703125, 0.7568359375, 0.021484375, 0.1455078125, 0.998046875, 0.134765625, 0.8583984375, 0.0595703125, 0.998046875, 0.025390625, 0.041015625, 0.03125, 0.044921875, 0.1484375, 0.1201171875, 0.0615234375, 0.130859375, 0.0771484375, 0.0625, 0.0947265625, 0.0244140625, 0.6708984375, 0.0361328125, 0.0205078125, 0.052734375, 0.04296875, 0.0146484375, 0.021484375, 0.9208984375, 0.9423828125, 0.0126953125, 0.802734375, 0.134765625, 0.1328125, 0.033203125, 0.0419921875, 0.7822265625, 0.86328125, 0.017578125, 0.0927734375, 0.0302734375, 0.03125, 0.0380859375, 0.0087890625, 0.0390625, 0.041015625, 0.8779296875, 0.041015625, 0.0966796875, 0.935546875, 0.93359375, 0.0283203125, 0.810546875, 0.0361328125, 0.1484375, 0.060546875, 0.0634765625, 0.103515625, 0.1240234375, 0.037109375, 0.845703125, 0.763671875, 0.916015625, 0.01953125, 0.998046875, 0.8818359375, 0.0185546875, 0.3408203125, 0.029296875, 0.1328125, 0.0537109375, 0.9345703125, 0.048828125, 0.931640625, 0.0224609375, 0.0224609375, 0.037109375, 0.119140625, 0.86328125, 0.15234375, 0.0380859375, 0.0263671875, 0.8896484375, 0.90234375, 0.998046875, 0.08984375, 0.958984375, 0.814453125, 0.0712890625, 0.197265625, 0.005859375, 0.056640625, 0.0732421875, 0.126953125, 0.01171875, 0.0439453125, 0.0546875, 0.0107421875, 0.046875, 0.8935546875, 0.021484375, 0.0244140625, 0.03125, 0.861328125, 0.0576171875, 0.0, 0.2041015625, 0.998046875, 0.966796875, 0.046875, 0.0283203125, 0.890625, 0.998046875, 0.0107421875, 0.0126953125, 0.0478515625, 0.0146484375, 0.0400390625, 0.998046875, 0.0078125, 0.046875, 0.0380859375, 0.03515625, 0.0166015625, 0.9189453125, 0.060546875, 0.1240234375, 0.05078125, 0.875, 0.1943359375, 0.998046875, 0.041015625, 0.83984375, 0.021484375, 0.0087890625, 0.1064453125, 0.0107421875, 0.015625, 0.04296875, 0.07421875, 0.0166015625, 0.1484375, 0.130859375, 0.04296875, 0.0166015625, 0.998046875, 0.0146484375, 0.3125, 0.05078125, 0.01953125, 0.0439453125, 0.0302734375, 0.0302734375, 0.96484375, 0.009765625, 0.1953125, 0.9970703125, 0.998046875, 0.0244140625, 0.125, 0.0615234375, 0.0361328125, 0.1181640625, 0.0205078125, 0.95703125, 0.1240234375, 0.0146484375, 0.0126953125, 0.09375, 0.998046875, 0.998046875, 0.0712890625, 0.0, 0.029296875, 0.0634765625, 0.9697265625, 0.107421875, 0.0263671875, 0.1064453125, 0.92578125, 0.021484375, 0.9287109375, 0.998046875, 0.1484375, 0.94921875, 0.0341796875, 0.0634765625, 0.0673828125, 0.0537109375, 0.0771484375, 0.0654296875, 0.8447265625, 0.0205078125, 0.8955078125, 0.005859375, 0.9150390625, 0.1142578125, 0.01953125, 0.044921875, 0.1484375, 0.1865234375, 0.0, 0.1181640625, 0.0380859375, 0.1328125, 0.3818359375, 0.0234375, 0.0283203125, 0.0390625, 0.111328125, 0.1337890625, 0.0087890625, 0.0224609375, 0.998046875, 0.94921875, 0.0439453125, 0.111328125, 0.138671875, 0.1630859375, 0.046875, 0.0302734375, 0.8046875, 0.0234375, 0.9423828125, 0.0615234375, 0.01953125, 0.0244140625, 0.080078125, 0.0869140625, 0.107421875, 0.8408203125, 0.91015625, 0.1474609375, 0.0341796875, 0.1123046875, 0.119140625, 0.998046875, 0.12109375, 0.19921875, 0.91015625, 0.0732421875, 0.037109375, 0.7626953125, 0.0791015625, 0.05859375, 0.0, 0.0986328125, 0.0224609375, 0.03515625, 0.9609375, 0.0439453125, 0.015625, 0.07421875, 0.025390625, 0.02734375, 0.9248046875, 0.005859375, 0.0078125, 0.037109375, 0.849609375, 0.0, 0.03515625, 0.0439453125, 0.0302734375, 0.0322265625, 0.044921875, 0.0498046875, 0.033203125, 0.0, 0.015625, 0.248046875, 0.0224609375, 0.0478515625, 0.94140625, 0.0322265625, 0.0986328125, 0.04296875, 0.0029296875, 0.88671875, 0.1982421875, 0.0654296875, 0.0205078125, 0.0498046875, 0.830078125, 0.03515625, 0.1279296875, 0.0, 0.072265625, 0.1240234375, 0.0263671875, 0.0302734375, 0.05078125, 0.236328125, 0.0224609375, 0.0498046875, 0.0302734375, 0.091796875, 0.833984375, 0.080078125, 0.08984375, 0.072265625, 0.9287109375, 0.9345703125, 0.0263671875, 0.0107421875, 0.017578125, 0.3701171875, 0.0322265625, 0.0224609375, 0.998046875, 0.9970703125, 0.10546875, 0.04296875, 0.1484375, 0.7939453125, 0.9990234375, 0.06640625, 0.0625, 0.1484375, 0.0126953125, 0.03515625, 0.1171875, 0.2548828125, 0.7724609375, 0.0, 0.138671875, 0.998046875, 0.0234375, 0.015625, 0.8837890625, 0.052734375, 0.0419921875, 0.033203125, 0.02734375, 0.6162109375, 0.0, 0.0302734375, 0.109375, 0.044921875, 0.9970703125, 0.13671875, 0.01953125, 0.015625, 0.0341796875, 0.8212890625, 0.03515625, 0.1181640625, 0.1044921875, 0.025390625, 0.0126953125, 0.8740234375, 0.017578125, 0.0283203125, 0.3525390625, 0.03515625, 0.11328125, 0.017578125, 0.0361328125, 0.0234375, 0.03515625, 0.0546875, 0.0224609375, 0.0478515625, 0.068359375, 0.0859375, 0.041015625, 0.0439453125, 0.029296875, 0.0556640625, 0.015625, 0.998046875, 0.1484375, 0.908203125, 0.998046875, 0.0361328125, 0.103515625, 0.025390625, 0.333984375, 0.1015625, 0.04296875, 0.8193359375, 0.998046875, 0.1123046875, 0.0185546875, 0.0283203125, 0.0400390625, 0.03515625, 0.0380859375, 0.9130859375, 0.998046875, 0.0224609375, 0.947265625, 0.0380859375, 0.931640625, 0.0771484375, 0.603515625, 0.0546875, 0.001953125, 0.0, 0.0166015625, 0.021484375, 0.0546875, 0.0, 0.0185546875, 0.9990234375, 0.0966796875, 0.064453125, 0.1142578125, 0.0771484375, 0.103515625, 0.1484375, 0.0400390625, 0.02734375, 0.0234375, 0.0439453125, 0.380859375, 0.029296875, 0.1064453125, 0.068359375, 0.01953125, 0.92578125, 0.9970703125, 0.921875, 0.0302734375, 0.0283203125, 0.1240234375, 0.0478515625, 0.017578125, 0.0986328125, 0.00390625, 0.005859375, 0.0341796875, 0.92578125, 0.0205078125, 0.0302734375, 0.0419921875, 0.9990234375, 0.0625, 0.078125, 0.0263671875, 0.0234375, 0.1337890625, 0.0224609375, 0.0068359375, 0.04296875, 0.9970703125, 0.19140625, 0.0703125, 0.041015625, 0.02734375, 0.0, 0.0087890625, 0.0390625, 0.0, 0.8369140625, 0.0380859375, 0.015625, 0.0205078125, 0.048828125, 0.0234375, 0.017578125, 0.0234375, 0.0537109375, 0.0, 0.998046875, 0.017578125, 0.025390625, 0.0498046875, 0.013671875, 0.037109375, 0.044921875, 0.9990234375, 0.0263671875, 0.1484375, 0.0615234375, 0.95703125, 0.998046875, 0.11328125, 0.8232421875, 0.07421875, 0.044921875, 0.044921875, 0.0107421875, 0.1240234375, 0.0400390625, 0.0, 0.033203125, 0.02734375, 0.099609375, 0.1318359375, 0.0634765625, 0.0244140625, 0.138671875, 0.0087890625, 0.8994140625, 0.9970703125, 0.0458984375, 0.998046875, 0.9990234375, 0.09765625, 0.998046875, 0.853515625, 0.009765625, 0.8662109375, 0.0185546875, 0.0537109375, 0.6611328125, 0.1201171875, 0.0048828125, 0.107421875, 0.0576171875, 0.083984375, 0.4951171875, 0.0146484375, 0.0390625, 0.1083984375, 0.0302734375, 0.033203125, 0.125, 0.0380859375, 0.939453125, 0.25, 0.1171875, 0.12890625, 0.0146484375, 0.1484375, 0.068359375, 0.0771484375, 0.0908203125, 0.9541015625, 0.1142578125, 0.8935546875, 0.927734375, 0.8701171875, 0.06640625, 0.998046875, 0.1318359375, 0.93359375, 0.998046875, 0.033203125, 0.947265625, 0.0244140625, 0.90625, 0.0283203125, 0.0244140625, 0.03515625, 0.05859375, 0.9150390625, 0.0146484375, 0.0458984375, 0.025390625, 0.0244140625, 0.19921875, 0.138671875, 0.1484375, 0.861328125, 0.837890625, 0.18359375, 0.998046875, 0.1015625, 0.0322265625, 0.0576171875, 0.146484375, 0.998046875, 0.0322265625, 0.873046875, 0.2275390625, 0.044921875, 0.9501953125, 0.1298828125, 0.048828125, 0.857421875, 0.9970703125, 0.0947265625, 0.107421875, 0.0634765625, 0.892578125, 0.0126953125, 0.97265625, 0.11328125, 0.998046875, 0.0166015625, 0.05859375, 0.9970703125, 0.12890625, 0.9150390625, 0.00390625, 0.935546875, 0.123046875, 0.0, 0.0654296875, 0.0302734375, 0.1220703125, 0.82421875, 0.0673828125, 0.05859375, 0.111328125, 0.0498046875, 0.953125, 0.6474609375, 0.845703125, 0.0673828125, 0.0126953125, 0.0, 0.1279296875, 0.0576171875, 0.04296875, 0.0078125, 0.845703125, 0.9990234375, 0.0771484375, 0.013671875, 0.0224609375, 0.0205078125, 0.9970703125, 0.017578125, 0.046875, 0.111328125, 0.91015625, 0.8037109375, 0.0673828125, 0.015625, 0.095703125, 0.0078125, 0.1298828125, 0.998046875, 0.09765625, 0.0439453125, 0.935546875, 0.7919921875, 0.2998046875, 0.9970703125, 0.9970703125, 0.0009765625, 0.0263671875, 0.04296875, 0.8876953125, 0.0712890625, 0.1259765625, 0.0302734375, 0.0224609375, 0.0322265625, 0.0341796875, 0.025390625, 0.05859375, 0.015625, 0.0263671875, 0.087890625, 0.017578125, 0.013671875, 0.9326171875, 0.83984375, 0.625, 0.0234375, 0.0458984375, 0.04296875, 0.02734375, 0.9970703125, 0.23828125, 0.064453125, 0.998046875, 0.998046875, 0.0, 0.0380859375, 0.0439453125, 0.091796875, 0.095703125, 0.998046875, 0.0947265625, 0.025390625, 0.134765625, 0.015625, 0.0, 0.0146484375, 0.0234375, 0.0537109375, 0.0302734375, 0.7333984375, 0.2890625, 0.0458984375, 0.0537109375, 0.0947265625, 0.67578125, 0.0107421875, 0.0, 0.6923828125, 0.921875, 0.0087890625, 0.9345703125, 0.1005859375, 0.9970703125, 0.8974609375, 0.0537109375, 0.041015625, 0.9990234375, 0.068359375, 0.9482421875, 0.0771484375, 0.9658203125, 0.0185546875, 0.02734375, 0.1396484375, 0.2998046875, 0.03125, 0.0361328125, 0.99609375, 0.1123046875, 0.0244140625, 0.0107421875, 0.0087890625, 0.0498046875, 0.248046875, 0.03125, 0.044921875, 0.0224609375, 0.009765625, 0.0458984375, 0.8056640625, 0.005859375, 0.4755859375, 0.083984375, 0.0244140625, 0.041015625, 0.0224609375, 0.00390625, 0.861328125, 0.0712890625, 0.953125, 0.0380859375, 0.0986328125, 0.037109375, 0.0263671875, 0.0361328125, 0.115234375, 0.041015625, 0.0, 0.015625, 0.046875, 0.0185546875, 0.009765625, 0.0361328125, 0.1484375, 0.998046875, 0.998046875, 0.0517578125, 0.9267578125, 0.0302734375, 0.0458984375, 0.0576171875, 0.015625, 0.01171875, 0.1484375, 0.119140625, 0.1259765625, 0.072265625, 0.1416015625, 0.10546875, 0.015625, 0.0322265625, 0.0546875, 0.8740234375, 0.068359375, 0.017578125, 0.072265625, 0.9970703125, 0.048828125, 0.0, 0.0390625, 0.02734375, 0.931640625, 0.0, 0.0146484375, 0.998046875, 0.0537109375, 0.916015625, 0.03125, 0.0107421875, 0.9970703125, 0.0, 0.0302734375, 0.03515625, 0.5625, 0.8193359375, 0.9267578125, 0.9140625, 0.1162109375, 0.1162109375, 0.0625, 0.0419921875, 0.0556640625, 0.1474609375, 0.0751953125, 0.00390625, 0.943359375, 0.0341796875, 0.0380859375, 0.095703125, 0.9970703125, 0.1484375, 0.0302734375, 0.591796875, 0.013671875, 0.033203125, 0.0224609375, 0.8359375, 0.0478515625, 0.052734375, 0.033203125, 0.0009765625, 0.8701171875, 0.828125, 0.0361328125, 0.03125, 0.1083984375, 0.0, 0.0400390625, 0.013671875, 0.96484375, 0.0185546875, 0.1396484375, 0.0830078125, 0.0322265625, 0.1484375, 0.0009765625, 0.0576171875, 0.0, 0.044921875, 0.015625, 0.1357421875, 0.1044921875, 0.119140625, 0.0537109375, 0.1318359375, 0.1337890625, 0.134765625, 0.140625, 0.0302734375, 0.8583984375, 0.033203125, 0.1337890625, 0.078125, 0.5595703125, 0.0732421875, 0.87890625, 0.0439453125, 0.0126953125, 0.0625, 0.9990234375, 0.091796875, 0.115234375, 0.9970703125, 0.05859375, 0.03515625, 0.8056640625, 0.0322265625, 0.2900390625, 0.046875, 0.02734375, 0.0322265625, 0.033203125, 0.0546875, 0.970703125, 0.716796875, 0.123046875, 0.12109375, 0.998046875, 0.037109375, 0.041015625, 0.1259765625, 0.0224609375, 0.1494140625, 0.8232421875, 0.06640625, 0.0439453125, 0.3857421875, 0.0546875, 0.1376953125, 0.0703125, 0.7802734375, 0.1796875, 0.04296875, 0.228515625, 0.919921875, 0.126953125, 0.0146484375, 0.03125, 0.029296875, 0.9990234375, 0.0517578125, 0.03125, 0.216796875, 0.0078125, 0.90625, 0.109375, 0.90625, 0.998046875, 0.009765625, 0.1259765625, 0.9306640625, 0.0791015625, 0.0234375, 0.8291015625, 0.03125, 0.072265625, 0.8671875, 0.9970703125, 0.21484375, 0.0302734375, 0.0439453125, 0.0263671875, 0.7978515625, 0.0185546875, 0.244140625, 0.8525390625, 0.0107421875, 0.0419921875, 0.998046875, 0.154296875, 0.001953125, 0.072265625, 0.9423828125, 0.0654296875, 0.998046875, 0.5908203125, 0.0810546875, 0.021484375, 0.052734375, 0.0517578125, 0.876953125, 0.9990234375, 0.0185546875, 0.0458984375, 0.0478515625, 0.107421875, 0.787109375, 0.02734375, 0.1123046875, 0.998046875, 0.0, 0.9970703125, 0.998046875, 0.068359375, 0.015625, 0.0, 0.04296875, 0.009765625, 0.998046875, 0.9970703125, 0.0078125, 0.0126953125, 0.806640625, 0.029296875, 0.876953125, 0.822265625, 0.0361328125, 0.7958984375, 0.9599609375, 0.01953125, 0.078125, 0.998046875, 0.0380859375, 0.1240234375, 0.998046875, 0.6904296875, 0.0390625, 0.9453125, 0.8359375, 0.791015625, 0.060546875, 0.0849609375, 0.1484375, 0.01953125, 0.00390625, 0.0146484375, 0.11328125, 0.1953125, 0.017578125, 0.01953125, 0.154296875, 0.0751953125, 0.0, 0.1376953125, 0.92578125, 0.291015625, 0.0, 0.025390625, 0.998046875]

 sparsity of   [0.08349609375, 0.05078125, 0.03271484375, 0.91845703125, 0.091796875, 0.068359375, 0.99951171875, 0.0244140625, 0.9990234375, 0.0185546875, 0.9990234375, 0.11572265625, 0.0341796875, 0.65283203125, 0.12060546875, 0.056640625, 0.1181640625, 0.96240234375, 0.04150390625, 0.07958984375, 0.62109375, 0.3193359375, 0.99951171875, 0.35986328125, 0.048828125, 0.09521484375, 0.9990234375, 0.58251953125, 0.04150390625, 0.13134765625, 0.51904296875, 0.9208984375, 0.0546875, 0.90576171875, 0.99951171875, 0.9990234375, 0.0478515625, 0.05859375, 0.0927734375, 0.61572265625, 0.08740234375, 0.99951171875, 0.04541015625, 0.09375, 0.0419921875, 0.0888671875, 0.85986328125, 0.025390625, 0.0390625, 0.603515625, 0.9990234375, 0.0986328125, 0.080078125, 0.06103515625, 0.59521484375, 0.05078125, 0.68994140625, 0.49365234375, 0.0625, 0.404296875, 0.04833984375, 0.04443359375, 0.0517578125, 0.03564453125, 0.66064453125, 0.06005859375, 0.02001953125, 0.609375, 0.06884765625, 0.0458984375, 0.03662109375, 0.9990234375, 0.53955078125, 0.1591796875, 0.09228515625, 0.06982421875, 0.89501953125, 0.55322265625, 0.0498046875, 0.232421875, 0.0693359375, 0.0390625, 0.66162109375, 0.74951171875, 0.9990234375, 0.0341796875, 0.3271484375, 0.92822265625, 0.99853515625, 0.1669921875, 0.0791015625, 0.0302734375, 0.1708984375, 0.634765625, 0.91064453125, 0.84130859375, 0.673828125, 0.06591796875, 0.99951171875, 0.04248046875, 0.67041015625, 0.0498046875, 0.13134765625, 0.91455078125, 0.57763671875, 0.068359375, 0.08740234375, 0.333984375, 0.0927734375, 0.5859375, 0.9169921875, 0.34228515625, 0.0498046875, 0.61474609375, 0.9443359375, 0.07568359375, 0.0439453125, 0.3330078125, 0.09228515625, 0.6162109375, 0.9990234375, 0.8515625, 0.794921875, 0.3056640625, 0.86669921875, 0.6240234375, 0.630859375, 0.8740234375, 0.99853515625, 0.08935546875, 0.99951171875, 0.02880859375, 0.8896484375, 0.052734375, 0.3271484375, 0.67431640625, 0.03515625, 0.0615234375, 0.3564453125, 0.04248046875, 0.05615234375, 0.62890625, 0.6337890625, 0.08203125, 0.64453125, 0.041015625, 0.7021484375, 0.02685546875, 0.07080078125, 0.53125, 0.08837890625, 0.13720703125, 0.8583984375, 0.0556640625, 0.03076171875, 0.99853515625, 0.037109375, 0.87841796875, 0.81005859375, 0.03125, 0.9990234375, 0.84765625, 0.9990234375, 0.03466796875, 0.99853515625, 0.91748046875, 0.044921875, 0.0361328125, 0.10400390625, 0.04296875, 0.52734375, 0.9990234375, 0.03173828125, 0.57958984375, 0.064453125, 0.6103515625, 0.916015625, 0.40478515625, 0.99853515625, 0.34912109375, 0.271484375, 0.60107421875, 0.6357421875, 0.0625, 0.59765625, 0.5986328125, 0.935546875, 0.9169921875, 0.09130859375, 0.50830078125, 0.59619140625, 0.14404296875, 0.23486328125, 0.04541015625, 0.08837890625, 0.63818359375, 0.59619140625, 0.05517578125, 0.6728515625, 0.55859375, 0.61962890625, 0.04150390625, 0.05029296875, 0.33984375, 0.0380859375, 0.55712890625, 0.9990234375, 0.16357421875, 0.05615234375, 0.5693359375, 0.8330078125, 0.0419921875, 0.0400390625, 0.89794921875, 0.06884765625, 0.638671875, 0.66259765625, 0.9990234375, 0.63671875, 0.0537109375, 0.0595703125, 0.99853515625, 0.04345703125, 0.072265625, 0.43603515625, 0.63330078125, 0.5625, 0.40966796875, 0.53857421875, 0.07421875, 0.0615234375, 0.017578125, 0.9091796875, 0.99853515625, 0.9990234375, 0.49072265625, 0.9990234375, 0.02490234375, 0.298828125, 0.14501953125, 0.8583984375, 0.03564453125, 0.02685546875, 0.99951171875, 0.9990234375, 0.9990234375, 0.18505859375, 0.67431640625, 0.890625, 0.04296875, 0.04931640625, 0.9326171875, 0.02294921875, 0.99951171875, 0.37109375, 0.55322265625, 0.556640625, 0.0244140625, 0.61279296875, 0.14990234375, 0.04833984375, 0.9990234375, 0.61474609375, 0.0166015625, 0.87255859375, 0.072265625, 0.08984375, 0.2568359375, 0.0458984375, 0.3955078125, 0.74169921875, 0.04150390625, 0.8779296875, 0.9990234375, 0.0771484375, 0.73681640625, 0.05908203125, 0.1494140625, 0.90869140625, 0.6103515625, 0.70751953125, 0.72412109375, 0.04541015625, 0.6533203125, 0.623046875, 0.0439453125, 0.89697265625, 0.6513671875, 0.60595703125, 0.90380859375, 0.9140625, 0.88427734375, 0.0341796875, 0.64111328125, 0.03662109375, 0.08984375, 0.07666015625, 0.0341796875, 0.0546875, 0.27978515625, 0.91455078125, 0.9990234375, 0.33837890625, 0.65234375, 0.03759765625, 0.99853515625, 0.0302734375, 0.6279296875, 0.046875, 0.75, 0.15185546875, 0.3662109375, 0.0517578125, 0.626953125, 0.84619140625, 0.38818359375, 0.0625, 0.32177734375, 0.99951171875, 0.26904296875, 0.53955078125, 0.744140625, 0.9990234375, 0.099609375, 0.2939453125, 0.9990234375, 0.0537109375, 0.9990234375, 0.45068359375, 0.4931640625, 0.3271484375, 0.99853515625, 0.99951171875, 0.9189453125, 0.48974609375, 0.88427734375, 0.02392578125, 0.4814453125, 0.99951171875, 0.42333984375, 0.04345703125, 0.1376953125, 0.77001953125, 0.23779296875, 0.8798828125, 0.0556640625, 0.0478515625, 0.60302734375, 0.6279296875, 0.87158203125, 0.62353515625, 0.13330078125, 0.3603515625, 0.14111328125, 0.78759765625, 0.03369140625, 0.2109375, 0.18115234375, 0.64892578125, 0.05615234375, 0.12939453125, 0.04248046875, 0.67041015625, 0.373046875, 0.205078125, 0.63525390625, 0.24365234375, 0.84716796875, 0.57373046875, 0.02734375, 0.64697265625, 0.025390625, 0.57421875, 0.6181640625, 0.06396484375, 0.6904296875, 0.6005859375, 0.0478515625, 0.06689453125, 0.9990234375, 0.11328125, 0.06494140625, 0.15478515625, 0.07080078125, 0.044921875, 0.08642578125, 0.54833984375, 0.0537109375, 0.11669921875, 0.9384765625, 0.99951171875, 0.0830078125, 0.4521484375, 0.021484375, 0.60498046875, 0.5126953125, 0.041015625, 0.49951171875, 0.76708984375, 0.6455078125, 0.06884765625, 0.28173828125, 0.708984375, 0.564453125, 0.02783203125, 0.6220703125, 0.9990234375, 0.05810546875, 0.1494140625, 0.09619140625, 0.04736328125, 0.681640625, 0.11572265625, 0.0986328125, 0.064453125, 0.05908203125, 0.494140625, 0.10986328125, 0.9052734375, 0.65283203125, 0.673828125, 0.9990234375, 0.048828125, 0.0908203125, 0.05322265625, 0.64892578125, 0.048828125, 0.08349609375, 0.05712890625, 0.90283203125, 0.9208984375, 0.587890625, 0.0654296875, 0.07421875, 0.9990234375, 0.9990234375, 0.6826171875, 0.052734375, 0.02587890625, 0.08837890625, 0.70849609375, 0.3447265625, 0.59619140625, 0.81787109375, 0.09912109375, 0.9990234375, 0.119140625, 0.61767578125, 0.05615234375, 0.04443359375, 0.9990234375, 0.9990234375, 0.865234375, 0.10791015625, 0.99951171875, 0.04150390625, 0.03564453125, 0.35595703125, 0.59814453125, 0.23095703125, 0.0927734375, 0.505859375, 0.3984375, 0.59228515625, 0.6650390625, 0.65673828125, 0.041015625, 0.583984375, 0.99853515625, 0.0849609375, 0.0634765625, 0.2294921875, 0.6630859375, 0.6787109375, 0.0380859375, 0.60009765625, 0.0322265625, 0.6474609375, 0.62841796875, 0.6796875, 0.9990234375, 0.630859375, 0.89892578125, 0.9990234375, 0.60302734375, 0.88916015625, 0.04248046875, 0.03466796875, 0.91845703125, 0.91748046875, 0.05615234375, 0.31396484375, 0.28369140625, 0.15771484375, 0.17919921875, 0.68798828125, 0.61865234375, 0.60009765625, 0.078125, 0.0390625, 0.6298828125, 0.9990234375, 0.9990234375, 0.99951171875, 0.03515625, 0.9248046875, 0.9990234375, 0.92919921875, 0.6220703125, 0.92724609375, 0.04248046875, 0.03125]

 sparsity of   [0.01909722201526165, 0.0004340277810115367, 0.00021701389050576836, 0.759765625, 0.0577256940305233, 0.0368923619389534, 0.11328125, 0.007378472480922937, 0.01888020895421505, 0.0909288227558136, 0.0271267369389534, 0.02864583395421505, 0.02582465298473835, 0.0414496548473835, 0.02083333395421505, 0.1028645858168602, 0.0859375, 0.0106336809694767, 0.3585069477558136, 0.1056857630610466, 0.0559895820915699, 0.0, 0.0028211805038154125, 0.0384114570915699, 0.02799479104578495, 0.0698784738779068, 0.0983072891831398, 0.00021701389050576836, 0.2604166567325592, 0.01584201492369175, 0.0457899309694767, 0.185546875, 0.02777777798473835, 0.0377604179084301, 0.007595486007630825, 0.01822916604578495, 0.0078125, 0.01692708395421505, 0.7866753339767456, 0.0069444444961845875, 0.02973090298473835, 0.0262586809694767, 0.9474826455116272, 0.083984375, 0.0440538190305233, 0.0362413190305233, 0.02495659701526165, 0.0577256940305233, 0.0544704869389534, 0.0473090298473835, 0.1336805522441864, 0.0740017369389534, 0.0473090298473835, 0.03125, 0.0846354141831398, 0.0301649309694767, 0.015407986007630825, 0.013454861007630825, 0.010850694961845875, 0.0657552108168602, 0.010850694961845875, 0.041015625, 0.0494791679084301, 0.0225694440305233, 0.0232204869389534, 0.080946184694767, 0.0596788190305233, 0.8062065839767456, 0.1078559011220932, 0.0546875, 0.0540364570915699, 0.7697482705116272, 0.01410590298473835, 0.01171875, 0.0006510416860692203, 0.794921875, 0.0310329869389534, 0.0646701380610466, 0.0, 0.1603732705116272, 0.0824652761220932, 0.0651041641831398, 0.0, 0.6410590410232544, 0.0405815988779068, 0.0614149309694767, 0.759765625, 0.0831163227558136, 0.0223524309694767, 0.001953125, 0.0336371548473835, 0.0835503488779068, 0.9231770634651184, 0.1623263955116272, 0.0861545130610466, 0.01822916604578495, 0.0909288227558136, 0.0271267369389534, 0.0271267369389534, 0.0397135429084301, 0.0377604179084301, 0.7886284589767456, 0.0431857630610466, 0.14453125, 0.1595052033662796, 0.0690104141831398, 0.7445746660232544, 0.0414496548473835, 0.130859375, 0.008029513992369175, 0.0451388880610466, 0.1002604141831398, 0.00021701389050576836, 0.0, 0.0455729179084301, 0.0223524309694767, 0.0466579869389534, 0.0262586809694767, 0.0049913194961845875, 0.0720486119389534, 0.0970052108168602, 0.02690972201526165, 0.0336371548473835, 0.048828125, 0.0323350690305233, 0.01974826492369175, 0.02777777798473835, 0.02473958395421505, 0.0, 0.0640190988779068, 0.048828125, 0.1087239608168602, 0.033203125, 0.02300347201526165, 0.0310329869389534, 0.0496961809694767, 0.0922309011220932, 0.0271267369389534, 0.433159738779068, 0.0316840298473835, 0.0414496548473835, 0.2120225727558136, 0.01171875, 0.008463541977107525, 0.0, 0.0470920130610466, 0.014973958022892475, 0.1052517369389534, 0.02105034701526165, 0.0340711809694767, 0.3778211772441864, 0.02864583395421505, 0.02864583395421505, 0.0004340277810115367, 0.1640625, 0.0755208358168602, 0.0674913227558136, 0.0225694440305233, 0.7102864384651184, 0.0748697891831398, 0.6039496660232544, 0.0264756940305233, 0.0514322929084301, 0.002170138992369175, 0.0086805559694767, 0.02213541604578495, 0.02799479104578495, 0.02105034701526165, 0.02170138992369175, 0.0314670130610466, 0.02105034701526165, 0.112196184694767, 0.0440538190305233, 0.02473958395421505, 0.0857204869389534, 0.0861545130610466, 0.0251736119389534, 0.0542534738779068, 0.201171875, 0.0590277798473835, 0.02300347201526165, 0.0045572915114462376, 0.0857204869389534, 0.0486111119389534, 0.0, 0.4967447817325592, 0.364800363779068, 0.060546875, 0.082899309694767, 0.1297743022441864, 0.1642795205116272, 0.0627170130610466, 0.0366753488779068, 0.1319444477558136, 0.0015190972480922937, 0.02973090298473835, 0.0, 0.1369357705116272, 0.0223524309694767, 0.0234375, 0.02105034701526165, 0.013888888992369175, 0.01714409701526165, 0.0321180559694767, 0.02799479104578495, 0.02300347201526165, 0.01627604104578495, 0.1000434011220932, 0.0390625, 0.0614149309694767, 0.0049913194961845875, 0.0737847238779068, 0.02018229104578495, 0.0991753488779068, 0.7517361044883728, 0.111328125, 0.0, 0.0379774309694767, 0.0360243059694767, 0.0436197929084301, 0.1336805522441864, 0.0234375, 0.02300347201526165, 0.0329861119389534, 0.0, 0.115234375, 0.5093315839767456, 0.0572916679084301, 0.3591579794883728, 0.03125, 0.0184461809694767, 0.00021701389050576836, 0.02777777798473835, 0.02777777798473835, 0.029296875, 0.0394965298473835, 0.0470920130610466, 0.041015625, 0.010416666977107525, 0.008463541977107525, 0.0, 0.0206163190305233, 0.104383684694767, 0.0262586809694767, 0.1117621511220932, 0.01171875, 0.0013020833721384406, 0.0922309011220932, 0.0375434048473835, 0.1378038227558136, 0.0067274305038154125, 0.02734375, 0.01692708395421505, 0.259331613779068, 0.0, 0.0568576380610466, 0.01410590298473835, 0.0334201380610466, 0.0978732630610466, 0.0496961809694767, 0.0538194440305233, 0.0358072929084301, 0.0397135429084301, 0.60546875, 0.0857204869389534, 0.0106336809694767, 0.1260850727558136, 0.0766059011220932, 0.0329861119389534, 0.1634114533662796, 0.013454861007630825, 0.0928819477558136, 0.2109375, 0.017578125, 0.00021701389050576836, 0.0531684048473835, 0.5842013955116272, 0.0323350690305233, 0.0290798619389534, 0.0262586809694767, 0.0455729179084301, 0.0223524309694767, 0.0251736119389534, 0.0338541679084301, 0.063368059694767, 0.1746961772441864, 0.0679253488779068, 0.0013020833721384406, 0.1714409738779068, 0.0243055559694767, 0.017578125, 0.0453559048473835, 0.01909722201526165, 0.0512152798473835, 0.0047743055038154125, 0.0223524309694767, 0.0379774309694767, 0.0329861119389534, 0.6032986044883728, 0.1538628488779068, 0.0184461809694767, 0.0815972238779068, 0.01779513992369175, 0.0904947891831398, 0.02777777798473835, 0.0648871511220932, 0.80078125, 0.02864583395421505, 0.1642795205116272, 0.02083333395421505, 0.6480034589767456, 0.01888020895421505, 0.1265190988779068, 0.1690538227558136, 0.1733940988779068, 0.0763888880610466, 0.0355902798473835, 0.0, 0.02213541604578495, 0.029296875, 0.1039496511220932, 0.0544704869389534, 0.067274309694767, 0.0427517369389534, 0.0802951380610466, 0.0462239570915699, 0.0944010391831398, 0.0334201380610466, 0.0416666679084301, 0.1280381977558136, 0.0342881940305233, 0.0438368059694767, 0.0505642369389534, 0.0, 0.0145399309694767, 0.02669270895421505, 0.0334201380610466, 0.0440538190305233, 0.0223524309694767, 0.02191840298473835, 0.1295572966337204, 0.02669270895421505, 0.008897569961845875, 0.03081597201526165, 0.1397569477558136, 0.01128472201526165, 0.0186631940305233, 0.02582465298473835, 0.01171875, 0.373046875, 0.0319010429084301, 0.009114583022892475, 0.02734375, 0.0483940988779068, 0.0514322929084301, 0.0661892369389534, 0.0327690988779068, 0.0193142369389534, 0.1317274272441864, 0.01519097201526165, 0.0243055559694767, 0.8125, 0.0321180559694767, 0.1312934011220932, 0.0373263880610466, 0.0310329869389534, 0.008463541977107525, 0.0045572915114462376, 0.0342881940305233, 0.1017795130610466, 0.0939670130610466, 0.0004340277810115367, 0.094618059694767, 0.076171875, 0.0347222238779068, 0.02994791604578495, 0.0316840298473835, 0.0763888880610466, 0.1471354216337204, 0.0184461809694767, 0.0321180559694767, 0.0310329869389534, 0.0638020858168602, 0.0338541679084301, 0.02690972201526165, 0.0592447929084301, 0.647569477558136, 0.0744357630610466, 0.0373263880610466, 0.0232204869389534, 0.0234375, 0.1126302108168602, 0.0251736119389534, 0.0023871527519077063, 0.02951388992369175, 0.025390625, 0.0549045130610466, 0.01779513992369175, 0.0440538190305233, 0.0271267369389534, 0.02604166604578495, 0.1193576380610466, 0.0503472238779068, 0.0377604179084301, 0.0290798619389534, 0.0863715261220932, 0.0, 0.0935329869389534, 0.1807725727558136, 0.01779513992369175, 0.011067708022892475, 0.001953125, 0.0462239570915699, 0.0362413190305233, 0.0622829869389534, 0.0796440988779068, 0.22265625, 0.0206163190305233, 0.02777777798473835, 0.0427517369389534, 0.02300347201526165, 0.02278645895421505, 0.0386284738779068, 0.0234375, 0.0, 0.0473090298473835, 0.0434027798473835, 0.0032552082557231188, 0.0657552108168602, 0.1124131977558136, 0.00021701389050576836, 0.0431857630610466, 0.0735677108168602, 0.5520833134651184, 0.3389756977558136, 0.1343315988779068, 0.0086805559694767, 0.2259114533662796, 0.3780381977558136, 0.0329861119389534, 0.0559895820915699, 0.7669270634651184, 0.011501736007630825, 0.0360243059694767, 0.5805121660232544, 0.0184461809694767, 0.0, 0.098524309694767, 0.075086809694767, 0.0826822891831398, 0.0590277798473835, 0.015625, 0.014322916977107525, 0.0993923619389534, 0.01953125, 0.0225694440305233, 0.01888020895421505, 0.0455729179084301, 0.0703125, 0.0186631940305233, 0.0284288190305233, 0.02473958395421505, 0.02994791604578495, 0.0336371548473835, 0.037109375, 0.0006510416860692203, 0.041015625, 0.0347222238779068, 0.0, 0.02604166604578495, 0.9993489384651184, 0.0972222238779068, 0.0243055559694767, 0.02756076492369175, 0.013671875, 0.0768229141831398, 0.0368923619389534, 0.0, 0.041015625, 0.0616319440305233, 0.0538194440305233, 0.0418836809694767, 0.0562065988779068, 0.0394965298473835, 0.02994791604578495, 0.017578125, 0.0855034738779068, 0.0390625, 0.0655381977558136, 0.0316840298473835, 0.0030381944961845875, 0.0, 0.0186631940305233, 0.011935763992369175, 0.0546875, 0.0407986119389534, 0.0913628488779068, 0.010850694961845875, 0.0483940988779068, 0.007595486007630825, 0.00021701389050576836, 0.0462239570915699, 0.009548611007630825, 0.02105034701526165, 0.02473958395421505, 0.0794270858168602, 0.0, 0.03125, 0.2120225727558136, 0.02690972201526165, 0.0368923619389534, 0.0381944440305233]

 sparsity of   [0.001953125, 0.01953125, 0.99609375, 0.138671875, 0.001953125, 0.017578125, 0.36328125, 0.056640625, 0.0390625, 0.015625, 0.05078125, 0.025390625, 0.029296875, 0.087890625, 0.0078125, 0.00390625, 0.017578125, 0.056640625, 0.001953125, 0.05859375, 0.046875, 0.96875, 0.126953125, 0.0, 0.013671875, 0.001953125, 0.07421875, 0.015625, 0.00390625, 0.06640625, 0.060546875, 0.009765625, 0.001953125, 0.923828125, 0.8828125, 0.0, 0.00390625, 0.02734375, 0.119140625, 0.001953125, 0.0390625, 0.015625, 0.001953125, 0.021484375, 0.013671875, 0.046875, 0.0, 0.013671875, 0.0234375, 0.0, 0.05859375, 0.001953125, 0.9375, 0.064453125, 0.0078125, 0.017578125, 0.599609375, 0.05078125, 0.02734375, 0.00390625, 0.013671875, 0.041015625, 0.025390625, 0.0, 0.07421875, 0.052734375, 0.0234375, 0.98046875, 0.95703125, 0.720703125, 0.0546875, 0.0, 0.298828125, 0.15625, 0.021484375, 0.021484375, 0.08984375, 0.0, 0.056640625, 0.033203125, 0.0, 0.056640625, 0.017578125, 0.017578125, 0.0, 0.8515625, 0.009765625, 0.021484375, 0.033203125, 0.140625, 0.017578125, 0.005859375, 0.0078125, 0.447265625, 0.0390625, 0.044921875, 0.025390625, 0.05078125, 0.021484375, 0.029296875, 0.19140625, 0.064453125, 0.00390625, 0.037109375, 0.001953125, 0.0859375, 0.0390625, 0.033203125, 0.041015625, 0.03515625, 0.134765625, 0.060546875, 0.0546875, 0.0, 0.0, 0.03125, 0.07421875, 0.021484375, 0.001953125, 0.05078125, 0.025390625, 0.001953125, 0.0078125, 0.99609375, 0.005859375, 0.017578125, 0.044921875, 0.080078125, 0.03125, 0.033203125, 0.029296875, 0.05078125, 0.01953125, 0.021484375, 0.03515625, 0.001953125, 0.005859375, 0.052734375, 0.009765625, 0.01953125, 0.0, 0.0234375, 0.009765625, 0.9375, 0.11328125, 0.017578125, 0.009765625, 0.13671875, 0.0, 0.1328125, 0.1484375, 0.83984375, 0.04296875, 0.9921875, 0.060546875, 0.130859375, 0.001953125, 0.009765625, 0.130859375, 0.0, 0.0234375, 0.0, 0.025390625, 0.08984375, 0.015625, 0.015625, 0.0, 0.048828125, 0.001953125, 0.033203125, 0.072265625, 0.02734375, 0.00390625, 0.0, 0.109375, 0.01953125, 0.09375, 0.078125, 0.12890625, 0.041015625, 0.07421875, 0.017578125, 0.13671875, 0.013671875, 0.005859375, 0.009765625, 0.0, 0.001953125, 0.009765625, 0.017578125, 0.078125, 0.013671875, 0.01953125, 0.26171875, 0.009765625, 0.0, 0.296875, 0.0, 0.96875, 0.0, 0.05078125, 0.9921875, 0.076171875, 0.009765625, 0.015625, 0.015625, 0.013671875, 0.001953125, 0.03125, 0.96484375, 0.02734375, 0.013671875, 0.0390625, 0.009765625, 0.0625, 0.935546875, 0.03515625, 0.884765625, 0.046875, 0.0, 0.033203125, 0.009765625, 0.09375, 0.0, 0.009765625, 0.0078125, 0.921875, 0.34375, 0.01171875, 0.455078125, 0.125, 0.025390625, 0.09765625, 0.001953125, 0.9765625, 0.03125, 0.037109375, 0.744140625, 0.0078125, 0.0, 0.845703125, 0.072265625, 0.056640625, 0.294921875, 0.0, 0.0078125, 0.01171875, 0.0078125, 0.029296875, 0.126953125, 0.013671875, 0.052734375, 0.0234375, 0.056640625, 0.009765625, 0.033203125, 0.015625, 0.08984375, 0.015625, 0.0, 0.072265625, 0.0390625, 0.990234375, 0.99609375, 0.1015625, 0.044921875, 0.025390625, 0.01953125, 0.1015625, 0.001953125, 0.978515625, 0.0, 0.1796875, 0.001953125, 0.02734375, 0.892578125, 0.029296875, 0.0, 0.0, 0.21875, 0.0, 0.0078125, 0.001953125, 0.0, 0.03125, 0.01171875, 0.0, 0.001953125, 0.005859375, 0.0, 0.041015625, 0.009765625, 0.001953125, 0.0, 0.017578125, 0.025390625, 0.197265625, 0.00390625, 0.0, 0.021484375, 0.029296875, 0.00390625, 0.03515625, 0.0546875, 0.005859375, 0.09765625, 0.015625, 0.099609375, 0.03125, 0.087890625, 0.001953125, 0.287109375, 0.0078125, 0.005859375, 0.068359375, 0.009765625, 0.001953125, 0.046875, 0.02734375, 0.568359375, 0.0390625, 0.001953125, 0.0234375, 0.025390625, 0.00390625, 0.0, 0.0234375, 0.0546875, 0.072265625, 0.046875, 0.015625, 0.015625, 0.009765625, 0.052734375, 0.951171875, 0.052734375, 0.150390625, 0.009765625, 0.109375, 0.06640625, 0.005859375, 0.013671875, 0.00390625, 0.00390625, 0.0078125, 0.138671875, 0.03515625, 0.0, 0.01953125, 0.0703125, 0.001953125, 0.05859375, 0.0, 0.0078125, 0.1015625, 0.0, 0.03125, 0.01171875, 0.046875, 0.015625, 0.041015625, 0.0, 0.013671875, 0.029296875, 0.029296875, 0.033203125, 0.025390625, 0.01171875, 0.76953125, 0.009765625, 0.154296875, 0.0, 0.0390625, 0.017578125, 0.025390625, 0.005859375, 0.013671875, 0.001953125, 0.05078125, 0.0, 0.11328125, 0.0390625, 0.021484375, 0.173828125, 0.18359375, 0.990234375, 0.001953125, 0.119140625, 0.05078125, 0.03125, 0.16015625, 0.0078125, 0.99609375, 0.0, 0.0, 0.88671875, 0.013671875, 0.96484375, 0.052734375, 0.05078125, 0.69921875, 0.015625, 0.001953125, 0.044921875, 0.0078125, 0.013671875, 0.029296875, 0.0, 0.005859375, 0.03515625, 0.01171875, 0.06640625, 0.21484375, 0.0, 0.91015625, 0.119140625, 0.060546875, 0.005859375, 0.05859375, 0.109375, 0.884765625, 0.001953125, 0.001953125, 0.001953125, 0.03125, 0.04296875, 0.015625, 0.0078125, 0.013671875, 0.01171875, 0.017578125, 0.0, 0.0, 0.091796875, 0.03515625, 0.88671875, 0.013671875, 0.0, 0.158203125, 0.048828125, 0.0390625, 0.00390625, 0.00390625, 0.998046875, 0.025390625, 0.03125, 0.05078125, 0.05859375, 0.90234375, 0.017578125, 0.017578125, 0.00390625, 0.001953125, 0.0, 0.01953125, 0.1640625, 0.8828125, 0.9921875, 0.029296875, 0.078125, 0.0, 0.0078125, 0.013671875, 0.005859375, 0.041015625, 0.001953125, 0.0, 0.0, 0.005859375, 0.033203125, 0.99609375, 0.099609375, 0.048828125, 0.009765625, 0.072265625, 0.03125, 0.025390625, 0.017578125, 0.822265625, 0.03515625, 0.0, 0.060546875, 0.00390625, 0.0, 0.0234375, 0.015625, 0.03515625, 0.0390625, 0.0, 0.0, 0.046875, 0.01953125, 0.0, 0.009765625, 0.0078125, 0.001953125, 0.021484375, 0.015625, 0.03125, 0.029296875, 0.0, 0.015625, 0.998046875, 0.01171875, 0.11328125, 0.00390625, 0.009765625, 0.06640625, 0.01953125, 0.013671875, 0.080078125, 0.138671875, 0.01171875, 0.052734375, 0.001953125, 0.021484375, 0.06640625, 0.00390625, 0.0, 0.044921875, 0.861328125, 0.0, 0.0, 0.0, 0.017578125, 0.044921875, 0.0, 0.923828125, 0.033203125, 0.0078125, 0.021484375, 0.107421875, 0.158203125, 0.00390625, 0.052734375, 0.05859375, 0.03125, 0.046875, 0.0234375, 0.0078125, 0.033203125, 0.228515625, 0.03515625, 0.0, 0.0, 0.0, 0.0234375, 0.037109375, 0.021484375, 0.0, 0.04296875, 0.134765625, 0.0, 0.0, 0.0, 0.02734375, 0.009765625, 0.005859375, 0.0390625, 0.087890625, 0.0, 0.998046875, 0.046875, 0.15234375, 0.958984375, 0.009765625, 0.041015625, 0.015625, 0.0390625, 0.017578125, 0.037109375, 0.126953125, 0.00390625, 0.009765625, 0.0, 0.0390625, 0.001953125, 0.001953125, 0.02734375, 0.001953125, 0.0, 0.06640625, 0.009765625, 0.013671875, 0.01171875, 0.05859375, 0.0390625, 0.013671875, 0.056640625, 0.048828125, 0.0, 0.0, 0.0, 0.169921875, 0.98828125, 0.08203125, 0.001953125, 0.0703125, 0.005859375, 0.060546875, 0.00390625, 0.0, 0.03125, 0.0234375, 0.048828125, 0.0, 0.013671875, 0.271484375, 0.0, 0.0078125, 0.029296875, 0.03515625, 0.0078125, 0.0, 0.021484375, 0.02734375, 0.009765625, 0.021484375, 0.021484375, 0.91015625, 0.015625, 0.0078125, 0.037109375, 0.013671875, 0.0625, 0.009765625, 0.0, 0.03125, 0.021484375, 0.2109375, 0.017578125, 0.005859375, 0.015625, 0.029296875, 0.009765625, 0.029296875, 0.125, 0.974609375, 0.0, 0.0078125, 0.416015625, 0.021484375, 0.0, 0.068359375, 0.05078125, 0.0, 0.9609375, 0.025390625, 0.037109375, 0.0546875, 0.001953125, 0.001953125, 0.005859375, 0.0234375, 0.220703125, 0.0, 0.021484375, 0.03125, 0.03515625, 0.03515625, 0.001953125, 0.02734375, 0.0234375, 0.087890625, 0.23828125, 0.16015625, 0.03515625, 0.044921875, 0.2109375, 0.00390625, 0.025390625, 0.01953125, 0.02734375, 0.05078125, 0.001953125, 0.998046875, 0.0, 0.71875, 0.0390625, 0.0, 0.015625, 0.009765625, 0.14453125, 0.0, 0.041015625, 0.017578125, 0.001953125, 0.044921875, 0.01953125, 0.0, 0.060546875, 0.78515625, 0.015625, 0.0, 0.029296875, 0.021484375, 0.03125, 0.017578125, 0.02734375, 0.044921875, 0.0, 0.00390625, 0.00390625, 0.9609375, 0.0, 0.01171875, 0.05859375, 0.0, 0.060546875, 0.009765625, 0.001953125, 0.02734375, 0.01953125, 0.08203125, 0.0, 0.068359375, 0.009765625, 0.787109375, 0.021484375, 0.029296875, 0.0, 0.072265625, 0.017578125, 0.0, 0.0, 0.94921875, 0.013671875, 0.08984375, 0.0234375, 0.052734375, 0.076171875, 0.0, 0.052734375, 0.0, 0.32421875, 0.04296875, 0.953125, 0.0, 0.00390625, 0.001953125, 0.029296875, 0.005859375, 0.10546875, 0.0, 0.0, 0.1171875, 0.009765625, 0.025390625, 0.0, 0.96875, 0.060546875, 0.01171875, 0.048828125, 0.001953125, 0.021484375, 0.03515625, 0.037109375, 0.9921875, 0.025390625, 0.0546875, 0.0, 0.1484375, 0.0234375, 0.0, 0.001953125, 0.021484375, 0.08984375, 0.01953125, 0.880859375, 0.0, 0.03125, 0.04296875, 0.994140625, 0.005859375, 0.056640625, 0.048828125, 0.001953125, 0.021484375, 0.005859375, 0.087890625, 0.005859375, 0.001953125, 0.037109375, 0.697265625, 0.029296875, 0.09765625, 0.93359375, 0.0, 0.083984375, 0.0, 0.177734375, 0.0, 0.025390625, 0.0, 0.1015625, 0.095703125, 0.0078125, 0.986328125, 0.021484375, 0.0, 0.013671875, 0.0625, 0.0078125, 0.0, 0.037109375, 0.03515625, 0.0, 0.115234375, 0.0, 0.0625, 0.0078125, 0.787109375, 0.03515625, 0.0, 0.876953125, 0.0, 0.013671875, 0.0, 0.041015625, 0.048828125, 0.01953125, 0.06640625, 0.0078125, 0.0, 0.87109375, 0.017578125, 0.00390625, 0.017578125, 0.005859375, 0.0, 0.1640625, 0.029296875, 0.01953125, 0.001953125, 0.0, 0.02734375, 0.03125, 0.03125, 0.0, 0.109375, 0.08984375, 0.001953125, 0.0546875, 0.00390625, 0.009765625, 0.033203125, 0.001953125, 0.99609375, 0.03125, 0.01171875, 0.01953125, 0.060546875, 0.02734375, 0.0625, 0.0859375, 0.0078125, 0.0, 0.017578125, 0.013671875, 0.0546875, 0.05078125, 0.978515625, 0.041015625, 0.001953125, 0.0, 0.025390625, 0.001953125, 0.0, 0.138671875, 0.037109375, 0.53515625, 0.068359375, 0.109375, 0.041015625, 0.03125, 0.9453125, 0.009765625, 0.0, 0.00390625, 0.044921875, 0.0859375, 0.021484375, 0.005859375, 0.001953125, 0.015625, 0.03125, 0.9765625, 0.0078125, 0.052734375, 0.115234375, 0.029296875, 0.11328125, 0.958984375, 0.0625, 0.990234375, 0.921875, 0.0, 0.0859375, 0.009765625, 0.0, 0.56640625, 0.0, 0.103515625, 0.1796875, 0.025390625, 0.0, 0.009765625, 0.927734375, 0.005859375, 0.041015625, 0.05078125, 0.99609375, 0.109375, 0.76953125, 0.00390625, 0.0, 0.005859375, 0.005859375, 0.0, 0.017578125, 0.0, 0.001953125, 0.02734375, 0.005859375, 0.0, 0.033203125, 0.00390625, 0.0078125, 0.017578125, 0.15234375, 0.015625, 0.0, 0.7109375, 0.04296875, 0.06640625, 0.048828125, 0.0078125, 0.025390625, 0.0078125, 0.033203125, 0.97265625, 0.0, 0.076171875, 0.01953125, 0.05078125, 0.099609375, 0.876953125, 0.0, 0.0, 0.048828125, 0.99609375, 0.0, 0.845703125, 0.025390625, 0.056640625, 0.00390625, 0.001953125, 0.31640625, 0.037109375, 0.01953125, 0.0390625, 0.03515625, 0.033203125, 0.046875, 0.99609375, 0.091796875, 0.0, 0.029296875, 0.0, 0.009765625, 0.0390625, 0.009765625, 0.03515625, 0.00390625, 0.00390625, 0.017578125, 0.0, 0.0390625, 0.0, 0.0078125, 0.08203125, 0.05859375, 0.013671875, 0.0, 0.0, 0.005859375, 0.0078125, 0.005859375, 0.083984375, 0.0, 0.138671875, 0.017578125, 0.00390625, 0.0, 0.099609375, 0.9921875, 0.0, 0.9375, 0.0, 0.005859375, 0.001953125, 0.009765625, 0.0234375, 0.025390625, 0.0, 0.876953125, 0.01953125, 0.013671875, 0.0, 0.048828125, 0.0, 0.130859375, 0.005859375, 0.03515625, 0.4765625, 0.0, 0.123046875, 0.0, 0.0, 0.01171875, 0.037109375, 0.0, 0.044921875, 0.03125, 0.01953125, 0.001953125, 0.033203125, 0.990234375, 0.05078125, 0.060546875, 0.0078125, 0.044921875, 0.015625, 0.919921875, 0.009765625, 0.001953125, 0.005859375, 0.076171875, 0.05078125, 0.01171875, 0.166015625, 0.201171875, 0.0625, 0.013671875, 0.03515625, 0.015625, 0.044921875, 0.986328125, 0.013671875, 0.470703125, 0.03515625, 0.0234375, 0.0, 0.021484375, 0.05078125, 0.0, 0.00390625, 0.021484375, 0.052734375, 0.037109375, 0.015625, 0.029296875, 0.0390625, 0.03125, 0.046875, 0.015625, 0.0, 0.0, 0.001953125, 0.001953125, 0.9375, 0.037109375, 0.0625, 0.021484375, 0.001953125, 0.052734375, 0.0390625, 0.00390625, 0.00390625, 0.0, 0.048828125, 0.111328125, 0.00390625, 0.0, 0.974609375, 0.0, 0.0, 0.021484375, 0.056640625, 0.01171875, 0.0, 0.55078125, 0.0703125, 0.171875, 0.015625, 0.99609375, 0.00390625, 0.072265625, 0.0234375, 0.89453125, 0.013671875, 0.23828125, 0.001953125, 0.05859375, 0.03515625, 0.015625, 0.017578125, 0.05078125, 0.0234375, 0.119140625, 0.095703125, 0.001953125, 0.005859375, 0.09375, 0.015625, 0.900390625, 0.0234375, 0.041015625, 0.109375, 0.0, 0.0078125, 0.01171875, 0.43359375, 0.09765625, 0.0, 0.037109375, 0.001953125, 0.0078125, 0.064453125, 0.03125, 0.0, 0.001953125, 0.115234375, 0.0234375, 0.025390625, 0.01953125, 0.025390625, 0.169921875, 0.03125, 0.0234375, 0.056640625, 0.107421875, 0.052734375, 0.076171875, 0.0234375, 0.009765625, 0.00390625, 0.03515625, 0.046875, 0.01953125, 0.046875, 0.94140625, 0.005859375, 0.021484375, 0.044921875, 0.0, 0.005859375, 0.033203125, 0.001953125, 0.048828125, 0.001953125, 0.896484375, 0.0390625, 0.0, 0.087890625, 0.916015625, 0.0, 0.087890625, 0.0, 0.013671875, 0.03125, 0.001953125, 0.015625, 0.0078125, 0.0, 0.01953125, 0.072265625, 0.0, 0.015625, 0.00390625, 0.001953125, 0.45703125, 0.0, 0.0, 0.001953125, 0.037109375, 0.041015625, 0.001953125, 0.0, 0.150390625, 0.04296875, 0.001953125, 0.013671875, 0.09375, 0.28125, 0.02734375, 0.01171875, 0.0, 0.017578125, 0.45703125, 0.041015625, 0.08203125, 0.009765625, 0.05078125, 0.01953125, 0.0234375, 0.0, 0.04296875, 0.03515625, 0.001953125, 0.0, 0.0, 0.015625, 0.03125, 0.109375, 0.041015625, 0.0234375, 0.005859375, 0.033203125, 0.037109375, 0.060546875, 0.04296875, 0.048828125, 0.994140625, 0.146484375, 0.115234375, 0.001953125, 0.001953125, 0.017578125, 0.919921875, 0.119140625, 0.111328125, 0.103515625, 0.03125, 0.10546875, 0.078125, 0.701171875, 0.013671875, 0.0, 0.04296875, 0.619140625, 0.0234375, 0.013671875, 0.091796875, 0.0, 0.0, 0.0, 0.013671875, 0.005859375, 0.072265625, 0.0, 0.046875, 0.0078125, 0.037109375, 0.1015625, 0.0625, 0.01171875, 0.107421875, 0.0, 0.041015625, 0.005859375, 0.05078125, 0.01171875, 0.13671875, 0.1015625, 0.671875, 0.0, 0.005859375, 0.2109375, 0.01953125, 0.01953125, 0.013671875, 0.0234375, 0.0, 0.01171875, 0.12890625, 0.072265625, 0.025390625, 0.892578125, 0.03515625, 0.0859375, 0.0625, 0.0, 0.599609375, 0.044921875, 0.150390625, 0.11328125, 0.208984375, 0.0, 0.029296875, 0.177734375, 0.0, 0.0, 0.03515625, 0.06640625, 0.0234375, 0.0, 0.00390625, 0.0078125, 0.005859375, 0.0, 0.001953125, 0.001953125, 0.009765625, 0.029296875, 0.001953125, 0.046875, 0.033203125, 0.982421875, 0.02734375, 0.009765625, 0.025390625, 0.701171875, 0.0, 0.025390625, 0.0078125, 0.02734375, 0.00390625, 0.0, 0.0, 0.00390625, 0.0, 0.015625, 0.0, 0.01953125, 0.01171875, 0.119140625, 0.48046875, 0.025390625, 0.001953125, 0.068359375, 0.009765625, 0.0, 0.005859375, 0.115234375, 0.02734375, 0.041015625, 0.115234375, 0.84375, 0.0546875, 0.9765625, 0.02734375, 0.01171875, 0.041015625, 0.0390625, 0.001953125, 0.0, 0.04296875, 0.037109375, 0.05078125, 0.001953125, 0.14453125, 0.0, 0.025390625, 0.013671875, 0.03515625, 0.021484375, 0.01171875, 0.076171875, 0.0078125, 0.181640625, 0.62890625, 0.009765625, 0.033203125, 0.0, 0.5078125, 0.037109375, 0.07421875, 0.0078125, 0.0, 0.966796875, 0.515625, 0.0, 0.0, 0.046875, 0.06640625, 0.556640625, 0.0, 0.021484375, 0.93359375, 0.146484375, 0.005859375, 0.986328125, 0.037109375, 0.052734375, 0.0, 0.013671875, 0.076171875, 0.162109375, 0.005859375, 0.08984375, 0.048828125, 0.001953125, 0.046875, 0.021484375, 0.0078125, 0.111328125, 0.052734375, 0.013671875, 0.0078125, 0.005859375, 0.16796875, 0.00390625, 0.0, 0.00390625, 0.001953125, 0.087890625, 0.001953125, 0.94921875, 0.01171875, 0.119140625, 0.076171875, 0.076171875, 0.103515625, 0.0, 0.021484375, 0.0, 0.005859375, 0.041015625, 0.923828125, 0.017578125, 0.048828125, 0.0, 0.0, 0.005859375, 0.890625, 0.072265625, 0.00390625, 0.01953125, 0.076171875, 0.013671875, 0.021484375, 0.9921875, 0.0, 0.017578125, 0.0625, 0.0, 0.001953125, 0.048828125, 0.005859375, 0.017578125, 0.125, 0.0, 0.005859375, 0.119140625, 0.4609375, 0.140625, 0.01953125, 0.01953125, 0.013671875, 0.01171875, 0.001953125, 0.0, 0.005859375, 0.0078125, 0.060546875, 0.013671875, 0.015625, 0.0703125, 0.0, 0.08984375, 0.05859375, 0.0, 0.0, 0.0078125, 0.072265625, 0.03515625, 0.005859375, 0.044921875, 0.0, 0.009765625, 0.265625, 0.005859375, 0.0, 0.501953125, 0.00390625, 0.0390625, 0.1015625, 0.0, 0.025390625, 0.0703125, 0.01171875, 0.0078125, 0.046875, 0.015625, 0.029296875, 0.03515625, 0.0859375, 0.0, 0.0, 0.982421875, 0.009765625, 0.03125, 0.103515625, 0.01953125, 0.0390625, 0.056640625, 0.994140625, 0.0078125, 0.041015625, 0.001953125, 0.0234375, 0.083984375, 0.033203125, 0.056640625, 0.005859375, 0.046875, 0.025390625, 0.18359375, 0.0, 0.380859375, 0.046875, 0.068359375, 0.021484375, 0.009765625, 0.044921875, 0.998046875, 0.0234375, 0.0390625, 0.00390625, 0.0, 0.029296875, 0.001953125, 0.0, 0.984375, 0.064453125, 0.021484375, 0.0, 0.060546875, 0.00390625, 0.0, 0.03125, 0.017578125, 0.03125, 0.01953125, 0.015625, 0.068359375, 0.09375, 0.01953125, 0.052734375, 0.0859375, 0.041015625, 0.82421875, 0.939453125, 0.072265625, 0.029296875, 0.05859375, 0.0078125, 0.09765625, 0.150390625, 0.1171875, 0.013671875, 0.0390625, 0.01171875, 0.025390625, 0.017578125, 0.02734375, 0.0, 0.060546875, 0.0859375, 0.001953125, 0.072265625, 0.146484375, 0.00390625, 0.01171875, 0.00390625, 0.009765625, 0.01953125, 0.044921875, 0.01953125, 0.033203125, 0.0078125, 0.00390625, 0.0, 0.052734375, 0.02734375, 0.064453125, 0.005859375, 0.029296875, 0.08984375, 0.03125, 0.017578125, 0.05859375, 0.01171875, 0.056640625, 0.02734375, 0.03125, 0.033203125, 0.998046875, 0.01953125, 0.0078125, 0.0, 0.0, 0.03515625, 0.97265625, 0.04296875, 0.0234375, 0.017578125, 0.046875, 0.025390625, 0.0, 0.017578125, 0.03125, 0.0, 0.037109375, 0.033203125, 0.0, 0.087890625, 0.04296875, 0.0, 0.0, 0.013671875, 0.1328125, 0.00390625, 0.9921875, 0.017578125, 0.04296875, 0.16015625, 0.009765625, 0.025390625, 0.00390625, 0.00390625, 0.00390625, 0.013671875, 0.09375, 0.01171875, 0.037109375, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.005859375, 0.001953125, 0.9921875, 0.00390625, 0.6484375, 0.0078125, 0.041015625, 0.0078125, 0.0, 0.0, 0.029296875, 0.03125, 0.04296875, 0.00390625, 0.0, 0.02734375, 0.00390625, 0.029296875, 0.04296875, 0.0, 0.0, 0.087890625, 0.130859375, 0.037109375, 0.0, 0.015625, 0.0, 0.03515625, 0.0078125, 0.0, 0.09765625, 0.00390625, 0.115234375, 0.013671875, 0.01953125, 0.0078125, 0.59375, 0.046875, 0.0, 0.03515625, 0.01953125, 0.001953125, 0.48828125, 0.775390625, 0.03125, 0.396484375, 0.037109375, 0.01171875, 0.06640625, 0.0, 0.021484375, 0.044921875, 0.984375, 0.0, 0.408203125, 0.134765625, 0.052734375, 0.0, 0.025390625, 0.009765625, 0.0, 0.888671875, 0.025390625, 0.0546875, 0.08984375, 0.037109375, 0.087890625, 0.00390625, 0.013671875, 0.029296875, 0.001953125, 0.0, 0.044921875, 0.01953125, 0.001953125, 0.13671875, 0.0703125, 0.001953125, 0.033203125, 0.044921875, 0.056640625, 0.0234375, 0.005859375, 0.01953125, 0.056640625, 0.0, 0.00390625, 0.060546875, 0.021484375, 0.00390625, 0.10546875, 0.02734375, 0.0546875, 0.9921875, 0.00390625, 0.021484375, 0.0, 0.01953125, 0.001953125, 0.0, 0.068359375, 0.140625, 0.05859375, 0.009765625, 0.046875, 0.01171875, 0.0234375, 0.0078125, 0.0, 0.0625, 0.888671875, 0.025390625, 0.0390625, 0.0, 0.00390625, 0.01171875, 0.025390625, 0.99609375, 0.0234375, 0.171875, 0.00390625, 0.01953125, 0.05078125, 0.0, 0.990234375, 0.091796875, 0.001953125, 0.017578125, 0.005859375, 0.03515625, 0.03125, 0.01171875, 0.009765625, 0.0, 0.048828125, 0.0, 0.0, 0.99609375, 0.033203125, 0.126953125, 0.00390625, 0.029296875, 0.0078125, 0.0, 0.02734375, 0.021484375, 0.091796875, 0.966796875, 0.01171875, 0.0, 0.0, 0.501953125, 0.037109375, 0.013671875, 0.056640625, 0.0234375, 0.01171875, 0.060546875, 0.0, 0.052734375, 0.0, 0.005859375, 0.005859375, 0.0, 0.001953125, 0.0234375, 0.05859375, 0.97265625, 0.005859375, 0.0546875, 0.03125, 0.0, 0.0, 0.0, 0.046875, 0.056640625, 0.001953125, 0.05859375, 0.0, 0.970703125, 0.958984375, 0.0625, 0.009765625, 0.029296875, 0.0, 0.92578125, 0.0, 0.01171875, 0.0, 0.001953125, 0.142578125, 0.064453125, 0.015625, 0.029296875, 0.001953125, 0.0078125, 0.0, 0.013671875, 0.0, 0.0, 0.052734375, 0.001953125, 0.009765625, 0.00390625, 0.005859375, 0.00390625, 0.0, 0.06640625, 0.0, 0.009765625, 0.005859375, 0.01171875, 0.046875, 0.01171875, 0.009765625, 0.0, 0.0625, 0.017578125, 0.927734375, 0.009765625, 0.966796875, 0.123046875, 0.033203125, 0.01171875, 0.013671875, 0.984375, 0.05078125, 0.955078125, 0.119140625, 0.0078125, 0.0234375, 0.044921875, 0.03515625, 0.001953125, 0.083984375, 0.052734375, 0.091796875, 0.01171875, 0.015625, 0.119140625, 0.00390625, 0.005859375, 0.03125, 0.0, 0.130859375, 0.12890625, 0.03515625, 0.01953125, 0.0078125, 0.080078125, 0.0078125, 0.033203125, 0.017578125, 0.033203125, 0.029296875, 0.0, 0.015625, 0.001953125, 0.052734375, 0.01953125, 0.009765625, 0.029296875, 0.091796875, 0.9296875, 0.03125, 0.001953125, 0.02734375, 0.013671875, 0.0, 0.083984375, 0.009765625, 0.02734375, 0.99609375, 0.005859375, 0.99609375, 0.009765625, 0.033203125, 0.021484375, 0.021484375, 0.07421875, 0.01171875, 0.01953125, 0.0, 0.005859375, 0.162109375, 0.083984375, 0.009765625, 0.01171875, 0.00390625, 0.001953125, 0.71484375, 0.02734375, 0.068359375, 0.044921875, 0.06640625, 0.05078125, 0.962890625, 0.017578125, 0.0625, 0.0, 0.0625, 0.0, 0.01953125, 0.0, 0.9921875, 0.001953125, 0.044921875, 0.029296875, 0.0, 0.00390625, 0.033203125, 0.33203125, 0.033203125, 0.0625, 0.044921875, 0.029296875, 0.2109375, 0.01953125, 0.021484375, 0.029296875, 0.00390625, 0.056640625, 0.0, 0.037109375, 0.08984375, 0.087890625, 0.046875, 0.083984375, 0.046875, 0.005859375, 0.033203125, 0.037109375, 0.017578125, 0.087890625, 0.009765625, 0.04296875, 0.00390625, 0.021484375, 0.041015625, 0.017578125, 0.021484375, 0.0, 0.0, 0.001953125, 0.00390625, 0.00390625, 0.998046875, 0.001953125, 0.939453125, 0.03125, 0.205078125, 0.046875, 0.015625, 0.0078125, 0.025390625, 0.029296875, 0.001953125, 0.99609375, 0.05078125, 0.189453125, 0.03515625, 0.046875, 0.0546875, 0.005859375, 0.0, 0.04296875, 0.021484375, 0.70703125, 0.013671875, 0.25, 0.03125, 0.015625, 0.046875, 0.04296875, 0.013671875, 0.0, 0.005859375, 0.07421875, 0.0234375, 0.021484375, 0.951171875, 0.28125, 0.99609375, 0.990234375, 0.00390625, 0.1953125, 0.0, 0.97265625, 0.01953125, 0.0078125, 0.009765625, 0.021484375, 0.01953125, 0.044921875, 0.015625, 0.037109375, 0.048828125, 0.107421875, 0.06640625, 0.03515625, 0.021484375, 0.052734375, 0.041015625, 0.00390625, 0.015625, 0.01171875, 0.01953125, 0.748046875, 0.021484375, 0.0, 0.900390625, 0.275390625, 0.001953125, 0.009765625, 0.0078125, 0.072265625]

 sparsity of   [0.03857421875, 0.02490234375, 0.0205078125, 0.07666015625, 0.0224609375, 0.2822265625, 0.9990234375, 0.677734375, 0.0947265625, 0.0263671875, 0.072265625, 0.048828125, 0.05126953125, 0.10986328125, 0.04052734375, 0.0439453125, 0.02099609375, 0.0673828125, 0.033203125, 0.04345703125, 0.04931640625, 0.03515625, 0.412109375, 0.01611328125, 0.037109375, 0.27783203125, 0.44287109375, 0.08056640625, 0.009765625, 0.0546875, 0.02392578125, 0.068359375, 0.29150390625, 0.28564453125, 0.0400390625, 0.06103515625, 0.654296875, 0.12255859375, 0.81005859375, 0.0400390625, 0.31591796875, 0.23583984375, 0.0244140625, 0.0400390625, 0.09130859375, 0.0263671875, 0.09814453125, 0.0380859375, 0.0673828125, 0.095703125, 0.1005859375, 0.03271484375, 0.1083984375, 0.0390625, 0.0498046875, 0.84228515625, 0.017578125, 0.19677734375, 0.0419921875, 0.07421875, 0.03173828125, 0.0771484375, 0.017578125, 0.0859375, 0.06494140625, 0.1328125, 0.0302734375, 0.09814453125, 0.060546875, 0.0546875, 0.0341796875, 0.08837890625, 0.9990234375, 0.03662109375, 0.029296875, 0.99951171875, 0.08935546875, 0.0712890625, 0.07958984375, 0.046875, 0.04833984375, 0.1005859375, 0.044921875, 0.140625, 0.89501953125, 0.05419921875, 0.1044921875, 0.0390625, 0.06787109375, 0.03125, 0.04931640625, 0.32666015625, 0.0634765625, 0.04443359375, 0.0888671875, 0.0341796875, 0.0595703125, 0.04541015625, 0.0390625, 0.05224609375, 0.25927734375, 0.04345703125, 0.02392578125, 0.04638671875, 0.04150390625, 0.05712890625, 0.17578125, 0.45751953125, 0.06591796875, 0.0478515625, 0.04150390625, 0.67236328125, 0.06298828125, 0.01953125, 0.06640625, 0.0283203125, 0.37158203125, 0.05712890625, 0.0166015625, 0.0263671875, 0.056640625, 0.03369140625, 0.38427734375, 0.0634765625, 0.02587890625, 0.041015625, 0.16259765625, 0.044921875, 0.05224609375, 0.0439453125, 0.04296875, 0.09716796875, 0.24462890625, 0.037109375, 0.05029296875, 0.02880859375, 0.22265625, 0.01611328125, 0.0234375, 0.0595703125, 0.03857421875, 0.0703125, 0.1435546875, 0.05615234375, 0.0810546875, 0.03564453125, 0.095703125, 0.060546875, 0.02197265625, 0.052734375, 0.00732421875, 0.1044921875, 0.0400390625, 0.07666015625, 0.09521484375, 0.19873046875, 0.0400390625, 0.611328125, 0.04736328125, 0.046875, 0.41943359375, 0.068359375, 0.0234375, 0.0107421875, 0.07421875, 0.04248046875, 0.13671875, 0.09765625, 0.0966796875, 0.060546875, 0.09326171875, 0.044921875, 0.046875, 0.03466796875, 0.11181640625, 0.99951171875, 0.03271484375, 0.4677734375, 0.04931640625, 0.08251953125, 0.0341796875, 0.30126953125, 0.03076171875, 0.2119140625, 0.66162109375, 0.00244140625, 0.0537109375, 0.13818359375, 0.6103515625, 0.015625, 0.12548828125, 0.10205078125, 0.03369140625, 0.0341796875, 0.08349609375, 0.0517578125, 0.05712890625, 0.04248046875, 0.0654296875, 0.9052734375, 0.00048828125, 0.0908203125, 0.060546875, 0.021484375, 0.41796875, 0.07080078125, 0.41357421875, 0.09130859375, 0.02783203125, 0.99951171875, 0.0322265625, 0.09716796875, 0.048828125, 0.021484375, 0.01171875, 0.1611328125, 0.037109375, 0.01416015625, 0.041015625, 0.083984375, 0.115234375, 0.19873046875, 0.03173828125, 0.015625, 0.0419921875, 0.04541015625, 0.6396484375, 0.08642578125, 0.04541015625, 0.44140625, 0.1943359375, 0.01318359375, 0.3212890625, 0.99853515625, 0.0498046875, 0.88330078125, 0.0703125, 0.0302734375, 0.04833984375, 0.0546875, 0.00634765625, 0.99951171875, 0.08740234375, 0.07080078125, 0.0673828125, 0.99853515625, 0.0771484375, 0.13232421875, 0.033203125, 0.99951171875, 0.1025390625, 0.01953125, 0.04296875, 0.072265625, 0.109375, 0.060546875, 0.04345703125, 0.037109375, 0.14892578125, 0.0439453125, 0.01123046875, 0.02490234375, 0.0927734375, 0.1708984375, 0.06591796875, 0.22265625, 0.0830078125, 0.02099609375, 0.06103515625, 0.7060546875, 0.08837890625, 0.0595703125, 0.02880859375, 0.0576171875, 0.92626953125, 0.40869140625, 0.1201171875, 0.02783203125, 0.08349609375, 0.09423828125, 0.0380859375, 0.08935546875, 0.46240234375, 0.06884765625, 0.0439453125, 0.8515625, 0.05419921875, 0.0966796875, 0.0986328125, 0.06396484375, 0.02587890625, 0.07861328125, 0.341796875, 0.05615234375, 0.02490234375, 0.06982421875, 0.08984375, 0.0693359375, 0.033203125, 0.04150390625, 0.11376953125, 0.0400390625, 0.0234375, 0.0302734375, 0.06982421875, 0.02783203125, 0.0390625, 0.03662109375, 0.181640625, 0.3544921875, 0.10400390625, 0.04345703125, 0.0107421875, 0.05517578125, 0.009765625, 0.412109375, 0.0400390625, 0.03173828125, 0.08154296875, 0.04052734375, 0.9140625, 0.08447265625, 0.0810546875, 0.01953125, 0.15087890625, 0.0166015625, 0.869140625, 0.04296875, 0.09716796875, 0.07177734375, 0.02099609375, 0.08544921875, 0.03125, 0.06494140625, 0.08984375, 0.0595703125, 0.044921875, 0.033203125, 0.1142578125, 0.03662109375, 0.03662109375, 0.0634765625, 0.05029296875, 0.046875, 0.06396484375, 0.037109375, 0.1689453125, 0.0244140625, 0.0400390625, 0.0458984375, 0.01171875, 0.06396484375, 0.02490234375, 0.28125, 0.0380859375, 0.03369140625, 0.32080078125, 0.4150390625, 0.05078125, 0.06494140625, 0.15185546875, 0.0478515625, 0.1201171875, 0.04736328125, 0.01318359375, 0.0830078125, 0.00830078125, 0.03369140625, 0.166015625, 0.00927734375, 0.08447265625, 0.181640625, 0.04638671875, 0.34814453125, 0.044921875, 0.06689453125, 0.20458984375, 0.25634765625, 0.2109375, 0.03369140625, 0.046875, 0.04150390625, 0.6689453125, 0.029296875, 0.396484375, 0.9990234375, 0.01171875, 0.16015625, 0.11376953125, 0.04052734375, 0.0400390625, 0.0810546875, 0.06201171875, 0.015625, 0.0244140625, 0.154296875, 0.0244140625, 0.04296875, 0.02294921875, 0.03125, 0.14697265625, 0.83642578125, 0.02685546875, 0.076171875, 0.10498046875, 0.220703125, 0.00732421875, 0.01123046875, 0.0234375, 0.07080078125, 0.0791015625, 0.11767578125, 0.0595703125, 0.99951171875, 0.078125, 0.02587890625, 0.01416015625, 0.03466796875, 0.02587890625, 0.0712890625, 0.046875, 0.052734375, 0.14111328125, 0.03662109375, 0.01171875, 0.02734375, 0.03955078125, 0.03076171875, 0.02880859375, 0.03759765625, 0.63037109375, 0.05322265625, 0.07421875, 0.0146484375, 0.0224609375, 0.01708984375, 0.0810546875, 0.05859375, 0.07861328125, 0.0615234375, 0.0400390625, 0.056640625, 0.06591796875, 0.0615234375, 0.05810546875, 0.9990234375, 0.18359375, 0.0283203125, 0.2421875, 0.02587890625, 0.021484375, 0.0634765625, 0.04296875, 0.13671875, 0.3466796875, 0.0380859375, 0.09423828125, 0.08740234375, 0.0986328125, 0.07568359375, 0.03955078125, 0.0419921875, 0.03857421875, 0.0400390625, 0.7734375, 0.373046875, 0.0419921875, 0.02197265625, 0.07958984375, 0.56103515625, 0.046875, 0.099609375, 0.05859375, 0.0537109375, 0.38232421875, 0.10400390625, 0.08203125, 0.10791015625, 0.61767578125, 0.123046875, 0.01953125, 0.0166015625, 0.0234375, 0.3349609375, 0.8447265625, 0.02099609375, 0.0625, 0.04833984375, 0.0458984375, 0.05078125, 0.1123046875, 0.07958984375, 0.03125, 0.04443359375, 0.05126953125, 0.10498046875, 0.09765625, 0.07666015625, 0.04150390625, 0.046875, 0.041015625, 0.0966796875, 0.06103515625, 0.09716796875, 0.99951171875, 0.03271484375, 0.013671875, 0.06005859375, 0.025390625, 0.45068359375, 0.033203125, 0.38525390625]

 sparsity of   [0.01627604104578495, 0.0362413190305233, 0.0388454869389534, 0.0206163190305233, 0.0557725690305233, 0.0922309011220932, 0.0616319440305233, 0.5603298544883728, 0.0262586809694767, 0.5959201455116272, 0.044921875, 0.012803819961845875, 0.017578125, 0.0792100727558136, 0.0321180559694767, 0.02408854104578495, 0.0538194440305233, 0.0427517369389534, 0.012369791977107525, 0.0423177070915699, 0.1740451455116272, 0.02973090298473835, 0.0746527761220932, 0.1165364608168602, 0.0442708320915699, 0.0397135429084301, 0.01692708395421505, 0.02365451492369175, 0.0544704869389534, 0.1223958358168602, 0.0466579869389534, 0.6056857705116272, 0.0776909738779068, 0.0477430559694767, 0.00390625, 0.0223524309694767, 0.0568576380610466, 0.01996527798473835, 0.007595486007630825, 0.552734375, 0.02191840298473835, 0.02799479104578495, 0.0920138880610466, 0.0290798619389534, 0.0167100690305233, 0.3461371660232544, 0.6536458134651184, 0.0555555559694767, 0.1260850727558136, 0.013020833022892475, 0.009982638992369175, 0.0373263880610466, 0.1028645858168602, 0.0544704869389534, 0.0049913194961845875, 0.1766493022441864, 0.0314670130610466, 0.0314670130610466, 0.0512152798473835, 0.0418836809694767, 0.0377604179084301, 0.0164930559694767, 0.02213541604578495, 0.02387152798473835, 0.1137152761220932, 0.0407986119389534, 0.02278645895421505, 0.004123263992369175, 0.0496961809694767, 0.065321184694767, 0.626085102558136, 0.0349392369389534, 0.0551215298473835, 0.0303819440305233, 0.1788194477558136, 0.0952690988779068, 0.0401475690305233, 0.3305121660232544, 0.0323350690305233, 0.1017795130610466, 0.025390625, 0.014756944961845875, 0.00933159701526165, 0.1569010466337204, 0.013888888992369175, 0.1163194477558136, 0.2293836772441864, 0.0557725690305233, 0.1041666641831398, 0.0562065988779068, 0.0342881940305233, 0.0501302070915699, 0.0846354141831398, 0.4186197817325592, 0.02495659701526165, 0.013020833022892475, 0.012803819961845875, 0.01410590298473835, 0.0323350690305233, 0.0523003488779068, 0.1119791641831398, 0.0462239570915699, 0.1714409738779068, 0.840928852558136, 0.0004340277810115367, 0.02799479104578495, 0.0670572891831398, 0.0353732630610466, 0.009548611007630825, 0.05078125, 0.0358072929084301, 0.0687934011220932, 0.0776909738779068, 0.115234375, 0.1349826455116272, 0.1059027761220932, 0.0483940988779068, 0.0896267369389534, 0.0243055559694767, 0.421440988779068, 0.0069444444961845875, 0.0323350690305233, 0.03125, 0.088758684694767, 0.041015625, 0.1022135391831398, 0.0776909738779068, 0.0620659738779068, 0.013454861007630825, 0.0451388880610466, 0.0583767369389534, 0.0455729179084301, 0.01410590298473835, 0.0403645820915699, 0.2432725727558136, 0.2330729216337204, 0.00933159701526165, 0.8532986044883728, 0.052734375, 0.060546875, 0.5399305820465088, 0.30859375, 0.0670572891831398, 0.0592447929084301, 0.1028645858168602, 0.7918837070465088, 0.0852864608168602, 0.0447048619389534, 0.9032118320465088, 0.017578125, 0.0282118059694767, 0.1050347238779068, 0.2387152761220932, 0.0052083334885537624, 0.041015625, 0.013888888992369175, 0.0145399309694767, 0.112196184694767, 0.1130642369389534, 0.0533854179084301, 0.3114149272441864, 0.0766059011220932, 0.015407986007630825, 0.033203125, 0.0065104165114462376, 0.080078125, 0.0640190988779068, 0.0460069440305233, 0.0759548619389534, 0.014973958022892475, 0.5794270634651184, 0.015407986007630825, 0.0366753488779068, 0.0514322929084301, 0.386284738779068, 0.015625, 0.529296875, 0.1723090261220932, 0.1089409738779068, 0.0349392369389534, 0.0045572915114462376, 0.0499131940305233, 0.0442708320915699, 0.0681423619389534, 0.0388454869389534, 0.1937934011220932, 0.0353732630610466, 0.3702256977558136, 0.1959635466337204, 0.0206163190305233, 0.0386284738779068, 0.0345052070915699, 0.017578125, 0.3155381977558136, 0.0349392369389534, 0.095703125, 0.0047743055038154125, 0.179470494389534, 0.1302083283662796, 0.0206163190305233, 0.2747395932674408, 0.0, 0.0321180559694767, 0.1983506977558136, 0.006076388992369175, 0.01584201492369175, 0.0844184011220932, 0.0340711809694767, 0.0052083334885537624, 0.0544704869389534, 0.394097238779068, 0.0423177070915699, 0.6584201455116272, 0.142361119389534, 0.2918836772441864, 0.0885416641831398, 0.0863715261220932, 0.0460069440305233, 0.0557725690305233, 0.0460069440305233, 0.0243055559694767, 0.0397135429084301, 0.01888020895421505, 0.0358072929084301, 0.006076388992369175, 0.7243923544883728, 0.02669270895421505, 0.0338541679084301, 0.00824652798473835, 0.01909722201526165, 0.0967881977558136, 0.02191840298473835, 0.02690972201526165, 0.4802517294883728, 0.0592447929084301, 0.00021701389050576836, 0.014322916977107525, 0.00629340298473835, 0.0822482630610466, 0.01692708395421505, 0.9993489384651184, 0.4281684160232544, 0.009765625, 0.1223958358168602, 0.0983072891831398, 0.00933159701526165, 0.025390625, 0.0329861119389534, 0.103515625, 0.0716145858168602, 0.0, 0.044921875, 0.0262586809694767, 0.01019965298473835, 0.0850694477558136, 0.5067274570465088, 0.0542534738779068, 0.8355034589767456, 0.1158854141831398, 0.0746527761220932, 0.0047743055038154125, 0.0319010429084301, 0.010850694961845875, 0.1193576380610466, 0.0377604179084301, 0.0499131940305233, 0.729600727558136, 0.02300347201526165, 0.0015190972480922937, 0.008029513992369175, 0.0086805559694767, 0.0319010429084301, 0.1397569477558136, 0.011935763992369175, 0.021484375, 0.01171875, 0.1905381977558136, 0.0340711809694767, 0.0570746548473835, 0.082899309694767, 0.0811631977558136, 0.02018229104578495, 0.0418836809694767, 0.0455729179084301, 0.0679253488779068, 0.0184461809694767, 0.0282118059694767, 0.1026475727558136, 0.2044270783662796, 0.5110676884651184, 0.5132378339767456, 0.2992621660232544, 0.041015625, 0.140407994389534, 0.0486111119389534, 0.1111111119389534, 0.1072048619389534, 0.0032552082557231188, 0.007378472480922937, 0.0514322929084301, 0.010850694961845875, 0.0590277798473835, 0.0562065988779068, 0.1388888955116272, 0.01323784701526165, 0.1148003488779068, 0.0164930559694767, 0.0462239570915699, 0.00629340298473835, 0.0512152798473835, 0.090711809694767, 0.4088541567325592, 0.0364583320915699, 0.4060329794883728, 0.7736545205116272, 0.429253488779068, 0.005859375, 0.013020833022892475, 0.0173611119389534, 0.02777777798473835, 0.092664934694767, 0.0520833320915699, 0.0364583320915699, 0.3756510317325592, 0.0462239570915699, 0.0327690988779068, 0.0896267369389534, 0.0568576380610466, 0.00933159701526165, 0.0358072929084301, 0.0164930559694767, 0.012369791977107525, 0.6304253339767456, 0.1395399272441864, 0.0787760391831398, 0.01605902798473835, 0.6538628339767456, 0.1098090261220932, 0.0655381977558136, 0.0405815988779068, 0.5073784589767456, 0.008029513992369175, 0.10546875, 0.0614149309694767, 0.1226128488779068, 0.0423177070915699, 0.18359375, 0.126736119389534, 0.0381944440305233, 0.0399305559694767, 0.0262586809694767, 0.0338541679084301, 0.0414496548473835, 0.5703125, 0.0164930559694767, 0.0399305559694767, 0.014973958022892475, 0.02278645895421505, 0.0755208358168602, 0.0353732630610466, 0.1273871511220932, 0.033203125, 0.02951388992369175, 0.005425347480922937, 0.0516493059694767, 0.009114583022892475, 0.009982638992369175, 0.0826822891831398, 0.007595486007630825, 0.0434027798473835, 0.3721788227558136, 0.0321180559694767, 0.02278645895421505, 0.2547743022441864, 0.1690538227558136, 0.1089409738779068, 0.014756944961845875, 0.0792100727558136, 0.0523003488779068, 0.0859375, 0.01692708395421505, 0.0607638880610466, 0.212673619389534, 0.0705295130610466, 0.014756944961845875, 0.048828125, 0.01171875, 0.0451388880610466, 0.0397135429084301, 0.0805121511220932, 0.03059895895421505, 0.120008684694767, 0.7293837070465088, 0.0462239570915699, 0.008897569961845875, 0.5297309160232544, 0.0423177070915699, 0.0622829869389534, 0.5086805820465088, 0.02408854104578495, 0.02278645895421505, 0.0412326380610466, 0.1360677033662796, 0.011935763992369175, 0.01019965298473835, 0.0223524309694767, 0.0303819440305233, 0.103515625, 0.0362413190305233, 0.077039934694767, 0.0375434048473835, 0.02560763992369175, 0.9631076455116272, 0.6814236044883728, 0.02191840298473835, 0.0358072929084301, 0.0622829869389534, 0.9995659589767456, 0.0559895820915699, 0.014973958022892475, 0.0407986119389534, 0.0440538190305233, 0.0863715261220932, 0.0553385429084301, 0.0894097238779068, 0.1263020783662796, 0.6467013955116272, 0.0440538190305233, 0.01953125, 0.01605902798473835, 0.05078125, 0.165798619389534, 0.002170138992369175, 0.0349392369389534, 0.02734375, 0.0436197929084301, 0.0902777761220932, 0.01996527798473835, 0.0490451380610466, 0.0514322929084301, 0.0928819477558136, 0.112196184694767, 0.03125, 0.2274305522441864, 0.02278645895421505, 0.2801649272441864, 0.0418836809694767, 0.02170138992369175, 0.0470920130610466, 0.0798611119389534, 0.4559461772441864, 0.8062065839767456, 0.007595486007630825, 0.01171875, 0.0290798619389534, 0.01953125, 0.0939670130610466, 0.065321184694767, 0.02473958395421505, 0.3704427182674408, 0.2842881977558136, 0.1438802033662796, 0.0013020833721384406, 0.0353732630610466, 0.1213107630610466, 0.012369791977107525, 0.0384114570915699, 0.01996527798473835, 0.02864583395421505, 0.1484375, 0.1048177108168602, 0.009765625, 0.0325520820915699, 0.02105034701526165, 0.0794270858168602, 0.009114583022892475, 0.010416666977107525, 0.0319010429084301, 0.005425347480922937, 0.0520833320915699, 0.0323350690305233, 0.01779513992369175, 0.114149309694767, 0.0327690988779068, 0.013888888992369175, 0.1282552033662796, 0.1705729216337204, 0.0726996511220932, 0.0032552082557231188, 0.86328125, 0.01801215298473835, 0.00434027798473835, 0.0759548619389534, 0.0436197929084301, 0.0533854179084301, 0.0334201380610466, 0.0512152798473835, 0.0694444477558136, 0.0232204869389534, 0.0416666679084301, 0.1458333283662796, 0.2003038227558136, 0.1067708358168602, 0.6814236044883728, 0.088758684694767, 0.0047743055038154125, 0.02973090298473835, 0.0434027798473835, 0.0052083334885537624, 0.0438368059694767, 0.2356770783662796, 0.0397135429084301]

 sparsity of   [0.86328125, 0.009765625, 0.03125, 0.0234375, 0.125, 0.01953125, 0.025390625, 0.025390625, 0.03125, 0.82421875, 0.99609375, 0.99609375, 0.99609375, 0.025390625, 0.80859375, 0.037109375, 0.01953125, 0.009765625, 0.017578125, 0.052734375, 0.072265625, 0.0234375, 0.083984375, 0.0, 0.93359375, 0.05859375, 0.009765625, 0.0078125, 0.068359375, 0.0234375, 0.623046875, 0.142578125, 0.0390625, 0.013671875, 0.87890625, 0.00390625, 0.02734375, 0.923828125, 0.044921875, 0.998046875, 0.75390625, 0.39453125, 0.05859375, 0.53515625, 0.919921875, 0.01953125, 0.884765625, 0.669921875, 0.90234375, 0.0390625, 0.013671875, 0.04296875, 0.11328125, 0.001953125, 0.1171875, 0.0546875, 0.0703125, 0.884765625, 0.0546875, 0.99609375, 0.01171875, 0.013671875, 0.8671875, 0.013671875, 0.6640625, 0.0078125, 0.90234375, 0.0, 0.328125, 0.125, 0.0234375, 0.01953125, 0.0, 0.056640625, 0.05859375, 0.015625, 0.029296875, 0.87890625, 0.02734375, 0.7421875, 0.99609375, 0.931640625, 0.021484375, 0.943359375, 0.916015625, 0.03515625, 0.927734375, 0.015625, 0.033203125, 0.921875, 0.05078125, 0.033203125, 0.0625, 0.0390625, 0.03515625, 0.037109375, 0.853515625, 0.029296875, 0.00390625, 0.01171875, 0.99609375, 0.0390625, 0.025390625, 0.1328125, 0.98046875, 0.03125, 0.01953125, 0.046875, 0.0625, 0.009765625, 0.994140625, 0.041015625, 0.017578125, 0.880859375, 0.81640625, 0.060546875, 0.009765625, 0.083984375, 0.0, 0.892578125, 0.10546875, 0.650390625, 0.80859375, 0.0390625, 0.025390625, 0.017578125, 0.994140625, 0.35546875, 0.03125, 0.845703125, 0.0390625, 0.0078125, 0.8515625, 0.923828125, 0.087890625, 0.94921875, 0.025390625, 0.013671875, 0.02734375, 0.99609375, 0.7421875, 0.017578125, 0.03125, 0.662109375, 0.03515625, 0.025390625, 0.09375, 0.03515625, 0.021484375, 0.03515625, 0.8671875, 0.943359375, 0.17578125, 0.078125, 0.033203125, 0.0703125, 0.009765625, 0.03125, 0.0, 0.037109375, 0.0546875, 0.0390625, 0.705078125, 0.8203125, 0.99609375, 0.078125, 0.931640625, 0.984375, 0.033203125, 0.912109375, 0.01171875, 0.09375, 0.0390625, 0.0703125, 0.01953125, 0.033203125, 0.033203125, 0.99609375, 0.05859375, 0.01171875, 0.01171875, 0.91796875, 0.033203125, 0.025390625, 0.0390625, 0.021484375, 0.001953125, 0.01171875, 0.0234375, 0.044921875, 0.0703125, 0.998046875, 0.03125, 0.013671875, 0.013671875, 0.984375, 0.03125, 0.99609375, 0.009765625, 0.01953125, 0.994140625, 0.001953125, 0.02734375, 0.0703125, 0.72265625, 0.025390625, 0.01953125, 0.9453125, 0.84375, 0.1484375, 0.98828125, 0.029296875, 0.052734375, 0.009765625, 0.015625, 0.568359375, 0.05078125, 0.16015625, 0.029296875, 0.01171875, 0.03125, 0.03125, 0.03515625, 0.01953125, 0.033203125, 0.0234375, 0.02734375, 0.119140625, 0.021484375, 0.021484375, 0.87890625, 0.74609375, 0.009765625, 0.13671875, 0.01953125, 0.943359375, 0.005859375, 0.013671875, 0.125, 0.169921875, 0.06640625, 0.05078125, 0.021484375, 0.15625, 0.033203125, 0.68359375, 0.037109375, 0.029296875, 0.09765625, 0.060546875, 0.046875, 0.857421875, 0.046875, 0.037109375, 0.994140625, 0.029296875, 0.021484375, 0.896484375, 0.01953125, 0.154296875, 0.091796875, 0.91796875, 0.0234375, 0.041015625, 0.986328125, 0.0234375, 0.9453125, 0.01171875, 0.203125, 0.5234375, 0.015625, 0.5390625, 0.009765625, 0.044921875, 0.833984375, 0.99609375, 0.033203125, 0.015625, 0.001953125, 0.158203125, 0.02734375, 0.046875, 0.546875, 0.005859375, 0.8984375, 0.671875, 0.08203125, 0.69921875, 0.017578125, 0.0, 0.056640625, 0.03515625, 0.826171875, 0.009765625, 0.091796875, 0.880859375, 0.015625, 0.119140625, 0.99609375, 0.05078125, 0.90234375, 0.0078125, 0.0, 0.0546875, 0.107421875, 0.044921875, 0.021484375, 0.880859375, 0.0078125, 0.005859375, 0.99609375, 0.833984375, 0.044921875, 0.0234375, 0.109375, 0.919921875, 0.017578125, 0.859375, 0.01171875, 0.03125, 0.021484375, 0.080078125, 0.02734375, 0.048828125, 0.99609375, 0.0, 0.029296875, 0.005859375, 0.765625, 0.009765625, 0.076171875, 0.837890625, 0.005859375, 0.9140625, 0.84765625, 0.048828125, 0.015625, 0.720703125, 0.025390625, 0.05078125, 0.00390625, 0.013671875, 0.064453125, 0.017578125, 0.048828125, 0.005859375, 0.01953125, 0.88671875, 0.07421875, 0.0078125, 0.091796875, 0.828125, 0.021484375, 0.283203125, 0.162109375, 0.8828125, 0.841796875, 0.021484375, 0.052734375, 0.8984375, 0.029296875, 0.083984375, 0.060546875, 0.1171875, 0.181640625, 0.095703125, 0.025390625, 0.87109375, 0.0859375, 0.875, 0.025390625, 0.09765625, 0.013671875, 0.87890625, 0.025390625, 0.01171875, 0.013671875, 0.046875, 0.23828125, 0.95703125, 0.021484375, 0.0390625, 0.001953125, 0.0546875, 0.99609375, 0.04296875, 0.068359375, 0.75, 0.99609375, 0.001953125, 0.111328125, 0.111328125, 0.03515625, 0.015625, 0.01953125, 0.01953125, 0.037109375, 0.62109375, 0.818359375, 0.05078125, 0.041015625, 0.923828125, 0.01953125, 0.056640625, 0.033203125, 0.005859375, 0.01171875, 0.111328125, 0.04296875, 0.02734375, 0.109375, 0.005859375, 0.01953125, 0.142578125, 0.95703125, 0.05078125, 0.005859375, 0.865234375, 0.05078125, 0.046875, 0.865234375, 0.03125, 0.994140625, 0.189453125, 0.69140625, 0.0078125, 0.046875, 0.00390625, 0.9453125, 0.173828125, 0.013671875, 0.09765625, 0.9296875, 0.037109375, 0.9375, 0.138671875, 0.92578125, 0.951171875, 0.943359375, 0.056640625, 0.0078125, 0.013671875, 0.052734375, 0.578125, 0.88671875, 0.01171875, 0.033203125, 0.10546875, 0.0390625, 0.033203125, 0.966796875, 0.013671875, 0.119140625, 0.056640625, 0.87890625, 0.013671875, 0.900390625, 0.994140625, 0.013671875, 0.02734375, 0.01171875, 0.044921875, 0.072265625, 0.8984375, 0.005859375, 0.017578125, 0.07421875, 0.783203125, 0.078125, 0.017578125, 0.015625, 0.013671875, 0.021484375, 0.99609375, 0.99609375, 0.751953125, 0.01953125, 0.103515625, 0.0234375, 0.92578125, 0.021484375, 0.021484375, 0.033203125, 0.00390625, 0.625, 0.029296875, 0.033203125, 0.068359375, 0.021484375, 0.001953125, 0.80078125, 0.07421875, 0.013671875, 0.857421875, 0.076171875, 0.021484375, 0.037109375, 0.0, 0.03125, 0.16015625, 0.02734375, 0.1015625, 0.958984375, 0.88671875, 0.0234375, 0.02734375, 0.005859375, 0.03125, 0.021484375, 0.85546875, 0.90625, 0.0234375, 0.15625, 0.03515625, 0.013671875, 0.001953125, 0.056640625, 0.009765625, 0.015625, 0.931640625, 0.033203125, 0.025390625, 0.0234375, 0.0234375, 0.037109375, 0.01953125, 0.037109375, 0.998046875, 0.08203125, 0.810546875, 0.02734375, 0.9609375, 0.0390625, 0.0, 0.044921875, 0.1796875, 0.046875, 0.046875, 0.78125, 0.029296875, 0.01953125, 0.158203125, 0.033203125, 0.83984375, 0.005859375, 0.078125, 0.033203125, 0.015625, 0.013671875, 0.041015625, 0.03515625, 0.123046875, 0.01171875, 0.958984375, 0.01171875, 0.05078125, 0.01171875, 0.0390625, 0.072265625, 0.689453125, 0.15625, 0.01171875, 0.013671875, 0.009765625, 0.0546875, 0.017578125, 0.228515625, 0.828125, 0.02734375, 0.048828125, 0.08203125, 0.021484375, 0.0234375, 0.033203125, 0.029296875, 0.021484375, 0.013671875, 0.83984375, 0.048828125, 0.994140625, 0.01953125, 0.025390625, 0.75, 0.037109375, 0.083984375, 0.015625, 0.017578125, 0.052734375, 0.0234375, 0.0546875, 0.1015625, 0.77734375, 0.02734375, 0.783203125, 0.015625, 0.966796875, 0.009765625, 0.021484375, 0.12890625, 0.041015625, 0.025390625, 0.025390625, 0.94140625, 0.99609375, 0.0390625, 0.99609375, 0.634765625, 0.005859375, 0.640625, 0.01171875, 0.056640625, 0.041015625, 0.943359375, 0.99609375, 0.017578125, 0.173828125, 0.85546875, 0.01953125, 0.01953125, 0.037109375, 0.0234375, 0.015625, 0.046875, 0.01953125, 0.01953125, 0.001953125, 0.044921875, 0.017578125, 0.0546875, 0.009765625, 0.029296875, 0.029296875, 0.916015625, 0.005859375, 0.044921875, 0.69921875, 0.025390625, 0.03515625, 0.03125, 0.021484375, 0.0234375, 0.07421875, 0.947265625, 0.041015625, 0.021484375, 0.04296875, 0.99609375, 0.03125, 0.0, 0.021484375, 0.009765625, 0.095703125, 0.01171875, 0.0703125, 0.009765625, 0.013671875, 0.890625, 0.05078125, 0.0234375, 0.0, 0.041015625, 0.072265625, 0.806640625, 0.0859375, 0.8984375, 0.044921875, 0.017578125, 0.03125, 0.01171875, 0.005859375, 0.013671875, 0.033203125, 0.029296875, 0.994140625, 0.001953125, 0.99609375, 0.076171875, 0.078125, 0.056640625, 0.912109375, 0.03515625, 0.052734375, 0.0546875, 0.03125, 0.8984375, 0.951171875, 0.01171875, 0.927734375, 0.87109375, 0.037109375, 0.025390625, 0.13671875, 0.056640625, 0.04296875, 0.919921875, 0.99609375, 0.029296875, 0.02734375, 0.02734375, 0.02734375, 0.044921875, 0.109375, 0.01171875, 0.333984375, 0.015625, 0.96875, 0.03125, 0.923828125, 0.0390625, 0.009765625, 0.044921875, 0.78515625, 0.99609375, 0.880859375, 0.189453125, 0.0546875, 0.01171875, 0.80078125, 0.013671875, 0.017578125, 0.134765625, 0.99609375, 0.005859375, 0.01171875, 0.0234375, 0.994140625, 0.109375, 0.0078125, 0.0234375, 0.025390625, 0.41796875, 0.0234375, 0.033203125, 0.01171875, 0.044921875, 0.35546875, 0.005859375, 0.095703125, 0.1015625, 0.017578125, 0.794921875, 0.177734375, 0.060546875, 0.05078125, 0.017578125, 0.994140625, 0.99609375, 0.06640625, 0.12890625, 0.068359375, 0.048828125, 0.0078125, 0.021484375, 0.025390625, 0.0546875, 0.0234375, 0.048828125, 0.03515625, 0.07421875, 0.84375, 0.052734375, 0.62890625, 0.078125, 0.005859375, 0.8046875, 0.0078125, 0.009765625, 0.033203125, 0.0234375, 0.04296875, 0.009765625, 0.015625, 0.994140625, 0.291015625, 0.0390625, 0.01171875, 0.6484375, 0.017578125, 0.900390625, 0.0546875, 0.087890625, 0.017578125, 0.029296875, 0.01953125, 0.07421875, 0.0390625, 0.912109375, 0.03125, 0.0546875, 0.9765625, 0.013671875, 0.115234375, 0.99609375, 0.03125, 0.916015625, 0.02734375, 0.013671875, 0.998046875, 0.033203125, 0.005859375, 0.033203125, 0.1484375, 0.08203125, 0.015625, 0.009765625, 0.982421875, 0.984375, 0.9140625, 0.013671875, 0.2109375, 0.875, 0.03125, 0.044921875, 0.021484375, 0.078125, 0.736328125, 0.037109375, 0.91015625, 0.076171875, 0.060546875, 0.01953125, 0.01953125, 0.99609375, 0.0234375, 0.037109375, 0.04296875, 0.0, 0.013671875, 0.0390625, 0.99609375, 0.017578125, 0.904296875, 0.02734375, 0.560546875, 0.783203125, 0.966796875, 0.029296875, 0.025390625, 0.017578125, 0.001953125, 0.00390625, 0.01171875, 0.033203125, 0.640625, 0.029296875, 0.3515625, 0.666015625, 0.853515625, 0.083984375, 0.01953125, 0.021484375, 0.009765625, 0.0234375, 0.015625, 0.025390625, 0.09375, 0.03515625, 0.66796875, 0.00390625, 0.015625, 0.99609375, 0.033203125, 0.87890625, 0.02734375, 0.203125, 0.05078125, 0.03125, 0.1328125, 0.046875, 0.99609375, 0.046875, 0.015625, 0.0078125, 0.025390625, 0.005859375, 0.03125, 0.236328125, 0.00390625, 0.04296875, 0.0078125, 0.94921875, 0.072265625, 0.029296875, 0.01953125, 0.025390625, 0.021484375, 0.931640625, 0.0234375, 0.025390625, 0.033203125, 0.029296875, 0.04296875, 0.025390625, 0.009765625, 0.052734375, 0.904296875, 0.0078125, 0.0546875, 0.017578125, 0.01171875, 0.0, 0.822265625, 0.17578125, 0.0546875, 0.01171875, 0.0625, 0.0, 0.908203125, 0.837890625, 0.01171875, 0.640625, 0.015625, 0.083984375, 0.00390625, 0.998046875, 0.064453125, 0.0078125, 0.07421875, 0.90234375, 0.078125, 0.943359375, 0.033203125, 0.0859375, 0.927734375, 0.376953125, 0.00390625, 0.083984375, 0.04296875, 0.025390625, 0.052734375, 0.04296875, 0.8359375, 0.083984375, 0.904296875, 0.033203125, 0.935546875, 0.029296875, 0.0625, 0.044921875, 0.953125, 0.994140625, 0.99609375, 0.017578125, 0.009765625, 0.005859375, 0.009765625, 0.99609375, 0.1171875, 0.09375, 0.01953125, 0.029296875, 0.0078125, 0.078125, 0.0234375, 0.03125, 0.009765625, 0.025390625, 0.14453125, 0.876953125, 0.052734375, 0.90234375, 0.998046875, 0.814453125, 0.013671875, 0.01953125, 0.001953125, 0.1484375, 0.00390625, 0.041015625, 0.021484375, 0.060546875, 0.0078125, 0.048828125, 0.03515625, 0.015625, 0.060546875, 0.017578125, 0.15625, 0.046875, 0.009765625, 0.0234375, 0.017578125, 0.052734375, 0.998046875, 0.03125, 0.84375, 0.005859375, 0.9453125, 0.03125, 0.046875, 0.072265625, 0.90625, 0.033203125, 0.052734375, 0.025390625, 0.10546875, 0.0546875, 0.1640625, 0.060546875, 0.01171875, 0.0234375, 0.9375, 0.025390625, 0.0078125, 0.021484375, 0.052734375, 0.357421875, 0.08203125, 0.05078125, 0.04296875, 0.931640625, 0.041015625, 0.8828125, 0.009765625, 0.955078125, 0.0078125, 0.8046875, 0.92578125, 0.28515625, 0.08203125, 0.84765625, 0.046875, 0.01171875, 0.76953125, 0.04296875, 0.638671875, 0.07421875, 0.90625, 0.955078125, 0.9609375, 0.033203125, 0.01953125, 0.017578125, 0.107421875, 0.021484375, 0.064453125, 0.87890625, 0.005859375, 0.029296875, 0.0, 0.171875, 0.994140625, 0.013671875, 0.013671875, 0.03515625, 0.078125, 0.009765625, 0.685546875, 0.04296875, 0.013671875, 0.015625, 0.140625, 0.046875, 0.150390625, 0.0, 0.994140625, 0.025390625, 0.99609375, 0.068359375, 0.029296875, 0.009765625, 0.99609375, 0.9609375, 0.013671875, 0.033203125, 0.017578125, 0.05078125, 0.076171875, 0.072265625, 0.01171875, 0.029296875, 0.931640625, 0.107421875, 0.029296875, 0.0078125, 0.884765625, 0.87890625, 0.02734375, 0.017578125, 0.0390625, 0.001953125, 0.015625, 0.99609375, 0.99609375, 0.0078125, 0.1171875, 0.935546875, 0.03125, 0.001953125, 0.134765625, 0.021484375, 0.015625, 0.1484375, 0.013671875, 0.033203125, 0.0078125, 0.080078125, 0.04296875, 0.0703125, 0.0, 0.0859375, 0.810546875, 0.00390625, 0.005859375, 0.154296875, 0.0, 0.01171875, 0.0390625, 0.00390625, 0.935546875, 0.072265625, 0.0703125, 0.0078125, 0.15625, 0.048828125, 0.109375, 0.04296875, 0.021484375, 0.951171875, 0.029296875, 0.9609375, 0.8671875, 0.99609375, 0.091796875, 0.818359375, 0.046875, 0.015625, 0.873046875, 0.0078125, 0.029296875, 0.03125, 0.01953125, 0.951171875, 0.041015625, 0.01953125, 0.05859375, 0.025390625, 0.99609375, 0.818359375, 0.859375, 0.0390625, 0.99609375, 0.884765625, 0.08203125, 0.0234375, 0.015625, 0.0, 0.076171875, 0.02734375, 0.029296875, 0.109375, 0.076171875, 0.046875, 0.779296875, 0.025390625, 0.0546875, 0.02734375, 0.86328125, 0.068359375, 0.0859375, 0.0078125, 0.88671875, 0.060546875, 0.005859375, 0.998046875, 0.02734375, 0.900390625, 0.291015625, 0.88671875, 0.013671875, 0.072265625, 0.017578125, 0.03125, 0.99609375, 0.017578125, 0.072265625, 0.025390625, 0.01171875, 0.837890625, 0.015625, 0.845703125, 0.02734375, 0.03515625, 0.029296875, 0.05859375, 0.998046875, 0.1875, 0.0390625, 0.056640625, 0.123046875, 0.052734375, 0.068359375, 0.125, 0.01953125, 0.00390625, 0.884765625, 0.041015625, 0.0078125, 0.02734375, 0.03515625, 0.01171875, 0.0546875, 0.033203125, 0.970703125, 0.005859375, 0.99609375, 0.876953125, 0.046875, 0.01171875, 0.087890625, 0.052734375, 0.013671875, 0.044921875, 0.025390625, 0.0, 0.021484375, 0.07421875, 0.99609375, 0.01171875, 0.04296875, 0.064453125, 0.044921875, 0.13671875, 0.033203125, 0.943359375, 0.05078125, 0.93359375, 0.01171875, 0.8984375, 0.021484375, 0.00390625, 0.029296875, 0.013671875, 0.1171875, 0.017578125, 0.875, 0.013671875, 0.572265625, 0.041015625, 0.84765625, 0.96484375, 0.833984375, 0.037109375, 0.015625, 0.015625, 0.01171875, 0.1171875, 0.041015625, 0.029296875, 0.8359375, 0.025390625, 0.076171875, 0.013671875, 0.072265625, 0.021484375, 0.017578125, 0.078125, 0.017578125, 0.625, 0.55859375, 0.037109375, 0.109375, 0.734375, 0.041015625, 0.060546875, 0.736328125, 0.029296875, 0.06640625, 0.048828125, 0.07421875, 0.056640625, 0.93359375, 0.064453125, 0.029296875, 0.1796875, 0.1171875, 0.017578125, 0.025390625, 0.037109375, 0.017578125, 0.015625, 0.009765625, 0.9609375, 0.998046875, 0.1171875, 0.03125, 0.09765625, 0.017578125, 0.060546875, 0.025390625, 0.880859375, 0.994140625, 0.021484375, 0.0, 0.0859375, 0.0390625, 0.861328125, 0.0, 0.029296875, 0.025390625, 0.0078125, 0.009765625, 0.138671875, 0.015625, 0.021484375, 0.669921875, 0.015625, 0.9453125, 0.021484375, 0.015625, 0.900390625, 0.0, 0.1796875, 0.037109375, 0.013671875, 0.0546875, 0.025390625, 0.052734375, 0.041015625, 0.03125, 0.01953125, 0.04296875, 0.037109375, 0.060546875, 0.00390625, 0.0859375, 0.859375, 0.017578125, 0.90625, 0.03515625, 0.904296875, 0.8359375, 0.48828125, 0.025390625, 0.03515625, 0.025390625, 0.015625, 0.0390625, 0.03515625, 0.0625, 0.015625, 0.875, 0.029296875, 0.03515625, 0.017578125, 0.08203125, 0.865234375, 0.99609375, 0.99609375, 0.01953125, 0.01953125, 0.99609375, 0.99609375, 0.91015625, 0.125, 0.078125, 0.1171875, 0.908203125, 0.619140625, 0.947265625, 0.013671875, 0.880859375, 0.01953125, 0.01171875, 0.279296875, 0.078125, 0.103515625, 0.4453125, 0.0234375, 0.078125, 0.826171875, 0.0234375, 0.0234375, 0.03125, 0.048828125, 0.884765625, 0.869140625, 0.068359375, 0.02734375, 0.111328125, 0.05078125, 0.02734375, 0.048828125, 0.021484375, 0.8125, 0.12890625, 0.01171875, 0.037109375, 0.033203125, 0.9140625, 0.2265625, 0.99609375, 0.0078125, 0.044921875, 0.880859375, 0.00390625, 0.94921875, 0.001953125, 0.0234375, 0.044921875, 0.900390625, 0.033203125, 0.0625, 0.009765625, 0.9375, 0.033203125, 0.029296875, 0.0234375, 0.015625, 0.021484375, 0.03515625, 0.017578125, 0.021484375, 0.79296875, 0.271484375, 0.025390625, 0.10546875, 0.8828125, 0.72265625, 0.041015625, 0.900390625, 0.892578125, 0.2265625, 0.71484375, 0.75390625, 0.029296875, 0.283203125, 0.009765625, 0.7578125, 0.046875, 0.98046875, 0.041015625, 0.998046875, 0.93359375, 0.041015625, 0.041015625, 0.029296875, 0.017578125, 0.0078125, 0.041015625, 0.078125, 0.041015625, 0.015625, 0.02734375, 0.0, 0.017578125, 0.578125, 0.037109375, 0.056640625, 0.826171875, 0.8203125, 0.12890625, 0.333984375, 0.033203125, 0.046875, 0.416015625, 0.037109375, 0.03515625, 0.025390625, 0.111328125, 0.037109375, 0.033203125, 0.15625, 0.021484375, 0.025390625, 0.044921875, 0.013671875, 0.083984375, 0.017578125, 0.03515625, 0.03515625, 0.90234375, 0.064453125, 0.009765625, 0.01171875, 0.029296875, 0.005859375, 0.00390625, 0.919921875, 0.009765625, 0.919921875, 0.818359375, 0.171875, 0.103515625, 0.99609375, 0.0234375, 0.037109375, 0.01953125, 0.009765625, 0.01953125, 0.765625, 0.654296875, 0.001953125, 0.0546875, 0.994140625, 0.04296875, 0.009765625, 0.087890625, 0.01953125, 0.904296875, 0.02734375, 0.9296875, 0.0234375, 0.03515625, 0.033203125, 0.0859375, 0.08203125, 0.1328125, 0.03125, 0.017578125, 0.001953125, 0.744140625, 0.908203125, 0.052734375, 0.99609375, 0.001953125, 0.025390625, 0.041015625, 0.125, 0.48828125, 0.0234375, 0.84375, 0.00390625, 0.087890625, 0.025390625, 0.017578125, 0.001953125, 0.005859375, 0.01171875, 0.03125, 0.080078125, 0.015625, 0.1953125, 0.021484375, 0.908203125, 0.033203125, 0.04296875, 0.0546875, 0.033203125, 0.02734375, 0.041015625, 0.837890625, 0.017578125, 0.017578125, 0.1796875, 0.009765625, 0.07421875, 0.009765625, 0.125, 0.921875, 0.99609375, 0.080078125, 0.091796875, 0.041015625, 0.015625, 0.029296875, 0.99609375, 0.22265625, 0.994140625, 0.01953125, 0.01171875, 0.056640625, 0.005859375, 0.994140625, 0.99609375, 0.994140625, 0.0234375, 0.013671875, 0.083984375, 0.1640625, 0.04296875, 0.029296875, 0.96484375, 0.046875, 0.720703125, 0.095703125, 0.033203125, 0.001953125, 0.048828125, 0.99609375, 0.0078125, 0.025390625, 0.01171875, 0.89453125, 0.998046875, 0.021484375, 0.021484375, 0.037109375, 0.01953125, 0.00390625, 0.037109375, 0.1328125, 0.021484375, 0.1875, 0.021484375, 0.935546875, 0.041015625, 0.029296875, 0.994140625, 0.203125, 0.017578125, 0.09765625, 0.03515625, 0.759765625, 0.20703125, 0.013671875, 0.572265625, 0.017578125, 0.810546875, 0.00390625, 0.125, 0.056640625, 0.9609375, 0.033203125, 0.10546875, 0.025390625, 0.099609375, 0.216796875, 0.66796875, 0.015625, 0.0625, 0.99609375, 0.04296875, 0.01171875, 0.9453125, 0.0234375, 0.998046875, 0.015625, 0.109375, 0.86328125, 0.15234375, 0.33984375, 0.95703125, 0.00390625, 0.078125, 0.99609375, 0.00390625, 0.01171875, 0.064453125, 0.017578125, 0.978515625, 0.111328125, 0.0078125, 0.99609375, 0.07421875, 0.0546875, 0.00390625, 0.013671875, 0.232421875, 0.02734375, 0.01171875, 0.01953125, 0.044921875, 0.017578125, 0.044921875, 0.0390625, 0.001953125, 0.1015625, 0.0078125, 0.01171875, 0.0078125, 0.693359375, 0.0234375, 0.873046875, 0.85546875, 0.033203125, 0.015625, 0.2578125, 0.974609375, 0.060546875, 0.017578125, 0.021484375, 0.046875, 0.09375, 0.00390625, 0.0234375, 0.013671875, 0.033203125, 0.033203125, 0.994140625, 0.01171875, 0.04296875, 0.04296875, 0.03125, 0.87109375, 0.0, 0.02734375, 0.03515625, 0.025390625, 0.021484375, 0.095703125, 0.03125, 0.01953125, 0.91796875, 0.9453125, 0.03515625, 0.01171875, 0.0078125, 0.025390625, 0.03125, 0.037109375, 0.0546875, 0.001953125, 0.0625, 0.103515625, 0.017578125, 0.013671875, 0.87109375, 0.05859375, 0.046875, 0.033203125, 0.015625, 0.115234375, 0.03515625, 0.01171875, 0.998046875, 0.9453125, 0.0, 0.025390625, 0.94921875, 0.00390625, 0.10546875, 0.162109375, 0.025390625, 0.025390625, 0.064453125, 0.021484375, 0.080078125, 0.8828125, 0.017578125, 0.029296875, 0.0234375, 0.037109375, 0.037109375, 0.025390625, 0.849609375, 0.017578125, 0.056640625, 0.859375, 0.013671875, 0.037109375, 0.07421875, 0.169921875, 0.99609375, 0.056640625, 0.123046875, 0.35546875, 0.072265625, 0.87890625, 0.0859375, 0.017578125, 0.0, 0.0703125, 0.72265625, 0.15625, 0.001953125, 0.091796875, 0.078125, 0.056640625, 0.078125, 0.892578125, 0.00390625, 0.021484375, 0.013671875, 0.0, 0.01953125, 0.994140625, 0.025390625, 0.859375, 0.69140625, 0.04296875, 0.017578125, 0.041015625, 0.03125, 0.005859375, 0.8984375, 0.013671875, 0.11328125, 0.033203125, 0.904296875, 0.029296875, 0.03515625, 0.025390625, 0.017578125, 0.02734375, 0.03515625, 0.01953125, 0.103515625, 0.95703125, 0.00390625, 0.03125, 0.00390625, 0.02734375, 0.0703125, 0.185546875, 0.912109375, 0.052734375, 0.0546875, 0.02734375, 0.0234375, 0.90234375, 0.03125, 0.06640625, 0.021484375, 0.06640625, 0.90234375, 0.994140625, 0.07421875, 0.037109375, 0.021484375, 0.900390625, 0.88671875, 0.0625, 0.994140625, 0.01953125, 0.037109375, 0.0390625, 0.033203125, 0.01171875, 0.134765625, 0.07421875, 0.015625, 0.0703125, 0.03515625, 0.048828125, 0.08984375, 0.017578125, 0.0859375, 0.015625, 0.01171875, 0.0390625, 0.08984375, 0.05859375, 0.1484375, 0.501953125, 0.013671875, 0.0078125, 0.013671875, 0.009765625, 0.99609375, 0.103515625, 0.00390625, 0.99609375, 0.11328125, 0.04296875, 0.115234375, 0.013671875, 0.017578125, 0.1015625, 0.015625, 0.015625, 0.912109375, 0.888671875, 0.01171875, 0.048828125, 0.0078125, 0.07421875, 0.615234375, 0.01171875, 0.900390625, 0.015625, 0.021484375, 0.052734375, 0.0859375, 0.158203125, 0.009765625, 0.763671875, 0.015625, 0.072265625, 0.0234375, 0.0546875, 0.0234375, 0.92578125, 0.025390625, 0.994140625, 0.0625, 0.025390625, 0.072265625, 0.03515625, 0.99609375, 0.009765625, 0.044921875, 0.03125, 0.912109375, 0.015625, 0.01953125, 0.0390625, 0.001953125, 0.03515625, 0.763671875, 0.009765625, 0.021484375, 0.8203125, 0.171875, 0.005859375, 0.025390625, 0.900390625, 0.01953125, 0.9921875, 0.013671875, 0.037109375, 0.00390625, 0.005859375, 0.068359375, 0.052734375, 0.076171875, 0.029296875, 0.009765625, 0.009765625, 0.021484375, 0.064453125, 0.90234375, 0.04296875, 0.099609375, 0.99609375, 0.02734375, 0.037109375, 0.013671875, 0.046875, 0.03125, 0.03125, 0.12109375, 0.017578125, 0.03515625, 0.921875, 0.0859375, 0.00390625, 0.029296875, 0.05859375, 0.021484375, 0.078125, 0.013671875, 0.013671875, 0.87109375, 0.068359375, 0.994140625, 0.1484375, 0.029296875, 0.1015625, 0.05859375, 0.728515625, 0.05078125, 0.869140625, 0.005859375, 0.83203125, 0.01171875, 0.330078125, 0.0625, 0.03515625, 0.22265625, 0.005859375, 0.888671875, 0.04296875, 0.005859375, 0.01171875, 0.001953125, 0.044921875, 0.87890625, 0.9453125, 0.07421875, 0.033203125, 0.001953125, 0.013671875, 0.783203125, 0.02734375, 0.99609375, 0.017578125, 0.0390625, 0.9140625, 0.0, 0.99609375, 0.404296875, 0.998046875, 0.998046875, 0.029296875, 0.025390625, 0.228515625, 0.017578125, 0.025390625, 0.392578125, 0.99609375, 0.0078125, 0.03125, 0.091796875, 0.0859375, 0.015625, 0.9296875, 0.8046875, 0.1328125, 0.27734375, 0.380859375, 0.044921875, 0.033203125, 0.025390625, 0.0078125, 0.01953125, 0.005859375, 0.02734375, 0.22265625, 0.939453125, 0.037109375, 0.029296875, 0.017578125, 0.861328125, 0.994140625, 0.0, 0.017578125, 0.041015625, 0.056640625, 0.046875, 0.099609375, 0.99609375, 0.0234375, 0.7265625, 0.064453125, 0.037109375, 0.98046875, 0.185546875, 0.083984375, 0.029296875, 0.017578125, 0.0234375, 0.01171875, 0.01953125, 0.025390625, 0.068359375, 0.044921875, 0.646484375, 0.025390625, 0.0234375, 0.28125, 0.0546875, 0.0, 0.029296875, 0.044921875, 0.025390625, 0.154296875, 0.994140625, 0.853515625, 0.025390625, 0.0078125, 0.76171875, 0.947265625, 0.021484375, 0.109375, 0.02734375, 0.029296875, 0.03125]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Total parameter pruned: 8655372.016737297 (unstructured) 0 (structured)

max weight is  tensor([3.4184e-01, 2.2856e-02, 3.3843e-01, 4.8788e-03, 2.7741e-09, 1.8864e-01,
        1.9846e-08, 4.8630e-09, 2.9235e-02, 2.0200e-01, 1.9846e-08, 2.5733e-09,
        2.0996e-01, 4.2183e-01, 4.6340e-03, 5.3234e-09, 2.5160e-02, 1.5208e-01,
        4.8630e-09, 1.9039e-09, 2.4293e-01, 3.0181e-01, 4.3812e-01, 8.2347e-01,
        2.9486e-02, 1.4469e-02, 2.0318e-10, 2.1085e-01, 7.3473e-02, 1.2014e-02,
        4.5152e-01, 1.9039e-09, 2.3912e-02, 3.3438e-01, 8.4711e-02, 1.9846e-08,
        4.4557e-02, 2.8382e-03, 2.8136e-01, 4.6442e-09, 1.7951e-09, 4.1761e-09,
        3.5154e-02, 2.5557e-02, 1.9846e-08, 2.9913e-09, 2.6252e-01, 5.4972e-01,
        1.6598e-02, 3.5777e-01, 3.3074e-01, 1.6723e-01, 5.1648e-10, 1.0186e-09,
        4.8630e-09, 1.3344e-10, 2.7845e-09, 6.8584e-02, 6.0746e-01, 3.1341e-09,
        3.1302e-01, 3.5857e-01, 5.0979e-04, 1.4197e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.5284e-01, 8.3869e-10, 1.3658e-08, 1.2985e-08, 6.1050e-08, 8.3869e-10,
        6.1050e-08, 2.3735e-08, 2.4038e-01, 9.3844e-02, 1.8263e-08, 6.1050e-08,
        7.9015e-08, 1.1541e-01, 6.1060e-08, 8.4430e-02, 6.1051e-08, 1.6258e-01,
        1.5862e-01, 1.7175e-08, 1.8316e-09, 6.1060e-08, 8.1998e-10, 1.3994e-08,
        2.2547e-08, 8.3869e-10, 3.1686e-08, 6.4811e-02, 1.7175e-08, 2.3303e-01,
        6.1050e-08, 2.9220e-01, 7.1932e-02, 6.1061e-08, 9.4109e-02, 1.0020e-08,
        1.8202e-08, 3.7929e-02, 6.1050e-08, 5.3054e-09, 1.9761e-01, 1.2874e-01,
        7.9015e-08, 3.1686e-08, 1.0021e-08, 1.7175e-08, 2.0805e-08, 5.1875e-02,
        1.2141e-01, 6.5777e-02, 1.6206e-01, 1.7175e-08, 9.2593e-02, 1.7175e-08,
        1.7175e-08, 8.5736e-10, 5.3056e-09, 1.1219e-08, 6.1050e-08, 8.3869e-10,
        1.1975e-01, 3.5499e-08, 6.1061e-08, 3.6699e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.6754e-07, 1.7978e-07, 3.4584e-02, 6.4004e-07, 9.1316e-07, 9.8266e-08,
        4.0409e-07, 3.0682e-07, 2.9929e-07, 3.0219e-07, 4.0409e-07, 2.2369e-07,
        1.8173e-07, 1.3416e-04, 3.0324e-02, 4.9292e-07, 1.6855e-07, 2.5152e-02,
        2.9929e-07, 5.0156e-07, 5.0156e-07, 4.9292e-07, 1.6695e-01, 4.7659e-03,
        3.2310e-02, 2.5238e-01, 2.2369e-07, 1.7893e-02, 2.8547e-07, 8.2376e-07,
        2.4397e-07, 1.8173e-07, 2.7394e-02, 9.1316e-07, 2.3814e-01, 1.7978e-07,
        6.5052e-02, 6.2490e-07, 2.6754e-07, 2.9929e-07, 7.9041e-07, 1.9777e-01,
        1.7978e-07, 4.0656e-02, 1.9054e-07, 1.0719e-06, 7.9040e-07, 2.6754e-07,
        8.2376e-07, 4.0688e-07, 1.6892e-07, 1.6205e-01, 2.0887e-01, 2.9929e-07,
        4.3966e-02, 1.9054e-07, 4.9648e-07, 1.0584e-07, 1.0584e-07, 1.7759e-07,
        3.1569e-07, 1.9054e-07, 1.0503e-06, 4.2747e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.9048e-08, 3.2283e-08, 6.9441e-08, 1.3588e-08, 5.4239e-03, 1.0584e-07,
        6.1632e-02, 6.1308e-03, 6.0014e-02, 2.5787e-02, 4.6975e-02, 1.1693e-02,
        1.8764e-08, 5.4257e-08, 1.8948e-08, 8.9570e-03, 2.0473e-08, 1.0634e-02,
        9.4776e-02, 4.5906e-08, 1.3442e-02, 1.7468e-02, 1.2179e-08, 7.0561e-08,
        1.0554e-01, 1.9048e-08, 8.0142e-02, 6.5074e-08, 4.8215e-08, 5.9409e-03,
        6.7891e-02, 4.7617e-08, 3.0474e-08, 4.5783e-08, 4.5906e-08, 3.2988e-02,
        5.1944e-08, 1.9754e-02, 7.2270e-03, 2.2068e-02, 3.8595e-08, 1.3665e-02,
        2.5556e-08, 1.0479e-07, 5.6155e-03, 9.3067e-08, 1.0041e-01, 4.5585e-08,
        2.5548e-08, 7.9344e-02, 1.1740e-07, 5.0239e-08, 1.5097e-08, 1.7386e-08,
        5.2780e-02, 6.8768e-08, 6.3025e-08, 5.1016e-02, 3.7339e-02, 7.4844e-02,
        1.8681e-08, 3.7632e-02, 3.2704e-08, 1.7755e-08, 3.8060e-08, 4.4489e-03,
        4.3920e-02, 3.4946e-02, 1.7209e-02, 2.2790e-02, 3.5669e-02, 2.2865e-02,
        5.3146e-08, 3.9996e-08, 6.6197e-08, 4.1579e-08, 5.3369e-02, 2.0303e-01,
        4.4444e-08, 7.1472e-08, 1.0421e-07, 1.2398e-03, 7.6962e-02, 1.0584e-07,
        6.6016e-03, 2.3790e-02, 2.5548e-08, 2.5432e-08, 6.0867e-02, 2.0799e-02,
        9.5051e-02, 6.3025e-08, 1.6194e-02, 5.1277e-08, 5.6893e-08, 3.5730e-08,
        4.7437e-08, 4.7629e-08, 2.1687e-08, 1.4679e-02, 2.3265e-02, 2.5548e-08,
        1.4301e-01, 4.1579e-08, 1.2728e-01, 1.8918e-03, 2.5548e-08, 2.2976e-08,
        2.3958e-02, 3.2920e-08, 3.8114e-08, 3.8060e-08, 8.0743e-03, 2.1936e-02,
        1.6114e-02, 1.7991e-02, 2.8736e-02, 2.4930e-08, 7.7911e-02, 6.3025e-08,
        5.4252e-08, 1.9459e-08, 2.5572e-08, 6.9390e-02, 2.5685e-08, 1.0584e-07,
        1.9048e-08, 5.5982e-08, 4.1166e-02, 1.3658e-07, 3.3073e-02, 1.4423e-01,
        1.7853e-02, 6.6197e-08, 6.3025e-08, 1.2934e-03, 5.7796e-03, 2.7559e-08,
        8.1986e-02, 3.0535e-02, 2.4701e-08, 4.8704e-08, 3.4815e-08, 1.1079e-02,
        9.0783e-02, 2.1529e-02, 1.0771e-01, 6.9441e-08, 1.9048e-08, 7.0561e-08,
        2.1376e-08, 5.5822e-02, 2.5548e-08, 3.9445e-08, 3.7397e-08, 6.9441e-08,
        6.9441e-08, 3.2764e-08, 1.0751e-01, 3.4622e-02, 4.9029e-02, 6.9441e-08,
        2.1114e-02, 3.3260e-02, 4.8215e-08, 7.5122e-08, 6.7195e-02, 3.7561e-08,
        1.0327e-07, 1.4558e-02, 1.0147e-02, 1.5789e-02, 3.6288e-02, 6.9764e-02,
        7.2534e-02, 1.0325e-07, 1.4817e-02, 1.6542e-02, 2.9649e-08, 4.9864e-08,
        3.8876e-02, 6.5248e-08, 1.0430e-01, 2.5917e-02, 6.9441e-08, 1.9743e-08,
        8.6617e-02, 2.5516e-08, 4.7370e-08, 6.9441e-08, 7.2630e-03, 3.8624e-08,
        8.3920e-02, 5.6041e-08, 1.0382e-07, 1.3141e-02, 1.4690e-01, 1.9048e-08,
        1.9292e-02, 2.1413e-08, 6.6197e-08, 1.1192e-01, 4.4484e-08, 2.4448e-02,
        4.6341e-08, 4.5906e-08, 1.7394e-02, 6.3135e-08, 4.6116e-02, 9.8411e-02,
        9.0339e-02, 5.6278e-02, 1.9100e-01, 6.9441e-08, 4.8215e-08, 1.9048e-08,
        5.5977e-08, 5.4972e-08, 2.5738e-08, 5.3075e-03, 1.8656e-02, 6.4280e-08,
        3.3719e-02, 1.0358e-01, 1.6718e-02, 4.6821e-02, 1.9048e-08, 2.5548e-08,
        1.1300e-01, 1.0249e-07, 1.5008e-02, 8.7232e-02, 9.4207e-02, 1.8408e-03,
        1.1382e-09, 5.9887e-03, 4.5783e-08, 1.5941e-01, 2.1413e-08, 2.1376e-08,
        6.6197e-02, 6.2942e-02, 6.1234e-02, 2.9972e-08, 4.7299e-02, 1.0576e-07,
        1.9048e-08, 2.1151e-01, 1.5732e-02, 5.4289e-02, 5.4972e-08, 2.5548e-08,
        4.0320e-02, 2.5139e-02, 4.8215e-08, 2.1102e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.9327e-08, 1.3214e-08, 1.0530e-08, 1.9331e-08, 5.3539e-02, 4.4974e-08,
        7.8128e-02, 8.3688e-02, 6.0587e-02, 9.6461e-02, 1.2535e-01, 4.7993e-02,
        1.3214e-08, 1.3214e-08, 3.2139e-03, 8.1704e-02, 1.3214e-08, 9.2113e-02,
        6.0495e-02, 3.1056e-08, 1.2501e-01, 1.2556e-01, 1.0284e-08, 1.3214e-08,
        1.6955e-01, 1.5092e-09, 7.5173e-02, 4.5119e-02, 1.9331e-08, 7.9515e-02,
        9.3287e-02, 8.6512e-02, 1.3214e-08, 1.1929e-08, 8.3059e-09, 1.3642e-01,
        1.3214e-08, 2.6384e-01, 1.2704e-01, 1.1865e-01, 7.0629e-02, 1.7449e-01,
        5.6773e-03, 2.9525e-08, 9.4504e-02, 5.2984e-02, 2.0447e-02, 1.3573e-02,
        1.3215e-08, 1.0051e-01, 2.0274e-10, 5.6596e-02, 6.7456e-08, 4.9786e-02,
        2.4489e-01, 8.1890e-02, 1.0485e-08, 7.4768e-02, 8.7495e-02, 9.8068e-02,
        1.3214e-08, 1.0021e-01, 1.3214e-08, 1.0485e-08, 4.5161e-08, 1.0678e-01,
        8.2306e-09, 4.3613e-03, 1.9490e-01, 1.7991e-01, 1.6334e-01, 2.4156e-01,
        7.3545e-02, 1.1622e-01, 9.1663e-09, 1.0485e-08, 7.6028e-02, 3.1925e-02,
        5.9897e-08, 1.9339e-02, 2.3628e-08, 4.9622e-02, 1.1610e-01, 1.3214e-08,
        1.1480e-01, 8.4415e-02, 2.3685e-09, 3.0953e-02, 1.0627e-01, 1.1818e-01,
        1.1289e-01, 1.8497e-08, 7.1949e-02, 6.8801e-02, 1.0625e-01, 1.3214e-08,
        9.0076e-02, 8.8068e-02, 3.7254e-02, 1.2950e-01, 9.0769e-02, 1.3214e-08,
        1.5692e-01, 1.3214e-08, 8.7983e-03, 1.1062e-01, 4.5477e-08, 1.3214e-08,
        1.1448e-01, 4.2529e-02, 6.1032e-02, 3.5619e-09, 1.1202e-01, 2.0718e-01,
        1.7305e-01, 1.5982e-01, 1.9332e-01, 8.5977e-02, 8.5959e-02, 1.3214e-08,
        1.3214e-08, 4.6222e-08, 5.9896e-08, 1.0181e-01, 6.7497e-08, 1.2785e-08,
        6.3765e-09, 5.3616e-03, 1.1577e-01, 1.3215e-08, 1.6721e-01, 2.1670e-02,
        1.1810e-01, 6.8429e-09, 1.9183e-08, 3.9315e-02, 6.4459e-02, 1.8641e-10,
        1.0752e-01, 1.2096e-01, 6.1112e-02, 1.2329e-01, 1.1053e-01, 1.9331e-01,
        1.5027e-01, 5.3757e-02, 1.2747e-01, 2.4414e-08, 2.0747e-09, 1.3214e-08,
        9.1663e-09, 1.0668e-01, 1.3215e-08, 3.8849e-02, 4.1701e-02, 8.4660e-09,
        9.1635e-09, 9.2565e-02, 3.1817e-02, 1.6659e-01, 8.7107e-02, 9.8587e-09,
        1.3321e-01, 2.3018e-01, 8.4660e-09, 1.1832e-01, 3.0016e-02, 1.6670e-08,
        1.9316e-08, 7.5504e-02, 7.1629e-02, 1.5500e-01, 7.6313e-02, 1.0212e-01,
        1.1618e-02, 1.3214e-08, 1.2715e-01, 1.8217e-01, 1.9331e-08, 1.2617e-08,
        7.5618e-02, 9.1663e-09, 9.8088e-02, 1.1546e-01, 3.8749e-08, 1.0743e-01,
        8.2569e-02, 4.7987e-02, 5.9920e-02, 1.9316e-08, 1.6105e-01, 1.3214e-08,
        9.0069e-02, 3.2280e-03, 1.3214e-08, 1.2432e-01, 1.7686e-02, 2.4498e-08,
        6.5584e-02, 7.3412e-09, 2.4414e-08, 1.5980e-01, 6.1763e-02, 8.4501e-02,
        9.4615e-02, 1.9166e-08, 1.6764e-01, 2.1594e-01, 2.2448e-02, 1.3350e-02,
        2.1709e-02, 8.3024e-02, 3.2796e-02, 9.8587e-09, 2.9950e-08, 1.3214e-08,
        9.1663e-09, 1.3214e-08, 1.7308e-08, 8.5394e-02, 1.6335e-01, 5.5108e-02,
        3.0443e-02, 1.1165e-01, 5.7370e-02, 8.0372e-02, 6.4019e-09, 9.1663e-09,
        1.2218e-01, 6.0056e-08, 1.7814e-01, 4.2018e-02, 6.5663e-02, 7.1874e-02,
        3.2394e-08, 1.1487e-01, 1.7920e-08, 2.5642e-02, 9.1663e-09, 6.2507e-08,
        1.1339e-01, 3.3543e-02, 8.5190e-02, 6.4772e-02, 8.3311e-02, 1.9174e-08,
        1.3214e-08, 2.6249e-02, 2.0687e-01, 1.7099e-02, 4.6516e-08, 2.3685e-09,
        8.3444e-02, 1.3138e-01, 2.3083e-08, 1.5326e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.7395e-08, 2.2687e-02, 5.9873e-08, 6.5620e-02, 5.9873e-08, 1.8531e-07,
        9.5381e-02, 3.7787e-02, 4.7503e-08, 4.4072e-02, 1.1030e-01, 3.3521e-07,
        8.0885e-02, 1.6263e-07, 2.5590e-07, 4.0255e-02, 8.9754e-02, 5.9873e-08,
        1.6841e-07, 1.6263e-07, 5.1428e-02, 5.8641e-02, 5.9926e-02, 1.2032e-01,
        2.9432e-02, 6.0216e-02, 1.0350e-01, 7.3009e-08, 1.2820e-07, 6.9087e-02,
        3.3217e-07, 1.5558e-02, 2.8663e-08, 8.2521e-08, 1.6263e-07, 7.3139e-02,
        1.6263e-07, 1.4047e-07, 4.7547e-08, 8.2521e-08, 1.0586e-07, 1.4260e-07,
        1.0589e-07, 8.0563e-02, 5.9873e-08, 4.9919e-02, 6.5539e-02, 2.9968e-02,
        3.0111e-02, 1.8531e-07, 1.6841e-07, 3.7664e-03, 2.5590e-07, 1.2567e-01,
        6.2082e-02, 1.6263e-07, 6.1165e-02, 4.2166e-07, 1.6841e-07, 1.5436e-07,
        3.8651e-02, 1.4047e-07, 1.6841e-07, 8.1577e-08], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.0720e-07, 2.2881e-07, 2.4859e-07, 6.5404e-02, 1.1011e-07, 1.1011e-07,
        3.2349e-07, 2.6631e-07, 1.1858e-07, 3.2535e-09, 2.7984e-07, 2.0434e-07,
        2.0245e-07, 2.3309e-07, 9.3287e-02, 2.8784e-07, 1.5423e-07, 5.8730e-02,
        1.3019e-01, 1.8937e-07, 3.1591e-07, 9.0771e-02, 9.8309e-02, 1.3783e-07,
        1.1858e-07, 2.7956e-07, 7.7043e-02, 3.0821e-07, 1.5582e-01, 1.3765e-01,
        8.1950e-02, 1.1858e-07, 2.2123e-01, 8.6794e-02, 1.3334e-01, 2.0245e-07,
        1.4794e-07, 6.6026e-07, 2.0245e-07, 2.9869e-07, 2.4859e-07, 1.0130e-01,
        9.1535e-02, 7.0576e-02, 5.4441e-07, 4.1069e-02, 2.9869e-07, 1.1600e-01,
        2.9936e-07, 2.4950e-07, 1.5712e-02, 8.5481e-02, 1.4454e-01, 6.8459e-07,
        4.6650e-08, 6.0720e-07, 6.0720e-07, 7.5186e-02, 3.1024e-07, 7.5112e-02,
        9.5962e-02, 1.4846e-01, 3.2349e-07, 3.0443e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.4547e-02, 1.7304e-08, 1.9574e-08, 1.1675e-02, 5.2149e-02, 9.4739e-08,
        3.9335e-02, 3.5452e-02, 1.9585e-02, 1.8820e-02, 2.7122e-02, 7.3253e-08,
        1.7150e-01, 8.8860e-02, 2.7063e-08, 1.1074e-01, 1.8320e-01, 6.8429e-02,
        2.4739e-02, 2.0944e-08, 8.2467e-02, 6.3274e-02, 1.0986e-08, 1.5650e-01,
        6.5967e-02, 5.3569e-08, 1.6727e-02, 3.0189e-08, 1.9462e-08, 6.4454e-02,
        2.2565e-02, 5.9310e-03, 6.1489e-02, 4.0449e-08, 1.2352e-08, 7.7782e-02,
        9.4371e-08, 4.1067e-02, 1.5214e-02, 4.7013e-02, 3.9353e-02, 8.1367e-02,
        3.6176e-08, 2.4941e-08, 1.4342e-02, 6.0508e-02, 1.3709e-02, 1.0091e-02,
        9.5749e-08, 1.6390e-02, 6.2253e-08, 2.9392e-02, 8.1034e-03, 4.1305e-08,
        1.5730e-01, 2.5382e-02, 5.3569e-08, 4.5290e-08, 6.7275e-03, 1.6742e-02,
        1.9307e-08, 1.3427e-02, 5.1219e-02, 2.3635e-02, 3.7885e-02, 2.5859e-08,
        3.3021e-02, 3.6338e-02, 3.6246e-02, 3.9558e-02, 5.8526e-02, 1.8593e-01,
        2.0398e-08, 2.1689e-02, 1.1939e-08, 3.2420e-02, 1.9977e-02, 1.6190e-02,
        1.6128e-08, 1.2465e-08, 4.7251e-08, 1.7460e-02, 1.1631e-02, 4.5575e-09,
        3.8917e-02, 9.1321e-02, 3.9353e-08, 2.2080e-08, 1.5254e-02, 1.0079e-01,
        2.4733e-02, 6.2253e-08, 5.8842e-02, 4.6806e-03, 6.0529e-08, 6.2253e-08,
        6.8989e-02, 3.0945e-02, 4.5447e-02, 1.8657e-02, 5.1237e-02, 5.7670e-02,
        1.4191e-02, 2.5984e-08, 4.4796e-02, 6.2987e-03, 1.5544e-01, 7.6098e-02,
        8.5845e-02, 2.0600e-08, 9.6182e-03, 6.8620e-08, 8.1614e-02, 3.0873e-02,
        3.6424e-02, 9.0204e-02, 4.5649e-02, 8.0719e-08, 1.3013e-02, 5.4609e-02,
        4.0449e-08, 1.6072e-01, 4.9868e-08, 1.2394e-02, 4.0319e-08, 7.2686e-02,
        5.4854e-03, 9.5240e-08, 2.1787e-02, 1.0663e-07, 9.3600e-02, 2.5160e-02,
        1.4739e-02, 5.6921e-08, 1.0089e-01, 6.3449e-02, 5.3492e-03, 1.2352e-08,
        1.3412e-02, 2.2780e-02, 1.0912e-01, 7.7335e-02, 1.6544e-02, 1.6658e-01,
        2.8554e-02, 6.5874e-08, 1.8139e-01, 8.8383e-08, 2.4227e-02, 5.6921e-08,
        1.0663e-07, 1.7644e-02, 8.1833e-02, 3.5963e-08, 3.8302e-02, 4.3465e-02,
        1.9664e-08, 3.4388e-02, 1.1491e-02, 2.0203e-02, 2.0673e-02, 4.6929e-02,
        1.7990e-02, 6.7918e-02, 1.9307e-08, 6.5125e-02, 2.5677e-02, 2.5979e-08,
        1.0663e-07, 2.1811e-02, 7.6997e-02, 8.7960e-02, 4.9323e-02, 1.9111e-02,
        3.3452e-02, 1.7304e-08, 8.9169e-02, 5.1850e-02, 4.4316e-02, 4.8746e-02,
        4.2733e-08, 1.0663e-07, 5.8647e-02, 4.7105e-08, 7.1026e-02, 3.2711e-02,
        1.2939e-02, 2.2549e-08, 1.0535e-01, 5.3569e-08, 1.6839e-02, 2.3401e-08,
        1.1785e-02, 2.8228e-08, 7.2460e-02, 1.9569e-02, 4.2921e-02, 4.7419e-02,
        8.6095e-02, 1.9527e-08, 1.9307e-08, 1.8739e-02, 3.9328e-02, 6.0113e-02,
        4.9028e-08, 4.6334e-02, 7.1621e-02, 1.0396e-02, 6.3240e-03, 3.8744e-02,
        1.9263e-02, 1.8025e-02, 2.0239e-02, 1.8068e-01, 8.3355e-02, 5.7415e-08,
        5.1197e-02, 7.3735e-02, 2.0944e-08, 1.0751e-02, 2.5535e-02, 1.9754e-02,
        1.7364e-02, 2.0878e-02, 1.0381e-01, 2.2760e-02, 2.7062e-08, 4.7251e-08,
        1.6378e-02, 6.9180e-02, 1.5969e-01, 2.3160e-02, 1.5414e-02, 4.6709e-02,
        6.6110e-02, 5.0057e-02, 4.5788e-08, 1.9374e-02, 6.3890e-02, 8.5144e-02,
        2.6766e-02, 1.3130e-02, 6.2139e-02, 2.3878e-02, 3.2572e-02, 5.3569e-08,
        7.6665e-02, 2.9313e-02, 7.3616e-02, 2.7722e-08, 4.0319e-08, 7.3226e-02,
        6.3730e-03, 6.7664e-03, 4.4672e-02, 2.8930e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([6.1348e-07, 7.4573e-03, 4.8098e-02, 1.4804e-02, 6.2430e-02, 4.3782e-02,
        1.6393e-01, 2.2723e-07, 2.1200e-07, 2.1830e-07, 2.1415e-07, 9.6049e-02,
        3.8455e-07, 7.1225e-03, 4.0670e-07, 3.9386e-02, 1.2176e-01, 1.2011e-07,
        3.7535e-07, 1.8360e-03, 4.0271e-07, 9.6519e-02, 4.0521e-02, 2.2779e-07,
        4.0301e-07, 5.8999e-02, 7.0863e-08, 9.2354e-07, 4.6637e-02, 3.6515e-07,
        2.7667e-02, 1.7536e-01, 2.8966e-07, 1.4005e-01, 3.7601e-07, 1.3095e-01,
        3.9777e-02, 1.2838e-01, 1.5587e-03, 1.3404e-07, 2.4153e-02, 4.2383e-07,
        2.9726e-07, 4.0685e-07, 1.1197e-07, 1.3192e-02, 1.4804e-02, 1.3446e-01,
        1.1926e-01, 1.4948e-02, 6.9897e-03, 2.0040e-02, 2.9253e-02, 1.4157e-02,
        1.4065e-07, 6.3504e-08, 1.5022e-07, 1.4421e-01, 3.7601e-07, 1.5279e-07,
        2.2723e-07, 7.1315e-10, 1.3258e-02, 1.0031e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.7306e-07, 7.6197e-02, 1.0316e-01, 8.5705e-07, 1.6848e-01, 6.3471e-02,
        8.1519e-07, 6.8757e-02, 9.0095e-02, 1.0997e-06, 7.7891e-07, 1.0160e-01,
        9.4441e-02, 9.8454e-02, 1.7123e-07, 2.6821e-07, 5.1887e-02, 4.0215e-02,
        1.1838e-06, 9.5212e-07, 6.3879e-07, 7.3802e-07, 5.2518e-02, 2.9584e-07,
        3.7058e-07, 8.0719e-02, 1.6032e-02, 1.1018e-06, 5.6369e-02, 3.2079e-02,
        4.5952e-02, 5.1181e-07, 4.4229e-02, 5.5856e-02, 1.4013e-01, 7.1347e-07,
        1.4047e-06, 9.4805e-02, 1.1481e-01, 1.1838e-06, 3.7058e-07, 4.1988e-07,
        1.3675e-01, 5.1957e-02, 6.7388e-07, 2.9993e-07, 7.6944e-07, 1.7716e-06,
        1.2099e-01, 8.9130e-02, 6.6596e-02, 5.4495e-07, 1.6894e-02, 4.0174e-07,
        1.4182e-02, 9.3028e-02, 1.6654e-02, 5.1790e-02, 7.7877e-02, 3.0153e-02,
        3.8962e-02, 1.7866e-02, 1.0436e-01, 9.7176e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.4107e-02, 5.1838e-03, 6.7352e-02, 1.3224e-01, 2.5205e-02, 9.3630e-02,
        1.2299e-02, 2.6216e-02, 6.8684e-03, 9.1283e-08, 4.2022e-02, 3.7460e-08,
        2.0425e-02, 5.4769e-03, 3.0498e-08, 3.8566e-02, 2.3215e-02, 1.0414e-02,
        9.2136e-03, 2.6689e-08, 3.5273e-02, 5.8027e-02, 3.3359e-08, 3.2723e-02,
        2.1090e-02, 1.3040e-01, 1.2168e-02, 5.9072e-08, 5.7998e-02, 8.3252e-03,
        4.8893e-02, 1.1860e-02, 7.5158e-02, 8.8987e-02, 5.3331e-02, 1.2771e-02,
        1.3415e-01, 2.2394e-02, 1.1091e-02, 2.5610e-02, 5.2007e-03, 3.5792e-02,
        3.7630e-08, 1.8458e-02, 4.5324e-03, 1.2789e-02, 1.1155e-02, 5.5556e-08,
        7.8411e-02, 4.1167e-02, 1.3005e-07, 3.8275e-08, 5.8133e-08, 9.2826e-09,
        7.9083e-02, 1.5211e-02, 7.2095e-02, 8.9082e-03, 1.2858e-02, 1.4149e-02,
        5.8366e-02, 1.2981e-02, 3.1693e-02, 5.9048e-08, 2.4895e-02, 2.0052e-02,
        1.8958e-02, 2.3104e-03, 2.6499e-02, 3.3817e-02, 2.9392e-02, 5.3973e-02,
        1.9129e-02, 6.0369e-03, 2.4646e-03, 8.1648e-08, 1.2335e-02, 1.5589e-02,
        1.2252e-01, 7.9173e-08, 8.6986e-02, 1.1937e-07, 1.2615e-02, 1.2220e-01,
        2.4531e-02, 3.5217e-02, 1.3005e-07, 5.7980e-08, 1.7913e-02, 5.5776e-02,
        1.2186e-02, 6.1231e-02, 3.7964e-02, 2.3645e-02, 4.9111e-02, 8.7240e-02,
        9.9135e-03, 8.9514e-08, 4.1485e-08, 1.7631e-02, 2.4241e-08, 7.0790e-02,
        3.4653e-02, 6.9434e-03, 3.8339e-02, 8.1153e-03, 6.7567e-03, 1.2854e-01,
        2.9373e-02, 6.5979e-08, 8.4559e-03, 7.6704e-02, 3.2711e-02, 2.8116e-02,
        4.6166e-02, 5.2167e-02, 3.9116e-02, 8.2093e-03, 1.7078e-02, 5.4802e-02,
        4.0507e-02, 5.1936e-02, 6.5911e-02, 1.5930e-02, 1.4295e-01, 6.1959e-02,
        1.5830e-08, 7.7442e-08, 2.0191e-02, 1.2211e-01, 9.9283e-03, 1.7333e-02,
        1.7093e-02, 7.4349e-02, 5.1831e-02, 5.6006e-03, 2.9109e-03, 2.6257e-01,
        1.0108e-02, 1.4656e-02, 3.0022e-02, 2.3323e-02, 8.0450e-03, 1.4891e-02,
        3.9155e-02, 8.5874e-03, 4.9741e-02, 8.9643e-02, 2.5822e-02, 1.0748e-01,
        9.2719e-02, 6.3439e-03, 7.4442e-03, 4.3523e-08, 7.9874e-08, 1.6257e-01,
        1.0660e-01, 3.9207e-02, 1.5047e-02, 4.2462e-02, 1.7246e-02, 6.4815e-02,
        3.0327e-02, 4.6790e-02, 3.8267e-08, 7.0075e-08, 3.6748e-02, 1.6932e-08,
        1.2908e-01, 9.7445e-03, 1.7543e-02, 5.1312e-02, 1.6068e-02, 6.8463e-02,
        9.6108e-03, 1.2412e-01, 1.9497e-02, 2.7937e-02, 2.5224e-02, 3.5035e-03,
        9.1876e-03, 9.2666e-02, 1.5694e-02, 1.3805e-02, 8.9134e-02, 9.3747e-03,
        1.2479e-02, 1.1460e-07, 1.1548e-02, 1.3623e-07, 1.2003e-02, 7.6461e-02,
        1.4301e-02, 3.3373e-08, 7.3967e-08, 1.6891e-02, 1.2697e-02, 1.2770e-02,
        3.4013e-02, 5.6777e-08, 5.9582e-02, 1.7052e-02, 1.4874e-02, 3.9345e-02,
        8.7330e-08, 1.3974e-01, 1.5822e-02, 1.1463e-02, 3.6311e-02, 4.7156e-02,
        1.5651e-02, 1.4504e-02, 1.8307e-02, 1.4829e-02, 2.1233e-08, 3.7969e-08,
        6.6021e-02, 3.8958e-02, 3.4075e-08, 9.1073e-03, 1.9471e-02, 4.3632e-08,
        4.4830e-08, 1.4424e-02, 1.2210e-02, 6.5728e-03, 1.4230e-01, 1.3880e-07,
        1.8681e-02, 4.9520e-02, 1.1437e-02, 1.7269e-02, 1.2780e-02, 8.8745e-03,
        9.7804e-02, 4.1757e-02, 9.8928e-02, 2.4077e-02, 5.4959e-03, 4.8711e-02,
        1.1857e-02, 1.2366e-01, 1.2265e-02, 1.1704e-02, 1.1999e-02, 7.4374e-02,
        7.0092e-02, 1.9032e-02, 1.3244e-02, 6.1343e-02, 1.5089e-01, 7.0847e-02,
        8.3928e-03, 4.5546e-02, 1.4786e-03, 1.4467e-02], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.5737e-02, 1.6425e-02, 6.8568e-08, 6.6639e-03, 1.0178e-07, 6.8613e-08,
        2.9870e-07, 6.5006e-02, 5.7523e-02, 2.0686e-07, 1.3259e-07, 2.3764e-07,
        1.3107e-07, 1.6138e-07, 4.6159e-02, 4.0909e-02, 6.8935e-02, 3.6509e-02,
        1.5248e-07, 5.9742e-02, 4.7594e-07, 7.1210e-02, 1.7773e-01, 6.8613e-08,
        4.2214e-03, 2.7545e-02, 2.0484e-07, 6.6668e-02, 2.9029e-07, 5.7140e-02,
        5.1065e-06, 2.5722e-07, 6.3725e-02, 2.3526e-07, 2.5746e-02, 5.1376e-08,
        1.4576e-02, 2.0998e-07, 2.1903e-07, 9.6505e-08, 1.4243e-07, 5.3402e-07,
        1.3492e-07, 1.9424e-01, 1.5562e-02, 2.0724e-07, 1.0178e-07, 1.6901e-03,
        3.0561e-07, 6.1662e-02, 1.1889e-07, 5.1597e-02, 4.5092e-02, 5.3753e-02,
        2.4468e-01, 3.0043e-07, 4.7590e-07, 9.6210e-08, 1.9478e-02, 6.2259e-02,
        3.8683e-02, 4.1548e-02, 5.4878e-02, 6.1417e-07, 2.0998e-07, 1.5659e-02,
        1.3492e-07, 1.2570e-01, 2.6491e-02, 6.8569e-08, 2.9583e-02, 1.4974e-07,
        1.4684e-07, 3.1410e-07, 2.4618e-07, 1.9733e-01, 5.6708e-02, 2.2076e-07,
        7.9741e-02, 5.8634e-02, 4.6621e-07, 6.1232e-02, 4.4823e-04, 3.1410e-07,
        2.3277e-01, 4.2724e-02, 3.5204e-02, 3.0561e-07, 1.0178e-07, 2.0723e-07,
        5.3402e-07, 6.8568e-08, 3.2737e-07, 3.8275e-02, 5.2690e-02, 2.0319e-02,
        2.1505e-02, 6.8613e-08, 1.4974e-07, 2.2703e-01, 1.5540e-01, 2.9870e-07,
        1.0144e-05, 4.4079e-02, 6.8568e-08, 2.8834e-02, 3.0561e-07, 4.2201e-02,
        1.5825e-01, 5.4269e-02, 1.3259e-07, 1.1312e-02, 6.8568e-08, 3.4415e-02,
        5.3402e-07, 2.2594e-02, 1.8807e-02, 1.1242e-07, 3.1410e-07, 6.5109e-02,
        1.4791e-07, 2.4944e-07, 1.7176e-02, 4.7697e-02, 5.7791e-02, 8.2090e-08,
        8.6628e-08, 8.6628e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.0234e-01, 9.1791e-02, 3.3568e-02, 3.5274e-07, 8.1462e-07, 1.5098e-06,
        3.5274e-07, 9.5334e-07, 4.4217e-07, 4.8342e-02, 1.0614e-06, 7.1562e-07,
        2.0678e-06, 6.0688e-07, 9.7571e-02, 5.2985e-07, 3.0479e-06, 3.1644e-02,
        2.2328e-06, 2.3625e-06, 6.6151e-07, 1.0634e-06, 6.6151e-07, 3.5271e-02,
        1.0141e-06, 1.9299e-06, 1.9638e-06, 1.0615e-01, 1.0169e-06, 1.1865e-01,
        9.6903e-07, 4.8779e-07, 1.8763e-06, 1.3726e-06, 6.5060e-07, 8.3963e-02,
        2.2715e-02, 8.4412e-07, 5.2985e-07, 3.5279e-02, 4.2524e-02, 2.2326e-06,
        3.3379e-02, 1.0685e-06, 3.5274e-07, 2.2328e-06, 3.5722e-02, 4.0131e-02,
        5.7925e-07, 2.4018e-06, 1.4112e-06, 2.2326e-06, 8.1462e-07, 8.6853e-07,
        1.2219e-01, 7.2846e-07, 9.3405e-02, 4.1235e-02, 1.1337e-06, 1.0470e-01,
        8.3015e-07, 9.5334e-07, 9.7271e-07, 2.7346e-06, 2.0258e-06, 1.9638e-06,
        1.0634e-06, 2.2326e-06, 3.4990e-02, 6.5060e-07, 5.2985e-07, 1.0975e-01,
        7.2846e-07, 1.4112e-06, 7.5259e-07, 1.0634e-06, 3.5988e-06, 1.4231e-01,
        9.9533e-02, 1.2967e-02, 2.2328e-06, 2.8029e-02, 6.5060e-07, 7.1562e-07,
        1.4577e-01, 9.5334e-07, 2.2328e-06, 8.8335e-02, 1.7360e-06, 1.8763e-06,
        2.7209e-02, 8.9083e-07, 1.4322e-06, 1.9617e-07, 7.5259e-07, 8.9658e-02,
        1.0169e-06, 1.5127e-06, 2.2326e-06, 2.2328e-06, 8.5069e-07, 9.5334e-07,
        1.5228e-06, 1.0874e-01, 2.9364e-02, 1.5810e-06, 1.1009e-01, 1.0614e-06,
        1.8197e-02, 7.5623e-07, 1.0303e-02, 2.2082e-07, 2.3625e-06, 1.2444e-01,
        2.4236e-02, 1.1597e-06, 9.7271e-07, 3.9152e-07, 1.8763e-06, 9.8125e-02,
        9.2446e-07, 1.2405e-01, 7.2846e-07, 4.2081e-02, 1.9539e-06, 6.1974e-07,
        6.7435e-07, 1.8067e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5541e-01, 1.3603e-07, 2.5347e-08, 7.9188e-02, 4.6787e-08, 8.2435e-02,
        7.3909e-08, 5.4562e-08, 8.4262e-02, 1.2477e-02, 2.5847e-08, 6.7941e-02,
        5.7926e-02, 7.5853e-08, 4.0805e-08, 6.6485e-02, 5.6135e-08, 4.8913e-02,
        5.5629e-03, 1.3089e-02, 1.4013e-02, 1.7130e-07, 4.4597e-02, 4.3765e-08,
        7.7067e-08, 4.7675e-02, 4.5544e-02, 9.2235e-02, 7.5853e-08, 7.3909e-08,
        3.4825e-02, 1.5925e-02, 4.9148e-08, 1.9391e-01, 1.0584e-07, 4.0828e-02,
        2.4470e-02, 6.0572e-02, 7.7040e-02, 2.8767e-02, 9.2514e-08, 1.8309e-02,
        6.4509e-08, 4.3765e-08, 1.2473e-07, 4.0805e-08, 5.7109e-02, 4.3934e-02,
        3.3831e-02, 7.5853e-08, 2.1787e-02, 7.3332e-02, 1.3603e-07, 7.4157e-08,
        4.5330e-02, 1.3119e-02, 3.1721e-02, 4.9369e-08, 4.5836e-02, 6.7413e-02,
        1.3127e-02, 2.6489e-02, 1.8354e-02, 4.6837e-02, 1.3126e-07, 2.5347e-08,
        2.5600e-07, 2.0035e-07, 6.7831e-08, 2.5347e-08, 4.0960e-02, 5.6406e-02,
        1.3603e-07, 5.8185e-08, 4.1203e-02, 5.6135e-08, 1.6749e-01, 4.3765e-08,
        5.2473e-02, 1.4972e-02, 5.6334e-03, 7.4157e-08, 1.5311e-07, 4.3106e-02,
        4.3765e-08, 6.6874e-02, 3.3574e-02, 1.4973e-01, 2.0842e-02, 3.5345e-02,
        1.1324e-02, 1.2473e-07, 4.0805e-08, 2.3472e-02, 9.7278e-03, 2.1813e-02,
        7.7949e-02, 1.2544e-02, 1.9539e-02, 8.5143e-02, 4.7012e-02, 6.4091e-02,
        2.5250e-08, 2.9683e-02, 3.9862e-02, 5.8729e-08, 2.4402e-02, 7.0353e-02,
        7.1684e-03, 5.3343e-02, 8.8678e-08, 7.5853e-08, 4.9148e-08, 1.8509e-02,
        9.2514e-08, 3.2942e-02, 3.0621e-02, 8.2325e-02, 2.2322e-07, 7.1314e-08,
        9.1735e-08, 6.9945e-02, 3.4548e-02, 4.9926e-02, 6.3073e-02, 7.4916e-03,
        6.9327e-02, 4.1185e-02, 2.5847e-08, 9.1735e-08, 2.0248e-02, 6.7827e-02,
        7.5853e-08, 7.7847e-02, 2.5347e-08, 5.1836e-02, 9.8971e-08, 1.1210e-07,
        1.3603e-07, 2.3413e-02, 1.7844e-01, 4.8778e-08, 7.1424e-02, 1.3126e-07,
        4.4633e-02, 1.3061e-02, 8.6617e-02, 1.3069e-02, 7.6094e-02, 1.8130e-02,
        8.9371e-02, 9.1735e-08, 1.8940e-01, 8.8678e-08, 1.4560e-02, 3.1591e-02,
        6.5183e-02, 8.6408e-03, 3.0645e-02, 1.4922e-02, 4.0805e-08, 5.6591e-08,
        4.3765e-08, 3.5876e-02, 3.4656e-02, 6.0926e-02, 7.2268e-02, 1.7493e-07,
        1.1438e-02, 5.2317e-08, 2.8918e-02, 1.3126e-07, 1.6633e-02, 1.0540e-01,
        7.0152e-02, 2.4097e-02, 5.7186e-02, 5.2633e-02, 2.3332e-08, 4.0805e-08,
        6.9020e-02, 1.9622e-07, 3.7039e-03, 2.1220e-01, 2.2612e-02, 1.4878e-02,
        1.7732e-01, 3.5853e-02, 3.8010e-02, 6.8303e-02, 3.2983e-08, 2.2322e-07,
        5.6591e-08, 4.2507e-02, 1.0457e-07, 4.4813e-02, 3.5985e-02, 5.0562e-02,
        4.0728e-08, 4.9148e-08, 1.5219e-01, 1.9354e-07, 5.4562e-08, 1.7194e-07,
        8.8678e-08, 9.1735e-08, 1.3603e-07, 2.6673e-02, 3.5181e-02, 5.0728e-02,
        2.7691e-02, 2.2269e-02, 3.7980e-02, 1.7194e-07, 2.4510e-02, 1.7255e-02,
        1.0388e-07, 2.5847e-08, 3.9550e-02, 1.4449e-02, 1.0830e-01, 2.2322e-07,
        4.1313e-02, 2.3706e-02, 8.8678e-08, 4.0805e-08, 6.2807e-08, 8.4998e-03,
        4.8778e-08, 3.8840e-02, 5.4694e-02, 1.1783e-02, 1.4873e-01, 4.3765e-08,
        9.7213e-03, 2.5847e-08, 1.2688e-02, 8.5449e-02, 2.5347e-08, 1.2384e-07,
        6.3794e-02, 5.4361e-08, 7.8881e-02, 5.6135e-08, 1.7130e-07, 9.8971e-08,
        8.4594e-08, 1.2473e-07, 1.6524e-01, 7.5853e-08, 7.1207e-02, 4.9966e-02,
        1.9852e-07, 4.8246e-02, 4.0729e-08, 1.6596e-02, 5.6969e-02, 1.1352e-07,
        4.9103e-08, 7.6607e-08, 2.6057e-07, 3.1533e-02, 3.1719e-02, 1.7194e-07,
        1.5855e-08, 9.7253e-02, 2.5347e-08, 4.3765e-08, 7.7331e-08, 1.5272e-01,
        3.2379e-02, 2.0035e-07, 5.5621e-02, 2.6117e-08, 2.5847e-08, 1.6584e-02,
        6.5687e-02, 1.0729e-01, 7.3118e-02, 1.5519e-02, 5.4562e-08, 9.9028e-08,
        3.0182e-02, 4.5373e-02, 7.4157e-08, 4.1547e-02, 8.8678e-08, 2.7386e-02,
        1.0622e-02, 1.7194e-07, 4.1351e-03, 3.6884e-02, 7.5704e-02, 1.2473e-07,
        8.0173e-02, 1.9540e-02, 5.6591e-08, 5.0879e-02, 2.5250e-08, 5.7635e-02,
        8.6173e-02, 1.0468e-02, 7.7432e-03, 6.7808e-03, 2.6056e-02, 1.4802e-02,
        2.5847e-08, 5.6591e-08, 5.4562e-08, 3.7443e-02, 5.5244e-02, 6.3922e-02,
        1.1183e-02, 7.2263e-02, 2.0035e-07, 8.4709e-02, 1.9693e-02, 1.6146e-02,
        1.2473e-07, 4.5713e-02, 2.6885e-02, 6.4165e-02, 5.4027e-02, 2.5347e-08,
        2.1194e-02, 4.5475e-02, 1.2969e-07, 3.1723e-02, 3.8639e-08, 4.0805e-08,
        3.0965e-02, 8.4594e-08, 8.8678e-08, 4.3765e-08, 1.3603e-07, 7.3004e-08,
        1.3145e-07, 5.1801e-02, 9.7911e-08, 4.8778e-08, 6.6595e-08, 1.0279e-07,
        4.4660e-02, 7.8370e-02, 2.4939e-02, 9.1735e-08, 8.3529e-08, 4.9103e-08,
        5.8729e-08, 6.0035e-02, 3.9696e-02, 8.1666e-02, 5.5220e-08, 4.8284e-02,
        2.2887e-02, 7.1314e-08, 1.1213e-02, 4.8004e-02, 5.5821e-02, 4.0415e-02,
        3.3068e-02, 6.1658e-02, 2.5347e-08, 4.6758e-02, 1.7297e-03, 5.2471e-02,
        2.0035e-07, 1.7344e-02, 2.8719e-08, 6.2509e-02, 2.9782e-02, 2.5347e-08,
        6.2224e-02, 5.0451e-02, 4.9148e-08, 3.8639e-08, 2.5847e-08, 4.2917e-02,
        8.2029e-02, 4.6641e-02, 1.5621e-01, 2.0035e-07, 4.0576e-02, 7.1256e-08,
        2.1158e-02, 9.2771e-02, 7.6987e-08, 1.0040e-07, 1.1342e-02, 3.8499e-03,
        3.7851e-03, 2.5347e-08, 4.6174e-06, 2.1777e-02, 1.6208e-02, 4.6013e-02,
        4.3765e-08, 5.8729e-08, 4.0729e-08, 1.1210e-07, 1.1362e-02, 2.1417e-02,
        1.0850e-07, 1.3216e-02, 1.0234e-07, 9.6478e-02, 2.3039e-02, 4.8778e-08,
        6.0133e-02, 6.3171e-02, 3.0943e-02, 1.8998e-01, 2.0109e-02, 1.1665e-01,
        1.1210e-07, 4.3765e-08, 3.2786e-08, 1.2473e-07, 1.2085e-02, 1.2473e-07,
        1.9698e-02, 4.0805e-08, 4.9915e-02, 8.9754e-02, 5.1562e-02, 6.4553e-03,
        5.4361e-08, 8.4760e-03, 5.8906e-02, 1.5863e-01, 5.4361e-08, 2.3164e-02,
        5.6912e-02, 8.8678e-08, 7.9253e-02, 1.0266e-01, 5.5955e-02, 3.5182e-02,
        7.5853e-08, 4.2437e-02, 7.1228e-08, 1.9174e-01, 6.3190e-02, 4.4448e-08,
        9.1735e-08, 7.6586e-02, 6.8449e-02, 1.4326e-01, 4.0805e-08, 5.4741e-02,
        4.9148e-08, 5.0258e-03, 3.8796e-02, 6.7527e-08, 7.1256e-08, 5.7859e-08,
        6.8196e-02, 1.2473e-07, 1.5190e-01, 1.1084e-02, 3.5667e-02, 4.6274e-08,
        1.8882e-02, 9.6111e-08, 1.1210e-07, 1.7130e-07, 5.6135e-08, 2.8719e-08,
        3.9794e-02, 1.0012e-02, 4.3992e-02, 9.1735e-08, 7.1314e-08, 9.7513e-03,
        3.8397e-02, 8.6625e-03, 7.7342e-02, 1.5581e-02, 8.8678e-08, 1.0069e-01,
        8.6402e-03, 2.5847e-08, 6.0316e-08, 2.9480e-02, 9.0329e-08, 5.4562e-08,
        1.0738e-01, 5.3192e-02, 1.5071e-01, 7.1314e-08, 5.6135e-08, 1.6462e-02,
        7.1314e-08, 4.6047e-03, 2.0035e-07, 6.4302e-02, 1.5907e-08, 3.1146e-02,
        3.0665e-02, 2.8445e-02, 6.8790e-03, 4.1050e-02, 8.8678e-08, 2.0035e-07,
        6.1643e-02, 7.5853e-08, 5.2280e-02, 4.9211e-02, 7.1314e-08, 8.8636e-02,
        6.3839e-03, 8.7016e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([7.7686e-03, 2.8347e-07, 1.8224e-07, 5.3512e-08, 1.3050e-07, 4.4987e-02,
        1.3631e-07, 2.8889e-07, 7.1692e-02, 8.7405e-02, 1.4468e-07, 7.3325e-02,
        5.6625e-02, 1.9225e-07, 3.4269e-03, 4.7517e-02, 2.9945e-07, 3.3648e-02,
        1.4468e-07, 5.6761e-02, 9.4454e-02, 6.9903e-08, 2.7769e-03, 1.1220e-07,
        6.0350e-08, 1.0236e-01, 5.3258e-02, 3.2546e-02, 1.8224e-07, 6.0350e-08,
        4.5019e-02, 1.4856e-01, 5.3571e-08, 1.1201e-01, 1.5544e-02, 7.0582e-02,
        7.5222e-08, 4.1042e-02, 5.7892e-02, 6.3252e-02, 1.1220e-07, 3.4513e-02,
        1.4382e-02, 2.8293e-07, 5.3571e-08, 2.9945e-07, 6.4148e-02, 3.9394e-03,
        6.7472e-02, 6.2483e-08, 7.2234e-02, 3.1733e-02, 2.0623e-07, 1.3050e-07,
        5.6453e-02, 1.0919e-07, 6.5004e-02, 4.7248e-02, 5.2359e-02, 4.8956e-02,
        8.2034e-02, 2.6188e-01, 5.9608e-02, 5.3375e-02, 3.8338e-08, 1.4035e-07,
        2.8347e-07, 5.3571e-08, 2.2351e-07, 1.1661e-07, 8.4718e-02, 6.5715e-02,
        2.0239e-07, 4.8706e-02, 3.5387e-02, 6.0350e-08, 6.8563e-02, 2.8293e-07,
        3.5721e-02, 3.2622e-02, 5.2413e-02, 6.2483e-08, 4.4756e-02, 4.8042e-03,
        9.0983e-08, 3.7092e-02, 5.8276e-02, 1.0692e-02, 8.0109e-02, 9.1369e-02,
        6.3668e-02, 1.3050e-07, 2.8889e-07, 8.0107e-02, 7.1183e-02, 5.2702e-02,
        8.7058e-02, 7.3518e-02, 8.0920e-02, 4.3208e-08, 4.1826e-02, 7.0888e-02,
        6.0350e-08, 7.5398e-02, 4.3929e-02, 6.9903e-08, 5.7623e-02, 3.5121e-02,
        5.2646e-02, 8.8783e-02, 5.3571e-08, 1.9225e-07, 6.3493e-08, 6.1003e-02,
        1.1768e-07, 8.1094e-02, 1.0407e-01, 4.6667e-02, 2.9945e-07, 1.9225e-07,
        5.3571e-08, 6.9438e-02, 6.2538e-02, 5.9802e-02, 5.8444e-02, 5.2865e-02,
        2.8881e-02, 1.1468e-01, 1.1220e-07, 1.1220e-07, 9.3907e-02, 8.0072e-02,
        6.0350e-08, 3.0717e-02, 2.8293e-07, 5.1990e-02, 1.0945e-07, 3.7598e-08,
        6.9253e-08, 8.6771e-02, 5.7138e-02, 3.7598e-08, 3.7710e-02, 9.0983e-08,
        7.8181e-02, 4.9038e-02, 5.3040e-02, 6.4362e-02, 7.5180e-02, 8.0320e-02,
        1.3816e-01, 2.8347e-07, 8.5162e-02, 2.8889e-07, 2.1132e-01, 4.1279e-02,
        4.9452e-02, 5.7469e-02, 4.0890e-02, 6.4685e-02, 8.4440e-08, 2.9945e-07,
        6.9903e-08, 6.8143e-02, 7.8589e-02, 5.1009e-02, 4.6512e-03, 2.8293e-07,
        5.6668e-02, 4.1189e-02, 7.1837e-08, 1.1220e-07, 5.3217e-02, 7.5156e-02,
        4.1170e-02, 7.2780e-02, 3.6286e-02, 7.1065e-02, 3.8338e-08, 1.0317e-07,
        6.9402e-03, 2.1610e-07, 2.3340e-02, 8.1696e-02, 2.2102e-01, 8.3509e-02,
        5.9044e-02, 6.5662e-02, 5.9551e-02, 3.8145e-02, 4.2778e-02, 2.9769e-07,
        3.7598e-08, 7.1187e-02, 1.4035e-07, 8.7700e-02, 4.9005e-02, 6.5232e-02,
        1.3200e-07, 6.9903e-08, 2.3690e-02, 1.5083e-07, 5.3571e-08, 1.6191e-07,
        1.9225e-07, 1.3344e-07, 2.8889e-07, 1.0673e-01, 2.6306e-02, 1.9323e-02,
        7.2072e-02, 1.0532e-01, 6.4353e-02, 1.9225e-07, 7.4136e-02, 2.2812e-01,
        9.4816e-03, 1.3631e-07, 8.0350e-02, 5.2426e-02, 2.1316e-02, 1.4468e-07,
        9.4641e-02, 1.2248e-07, 6.3493e-08, 6.0350e-08, 4.5309e-08, 7.2071e-02,
        2.8889e-07, 6.0095e-02, 6.4905e-02, 5.5713e-02, 1.3170e-02, 5.3571e-08,
        8.5261e-03, 1.0317e-07, 7.8252e-02, 1.2423e-02, 2.1017e-07, 8.3742e-03,
        6.4763e-02, 4.5309e-08, 6.5647e-02, 2.1744e-07, 2.2633e-08, 2.7241e-08,
        1.9047e-07, 1.4035e-07, 9.6370e-03, 5.3571e-08, 4.6076e-02, 6.8543e-02,
        1.9047e-07, 8.0253e-02, 4.5309e-08, 1.0736e-01, 7.0914e-02, 1.1661e-07,
        1.5078e-07, 2.6888e-02, 1.9047e-07, 6.8955e-02, 6.3002e-02, 1.1220e-07,
        1.5947e-08, 4.1261e-02, 1.1220e-07, 1.8216e-07, 2.6612e-02, 7.0457e-03,
        4.4780e-02, 6.0316e-08, 6.4599e-02, 6.0350e-08, 6.9903e-08, 6.5341e-02,
        6.0752e-02, 2.6516e-02, 6.2561e-03, 8.3921e-02, 3.7598e-08, 2.1927e-07,
        7.8832e-02, 4.8183e-08, 1.9047e-07, 7.6569e-02, 2.8293e-07, 1.1215e-01,
        3.1198e-02, 3.7598e-08, 2.6793e-02, 7.7623e-02, 3.3358e-02, 6.0316e-08,
        4.7609e-02, 4.8500e-02, 1.9225e-07, 6.4020e-02, 5.1196e-08, 2.6455e-02,
        5.6187e-02, 5.6980e-02, 4.7326e-02, 5.1995e-02, 5.3184e-02, 4.0781e-02,
        1.9225e-07, 6.2483e-08, 1.8216e-07, 1.0729e-01, 3.4854e-02, 4.0303e-02,
        4.7345e-02, 7.6614e-03, 1.9047e-07, 3.7713e-02, 5.1922e-02, 5.5761e-02,
        2.8889e-07, 1.1747e-07, 2.5414e-02, 4.7083e-02, 2.7712e-02, 7.5457e-08,
        1.2177e-01, 4.5007e-02, 1.2971e-03, 6.7917e-02, 1.4468e-07, 3.7598e-08,
        6.0399e-02, 1.0945e-07, 2.1017e-07, 1.1220e-07, 1.0470e-08, 4.8690e-02,
        9.3599e-08, 5.6834e-02, 5.3571e-08, 1.8224e-07, 3.5637e-02, 5.3571e-08,
        7.8411e-02, 8.4204e-02, 2.1535e-02, 6.9903e-08, 1.8216e-07, 6.2095e-08,
        4.4193e-08, 4.1367e-02, 6.5706e-02, 4.1159e-02, 3.6562e-02, 8.8543e-02,
        1.2912e-07, 1.0317e-07, 4.3916e-02, 6.7714e-02, 1.0058e-07, 7.0620e-02,
        7.5900e-02, 4.2029e-02, 1.1860e-07, 2.1799e-02, 2.3242e-02, 3.5729e-02,
        1.9483e-07, 7.6405e-02, 2.7241e-08, 7.3118e-02, 5.8310e-02, 2.1990e-08,
        9.8237e-08, 3.4316e-02, 2.1066e-08, 3.8338e-08, 6.0350e-08, 8.1278e-02,
        9.3428e-02, 7.2401e-02, 5.6713e-02, 5.3571e-08, 6.3828e-02, 1.0619e-07,
        4.2945e-02, 6.7037e-02, 1.1572e-07, 2.2999e-02, 5.5253e-02, 6.4932e-02,
        1.4949e-07, 6.2483e-08, 3.6675e-02, 5.8564e-02, 2.0594e-02, 6.2881e-02,
        9.3599e-08, 2.1017e-07, 1.9047e-07, 1.1220e-07, 4.4981e-02, 8.7028e-02,
        4.5711e-02, 6.3175e-02, 5.8783e-02, 7.7653e-02, 6.4998e-02, 2.8889e-07,
        7.7655e-03, 3.2070e-02, 7.9658e-02, 6.4295e-03, 6.2339e-02, 8.5499e-03,
        2.8776e-07, 1.1220e-07, 1.8224e-07, 6.0350e-08, 2.4059e-02, 1.3201e-07,
        9.6910e-02, 6.9903e-08, 7.3757e-08, 4.1668e-02, 1.9457e-02, 3.9352e-02,
        8.4440e-08, 4.7021e-02, 4.4066e-02, 8.9601e-03, 5.3571e-08, 9.4000e-02,
        8.5525e-02, 9.9921e-08, 3.2217e-02, 3.2499e-03, 3.1922e-02, 6.3175e-08,
        6.9903e-08, 6.4340e-02, 2.6343e-03, 6.7307e-03, 5.1957e-02, 5.0956e-02,
        1.1661e-07, 6.6362e-03, 5.4655e-02, 6.8272e-03, 6.0350e-08, 9.9481e-02,
        2.1017e-07, 4.5832e-02, 4.6754e-03, 1.7463e-02, 2.0239e-07, 3.2374e-02,
        4.7129e-02, 8.4440e-08, 4.3125e-02, 8.5234e-02, 7.1416e-02, 3.6265e-02,
        8.0317e-02, 1.3609e-07, 1.1220e-07, 1.0945e-07, 2.8889e-07, 1.9225e-07,
        8.9220e-02, 3.6029e-02, 6.9243e-02, 6.0350e-08, 2.1927e-07, 6.7580e-02,
        4.6544e-02, 8.4320e-02, 1.7383e-02, 4.1028e-02, 1.5083e-07, 1.0896e-02,
        5.9573e-02, 8.4440e-08, 2.8670e-02, 9.7842e-02, 1.1641e-07, 4.9166e-08,
        1.1080e-02, 8.7850e-02, 5.2316e-03, 5.3571e-08, 6.0350e-08, 5.9426e-02,
        9.3599e-08, 1.7085e-07, 2.8347e-07, 6.7448e-02, 6.0350e-08, 1.0089e-01,
        5.2429e-02, 4.6572e-02, 6.3691e-02, 6.6961e-02, 1.8216e-07, 2.1010e-08,
        6.3326e-02, 3.2976e-07, 5.0672e-02, 8.5453e-02, 2.1066e-08, 6.3086e-02,
        1.0061e-07, 6.2483e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.3119e-02, 1.6622e-07, 5.1381e-02, 3.4128e-02, 4.1512e-07, 3.4000e-07,
        1.4310e-07, 2.2532e-07, 1.4310e-07, 6.7331e-02, 1.4310e-07, 5.3653e-02,
        1.0553e-07, 8.7333e-02, 4.8219e-02, 1.2495e-07, 3.6473e-07, 1.6622e-07,
        3.7551e-02, 9.0427e-07, 1.4912e-02, 6.5936e-02, 1.1371e-07, 7.0315e-02,
        9.9182e-08, 1.2495e-07, 1.4310e-07, 4.1670e-07, 5.0452e-07, 5.7544e-02,
        6.3469e-02, 6.3077e-02, 1.4310e-07, 4.1512e-07, 4.4251e-02, 5.1812e-02,
        9.1293e-08, 2.2850e-01, 7.0194e-07, 5.4441e-02, 9.9182e-08, 6.8216e-07,
        3.4000e-07, 1.4310e-07, 9.0426e-07, 1.2495e-07, 5.0543e-07, 1.6533e-02,
        1.1047e-07, 4.0556e-07, 2.7810e-02, 8.6358e-02, 1.1060e-07, 5.0543e-07,
        1.2495e-07, 4.1512e-07, 1.1371e-07, 4.0556e-07, 5.0241e-02, 3.8685e-07,
        4.0246e-02, 3.7657e-02, 1.5197e-07, 6.6999e-02, 1.5197e-07, 2.6931e-02,
        1.5197e-07, 5.0452e-07, 5.9844e-02, 4.8908e-02, 2.4375e-07, 6.7985e-02,
        4.0388e-02, 3.2018e-07, 5.6000e-02, 6.9337e-07, 6.8168e-07, 4.7568e-02,
        3.9095e-02, 2.2533e-07, 5.7836e-02, 8.7375e-02, 1.4310e-07, 5.6177e-08,
        6.8476e-07, 1.5197e-07, 7.3226e-02, 2.6468e-02, 5.0543e-07, 4.1512e-07,
        3.8685e-07, 1.2495e-07, 1.2495e-07, 6.5768e-02, 1.1252e-01, 4.6230e-02,
        1.0784e-07, 2.6543e-02, 9.9182e-08, 5.0317e-02, 4.1766e-02, 8.8251e-02,
        4.4983e-02, 6.2925e-02, 5.8498e-02, 1.1700e-02, 3.2105e-07, 1.1060e-07,
        1.2495e-07, 7.5837e-02, 5.1752e-02, 4.1699e-07, 1.2495e-07, 1.1057e-01,
        4.9662e-07, 5.0452e-07, 3.8165e-02, 5.3838e-02, 3.5109e-07, 1.5197e-07,
        1.4310e-07, 2.8465e-02, 5.2539e-02, 5.2226e-02, 1.2495e-07, 1.4310e-07,
        3.2950e-02, 2.2525e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.4788e-02, 3.4395e-08, 3.0541e-07, 1.3996e-01, 1.9684e-07, 1.1585e-06,
        5.2101e-07, 1.5328e-07, 1.1349e-01, 1.2075e-01, 1.2192e-01, 7.9455e-07,
        2.4846e-07, 1.0987e-06, 1.3285e-01, 1.1818e-01, 1.0668e-01, 8.8233e-02,
        1.1585e-06, 1.9684e-07, 1.0987e-06, 1.0040e-06, 1.6909e-07, 1.3096e-01,
        7.1148e-02, 7.1063e-07, 1.5299e-01, 1.4740e-01, 2.3936e-07, 1.0034e-06,
        3.2195e-07, 1.1600e-06, 1.0987e-06, 3.0542e-07, 1.0987e-06, 9.3280e-02,
        2.5100e-08, 2.4846e-07, 1.5992e-01, 5.8299e-07, 1.1690e-06, 7.6162e-02,
        2.4066e-07, 1.1585e-06, 9.8447e-02, 2.0595e-07, 3.6972e-02, 7.0945e-02,
        3.9856e-07, 8.5854e-02, 3.9856e-07, 1.2344e-01, 9.1658e-02, 6.1129e-07,
        2.0595e-07, 1.1956e-01, 1.5037e-01, 5.5975e-02, 2.4066e-07, 7.1063e-07,
        4.8482e-07, 1.0875e-01, 1.3280e-01, 7.6838e-02, 7.1063e-07, 1.1585e-06,
        1.0987e-06, 7.1063e-07, 5.0247e-07, 5.0025e-02, 7.3055e-02, 1.3201e-01,
        1.2657e-01, 1.2443e-01, 4.1241e-02, 2.1224e-07, 2.1224e-07, 5.2101e-07,
        1.3389e-01, 4.3684e-07, 9.3162e-02, 3.7659e-07, 1.1689e-06, 1.0369e-01,
        1.1541e-01, 1.2718e-01, 1.0784e-01, 2.1224e-07, 1.0106e-01, 8.9209e-02,
        5.8166e-07, 1.1585e-06, 5.2548e-07, 3.2194e-07, 7.1063e-07, 1.1585e-06,
        1.1742e-01, 1.1666e-01, 1.1585e-06, 6.5200e-02, 1.3527e-01, 1.9684e-07,
        3.1158e-02, 1.1585e-06, 2.1224e-07, 1.1585e-06, 5.8299e-07, 1.1553e-01,
        1.1099e-01, 1.0491e-01, 1.9578e-07, 9.2698e-02, 2.3936e-07, 7.1063e-07,
        1.1011e-01, 3.0541e-07, 4.0244e-02, 2.4066e-07, 5.8773e-07, 1.0562e-01,
        5.3444e-07, 1.3280e-01, 1.1585e-06, 2.0595e-07, 7.1063e-07, 1.0623e-01,
        2.4066e-07, 1.1690e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.7867e-02, 1.4642e-02, 7.7307e-08, 2.2245e-07, 5.4925e-02, 3.2958e-02,
        1.0220e-07, 5.5188e-08, 1.0149e-02, 4.1596e-02, 1.3403e-01, 7.0151e-02,
        4.9162e-02, 1.3431e-07, 2.4482e-02, 1.3162e-02, 1.0220e-07, 1.3877e-07,
        4.7493e-02, 2.2478e-02, 5.6823e-02, 1.4449e-01, 4.0595e-02, 1.1800e-07,
        6.2230e-08, 3.7219e-02, 5.5503e-02, 1.3706e-01, 4.2581e-02, 7.3873e-02,
        5.6685e-02, 5.4802e-02, 2.7398e-08, 3.7754e-02, 5.3397e-03, 5.5606e-02,
        4.2517e-03, 2.3996e-02, 6.3832e-02, 5.9598e-02, 6.4603e-02, 7.6289e-08,
        1.8665e-02, 5.3569e-08, 3.8375e-02, 6.0071e-02, 3.2364e-02, 4.1569e-02,
        1.5086e-01, 9.6844e-08, 5.5404e-02, 2.4046e-02, 2.4704e-07, 6.2497e-08,
        9.5154e-03, 6.9836e-08, 5.4331e-02, 4.6856e-02, 3.0178e-02, 5.4613e-02,
        3.7371e-02, 6.8225e-02, 3.9993e-02, 2.7393e-02, 5.3569e-08, 2.2359e-07,
        2.2233e-07, 5.4760e-08, 6.8151e-08, 1.0140e-07, 5.2755e-02, 4.3385e-02,
        4.2830e-08, 2.8636e-02, 4.6632e-02, 4.5756e-02, 2.0096e-02, 5.3075e-02,
        4.1097e-02, 3.8480e-02, 4.0119e-02, 1.2393e-07, 7.4418e-02, 3.1551e-02,
        8.7372e-08, 3.5635e-02, 5.4119e-02, 2.5199e-02, 5.9172e-02, 6.3848e-02,
        3.5402e-02, 1.7567e-03, 2.8581e-08, 7.9350e-02, 3.8153e-02, 1.2714e-02,
        9.4014e-03, 6.1410e-02, 5.4432e-02, 5.9612e-02, 7.8634e-02, 5.2130e-02,
        2.1951e-07, 3.7720e-02, 4.7587e-03, 6.8151e-08, 6.7833e-02, 4.4998e-02,
        3.7858e-02, 5.5101e-02, 5.3885e-02, 1.2857e-01, 1.7491e-07, 9.5193e-02,
        2.2400e-07, 6.0742e-02, 5.4038e-02, 7.2096e-02, 2.1844e-07, 9.5458e-08,
        2.4823e-02, 6.6459e-02, 6.5926e-02, 4.1668e-02, 2.3800e-02, 2.3432e-02,
        4.1342e-02, 1.0381e-01, 6.9728e-08, 5.8199e-02, 1.5152e-02, 7.7390e-02,
        1.1729e-07, 2.3332e-02, 2.7459e-02, 5.6444e-02, 7.7334e-02, 3.9213e-02,
        4.6673e-08, 4.8334e-02, 8.0461e-02, 4.2966e-08, 7.2272e-02, 7.8559e-02,
        1.9498e-02, 5.4964e-02, 2.3856e-02, 1.0766e-02, 4.5507e-02, 5.2335e-02,
        4.0935e-02, 2.6642e-02, 2.4298e-02, 3.4901e-08, 3.7370e-02, 5.7119e-02,
        3.0667e-02, 6.4226e-02, 4.0443e-02, 5.6185e-02, 5.2403e-02, 4.4541e-02,
        1.4027e-07, 2.8114e-02, 7.6004e-02, 4.0369e-02, 2.5546e-02, 6.8151e-08,
        8.5358e-03, 4.6411e-02, 1.8364e-02, 2.2238e-07, 6.8080e-08, 5.6721e-02,
        3.3403e-02, 1.1632e-01, 3.3099e-02, 5.0097e-02, 4.0079e-08, 1.0140e-07,
        5.3106e-03, 1.1729e-07, 2.1347e-02, 2.8223e-02, 4.4649e-02, 1.1881e-02,
        1.1026e-02, 6.0907e-02, 4.4422e-02, 6.3119e-02, 1.6018e-02, 4.0079e-08,
        3.2790e-08, 5.7474e-02, 2.8581e-08, 4.2014e-02, 7.3510e-02, 5.6880e-02,
        4.4831e-08, 7.0606e-02, 2.9418e-02, 4.6498e-02, 5.4015e-08, 1.4696e-07,
        6.3682e-02, 1.3906e-02, 5.3892e-08, 2.0501e-02, 2.5187e-07, 6.7903e-02,
        7.8387e-02, 6.5887e-02, 7.1313e-02, 8.3926e-08, 5.1575e-02, 7.1776e-02,
        1.0118e-07, 1.0981e-07, 3.4433e-02, 4.1235e-02, 4.5115e-02, 1.7211e-02,
        1.5955e-02, 2.8444e-08, 1.7491e-07, 3.9518e-08, 4.5818e-02, 3.5911e-02,
        3.8992e-02, 4.5004e-02, 5.2201e-02, 6.0205e-02, 8.0863e-03, 1.0910e-07,
        5.2519e-08, 5.1472e-02, 1.7111e-02, 1.6232e-02, 2.7398e-08, 6.8216e-08,
        6.8703e-02, 5.5188e-08, 2.4819e-02, 5.5188e-08, 3.6535e-08, 5.9940e-02,
        7.0042e-02, 3.1757e-08, 4.8611e-02, 2.2522e-02, 1.0100e-02, 6.6910e-02,
        8.5629e-02, 5.0437e-02, 2.4704e-07, 5.7587e-02, 2.8757e-02, 4.7015e-02,
        4.2966e-08, 1.1682e-02, 1.1729e-07, 7.6654e-02, 6.5805e-02, 6.4339e-02,
        5.4670e-02, 4.2304e-02, 9.8302e-02, 1.1775e-07, 2.6122e-02, 5.9890e-02,
        4.6219e-02, 6.2097e-08, 1.0578e-02, 1.0325e-01, 7.3039e-08, 5.3922e-02,
        5.8375e-02, 2.6036e-02, 5.5371e-02, 4.8248e-02, 1.1800e-07, 1.6324e-07,
        5.8654e-02, 2.8032e-08, 1.0463e-07, 2.2934e-02, 1.5403e-01, 7.5544e-02,
        5.8173e-02, 3.2921e-02, 2.0000e-01, 5.0880e-02, 5.8850e-02, 2.5885e-02,
        5.6088e-02, 1.9641e-02, 1.5556e-07, 5.3808e-02, 1.0041e-01, 1.6401e-03,
        1.2966e-02, 1.3277e-02, 7.4889e-02, 1.2985e-01, 3.8968e-02, 1.3329e-02,
        1.0140e-07, 3.4383e-02, 4.1080e-02, 1.6008e-02, 3.8227e-02, 2.4014e-02,
        3.2637e-02, 8.6451e-08, 1.1729e-07, 6.9502e-02, 2.2946e-02, 2.5083e-02,
        1.2129e-01, 5.7236e-02, 1.7028e-07, 6.3025e-02, 3.5123e-02, 4.4531e-02,
        2.3939e-02, 6.4270e-02, 1.2393e-07, 6.4087e-02, 2.1377e-07, 1.0220e-07,
        4.3129e-02, 6.6931e-02, 1.7491e-07, 1.0140e-07, 1.9735e-02, 5.3983e-08,
        4.0899e-02, 1.3591e-02, 4.8486e-08, 7.0286e-02, 1.1064e-07, 6.3052e-02,
        1.2931e-02, 1.8269e-02, 5.9586e-02, 1.0140e-07, 5.3569e-08, 1.4027e-07,
        2.5416e-02, 7.2135e-02, 7.4960e-02, 3.3475e-02, 4.4404e-02, 2.8854e-02,
        1.0258e-07, 7.0862e-02, 1.8366e-01, 4.3907e-02, 9.3034e-03, 1.0767e-01,
        3.6560e-02, 5.2674e-02, 1.7491e-07, 4.7326e-02, 4.2148e-02, 2.0011e-07,
        3.3266e-02, 6.8500e-02, 3.3704e-02, 2.5102e-02, 1.0148e-02, 7.2624e-02,
        1.7080e-07, 2.2386e-02, 4.0079e-08, 7.1637e-08, 8.3952e-02, 4.3129e-02,
        6.0843e-02, 5.5714e-02, 2.5897e-02, 3.1301e-02, 2.4783e-02, 5.7535e-02,
        5.6697e-02, 1.2660e-02, 1.3392e-07, 1.0902e-07, 3.7924e-02, 3.3050e-02,
        6.6846e-08, 4.2693e-08, 8.9946e-03, 3.2345e-02, 2.6788e-02, 4.5663e-02,
        6.0415e-02, 1.0140e-07, 1.4027e-07, 1.0220e-07, 4.6081e-02, 6.8760e-02,
        1.3376e-03, 7.7955e-03, 1.3948e-07, 2.5586e-02, 7.9018e-02, 2.0111e-01,
        6.4255e-02, 7.0892e-02, 3.0369e-02, 7.1254e-02, 4.9418e-02, 3.9415e-02,
        4.1327e-02, 3.5570e-02, 4.0079e-08, 3.5346e-08, 7.1476e-02, 2.1304e-07,
        6.1219e-02, 5.3941e-02, 4.2117e-02, 1.4758e-02, 1.0735e-02, 4.2833e-02,
        5.2900e-02, 3.0997e-02, 2.4031e-02, 2.5828e-02, 1.2914e-01, 4.0369e-02,
        4.6563e-02, 1.4027e-07, 3.5135e-02, 6.2279e-02, 3.8936e-08, 3.7692e-08,
        7.1637e-08, 2.2633e-02, 1.7848e-02, 2.4489e-02, 6.7970e-02, 2.6463e-02,
        6.8151e-08, 2.2210e-02, 3.4771e-02, 2.5868e-02, 6.1650e-02, 5.7680e-02,
        8.1480e-02, 5.8424e-03, 2.4228e-02, 9.5988e-08, 4.0080e-08, 6.5107e-08,
        4.4094e-02, 6.8760e-02, 2.2418e-02, 2.3239e-02, 2.9745e-02, 6.1239e-08,
        6.1913e-02, 9.6844e-08, 5.4760e-08, 2.4704e-07, 9.7703e-02, 5.3569e-08,
        4.2019e-02, 1.5687e-07, 5.3735e-02, 6.3969e-02, 9.5458e-08, 2.7182e-02,
        6.7600e-02, 6.1739e-02, 7.8377e-02, 3.4484e-02, 3.8436e-02, 1.4866e-02,
        2.1621e-02, 1.4155e-07, 6.0247e-02, 9.4128e-02, 1.1798e-07, 7.6288e-02,
        3.0079e-02, 5.1252e-02, 3.2502e-02, 5.5188e-08, 7.0661e-08, 1.8353e-02,
        3.5094e-08, 1.0978e-07, 2.7119e-08, 3.0419e-02, 5.7502e-07, 4.0109e-02,
        2.8854e-02, 2.4614e-02, 1.6140e-01, 1.0845e-01, 6.8151e-08, 4.0079e-08,
        5.7652e-02, 1.5316e-07, 6.0913e-02, 1.5830e-02, 6.2097e-08, 1.1357e-02,
        1.0826e-07, 2.0843e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.7132e-07, 1.9082e-01, 5.1569e-02, 2.9880e-07, 4.7067e-02, 3.5470e-02,
        3.5964e-02, 2.2360e-08, 2.9790e-07, 4.8345e-07, 2.1158e-07, 2.7859e-02,
        3.8359e-07, 1.1550e-06, 5.0551e-02, 2.6218e-02, 3.7587e-02, 4.8852e-02,
        5.3427e-07, 4.4053e-02, 2.9790e-07, 7.5774e-07, 5.5902e-07, 1.1642e-06,
        2.1158e-07, 5.1531e-02, 1.3675e-01, 1.2084e-07, 4.3626e-07, 3.2948e-02,
        1.0594e-06, 2.9790e-07, 2.1238e-01, 2.5347e-07, 3.5578e-07, 2.9958e-02,
        4.1239e-07, 1.3148e-07, 1.1153e-06, 1.1237e-06, 1.4201e-01, 3.8312e-02,
        1.5439e-07, 2.1158e-07, 4.1965e-02, 4.0776e-02, 4.0842e-03, 2.1256e-07,
        1.9372e-07, 1.4937e-06, 3.8272e-02, 6.6335e-02, 5.0422e-02, 8.5453e-08,
        1.1548e-06, 8.2787e-07, 3.2887e-02, 3.1272e-02, 1.3252e-07, 2.2667e-07,
        5.8168e-07, 3.7132e-07, 1.7027e-01, 2.9260e-07, 1.6448e-01, 2.5786e-07,
        1.8479e-07, 4.4302e-07, 2.8983e-07, 2.7949e-02, 1.1249e-06, 3.0271e-02,
        4.6531e-02, 1.3937e-01, 2.9546e-07, 5.6145e-07, 4.7131e-02, 4.3626e-07,
        1.0216e-06, 4.0739e-02, 1.5643e-07, 5.3427e-07, 2.0547e-08, 7.5497e-07,
        2.5786e-07, 1.5222e-01, 2.3594e-07, 5.4253e-02, 6.5267e-07, 3.9370e-07,
        6.2340e-02, 6.3309e-07, 6.6120e-07, 8.9845e-07, 2.1771e-01, 4.5455e-07,
        4.9880e-02, 7.9960e-07, 3.1718e-07, 3.1881e-02, 4.0288e-02, 1.2241e-01,
        3.4625e-02, 2.9360e-07, 3.9935e-02, 3.1718e-07, 4.7700e-02, 2.8406e-07,
        2.7706e-07, 3.6116e-02, 2.7204e-02, 4.4212e-02, 1.1689e-02, 4.3856e-02,
        5.3427e-07, 1.0815e-01, 5.3155e-07, 4.5277e-02, 1.0161e-07, 6.6120e-07,
        1.5643e-07, 8.7548e-07, 2.3541e-07, 6.0557e-07, 4.4074e-02, 9.8661e-07,
        8.9518e-07, 4.7816e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.1271e-02, 1.1938e-06, 8.3684e-02, 1.0494e-01, 2.3243e-06, 2.1338e-06,
        1.8510e-06, 1.3340e-06, 7.2377e-07, 7.2880e-07, 1.3493e-06, 1.6772e-06,
        9.1058e-02, 9.8501e-07, 1.2893e-06, 1.3822e-06, 1.1774e-06, 1.0045e-01,
        9.9890e-02, 7.2377e-07, 8.8841e-02, 2.5087e-06, 2.3243e-06, 8.0920e-02,
        2.5087e-06, 3.4566e-06, 1.1351e-06, 1.2545e-06, 7.8555e-02, 1.0923e-06,
        1.1095e-06, 7.1203e-02, 6.7083e-07, 9.5250e-07, 9.1623e-07, 1.1786e-01,
        1.0369e-06, 1.6096e-06, 1.7865e-06, 4.6609e-07, 1.0824e-01, 1.3534e-06,
        2.0944e-06, 9.1623e-07, 9.6114e-02, 1.1974e-01, 2.3243e-06, 7.9700e-02,
        3.3240e-06, 1.0084e-01, 1.6968e-06, 2.3844e-06, 9.6205e-02, 6.9120e-02,
        7.8544e-07, 1.0336e-01, 4.6609e-07, 1.3508e-06, 1.0654e-06, 7.2377e-07,
        9.1623e-07, 8.2904e-02, 8.3284e-07, 1.4740e-06, 7.5838e-07, 1.6725e-06,
        6.4601e-07, 1.1069e-01, 8.7485e-02, 1.3822e-06, 9.5749e-02, 1.0848e-01,
        1.2391e-06, 3.3430e-06, 1.4217e-06, 3.7449e-02, 7.3082e-02, 1.9849e-06,
        4.2716e-02, 3.9638e-02, 8.6322e-07, 4.6609e-07, 1.2621e-01, 9.8762e-07,
        6.9898e-07, 9.0534e-07, 1.2893e-06, 1.5514e-06, 1.1966e-06, 9.5454e-02,
        8.5349e-02, 2.7356e-06, 1.6968e-06, 1.2856e-06, 3.5929e-07, 8.0158e-02,
        7.1399e-02, 1.7312e-06, 7.5901e-02, 8.2109e-07, 2.2814e-06, 8.5687e-02,
        6.5832e-07, 3.8388e-02, 1.1607e-01, 1.5930e-06, 8.4478e-07, 7.0237e-07,
        1.6498e-06, 1.6604e-06, 1.0058e-01, 1.0593e-06, 1.3784e-06, 2.3668e-06,
        4.0278e-02, 1.0973e-01, 8.0009e-02, 9.6928e-02, 1.8132e-06, 1.6968e-06,
        1.3340e-06, 1.4755e-06, 1.5628e-06, 9.7458e-07, 8.7667e-02, 1.8745e-06,
        7.2377e-07, 1.1152e-01], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2216e-02, 2.0240e-02, 9.9177e-08, 3.3338e-08, 7.5048e-03, 5.3633e-02,
        1.3983e-07, 4.2923e-02, 5.3219e-02, 5.0227e-02, 1.7267e-02, 5.3216e-02,
        5.3424e-02, 4.5050e-08, 5.2533e-02, 5.0406e-02, 6.8515e-08, 1.6599e-02,
        1.5625e-01, 1.3236e-01, 2.8483e-02, 1.0113e-02, 2.4485e-02, 5.0116e-08,
        7.8334e-08, 3.8545e-02, 3.6943e-02, 4.0520e-02, 8.1244e-02, 1.3683e-02,
        6.6180e-02, 4.4340e-02, 7.4793e-02, 1.7828e-01, 3.1870e-08, 5.7502e-02,
        1.0634e-07, 6.2470e-02, 4.2131e-02, 2.8894e-02, 4.5786e-02, 3.4419e-02,
        7.0691e-08, 1.2173e-07, 6.2149e-02, 6.2245e-02, 1.1090e-02, 1.5665e-02,
        5.2709e-02, 7.7824e-02, 2.8474e-02, 4.7398e-02, 3.2709e-02, 5.9883e-02,
        4.3087e-02, 4.1635e-08, 8.8886e-03, 1.4151e-02, 1.2155e-02, 3.6590e-02,
        3.8212e-02, 5.1930e-02, 3.7882e-02, 4.6121e-02, 1.9630e-01, 2.4230e-02,
        7.3501e-08, 4.2241e-02, 2.2332e-07, 4.6570e-08, 2.5442e-02, 3.2278e-02,
        1.7978e-07, 1.7838e-07, 1.2252e-07, 5.9580e-02, 2.1011e-02, 2.1119e-02,
        4.5418e-02, 4.4303e-02, 4.6385e-02, 5.0116e-08, 2.8384e-02, 1.1033e-07,
        6.0043e-02, 1.2606e-01, 5.4604e-02, 3.0055e-02, 3.1959e-02, 3.5191e-02,
        6.5467e-02, 1.3528e-01, 4.3627e-02, 7.9778e-02, 2.7464e-02, 5.8151e-02,
        5.0699e-02, 3.6363e-02, 3.7728e-02, 6.7608e-08, 3.2341e-08, 3.8687e-02,
        1.7978e-07, 2.8783e-02, 4.1831e-02, 1.8429e-07, 8.2843e-02, 3.1849e-02,
        9.3303e-02, 2.3880e-02, 2.7539e-02, 7.9225e-02, 5.6771e-02, 3.1473e-02,
        4.6570e-08, 3.5265e-02, 2.4684e-02, 3.6716e-02, 6.1654e-02, 1.4414e-02,
        5.1024e-02, 3.6160e-02, 1.7364e-02, 7.1235e-02, 2.1758e-02, 4.9469e-03,
        6.2500e-02, 4.1331e-02, 6.8699e-02, 2.5688e-02, 3.2711e-02, 4.4786e-02,
        7.3784e-02, 4.2745e-02, 4.9221e-02, 6.3359e-02, 3.0336e-02, 2.7118e-02,
        9.7980e-02, 3.8942e-02, 4.4928e-02, 2.2504e-07, 5.1983e-02, 4.6324e-02,
        7.3422e-03, 3.8956e-02, 3.3593e-02, 3.3878e-02, 2.7353e-02, 2.9965e-02,
        2.9172e-02, 4.6311e-02, 4.5753e-02, 3.6912e-02, 2.6040e-02, 4.5240e-02,
        1.2574e-07, 5.1301e-02, 3.8932e-02, 2.5658e-02, 3.9717e-02, 2.7447e-02,
        3.5187e-02, 1.6248e-02, 5.6311e-02, 3.8952e-02, 4.6707e-02, 4.8843e-02,
        3.9214e-02, 4.4202e-02, 4.5833e-08, 2.9660e-07, 2.2223e-02, 4.3535e-02,
        1.3490e-01, 3.6565e-02, 6.9054e-03, 4.7147e-02, 5.3018e-02, 9.3379e-02,
        2.6214e-07, 2.8005e-02, 1.6052e-01, 1.8367e-01, 3.5759e-02, 1.5213e-02,
        1.6069e-02, 2.5951e-02, 4.3057e-02, 2.1085e-02, 5.2267e-02, 9.5195e-02,
        7.8624e-02, 4.3853e-02, 5.6205e-02, 3.0853e-02, 4.1863e-02, 2.4516e-02,
        7.8334e-08, 5.0083e-02, 1.0251e-02, 6.4152e-02, 6.1022e-03, 4.6570e-08,
        1.1227e-02, 3.5167e-02, 4.6371e-08, 2.3189e-02, 7.3401e-08, 5.7562e-02,
        3.4596e-02, 2.1812e-02, 4.0758e-02, 2.2794e-01, 4.7160e-02, 4.9615e-02,
        5.2019e-03, 2.3166e-02, 4.1675e-02, 7.9501e-02, 4.9059e-02, 3.4949e-02,
        1.5502e-02, 7.8046e-08, 5.7625e-02, 1.6476e-02, 7.5333e-02, 1.7253e-02,
        1.0748e-07, 2.9981e-02, 3.7705e-02, 6.5732e-02, 3.2146e-02, 6.8392e-02,
        2.5703e-07, 3.2517e-02, 4.3353e-02, 3.9860e-02, 5.5318e-02, 3.4179e-02,
        6.1206e-02, 6.8515e-08, 2.6457e-02, 1.2953e-04, 5.8076e-02, 4.4788e-02,
        2.6912e-02, 7.1113e-08, 2.8115e-02, 1.1865e-02, 5.7828e-02, 3.8893e-02,
        3.5093e-02, 6.9935e-02, 6.2714e-08, 2.8914e-02, 3.2771e-02, 1.1716e-02,
        6.3333e-02, 4.8865e-02, 5.8427e-02, 1.8723e-02, 3.2646e-02, 1.1976e-02,
        3.7034e-02, 2.4076e-02, 2.0048e-02, 4.0524e-02, 1.0384e-02, 3.1034e-02,
        6.6080e-02, 7.1783e-02, 5.1991e-03, 3.5999e-02, 7.3501e-08, 3.3033e-02,
        5.5147e-02, 1.5044e-01, 1.1935e-02, 1.9795e-02, 5.4235e-02, 1.0471e-05,
        4.0990e-02, 9.6897e-08, 4.0310e-02, 1.2883e-02, 2.5702e-02, 3.2316e-02,
        7.0375e-02, 3.5120e-02, 2.7579e-02, 4.3903e-02, 3.6391e-02, 3.3436e-02,
        2.7143e-02, 4.3593e-02, 5.9685e-02, 1.0371e-02, 4.0369e-02, 2.3156e-02,
        5.6857e-02, 1.3948e-02, 4.0170e-02, 2.6521e-02, 3.6115e-02, 7.8325e-02,
        4.5443e-02, 8.1087e-02, 7.5513e-02, 4.1636e-02, 3.3073e-02, 4.5591e-02,
        3.0426e-02, 8.1168e-08, 3.4330e-08, 4.1569e-02, 4.4466e-02, 8.7820e-02,
        7.6608e-02, 1.5835e-07, 3.0815e-02, 4.4569e-02, 4.2328e-02, 4.4615e-08,
        2.0200e-02, 2.6803e-02, 4.8826e-08, 3.8910e-02, 8.9700e-08, 6.8154e-02,
        8.4275e-02, 8.9526e-03, 1.0502e-01, 5.8783e-02, 2.7798e-02, 4.8498e-08,
        7.3537e-02, 3.9527e-02, 1.1101e-02, 1.0239e-07, 2.2351e-02, 5.1523e-08,
        3.7102e-02, 7.2822e-02, 2.7718e-02, 2.4815e-02, 3.5516e-02, 1.6705e-01,
        1.8827e-02, 2.1027e-02, 1.6396e-02, 4.4729e-02, 2.2293e-02, 1.5163e-02,
        6.5275e-03, 2.7140e-02, 1.8465e-02, 4.7582e-02, 5.7590e-08, 3.6583e-02,
        4.8325e-02, 7.3136e-02, 1.9204e-01, 3.1448e-02, 2.7314e-02, 5.7564e-08,
        1.3075e-01, 5.0599e-02, 4.0653e-08, 3.3945e-02, 3.9196e-02, 3.5043e-02,
        4.6927e-08, 3.8089e-02, 1.0555e-07, 9.7655e-02, 7.4372e-02, 1.9162e-02,
        3.3596e-02, 2.1535e-02, 2.3499e-02, 9.1266e-03, 3.6813e-02, 6.6549e-02,
        4.2484e-02, 1.5385e-02, 5.5677e-08, 4.1774e-08, 2.3280e-02, 2.0935e-02,
        4.1458e-08, 1.4639e-07, 7.7169e-08, 2.8584e-02, 7.4815e-02, 4.8469e-02,
        6.8370e-03, 1.5826e-07, 6.3622e-02, 1.8357e-01, 5.2251e-02, 5.5382e-02,
        2.4764e-02, 3.9651e-02, 5.8100e-02, 3.2887e-02, 4.8162e-02, 1.6165e-02,
        1.6035e-02, 2.3852e-02, 2.9362e-02, 2.5077e-02, 2.9031e-02, 3.6355e-02,
        4.9647e-02, 2.9575e-02, 1.0636e-07, 1.2972e-04, 5.0038e-02, 4.6371e-08,
        4.8657e-02, 1.5422e-02, 1.6721e-02, 1.4810e-02, 6.8808e-03, 1.6094e-02,
        9.0870e-08, 8.9925e-03, 5.0375e-02, 9.9530e-03, 2.4826e-02, 2.1171e-02,
        5.2656e-02, 5.4921e-02, 9.5612e-03, 3.1990e-02, 3.7278e-02, 6.2273e-08,
        4.1727e-08, 5.5447e-02, 1.9157e-02, 1.7893e-02, 4.9347e-02, 7.9098e-02,
        8.3380e-03, 2.3695e-02, 4.5771e-02, 2.6145e-02, 1.0745e-07, 4.7762e-02,
        1.7567e-02, 5.9675e-02, 3.0995e-02, 6.0614e-02, 1.8429e-07, 4.7672e-08,
        4.0924e-02, 5.4490e-02, 2.1494e-02, 1.7195e-02, 3.4820e-02, 1.1377e-07,
        3.5973e-02, 8.9700e-08, 7.7755e-08, 1.1318e-01, 5.9180e-02, 1.6036e-01,
        2.4469e-02, 1.3116e-01, 3.6013e-02, 1.7063e-07, 3.7799e-02, 5.6793e-02,
        2.8570e-02, 4.4175e-02, 1.7327e-02, 3.4786e-02, 4.0230e-02, 6.3093e-02,
        2.3059e-02, 5.0116e-08, 7.7944e-08, 7.2487e-02, 2.5722e-07, 2.1069e-02,
        1.6659e-02, 4.9845e-02, 2.9056e-02, 8.2088e-02, 7.8155e-02, 2.4509e-02,
        1.4670e-07, 1.7968e-07, 8.5014e-09, 3.9265e-02, 1.5203e-07, 2.4444e-02,
        5.4986e-02, 1.6120e-01, 3.3695e-02, 3.8386e-02, 1.5583e-08, 4.8249e-02,
        3.1244e-02, 2.5722e-07, 4.1384e-02, 2.1738e-02, 1.5261e-01, 1.6483e-02,
        1.2890e-07, 1.1644e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.5702e-02, 4.8428e-07, 1.0820e-01, 3.9246e-02, 3.6327e-07, 5.1410e-02,
        4.1071e-07, 5.1281e-02, 4.6823e-02, 5.8654e-07, 1.0482e-01, 2.9013e-07,
        5.4508e-07, 5.2396e-02, 3.4837e-07, 5.6504e-02, 4.5883e-02, 3.8152e-02,
        4.9586e-02, 3.3887e-07, 4.5011e-07, 1.4627e-01, 8.9608e-02, 1.6859e-01,
        2.9013e-07, 8.7137e-02, 4.1671e-02, 4.8112e-07, 8.6355e-02, 9.7103e-02,
        3.8160e-02, 4.6306e-02, 4.5737e-02, 8.6044e-07, 4.5530e-02, 3.8473e-02,
        3.3238e-02, 4.2860e-02, 9.0572e-02, 4.1071e-07, 9.9602e-07, 3.6123e-02,
        2.6058e-07, 3.7455e-02, 5.8975e-02, 3.9208e-02, 4.5166e-02, 4.6256e-02,
        9.8979e-02, 5.2279e-02, 3.6288e-07, 1.4385e-01, 7.4988e-02, 9.9413e-02,
        5.3725e-02, 3.6046e-02, 9.2700e-02, 1.3567e-07, 8.0547e-07, 4.4767e-02,
        9.9602e-07, 6.1784e-07, 3.9723e-07, 2.0442e-07, 4.7643e-07, 5.1007e-02,
        9.3390e-07, 8.9966e-02, 5.7036e-02, 4.9036e-02, 5.2351e-07, 3.2513e-07,
        4.9087e-02, 5.2475e-02, 3.7137e-07, 7.7629e-07, 9.8332e-02, 1.3647e-06,
        6.4541e-07, 4.9661e-02, 3.2885e-02, 5.0722e-02, 9.1621e-02, 3.9667e-02,
        9.9603e-02, 3.1911e-07, 5.3914e-02, 3.8104e-02, 4.7342e-07, 1.1121e-01,
        1.4977e-01, 1.2466e-01, 9.9230e-02, 3.5516e-07, 1.6233e-01, 8.6676e-02,
        1.1198e-01, 5.4305e-02, 2.6077e-07, 3.5129e-02, 2.6077e-07, 5.4478e-02,
        1.0804e-01, 1.0687e-01, 1.0734e-01, 3.1216e-07, 1.0880e-01, 3.0073e-07,
        1.2196e-06, 1.0712e-06, 9.2303e-02, 4.8930e-02, 3.3000e-07, 3.2413e-07,
        3.3574e-07, 4.9700e-02, 1.3647e-06, 1.0224e-07, 4.7195e-02, 2.6544e-07,
        4.1680e-02, 5.3794e-02, 4.9148e-02, 9.7886e-02, 5.9743e-02, 9.5262e-02,
        4.7342e-07, 4.7695e-02], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.5940e-02, 7.0271e-07, 5.7939e-07, 1.1986e-06, 3.9704e-02, 1.4804e-07,
        1.1092e-01, 1.9929e-06, 7.5235e-07, 1.0720e-06, 1.7909e-06, 7.0271e-07,
        3.9065e-07, 1.5722e-06, 1.0249e-06, 8.9028e-02, 1.1587e-06, 7.9597e-07,
        9.8546e-07, 1.9195e-06, 2.0278e-06, 8.5372e-02, 5.9551e-02, 1.6050e-06,
        4.4634e-02, 7.9838e-02, 8.5880e-02, 1.3748e-06, 7.4346e-02, 9.8799e-07,
        4.9508e-02, 1.2786e-01, 7.9597e-07, 1.1224e-06, 5.6569e-02, 4.3670e-02,
        3.9540e-02, 5.0176e-02, 1.0770e-06, 8.3880e-07, 1.6600e-06, 5.6962e-02,
        7.9499e-07, 9.0124e-07, 3.3634e-02, 2.2044e-06, 3.8207e-07, 8.8877e-07,
        9.6169e-02, 1.9776e-06, 5.2450e-02, 4.5549e-02, 1.6600e-06, 7.9597e-07,
        3.9217e-07, 1.2209e-06, 1.1987e-06, 8.9096e-02, 9.5500e-02, 1.2770e-06,
        7.0271e-07, 3.3902e-07, 4.2425e-02, 8.9338e-02, 1.0843e-01, 8.4798e-02,
        3.9065e-07, 4.7017e-02, 8.5673e-02, 1.5722e-06, 2.1146e-06, 7.1269e-02,
        9.5686e-02, 6.4038e-02, 9.5313e-02, 9.1596e-02, 1.1177e-01, 4.5170e-02,
        4.5275e-02, 6.3050e-02, 7.9721e-02, 9.0383e-02, 1.1986e-06, 1.4235e-01,
        1.6807e-01, 4.7903e-02, 3.9065e-07, 1.0720e-06, 6.0792e-07, 8.3051e-07,
        3.0410e-06, 4.6781e-02, 5.9245e-02, 1.5112e-06, 1.5859e-06, 7.9499e-07,
        1.1127e-01, 1.0324e-01, 5.3758e-02, 1.8686e-06, 4.1241e-06, 5.1242e-02,
        7.9597e-07, 7.9296e-07, 8.3051e-07, 1.1727e-06, 5.1626e-02, 1.0770e-06,
        4.0760e-02, 1.5757e-06, 1.1043e-06, 5.5594e-06, 8.8809e-07, 4.7182e-02,
        8.8276e-02, 2.2028e-06, 8.2495e-02, 1.6600e-06, 1.1197e-06, 5.2309e-02,
        5.9247e-02, 5.0729e-02, 1.8686e-06, 1.3514e-06, 4.3097e-02, 9.2180e-02,
        1.6243e-07, 1.5757e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.8859e-02, 2.5901e-07, 3.5232e-08, 2.2567e-02, 4.4886e-08, 2.8135e-02,
        7.1168e-08, 1.0341e-07, 3.2565e-02, 3.9238e-02, 6.8280e-08, 4.9715e-02,
        4.0792e-02, 6.0602e-02, 1.8021e-02, 4.5104e-02, 2.2584e-07, 1.4841e-07,
        3.3692e-02, 1.8917e-02, 1.6074e-02, 4.5369e-02, 1.4774e-02, 2.1700e-07,
        1.5402e-07, 3.8826e-02, 7.5854e-02, 2.0997e-02, 9.2789e-02, 1.1523e-02,
        5.6016e-02, 2.0708e-02, 9.6506e-03, 2.1304e-02, 9.9984e-03, 3.8941e-02,
        4.5030e-08, 5.0922e-02, 1.8838e-01, 3.6339e-02, 5.6577e-02, 1.1119e-07,
        8.7740e-08, 1.5402e-07, 4.0134e-02, 4.7664e-02, 4.3798e-02, 4.8141e-03,
        5.5161e-02, 1.0215e-07, 3.1543e-02, 2.4460e-02, 5.7606e-02, 5.1919e-03,
        1.6176e-02, 8.8038e-08, 1.9603e-02, 9.5848e-03, 4.8374e-02, 8.1853e-02,
        1.5803e-02, 5.8462e-02, 3.1517e-02, 2.0723e-02, 1.6198e-02, 6.4159e-08,
        4.1711e-03, 5.6533e-02, 8.4543e-08, 3.4382e-08, 2.5803e-02, 5.2008e-02,
        1.0214e-07, 2.8913e-07, 1.0524e-02, 1.0705e-02, 8.3729e-02, 7.1983e-08,
        4.7674e-02, 3.6725e-02, 5.1152e-02, 1.1770e-07, 6.1039e-02, 4.7185e-02,
        4.5152e-02, 6.7276e-02, 4.6723e-02, 3.7820e-02, 2.6773e-02, 6.9389e-02,
        5.3558e-02, 7.0410e-02, 4.0049e-08, 3.8920e-02, 2.0417e-02, 3.1964e-02,
        1.9858e-02, 5.8320e-02, 2.6767e-02, 1.1149e-07, 3.0831e-02, 4.0571e-02,
        8.3332e-02, 2.3180e-02, 2.2219e-02, 1.5544e-01, 3.6897e-02, 7.6647e-02,
        4.3134e-02, 2.5270e-02, 4.1672e-08, 2.9306e-02, 4.8659e-02, 4.4712e-02,
        6.0792e-08, 3.2374e-02, 2.5313e-02, 1.6967e-02, 8.2547e-08, 1.0432e-07,
        5.1635e-02, 2.5615e-02, 1.1880e-02, 3.8579e-02, 3.4572e-02, 1.2588e-07,
        6.2117e-02, 6.0406e-02, 3.8167e-02, 2.0363e-02, 4.4950e-02, 1.3242e-01,
        1.7623e-02, 1.3449e-02, 1.3787e-02, 4.4409e-02, 1.5974e-02, 6.0546e-08,
        7.7796e-02, 6.1154e-03, 1.4332e-01, 1.9988e-02, 1.2156e-01, 4.6120e-02,
        1.8301e-02, 5.8100e-02, 1.5350e-02, 2.1451e-02, 3.0803e-02, 4.7599e-02,
        4.3784e-02, 5.8606e-02, 9.5105e-02, 1.7961e-01, 2.8118e-02, 5.2148e-02,
        9.8002e-08, 1.5487e-01, 6.5232e-08, 2.2432e-02, 2.2391e-07, 4.0117e-08,
        3.6779e-02, 1.9785e-02, 3.3429e-02, 4.1493e-02, 1.8780e-07, 9.7455e-02,
        9.8769e-03, 5.3829e-02, 4.0126e-08, 1.1888e-07, 6.2059e-03, 4.6232e-02,
        2.9161e-02, 5.2023e-02, 1.1709e-07, 3.8150e-02, 1.2012e-07, 1.8422e-02,
        1.9645e-02, 1.0201e-07, 9.9986e-02, 7.0067e-03, 4.5120e-02, 3.8949e-02,
        5.2184e-02, 2.0267e-02, 2.7967e-02, 4.3659e-02, 9.1260e-03, 8.2971e-02,
        2.8887e-02, 4.5963e-02, 4.4469e-02, 1.6905e-01, 4.0259e-02, 4.8919e-02,
        1.4143e-07, 4.4072e-02, 3.3470e-02, 6.5492e-02, 4.7258e-08, 2.4886e-07,
        7.1848e-08, 5.5403e-02, 8.7999e-08, 3.1493e-02, 1.8022e-02, 7.3349e-02,
        5.7433e-02, 4.7586e-02, 4.8035e-02, 3.6126e-02, 5.9567e-02, 4.0148e-02,
        1.1806e-01, 1.4814e-07, 5.5889e-02, 5.4077e-02, 6.3782e-02, 1.4439e-02,
        5.4068e-02, 5.6015e-08, 1.4420e-02, 2.0052e-02, 1.5166e-02, 3.0601e-02,
        2.5265e-02, 5.4987e-02, 1.4540e-02, 5.4973e-02, 2.7482e-02, 5.3187e-02,
        3.9769e-06, 1.4785e-01, 5.5488e-02, 9.0318e-03, 4.0885e-03, 1.2553e-07,
        5.0514e-02, 1.1867e-01, 3.4992e-02, 7.3812e-08, 1.0447e-02, 1.7603e-01,
        4.8153e-02, 1.5050e-08, 6.9184e-02, 2.0021e-01, 4.8329e-02, 4.5455e-02,
        1.4269e-07, 2.8120e-02, 6.0707e-08, 2.7223e-02, 2.3001e-02, 1.7457e-02,
        8.0303e-03, 7.4732e-08, 4.9375e-08, 5.1535e-02, 3.2798e-02, 3.8739e-02,
        6.2412e-08, 9.5341e-02, 4.4336e-02, 1.6650e-01, 1.4239e-01, 4.2760e-02,
        1.0421e-01, 9.4496e-02, 2.6277e-02, 1.6193e-01, 5.6759e-02, 5.4145e-02,
        4.9384e-02, 5.4562e-02, 3.6280e-02, 1.7873e-02, 6.3416e-08, 2.4884e-07,
        2.5611e-02, 6.0086e-08, 5.7379e-02, 2.7847e-02, 2.4847e-02, 4.9982e-02,
        2.9052e-02, 1.3454e-07, 2.9839e-02, 1.4108e-07, 2.4136e-02, 2.4914e-07,
        4.4930e-02, 5.3212e-02, 1.5681e-02, 3.2403e-02, 6.5863e-02, 2.3748e-02,
        5.4068e-02, 6.6943e-03, 3.8782e-02, 2.5521e-02, 4.3234e-02, 3.5755e-02,
        6.8739e-08, 2.2943e-02, 9.0224e-03, 3.6631e-02, 2.1047e-02, 6.8337e-02,
        5.4437e-02, 2.4999e-02, 1.9124e-01, 5.7149e-02, 1.7777e-02, 2.6446e-02,
        1.3651e-01, 1.4309e-02, 2.2472e-02, 5.9403e-08, 2.5922e-02, 9.8893e-08,
        3.6732e-02, 3.1721e-02, 2.6740e-07, 4.0450e-02, 4.5141e-08, 2.5267e-02,
        5.7049e-02, 1.0197e-02, 3.9112e-02, 1.4876e-07, 9.3485e-08, 4.0321e-08,
        4.7041e-02, 4.3458e-02, 3.6411e-08, 1.0140e-02, 5.9558e-08, 2.8366e-02,
        1.1125e-02, 6.2188e-03, 2.5701e-02, 1.1242e-07, 3.4071e-02, 3.5922e-02,
        2.0754e-02, 4.1265e-02, 2.3708e-02, 6.0125e-02, 5.8100e-02, 4.8201e-02,
        6.3245e-08, 3.0530e-02, 1.4141e-02, 4.6312e-02, 5.1759e-08, 5.2416e-02,
        4.5406e-02, 2.3105e-02, 2.8543e-02, 1.5949e-02, 6.4062e-08, 2.8218e-02,
        4.6720e-02, 2.6122e-02, 2.4954e-07, 4.2071e-02, 9.0401e-03, 3.3744e-02,
        2.6848e-08, 8.6833e-02, 1.0214e-07, 1.6728e-02, 2.3370e-02, 3.3646e-02,
        4.2529e-02, 2.7039e-02, 2.2947e-02, 4.1112e-08, 2.7739e-02, 6.0827e-02,
        2.8183e-02, 5.4044e-02, 1.0214e-07, 1.4149e-07, 1.2060e-01, 8.0885e-02,
        1.0683e-07, 6.8681e-08, 1.3701e-07, 4.8679e-02, 6.5367e-02, 7.1514e-02,
        4.0289e-08, 1.1770e-07, 2.1205e-02, 3.5719e-02, 6.3098e-02, 4.1017e-02,
        5.3780e-02, 2.4373e-02, 2.7678e-02, 4.0642e-03, 2.8265e-02, 1.8588e-02,
        4.2228e-02, 5.4628e-02, 3.8634e-02, 3.5127e-02, 9.3068e-02, 2.2460e-02,
        4.7521e-02, 7.2264e-02, 5.7681e-08, 2.3163e-01, 5.3036e-02, 4.7057e-02,
        4.7017e-02, 7.6395e-02, 5.9371e-08, 5.7483e-02, 4.3430e-02, 1.2443e-07,
        2.0784e-07, 1.8423e-02, 5.6569e-02, 4.3593e-02, 6.3829e-02, 2.1147e-02,
        2.1402e-02, 7.4710e-03, 4.3978e-02, 1.5588e-02, 2.0016e-02, 1.8793e-07,
        1.4785e-07, 4.5198e-02, 1.0829e-07, 4.3745e-02, 1.3188e-01, 5.1504e-02,
        6.3126e-08, 6.2703e-02, 1.6745e-02, 2.0582e-02, 1.4292e-02, 4.1514e-02,
        1.4869e-02, 9.5180e-03, 2.9210e-02, 2.0236e-02, 3.9350e-03, 6.3364e-08,
        3.0611e-02, 3.2613e-02, 3.0085e-02, 8.5428e-03, 1.2597e-02, 1.1645e-07,
        3.3255e-02, 2.0574e-01, 6.6972e-02, 4.3692e-02, 3.2214e-02, 3.0020e-02,
        6.0549e-02, 1.9932e-01, 2.7300e-02, 1.3221e-07, 4.4431e-03, 2.8583e-02,
        2.2800e-02, 2.3222e-02, 2.6327e-07, 3.0668e-02, 7.3289e-08, 5.8192e-02,
        2.6067e-02, 5.6349e-02, 2.7188e-02, 4.0209e-02, 4.0368e-08, 5.0332e-02,
        5.6843e-02, 2.3482e-02, 6.4896e-02, 1.6606e-02, 2.5877e-02, 4.6324e-02,
        1.4293e-01, 1.4139e-07, 1.1808e-07, 5.4162e-02, 1.1888e-07, 2.5384e-02,
        5.5843e-02, 2.9755e-02, 3.5134e-02, 3.4206e-02, 1.5961e-01, 3.2750e-02,
        4.8247e-02, 5.4204e-08, 1.3662e-01, 4.2399e-02, 1.9503e-02, 3.9856e-02,
        8.4716e-08, 4.5225e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.1317e-01, 9.6735e-02, 1.1839e-01, 6.0017e-07, 4.2562e-07, 1.8508e-07,
        9.4137e-02, 1.0102e-01, 1.3511e-07, 4.0707e-08, 9.7931e-08, 3.6440e-07,
        1.1140e-01, 3.6457e-07, 2.8944e-07, 1.0405e-01, 3.8135e-07, 1.1078e-01,
        2.0639e-07, 1.3991e-07, 1.1358e-01, 3.8135e-07, 5.6599e-07, 9.4307e-02,
        9.5792e-02, 1.1345e-01, 8.4668e-02, 4.2046e-07, 3.8135e-07, 1.0483e-01,
        9.6691e-02, 2.8944e-07, 1.2073e-07, 2.7225e-07, 1.2611e-01, 2.8944e-07,
        1.0920e-01, 1.0717e-07, 1.0370e-01, 2.3326e-07, 1.0197e-01, 1.2845e-01,
        1.1774e-01, 1.2414e-01, 1.1500e-01, 1.0783e-01, 1.0717e-07, 1.6892e-07,
        5.6599e-07, 2.4430e-07, 3.8135e-07, 1.8723e-07, 1.0395e-01, 2.1896e-07,
        1.1334e-01, 1.2684e-01, 1.0717e-07, 3.6457e-07, 1.2973e-07, 1.1374e-01,
        1.1784e-01, 8.4985e-02, 1.1281e-01, 1.2779e-01, 2.8944e-07, 8.4864e-02,
        1.3991e-07, 2.4959e-01, 1.2073e-07, 3.8135e-07, 2.2447e-07, 1.1089e-01,
        9.8785e-02, 3.6440e-07, 3.8135e-07, 1.0237e-01, 7.6443e-02, 2.8075e-07,
        1.3991e-07, 2.8944e-07, 2.2447e-07, 3.8135e-07, 1.1005e-01, 3.4423e-07,
        5.6599e-07, 8.1667e-02, 1.1034e-01, 1.8508e-07, 7.6695e-08, 1.0845e-01,
        7.5249e-08, 1.0375e-01, 1.1390e-01, 1.1819e-01, 1.0623e-01, 1.0717e-07,
        1.1372e-01, 2.2447e-07, 1.3991e-07, 1.1817e-01, 1.8508e-07, 1.1129e-01,
        1.1465e-01, 3.6440e-07, 1.0169e-01, 9.1673e-02, 1.3511e-07, 2.1896e-07,
        5.6599e-07, 3.6457e-07, 1.2783e-01, 7.7644e-07, 1.2073e-07, 1.0363e-01,
        1.2073e-07, 1.1612e-01, 1.0292e-01, 6.0015e-07, 5.7293e-07, 5.7526e-07,
        1.1721e-01, 1.1543e-01, 9.5487e-02, 4.3338e-07, 1.1529e-01, 1.1692e-01,
        9.4938e-02, 4.0713e-08, 1.1909e-01, 1.0591e-01, 6.4397e-02, 7.0466e-03,
        1.0717e-07, 1.1851e-01, 4.2046e-07, 9.1256e-02, 1.0605e-01, 1.1215e-01,
        1.1100e-01, 1.9737e-07, 2.6105e-07, 1.0860e-01, 5.6599e-07, 4.0480e-07,
        3.6440e-07, 7.6695e-08, 2.8075e-07, 9.3857e-02, 1.1317e-01, 1.2408e-07,
        2.8944e-07, 1.2400e-01, 8.2352e-02, 4.2046e-07, 2.3326e-07, 8.2533e-02,
        6.0015e-07, 4.0707e-08, 5.6599e-07, 2.2447e-07, 1.0832e-01, 3.5820e-07,
        1.1755e-01, 1.3991e-07, 1.0118e-01, 2.6411e-07, 1.1427e-01, 1.0197e-01,
        8.6274e-02, 7.6695e-08, 1.2073e-07, 1.2073e-07, 3.6440e-07, 1.3000e-01,
        1.0717e-07, 1.1500e-01, 2.1896e-07, 8.1950e-02, 2.2447e-07, 1.1399e-01,
        1.1022e-01, 1.6892e-07, 3.3828e-07, 1.0584e-01, 1.1825e-01, 1.0500e-01,
        1.1108e-01, 5.8606e-07, 2.8944e-07, 1.2138e-01, 1.0368e-01, 4.2046e-07,
        1.1181e-01, 1.0447e-01, 1.8723e-07, 6.2149e-08, 2.2447e-07, 1.0224e-01,
        1.0309e-01, 2.8944e-07, 1.0392e-01, 1.6892e-07, 1.2324e-07, 3.6440e-07,
        1.2073e-07, 7.5249e-08, 1.0717e-07, 9.3819e-02, 5.6599e-07, 2.2447e-07,
        1.0576e-01, 1.1098e-01, 9.3156e-02, 1.0194e-01, 7.5249e-08, 8.4853e-02,
        1.8508e-07, 1.1820e-01, 7.5249e-08, 9.7931e-08, 7.5444e-02, 1.0998e-01,
        7.6695e-08, 2.1896e-07, 5.7538e-07, 1.0717e-07, 1.2324e-07, 9.7913e-02,
        1.1941e-01, 1.1241e-01, 1.0946e-01, 7.6695e-08, 7.7353e-02, 2.2447e-07,
        1.2909e-01, 2.2447e-07, 1.2073e-07, 7.5249e-08, 1.8091e-08, 1.0689e-01,
        1.0161e-01, 1.6892e-07, 1.0999e-01, 2.2447e-07, 1.0717e-07, 7.5249e-08,
        1.2520e-01, 1.6892e-07, 9.3921e-02, 1.0515e-01, 9.9575e-02, 1.0037e-01,
        1.0539e-01, 2.3326e-07, 5.6010e-02, 1.1607e-01], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([8.1430e-07, 4.3229e-07, 6.0549e-07, 9.1035e-02, 1.1305e-06, 9.3423e-02,
        6.7805e-02, 1.6456e-06, 4.9012e-02, 8.1430e-07, 2.0116e-06, 1.6519e-06,
        3.3138e-06, 8.2126e-07, 4.3229e-07, 3.6026e-07, 1.6456e-06, 3.3984e-06,
        2.2833e-06, 9.2155e-02, 9.1817e-02, 1.5087e-06, 8.2126e-07, 8.7680e-02,
        6.0549e-07, 2.0385e-07, 7.5540e-07, 4.3229e-07, 8.4346e-02, 8.8964e-02,
        2.0439e-06, 8.6644e-02, 1.8201e-06, 8.0759e-02, 2.0439e-06, 8.1430e-07,
        1.1305e-06, 3.3029e-06, 1.4162e-06, 2.1658e-06, 2.0384e-07, 1.6456e-06,
        7.5539e-07, 2.2171e-06, 1.2653e-06, 9.0238e-02, 9.4987e-02, 2.8966e-06,
        9.3499e-02, 8.1430e-07, 6.0549e-07, 8.1430e-07, 4.3229e-07, 3.7307e-06,
        1.2680e-06, 1.6456e-06, 7.5540e-07, 8.3776e-02, 8.9785e-07, 8.9174e-02,
        8.2126e-07, 4.3229e-07, 3.6923e-07, 8.0346e-02, 9.4798e-02, 1.5527e-06,
        1.4162e-06, 9.6077e-07, 9.3274e-02, 8.1758e-02, 3.3120e-06, 1.4489e-06,
        2.2833e-06, 2.0385e-07, 2.1658e-06, 9.9290e-07, 1.8201e-06, 1.6456e-06,
        2.0439e-06, 1.8201e-06, 7.1013e-02, 8.8882e-02, 8.7244e-02, 4.3229e-07,
        1.0148e-06, 5.3123e-07, 1.1074e-06, 8.2223e-02, 1.0316e-06, 1.5527e-06,
        8.3789e-02, 1.0271e-01, 9.0930e-07, 1.4489e-06, 7.5540e-07, 4.3229e-07,
        7.9089e-02, 9.2914e-02, 1.0473e-01, 4.3752e-06, 6.0549e-07, 1.6456e-06,
        9.4917e-02, 2.0385e-07, 7.7003e-07, 4.3229e-07, 7.5452e-02, 7.6286e-08,
        4.1801e-07, 7.8097e-02, 8.2445e-02, 1.8747e-06, 9.6077e-07, 1.6936e-06,
        1.2680e-06, 3.3029e-06, 8.1430e-07, 7.5540e-07, 1.6456e-06, 8.1430e-07,
        7.5540e-07, 8.8597e-02, 1.5527e-06, 4.3229e-07, 3.7307e-06, 3.3985e-06,
        7.8939e-02, 8.0917e-02, 8.1430e-07, 6.0549e-07, 2.6102e-06, 1.6456e-06,
        1.4489e-06, 1.5348e-06, 4.3229e-07, 3.3985e-06, 1.2653e-06, 3.3984e-06,
        3.6923e-07, 4.3229e-07, 1.0148e-06, 9.6073e-07, 2.0439e-06, 1.1305e-06,
        8.6188e-02, 8.5239e-02, 1.4489e-06, 1.8131e-06, 2.1658e-06, 8.8873e-02,
        1.6456e-06, 1.0380e-01, 7.5539e-07, 8.1430e-07, 2.2833e-06, 3.7307e-06,
        6.0549e-07, 9.9290e-07, 3.3985e-06, 9.5955e-02, 3.3120e-06, 3.7307e-06,
        1.4162e-06, 2.8712e-06, 8.8368e-02, 8.9777e-07, 7.5540e-07, 5.9017e-02,
        9.3515e-02, 8.3867e-02, 2.7504e-06, 8.7471e-02, 1.6519e-06, 8.2126e-07,
        8.1430e-07, 8.4094e-02, 7.5540e-07, 3.3985e-06, 8.6345e-02, 1.6456e-06,
        6.0549e-07, 8.9468e-02, 7.5539e-07, 7.6646e-02, 7.5540e-07, 2.8705e-06,
        2.3587e-06, 7.5221e-02, 4.3229e-07, 9.2609e-02, 3.3985e-06, 9.0930e-07,
        9.4757e-07, 4.3229e-07, 4.3752e-06, 3.2063e-06, 1.0362e-06, 8.2240e-02,
        1.2680e-06, 7.9697e-02, 1.2653e-06, 1.8747e-06, 2.0439e-06, 6.7609e-07,
        1.4162e-06, 8.4158e-02, 8.6690e-02, 8.1130e-02, 7.8180e-02, 2.1658e-06,
        1.5329e-06, 4.3229e-07, 4.3229e-07, 2.2833e-06, 1.8201e-06, 2.0439e-06,
        1.6519e-06, 1.8201e-06, 8.8461e-02, 1.2680e-06, 3.2063e-06, 8.5899e-02,
        8.0657e-02, 1.2680e-06, 2.2666e-06, 8.9347e-02, 5.4941e-07, 8.4372e-02,
        1.6456e-06, 1.5087e-06, 9.0930e-07, 1.6456e-06, 2.1171e-06, 1.8201e-06,
        3.3985e-06, 2.0116e-06, 1.4489e-06, 2.0439e-06, 9.0930e-07, 1.4162e-06,
        1.6456e-06, 8.5962e-02, 1.4489e-06, 4.3229e-07, 7.5539e-07, 1.5087e-06,
        3.2417e-06, 8.6125e-02, 1.6519e-06, 3.2415e-06, 1.6456e-06, 3.2541e-06,
        1.8201e-06, 1.7909e-06, 2.1658e-06, 2.0385e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.7803e-07, 7.3907e-02, 3.7823e-02,  ..., 6.8933e-02, 5.3006e-02,
        3.1491e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.8001e-07, 2.7089e-02, 1.9379e-07,  ..., 3.3873e-02, 2.8525e-02,
        1.4711e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.2632e-07, 7.6195e-07, 8.3500e-07, 3.8444e-07, 6.9363e-07, 2.9960e-07,
        2.8797e-07, 4.0716e-02, 2.4407e-07, 1.3659e-06, 5.2935e-02, 1.3159e-06,
        9.2486e-07, 3.2495e-07, 4.9332e-07, 4.6784e-02, 5.1661e-02, 1.6789e-06,
        4.3183e-07, 2.6091e-02, 4.5867e-07, 3.7035e-02, 1.4500e-06, 3.7470e-02,
        3.9012e-02, 1.5354e-06, 3.4766e-07, 6.9363e-07, 1.3659e-06, 7.6912e-07,
        8.0445e-07, 2.1158e-06, 1.5125e-06, 4.9626e-02, 2.0753e-06, 3.2495e-07,
        4.3764e-07, 4.6617e-02, 4.2632e-07, 1.2442e-01, 8.0498e-07, 6.9363e-07,
        1.6789e-06, 5.4046e-02, 1.3680e-06, 4.3728e-02, 8.0498e-07, 1.5326e-06,
        4.5867e-07, 7.1571e-07, 3.6325e-02, 4.1734e-02, 1.0733e-06, 4.5867e-07,
        8.9417e-07, 5.7683e-07, 9.6677e-07, 4.6122e-07, 7.4226e-07, 6.5909e-07,
        9.9945e-07, 8.0498e-07, 6.7551e-07, 4.9977e-07, 2.0742e-06, 5.7683e-07,
        7.6195e-07, 7.2246e-07, 6.5610e-07, 4.4170e-02, 1.9617e-06, 3.9897e-02,
        2.1117e-06, 9.8619e-07, 3.5679e-02, 3.8055e-02, 1.1317e-06, 5.0478e-07,
        1.2648e-06, 3.4054e-07, 7.2240e-07, 1.4499e-06, 8.9417e-07, 1.3659e-06,
        5.0478e-07, 4.6122e-07, 1.4677e-01, 6.5610e-07, 1.1602e-06, 1.5143e-07,
        2.9960e-07, 5.7683e-07, 4.5200e-07, 6.5909e-07, 9.9945e-07, 1.6789e-06,
        3.1418e-02, 6.5909e-07, 8.0498e-07, 1.5145e-06, 1.5354e-06, 7.4226e-07,
        1.3802e-06, 6.5610e-07, 7.6912e-07, 6.5610e-07, 2.2887e-02, 7.2246e-07,
        2.1101e-06, 5.7683e-07, 5.7683e-07, 3.7319e-02, 4.0250e-02, 5.2055e-02,
        1.2582e-06, 1.0175e-06, 2.0254e-07, 4.7500e-02, 3.9805e-02, 1.3802e-06,
        3.2224e-02, 8.0498e-07, 1.8155e-06, 1.3659e-06, 4.8076e-02, 4.3415e-07,
        8.0498e-07, 4.6948e-02, 4.8340e-07, 7.6912e-07, 3.8590e-02, 4.2632e-07,
        4.4452e-07, 4.3376e-02, 2.8797e-07, 8.9417e-07, 3.9830e-02, 2.7022e-06,
        1.8155e-06, 5.7683e-07, 6.0765e-07, 1.3802e-06, 5.0923e-07, 3.6568e-07,
        4.6122e-07, 5.0804e-02, 6.5909e-07, 4.8118e-02, 5.0923e-07, 1.4500e-06,
        3.4054e-07, 4.9977e-07, 6.9363e-07, 2.1114e-06, 4.9977e-07, 1.4500e-06,
        1.5354e-06, 5.7683e-07, 2.7490e-07, 7.2240e-07, 6.9363e-07, 6.6321e-07,
        4.6170e-07, 3.8444e-07, 3.8444e-07, 4.5445e-02, 4.1161e-02, 6.2942e-07,
        5.7683e-07, 1.2883e-06, 6.0336e-07, 1.4500e-06, 4.3513e-02, 5.1074e-02,
        7.2246e-07, 8.0498e-07, 1.4499e-06, 3.6568e-07, 4.3415e-07, 4.9977e-07,
        3.5574e-02, 5.7683e-07, 1.3659e-06, 3.5811e-02, 4.2420e-02, 2.0254e-07,
        3.4054e-07, 6.0764e-07, 4.6086e-02, 4.0316e-07, 1.0175e-06, 1.8155e-06,
        1.6789e-06, 1.0174e-06, 4.6170e-07, 6.0765e-07, 2.0641e-06, 5.7683e-07,
        2.0641e-06, 4.4731e-02, 2.0558e-01, 3.1624e-02, 1.8155e-06, 7.4226e-07,
        5.9994e-07, 5.6388e-02, 2.0254e-07, 6.5610e-07, 8.3500e-07, 3.4406e-02,
        2.0254e-07, 3.8444e-07, 1.4500e-06, 2.0753e-06, 9.9681e-07, 8.9417e-07,
        2.1715e-07, 9.9954e-07, 2.0254e-07, 9.9945e-07, 5.7683e-07, 7.2240e-07,
        3.4054e-07, 3.5477e-02, 8.9417e-07, 5.0676e-02, 7.6195e-07, 1.6789e-06,
        7.2240e-07, 1.2932e-01, 1.1150e-01, 5.4920e-02, 8.9417e-07, 5.2395e-02,
        4.2115e-02, 2.0742e-06, 1.0560e-06, 2.2660e-01, 2.0254e-07, 2.7848e-02,
        1.3659e-06, 5.9994e-07, 5.1066e-02, 1.3659e-06, 9.8619e-07, 8.0498e-07,
        2.0254e-07, 7.2246e-07, 7.2246e-07, 4.8127e-02, 4.2632e-07, 5.2929e-02,
        6.5610e-07, 4.8340e-07, 4.9977e-07, 1.6789e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([4.9228e-06, 3.0572e-06, 7.6925e-02, 2.5034e-06, 1.3540e-06, 4.5753e-02,
        7.7046e-07, 6.1036e-02, 1.4987e-06, 6.0532e-02, 5.4419e-07, 7.7046e-07,
        6.8661e-07, 1.6549e-06, 3.8153e-06, 9.2584e-07, 5.4419e-07, 3.7587e-06,
        6.7052e-02, 6.1996e-02, 4.8977e-07, 5.1112e-02, 6.4062e-02, 7.1303e-02,
        2.3866e-06, 7.7046e-07, 7.4155e-07, 3.8153e-06, 4.9228e-06, 3.8153e-06,
        9.2082e-07, 1.3789e-06, 6.8661e-07, 5.8345e-02, 6.5538e-02, 6.8661e-07,
        5.8056e-02, 6.7425e-02, 2.1240e-06, 9.2584e-07, 2.0203e-06, 9.2486e-07,
        5.3290e-02, 3.6538e-06, 4.8732e-06, 6.1270e-02, 5.2383e-02, 7.7046e-07,
        9.2584e-07, 5.5325e-02, 6.8661e-07, 3.8153e-06, 5.9051e-02, 4.9228e-06,
        2.5034e-06, 6.8661e-07, 1.0688e-06, 1.1827e-06, 1.2879e-06, 1.9555e-06,
        2.0203e-06, 5.4003e-02, 7.4037e-02, 2.0203e-06, 9.2584e-07, 4.8732e-06,
        2.3866e-06, 9.2082e-07, 2.3866e-06, 6.0328e-02, 2.0203e-06, 2.1240e-06,
        9.2082e-07, 6.8926e-02, 7.0316e-02, 7.0514e-02, 7.5938e-02, 1.1827e-06,
        2.5427e-06, 6.3751e-02, 2.3155e-07, 4.8076e-02, 9.2082e-07, 3.8153e-06,
        2.5427e-06, 5.4419e-07, 5.4419e-07, 1.6549e-06, 1.0688e-06, 2.2389e-06,
        6.5614e-02, 9.2082e-07, 2.3866e-06, 2.3866e-06, 2.4167e-06, 6.8661e-07,
        1.9445e-06, 9.2738e-02, 2.3866e-06, 6.8661e-07, 9.2584e-07, 6.4740e-02,
        1.7379e-06, 3.0271e-07, 6.8661e-07, 2.3866e-06, 5.4419e-07, 1.6549e-06,
        3.8244e-06, 3.0271e-07, 3.0271e-07, 1.2879e-06, 3.8153e-06, 7.5114e-02,
        2.3866e-06, 2.3866e-06, 1.6549e-06, 6.8661e-07, 1.7379e-06, 5.4419e-07,
        2.0203e-06, 6.9490e-02, 3.0572e-06, 6.2238e-02, 1.1827e-06, 9.3484e-07,
        2.5034e-06, 6.8661e-07, 3.2438e-06, 1.6549e-06, 6.8661e-07, 1.9445e-06,
        7.2602e-02, 2.3155e-07, 6.8939e-02, 2.3866e-06, 4.8732e-06, 1.0674e-06,
        2.3866e-06, 9.2082e-07, 1.1827e-06, 6.3427e-02, 1.2879e-06, 1.2121e-06,
        1.7379e-06, 6.8934e-02, 9.2584e-07, 5.5294e-02, 2.5034e-06, 2.3866e-06,
        7.5038e-02, 6.3889e-02, 6.8661e-07, 7.4986e-02, 7.7046e-07, 3.1503e-06,
        1.1827e-06, 5.4419e-07, 3.6538e-06, 3.0572e-06, 3.0572e-06, 3.8153e-06,
        6.2515e-02, 2.2389e-06, 2.5034e-06, 2.3866e-06, 1.0688e-06, 4.7507e-02,
        8.1097e-02, 3.7586e-06, 5.4419e-07, 1.6549e-06, 9.2584e-07, 3.6539e-06,
        2.3866e-06, 2.3866e-06, 7.3380e-02, 7.7046e-07, 1.6549e-06, 6.8661e-07,
        1.0688e-06, 1.7379e-06, 6.7186e-07, 1.3789e-06, 1.7379e-06, 2.3866e-06,
        3.7586e-06, 5.9613e-02, 1.0688e-06, 1.7379e-06, 9.2584e-07, 1.7379e-06,
        6.2232e-02, 7.4156e-07, 1.9555e-06, 2.1575e-06, 6.3567e-02, 7.0828e-02,
        6.7187e-07, 1.6549e-06, 4.8977e-07, 2.5034e-06, 2.3866e-06, 1.4987e-06,
        1.1827e-06, 6.8661e-07, 7.4891e-02, 6.4000e-02, 6.6323e-02, 2.3866e-06,
        6.2993e-02, 6.8661e-07, 2.0203e-06, 4.2874e-02, 6.6777e-02, 4.9228e-06,
        1.6501e-06, 7.6266e-02, 8.2815e-07, 2.3866e-06, 4.9228e-06, 6.5699e-02,
        3.7586e-06, 7.4155e-07, 7.4156e-07, 5.7227e-02, 8.2815e-07, 1.7379e-06,
        2.3866e-06, 3.0572e-06, 7.0935e-02, 6.8661e-07, 3.8153e-06, 9.2082e-07,
        7.4156e-07, 1.2879e-06, 2.3866e-06, 5.1926e-02, 1.9555e-06, 9.2082e-07,
        7.1079e-02, 5.2390e-02, 1.4987e-06, 2.3866e-06, 1.6549e-06, 2.0203e-06,
        2.1240e-06, 3.8153e-06, 2.0181e-06, 6.1410e-02, 5.9066e-07, 3.8153e-06,
        7.7046e-07, 2.3866e-06, 6.2690e-02, 3.8153e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.4048e-07, 6.9992e-02, 5.5842e-08,  ..., 8.0935e-02, 4.2100e-02,
        7.5202e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([2.7476e-07, 6.7745e-02, 2.5354e-06, 2.7476e-07, 4.2431e-07, 3.9043e-07,
        1.0797e-06, 4.4920e-07, 7.1589e-02, 6.7575e-07, 1.8629e-06, 1.1254e-06,
        1.0797e-06, 4.3215e-07, 5.0971e-02, 1.3573e-06, 1.3573e-06, 5.6935e-02,
        2.1048e-06, 6.4224e-02, 9.2714e-07, 4.9264e-02, 5.8999e-02, 2.9925e-07,
        5.9058e-02, 6.4762e-02, 7.0268e-02, 5.6550e-02, 4.6967e-02, 2.1048e-06,
        6.1323e-02, 5.7112e-02, 4.3190e-07, 6.2050e-02, 7.1372e-07, 2.5356e-06,
        2.8490e-07, 4.3586e-07, 3.5976e-07, 1.0797e-06, 7.5670e-02, 6.1876e-02,
        4.8410e-07, 4.8461e-02, 4.2431e-07, 6.4724e-02, 6.6433e-02, 4.3215e-07,
        1.2202e-06, 1.0797e-06, 4.4920e-07, 5.8709e-02, 3.5226e-07, 1.3573e-06,
        9.1663e-07, 5.1135e-02, 5.6334e-02, 7.1875e-07, 7.3232e-02, 3.9043e-07,
        2.8490e-07, 1.2202e-06, 6.9829e-02, 3.7614e-02, 1.2729e-06, 4.8410e-07,
        6.3301e-02, 9.8741e-07, 6.1709e-02, 8.4129e-07, 6.0251e-02, 1.4093e-01,
        4.3633e-07, 6.0843e-02, 6.5203e-07, 6.1526e-07, 2.1070e-07, 6.0785e-02,
        4.4920e-07, 1.9731e-06, 1.2056e-06, 6.5203e-07, 2.5345e-06, 5.7539e-02,
        9.5290e-07, 6.8566e-02, 2.3707e-06, 4.2431e-07, 1.1248e-01, 1.2729e-06,
        6.2567e-02, 1.3573e-06, 5.4020e-07, 1.9725e-06, 1.4749e-01, 3.5226e-07,
        7.1240e-07, 4.9463e-07, 1.3091e-01, 5.9866e-07, 6.3281e-07, 6.9371e-02,
        2.9925e-07, 1.2202e-06, 1.2202e-06, 7.7450e-07, 6.3272e-02, 6.3281e-07,
        9.8741e-07, 1.7724e-07, 2.8490e-07, 5.3919e-02, 1.3573e-06, 4.2431e-07,
        1.0889e-06, 4.3190e-07, 6.8884e-02, 4.9463e-07, 5.4007e-02, 8.3741e-07,
        1.9725e-06, 3.8840e-07, 6.6059e-02, 5.4636e-02, 7.1240e-07, 4.8410e-07,
        2.8490e-07, 2.3077e-07, 6.7774e-02, 1.0889e-06, 6.2883e-02, 5.8845e-02,
        2.7928e-07, 6.1526e-07, 9.0936e-07, 4.2431e-07, 5.8484e-02, 4.3215e-07,
        2.5344e-06, 5.7253e-02, 5.7222e-02, 6.0264e-02, 8.4129e-07, 1.2056e-06,
        1.0471e-06, 6.5548e-02, 5.6001e-07, 8.4129e-07, 6.7547e-02, 6.0579e-02,
        4.3215e-07, 6.7234e-08, 5.7434e-03, 1.2729e-06, 1.2729e-06, 4.2431e-07,
        1.9039e-07, 4.3586e-07, 4.4920e-07, 7.1240e-07, 6.3362e-02, 6.5203e-07,
        1.2729e-06, 4.3490e-07, 7.4945e-07, 5.5710e-07, 4.3490e-07, 6.4458e-02,
        7.4945e-07, 2.9151e-02, 8.4129e-07, 6.1722e-02, 1.3573e-06, 1.9687e-06,
        1.1344e-06, 9.8741e-07, 5.0599e-02, 2.9925e-07, 9.8741e-07, 9.8741e-07,
        5.5710e-07, 8.9716e-07, 3.9043e-07, 5.9992e-02, 5.3659e-02, 1.3893e-07,
        4.9463e-07, 4.1332e-08, 8.4129e-07, 1.2729e-06, 4.3190e-07, 4.4319e-02,
        5.8681e-07, 8.4129e-07, 6.5203e-07, 9.4813e-07, 6.2402e-02, 2.1048e-06,
        6.6091e-02, 3.5976e-07, 9.5290e-07, 4.2431e-07, 5.2908e-02, 1.0797e-06,
        1.1254e-06, 5.5710e-07, 4.8396e-07, 5.0803e-02, 5.8772e-02, 1.0338e-01,
        5.2175e-02, 6.0629e-02, 1.9471e-06, 5.5710e-07, 6.2670e-02, 7.1846e-02,
        1.9390e-06, 5.5253e-02, 9.5290e-07, 4.9463e-07, 6.8786e-02, 1.9687e-06,
        1.2606e-06, 4.4920e-07, 9.8741e-07, 6.7995e-02, 6.3149e-02, 6.1477e-02,
        6.1004e-07, 6.6845e-02, 1.2729e-06, 1.3573e-06, 1.1366e-07, 2.1048e-06,
        4.0280e-02, 1.9387e-06, 1.9387e-06, 1.3264e-06, 3.2395e-07, 5.4020e-07,
        4.3490e-07, 2.9925e-07, 9.8741e-07, 9.4813e-07, 6.5130e-02, 1.2729e-06,
        1.2606e-06, 5.8026e-02, 9.8741e-07, 6.0087e-02, 1.3885e-07, 6.2824e-02,
        6.0089e-02, 1.9725e-06, 5.4708e-02, 2.1048e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.1864e-06, 1.3371e-06, 1.1784e-06, 7.0492e-02, 7.7452e-06, 1.5873e-06,
        3.2371e-06, 3.1938e-06, 1.0765e-06, 4.3803e-06, 1.9801e-06, 9.8420e-06,
        2.9468e-02, 2.6839e-06, 2.5122e-02, 3.1976e-06, 6.0925e-06, 7.6213e-07,
        2.0659e-06, 3.1060e-02, 7.3106e-02, 1.0772e-06, 1.9762e-02, 7.6213e-07,
        2.4581e-02, 6.7372e-02, 2.8016e-02, 3.2040e-06, 1.5460e-06, 9.7443e-07,
        1.5620e-06, 2.5883e-06, 3.1293e-02, 3.4228e-06, 2.7528e-06, 2.6154e-02,
        4.1789e-06, 3.8283e-06, 1.6347e-06, 5.1389e-06, 3.0315e-06, 7.2954e-02,
        2.5795e-02, 2.8791e-06, 7.8941e-02, 3.0274e-02, 4.1417e-06, 4.1045e-06,
        3.7390e-06, 3.1084e-06, 2.0210e-02, 4.1045e-06, 3.7920e-06, 2.8261e-06,
        1.7031e-06, 6.7615e-06, 2.3998e-06, 1.8937e-02, 3.4059e-06, 2.5944e-06,
        2.5545e-02, 2.0571e-06, 2.7267e-06, 4.7733e-06, 6.0925e-06, 1.6347e-06,
        4.2239e-06, 2.5883e-06, 1.1718e-02, 3.4677e-06, 9.7443e-07, 1.7902e-02,
        9.8922e-07, 1.2300e-06, 4.0877e-06, 1.3371e-06, 3.8158e-02, 2.8261e-06,
        3.4797e-06, 2.6053e-06, 3.1938e-06, 1.4110e-06, 6.9882e-06, 9.7865e-07,
        2.9653e-06, 2.6394e-06, 2.7483e-06, 2.9176e-06, 4.0068e-06, 1.9590e-06,
        1.5468e-06, 3.0040e-02, 3.3994e-06, 2.7127e-02, 6.9483e-06, 2.1311e-06,
        4.4836e-06, 2.4981e-06, 6.9483e-06, 1.6543e-06, 1.8393e-06, 4.9909e-06,
        1.4669e-06, 4.1045e-06, 2.3193e-06, 2.8791e-06, 2.6394e-06, 1.6543e-06,
        1.9801e-06, 8.9498e-06, 2.5480e-06, 3.0491e-02, 6.9882e-06, 1.0066e-06,
        6.9629e-02, 2.8624e-06, 1.9048e-06, 5.3541e-02, 6.8826e-02, 7.8192e-02,
        2.0808e-02, 3.2371e-06, 5.7252e-06, 2.4476e-06, 5.4287e-06, 2.2375e-06,
        4.8354e-06, 1.2943e-06, 2.1516e-02, 7.4542e-02, 2.6839e-06, 2.4190e-02,
        2.8791e-06, 5.7157e-07, 3.1487e-02, 8.6269e-06, 2.3791e-06, 2.7553e-02,
        2.0571e-06, 2.4935e-06, 2.8724e-02, 3.9831e-06, 3.3056e-02, 1.2943e-06,
        3.1084e-06, 2.8734e-02, 4.4024e-06, 2.8791e-06, 7.0848e-02, 1.0283e-06,
        1.1685e-06, 2.5883e-06, 2.0650e-06, 3.7390e-06, 5.5939e-06, 6.6324e-06,
        2.3392e-06, 2.2375e-06, 2.5437e-06, 1.6347e-06, 4.1489e-06, 7.4347e-06,
        4.6706e-02, 3.0600e-06, 3.0798e-06, 2.3976e-02, 2.6839e-06, 4.4248e-06,
        2.4476e-06, 4.3803e-06, 2.6504e-02, 1.3958e-06, 7.6195e-06, 7.5081e-02,
        3.5034e-06, 2.9350e-02, 6.1107e-06, 2.6914e-02, 3.7390e-06, 2.3945e-02,
        2.9991e-02, 2.6178e-06, 1.1487e-06, 1.8393e-06, 5.2563e-06, 4.0068e-06,
        9.7493e-06, 2.3567e-02, 2.3791e-06, 2.6237e-02, 2.2286e-02, 2.3193e-06,
        8.0630e-02, 2.7967e-06, 3.2381e-06, 2.5556e-06, 2.2375e-06, 3.2368e-02,
        7.5813e-02, 2.8186e-06, 8.5844e-02, 3.2792e-06, 2.2052e-06, 6.7615e-06,
        9.7443e-07, 1.8393e-06, 3.1783e-02, 1.1685e-06, 1.1487e-06, 5.7157e-07,
        2.6999e-02, 1.6543e-06, 2.6741e-02, 1.0066e-06, 5.7252e-06, 3.4797e-06,
        2.3133e-06, 7.5636e-07, 3.2680e-02, 1.0765e-06, 2.4935e-06, 4.0877e-06,
        6.1107e-06, 1.6347e-06, 2.7285e-02, 7.6198e-06, 2.7224e-06, 2.4935e-06,
        1.8609e-06, 8.9386e-07, 3.5617e-06, 2.5047e-02, 4.1045e-06, 7.2945e-06,
        2.4499e-02, 8.6287e-07, 3.1516e-02, 2.6826e-06, 2.2256e-06, 1.6543e-06,
        2.0571e-06, 2.3608e-06, 5.2306e-06, 1.2943e-06, 2.0571e-06, 1.7031e-06,
        1.7432e-06, 2.7347e-02, 2.6859e-02, 3.0027e-02, 4.9907e-02, 2.2375e-06,
        2.2435e-02, 4.9909e-06, 7.5636e-07, 3.1882e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.0581e-07, 1.6246e-07, 2.4186e-07,  ..., 1.1558e-01, 3.7889e-07,
        2.5313e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([9.9967e-07, 1.5642e-06, 1.3201e-06, 1.6689e-06, 1.8896e-06, 3.7192e-07,
        8.9126e-07, 2.0932e-06, 5.9301e-07, 2.4658e-06, 2.0864e-06, 9.9830e-07,
        1.6334e-06, 9.4473e-07, 3.2021e-07, 1.5820e-06, 1.7704e-06, 1.2741e-06,
        1.3312e-06, 1.1075e-06, 1.2741e-06, 4.5964e-07, 1.3201e-06, 2.4658e-06,
        2.4144e-06, 1.1878e-06, 7.8216e-07, 2.3282e-06, 1.5129e-06, 2.4144e-06,
        2.4144e-06, 4.5964e-07, 1.3201e-06, 7.0486e-07, 2.9950e-06, 1.0584e-06,
        8.9126e-07, 1.4903e-06, 6.5139e-07, 1.0380e-06, 1.8140e-06, 2.1812e-06,
        7.3980e-07, 3.7192e-07, 2.0362e-06, 1.8896e-06, 6.6553e-07, 1.4349e-06,
        3.4301e-06, 1.5129e-06, 1.1615e-06, 2.1516e-06, 5.9301e-07, 1.5108e-06,
        1.6951e-06, 1.4667e-06, 4.5518e-07, 1.7774e-06, 1.3201e-06, 2.9950e-06,
        1.7172e-06, 1.7937e-06, 1.9557e-06, 1.4862e-06, 2.9137e-06, 1.5063e-06,
        2.5177e-06, 5.1295e-07, 1.3971e-06, 1.3201e-06, 1.2416e-06, 2.0864e-06,
        1.9169e-06, 9.2311e-07, 1.6067e-06, 1.0380e-06, 1.3971e-06, 1.8896e-06,
        6.5617e-07, 1.7063e-06, 1.0210e-06, 3.0121e-06, 1.8140e-06, 1.4641e-06,
        9.1919e-07, 1.6512e-06, 4.6066e-07, 1.1162e-06, 1.8896e-06, 1.2416e-06,
        1.0556e-06, 1.5129e-06, 1.2599e-06, 3.8562e-07, 1.2442e-06, 1.3971e-06,
        1.0210e-06, 9.2301e-07, 1.3201e-06, 1.1187e-06, 7.8612e-07, 7.8612e-07,
        9.9830e-07, 1.5820e-06, 9.9967e-07, 1.1162e-06, 1.0777e-06, 1.7704e-06,
        1.6689e-06, 1.2687e-06, 5.9301e-07, 9.9830e-07, 2.9950e-06, 7.0820e-07,
        2.2842e-06, 1.8896e-06, 1.5642e-06, 8.8991e-07, 1.5108e-06, 1.6689e-06,
        3.7192e-07, 2.9950e-06, 1.6512e-06, 1.3457e-06, 7.8612e-07, 5.9301e-07,
        1.3045e-06, 3.0121e-06, 1.1162e-06, 1.8896e-06, 8.1550e-07, 1.2667e-06,
        1.1380e-06, 5.5840e-07, 1.6334e-06, 1.7704e-06, 3.0121e-06, 6.8312e-07,
        1.9341e-06, 3.0121e-06, 4.1997e-07, 1.5108e-06, 2.1516e-06, 2.1516e-06,
        2.1812e-06, 1.3697e-06, 2.0934e-06, 2.2067e-06, 7.9820e-07, 1.3971e-06,
        4.3399e-07, 3.0121e-06, 5.9301e-07, 2.1516e-06, 2.4144e-06, 1.7874e-06,
        1.4667e-06, 2.1516e-06, 1.8140e-06, 8.2294e-07, 1.2657e-06, 1.2657e-06,
        9.6452e-07, 7.8612e-07, 1.6689e-06, 5.0411e-07, 2.0864e-06, 8.8724e-07,
        1.6689e-06, 3.8054e-07, 1.0556e-06, 2.9950e-06, 1.1948e-06, 7.1375e-07,
        1.9585e-06, 1.5129e-06, 1.1069e-06, 1.6512e-06, 2.9950e-06, 1.0257e-06,
        4.1997e-07, 2.0555e-06, 1.2599e-06, 8.9126e-07, 2.0934e-06, 3.0121e-06,
        1.2741e-06, 8.8724e-07, 2.4144e-06, 6.1296e-07, 1.1162e-06, 3.2280e-06,
        2.9950e-06, 3.0121e-06, 4.9121e-07, 8.2547e-07, 1.8271e-06, 2.2428e-06,
        1.1162e-06, 1.5108e-06, 1.1069e-06, 2.1812e-06, 1.7704e-06, 1.4500e-06,
        1.1868e-06, 8.3396e-07, 2.0404e-06, 1.2333e-06, 2.1812e-06, 9.9967e-07,
        3.0121e-06, 1.1761e-06, 1.1868e-06, 1.1178e-06, 4.1997e-07, 1.0753e-06,
        1.1948e-06, 1.2154e-06, 8.9126e-07, 3.5258e-07, 4.1997e-07, 1.1615e-06,
        1.9341e-06, 1.5005e-06, 4.5964e-07, 1.7172e-06, 2.4144e-06, 1.0713e-06,
        1.3971e-06, 4.5964e-07, 9.3210e-07, 1.6689e-06, 1.6136e-06, 2.7541e-06,
        1.7874e-06, 1.8502e-06, 2.3282e-06, 1.1300e-06, 3.5258e-07, 1.3201e-06,
        5.9301e-07, 1.7704e-06, 1.6334e-06, 1.5063e-06, 6.2963e-07, 7.8311e-07,
        2.2604e-06, 1.3971e-06, 8.3396e-07, 5.9301e-07, 3.0121e-06, 2.1483e-06,
        1.1868e-06, 1.3201e-06, 3.1128e-06, 1.2687e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([1.7881e-06, 6.9036e-06, 5.5855e-06, 4.0089e-06, 4.7361e-06, 3.3611e-06,
        4.4339e-06, 5.1302e-06, 2.7430e-06, 5.5298e-06, 5.2293e-06, 8.1632e-06,
        4.0749e-06, 2.8017e-06, 4.4502e-06, 1.1422e-06, 3.4130e-06, 2.6777e-06,
        3.7567e-06, 2.5668e-06, 3.4212e-06, 6.3517e-06, 6.3415e-06, 3.5640e-06,
        4.7809e-06, 1.8451e-06, 2.5330e-06, 4.7326e-06, 1.4459e-06, 5.3015e-06,
        3.7024e-06, 2.2265e-06, 3.7810e-06, 5.6277e-06, 3.6268e-06, 2.8430e-06,
        1.7881e-06, 4.6161e-06, 7.0223e-06, 2.5436e-06, 1.0945e-05, 1.2102e-06,
        3.3611e-06, 2.6875e-06, 2.5668e-06, 3.3570e-06, 1.1242e-06, 4.7500e-06,
        6.3993e-06, 3.7651e-06, 6.5122e-06, 4.2613e-07, 2.0850e-06, 1.0373e-06,
        3.5942e-06, 3.7810e-06, 2.3667e-06, 6.8896e-06, 7.5091e-06, 1.5528e-06,
        8.2954e-06, 1.5753e-06, 3.3752e-06, 4.2439e-06, 3.4130e-06, 7.0458e-06,
        4.6920e-06, 7.5254e-06, 6.8950e-06, 2.9767e-06, 6.5116e-06, 7.0458e-06,
        1.9451e-06, 5.7118e-06, 2.6394e-06, 3.1068e-07, 3.3648e-06, 3.9984e-06,
        1.5838e-06, 3.5024e-06, 4.4138e-06, 1.1242e-06, 3.7658e-06, 1.5528e-06,
        3.1623e-06, 3.3577e-06, 3.7914e-06, 7.1883e-06, 2.5447e-06, 4.7748e-06,
        8.6250e-06, 5.6625e-06, 1.9101e-06, 1.6446e-06, 4.2728e-06, 1.9101e-06,
        2.5657e-06, 1.7038e-06, 8.4042e-06, 3.8604e-06, 4.1320e-06, 6.3697e-06,
        5.6988e-06, 6.8245e-06, 1.4733e-06, 7.2896e-06, 4.2439e-06, 1.5528e-06,
        2.2739e-06, 4.9821e-06, 3.3611e-06, 2.9732e-06, 7.1883e-06, 5.1302e-06,
        8.6985e-06, 2.5447e-06, 1.6035e-06, 4.6815e-06, 4.6161e-06, 3.2053e-06,
        5.0833e-06, 1.8669e-06, 4.6971e-06, 2.5463e-06, 3.7658e-06, 4.6161e-06,
        4.7128e-06, 5.0307e-06, 8.1425e-06, 3.6875e-06, 4.7361e-06, 8.1584e-06,
        2.9767e-06, 1.9930e-06, 4.4115e-06, 2.5290e-06, 2.8253e-06, 2.9767e-06,
        4.9261e-06, 3.9984e-06, 4.1048e-06, 4.1687e-06, 8.9705e-06, 2.9473e-06,
        2.8533e-06, 3.1854e-06, 3.1623e-06, 1.9101e-06, 2.5846e-06, 8.6893e-06,
        3.5024e-06, 4.9213e-06, 1.9479e-06, 1.8815e-06, 3.4130e-06, 4.9142e-06,
        3.9392e-06, 1.9451e-06, 5.6625e-06, 4.2613e-07, 2.7852e-06, 1.4120e-06,
        3.6819e-06, 1.6035e-06, 3.7914e-06, 3.9984e-06, 6.8245e-06, 1.9451e-06,
        4.4115e-06, 5.0728e-06, 1.6191e-06, 3.7658e-06, 3.6875e-06, 4.3402e-06,
        2.5657e-06, 3.2175e-06, 6.8950e-06, 7.8624e-06, 2.8318e-06, 4.3106e-06,
        8.2469e-06, 3.8507e-06, 5.0937e-06, 5.7118e-06, 4.4707e-06, 1.1422e-06,
        2.9155e-06, 2.9436e-07, 2.5330e-06, 2.5436e-06, 3.9700e-06, 3.8604e-06,
        4.2439e-06, 4.1673e-06, 4.7841e-06, 3.1133e-06, 4.2728e-06, 4.0330e-06,
        1.1734e-06, 1.3332e-06, 6.2205e-06, 3.8604e-06, 2.8533e-06, 2.8792e-06,
        1.0100e-06, 6.3269e-06, 2.0335e-06, 1.7881e-06, 2.4209e-06, 2.6394e-06,
        3.5640e-06, 4.1504e-06, 1.7881e-06, 2.1602e-06, 2.3552e-06, 1.3457e-06,
        7.9995e-06, 3.0194e-06, 5.5528e-06, 9.0399e-07, 4.7748e-06, 6.7924e-06,
        4.6161e-06, 2.4209e-06, 5.5607e-06, 3.9984e-06, 1.8366e-06, 1.7678e-06,
        2.2098e-06, 4.3177e-06, 2.6394e-06, 6.1369e-06, 2.0058e-06, 1.8366e-06,
        2.7564e-06, 4.3134e-06, 3.9984e-06, 1.6768e-06, 4.3377e-06, 6.2190e-06,
        1.9451e-06, 2.8430e-06, 1.8451e-06, 4.1447e-06, 6.9669e-06, 6.5116e-06,
        5.0292e-06, 2.2007e-06, 2.2670e-07, 9.4523e-06, 3.4212e-06, 4.6920e-06,
        7.2982e-06, 1.5528e-06, 3.5392e-06, 3.6359e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.3588e-07, 2.3090e-07, 2.9525e-07,  ..., 1.6987e-07, 2.1527e-07,
        2.3962e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5656e-06, 2.0594e-07, 1.4735e-06, 4.5415e-07, 8.7697e-07, 7.0566e-07,
        1.9301e-06, 1.7038e-06, 7.4431e-07, 7.3738e-07, 9.4440e-07, 1.9555e-06,
        2.8163e-07, 6.6784e-07, 9.5597e-07, 2.7152e-06, 1.5868e-06, 2.7003e-07,
        3.4985e-06, 1.0639e-06, 6.5204e-07, 1.8668e-06, 1.7554e-06, 3.5617e-07,
        7.0566e-07, 9.2397e-07, 6.2375e-07, 1.8882e-06, 4.6154e-07, 5.8392e-07,
        6.7432e-07, 5.4234e-07, 1.7038e-06, 5.4852e-07, 7.3738e-07, 9.2583e-07,
        1.3603e-01, 9.4440e-07, 3.1761e-07, 6.2984e-07, 7.4431e-07, 7.3738e-07,
        1.8882e-06, 1.8668e-06, 8.7697e-07, 6.5204e-07, 9.8028e-07, 1.9301e-06,
        1.8000e-06, 1.9555e-06, 1.7004e-06, 6.6784e-07, 1.2005e-06, 2.3682e-06,
        2.4085e-06, 1.0837e-06, 2.9014e-06, 3.0090e-07, 2.8803e-06, 2.2746e-06,
        1.9303e-06, 5.6367e-07, 1.5778e-06, 6.6795e-07, 7.3738e-07, 1.9555e-06,
        1.8882e-06, 1.6394e-06, 8.9457e-07, 1.0639e-06, 2.4067e-06, 4.2089e-07,
        1.6324e-06, 2.5580e-06, 9.5598e-07, 2.0940e-06, 1.4735e-06, 1.5897e-06,
        1.3986e-06, 1.9555e-06, 3.4763e-07, 8.1063e-07, 1.3956e-06, 3.5964e-02,
        7.9639e-07, 3.3228e-07, 1.5868e-06, 1.9567e-06, 1.7038e-06, 4.2686e-07,
        1.6394e-06, 1.8882e-06, 1.0639e-06, 1.7554e-06, 3.4763e-07, 7.7103e-07,
        1.5897e-06, 1.7554e-06, 6.2375e-07, 7.3165e-07, 7.4431e-07, 1.0587e-06,
        8.8480e-07, 1.5388e-06, 1.7038e-06, 3.0231e-07, 6.0751e-07, 1.7554e-06,
        1.5897e-06, 1.5388e-06, 1.9555e-06, 1.0287e-06, 4.2089e-07, 1.0587e-06,
        2.5721e-06, 9.2332e-07, 8.1063e-07, 7.0645e-07, 6.6784e-07, 6.5204e-07,
        5.6339e-07, 1.9301e-06, 1.5868e-06, 1.3956e-06, 6.2375e-07, 2.9014e-06,
        7.7103e-07, 6.6784e-07, 9.5157e-07, 1.4251e-06, 7.3738e-07, 2.3682e-06,
        7.3165e-07, 2.5721e-06, 8.1063e-07, 1.0639e-06, 1.5868e-06, 6.9088e-07,
        7.0566e-07, 1.2204e-06, 9.4440e-07, 1.9555e-06, 1.2700e-06, 1.1030e-06,
        1.2005e-06, 1.7691e-06, 2.8788e-06, 1.9555e-06, 7.0645e-07, 5.4853e-07,
        8.8744e-07, 1.2005e-06, 6.6795e-07, 1.2731e-06, 3.5843e-07, 1.6587e-06,
        7.9385e-07, 8.4696e-07, 1.9301e-06, 3.3925e-06, 1.0837e-06, 1.5778e-06,
        1.8768e-06, 1.8908e-06, 1.2204e-06, 1.1074e-06, 1.7554e-06, 1.9301e-06,
        1.5868e-06, 1.3976e-06, 1.2264e-06, 9.4440e-07, 1.2943e-06, 5.1580e-07,
        7.0566e-07, 2.5580e-06, 6.3013e-07, 3.5926e-07, 1.6394e-06, 3.3228e-07,
        1.6394e-06, 1.6394e-06, 1.1011e-06, 9.7920e-07, 1.7554e-06, 2.9014e-06,
        6.6795e-07, 7.0645e-07, 6.4129e-07, 3.3890e-07, 1.2204e-06, 1.7038e-06,
        1.1256e-06, 2.5580e-06, 7.0566e-07, 1.9555e-06, 8.1063e-07, 1.9301e-06,
        7.6079e-07, 5.5979e-07, 1.5004e-01, 5.5979e-07, 6.6784e-07, 2.3682e-06,
        1.3986e-06, 6.6634e-07, 8.5015e-07, 8.7404e-07, 2.6700e-06, 9.5597e-07,
        5.5979e-07, 1.3072e-06, 2.3826e-06, 4.8206e-07, 5.2565e-07, 1.6394e-06,
        2.7212e-06, 2.8052e-06, 2.8163e-07, 2.6358e-06, 2.0940e-06, 1.0477e-06,
        1.6587e-06, 5.4852e-07, 4.2089e-07, 1.5868e-06, 8.8744e-07, 1.6155e-06,
        6.2375e-07, 8.7404e-07, 1.2943e-06, 9.9407e-07, 1.6755e-06, 6.0915e-07,
        1.8000e-06, 8.2600e-07, 2.2746e-06, 7.3738e-07, 1.0287e-06, 1.2731e-06,
        1.3986e-06, 1.5656e-06, 1.5897e-06, 7.0645e-07, 1.6587e-06, 2.8545e-06,
        7.0566e-07, 8.5900e-07, 8.8744e-07, 3.4763e-07, 8.2600e-07, 1.0636e-06,
        1.0837e-06, 1.8378e-06, 9.4440e-07, 2.1507e-07], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.3610e-06, 1.6785e-06, 4.1301e-06, 3.9121e-06, 2.2648e-06, 1.5113e-06,
        3.4029e-06, 7.2522e-06, 2.9545e-06, 4.4178e-06, 4.1540e-06, 1.1646e-06,
        3.9600e-06, 5.2279e-06, 6.0761e-06, 2.4323e-06, 5.0691e-06, 6.4436e-06,
        3.1686e-06, 3.2937e-06, 2.4323e-06, 3.1755e-06, 3.8956e-06, 4.2679e-06,
        4.1301e-06, 3.4476e-06, 2.1586e-06, 1.2228e-06, 2.8022e-06, 4.0658e-06,
        5.8009e-06, 1.2549e-06, 1.2228e-06, 3.4476e-06, 3.0157e-06, 2.1554e-06,
        3.9338e-06, 4.1301e-06, 5.6902e-07, 2.5506e-06, 3.9600e-06, 3.7145e-07,
        2.8021e-06, 4.1301e-06, 1.0006e-06, 4.0804e-06, 3.4715e-06, 5.3667e-06,
        2.2661e-06, 3.9600e-06, 4.9614e-06, 2.5732e-06, 1.9035e-06, 7.8924e-07,
        5.3667e-06, 2.6238e-06, 4.0804e-06, 4.3550e-06, 3.9834e-06, 2.6673e-06,
        4.9614e-06, 2.3855e-06, 3.5408e-06, 2.6682e-06, 3.1697e-06, 2.8097e-06,
        1.6251e-06, 2.6682e-06, 4.1992e-06, 2.8623e-06, 7.5912e-07, 2.0451e-06,
        1.0006e-06, 2.8112e-06, 7.5210e-07, 1.0068e-06, 5.6904e-06, 1.6251e-06,
        9.6863e-07, 1.4045e-06, 1.6447e-06, 6.9467e-06, 2.6238e-06, 3.5583e-06,
        9.6892e-06, 9.6892e-06, 3.2187e-06, 5.1975e-06, 5.6904e-06, 2.3927e-06,
        3.9600e-06, 4.2679e-06, 2.3927e-06, 2.2761e-06, 3.4029e-06, 4.1301e-06,
        4.1334e-06, 6.9467e-06, 3.4476e-06, 3.4715e-06, 1.6932e-06, 3.5860e-06,
        1.6932e-06, 1.0704e-06, 4.9614e-06, 2.8465e-06, 7.2522e-06, 1.6251e-06,
        2.5506e-06, 1.6447e-06, 1.2228e-06, 2.6612e-06, 5.1075e-06, 4.2393e-06,
        2.6673e-06, 7.3881e-06, 2.5732e-06, 1.5343e-06, 6.9070e-06, 3.4476e-06,
        1.7841e-06, 4.7919e-06, 3.1567e-06, 6.9070e-06, 6.9467e-06, 6.7211e-06,
        3.6649e-06, 3.5408e-06, 2.5279e-06, 2.5732e-06, 4.1540e-06, 1.6932e-06,
        2.4111e-06, 2.3295e-06, 1.4728e-06, 4.8231e-06, 2.8623e-06, 4.3992e-06,
        1.1815e-01, 3.6056e-06, 2.8623e-06, 3.3432e-06, 1.4584e-06, 1.8824e-06,
        4.9171e-06, 5.1075e-06, 2.0451e-06, 3.9563e-06, 5.6902e-07, 5.9573e-06,
        2.8097e-06, 3.5408e-06, 1.6251e-06, 9.6892e-06, 5.5621e-06, 9.6863e-07,
        5.6904e-06, 3.4476e-06, 5.1697e-06, 6.4656e-06, 3.2187e-06, 1.0006e-06,
        9.6892e-06, 1.3937e-06, 9.6892e-06, 5.4345e-06, 3.9600e-06, 5.8009e-06,
        7.5982e-06, 2.8623e-06, 3.0424e-06, 1.9348e-06, 1.6447e-06, 1.5185e-06,
        2.0451e-06, 2.9933e-06, 1.0714e-06, 2.3607e-06, 1.2228e-06, 4.1181e-06,
        2.7366e-06, 5.4345e-06, 1.7642e-06, 6.5938e-06, 4.8231e-06, 8.9364e-07,
        2.4111e-06, 1.5185e-06, 1.6932e-06, 1.0006e-06, 2.7808e-06, 2.8021e-06,
        1.5364e-06, 5.2199e-06, 3.2950e-06, 4.4577e-06, 1.8905e-06, 7.5210e-07,
        1.0006e-06, 9.6863e-07, 3.2066e-06, 3.4864e-06, 3.1697e-06, 4.4585e-06,
        2.2718e-06, 1.2943e-06, 5.6216e-06, 1.1618e-06, 2.4323e-06, 3.1697e-06,
        4.7404e-06, 1.4703e-06, 2.8097e-06, 4.3467e-06, 9.6892e-06, 2.5271e-06,
        5.1075e-06, 4.1081e-06, 4.9107e-07, 1.1618e-06, 9.6863e-07, 2.6682e-06,
        2.8458e-06, 8.5774e-07, 2.5506e-06, 1.5661e-06, 5.6904e-06, 5.3667e-06,
        2.8419e-06, 9.3511e-06, 4.0804e-06, 5.0607e-06, 9.8309e-07, 1.4345e-06,
        3.1788e-06, 1.2549e-06, 1.9958e-06, 8.5774e-07, 2.8458e-06, 2.7688e-06,
        2.3055e-06, 2.6238e-06, 1.1925e-01, 2.0127e-06, 3.6056e-06, 3.4829e-06,
        2.8097e-06, 5.0239e-06, 1.5364e-06, 3.5408e-06, 1.0294e-06, 4.7404e-06,
        3.7157e-06, 2.6334e-06, 4.1334e-06, 6.9070e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.5382e-07, 1.6903e-07, 5.1866e-07,  ..., 1.2460e-07, 1.2791e-07,
        3.1741e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.8224e-07, 8.7145e-07, 1.5795e-06, 1.4815e-06, 3.4991e-07, 6.8224e-07,
        1.0773e-06, 1.4815e-06, 8.9965e-07, 8.7556e-07, 1.1416e-06, 1.1157e-06,
        1.7981e-07, 1.3092e-06, 9.1722e-07, 1.5881e-06, 6.3798e-07, 1.6827e-06,
        1.0734e-06, 1.8160e-02, 1.9297e-06, 1.4861e-06, 5.2962e-07, 3.6269e-07,
        1.8007e-07, 8.2219e-07, 2.7982e-07, 2.4929e-06, 1.2811e-06, 1.7983e-06,
        8.3103e-07, 8.4423e-07, 5.7746e-07, 4.0653e-07, 1.0773e-06, 8.0686e-07,
        2.4929e-06, 9.7147e-07, 2.3734e-06, 9.7756e-07, 4.7198e-07, 2.2276e-06,
        8.3103e-07, 7.5116e-07, 1.5795e-06, 1.5736e-06, 4.0469e-07, 5.1825e-07,
        7.5116e-07, 2.0301e-07, 2.0301e-07, 2.1086e-07, 8.6158e-07, 1.5794e-06,
        2.4769e-06, 1.4535e-06, 2.7508e-07, 1.6722e-06, 5.2915e-07, 1.3229e-06,
        1.8969e-06, 9.0485e-07, 5.1544e-07, 1.5795e-06, 1.5881e-06, 1.2388e-06,
        4.0653e-07, 1.6804e-06, 1.5881e-06, 1.5881e-06, 8.0831e-07, 1.2997e-06,
        8.3103e-07, 1.1068e-06, 9.9811e-07, 1.8032e-06, 1.6722e-06, 9.7147e-07,
        9.5794e-07, 1.1416e-06, 7.6066e-07, 6.1190e-07, 2.3734e-06, 4.4194e-07,
        1.4815e-06, 7.5116e-07, 8.6158e-07, 7.6066e-07, 9.7147e-07, 9.9999e-07,
        1.0266e-06, 1.0571e-06, 8.4423e-07, 7.5116e-07, 1.4861e-06, 1.5881e-06,
        4.4194e-07, 2.2276e-06, 3.4617e-07, 4.9625e-07, 5.2915e-07, 2.6101e-07,
        4.5321e-07, 1.4815e-06, 1.0485e-01, 5.2915e-07, 8.5157e-07, 4.4194e-07,
        1.5377e-06, 1.8746e-08, 1.0266e-06, 2.3517e-06, 1.7286e-06, 4.0653e-07,
        1.6062e-06, 6.6230e-07, 6.7736e-07, 9.9811e-07, 1.5795e-06, 6.2604e-07,
        3.8945e-07, 5.2962e-07, 1.0734e-06, 6.0503e-07, 9.4185e-07, 1.5194e-06,
        6.6364e-07, 1.3629e-06, 1.7286e-06, 1.5881e-06, 4.5321e-07, 1.6769e-06,
        1.0734e-06, 2.7456e-07, 1.4338e-06, 5.2915e-07, 1.6601e-06, 1.0571e-06,
        1.2154e-06, 5.2983e-07, 5.2915e-07, 7.5116e-07, 4.9661e-07, 1.2997e-06,
        1.6722e-06, 9.7147e-07, 6.8636e-07, 1.5881e-06, 2.8749e-07, 1.2832e-06,
        2.2276e-06, 1.7286e-06, 6.9378e-07, 1.5881e-06, 2.0301e-07, 6.8636e-07,
        3.1012e-07, 5.1228e-07, 1.8747e-08, 8.3103e-07, 9.9813e-07, 2.3734e-06,
        5.2914e-07, 1.6090e-06, 1.1416e-06, 5.2915e-07, 9.2535e-07, 7.6066e-07,
        4.0653e-07, 1.7383e-01, 1.5881e-06, 8.3103e-07, 1.5736e-06, 1.8246e-06,
        7.3958e-07, 8.8099e-07, 5.7746e-07, 1.1680e-06, 8.4423e-07, 1.4815e-06,
        9.7147e-07, 8.3738e-07, 7.5116e-07, 8.7765e-07, 2.0301e-07, 6.7736e-07,
        2.8749e-07, 9.3733e-07, 1.5881e-06, 8.8099e-07, 8.3103e-07, 3.4617e-07,
        1.0571e-06, 1.0152e-06, 6.0271e-07, 3.6269e-07, 4.3521e-07, 1.7417e-06,
        1.0751e-06, 1.5881e-06, 2.8169e-07, 8.0686e-07, 1.2759e-06, 1.4861e-06,
        4.7394e-07, 6.3798e-07, 1.8969e-06, 8.3103e-07, 6.3435e-07, 1.7284e-06,
        3.4991e-07, 1.0571e-06, 1.4815e-06, 1.8246e-06, 9.9811e-07, 1.6722e-06,
        1.4861e-06, 4.9292e-07, 1.8246e-06, 1.6065e-06, 6.0271e-07, 2.3734e-06,
        2.6101e-07, 6.9378e-07, 5.5619e-07, 9.9811e-07, 5.2914e-07, 1.7417e-06,
        4.5004e-07, 1.9388e-05, 1.0152e-06, 1.6090e-06, 6.6230e-07, 2.4171e-06,
        1.8246e-06, 6.6506e-07, 3.4617e-07, 5.2915e-07, 4.7394e-07, 6.9378e-07,
        5.2915e-07, 1.6390e-06, 1.0840e-06, 6.4531e-07, 1.0734e-06, 1.1256e-06,
        7.5116e-07, 5.1544e-07, 5.2962e-07, 1.2154e-06, 1.5795e-06, 7.7524e-07,
        7.7524e-07, 1.5859e-06, 8.3103e-07, 1.6722e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([2.0447e-06, 3.6531e-06, 3.5953e-06, 1.0227e-05, 3.3191e-06, 1.9975e-06,
        1.4338e-06, 1.4630e-06, 4.5968e-07, 3.8274e-06, 1.3445e-06, 9.3484e-06,
        1.0168e-06, 2.7987e-06, 1.0204e-06, 7.2914e-06, 2.7987e-06, 4.5872e-06,
        5.8266e-06, 3.9502e-06, 1.7116e-06, 3.0020e-06, 3.5953e-06, 6.8636e-06,
        4.5565e-06, 7.8316e-06, 2.4549e-06, 4.8099e-06, 3.8814e-06, 3.9502e-06,
        3.5422e-06, 2.8863e-06, 1.5453e-06, 1.7756e-06, 5.8266e-06, 2.9459e-06,
        1.9821e-06, 2.3332e-06, 6.8636e-06, 4.1522e-06, 4.1702e-06, 2.4360e-06,
        2.9459e-06, 2.5209e-06, 2.1487e-06, 4.5118e-06, 1.8780e-06, 2.9459e-06,
        3.0020e-06, 3.9502e-06, 2.4376e-06, 1.7756e-06, 1.0044e-05, 2.4549e-06,
        2.3332e-06, 2.7627e-06, 3.8814e-06, 1.7942e-06, 2.8714e-06, 3.7525e-06,
        1.5040e-06, 1.0168e-06, 1.5826e-06, 9.0747e-02, 2.6671e-06, 7.1321e-07,
        2.1446e-06, 5.8266e-06, 1.6307e-06, 5.9796e-06, 1.5453e-06, 3.3277e-06,
        3.0917e-06, 4.2118e-06, 1.0168e-06, 1.4630e-06, 1.9975e-06, 4.6734e-06,
        2.2924e-06, 4.1114e-06, 4.3670e-07, 1.2637e-06, 3.9352e-07, 3.0377e-06,
        5.9796e-06, 1.5858e-06, 2.9392e-06, 1.9975e-06, 3.0020e-06, 1.9975e-06,
        2.9459e-06, 8.3016e-06, 3.3081e-06, 2.6784e-07, 2.5799e-06, 3.5953e-06,
        7.2496e-06, 2.2832e-06, 4.4890e-06, 3.5000e-06, 9.4323e-07, 1.7116e-06,
        1.7048e-06, 7.2496e-06, 5.4722e-06, 3.1642e-06, 6.0775e-06, 1.3373e-06,
        3.0020e-06, 3.1340e-06, 1.8137e-06, 7.2496e-06, 4.0554e-06, 2.9763e-06,
        7.2496e-06, 1.9975e-06, 5.1598e-06, 8.9008e-07, 2.5089e-06, 1.0168e-06,
        5.4656e-06, 7.2496e-06, 7.2496e-06, 7.2914e-06, 6.8636e-06, 1.5175e-06,
        3.9502e-06, 3.7175e-06, 7.2914e-06, 2.8452e-06, 2.6671e-06, 3.2420e-06,
        5.8266e-06, 4.6733e-06, 2.5089e-06, 1.2178e-06, 2.3848e-06, 3.7589e-06,
        4.5838e-06, 2.7988e-06, 6.5490e-06, 9.3184e-06, 7.2914e-06, 3.0609e-06,
        3.9502e-06, 1.3328e-06, 4.1096e-06, 2.1446e-06, 2.7476e-06, 2.8863e-06,
        1.0179e-06, 6.8636e-06, 2.9459e-06, 2.8452e-06, 3.8274e-06, 3.7589e-06,
        5.8266e-06, 3.1799e-06, 4.6734e-06, 4.5838e-06, 2.6991e-06, 2.2924e-06,
        1.0890e-01, 3.9502e-06, 3.7525e-06, 2.5089e-06, 5.8266e-06, 5.2238e-06,
        2.5490e-06, 2.7777e-06, 2.5089e-06, 1.6091e-06, 4.5838e-06, 3.9502e-06,
        3.4311e-06, 2.6991e-06, 5.2595e-06, 3.7146e-06, 1.9821e-06, 2.6784e-07,
        3.0020e-06, 3.9502e-06, 1.0044e-05, 1.5294e-06, 1.7756e-06, 2.7987e-06,
        1.7116e-06, 3.0377e-06, 4.2118e-06, 5.9053e-07, 4.6123e-06, 3.5422e-06,
        4.2344e-06, 6.8636e-06, 5.0546e-06, 2.5999e-06, 1.9821e-06, 2.6671e-06,
        2.3568e-06, 4.5838e-06, 2.5490e-06, 3.1340e-06, 2.6785e-07, 4.5872e-06,
        7.2914e-06, 2.4360e-06, 1.6307e-06, 1.5826e-06, 2.2154e-06, 1.4427e-06,
        2.7529e-06, 4.6123e-06, 1.3195e-06, 2.6922e-06, 1.4556e-06, 1.1995e-06,
        1.9975e-06, 3.0377e-06, 3.9502e-06, 2.6731e-06, 3.0377e-06, 3.5953e-06,
        3.6064e-06, 2.6645e-06, 9.3158e-07, 5.4656e-06, 1.5748e-06, 2.4376e-06,
        2.9354e-06, 3.6050e-06, 3.7091e-06, 5.5214e-06, 2.5752e-06, 5.9796e-06,
        2.6345e-06, 1.1214e-06, 2.9354e-06, 2.7988e-06, 3.0903e-06, 6.8636e-06,
        1.6307e-06, 5.1598e-06, 2.2958e-06, 7.2496e-06, 1.7607e-06, 3.1340e-06,
        7.2496e-06, 1.9975e-06, 4.4043e-06, 2.3080e-06, 7.0569e-06, 1.7116e-06,
        6.8636e-06, 1.8884e-06, 1.5748e-06, 2.2154e-06], device='cuda:0',
       grad_fn=<NormBackward1>)

max weight is  tensor([3.2365e-07, 1.3205e-07, 3.7864e-07,  ..., 3.4285e-07, 1.5636e-07,
        6.5477e-08], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.6460e-07, 4.3523e-07, 1.0923e-06, 4.3289e-07, 2.0092e-06, 4.9005e-07,
        9.5982e-07, 3.7229e-07, 1.3408e-07, 1.0923e-06, 2.0092e-06, 1.1919e-06,
        4.3289e-07, 5.7445e-07, 9.5265e-07, 8.2454e-07, 5.7445e-07, 3.2596e-07,
        7.9447e-07, 1.7727e-06, 5.4566e-07, 2.5825e-07, 1.8067e-06, 2.0092e-06,
        3.9768e-07, 1.7847e-06, 6.5624e-07, 2.2898e-07, 7.7969e-07, 8.5711e-09,
        9.5982e-07, 9.5265e-07, 8.3103e-07, 3.7229e-07, 5.5479e-07, 8.5639e-09,
        4.3289e-07, 2.0092e-06, 4.6051e-07, 3.6591e-07, 1.1919e-06, 8.2454e-07,
        6.9350e-07, 1.7216e-06, 4.9356e-07, 2.9411e-07, 8.3102e-07, 1.1919e-06,
        5.7445e-07, 9.5265e-07, 1.3301e-06, 3.2894e-07, 3.7229e-07, 4.6051e-07,
        1.8261e-06, 8.2454e-07, 5.7445e-07, 3.7229e-07, 4.9356e-07, 3.7229e-07,
        9.5265e-07, 1.7727e-06, 5.7445e-07, 8.5711e-09, 8.2454e-07, 1.1919e-06,
        7.0081e-07, 5.0813e-07, 1.7753e-06, 5.4188e-07, 1.0077e-06, 1.3422e-07,
        7.1030e-07, 1.7216e-06, 4.3289e-07, 3.2596e-07, 3.2596e-07, 3.2596e-07,
        6.0750e-07, 1.1430e-06, 3.7229e-07, 2.9411e-07, 3.9768e-07, 3.2596e-07,
        7.1030e-07, 1.0077e-06, 3.8034e-07, 5.4188e-07, 3.7229e-07, 4.3289e-07,
        2.0092e-06, 9.5982e-07, 9.5265e-07, 1.3301e-06, 5.4188e-07, 8.6460e-07,
        9.5265e-07, 2.9411e-07, 3.2596e-07, 9.5008e-07, 9.5982e-07, 1.1192e-06,
        4.9356e-07, 8.9330e-07, 9.5982e-07, 2.5825e-07, 2.9411e-07, 1.4146e-06,
        3.2596e-07, 3.6426e-07, 3.2596e-07, 9.5982e-07, 8.2454e-07, 3.2596e-07,
        3.2596e-07, 3.7229e-07, 9.5982e-07, 8.6460e-07, 6.8256e-07, 9.5982e-07,
        3.2596e-07, 2.0092e-06, 3.7229e-07, 3.2596e-07, 7.0081e-07, 2.5504e-07,
        1.7727e-06, 3.2596e-07, 1.1919e-06, 1.1430e-06, 1.7727e-06, 3.6426e-07,
        9.5982e-07, 3.9768e-07, 4.9005e-07, 1.7728e-06, 9.5265e-07, 7.0081e-07,
        4.3289e-07, 2.0092e-06, 9.5265e-07, 2.9411e-07, 9.5265e-07, 3.7229e-07,
        3.2596e-07, 8.2454e-07, 3.7229e-07, 4.9356e-07, 5.0813e-07, 4.3016e-07,
        7.1030e-07, 4.8697e-07, 9.1168e-07, 2.1190e-06, 3.9768e-07, 2.6170e-07,
        5.5702e-07, 5.7445e-07, 1.1919e-06, 5.7445e-07, 4.6051e-07, 9.5265e-07,
        5.5479e-07, 9.5982e-07, 4.9356e-07, 2.5825e-07, 8.2761e-07, 3.2596e-07,
        7.1030e-07, 8.2454e-07, 4.3289e-07, 8.2454e-07, 5.4188e-07, 1.8261e-06,
        8.6460e-07, 9.5982e-07, 1.1919e-06, 9.5982e-07, 8.2454e-07, 3.2596e-07,
        1.7864e-06, 8.2454e-07, 1.2411e-06, 3.2596e-07, 8.3102e-07, 8.2454e-07,
        9.5982e-07, 5.5479e-07, 1.7727e-06, 2.5825e-07, 4.3523e-07, 5.7730e-07,
        3.5739e-07, 3.7229e-07, 2.0092e-06, 5.5479e-07, 8.2454e-07, 1.4146e-06,
        9.5982e-07, 1.4146e-06, 9.4032e-07, 2.9411e-07, 3.2596e-07, 1.1919e-06,
        2.0092e-06, 4.4666e-07, 2.6997e-07, 5.7729e-07, 7.9221e-07, 1.2614e-07,
        5.5702e-07, 9.5982e-07, 8.2454e-07, 3.3702e-07, 8.2454e-07, 4.3289e-07,
        6.9350e-07, 1.6559e-07, 9.5982e-07, 4.3289e-07, 1.1919e-06, 3.5740e-07,
        5.5479e-07, 6.1344e-07, 8.2454e-07, 4.9356e-07, 8.3103e-07, 9.5982e-07,
        1.7727e-06, 1.7727e-06, 2.0092e-06, 4.3289e-07, 4.3523e-07, 1.0077e-06,
        1.7216e-06, 1.7846e-06, 5.7445e-07, 8.5711e-09, 7.9447e-07, 1.0921e-06,
        9.5265e-07, 4.3289e-07, 9.5982e-07, 9.5982e-07, 2.9411e-07, 1.2614e-07,
        1.0921e-06, 5.5479e-07, 1.2411e-06, 8.2454e-07, 8.2454e-07, 1.3422e-07,
        6.5624e-07, 3.2596e-07, 9.5982e-07, 1.8067e-06, 5.7445e-07, 9.5982e-07,
        2.0092e-06, 7.7969e-07, 1.0923e-06, 1.1919e-06, 8.4327e-07, 8.4327e-07,
        9.5265e-07, 1.1919e-06, 7.7969e-07, 5.7445e-07, 8.3102e-07, 5.6735e-07,
        3.6591e-07, 1.3301e-06, 7.0081e-07, 1.7863e-06, 5.7445e-07, 6.5624e-07,
        1.4146e-06, 5.5702e-07, 9.4032e-07, 7.0081e-07, 9.5982e-07, 1.0077e-06,
        4.3289e-07, 3.7229e-07, 8.5620e-09, 9.5265e-07, 1.2411e-06, 4.4666e-07,
        7.9447e-07, 7.9447e-07, 5.5479e-07, 8.6460e-07, 1.0921e-06, 3.9114e-07,
        8.2454e-07, 9.5265e-07, 5.4566e-07, 1.1919e-06, 1.1951e-06, 3.7229e-07,
        5.7730e-07, 2.9411e-07, 4.6051e-07, 3.7229e-07, 3.2596e-07, 3.2596e-07,
        4.3289e-07, 1.7727e-06, 1.0384e-06, 1.1919e-06, 5.5702e-07, 3.6590e-07,
        1.1919e-06, 8.9330e-07, 3.7229e-07, 8.2454e-07, 2.9411e-07, 5.4188e-07,
        8.5620e-09, 2.8589e-07, 9.5982e-07, 3.2596e-07, 8.2454e-07, 5.4528e-07,
        4.3289e-07, 4.9356e-07, 5.7445e-07, 2.5825e-07, 3.7229e-07, 1.7846e-06,
        1.7727e-06, 4.6051e-07, 1.7864e-06, 1.7727e-06, 5.7445e-07, 8.5711e-09,
        1.8067e-06, 1.7727e-06, 8.2454e-07, 3.9768e-07, 5.5702e-07, 3.2596e-07,
        9.5265e-07, 3.2866e-07, 9.5982e-07, 4.9356e-07, 1.1919e-06, 5.7445e-07,
        1.6307e-06, 5.5702e-07, 1.1919e-06, 8.2454e-07, 2.9411e-07, 5.7445e-07,
        4.6051e-07, 9.5982e-07, 3.7229e-07, 2.5825e-07, 5.7445e-07, 6.8255e-07,
        9.5982e-07, 3.2596e-07, 8.2454e-07, 2.0092e-06, 1.0923e-06, 1.7859e-06,
        9.5265e-07, 4.3289e-07, 2.9411e-07, 1.1919e-06, 5.5479e-07, 8.2454e-07,
        9.5982e-07, 7.1030e-07, 3.7229e-07, 2.0092e-06, 3.6591e-07, 5.5479e-07,
        1.1919e-06, 1.2411e-06, 3.2596e-07, 8.2454e-07, 1.1919e-06, 1.7216e-06,
        3.7229e-07, 8.3102e-07, 8.2454e-07, 5.5479e-07, 5.7445e-07, 9.5982e-07,
        9.5265e-07, 9.5982e-07, 7.1030e-07, 5.7445e-07, 9.5982e-07, 4.5807e-07,
        8.2454e-07, 2.9411e-07, 4.4666e-07, 5.7445e-07, 8.3102e-07, 1.8259e-06,
        2.0092e-06, 3.7229e-07, 3.7229e-07, 3.7229e-07, 1.7845e-06, 1.0071e-07,
        8.2454e-07, 1.7216e-06, 4.3289e-07, 2.0092e-06, 6.8255e-07, 9.5265e-07,
        1.7864e-06, 1.7846e-06, 1.2411e-06, 5.7729e-07, 9.0169e-07, 1.1919e-06,
        9.5982e-07, 3.7229e-07, 4.6051e-07, 1.1919e-06, 1.0923e-06, 6.1179e-07,
        3.7229e-07, 1.8067e-06, 1.7727e-06, 2.6170e-07, 1.1919e-06, 9.5982e-07,
        2.0092e-06, 1.1919e-06, 9.5982e-07, 9.5982e-07, 8.6460e-07, 5.5702e-07,
        4.6051e-07, 4.3289e-07, 1.4146e-06, 5.5479e-07, 1.3301e-06, 9.5982e-07,
        1.7846e-06, 3.2596e-07, 9.4032e-07, 8.2454e-07, 4.3289e-07, 3.2596e-07,
        7.2845e-07, 7.2846e-07, 8.3102e-07, 8.3102e-07, 2.0092e-06, 7.9447e-07,
        1.2411e-06, 9.5982e-07, 3.5739e-07, 3.7229e-07, 5.7445e-07, 3.2596e-07,
        1.0077e-06, 5.5479e-07, 2.0403e-07, 9.5008e-07, 1.1919e-06, 9.5265e-07,
        1.7862e-06, 5.4188e-07, 8.6460e-07, 3.2596e-07, 8.3102e-07, 7.2846e-07,
        6.8255e-07, 2.7618e-07, 8.2454e-07, 5.5479e-07, 7.1030e-07, 1.1919e-06,
        3.0599e-09, 5.5479e-07, 9.5982e-07, 1.1919e-06, 8.5639e-09, 2.5825e-07,
        3.7229e-07, 1.1919e-06, 2.0092e-06, 4.3523e-07, 8.2454e-07, 3.9242e-07,
        7.9447e-07, 3.2596e-07, 2.0092e-06, 5.7445e-07, 8.5639e-09, 8.4327e-07,
        3.9768e-07, 9.6538e-07, 2.1190e-06, 9.5982e-07, 2.0092e-06, 1.4146e-06,
        3.5739e-07, 4.3289e-07, 9.5265e-07, 4.6051e-07, 8.2454e-07, 1.1919e-06,
        9.5265e-07, 9.5265e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5265e-06, 4.6853e-06, 1.2785e-06, 4.3798e-06, 7.5571e-06, 4.3798e-06,
        4.9277e-06, 4.2396e-06, 3.0283e-06, 7.2700e-06, 3.3872e-06, 3.4326e-06,
        7.5571e-06, 2.0210e-06, 3.5446e-06, 7.6506e-06, 1.7741e-06, 7.2700e-06,
        6.7749e-06, 2.2690e-06, 7.9106e-06, 5.2213e-06, 1.5530e-05, 6.0434e-06,
        3.9403e-06, 1.1043e-05, 1.0764e-05, 3.8006e-06, 2.4243e-06, 7.2700e-06,
        7.2700e-06, 7.2266e-06, 5.8669e-06, 7.2266e-06, 1.2845e-06, 5.3700e-06,
        1.2080e-06, 1.5992e-06, 6.9406e-06, 2.7844e-06, 3.0283e-06, 3.8662e-06,
        1.4365e-05, 7.2700e-06, 5.9989e-06, 3.8006e-06, 1.8621e-06, 3.8006e-06,
        4.2316e-06, 1.1163e-05, 3.0329e-06, 7.2266e-06, 2.7113e-06, 1.4892e-06,
        9.1023e-07, 2.5422e-06, 7.6506e-06, 9.7581e-06, 7.6506e-06, 3.4798e-06,
        3.9822e-06, 7.4788e-06, 3.3872e-06, 4.3798e-06, 3.7642e-06, 2.6580e-06,
        7.2266e-06, 7.3398e-06, 9.5080e-06, 3.8662e-06, 2.9198e-06, 7.2266e-06,
        2.4243e-06, 7.6506e-06, 7.6506e-06, 4.6853e-06, 7.2266e-06, 7.6506e-06,
        2.1817e-06, 2.0210e-06, 1.0192e-05, 7.6506e-06, 2.0600e-06, 7.2266e-06,
        3.4326e-06, 3.8662e-06, 1.2785e-06, 2.3015e-06, 4.2316e-06, 3.9403e-06,
        2.4193e-06, 2.2543e-06, 2.4193e-06, 7.2266e-06, 6.9530e-06, 5.6715e-06,
        4.6853e-06, 3.4520e-06, 7.2700e-06, 1.2080e-06, 9.5873e-06, 3.4496e-06,
        2.0210e-06, 8.3341e-06, 2.7994e-06, 3.9403e-06, 2.4193e-06, 4.1110e-06,
        5.8669e-06, 8.3341e-06, 4.3798e-06, 1.3342e-06, 5.1476e-06, 3.5966e-06,
        1.8621e-06, 7.4973e-06, 6.9406e-06, 4.2235e-06, 4.2396e-06, 3.2829e-06,
        7.6506e-06, 3.8662e-06, 2.3752e-06, 8.3341e-06, 6.9406e-06, 7.4436e-06,
        1.2080e-06, 2.0210e-06, 4.3798e-06, 3.4326e-06, 3.8662e-06, 5.0723e-06,
        3.4475e-06, 4.3798e-06, 3.5446e-06, 3.8006e-06, 2.1681e-06, 7.2266e-06,
        2.5422e-06, 4.3075e-06, 2.3429e-06, 2.0210e-06, 7.2266e-06, 8.6192e-06,
        4.3798e-06, 8.3341e-06, 1.1140e-05, 2.8783e-06, 7.2820e-06, 7.2700e-06,
        7.6506e-06, 1.4364e-05, 7.2266e-06, 1.1140e-05, 7.2700e-06, 1.9831e-06,
        1.1401e-05, 1.1230e-05, 5.3700e-06, 1.1045e-05, 3.9822e-06, 1.1230e-05,
        4.0935e-06, 3.8662e-06, 4.4117e-06, 4.0299e-06, 6.4697e-06, 3.4798e-06,
        1.0764e-05, 5.1595e-06, 6.7749e-06, 2.0789e-06, 5.3700e-06, 4.7501e-06,
        4.9252e-06, 4.3798e-06, 7.8363e-06, 3.4798e-06, 3.0343e-06, 4.6632e-06,
        7.2724e-06, 5.3700e-06, 3.8006e-06, 2.4193e-06, 1.1163e-05, 7.3398e-06,
        1.8983e-06, 2.6637e-06, 2.7994e-06, 4.2147e-06, 1.2785e-06, 7.5571e-06,
        7.6506e-06, 2.0789e-06, 8.6192e-06, 3.4798e-06, 6.7749e-06, 4.0299e-06,
        2.4193e-06, 3.3872e-06, 9.7581e-06, 7.2266e-06, 2.4193e-06, 7.2700e-06,
        1.4365e-05, 4.2722e-06, 1.7750e-06, 2.0210e-06, 4.4431e-06, 3.0283e-06,
        3.5446e-06, 2.3752e-06, 9.9177e-07, 7.6506e-06, 4.1908e-06, 9.5270e-06,
        1.3341e-06, 8.3341e-06, 9.1023e-07, 2.4193e-06, 3.0283e-06, 4.6946e-06,
        7.6506e-06, 4.3798e-06, 6.0785e-06, 1.1163e-05, 4.3798e-06, 4.0935e-06,
        3.8006e-06, 5.3700e-06, 7.6506e-06, 2.4193e-06, 1.5484e-06, 1.2080e-06,
        7.9105e-06, 3.8006e-06, 6.4697e-06, 7.2266e-06, 7.2266e-06, 2.4193e-06,
        2.8393e-06, 7.3728e-06, 5.5587e-06, 4.8300e-06, 2.0210e-06, 1.4365e-05,
        4.9137e-06, 5.5587e-06, 7.2266e-06, 3.0050e-06, 6.3846e-06, 3.4326e-06,
        4.2879e-06, 2.4243e-06, 1.1043e-05, 3.4798e-06, 6.7749e-06, 2.1681e-06,
        5.2213e-06, 4.3798e-06, 1.1163e-05, 4.1908e-06, 1.0764e-05, 2.4193e-06,
        3.2918e-06, 3.8260e-06, 7.2700e-06, 3.7642e-06, 6.7749e-06, 7.6506e-06,
        1.5794e-06, 2.9519e-06, 7.2700e-06, 7.2700e-06, 7.4788e-06, 4.3075e-06,
        1.4365e-05, 2.4193e-06, 7.5571e-06, 3.9403e-06, 7.3728e-06, 3.8662e-06,
        3.6589e-06, 2.9803e-06, 3.4798e-06, 4.8300e-06, 1.2080e-06, 8.3341e-06,
        7.2266e-06, 1.2845e-06, 1.0853e-06, 3.6029e-06, 4.2147e-06, 3.5446e-06,
        2.2373e-06, 4.3075e-06, 3.5446e-06, 6.3846e-06, 2.4193e-06, 4.8300e-06,
        1.2785e-06, 1.8621e-06, 1.2785e-06, 6.2317e-06, 7.6506e-06, 2.7994e-06,
        8.1658e-06, 7.1910e-06, 5.2377e-06, 2.9519e-06, 3.4798e-06, 1.0764e-05,
        2.9519e-06, 2.8283e-06, 7.5571e-06, 1.6606e-06, 2.0210e-06, 7.2700e-06,
        4.3075e-06, 2.7813e-06, 1.1463e-06, 7.6506e-06, 2.7994e-06, 2.4193e-06,
        2.6637e-06, 6.9406e-06, 3.8662e-06, 9.7581e-06, 2.7994e-06, 9.7581e-06,
        4.2722e-06, 2.2320e-06, 1.2785e-06, 2.2543e-06, 3.8662e-06, 2.9519e-06,
        2.0210e-06, 4.0245e-06, 2.4258e-06, 3.8006e-06, 4.3798e-06, 2.8393e-06,
        1.1230e-05, 6.7749e-06, 4.1110e-06, 3.2017e-06, 7.1052e-06, 7.2700e-06,
        2.4243e-06, 6.9406e-06, 6.9406e-06, 6.1669e-07, 1.6606e-06, 7.6506e-06,
        3.0758e-06, 7.6506e-06, 7.6506e-06, 6.7749e-06, 2.4193e-06, 2.4243e-06,
        2.2373e-06, 2.6609e-06, 5.0723e-06, 5.0723e-06, 1.2080e-06, 1.2080e-06,
        2.4193e-06, 4.6853e-06, 3.0283e-06, 1.2372e-05, 7.4436e-06, 3.8006e-06,
        1.0764e-05, 2.9803e-06, 7.2700e-06, 1.2080e-06, 7.6506e-06, 7.3398e-06,
        2.4193e-06, 5.6986e-06, 7.2266e-06, 6.9406e-06, 5.3700e-06, 1.2845e-06,
        1.1862e-06, 2.4193e-06, 2.2373e-06, 9.7581e-06, 7.6506e-06, 2.2373e-06,
        4.6853e-06, 3.4798e-06, 4.6632e-06, 2.5733e-06, 6.0434e-06, 8.3341e-06,
        2.4193e-06, 4.2316e-06, 7.6506e-06, 3.4191e-06, 5.1476e-06, 2.4193e-06,
        1.2785e-06, 4.4431e-06, 1.2785e-06, 1.6093e-06, 7.2700e-06, 5.5587e-06,
        2.4193e-06, 4.3798e-06, 7.6506e-06, 5.4722e-06, 2.0210e-06, 7.6506e-06,
        2.4243e-06, 3.4496e-06, 1.5794e-06, 1.1140e-05, 5.0723e-06, 1.4364e-05,
        1.1044e-05, 9.5837e-06, 3.8662e-06, 2.9519e-06, 2.9519e-06, 2.9519e-06,
        7.2266e-06, 4.1087e-06, 7.2266e-06, 4.6853e-06, 3.0343e-06, 1.0764e-05,
        3.8006e-06, 4.9963e-06, 4.2316e-06, 5.8669e-06, 7.2700e-06, 3.0283e-06,
        7.4436e-06, 3.8006e-06, 1.8139e-06, 2.3752e-06, 5.3700e-06, 3.0283e-06,
        7.2700e-06, 7.2266e-06, 3.2875e-06, 1.1218e-05, 7.6506e-06, 1.2785e-06,
        2.4193e-06, 3.2750e-06, 4.2722e-06, 7.5571e-06, 3.4798e-06, 7.2266e-06,
        7.2700e-06, 5.3700e-06, 2.4243e-06, 2.8783e-06, 7.2700e-06, 7.5571e-06,
        2.2690e-06, 7.6506e-06, 4.0245e-06, 4.8300e-06, 2.4193e-06, 7.2820e-06,
        6.8939e-06, 3.8006e-06, 4.6940e-06, 1.7350e-06, 2.7994e-06, 3.5446e-06,
        7.2266e-06, 6.7749e-06, 2.2690e-06, 1.1044e-05, 7.6506e-06, 2.9519e-06,
        2.5422e-06, 9.7581e-06, 9.7581e-06, 1.3341e-06, 8.3341e-06, 6.1529e-06,
        3.3872e-06, 1.0923e-05, 4.9252e-06, 7.2266e-06, 2.1681e-06, 5.3700e-06,
        7.2266e-06, 3.1006e-06, 1.0717e-06, 5.6294e-06, 2.3429e-06, 3.9403e-06,
        4.5901e-06, 3.8006e-06, 7.2700e-06, 5.5587e-06, 1.7750e-06, 5.2213e-06,
        4.4431e-06, 7.5571e-06, 2.7113e-06, 7.5571e-06, 2.9803e-06, 7.2266e-06,
        2.9604e-06, 4.3798e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([4.0792e-07, 4.9626e-07, 1.1136e-06,  ..., 5.3785e-07, 2.8358e-07,
        2.7287e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5128e-06, 8.3099e-07, 8.6220e-07,  ..., 5.6204e-05, 6.3287e-07,
        1.3348e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([6.3541e-07, 2.8018e-06, 4.2642e-06, 4.4574e-06, 1.2061e-06, 1.9811e-06,
        6.9481e-07, 6.1711e-06, 2.6683e-06, 1.2040e-06, 1.7587e-06, 1.4735e-06,
        5.9693e-06, 3.8340e-06, 1.2540e-06, 1.0194e-06, 3.3060e-06, 3.1354e-06,
        1.7668e-06, 3.4250e-06, 2.9042e-06, 1.2478e-06, 1.5808e-06, 3.1981e-06,
        2.1364e-06, 3.1906e-06, 1.7668e-06, 2.3021e-06, 2.3817e-06, 1.2689e-06,
        2.1314e-06, 2.7700e-06, 2.4024e-06, 2.0257e-06, 3.7631e-07, 5.7740e-06,
        4.3249e-06, 4.7869e-06, 2.9996e-06, 1.9277e-06, 2.1943e-06, 1.5696e-06,
        1.2540e-06, 2.4611e-06, 3.1906e-06, 2.8880e-06, 1.2922e-06, 4.7869e-06,
        2.1008e-06, 2.3817e-06, 3.0663e-06, 3.8340e-06, 2.2123e-06, 4.2642e-06,
        4.2642e-06, 2.4091e-06, 2.5330e-06, 2.6479e-06, 3.7793e-06, 2.0493e-06,
        1.7587e-06, 3.2375e-06, 1.5412e-06, 8.4881e-07, 6.4225e-06, 1.3541e-06,
        3.2587e-06, 1.9811e-06, 2.1008e-06, 2.8018e-06, 4.3840e-06, 1.7668e-06,
        6.3541e-07, 2.0576e-06, 3.1013e-06, 2.9946e-06, 2.4450e-06, 2.9946e-06,
        7.3707e-07, 3.1668e-06, 1.9468e-06, 2.8018e-06, 2.3523e-06, 2.0328e-06,
        1.7668e-06, 4.7869e-06, 1.9811e-06, 1.7635e-06, 2.7700e-06, 1.9811e-06,
        2.0122e-06, 6.8704e-07, 3.2375e-06, 1.5374e-06, 1.2922e-06, 2.1956e-06,
        2.6326e-06, 1.5727e-06, 5.4798e-08, 2.1008e-06, 4.3017e-06, 2.7918e-06,
        6.7759e-07, 4.0437e-06, 2.0926e-06, 4.7869e-06, 4.0355e-06, 1.5508e-06,
        2.9645e-06, 1.1968e-06, 2.4682e-06, 2.2123e-06, 2.0576e-06, 2.7700e-06,
        1.2922e-06, 3.8340e-06, 7.3707e-07, 2.1314e-06, 1.1754e-06, 1.9468e-06,
        1.7587e-06, 2.0257e-06, 1.9811e-06, 1.9811e-06, 6.3541e-07, 2.2123e-06,
        4.1496e-06, 1.9811e-06, 4.0565e-06, 4.0355e-06, 7.1847e-07, 1.9792e-06,
        2.0257e-06, 3.2375e-06, 1.0194e-06, 4.7869e-06, 4.2642e-06, 3.1668e-06,
        4.2642e-06, 2.2557e-06, 3.7686e-06, 1.6165e-06, 8.0404e-06, 2.4091e-06,
        4.9562e-06, 2.2123e-06, 1.5727e-06, 2.9861e-06, 1.7587e-06, 1.2540e-06,
        4.4796e-06, 4.0565e-06, 1.6360e-06, 2.7700e-06, 4.4796e-06, 2.9784e-07,
        4.2642e-06, 6.3541e-07, 1.9811e-06, 2.0257e-06, 4.9339e-07, 9.0219e-07,
        2.0257e-06, 4.2642e-06, 2.5086e-06, 2.4611e-06, 2.8018e-06, 3.1496e-06,
        4.7869e-06, 1.1968e-06, 4.0736e-06, 1.4662e-06, 7.3707e-07, 2.1830e-06,
        4.0355e-06, 6.8704e-07, 4.0437e-06, 6.2067e-06, 2.5086e-06, 2.3021e-06,
        1.9811e-06, 3.2375e-06, 7.3707e-07, 2.1830e-06, 6.6913e-06, 1.7293e-06,
        1.4132e-06, 6.3541e-07, 1.2063e-06, 2.5591e-07, 2.2123e-06, 3.3060e-06,
        4.8854e-06, 1.9466e-06, 3.3804e-06, 2.3021e-06, 2.7258e-06, 1.5580e-06,
        2.0581e-06, 3.2375e-06, 2.2123e-06, 1.7293e-06, 1.0512e-06, 4.0736e-06,
        2.9996e-06, 3.8340e-06, 1.9501e-06, 4.9561e-06, 3.5437e-06, 2.7899e-06,
        9.0632e-07, 2.0257e-06, 1.7635e-06, 8.3228e-07, 4.0437e-06, 4.2642e-06,
        2.3021e-06, 1.7668e-06, 2.4164e-06, 1.9466e-06, 1.9811e-06, 2.4611e-06,
        4.2642e-06, 2.0576e-06, 2.2123e-06, 1.7574e-06, 2.0926e-06, 2.3686e-06,
        2.3686e-06, 2.0576e-06, 2.2399e-06, 3.1496e-06, 2.0257e-06, 4.0437e-06,
        1.7668e-06, 2.7697e-06, 1.4662e-06, 6.1711e-06, 5.9355e-06, 1.3971e-06,
        4.4796e-06, 3.6166e-06, 6.1711e-06, 1.1132e-06, 1.7668e-06, 1.1754e-06,
        9.1731e-06, 4.7869e-06, 6.3541e-07, 1.1968e-06, 6.3541e-07, 5.7740e-06,
        5.3025e-06, 3.3523e-07, 3.2375e-06, 3.2375e-06, 2.7152e-06, 2.3021e-06,
        3.5437e-06, 4.1496e-06, 5.9693e-06, 2.1314e-06, 1.9811e-06, 5.7740e-06,
        1.0240e-06, 2.0122e-06, 2.4513e-06, 2.4164e-06, 2.7918e-06, 1.8519e-06,
        1.9811e-06, 1.4561e-06, 4.4796e-06, 1.7668e-06, 3.1906e-06, 2.8739e-06,
        1.9544e-06, 4.3141e-06, 2.0257e-06, 2.2935e-06, 1.2277e-06, 1.0512e-06,
        4.1496e-06, 1.2922e-06, 1.5412e-06, 9.2378e-07, 2.4450e-06, 6.1711e-06,
        2.0493e-06, 2.4450e-06, 1.7587e-06, 2.4450e-06, 3.5437e-06, 3.1013e-06,
        3.1906e-06, 2.3686e-06, 3.2587e-06, 9.2378e-07, 1.2540e-06, 3.3609e-06,
        2.0257e-06, 1.7668e-06, 1.9811e-06, 1.7367e-06, 5.9693e-06, 3.1906e-06,
        1.9586e-06, 1.8726e-06, 2.2123e-06, 4.0437e-06, 3.3060e-06, 1.3541e-06,
        2.0576e-06, 1.7146e-06, 6.3541e-07, 7.2488e-06, 3.8340e-06, 8.6018e-07,
        6.5594e-06, 3.3476e-06, 3.2375e-06, 1.9811e-06, 1.2172e-06, 3.4250e-06,
        1.9811e-06, 4.9339e-07, 4.1496e-06, 1.4197e-06, 3.2375e-06, 3.4250e-06,
        4.4796e-06, 2.7258e-06, 3.4000e-07, 1.6036e-07, 1.7635e-06, 2.0257e-06,
        2.5086e-06, 1.4561e-06, 1.1850e-06, 3.3060e-06, 1.3541e-06, 1.3619e-06,
        1.7635e-06, 3.1906e-06, 2.7700e-06, 1.8515e-06, 2.7258e-06, 4.3249e-06,
        2.2162e-06, 1.0168e-06, 2.8574e-06, 1.7146e-06, 8.3228e-07, 2.5591e-07,
        1.9811e-06, 7.2488e-06, 3.1906e-06, 1.9811e-06, 1.5374e-06, 2.0576e-06,
        4.2642e-06, 4.0736e-06, 3.8852e-06, 4.5275e-06, 1.5374e-06, 2.0926e-06,
        4.0577e-06, 1.4856e-06, 2.2123e-06, 2.3686e-06, 4.4561e-06, 4.2642e-06,
        2.7258e-06, 1.4561e-06, 2.2123e-06, 9.0219e-07, 2.7697e-06, 5.9693e-06,
        2.4682e-06, 9.1303e-07, 4.7869e-06, 2.5086e-06, 2.2123e-06, 2.9042e-06,
        4.4796e-06, 1.5238e-06, 4.7869e-06, 1.9466e-06, 1.9811e-06, 2.4239e-06,
        7.3715e-07, 2.4984e-06, 1.9811e-06, 3.1013e-06, 1.1754e-06, 4.2261e-06,
        1.7587e-06, 2.4611e-06, 1.2922e-06, 4.7869e-06, 2.2123e-06, 3.3060e-06,
        2.1943e-06, 4.7869e-06, 3.2375e-06, 1.2430e-06, 1.7668e-06, 5.7740e-06,
        1.7587e-06, 6.8704e-07, 3.2375e-06, 1.2922e-06, 4.4987e-06, 2.3021e-06,
        1.8515e-06, 3.1013e-06, 4.3736e-06, 2.1848e-06, 2.0257e-06, 1.7587e-06,
        5.9355e-06, 3.1006e-06, 1.2061e-06, 2.4164e-06, 2.0576e-06, 4.4796e-06,
        2.1943e-06, 2.8103e-06, 2.1848e-06, 4.0437e-06, 1.3971e-06, 1.7587e-06,
        3.7394e-06, 4.0736e-06, 1.7587e-06, 1.7668e-06, 2.5405e-06, 4.2642e-06,
        3.4250e-06, 1.7587e-06, 1.2277e-06, 2.9275e-06, 7.3707e-07, 1.9466e-06,
        2.0312e-06, 1.9792e-06, 8.6018e-07, 1.2277e-06, 1.0194e-06, 2.9540e-06,
        2.7521e-06, 1.4662e-06, 2.4239e-06, 2.3817e-06, 1.5183e-06, 1.9811e-06,
        4.7869e-06, 4.4796e-06, 1.7293e-06, 2.0576e-06, 1.2172e-06, 2.2123e-06,
        1.6360e-06, 3.2375e-06, 6.3541e-07, 2.3817e-06, 7.3707e-07, 1.8515e-06,
        2.4611e-06, 1.1754e-06, 4.7869e-06, 1.6521e-06, 3.0302e-06, 3.3060e-06,
        8.3228e-07, 1.4853e-06, 8.8838e-07, 1.7293e-06, 2.0576e-06, 4.7869e-06,
        1.7587e-06, 4.2642e-06, 2.4450e-06, 4.9339e-07, 2.7258e-06, 2.5591e-07,
        2.2123e-06, 1.1754e-06, 2.8574e-06, 1.9811e-06, 5.9355e-06, 2.7700e-06,
        2.0912e-06, 1.7587e-06, 1.9466e-06, 1.8084e-06, 8.3228e-07, 5.6295e-06,
        2.9946e-06, 4.7869e-06, 4.4796e-06, 4.8854e-06, 8.0404e-06, 5.5448e-07,
        2.5290e-06, 4.0437e-06, 1.4433e-06, 1.9811e-06, 2.0926e-06, 2.9415e-06,
        1.2540e-06, 4.2642e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([3.7720e-06, 4.9415e-06, 9.4149e-06, 6.1879e-06, 2.3237e-06, 8.1066e-06,
        1.2920e-05, 1.3532e-05, 4.0367e-06, 1.4427e-06, 7.9283e-06, 3.5169e-06,
        6.9292e-06, 7.6593e-06, 1.6863e-05, 8.7814e-06, 2.0410e-05, 1.8689e-05,
        2.1447e-05, 1.4134e-05, 7.2808e-06, 5.5235e-06, 1.4930e-05, 4.2789e-06,
        8.3823e-06, 1.2870e-05, 1.2961e-05, 4.5409e-06, 9.8716e-06, 8.8403e-06,
        1.2485e-05, 9.3172e-06, 4.7801e-06, 2.3056e-05, 7.7776e-06, 1.3532e-05,
        7.7842e-06, 8.5505e-06, 4.3410e-06, 3.9764e-06, 9.9120e-06, 8.8063e-06,
        5.4692e-06, 1.1630e-05, 5.8911e-06, 1.9375e-05, 1.0355e-05, 7.7265e-06,
        4.6547e-06, 2.3633e-05, 1.2485e-05, 5.1747e-06, 1.0558e-05, 1.3163e-05,
        7.7822e-06, 8.8062e-06, 1.1547e-05, 2.0447e-06, 1.6590e-05, 1.5141e-05,
        1.1830e-05, 1.3451e-05, 5.9386e-06, 6.3918e-06, 1.8876e-06, 7.3796e-06,
        1.5049e-05, 1.2485e-05, 1.8404e-05, 1.1656e-05, 1.0086e-05, 6.1879e-06,
        6.7265e-06, 2.0410e-05, 1.7090e-05, 6.1879e-06, 1.2870e-05, 1.3630e-05,
        1.0086e-05, 8.7809e-06, 1.4152e-05, 1.5519e-05, 7.9499e-06, 1.3630e-05,
        1.4292e-05, 3.2644e-06, 6.1879e-06, 8.7718e-06, 5.2976e-06, 1.3997e-05,
        4.7440e-06, 3.5175e-06, 4.4935e-06, 1.0491e-05, 4.6163e-06, 8.6982e-06,
        1.0794e-05, 1.3137e-05, 7.7034e-06, 1.1456e-05, 1.3684e-05, 6.1879e-06,
        9.1285e-06, 1.2308e-05, 6.6611e-06, 5.7104e-06, 1.1494e-05, 6.3700e-06,
        1.2684e-05, 1.0358e-05, 1.0263e-05, 9.5054e-06, 2.7744e-05, 1.0653e-05,
        1.1656e-05, 6.0451e-06, 5.8092e-06, 7.9091e-06, 6.1344e-06, 4.6543e-06,
        4.3410e-06, 7.1421e-06, 1.1642e-05, 2.3210e-05, 1.9230e-05, 6.6446e-06,
        1.6500e-05, 1.0947e-05, 2.1678e-05, 8.3505e-06, 9.6462e-06, 2.1378e-05,
        4.8328e-06, 9.8969e-06, 9.9033e-06, 4.7832e-06, 4.2314e-06, 8.4827e-06,
        1.3441e-05, 8.1066e-06, 1.0330e-05, 9.0212e-06, 1.2064e-05, 5.7509e-06,
        1.5711e-05, 2.1378e-05, 1.7550e-05, 5.6139e-06, 6.2060e-06, 8.8063e-06,
        1.2175e-05, 1.7428e-05, 1.6863e-05, 1.2861e-05, 8.4037e-06, 9.0022e-06,
        7.5958e-06, 1.2593e-05, 6.6200e-06, 7.5777e-06, 1.1553e-05, 6.3918e-06,
        4.6163e-06, 2.4107e-05, 7.7776e-06, 1.1260e-05, 8.8062e-06, 1.4152e-05,
        4.4502e-06, 7.7842e-06, 3.8012e-06, 5.4592e-06, 1.2175e-05, 9.8471e-06,
        1.3698e-05, 4.6547e-06, 4.1458e-06, 1.2970e-05, 1.1100e-05, 1.0820e-05,
        2.1378e-05, 1.3802e-05, 1.2485e-05, 1.1091e-05, 8.1535e-06, 5.2972e-06,
        1.1555e-05, 1.2515e-05, 1.0086e-05, 5.9675e-06, 5.6906e-06, 1.3721e-05,
        1.3068e-05, 1.9187e-05, 4.6964e-06, 1.7973e-05, 7.8634e-06, 4.8339e-06,
        5.2316e-06, 5.1435e-06, 1.3036e-05, 1.9187e-05, 1.6555e-05, 6.5750e-06,
        1.2014e-05, 7.9263e-06, 8.2631e-06, 3.5602e-06, 1.3472e-05, 2.1378e-05,
        1.7090e-05, 8.5477e-06, 1.2114e-05, 6.8611e-06, 6.1879e-06, 1.5556e-05,
        4.5127e-06, 1.2271e-05, 6.2060e-06, 1.8404e-05, 9.0316e-06, 1.3469e-05,
        1.1274e-05, 1.6443e-05, 1.0071e-05, 1.5552e-05, 7.0488e-06, 5.5755e-06,
        2.1447e-05, 1.0456e-05, 6.3624e-06, 5.7104e-06, 6.2060e-06, 5.3661e-06,
        6.9829e-06, 7.9532e-06, 1.0606e-05, 8.5806e-06, 1.6501e-05, 1.3613e-05,
        1.2271e-05, 1.2522e-05, 7.4182e-06, 1.2870e-05, 7.4186e-06, 6.2118e-06,
        2.0410e-05, 1.1123e-05, 1.2920e-05, 8.7809e-06, 2.1887e-05, 8.2314e-06,
        1.4937e-05, 5.2972e-06, 9.5820e-06, 1.8378e-05, 1.0903e-05, 1.1408e-06,
        9.8721e-06, 1.9881e-05, 1.3367e-05, 3.4103e-06, 4.8069e-06, 1.1165e-05,
        1.1165e-05, 1.5448e-05, 2.2174e-05, 4.4935e-06, 7.7776e-06, 8.4116e-06,
        1.3532e-05, 6.4366e-06, 9.4751e-06, 1.3757e-05, 2.2596e-05, 1.8632e-05,
        6.6200e-06, 3.8012e-06, 1.2561e-05, 6.4767e-06, 1.2013e-05, 7.4182e-06,
        1.3757e-05, 1.8743e-05, 1.4534e-05, 1.8378e-05, 1.2822e-05, 5.1515e-06,
        4.2314e-06, 8.4544e-06, 1.2271e-05, 1.7973e-05, 1.6863e-05, 1.7594e-05,
        1.3630e-05, 1.2843e-05, 6.4767e-06, 1.8484e-05, 1.1553e-05, 7.4395e-06,
        1.1253e-05, 9.8996e-06, 5.1435e-06, 1.4287e-05, 1.4040e-05, 7.6593e-06,
        8.6721e-06, 8.3822e-06, 1.2961e-05, 1.3532e-05, 1.1629e-05, 1.0008e-05,
        2.1678e-05, 1.6796e-05, 8.8868e-06, 3.5602e-06, 6.6495e-06, 1.5340e-05,
        6.0451e-06, 1.0260e-05, 4.3803e-06, 1.5207e-05, 1.0355e-05, 9.8256e-06,
        1.7554e-05, 9.9120e-06, 2.0780e-05, 1.4187e-05, 4.7373e-06, 6.1879e-06,
        1.3532e-05, 5.6962e-06, 3.5602e-06, 5.5508e-06, 1.3532e-05, 2.1378e-05,
        1.2485e-05, 1.0004e-05, 9.4149e-06, 1.0823e-05, 1.7973e-05, 5.9386e-06,
        1.0086e-05, 6.6825e-06, 4.3410e-06, 9.0007e-06, 2.2596e-05, 3.5602e-06,
        6.0451e-06, 1.4876e-05, 1.1343e-05, 1.1549e-05, 6.6496e-06, 1.2515e-05,
        5.6962e-06, 9.6462e-06, 9.2527e-06, 1.2822e-05, 1.5666e-05, 4.1969e-06,
        8.8063e-06, 1.2485e-05, 2.1378e-05, 4.1969e-06, 5.8211e-06, 2.6475e-05,
        9.8969e-06, 1.4472e-06, 1.5038e-05, 6.4943e-06, 6.1879e-06, 5.4087e-06,
        6.2193e-06, 8.3699e-06, 9.3473e-06, 2.4157e-06, 1.2870e-05, 7.7842e-06,
        4.3410e-06, 6.6521e-06, 8.8062e-06, 1.2152e-05, 2.3163e-06, 3.2106e-05,
        8.7795e-06, 2.7236e-05, 6.0451e-06, 1.5585e-05, 5.1308e-06, 8.5537e-06,
        1.6860e-05, 1.3733e-05, 5.9041e-06, 1.0355e-05, 1.3615e-05, 9.9062e-06,
        1.5706e-05, 5.5049e-06, 1.2383e-05, 2.4072e-06, 1.3451e-05, 1.2684e-05,
        1.7090e-05, 1.9552e-05, 6.7403e-06, 6.7001e-06, 1.4288e-05, 7.7842e-06,
        1.1642e-05, 7.7776e-06, 4.9537e-06, 1.0355e-05, 1.1714e-05, 1.4651e-05,
        1.5620e-05, 2.1815e-05, 1.1704e-05, 1.2561e-05, 6.8893e-06, 8.8757e-06,
        1.5946e-05, 7.8452e-06, 5.5755e-06, 1.8531e-05, 1.4937e-05, 2.3631e-05,
        1.7594e-05, 1.8487e-05, 9.4373e-06, 1.0219e-05, 1.1572e-05, 6.6377e-06,
        9.9033e-06, 2.0817e-05, 1.1907e-05, 1.3709e-05, 1.3225e-05, 7.4881e-06,
        1.2920e-05, 1.0454e-05, 2.1378e-05, 1.1553e-05, 9.2598e-06, 9.6255e-06,
        6.6200e-06, 9.2307e-06, 1.0244e-05, 6.0451e-06, 4.3410e-06, 4.0345e-06,
        1.0798e-05, 1.9765e-05, 1.4292e-05, 8.3699e-06, 1.0222e-05, 2.3210e-05,
        1.0888e-05, 6.0451e-06, 5.9851e-06, 8.6990e-06, 2.0884e-05, 2.5683e-05,
        1.3733e-05, 8.7718e-06, 8.3851e-06, 1.4538e-05, 1.1889e-05, 1.5439e-05,
        2.1612e-05, 8.3680e-06, 8.5739e-06, 1.2555e-05, 4.8039e-06, 9.9120e-06,
        1.5471e-05, 5.7104e-06, 1.4534e-05, 6.0451e-06, 6.6495e-06, 1.4846e-05,
        4.1969e-06, 1.7554e-05, 7.1705e-06, 6.3248e-06, 9.5054e-06, 1.0086e-05,
        1.8987e-05, 1.4288e-05, 8.7557e-06, 2.3163e-06, 1.1274e-05, 6.6167e-06,
        8.5739e-06, 6.1292e-06, 7.6521e-06, 1.6157e-05, 1.5519e-05, 7.4358e-06,
        4.3410e-06, 9.3174e-06, 1.9601e-05, 6.0451e-06, 3.4534e-06, 1.3532e-05,
        1.8378e-05, 8.1066e-06, 1.9251e-05, 9.5685e-06, 1.1704e-05, 1.7000e-05,
        4.9511e-06, 1.0771e-05], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([5.8494e-07, 1.5014e-06, 1.2452e-06,  ..., 1.0936e-06, 2.3885e-06,
        7.6944e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.5555e-06, 1.7156e-06, 8.5912e-07, 3.1654e-06, 4.6133e-06, 4.3122e-06,
        2.2250e-06, 6.4926e-06, 1.5134e-06, 8.6386e-06, 5.8647e-06, 5.7155e-06,
        8.6386e-06, 2.0264e-06, 5.2182e-06, 4.1912e-06, 4.6948e-06, 4.1881e-06,
        6.1925e-06, 7.1576e-06, 4.3403e-06, 2.0905e-06, 2.9773e-06, 3.8362e-06,
        7.9070e-06, 6.8481e-07, 1.1395e-06, 3.7803e-06, 2.1912e-06, 2.1466e-06,
        4.7304e-07, 3.1295e-06, 5.0053e-06, 1.7134e-06, 2.2206e-06, 7.2886e-06,
        2.4153e-06, 3.9445e-06, 2.8113e-06, 8.2924e-07, 8.2929e-06, 2.2738e-06,
        4.2769e-06, 2.4025e-06, 6.4339e-06, 3.3425e-06, 4.1317e-06, 3.8881e-06,
        9.4034e-06, 9.8868e-07, 6.4339e-06, 4.6951e-06, 3.8448e-06, 4.3122e-06,
        4.2693e-06, 8.4011e-07, 2.4225e-06, 7.8600e-06, 4.1711e-06, 3.5658e-06,
        1.0504e-05, 4.4847e-06, 3.3044e-06, 5.3165e-06, 3.8003e-06, 3.9033e-06,
        6.4085e-06, 1.0498e-05, 8.3089e-06, 3.1654e-06, 1.5483e-06, 3.3654e-06,
        2.4325e-06, 1.8712e-06, 1.5972e-06, 2.2368e-06, 3.3425e-06, 2.4225e-06,
        3.4381e-06, 3.5015e-06, 3.2074e-06, 5.7054e-06, 2.0307e-06, 3.7533e-06,
        7.1051e-07, 1.7156e-06, 6.4339e-06, 4.0625e-06, 5.7155e-06, 2.8113e-06,
        4.3403e-06, 2.2380e-06, 5.6650e-06, 8.2924e-07, 9.8873e-07, 5.6650e-06,
        6.3233e-06, 4.4097e-06, 8.6386e-06, 9.8854e-07, 5.3127e-06, 2.9175e-06,
        3.6387e-06, 2.0307e-06, 4.1912e-06, 3.9994e-06, 4.7304e-07, 5.0879e-06,
        1.7952e-06, 3.2676e-06, 3.7090e-06, 3.6577e-06, 9.2370e-07, 2.1182e-06,
        2.5722e-06, 1.7156e-06, 1.7134e-06, 3.7644e-06, 2.4025e-06, 6.4337e-06,
        5.7054e-06, 2.6307e-06, 5.5476e-06, 4.2416e-06, 1.7156e-06, 4.0625e-06,
        3.7091e-06, 5.7155e-06, 4.6084e-06, 1.7543e-06, 1.1984e-06, 6.4893e-06,
        1.7134e-06, 4.1647e-06, 4.7102e-06, 1.7156e-06, 2.9199e-06, 6.2822e-06,
        1.6422e-06, 6.1819e-06, 3.7742e-06, 3.1295e-06, 2.7728e-06, 8.2924e-07,
        3.1195e-06, 2.0357e-06, 1.6847e-06, 3.1690e-06, 1.7156e-06, 5.1044e-06,
        3.6410e-06, 8.1745e-06, 5.6316e-06, 3.7803e-06, 5.2851e-06, 3.8759e-06,
        3.3966e-06, 2.9029e-06, 4.9931e-06, 9.3376e-06, 4.4297e-06, 3.2525e-06,
        7.4744e-06, 2.6307e-06, 2.0357e-06, 5.9262e-06, 5.9845e-06, 3.4406e-06,
        2.6470e-06, 3.9420e-06, 9.8872e-07, 3.8164e-06, 1.5483e-06, 5.2128e-06,
        4.0892e-06, 7.5188e-07, 3.8569e-06, 2.1648e-06, 2.4225e-06, 1.7156e-06,
        6.5326e-06, 3.1759e-06, 9.0487e-06, 8.2924e-07, 7.1576e-06, 3.9033e-06,
        1.9025e-06, 4.9755e-06, 2.3780e-06, 2.1772e-06, 3.3582e-06, 3.7803e-06,
        4.2923e-06, 3.8164e-06, 4.6936e-07, 7.3221e-07, 7.3067e-06, 2.0249e-06,
        3.0760e-06, 4.6317e-07, 1.5972e-06, 4.5513e-06, 4.3403e-06, 3.9103e-06,
        3.7644e-06, 3.3167e-06, 4.4297e-06, 3.4202e-06, 3.7803e-06, 3.0762e-07,
        1.9480e-06, 1.5199e-06, 4.0625e-06, 4.8727e-06, 2.7631e-06, 4.0346e-06,
        1.9628e-06, 3.9553e-06, 3.7644e-06, 2.9773e-06, 6.1925e-06, 2.6816e-06,
        3.3732e-06, 2.2738e-06, 9.4496e-07, 6.3370e-06, 4.4635e-06, 4.1317e-06,
        6.0262e-06, 2.1648e-06, 2.4859e-06, 2.9887e-06, 9.9790e-06, 7.3719e-07,
        2.8005e-06, 2.0307e-06, 3.9210e-06, 2.1073e-06, 3.7644e-06, 3.4381e-06,
        4.4516e-06, 4.1637e-06, 3.0882e-06, 5.7155e-06, 3.7131e-06, 2.2164e-06,
        5.1839e-06, 4.2288e-06, 3.6617e-06, 4.8872e-07, 5.1006e-06, 3.8362e-06,
        5.6454e-06, 5.1006e-06, 3.3167e-06, 4.7102e-06, 3.2926e-06, 6.5602e-06,
        4.9755e-06, 4.7102e-06, 3.6387e-06, 1.7156e-06, 4.6084e-06, 3.1759e-06,
        5.4155e-06, 4.6780e-06, 4.4297e-06, 1.7156e-06, 1.0730e-06, 4.5935e-06,
        2.0307e-06, 3.7090e-06, 3.2775e-06, 2.2250e-06, 6.9269e-07, 4.4635e-06,
        4.2577e-06, 3.6665e-06, 3.1759e-06, 2.7871e-06, 1.7654e-06, 4.3122e-06,
        3.6372e-06, 1.4263e-06, 1.9464e-06, 1.8941e-06, 9.7143e-06, 3.2481e-06,
        2.9981e-06, 4.0625e-06, 1.7156e-06, 5.0053e-06, 9.6913e-07, 3.6801e-06,
        4.8727e-06, 2.1648e-06, 2.7876e-06, 2.5707e-06, 3.3223e-06, 2.9694e-06,
        2.9842e-06, 2.6307e-06, 3.5012e-06, 7.5913e-06, 4.2217e-07, 4.3637e-06,
        6.3858e-06, 8.2924e-07, 4.0346e-06, 9.2371e-07, 2.7496e-06, 1.7828e-06,
        3.6665e-06, 4.1317e-06, 2.8115e-06, 6.0408e-06, 5.7801e-06, 6.8879e-06,
        3.6387e-06, 6.3991e-06, 4.5843e-06, 3.5649e-06, 2.7357e-06, 4.0625e-06,
        2.1484e-06, 4.2923e-06, 2.2250e-06, 4.6780e-06, 3.5658e-06, 8.6897e-06,
        4.0510e-06, 2.2250e-06, 3.4688e-06, 3.1690e-06, 4.5804e-06, 4.7887e-06,
        8.5547e-07, 2.0307e-06, 9.8818e-07, 2.7879e-06, 3.1635e-06, 8.8177e-06,
        6.7175e-06, 8.2924e-07, 6.4337e-06, 2.2738e-06, 4.1429e-06, 2.0869e-06,
        5.2182e-06, 3.4381e-06, 4.1881e-06, 3.4502e-06, 5.3127e-06, 3.1653e-06,
        2.7129e-06, 9.4106e-06, 3.1634e-06, 4.4297e-06, 3.6387e-06, 2.3779e-06,
        6.2655e-06, 5.7416e-06, 5.7155e-06, 5.3177e-06, 4.1821e-06, 3.1690e-06,
        8.6386e-06, 1.0033e-05, 3.5921e-06, 2.2739e-06, 5.1831e-06, 3.4502e-06,
        3.6387e-06, 7.1576e-06, 8.6897e-06, 2.0697e-06, 2.2235e-06, 3.1759e-06,
        4.6936e-07, 4.0625e-06, 3.2074e-06, 2.5921e-06, 5.1887e-06, 5.8806e-06,
        3.5921e-06, 3.5913e-06, 3.6665e-06, 9.0487e-06, 3.3654e-06, 3.7803e-06,
        1.5972e-06, 2.8508e-06, 3.3044e-06, 1.8384e-06, 6.5602e-06, 4.4516e-06,
        1.1040e-06, 4.6951e-06, 7.4891e-06, 7.0538e-06, 4.9755e-06, 2.4153e-06,
        3.9033e-06, 3.9969e-06, 3.9210e-06, 2.7496e-06, 1.0504e-05, 1.8187e-06,
        3.8402e-06, 2.3408e-06, 1.0881e-05, 3.3044e-06, 3.9969e-06, 2.2368e-06,
        1.5231e-06, 2.6375e-06, 6.1925e-06, 2.2738e-06, 1.9717e-06, 2.2250e-06,
        5.2182e-06, 2.5326e-06, 4.6176e-06, 6.1614e-06, 6.3370e-06, 2.2067e-06,
        4.6906e-06, 5.7801e-06, 1.9025e-06, 1.0493e-06, 2.5722e-06, 2.4614e-06,
        2.4190e-06, 5.1700e-06, 6.4339e-06, 2.5310e-06, 2.2490e-06, 3.1295e-06,
        4.3403e-06, 9.1481e-06, 4.7102e-06, 5.8178e-06, 9.1481e-06, 9.7273e-07,
        3.2676e-06, 1.7941e-07, 4.0346e-06, 7.4903e-06, 4.1158e-06, 4.4689e-06,
        1.9746e-06, 7.1565e-06, 1.9945e-06, 3.3425e-06, 1.2166e-06, 2.9006e-06,
        1.1420e-06, 5.7155e-06, 5.7054e-06, 4.2263e-06, 2.8062e-06, 7.9070e-06,
        3.7803e-06, 3.7984e-06, 3.5649e-06, 3.2207e-06, 4.3403e-06, 1.6422e-06,
        3.2913e-06, 3.5658e-06, 3.5332e-06, 2.0869e-06, 2.7387e-06, 7.2840e-06,
        6.4926e-06, 2.2250e-06, 2.0264e-06, 3.6043e-06, 5.7402e-06, 5.4254e-06,
        1.9746e-06, 4.7443e-06, 1.3568e-06, 9.0487e-06, 4.0360e-06, 2.3741e-06,
        2.1971e-06, 3.6043e-06, 6.3858e-06, 3.0136e-06, 5.2407e-06, 4.5542e-06,
        1.7156e-06, 3.6387e-06, 3.5649e-06, 2.7728e-06, 2.1772e-06, 3.9420e-06,
        4.7302e-06, 3.0877e-06, 2.9592e-06, 2.0357e-06, 7.2845e-06, 3.3621e-06,
        5.4314e-06, 2.8082e-06, 4.3403e-06, 5.2053e-06, 4.7102e-06, 2.6849e-06,
        3.0705e-06, 2.3780e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.2438e-05, 7.8761e-06, 3.5706e-06, 6.8629e-06, 9.7726e-06, 6.1431e-06,
        6.7814e-06, 5.1915e-06, 8.0919e-06, 5.1915e-06, 4.4401e-06, 3.7834e-06,
        6.4150e-06, 8.1808e-06, 8.5311e-06, 6.8535e-06, 6.2368e-06, 1.7721e-05,
        6.1262e-06, 3.6924e-06, 1.6647e-05, 8.0702e-06, 5.9400e-06, 5.1855e-06,
        6.1262e-06, 1.2104e-05, 6.4256e-06, 6.4150e-06, 3.2544e-06, 5.1002e-06,
        1.0436e-05, 7.2566e-07, 7.0522e-06, 2.4784e-05, 2.6574e-06, 7.5209e-06,
        7.2274e-06, 1.7053e-05, 1.8290e-05, 5.1915e-06, 1.4842e-05, 9.1101e-06,
        1.5632e-05, 1.2537e-05, 1.0047e-05, 2.4762e-06, 6.7321e-06, 3.7978e-06,
        1.7300e-05, 9.7354e-06, 7.8761e-06, 1.0391e-05, 1.2417e-05, 5.0842e-06,
        1.0436e-05, 3.5643e-06, 1.0419e-05, 1.5149e-05, 9.2744e-06, 2.5049e-06,
        9.2057e-06, 7.1185e-06, 1.1008e-05, 4.4235e-06, 7.4661e-06, 4.2905e-06,
        5.8194e-06, 1.5590e-05, 7.4235e-06, 5.6523e-06, 7.9803e-06, 1.0480e-05,
        6.5672e-06, 6.5470e-06, 1.8460e-05, 1.0062e-05, 1.0102e-05, 1.2417e-05,
        8.3218e-06, 7.1148e-06, 8.3217e-06, 6.4882e-06, 6.3188e-06, 1.4997e-05,
        2.5760e-06, 7.6371e-06, 7.4856e-06, 1.1994e-05, 8.3203e-06, 1.1150e-05,
        3.4731e-06, 6.4374e-06, 1.3339e-05, 5.6350e-06, 9.1101e-06, 2.1392e-05,
        6.2711e-06, 6.1431e-06, 2.5611e-05, 9.7354e-06, 6.0768e-06, 5.1829e-06,
        9.6411e-06, 4.5638e-06, 7.2031e-06, 7.1185e-06, 5.1915e-06, 1.0332e-05,
        1.2093e-05, 3.7978e-06, 2.6574e-06, 9.1211e-06, 1.0915e-05, 8.0385e-06,
        6.4374e-06, 2.6237e-05, 1.2675e-05, 1.6647e-05, 1.2417e-05, 5.6350e-06,
        1.0630e-05, 9.6411e-06, 1.2675e-05, 1.0130e-05, 8.0791e-06, 7.3362e-06,
        6.8696e-06, 1.5655e-05, 1.0182e-05, 2.9263e-06, 1.0017e-05, 7.8479e-06,
        8.4927e-06, 6.9479e-06, 1.2417e-05, 4.7530e-06, 3.5643e-06, 1.0817e-05,
        4.9888e-06, 8.9339e-06, 5.1915e-06, 5.3060e-06, 3.6924e-06, 6.7814e-06,
        5.1506e-06, 8.7767e-06, 7.1148e-06, 6.1337e-06, 7.4661e-06, 7.5870e-06,
        1.0532e-05, 1.1649e-05, 9.8882e-06, 1.0527e-05, 7.9753e-06, 3.7978e-06,
        5.9400e-06, 5.1855e-06, 4.2906e-06, 1.0130e-05, 5.6766e-06, 6.9479e-06,
        1.0237e-05, 5.9400e-06, 3.1627e-06, 3.7327e-07, 9.1101e-06, 1.6647e-05,
        1.4485e-05, 4.0064e-06, 1.2675e-05, 7.0643e-06, 9.1101e-06, 7.5870e-06,
        8.9578e-06, 1.0558e-05, 5.1915e-06, 6.9447e-06, 8.0919e-06, 6.4468e-06,
        5.5032e-06, 1.7605e-05, 1.3410e-05, 9.5365e-06, 4.6251e-06, 6.1431e-06,
        1.3760e-05, 1.3957e-05, 1.6302e-05, 6.0017e-06, 2.2109e-05, 8.4927e-06,
        1.1432e-05, 4.8392e-06, 1.4461e-05, 8.3203e-06, 4.7495e-06, 1.0184e-05,
        4.3551e-06, 2.3321e-06, 1.3580e-05, 1.3402e-05, 6.4844e-06, 1.2765e-05,
        5.2358e-06, 6.0017e-06, 6.8535e-06, 4.7495e-06, 5.5032e-06, 3.5329e-06,
        5.6350e-06, 1.0329e-05, 1.0838e-05, 1.6647e-05, 1.0792e-06, 7.4893e-06,
        1.6302e-05, 1.3760e-05, 3.7437e-07, 3.7510e-06, 9.6411e-06, 8.6203e-06,
        3.7252e-06, 3.1507e-06, 1.1055e-05, 5.5277e-07, 1.2632e-05, 1.2675e-05,
        8.1596e-06, 6.5988e-06, 7.0522e-06, 1.9539e-05, 7.4616e-06, 1.6358e-05,
        7.7329e-06, 1.6765e-05, 1.1649e-05, 6.9715e-06, 1.4789e-05, 5.5509e-06,
        1.7057e-05, 4.6372e-06, 7.0643e-06, 1.0062e-05, 1.2426e-05, 1.2438e-05,
        1.4484e-05, 3.5706e-06, 8.1910e-06, 9.1211e-06, 8.4927e-06, 1.3826e-05,
        9.6411e-06, 9.8786e-06, 2.6574e-06, 1.2417e-05, 1.3836e-05, 3.8319e-06,
        9.5365e-06, 1.0627e-05, 9.4567e-06, 3.6237e-06, 9.6471e-06, 7.4856e-06,
        2.9655e-06, 1.3224e-05, 1.3242e-05, 2.2114e-05, 3.7942e-06, 7.8001e-06,
        7.0643e-06, 1.0017e-05, 7.4857e-06, 8.0702e-06, 1.1132e-05, 1.5532e-05,
        6.7814e-06, 9.1101e-06, 1.0277e-05, 2.7697e-06, 8.5062e-06, 5.9400e-06,
        9.1029e-06, 7.4713e-06, 7.9753e-06, 9.2689e-06, 6.6508e-06, 8.3203e-06,
        1.5591e-05, 5.6350e-06, 7.7808e-06, 5.6350e-06, 6.6635e-06, 9.4567e-06,
        9.9726e-06, 5.4499e-06, 8.3814e-06, 3.5706e-06, 9.4567e-06, 7.1342e-06,
        7.0643e-06, 1.6069e-05, 8.0919e-06, 1.0838e-05, 6.1431e-06, 1.2765e-05,
        1.2745e-05, 4.0788e-06, 8.5485e-06, 2.0747e-05, 7.4236e-06, 8.5951e-06,
        9.0252e-06, 1.0436e-05, 1.0817e-05, 5.6766e-06, 9.4567e-06, 1.0630e-05,
        1.6759e-05, 3.5706e-06, 8.3203e-06, 2.0166e-05, 1.0062e-05, 5.6350e-06,
        6.5672e-06, 1.1897e-05, 7.4856e-06, 1.3242e-05, 1.2154e-05, 7.4856e-06,
        1.2231e-05, 3.2310e-06, 4.6779e-06, 9.2622e-06, 9.9827e-06, 6.1367e-06,
        1.1055e-05, 6.1431e-06, 8.4917e-06, 1.5310e-05, 4.1921e-06, 9.4567e-06,
        5.9400e-06, 1.3629e-05, 6.0032e-06, 6.6635e-06, 7.4484e-06, 9.4567e-06,
        4.5957e-06, 6.4373e-06, 1.0047e-05, 4.7315e-06, 7.7328e-06, 5.1915e-06,
        8.7829e-06, 6.6508e-06, 1.4239e-05, 8.1910e-06, 1.7106e-05, 8.8058e-06,
        4.0064e-06, 7.3023e-06, 1.4329e-06, 1.3833e-05, 6.7321e-06, 6.0853e-06,
        6.3386e-06, 1.0130e-05, 1.0332e-05, 1.0627e-05, 3.5389e-06, 1.9082e-05,
        8.9434e-06, 1.0436e-05, 2.2984e-05, 1.7519e-05, 7.5630e-06, 2.7055e-06,
        6.7814e-06, 6.2711e-06, 5.1829e-06, 7.4235e-06, 9.8882e-06, 9.1211e-06,
        6.6930e-06, 7.4235e-06, 1.1497e-05, 1.2224e-05, 1.3026e-05, 1.7193e-06,
        1.0532e-05, 4.0064e-06, 3.4498e-06, 7.4746e-06, 4.7668e-06, 5.1915e-06,
        7.6564e-06, 7.5261e-06, 1.0792e-06, 1.0521e-05, 1.4258e-05, 2.1084e-05,
        4.4208e-06, 9.5189e-06, 9.4567e-06, 1.4239e-05, 1.0061e-05, 9.4567e-06,
        9.6411e-06, 6.1431e-06, 5.4751e-06, 4.5482e-06, 6.5673e-06, 3.2069e-06,
        1.4814e-05, 9.2689e-06, 1.5532e-05, 8.8837e-06, 1.1283e-05, 1.0316e-05,
        1.4842e-05, 7.7088e-06, 3.2825e-06, 5.4499e-06, 8.3217e-06, 1.0691e-05,
        5.5502e-06, 1.2007e-05, 1.4227e-05, 1.3173e-05, 3.6924e-06, 5.1002e-06,
        7.4235e-06, 1.4562e-05, 1.0734e-06, 1.3878e-05, 9.7167e-06, 3.3068e-06,
        1.0627e-05, 7.7328e-06, 1.2879e-05, 9.6411e-06, 1.7492e-05, 1.7839e-05,
        1.4562e-05, 1.0838e-05, 1.0958e-05, 8.0791e-06, 2.0747e-05, 8.0919e-06,
        5.1915e-06, 5.6523e-06, 8.4927e-06, 5.7020e-06, 6.1431e-06, 9.6411e-06,
        1.4356e-05, 1.0978e-05, 3.7942e-06, 1.2261e-05, 3.4684e-06, 4.9351e-06,
        1.0521e-05, 5.4390e-06, 9.4567e-06, 4.7371e-06, 4.5957e-06, 9.1676e-06,
        4.7530e-06, 2.4932e-06, 4.7530e-06, 4.7371e-06, 6.6878e-06, 5.4919e-06,
        9.1101e-06, 1.7721e-05, 2.7703e-06, 1.2632e-05, 5.9400e-06, 4.4235e-06,
        6.5561e-06, 1.0958e-05, 1.0281e-05, 8.0919e-06, 6.6122e-06, 8.8490e-06,
        5.0842e-06, 8.6203e-06, 6.1431e-06, 3.1507e-06, 7.8327e-06, 3.4684e-06,
        4.1716e-06, 7.4746e-06, 6.9479e-06, 5.9400e-06, 1.6871e-05, 1.1497e-05,
        1.4239e-05, 1.2224e-05, 9.6411e-06, 4.7371e-06, 1.4531e-05, 5.8708e-06,
        1.4997e-05, 9.4567e-06, 5.6082e-06, 7.3827e-06, 9.4567e-06, 1.5955e-06,
        9.8882e-06, 8.0919e-06], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([8.4379e-07, 1.1303e-06, 6.3569e-07,  ..., 6.9223e-07, 5.1908e-07,
        3.9109e-07], device='cuda:0', grad_fn=<NormBackward1>)

max weight is  tensor([1.3505, 1.2234, 1.0756, 1.0905, 1.3326, 1.2318, 1.8182, 1.4161, 1.4776,
        1.2202, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215, 0.0215,
        0.0215], device='cuda:0', grad_fn=<NormBackward1>)

 sparsity of   [0.0, 0.0, 0.0, 0.1111111119389534, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.03703703731298447, 0.0, 0.03703703731298447, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444477558136, 0.0]

 sparsity of   [0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 1.0, 0.375, 1.0, 0.34375, 1.0, 0.34375, 0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.34375, 0.375, 1.0, 0.375, 1.0, 1.0, 0.375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.34375, 0.34375, 0.34375, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.375]

 sparsity of   [1.0, 1.0, 0.647569477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975694477558136, 0.6458333134651184, 1.0, 1.0, 0.6579861044883728, 1.0, 1.0, 1.0, 1.0, 0.631944477558136, 0.6736111044883728, 0.6458333134651184, 0.6284722089767456, 1.0, 0.6597222089767456, 1.0, 1.0, 1.0, 1.0, 0.6510416865348816, 1.0, 0.6302083134651184, 1.0, 0.6371527910232544, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 0.6493055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6371527910232544, 0.625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6440972089767456]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.71875, 0.734375, 0.71875, 0.734375, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.75, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.6875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.734375, 0.703125, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.6875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.734375, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.71875, 1.0, 0.71875, 1.0, 0.71875, 0.734375, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.671875, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.734375, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 0.75, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 0.734375, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.734375, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 0.6875, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.734375, 0.703125, 0.65625, 0.71875, 1.0, 0.734375, 1.0, 0.6875, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.703125, 0.734375, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.34375, 0.375, 0.34375, 1.0, 1.0, 0.5, 0.34375, 1.0, 0.34375, 0.375, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.375, 1.0, 0.375, 0.34375, 1.0, 0.375, 0.390625, 0.390625, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.34375, 0.40625, 0.359375, 0.359375, 0.4375, 1.0, 0.34375, 0.390625, 0.40625, 0.4375, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.390625, 0.359375, 0.34375, 1.0, 0.40625, 0.34375, 0.34375, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.375, 1.0, 0.5, 0.34375, 0.359375, 0.34375, 0.375, 0.359375, 0.34375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 0.34375, 1.0, 0.34375, 0.359375, 1.0, 0.34375, 0.34375, 1.0, 0.34375, 0.34375, 0.390625, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 0.34375, 0.375, 0.375, 0.375, 0.359375, 1.0, 0.34375, 1.0, 0.484375, 0.34375, 1.0, 1.0, 0.34375, 0.390625, 0.34375, 1.0, 0.359375, 0.34375, 0.359375, 0.34375, 0.375, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.46875, 0.359375, 1.0, 0.375, 0.40625, 0.359375, 1.0, 1.0, 0.375, 0.359375, 1.0, 0.34375, 0.34375, 0.359375, 0.359375, 0.34375, 0.34375, 0.34375, 0.34375, 0.390625, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.390625, 0.40625, 0.34375, 0.390625, 1.0, 0.359375, 0.359375, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.359375, 0.34375, 0.34375, 0.34375, 0.390625, 1.0, 0.34375, 0.375, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.359375, 1.0, 0.359375, 0.34375, 0.390625, 0.375, 1.0, 0.34375, 1.0, 0.359375, 0.625, 1.0, 0.34375, 0.390625, 1.0, 0.34375, 1.0, 1.0, 0.34375, 0.359375, 0.359375, 0.375, 1.0, 0.34375, 0.34375, 0.421875, 0.375, 0.390625, 0.375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 0.390625, 0.34375, 0.375, 0.34375, 1.0, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.34375, 1.0, 0.375, 1.0, 0.359375, 1.0, 1.0, 0.375, 0.34375, 0.375, 0.375, 0.34375, 1.0, 1.0, 0.4375, 0.359375, 0.4375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 0.359375]

 sparsity of   [1.0, 0.3671875, 1.0, 0.35546875, 1.0, 1.0, 0.36328125, 0.35546875, 1.0, 0.359375, 0.3515625, 1.0, 0.34765625, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 0.36328125, 0.3515625, 0.35546875, 0.34765625, 0.359375, 0.36328125, 0.34765625, 1.0, 1.0, 0.359375, 1.0, 0.38671875, 1.0, 1.0, 1.0, 0.35546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35546875, 1.0, 0.35546875, 0.35546875, 0.3671875, 0.359375, 1.0, 1.0, 0.4140625, 1.0, 0.3515625, 0.36328125, 1.0, 0.36328125, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5190972089767456, 1.0, 1.0, 0.53125, 0.522569477558136, 1.0, 1.0, 0.5295138955116272, 0.5347222089767456, 1.0, 1.0, 1.0, 0.538194477558136, 1.0, 0.5086805820465088, 0.5138888955116272, 0.53125, 1.0, 0.5052083134651184, 0.5260416865348816, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5208333134651184, 0.5277777910232544, 0.5399305820465088, 1.0, 0.5416666865348816, 1.0, 0.5277777910232544, 1.0, 1.0, 0.5555555820465088, 0.53125, 0.5190972089767456, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 1.0, 0.5347222089767456, 0.5277777910232544, 0.5190972089767456, 1.0, 1.0]

 sparsity of   [0.59375, 1.0, 1.0, 0.609375, 0.59375, 1.0, 0.578125, 0.609375, 0.59375, 0.609375, 0.609375, 1.0, 0.5625, 0.59375, 1.0, 0.59375, 0.578125, 0.59375, 0.609375, 1.0, 0.609375, 0.59375, 1.0, 0.59375, 0.578125, 1.0, 0.59375, 1.0, 1.0, 0.609375, 0.609375, 0.609375, 0.609375, 1.0, 1.0, 0.5625, 1.0, 0.609375, 0.609375, 0.609375, 0.59375, 0.59375, 1.0, 1.0, 0.609375, 0.609375, 0.609375, 0.609375, 1.0, 0.609375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.609375, 1.0, 1.0, 0.609375, 0.609375, 1.0, 0.609375, 0.609375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.609375, 0.578125, 0.59375, 1.0, 0.609375, 1.0, 0.609375, 0.609375, 0.609375, 1.0, 1.0, 1.0, 0.609375, 0.609375, 1.0, 0.578125, 0.578125, 1.0, 1.0, 0.609375, 0.578125, 0.625, 1.0, 0.578125, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.578125, 0.625, 0.609375, 0.59375, 0.609375, 1.0, 0.59375, 0.625, 0.59375, 0.578125, 0.578125, 1.0, 0.609375, 1.0, 0.578125, 0.59375, 0.609375, 0.578125, 0.609375, 1.0, 0.609375, 0.609375, 1.0, 0.609375, 1.0, 0.609375, 1.0, 0.59375, 0.625, 1.0, 0.59375, 1.0, 0.578125, 0.609375, 0.609375, 1.0, 0.609375, 0.578125, 0.609375, 1.0, 0.609375, 0.609375, 0.578125, 0.59375, 0.59375, 0.578125, 0.578125, 1.0, 0.59375, 1.0, 0.609375, 1.0, 1.0, 0.609375, 0.5625, 1.0, 0.59375, 0.578125, 1.0, 0.609375, 0.609375, 0.59375, 0.609375, 0.609375, 0.609375, 0.578125, 1.0, 0.609375, 0.609375, 1.0, 1.0, 0.625, 0.609375, 0.5625, 0.625, 0.609375, 0.609375, 1.0, 0.578125, 0.59375, 0.609375, 0.578125, 1.0, 1.0, 0.59375, 1.0, 0.578125, 0.578125, 0.609375, 1.0, 0.578125, 1.0, 0.59375, 1.0, 0.609375, 1.0, 0.59375, 0.625, 0.625, 0.609375, 0.59375, 1.0, 1.0, 0.609375, 0.59375, 0.609375, 1.0, 0.59375, 0.578125, 0.609375, 0.609375, 0.609375, 0.59375, 0.609375, 0.625, 0.5625, 0.5625, 1.0, 0.59375, 0.59375, 1.0, 0.609375, 0.609375, 0.609375, 0.59375, 0.609375, 0.578125, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.5625, 0.609375, 0.609375, 0.59375, 0.609375, 0.578125, 1.0, 0.609375, 0.59375, 0.59375, 0.609375, 0.609375, 0.59375, 0.609375, 0.609375, 1.0, 0.578125, 0.609375, 0.578125, 1.0, 1.0, 0.59375, 0.609375, 0.609375, 0.609375, 0.609375]

 sparsity of   [1.0, 0.2109375, 0.203125, 0.20703125, 0.19140625, 0.203125, 0.19921875, 1.0, 1.0, 1.0, 1.0, 0.19140625, 1.0, 0.2109375, 1.0, 0.203125, 0.19921875, 1.0, 1.0, 0.2890625, 1.0, 0.1953125, 0.1953125, 1.0, 1.0, 0.19921875, 1.0, 1.0, 0.1953125, 1.0, 0.1953125, 0.19921875, 1.0, 0.203125, 1.0, 0.19921875, 0.1953125, 0.1953125, 0.28125, 1.0, 0.21484375, 1.0, 1.0, 1.0, 1.0, 0.21875, 0.19921875, 0.19921875, 0.19140625, 0.20703125, 0.21484375, 0.203125, 0.19921875, 0.21484375, 1.0, 1.0, 1.0, 0.19140625, 1.0, 1.0, 1.0, 1.0, 0.203125, 0.19921875]

 sparsity of   [1.0, 0.4340277910232544, 0.4357638955116272, 1.0, 0.4305555522441864, 0.4305555522441864, 1.0, 0.4288194477558136, 0.4322916567325592, 1.0, 1.0, 0.4322916567325592, 0.4305555522441864, 0.4322916567325592, 1.0, 1.0, 0.4288194477558136, 0.4305555522441864, 1.0, 1.0, 1.0, 1.0, 0.4288194477558136, 1.0, 1.0, 0.4340277910232544, 0.4583333432674408, 1.0, 0.425347238779068, 0.4357638955116272, 0.4322916567325592, 1.0, 0.4305555522441864, 0.4270833432674408, 0.4288194477558136, 1.0, 1.0, 0.425347238779068, 0.4270833432674408, 1.0, 1.0, 1.0, 0.4392361044883728, 0.4305555522441864, 1.0, 1.0, 1.0, 1.0, 0.4322916567325592, 0.4322916567325592, 0.4270833432674408, 1.0, 0.456597238779068, 1.0, 0.4479166567325592, 0.4322916567325592, 0.4548611044883728, 0.4288194477558136, 0.4357638955116272, 0.4322916567325592, 0.4288194477558136, 0.4496527910232544, 0.425347238779068, 1.0]

 sparsity of   [0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.4375, 0.4375, 0.421875, 1.0, 0.421875, 1.0, 0.4375, 0.4375, 1.0, 0.421875, 0.453125, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 1.0, 0.421875, 0.4375, 0.421875, 0.421875, 1.0, 0.4375, 0.453125, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.453125, 0.4375, 0.421875, 0.4375, 1.0, 0.4375, 0.4375, 0.421875, 0.4375, 1.0, 0.421875, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 1.0, 0.4375, 0.4375, 0.421875, 1.0, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.4375, 0.4375, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.4375, 0.4375, 1.0, 1.0, 0.4375, 1.0, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.4375, 0.4375, 0.4375, 0.421875, 0.421875, 0.453125, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 1.0, 0.421875, 0.4375, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 1.0, 0.453125, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.4375, 0.4375, 0.421875, 0.421875, 0.4375, 0.4375, 0.421875, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.4375, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.484375, 0.421875]

 sparsity of   [0.05859375, 0.05859375, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.04296875, 0.0390625, 0.0546875, 1.0, 0.04296875, 1.0, 0.04296875, 0.04296875, 1.0, 0.12890625, 0.05859375, 1.0, 0.03515625, 1.0, 0.04296875, 1.0, 1.0, 0.05078125, 1.0, 0.0546875, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.05859375, 1.0, 1.0, 0.1640625, 1.0, 0.046875, 1.0, 0.046875, 0.0546875, 0.046875, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.0390625, 0.0390625, 0.05078125, 0.0546875, 1.0, 1.0, 0.0546875, 1.0, 0.03515625, 0.0546875, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 1.0, 0.04296875, 0.046875, 1.0, 0.0390625, 0.4609375, 1.0, 0.05078125, 0.05859375, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.046875, 0.0625, 0.05859375, 1.0, 1.0, 0.04296875, 0.03515625, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 1.0, 0.06640625, 0.05078125, 0.046875, 1.0, 0.08984375, 1.0, 0.046875, 1.0, 0.06640625, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.05078125, 0.0546875, 0.046875, 1.0, 1.0, 1.0]

 sparsity of   [0.4835069477558136, 0.4809027910232544, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 0.4817708432674408, 1.0, 1.0, 0.4947916567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.487847238779068, 1.0, 1.0, 1.0, 0.487847238779068, 1.0, 0.4791666567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4861111044883728, 0.487847238779068, 1.0, 1.0, 0.4852430522441864, 0.484375, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 0.480034738779068, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4904513955116272, 1.0, 0.487847238779068, 0.4852430522441864, 1.0, 0.4817708432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.4861111044883728, 0.5008680820465088, 1.0, 0.4913194477558136, 1.0, 1.0, 0.484375, 1.0, 1.0, 0.4817708432674408, 1.0, 1.0, 0.4973958432674408, 1.0, 1.0, 1.0, 1.0, 0.4869791567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.487847238779068, 0.4895833432674408, 1.0, 0.487847238779068, 1.0, 0.495659738779068, 1.0, 0.5008680820465088, 1.0, 1.0, 0.480034738779068, 0.4921875, 1.0, 1.0, 1.0, 1.0, 0.4730902910232544, 1.0, 0.484375, 1.0, 0.4852430522441864, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6796875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.703125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6953125, 0.6953125, 0.6953125, 1.0, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.703125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.703125, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0]

 sparsity of   [0.0859375, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 0.05859375, 0.046875, 1.0, 0.046875, 0.046875, 1.0, 0.171875, 0.046875, 1.0, 0.06640625, 1.0, 0.05078125, 0.046875, 1.0, 0.1953125, 1.0, 1.0, 0.04296875, 0.05859375, 0.06640625, 1.0, 1.0, 0.04296875, 0.04296875, 1.0, 0.03515625, 0.05859375, 0.046875, 1.0, 0.05078125, 0.0390625, 0.05078125, 1.0, 0.05859375, 0.08984375, 1.0, 1.0, 1.0, 0.04296875, 0.18359375, 0.046875, 1.0, 0.046875, 0.0625, 1.0, 1.0, 0.05078125, 1.0, 0.046875, 0.05859375, 0.05078125, 0.046875, 0.05078125, 0.03515625, 0.05078125, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 1.0, 0.046875, 0.046875, 1.0, 0.04296875, 1.0, 0.05859375, 0.0546875, 0.05859375, 1.0, 0.06640625, 0.1875, 1.0, 0.046875, 0.046875, 0.078125, 0.05859375, 0.04296875, 0.05078125, 1.0, 1.0, 0.046875, 0.046875, 0.046875, 0.0625, 0.05078125, 0.05078125, 1.0, 0.05078125, 0.046875, 1.0, 0.04296875, 0.046875, 1.0, 0.03515625, 0.05078125, 0.046875, 0.0390625, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05078125, 0.0546875, 0.046875, 1.0, 1.0, 1.0, 0.04296875, 0.0546875, 0.0390625, 0.0390625, 0.046875, 0.05859375, 0.04296875, 1.0, 1.0, 0.0546875, 0.046875, 1.0, 0.05859375, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 0.0546875, 1.0, 0.046875, 1.0, 0.0390625, 0.0546875, 0.046875, 0.04296875, 0.046875, 0.04296875, 0.03125, 1.0, 0.03515625, 1.0, 0.0390625, 0.05078125, 0.04296875, 0.04296875, 0.05859375, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 0.05859375, 0.05859375, 0.16015625, 1.0, 0.046875, 0.05078125, 1.0, 1.0, 0.05078125, 0.05859375, 0.05078125, 0.04296875, 0.05078125, 0.04296875, 1.0, 1.0, 0.09375, 1.0, 0.0703125, 0.046875, 0.046875, 0.0625, 0.0546875, 0.04296875, 0.0546875, 0.0546875, 0.0546875, 1.0, 1.0, 0.05078125, 1.0, 0.0390625, 0.05859375, 0.0546875, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.05859375, 0.078125, 0.04296875, 0.0390625, 0.046875, 1.0, 0.04296875, 0.05078125, 0.0703125, 1.0, 0.0390625, 0.046875, 0.0625, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 0.05078125, 0.046875, 0.0390625, 0.0625, 1.0, 0.1015625, 1.0, 0.0390625, 0.08203125, 1.0, 0.09375, 0.046875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.0546875, 0.04296875, 1.0, 0.04296875, 1.0, 0.0390625, 0.046875, 1.0, 1.0, 0.0625, 1.0, 0.05078125, 0.04296875, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.0703125, 0.1171875, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.05078125, 0.07421875, 0.1171875, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.0390625, 1.0, 0.05078125, 0.05859375, 1.0, 0.05078125, 0.0546875, 0.0546875, 1.0, 0.05859375, 0.0390625, 1.0, 0.04296875, 1.0, 0.0703125, 0.0390625, 0.04296875, 0.0546875, 0.046875, 0.0390625, 0.0546875, 1.0, 1.0, 1.0, 0.04296875, 0.07421875, 0.0546875, 0.05078125, 0.0859375, 1.0, 0.05078125, 0.046875, 0.04296875, 1.0, 1.0, 0.078125, 0.046875, 0.06640625, 1.0, 0.046875, 0.05078125, 0.3046875, 0.0390625, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.05859375, 1.0, 1.0, 0.05078125, 1.0, 0.04296875, 0.05078125, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.05078125, 0.06640625, 0.04296875, 0.04296875, 1.0, 1.0, 0.04296875, 0.0390625, 1.0, 0.04296875, 0.0546875, 0.05859375, 1.0, 0.06640625, 0.078125, 0.0546875, 1.0, 0.04296875, 1.0, 0.05078125, 0.04296875, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.0546875, 0.04296875, 0.0546875, 0.0390625, 1.0, 0.046875, 1.0, 0.05859375, 0.046875, 1.0, 0.078125, 0.05859375, 0.0390625, 1.0, 1.0, 0.0546875, 0.0546875, 0.05078125, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.03515625, 0.0546875, 0.046875, 0.04296875, 0.05078125, 0.05859375, 1.0, 0.1015625, 0.05078125, 0.04296875, 0.13671875, 0.0546875, 0.07421875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.0390625, 1.0, 1.0, 0.046875, 0.0546875, 0.046875, 1.0, 0.05859375, 0.05078125, 0.078125, 1.0, 0.05078125, 0.05078125, 1.0, 0.05078125, 0.140625, 0.06640625, 1.0, 1.0, 0.04296875, 0.1953125, 0.1015625, 0.046875, 0.06640625, 1.0, 0.078125, 0.05078125, 0.125, 1.0, 0.046875, 1.0, 0.0546875, 0.1484375, 0.0703125, 1.0, 0.07421875, 0.05078125, 1.0, 0.0546875, 0.046875, 0.04296875, 0.05078125, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 0.046875, 1.0, 1.0, 0.04296875, 0.0546875, 0.046875, 0.0859375, 0.046875, 1.0, 0.09375, 0.05078125, 1.0, 0.08984375, 0.0390625, 1.0, 1.0, 0.0625, 0.0546875, 0.13671875, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.05078125, 0.0390625, 0.0546875, 0.04296875, 0.05078125, 1.0, 1.0, 0.04296875, 1.0, 0.0546875, 0.0390625, 1.0, 0.04296875, 1.0, 1.0]

 sparsity of   [0.3671875, 1.0, 0.36328125, 0.36328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.365234375, 1.0, 0.357421875, 1.0, 0.349609375, 0.361328125, 1.0, 1.0, 1.0, 0.36328125, 1.0, 0.365234375, 0.359375, 1.0, 0.3515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.36328125, 0.3671875, 1.0, 1.0, 0.36328125, 0.359375, 1.0, 0.357421875, 1.0, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 0.37109375, 0.349609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.361328125, 1.0, 0.365234375, 0.36328125, 1.0, 0.353515625, 1.0, 0.3671875, 1.0, 1.0, 0.359375, 0.357421875, 1.0, 0.35546875, 0.3671875, 1.0, 0.35546875, 1.0, 1.0, 0.357421875, 0.365234375, 1.0, 0.35546875, 0.3515625, 1.0, 1.0, 1.0, 1.0, 0.353515625, 0.361328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.361328125, 0.353515625, 0.361328125, 1.0, 0.365234375, 1.0, 0.36328125, 0.357421875, 0.359375, 0.361328125, 0.3671875, 0.36328125, 0.37890625, 1.0, 1.0, 1.0, 0.3515625, 0.365234375, 1.0, 1.0, 0.34375, 1.0, 1.0, 0.36328125, 0.36328125, 1.0, 1.0, 1.0, 0.3671875, 0.36328125, 0.3671875, 1.0, 1.0, 0.3671875, 1.0]

 sparsity of   [0.5399305820465088, 1.0, 1.0, 0.5373263955116272, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 0.538194477558136, 0.5399305820465088, 1.0, 1.0, 1.0, 0.5407986044883728, 0.538194477558136, 0.5390625, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 0.546875, 1.0, 0.5373263955116272, 0.5364583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 1.0, 1.0, 0.538194477558136, 1.0, 1.0, 0.546875, 1.0, 1.0, 0.5373263955116272, 1.0, 0.5512152910232544, 0.5477430820465088, 1.0, 0.5503472089767456, 1.0, 0.5390625, 0.5477430820465088, 1.0, 1.0, 0.538194477558136, 0.5416666865348816, 0.5512152910232544, 1.0, 1.0, 1.0, 0.5442708134651184, 0.5364583134651184, 0.5451388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5520833134651184, 0.5477430820465088, 0.5434027910232544, 0.5373263955116272, 0.5425347089767456, 0.5607638955116272, 1.0, 1.0, 1.0, 0.5390625, 1.0, 0.5416666865348816, 1.0, 1.0, 0.5416666865348816, 0.5399305820465088, 0.5442708134651184, 0.5434027910232544, 1.0, 0.5442708134651184, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 0.5373263955116272, 1.0, 0.553819477558136, 0.5416666865348816, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 0.546875, 0.5425347089767456, 1.0, 0.5451388955116272, 1.0, 1.0, 0.5407986044883728, 1.0, 0.5598958134651184, 1.0, 1.0, 0.5425347089767456, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0]

 sparsity of   [0.5859375, 0.5625, 1.0, 1.0, 0.5625, 0.5546875, 1.0, 1.0, 0.5625, 0.5546875, 0.5546875, 0.5390625, 0.5625, 1.0, 0.5703125, 0.5625, 1.0, 1.0, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5546875, 1.0, 1.0, 0.5546875, 0.5546875, 0.5390625, 0.5546875, 0.546875, 0.5625, 0.5390625, 1.0, 0.5625, 0.5625, 0.546875, 0.59375, 0.5703125, 0.5390625, 0.5546875, 0.5625, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.5703125, 0.5625, 0.546875, 1.0, 0.5546875, 0.5625, 1.0, 1.0, 0.5625, 1.0, 0.546875, 0.5703125, 0.5625, 0.546875, 0.5625, 0.546875, 0.5625, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 0.5546875, 1.0, 0.5625, 0.5625, 0.546875, 0.578125, 0.5625, 0.5625, 0.5546875, 0.5546875, 1.0, 0.5546875, 0.5703125, 1.0, 0.5625, 0.546875, 0.5625, 0.5546875, 0.5390625, 0.5703125, 0.578125, 1.0, 0.5390625, 0.5625, 0.5703125, 0.5703125, 0.5546875, 0.5546875, 0.5546875, 0.5546875, 0.5625, 1.0, 0.5625, 0.5625, 1.0, 0.5390625, 0.5625, 0.5625, 0.5546875, 0.5625, 0.5625, 1.0, 0.546875, 1.0, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5703125, 0.546875, 0.5546875, 0.5625, 0.5625, 0.578125, 0.5546875, 0.5703125, 1.0, 0.5703125, 0.5625, 0.546875, 1.0, 0.5703125, 0.5625, 0.546875, 0.5625, 0.578125, 1.0, 0.5546875, 0.53125, 1.0, 0.5625, 0.5546875, 0.5625, 0.5546875, 0.5625, 0.578125, 0.546875, 0.5390625, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5625, 0.5625, 0.5625, 0.5625, 0.546875, 0.5625, 0.578125, 1.0, 0.5625, 0.546875, 0.5625, 0.5625, 1.0, 0.5703125, 0.5625, 0.5703125, 1.0, 1.0, 0.5546875, 0.5390625, 0.5625, 0.5625, 0.5390625, 1.0, 1.0, 0.609375, 1.0, 0.578125, 0.5703125, 0.5625, 0.59375, 0.5625, 0.546875, 0.5625, 0.546875, 0.5625, 1.0, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.546875, 1.0, 0.546875, 0.5625, 0.5546875, 1.0, 1.0, 0.5625, 0.5625, 1.0, 0.5625, 1.0, 0.5546875, 0.5625, 0.5390625, 0.546875, 1.0, 0.5546875, 0.5390625, 1.0, 1.0, 0.5703125, 0.5546875, 0.5546875, 0.5625, 0.5625, 1.0, 1.0, 1.0, 0.546875, 0.5703125, 0.5625, 0.5546875, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5703125, 0.5625, 0.5703125, 1.0, 1.0, 0.5625, 1.0, 0.5625, 1.0, 1.0, 0.53125, 0.5546875, 1.0, 0.5703125, 0.5625, 0.578125, 0.546875, 0.5625, 0.5546875, 1.0, 0.5625, 0.5625, 0.5390625, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.546875, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5546875, 0.546875, 1.0, 0.5703125, 0.546875, 1.0, 0.5625, 0.5546875, 0.5703125, 0.5390625, 0.5546875, 1.0, 1.0, 0.5625, 1.0, 1.0, 0.5703125, 0.5390625, 0.5625, 0.546875, 0.5625, 0.546875, 0.5546875, 0.546875, 0.5625, 0.546875, 0.5625, 1.0, 0.5625, 0.546875, 0.5859375, 0.5625, 0.578125, 0.546875, 0.5546875, 0.5625, 0.5625, 1.0, 0.5625, 0.5546875, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 1.0, 0.546875, 0.5625, 0.5703125, 0.5546875, 0.5546875, 1.0, 0.5546875, 0.5546875, 0.5625, 0.5625, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.5625, 0.5546875, 1.0, 1.0, 0.5703125, 1.0, 0.5546875, 0.5625, 1.0, 0.546875, 1.0, 0.5546875, 0.5625, 0.5625, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 0.5390625, 0.5546875, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5546875, 0.5546875, 0.578125, 0.5546875, 0.5625, 0.5546875, 1.0, 0.5546875, 0.5625, 1.0, 0.5390625, 0.5546875, 0.5703125, 0.5625, 0.5703125, 0.5625, 1.0, 0.5703125, 1.0, 1.0, 0.5625, 0.5625, 0.546875, 0.5703125, 0.5625, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5625, 0.5625, 1.0, 1.0, 0.5703125, 0.5625, 0.578125, 0.5625, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 0.5390625, 0.609375, 0.5625, 1.0, 0.5625, 0.5546875, 0.5390625, 0.546875, 0.546875, 0.5703125, 0.5546875, 0.5546875, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5546875, 1.0, 0.5546875, 0.546875, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5625, 0.546875, 0.5625, 0.5546875, 0.5546875, 0.5625, 0.578125, 1.0, 0.5625, 0.5625, 1.0, 1.0, 1.0, 0.5703125, 0.5625, 0.5703125, 0.5546875, 0.5625, 1.0, 0.5703125, 0.5625, 0.5625, 0.5546875, 0.5390625, 0.5625, 0.5703125, 0.5625, 1.0, 1.0, 1.0, 0.5390625, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 0.5625, 1.0, 1.0, 1.0, 0.546875, 1.0, 0.546875, 1.0, 0.5546875, 0.5625, 1.0, 0.5625, 0.5546875, 0.5625, 0.546875, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5625, 0.5546875, 0.5546875, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 1.0, 0.5625, 0.5703125, 0.5703125, 0.546875, 0.546875, 1.0, 1.0, 0.5625, 1.0, 0.5390625, 0.5625, 1.0, 0.5625, 1.0, 1.0]

 sparsity of   [1.0, 0.201171875, 0.197265625, 1.0, 0.197265625, 0.21484375, 0.205078125, 1.0, 1.0, 1.0, 1.0, 0.2109375, 1.0, 1.0, 0.205078125, 0.22265625, 0.20703125, 0.201171875, 1.0, 0.208984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 0.19921875, 1.0, 1.0, 0.212890625, 1.0, 1.0, 0.19921875, 1.0, 1.0, 0.21484375, 1.0, 1.0, 1.0, 1.0, 0.193359375, 0.205078125, 1.0, 1.0, 0.20703125, 0.203125, 0.24609375, 1.0, 1.0, 1.0, 0.21875, 0.197265625, 0.19921875, 1.0, 1.0, 1.0, 0.216796875, 0.2109375, 1.0, 1.0, 1.0, 1.0, 0.193359375, 1.0, 0.203125, 1.0, 1.0, 1.0, 1.0, 0.220703125, 1.0, 0.203125, 0.20703125, 0.205078125, 1.0, 1.0, 0.203125, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.208984375, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 0.1953125, 1.0, 0.205078125, 1.0, 1.0, 0.21484375, 0.205078125, 0.201171875, 0.203125, 1.0, 0.208984375, 1.0, 0.19921875, 1.0, 1.0, 0.201171875, 0.212890625, 0.205078125, 0.22265625, 0.201171875, 1.0, 0.193359375, 1.0, 0.2109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.205078125, 1.0, 1.0, 0.2109375]

 sparsity of   [0.5789930820465088, 1.0, 0.5729166865348816, 0.5711805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 1.0, 1.0, 1.0, 1.0, 0.5763888955116272, 0.5763888955116272, 1.0, 0.5729166865348816, 1.0, 1.0, 0.5729166865348816, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 0.5763888955116272, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 1.0, 0.5763888955116272, 0.577256977558136, 1.0, 0.5763888955116272, 1.0, 0.577256977558136, 1.0, 1.0, 0.5720486044883728, 0.578125, 1.0, 0.5729166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5755208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 0.578125, 1.0, 0.578125, 0.5746527910232544, 1.0, 1.0, 1.0, 0.5763888955116272, 0.5737847089767456, 1.0, 0.5729166865348816, 0.5755208134651184, 1.0, 1.0, 0.5746527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 0.577256977558136, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 0.5737847089767456, 1.0, 0.5798611044883728, 1.0, 1.0, 0.5720486044883728, 1.0, 0.5763888955116272, 0.5737847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5737847089767456, 1.0, 1.0, 1.0, 0.578125, 0.5789930820465088, 0.5798611044883728, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 0.5763888955116272]

 sparsity of   [0.6640625, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 0.6484375, 0.65625, 0.6484375, 0.65625, 1.0, 1.0, 0.6484375, 0.640625, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.671875, 0.6484375, 0.6484375, 0.6484375, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.640625, 0.671875, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6640625, 1.0, 0.640625, 0.640625, 0.640625, 0.65625, 0.65625, 0.6484375, 0.640625, 0.6484375, 0.640625, 0.640625, 0.6640625, 0.640625, 0.640625, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 1.0, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.640625, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.640625, 0.6484375, 1.0, 0.6484375, 0.671875, 0.6484375, 0.6640625, 1.0, 0.65625, 0.65625, 1.0, 0.6484375, 1.0, 0.640625, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.65625, 1.0, 0.6484375, 0.65625, 0.640625, 0.65625, 1.0, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.65625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.7578125, 0.6484375, 0.65625, 0.65625, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.640625, 0.65625, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.65625, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6640625, 1.0, 0.640625, 0.65625, 0.6640625, 1.0, 0.6484375, 1.0, 0.6484375, 0.65625, 0.65625, 0.65625, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.65625, 0.6484375, 0.6640625, 0.65625, 0.65625, 0.65625, 0.6484375, 1.0, 0.65625, 0.65625, 0.640625, 0.6484375, 0.671875, 0.6484375, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.671875, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.640625, 0.6484375, 1.0, 0.75, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 1.0, 0.65625, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.640625, 0.65625, 0.6484375, 0.6484375, 1.0, 1.0, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.65625, 1.0, 0.6484375, 0.640625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6640625, 0.640625, 0.671875, 1.0, 0.65625]

 sparsity of   [0.087890625, 1.0, 0.080078125, 0.0859375, 1.0, 0.078125, 1.0, 0.080078125, 0.0859375, 1.0, 0.08203125, 1.0, 1.0, 0.08203125, 1.0, 0.072265625, 0.087890625, 0.0859375, 0.083984375, 1.0, 1.0, 0.087890625, 0.0703125, 0.07421875, 1.0, 0.07421875, 0.080078125, 1.0, 0.072265625, 0.083984375, 0.091796875, 0.076171875, 0.076171875, 1.0, 0.07421875, 0.078125, 0.083984375, 0.083984375, 0.07421875, 1.0, 1.0, 0.091796875, 1.0, 0.08203125, 0.0703125, 0.0859375, 0.080078125, 0.07421875, 0.076171875, 0.078125, 1.0, 0.080078125, 0.078125, 0.080078125, 0.08203125, 0.087890625, 0.076171875, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.0859375, 0.07421875, 0.08203125, 1.0, 1.0, 0.08203125, 0.08203125, 1.0, 1.0, 0.08984375, 1.0, 1.0, 0.080078125, 0.08984375, 0.08203125, 0.07421875, 0.080078125, 0.083984375, 1.0, 0.07421875, 0.095703125, 1.0, 0.07421875, 0.078125, 0.078125, 0.0703125, 1.0, 0.07421875, 0.072265625, 0.07421875, 0.080078125, 1.0, 0.0859375, 1.0, 0.080078125, 0.080078125, 0.076171875, 0.078125, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 0.08203125, 1.0, 0.080078125, 0.080078125, 0.0859375, 0.080078125, 0.076171875, 0.080078125, 1.0, 0.083984375]

 sparsity of   [0.3611111044883728, 1.0, 1.0, 1.0, 0.3611111044883728, 1.0, 0.3611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3637152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3611111044883728, 0.3619791567325592, 1.0, 0.362847238779068, 0.362847238779068, 0.3637152910232544, 1.0, 0.3611111044883728, 1.0, 0.3645833432674408, 0.3611111044883728, 1.0, 1.0, 0.3637152910232544, 0.362847238779068, 0.3611111044883728, 0.3602430522441864, 1.0, 1.0, 1.0, 0.3611111044883728, 1.0, 1.0, 0.3602430522441864, 1.0, 1.0, 1.0, 0.3619791567325592, 1.0, 0.3602430522441864, 0.3611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3602430522441864, 0.3645833432674408, 1.0, 1.0, 1.0, 0.3645833432674408, 0.3645833432674408, 0.3611111044883728, 0.3637152910232544, 1.0, 0.3637152910232544, 0.359375, 1.0, 1.0, 0.3611111044883728, 0.3645833432674408, 0.3602430522441864, 0.3602430522441864, 0.3602430522441864, 0.362847238779068, 0.359375, 0.3619791567325592, 0.3611111044883728, 0.3654513955116272, 0.3611111044883728, 1.0, 0.359375, 0.3611111044883728, 0.3619791567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.3619791567325592, 1.0, 1.0, 1.0, 0.3611111044883728, 0.3602430522441864, 0.362847238779068, 1.0, 1.0, 0.3619791567325592, 1.0, 1.0, 1.0, 1.0, 0.362847238779068, 1.0, 0.362847238779068, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.3602430522441864, 1.0, 0.3619791567325592, 1.0, 1.0, 0.3602430522441864, 0.3611111044883728, 0.362847238779068, 1.0, 1.0, 0.3611111044883728, 0.3585069477558136, 1.0, 1.0]

 sparsity of   [0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 1.0, 1.0, 0.546875, 0.546875, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5546875, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5546875, 0.5390625, 0.546875, 0.546875, 0.546875, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 1.0, 0.546875, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 0.5546875, 0.546875, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.546875, 0.546875, 0.5390625, 0.546875, 1.0, 0.5546875, 0.5390625, 0.5546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5546875, 0.546875, 1.0, 1.0]

 sparsity of   [0.0546875, 0.05859375, 0.05859375, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.052734375, 1.0, 0.052734375, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.05078125, 0.046875, 0.056640625, 0.05078125, 1.0, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.056640625, 1.0, 0.05078125, 1.0, 0.0546875, 0.052734375, 0.052734375, 0.048828125, 0.05078125, 0.048828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.05078125, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 0.048828125, 0.05859375, 0.060546875, 0.0546875, 1.0, 0.048828125, 1.0, 0.046875, 1.0, 1.0, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 0.052734375, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.0546875, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.048828125, 0.056640625, 0.052734375, 0.0546875, 1.0, 0.0546875, 1.0, 1.0, 0.046875, 1.0, 0.05078125, 0.044921875, 1.0, 0.0546875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.052734375, 1.0, 0.05078125, 0.0546875, 1.0, 1.0, 1.0, 0.0546875, 0.052734375, 0.0546875, 1.0, 0.0546875, 0.046875, 0.0546875, 1.0, 0.05078125, 0.05078125, 0.05859375, 0.12890625, 1.0, 0.046875, 1.0, 0.05859375, 0.05859375, 0.056640625, 0.048828125, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 0.05078125, 1.0, 1.0, 0.05078125, 0.056640625, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.05078125, 1.0, 0.048828125, 1.0, 0.04296875, 0.05078125, 0.048828125, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.048828125, 1.0, 0.048828125, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 0.05078125, 0.046875, 0.048828125, 0.044921875, 1.0, 1.0, 0.05078125, 0.056640625, 1.0, 0.056640625, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.046875, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.05078125, 0.05078125, 0.056640625, 0.052734375, 1.0, 0.052734375, 1.0, 0.052734375, 1.0, 1.0, 0.060546875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.05078125, 0.046875, 0.048828125, 1.0, 0.0625, 1.0, 0.048828125, 1.0, 1.0, 1.0, 1.0, 0.048828125, 0.060546875, 1.0, 0.048828125, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.052734375, 0.044921875, 0.0546875, 0.0546875, 0.0546875, 1.0, 0.068359375, 0.056640625]

 sparsity of   [1.0, 1.0, 1.0, 0.5138888955116272, 1.0, 0.5138888955116272, 0.514756977558136, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5125868320465088, 0.5134548544883728, 1.0, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 0.5134548544883728, 0.5160590410232544, 1.0, 0.5151909589767456, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5130208134651184, 0.5138888955116272, 1.0, 0.5160590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 0.5138888955116272, 1.0, 1.0, 1.0, 0.514756977558136, 0.5138888955116272, 1.0, 1.0, 1.0, 0.514756977558136, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 0.5143229365348816, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 0.5125868320465088, 0.5134548544883728, 1.0, 1.0, 1.0, 1.0, 0.5182291865348816, 0.514756977558136, 0.515625, 1.0, 1.0, 1.0, 0.5130208134651184, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 0.5138888955116272, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 0.5169270634651184, 0.5130208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5130208134651184, 0.5151909589767456, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 0.5138888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 0.5160590410232544, 0.5143229365348816, 0.5169270634651184, 1.0, 0.5134548544883728, 1.0, 1.0, 1.0, 0.5134548544883728, 1.0, 1.0, 0.515625, 1.0, 1.0, 0.515625, 1.0, 0.5134548544883728, 1.0, 1.0, 1.0, 0.5190972089767456, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5121527910232544, 1.0, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5173611044883728, 0.5177951455116272, 0.514756977558136, 0.5130208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 0.5151909589767456, 0.5138888955116272, 1.0, 1.0, 0.514756977558136, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.74609375, 0.74609375, 0.73828125, 0.75390625, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.75, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.75, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.74609375, 0.74609375, 0.74609375, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 1.0, 0.7421875, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 0.75, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.75, 0.75, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 0.73828125, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.75, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.75390625, 0.7421875, 0.73828125, 0.74609375, 1.0, 0.74609375, 0.73828125, 0.74609375, 1.0, 0.74609375, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 0.74609375, 0.75, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.75390625, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 0.73828125, 0.7421875, 0.73828125, 0.7421875, 0.7421875, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.7421875, 0.73828125, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 0.75, 0.75, 1.0, 0.74609375, 0.73828125, 0.7421875, 1.0, 1.0, 0.75, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 0.7421875, 0.73828125, 0.74609375, 0.73828125, 1.0, 0.73828125, 0.7421875, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.73828125, 0.7421875, 0.74609375, 0.74609375, 0.7421875, 0.73828125, 0.7578125, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.7421875, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 0.73828125, 0.7421875, 0.7421875, 1.0, 1.0, 0.75, 1.0, 0.7421875, 0.75, 1.0, 1.0, 0.75, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 0.74609375, 0.73828125, 0.73828125, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.73828125, 0.7421875, 1.0, 0.7421875, 0.7421875, 0.73828125, 1.0, 0.74609375, 1.0, 0.73828125, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.75, 0.73828125, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 0.75, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.75, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.75, 1.0, 0.73828125, 1.0, 1.0, 0.7421875, 0.7578125, 0.75, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 1.0, 0.74609375, 0.75, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 0.74609375, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 0.74609375, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 0.74609375, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 0.75, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.73828125, 0.74609375, 1.0]

 sparsity of   [1.0, 0.0625, 1.0, 0.052734375, 0.060546875, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 0.060546875, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.056640625, 0.068359375, 1.0, 1.0, 0.044921875, 1.0, 1.0, 0.248046875, 0.056640625, 1.0, 1.0, 0.05859375, 0.06640625, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.05859375, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.068359375, 0.064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.05859375, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 0.056640625, 1.0, 0.068359375, 1.0, 0.044921875, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 0.05078125, 1.0, 0.05859375, 1.0, 0.0546875, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.068359375, 0.060546875, 0.056640625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.080078125, 1.0, 0.056640625, 0.056640625, 1.0, 0.052734375, 1.0, 1.0, 1.0, 0.0703125, 1.0, 0.0546875, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 0.05078125, 1.0, 0.064453125, 0.060546875, 0.0546875, 1.0, 1.0, 0.0625, 0.048828125, 1.0, 1.0, 1.0, 0.13671875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.052734375, 0.072265625, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.068359375, 0.0546875, 0.068359375, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.052734375, 1.0, 0.068359375, 0.080078125, 1.0, 0.064453125, 1.0, 0.05859375, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.0625, 0.052734375, 0.0546875, 1.0, 1.0, 0.060546875, 0.064453125, 1.0, 0.064453125, 0.05078125, 0.0546875, 1.0, 0.060546875, 0.056640625, 0.05078125, 1.0, 0.060546875, 0.05859375, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 0.0546875, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.060546875, 1.0, 1.0, 0.05859375, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.06640625, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 0.048828125, 1.0, 0.05859375, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.0546875, 0.0703125, 1.0, 1.0, 0.05859375, 1.0, 0.06640625, 1.0, 0.0546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 0.060546875, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.0625, 1.0, 0.0546875, 1.0, 0.056640625, 0.056640625, 0.052734375, 0.052734375, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.0546875, 0.064453125, 0.056640625, 1.0, 1.0, 0.0625, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.06640625, 1.0, 0.0546875, 1.0, 0.095703125, 0.0546875, 1.0, 0.048828125, 0.0625, 0.05859375, 1.0, 1.0, 0.09375, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.078125, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0625, 0.0546875, 0.056640625, 1.0, 0.05859375, 0.06640625, 0.060546875, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.06640625, 1.0, 1.0, 0.06640625, 0.0546875, 1.0, 0.05078125, 1.0, 0.056640625, 1.0, 1.0, 0.06640625, 1.0, 0.056640625, 1.0, 0.05859375, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.048828125, 1.0, 1.0, 0.0625, 0.056640625, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.05859375, 1.0, 0.0625, 0.09375, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.052734375, 0.064453125, 0.06640625, 0.0703125, 0.07421875, 0.05078125, 0.060546875, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.083984375, 0.05859375, 0.05859375, 0.056640625, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.0703125, 0.05859375, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.048828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 0.060546875, 0.05859375, 0.09765625, 1.0, 1.0, 0.052734375, 1.0, 0.0546875, 0.0625, 1.0, 1.0, 0.056640625, 0.052734375, 1.0, 1.0, 0.05859375, 0.0546875, 1.0, 0.056640625, 0.0546875, 0.060546875, 0.0546875, 0.068359375, 1.0, 1.0, 0.05859375, 1.0, 0.078125, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.056640625, 0.091796875, 1.0, 0.0625, 1.0, 0.056640625, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.056640625, 0.134765625, 1.0, 1.0, 0.060546875, 0.064453125, 1.0, 0.052734375, 1.0, 0.056640625, 0.05078125, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.056640625, 0.060546875, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.056640625, 0.056640625, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.060546875, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.072265625, 0.0546875, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.060546875, 0.06640625, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.064453125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.060546875, 1.0, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.1015625, 1.0, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 1.0, 0.13671875, 1.0, 0.130859375, 1.0, 1.0, 0.076171875, 0.052734375, 1.0, 1.0, 0.0703125, 0.060546875, 1.0, 1.0, 0.064453125, 0.052734375, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.076171875, 0.052734375, 0.0546875, 1.0, 1.0, 0.05859375, 0.064453125, 1.0, 0.056640625, 1.0, 1.0, 0.05859375, 0.14453125, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.09765625, 1.0, 1.0, 1.0, 0.123046875, 0.044921875, 1.0, 1.0, 0.064453125, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.05859375, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.0546875, 0.072265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.095703125, 0.056640625, 1.0, 0.0546875, 1.0, 1.0, 0.064453125, 0.048828125, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.07421875, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.0546875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 0.611328125, 1.0, 1.0, 1.0, 1.0, 0.6162109375, 0.6103515625, 1.0, 1.0, 0.615234375, 1.0, 0.6103515625, 1.0, 0.6103515625, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 0.611328125, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 0.6103515625, 1.0, 1.0, 0.6162109375, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6162109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 1.0, 1.0, 0.6142578125, 0.611328125, 0.607421875, 1.0, 1.0, 1.0, 0.611328125, 0.6103515625, 1.0, 0.6171875, 1.0, 1.0, 1.0, 0.611328125, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.615234375, 1.0, 1.0, 0.609375, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 0.611328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 0.6083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 0.61328125, 0.6103515625, 1.0, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.61328125, 0.60546875, 0.6142578125, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 0.6142578125, 1.0, 1.0, 1.0, 0.6044921875, 0.6015625, 0.6064453125, 1.0, 0.6083984375, 0.615234375, 1.0, 1.0, 0.6103515625, 1.0, 0.6142578125, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.611328125, 1.0, 0.6083984375, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 0.7677951455116272, 1.0, 0.764756977558136, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 0.7660590410232544, 1.0, 0.7677951455116272, 0.7677951455116272, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 0.765625, 1.0, 0.765625, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 0.7638888955116272, 0.7673611044883728, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 0.7634548544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 0.7673611044883728, 0.7664930820465088, 0.7677951455116272, 0.7643229365348816, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7621527910232544, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7604166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7651909589767456, 1.0, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7643229365348816, 1.0, 1.0, 0.76171875, 0.7651909589767456, 1.0, 0.7612847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 0.768663227558136, 0.7651909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7651909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7677951455116272, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 0.765625, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.764756977558136, 0.7660590410232544, 1.0, 0.7651909589767456, 1.0, 1.0, 0.7677951455116272, 0.7660590410232544, 1.0, 1.0, 0.764756977558136, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.764756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0]

 sparsity of   [1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.7578125, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.76171875, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75390625, 1.0, 0.76953125, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.765625, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75390625, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.7578125, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 0.76171875, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75390625, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.7578125, 0.75, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 0.7578125, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.7578125, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.75, 1.0, 0.7578125, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 0.75390625, 1.0, 0.75, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75390625, 0.75, 0.75, 0.75, 0.75, 0.75, 0.76171875, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 0.75390625, 1.0, 0.75390625, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.7578125, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]

 sparsity of   [1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 0.5732421875, 1.0, 0.5673828125, 1.0, 0.576171875, 0.572265625, 1.0, 0.572265625, 0.5693359375, 0.5654296875, 0.5732421875, 0.5732421875, 1.0, 0.568359375, 0.572265625, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 0.564453125, 1.0, 0.576171875, 1.0, 0.5615234375, 0.56640625, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 0.57421875, 0.5771484375, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 0.5703125, 0.5830078125, 1.0, 1.0, 0.572265625, 1.0, 0.568359375, 1.0, 0.572265625, 0.56640625, 1.0, 0.568359375, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 0.564453125, 1.0, 1.0, 0.56640625, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.564453125, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 0.56640625, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.57421875, 1.0, 1.0, 1.0, 0.5732421875, 0.568359375, 1.0, 1.0, 1.0, 1.0, 0.5634765625, 1.0, 0.5673828125, 0.572265625, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 0.5751953125, 0.57421875, 0.5693359375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.572265625, 0.5712890625, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 0.58203125, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 0.5693359375, 0.5625, 0.576171875, 0.5732421875, 1.0, 1.0, 0.5712890625, 0.5634765625, 1.0, 0.568359375, 1.0, 1.0, 0.5693359375, 1.0, 1.0, 1.0, 1.0, 0.568359375, 0.5654296875, 0.57421875, 1.0, 0.5673828125, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.564453125, 1.0, 0.5732421875, 1.0, 0.5703125, 0.568359375, 1.0, 0.572265625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 0.6618923544883728, 1.0, 0.6653645634651184, 1.0, 0.6636284589767456, 0.6605902910232544, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6610243320465088, 0.6627604365348816, 1.0, 0.6623263955116272, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6644965410232544, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6605902910232544, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 0.6610243320465088, 1.0, 1.0, 0.6618923544883728, 0.6614583134651184, 0.6623263955116272, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6623263955116272, 0.6610243320465088, 1.0, 0.663194477558136, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 0.6627604365348816, 1.0, 0.6618923544883728, 1.0, 1.0, 0.6605902910232544, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6605902910232544, 1.0, 1.0, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 0.6605902910232544, 1.0, 0.6610243320465088, 1.0, 0.6627604365348816, 1.0, 0.6610243320465088, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.6636284589767456, 0.6610243320465088, 1.0, 0.6605902910232544, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 0.6623263955116272, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6610243320465088, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.6605902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 0.6605902910232544, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6618923544883728, 0.6623263955116272, 0.6610243320465088, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.75, 1.0, 1.0, 0.74609375, 0.7421875, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7578125, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.74609375, 1.0, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 0.74609375, 0.74609375, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 0.7421875, 1.0, 0.75, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.75390625, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9592013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5615234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5576171875, 1.0, 1.0, 0.548828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5478515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6181640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62109375, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.828125, 0.82763671875, 0.82763671875, 0.82861328125, 0.82763671875, 0.8291015625, 0.82861328125, 0.828125, 0.82763671875, 0.82861328125, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]

Total parameter pruned: 22672756.005090863 (unstructured) 21537042 (structured)

Test: [0/79]	Time 0.187 (0.187)	Loss 0.2488 (0.2488) ([0.136]+[0.113])	Prec@1 94.531 (94.531)
 * Prec@1 93.830

 Total elapsed time  3:56:29.858186 
 FINETUNING


 sparsity of   [0.0, 0.0, 0.0, 0.1111111119389534, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.03703703731298447, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03703703731298447, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.03703703731298447, 0.0, 0.03703703731298447, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444477558136, 0.0]

 sparsity of   [0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.375, 1.0, 1.0, 1.0, 0.375, 1.0, 0.34375, 1.0, 0.34375, 0.34375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.34375, 0.375, 1.0, 0.375, 1.0, 1.0, 0.375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.34375, 0.34375, 0.34375, 1.0, 0.390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.375]

 sparsity of   [1.0, 1.0, 0.647569477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975694477558136, 0.6458333134651184, 1.0, 1.0, 0.6579861044883728, 1.0, 1.0, 1.0, 1.0, 0.631944477558136, 0.6736111044883728, 0.6458333134651184, 0.6284722089767456, 1.0, 0.6597222089767456, 1.0, 1.0, 1.0, 1.0, 0.6510416865348816, 1.0, 0.6302083134651184, 1.0, 0.6371527910232544, 1.0, 1.0, 1.0, 1.0, 0.6354166865348816, 1.0, 0.6493055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6371527910232544, 0.625, 1.0, 0.640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6440972089767456]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.734375, 1.0, 0.71875, 0.734375, 0.71875, 0.734375, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.75, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.6875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.734375, 0.703125, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.6875, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.734375, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.734375, 0.71875, 1.0, 0.71875, 1.0, 0.71875, 0.734375, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 0.734375, 0.71875, 0.71875, 0.71875, 1.0, 0.671875, 1.0, 1.0, 1.0, 1.0, 0.703125, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.671875, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 0.734375, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 0.75, 1.0, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 0.71875, 0.734375, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.734375, 1.0, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.71875, 0.71875, 0.71875, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875, 0.6875, 0.71875, 0.71875, 1.0, 1.0, 0.71875, 1.0, 0.734375, 0.703125, 0.65625, 0.71875, 1.0, 0.734375, 1.0, 0.6875, 1.0, 1.0, 0.71875, 0.703125, 0.71875, 1.0, 0.71875, 1.0, 1.0, 0.703125, 0.734375, 0.71875, 1.0, 1.0, 0.71875, 0.71875, 1.0, 0.71875]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.34375, 0.375, 0.34375, 1.0, 1.0, 0.5, 0.34375, 1.0, 0.34375, 0.375, 1.0, 0.359375, 0.359375, 1.0, 1.0, 0.375, 1.0, 0.375, 0.34375, 1.0, 0.375, 0.390625, 0.390625, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.34375, 0.40625, 0.359375, 0.359375, 0.4375, 1.0, 0.34375, 0.390625, 0.40625, 0.4375, 1.0, 0.359375, 1.0, 0.34375, 1.0, 0.390625, 0.359375, 0.34375, 1.0, 0.40625, 0.34375, 0.34375, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.375, 1.0, 0.5, 0.34375, 0.359375, 0.34375, 0.375, 0.359375, 0.34375, 1.0, 1.0, 0.359375, 0.359375, 1.0, 0.34375, 1.0, 0.34375, 0.359375, 1.0, 0.34375, 0.34375, 1.0, 0.34375, 0.34375, 0.390625, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 1.0, 0.34375, 0.375, 0.375, 0.375, 0.359375, 1.0, 0.34375, 1.0, 0.484375, 0.34375, 1.0, 1.0, 0.34375, 0.390625, 0.34375, 1.0, 0.359375, 0.34375, 0.359375, 0.34375, 0.375, 0.34375, 0.359375, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 1.0, 1.0, 0.46875, 0.359375, 1.0, 0.375, 0.40625, 0.359375, 1.0, 1.0, 0.375, 0.359375, 1.0, 0.34375, 0.34375, 0.359375, 0.359375, 0.34375, 0.34375, 0.34375, 0.34375, 0.390625, 1.0, 1.0, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.390625, 0.40625, 0.34375, 0.390625, 1.0, 0.359375, 0.359375, 1.0, 0.359375, 0.375, 1.0, 1.0, 0.359375, 0.359375, 0.34375, 0.34375, 0.34375, 0.390625, 1.0, 0.34375, 0.375, 1.0, 1.0, 0.34375, 1.0, 0.359375, 0.359375, 1.0, 0.359375, 0.34375, 0.390625, 0.375, 1.0, 0.34375, 1.0, 0.359375, 0.625, 1.0, 0.34375, 0.390625, 1.0, 0.34375, 1.0, 1.0, 0.34375, 0.359375, 0.359375, 0.375, 1.0, 0.34375, 0.34375, 0.421875, 0.375, 0.390625, 0.375, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.34375, 0.34375, 0.34375, 0.390625, 0.34375, 0.375, 0.34375, 1.0, 1.0, 0.359375, 1.0, 0.34375, 0.34375, 0.34375, 0.34375, 1.0, 0.375, 1.0, 0.359375, 1.0, 1.0, 0.375, 0.34375, 0.375, 0.375, 0.34375, 1.0, 1.0, 0.4375, 0.359375, 0.4375, 1.0, 1.0, 0.34375, 0.359375, 1.0, 0.359375]

 sparsity of   [1.0, 0.3671875, 1.0, 0.35546875, 1.0, 1.0, 0.36328125, 0.35546875, 1.0, 0.359375, 0.3515625, 1.0, 0.34765625, 1.0, 1.0, 0.359375, 0.359375, 1.0, 1.0, 1.0, 0.36328125, 0.3515625, 0.35546875, 0.34765625, 0.359375, 0.36328125, 0.34765625, 1.0, 1.0, 0.359375, 1.0, 0.38671875, 1.0, 1.0, 1.0, 0.35546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35546875, 1.0, 0.35546875, 0.35546875, 0.3671875, 0.359375, 1.0, 1.0, 0.4140625, 1.0, 0.3515625, 0.36328125, 1.0, 0.36328125, 1.0, 1.0, 1.0, 0.359375, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5190972089767456, 1.0, 1.0, 0.53125, 0.522569477558136, 1.0, 1.0, 0.5295138955116272, 0.5347222089767456, 1.0, 1.0, 1.0, 0.538194477558136, 1.0, 0.5086805820465088, 0.5138888955116272, 0.53125, 1.0, 0.5052083134651184, 0.5260416865348816, 0.5243055820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5208333134651184, 0.5277777910232544, 0.5399305820465088, 1.0, 0.5416666865348816, 1.0, 0.5277777910232544, 1.0, 1.0, 0.5555555820465088, 0.53125, 0.5190972089767456, 1.0, 1.0, 1.0, 1.0, 0.5416666865348816, 1.0, 0.5347222089767456, 0.5277777910232544, 0.5190972089767456, 1.0, 1.0]

 sparsity of   [0.59375, 1.0, 1.0, 0.609375, 0.59375, 1.0, 0.578125, 0.609375, 0.59375, 0.609375, 0.609375, 1.0, 0.5625, 0.59375, 1.0, 0.59375, 0.578125, 0.59375, 0.609375, 1.0, 0.609375, 0.59375, 1.0, 0.59375, 0.578125, 1.0, 0.59375, 1.0, 1.0, 0.609375, 0.609375, 0.609375, 0.609375, 1.0, 1.0, 0.5625, 1.0, 0.609375, 0.609375, 0.609375, 0.59375, 0.59375, 1.0, 1.0, 0.609375, 0.609375, 0.609375, 0.609375, 1.0, 0.609375, 1.0, 0.59375, 0.609375, 1.0, 0.59375, 0.609375, 1.0, 1.0, 0.609375, 0.609375, 1.0, 0.609375, 0.609375, 0.59375, 0.59375, 1.0, 0.59375, 0.59375, 0.59375, 0.609375, 0.578125, 0.59375, 1.0, 0.609375, 1.0, 0.609375, 0.609375, 0.609375, 1.0, 1.0, 1.0, 0.609375, 0.609375, 1.0, 0.578125, 0.578125, 1.0, 1.0, 0.609375, 0.578125, 0.625, 1.0, 0.578125, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.578125, 0.625, 0.609375, 0.59375, 0.609375, 1.0, 0.59375, 0.625, 0.59375, 0.578125, 0.578125, 1.0, 0.609375, 1.0, 0.578125, 0.59375, 0.609375, 0.578125, 0.609375, 1.0, 0.609375, 0.609375, 1.0, 0.609375, 1.0, 0.609375, 1.0, 0.59375, 0.625, 1.0, 0.59375, 1.0, 0.578125, 0.609375, 0.609375, 1.0, 0.609375, 0.578125, 0.609375, 1.0, 0.609375, 0.609375, 0.578125, 0.59375, 0.59375, 0.578125, 0.578125, 1.0, 0.59375, 1.0, 0.609375, 1.0, 1.0, 0.609375, 0.5625, 1.0, 0.59375, 0.578125, 1.0, 0.609375, 0.609375, 0.59375, 0.609375, 0.609375, 0.609375, 0.578125, 1.0, 0.609375, 0.609375, 1.0, 1.0, 0.625, 0.609375, 0.5625, 0.625, 0.609375, 0.609375, 1.0, 0.578125, 0.59375, 0.609375, 0.578125, 1.0, 1.0, 0.59375, 1.0, 0.578125, 0.578125, 0.609375, 1.0, 0.578125, 1.0, 0.59375, 1.0, 0.609375, 1.0, 0.59375, 0.625, 0.625, 0.609375, 0.59375, 1.0, 1.0, 0.609375, 0.59375, 0.609375, 1.0, 0.59375, 0.578125, 0.609375, 0.609375, 0.609375, 0.59375, 0.609375, 0.625, 0.5625, 0.5625, 1.0, 0.59375, 0.59375, 1.0, 0.609375, 0.609375, 0.609375, 0.59375, 0.609375, 0.578125, 0.609375, 1.0, 1.0, 0.59375, 0.59375, 0.5625, 0.609375, 0.609375, 0.59375, 0.609375, 0.578125, 1.0, 0.609375, 0.59375, 0.59375, 0.609375, 0.609375, 0.59375, 0.609375, 0.609375, 1.0, 0.578125, 0.609375, 0.578125, 1.0, 1.0, 0.59375, 0.609375, 0.609375, 0.609375, 0.609375]

 sparsity of   [1.0, 0.2109375, 0.203125, 0.20703125, 0.19140625, 0.203125, 0.19921875, 1.0, 1.0, 1.0, 1.0, 0.19140625, 1.0, 0.2109375, 1.0, 0.203125, 0.19921875, 1.0, 1.0, 0.2890625, 1.0, 0.1953125, 0.1953125, 1.0, 1.0, 0.19921875, 1.0, 1.0, 0.1953125, 1.0, 0.1953125, 0.19921875, 1.0, 0.203125, 1.0, 0.19921875, 0.1953125, 0.1953125, 0.28125, 1.0, 0.21484375, 1.0, 1.0, 1.0, 1.0, 0.21875, 0.19921875, 0.19921875, 0.19140625, 0.20703125, 0.21484375, 0.203125, 0.19921875, 0.21484375, 1.0, 1.0, 1.0, 0.19140625, 1.0, 1.0, 1.0, 1.0, 0.203125, 0.19921875]

 sparsity of   [1.0, 0.4340277910232544, 0.4357638955116272, 1.0, 0.4305555522441864, 0.4305555522441864, 1.0, 0.4288194477558136, 0.4322916567325592, 1.0, 1.0, 0.4322916567325592, 0.4305555522441864, 0.4322916567325592, 1.0, 1.0, 0.4288194477558136, 0.4305555522441864, 1.0, 1.0, 1.0, 1.0, 0.4288194477558136, 1.0, 1.0, 0.4340277910232544, 0.4583333432674408, 1.0, 0.425347238779068, 0.4357638955116272, 0.4322916567325592, 1.0, 0.4305555522441864, 0.4270833432674408, 0.4288194477558136, 1.0, 1.0, 0.425347238779068, 0.4270833432674408, 1.0, 1.0, 1.0, 0.4392361044883728, 0.4305555522441864, 1.0, 1.0, 1.0, 1.0, 0.4322916567325592, 0.4322916567325592, 0.4270833432674408, 1.0, 0.456597238779068, 1.0, 0.4479166567325592, 0.4322916567325592, 0.4548611044883728, 0.4288194477558136, 0.4357638955116272, 0.4322916567325592, 0.4288194477558136, 0.4496527910232544, 0.425347238779068, 1.0]

 sparsity of   [0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.4375, 0.4375, 0.421875, 1.0, 0.421875, 1.0, 0.4375, 0.4375, 1.0, 0.421875, 0.453125, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 1.0, 0.421875, 0.4375, 0.421875, 0.421875, 1.0, 0.4375, 0.453125, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.453125, 0.4375, 0.421875, 0.4375, 1.0, 0.4375, 0.4375, 0.421875, 0.4375, 1.0, 0.421875, 0.421875, 1.0, 1.0, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 1.0, 0.4375, 0.4375, 0.421875, 1.0, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.4375, 0.4375, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.4375, 0.4375, 1.0, 1.0, 0.4375, 1.0, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.4375, 0.4375, 0.4375, 0.421875, 0.421875, 0.453125, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 1.0, 0.421875, 0.4375, 0.453125, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 1.0, 0.453125, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.4375, 0.4375, 0.421875, 0.421875, 0.4375, 0.4375, 0.421875, 0.4375, 1.0, 1.0, 0.4375, 0.4375, 1.0, 0.421875, 0.421875, 1.0, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 1.0, 0.421875, 0.421875, 0.421875, 0.421875, 0.4375, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.4375, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.421875, 0.453125, 0.421875, 0.484375, 0.421875]

 sparsity of   [0.05859375, 0.05859375, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.04296875, 0.0390625, 0.0546875, 1.0, 0.04296875, 1.0, 0.04296875, 0.04296875, 1.0, 0.12890625, 0.05859375, 1.0, 0.03515625, 1.0, 0.04296875, 1.0, 1.0, 0.05078125, 1.0, 0.0546875, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.05859375, 1.0, 1.0, 0.1640625, 1.0, 0.046875, 1.0, 0.046875, 0.0546875, 0.046875, 0.03515625, 1.0, 1.0, 1.0, 0.04296875, 0.0390625, 0.0390625, 0.05078125, 0.0546875, 1.0, 1.0, 0.0546875, 1.0, 0.03515625, 0.0546875, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 1.0, 0.04296875, 0.046875, 1.0, 0.0390625, 0.4609375, 1.0, 0.05078125, 0.05859375, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.046875, 0.0625, 0.05859375, 1.0, 1.0, 0.04296875, 0.03515625, 1.0, 1.0, 0.04296875, 1.0, 0.046875, 1.0, 0.06640625, 0.05078125, 0.046875, 1.0, 0.08984375, 1.0, 0.046875, 1.0, 0.06640625, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.05078125, 0.0546875, 0.046875, 1.0, 1.0, 1.0]

 sparsity of   [0.4835069477558136, 0.4809027910232544, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 1.0, 1.0, 1.0, 1.0, 0.4817708432674408, 1.0, 1.0, 0.4947916567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.487847238779068, 1.0, 1.0, 1.0, 0.487847238779068, 1.0, 0.4791666567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4861111044883728, 0.487847238779068, 1.0, 1.0, 0.4852430522441864, 0.484375, 1.0, 0.5008680820465088, 1.0, 1.0, 1.0, 0.480034738779068, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4904513955116272, 1.0, 0.487847238779068, 0.4852430522441864, 1.0, 0.4817708432674408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4913194477558136, 1.0, 1.0, 0.484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.484375, 0.4861111044883728, 0.5008680820465088, 1.0, 0.4913194477558136, 1.0, 1.0, 0.484375, 1.0, 1.0, 0.4817708432674408, 1.0, 1.0, 0.4973958432674408, 1.0, 1.0, 1.0, 1.0, 0.4869791567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.487847238779068, 0.4895833432674408, 1.0, 0.487847238779068, 1.0, 0.495659738779068, 1.0, 0.5008680820465088, 1.0, 1.0, 0.480034738779068, 0.4921875, 1.0, 1.0, 1.0, 1.0, 0.4730902910232544, 1.0, 0.484375, 1.0, 0.4852430522441864, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6953125, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6796875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 0.6875, 0.6953125, 0.6953125, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.703125, 0.6953125, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.703125, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6953125, 0.6953125, 0.6953125, 1.0, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 0.6953125, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.703125, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.703125, 0.6875, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 1.0, 1.0, 0.6953125, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6796875, 0.6953125, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 1.0, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 1.0, 0.6875, 0.6953125, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.703125, 1.0, 0.6875, 1.0, 0.6875, 1.0, 0.6953125, 0.6875, 0.6875, 0.6953125, 0.6875, 1.0, 1.0, 0.6875, 1.0, 0.6875, 0.6875, 1.0, 0.6875, 0.6875, 1.0]

 sparsity of   [0.0859375, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 0.05859375, 0.046875, 1.0, 0.046875, 0.046875, 1.0, 0.171875, 0.046875, 1.0, 0.06640625, 1.0, 0.05078125, 0.046875, 1.0, 0.1953125, 1.0, 1.0, 0.04296875, 0.05859375, 0.06640625, 1.0, 1.0, 0.04296875, 0.04296875, 1.0, 0.03515625, 0.05859375, 0.046875, 1.0, 0.05078125, 0.0390625, 0.05078125, 1.0, 0.05859375, 0.08984375, 1.0, 1.0, 1.0, 0.04296875, 0.18359375, 0.046875, 1.0, 0.046875, 0.0625, 1.0, 1.0, 0.05078125, 1.0, 0.046875, 0.05859375, 0.05078125, 0.046875, 0.05078125, 0.03515625, 0.05078125, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0390625, 0.04296875, 1.0, 0.046875, 0.046875, 1.0, 0.04296875, 1.0, 0.05859375, 0.0546875, 0.05859375, 1.0, 0.06640625, 0.1875, 1.0, 0.046875, 0.046875, 0.078125, 0.05859375, 0.04296875, 0.05078125, 1.0, 1.0, 0.046875, 0.046875, 0.046875, 0.0625, 0.05078125, 0.05078125, 1.0, 0.05078125, 0.046875, 1.0, 0.04296875, 0.046875, 1.0, 0.03515625, 0.05078125, 0.046875, 0.0390625, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05078125, 0.0546875, 0.046875, 1.0, 1.0, 1.0, 0.04296875, 0.0546875, 0.0390625, 0.0390625, 0.046875, 0.05859375, 0.04296875, 1.0, 1.0, 0.0546875, 0.046875, 1.0, 0.05859375, 1.0, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 0.0546875, 1.0, 0.046875, 1.0, 0.0390625, 0.0546875, 0.046875, 0.04296875, 0.046875, 0.04296875, 0.03125, 1.0, 0.03515625, 1.0, 0.0390625, 0.05078125, 0.04296875, 0.04296875, 0.05859375, 0.05078125, 1.0, 1.0, 1.0, 0.046875, 0.05859375, 0.05859375, 0.16015625, 1.0, 0.046875, 0.05078125, 1.0, 1.0, 0.05078125, 0.05859375, 0.05078125, 0.04296875, 0.05078125, 0.04296875, 1.0, 1.0, 0.09375, 1.0, 0.0703125, 0.046875, 0.046875, 0.0625, 0.0546875, 0.04296875, 0.0546875, 0.0546875, 0.0546875, 1.0, 1.0, 0.05078125, 1.0, 0.0390625, 0.05859375, 0.0546875, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.04296875, 0.05859375, 0.078125, 0.04296875, 0.0390625, 0.046875, 1.0, 0.04296875, 0.05078125, 0.0703125, 1.0, 0.0390625, 0.046875, 0.0625, 1.0, 0.04296875, 1.0, 1.0, 1.0, 1.0, 0.0390625, 1.0, 0.05078125, 0.046875, 0.0390625, 0.0625, 1.0, 0.1015625, 1.0, 0.0390625, 0.08203125, 1.0, 0.09375, 0.046875, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.078125, 1.0, 0.0546875, 0.04296875, 1.0, 0.04296875, 1.0, 0.0390625, 0.046875, 1.0, 1.0, 0.0625, 1.0, 0.05078125, 0.04296875, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.0703125, 0.1171875, 0.046875, 1.0, 0.04296875, 1.0, 1.0, 0.046875, 0.05078125, 0.07421875, 0.1171875, 0.05078125, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.0390625, 1.0, 0.05078125, 0.05859375, 1.0, 0.05078125, 0.0546875, 0.0546875, 1.0, 0.05859375, 0.0390625, 1.0, 0.04296875, 1.0, 0.0703125, 0.0390625, 0.04296875, 0.0546875, 0.046875, 0.0390625, 0.0546875, 1.0, 1.0, 1.0, 0.04296875, 0.07421875, 0.0546875, 0.05078125, 0.0859375, 1.0, 0.05078125, 0.046875, 0.04296875, 1.0, 1.0, 0.078125, 0.046875, 0.06640625, 1.0, 0.046875, 0.05078125, 0.3046875, 0.0390625, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.05859375, 1.0, 1.0, 0.05078125, 1.0, 0.04296875, 0.05078125, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.05078125, 0.06640625, 0.04296875, 0.04296875, 1.0, 1.0, 0.04296875, 0.0390625, 1.0, 0.04296875, 0.0546875, 0.05859375, 1.0, 0.06640625, 0.078125, 0.0546875, 1.0, 0.04296875, 1.0, 0.05078125, 0.04296875, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.0546875, 0.04296875, 0.0546875, 0.0390625, 1.0, 0.046875, 1.0, 0.05859375, 0.046875, 1.0, 0.078125, 0.05859375, 0.0390625, 1.0, 1.0, 0.0546875, 0.0546875, 0.05078125, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.03515625, 0.0546875, 0.046875, 0.04296875, 0.05078125, 0.05859375, 1.0, 0.1015625, 0.05078125, 0.04296875, 0.13671875, 0.0546875, 0.07421875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.0390625, 1.0, 1.0, 0.046875, 0.0546875, 0.046875, 1.0, 0.05859375, 0.05078125, 0.078125, 1.0, 0.05078125, 0.05078125, 1.0, 0.05078125, 0.140625, 0.06640625, 1.0, 1.0, 0.04296875, 0.1953125, 0.1015625, 0.046875, 0.06640625, 1.0, 0.078125, 0.05078125, 0.125, 1.0, 0.046875, 1.0, 0.0546875, 0.1484375, 0.0703125, 1.0, 0.07421875, 0.05078125, 1.0, 0.0546875, 0.046875, 0.04296875, 0.05078125, 0.04296875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 0.046875, 0.046875, 1.0, 1.0, 0.04296875, 0.0546875, 0.046875, 0.0859375, 0.046875, 1.0, 0.09375, 0.05078125, 1.0, 0.08984375, 0.0390625, 1.0, 1.0, 0.0625, 0.0546875, 0.13671875, 1.0, 1.0, 0.046875, 1.0, 1.0, 1.0, 0.04296875, 1.0, 0.05078125, 0.0390625, 0.0546875, 0.04296875, 0.05078125, 1.0, 1.0, 0.04296875, 1.0, 0.0546875, 0.0390625, 1.0, 0.04296875, 1.0, 1.0]

 sparsity of   [0.3671875, 1.0, 0.36328125, 0.36328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.365234375, 1.0, 0.357421875, 1.0, 0.349609375, 0.361328125, 1.0, 1.0, 1.0, 0.36328125, 1.0, 0.365234375, 0.359375, 1.0, 0.3515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.36328125, 0.3671875, 1.0, 1.0, 0.36328125, 0.359375, 1.0, 0.357421875, 1.0, 0.359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.388671875, 1.0, 1.0, 0.37109375, 0.349609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.361328125, 1.0, 0.365234375, 0.36328125, 1.0, 0.353515625, 1.0, 0.3671875, 1.0, 1.0, 0.359375, 0.357421875, 1.0, 0.35546875, 0.3671875, 1.0, 0.35546875, 1.0, 1.0, 0.357421875, 0.365234375, 1.0, 0.35546875, 0.3515625, 1.0, 1.0, 1.0, 1.0, 0.353515625, 0.361328125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.361328125, 0.353515625, 0.361328125, 1.0, 0.365234375, 1.0, 0.36328125, 0.357421875, 0.359375, 0.361328125, 0.3671875, 0.36328125, 0.37890625, 1.0, 1.0, 1.0, 0.3515625, 0.365234375, 1.0, 1.0, 0.34375, 1.0, 1.0, 0.36328125, 0.36328125, 1.0, 1.0, 1.0, 0.3671875, 0.36328125, 0.3671875, 1.0, 1.0, 0.3671875, 1.0]

 sparsity of   [0.5399305820465088, 1.0, 1.0, 0.5373263955116272, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 0.538194477558136, 0.5399305820465088, 1.0, 1.0, 1.0, 0.5407986044883728, 0.538194477558136, 0.5390625, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5373263955116272, 0.546875, 1.0, 0.5373263955116272, 0.5364583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5434027910232544, 1.0, 1.0, 0.538194477558136, 1.0, 1.0, 0.546875, 1.0, 1.0, 0.5373263955116272, 1.0, 0.5512152910232544, 0.5477430820465088, 1.0, 0.5503472089767456, 1.0, 0.5390625, 0.5477430820465088, 1.0, 1.0, 0.538194477558136, 0.5416666865348816, 0.5512152910232544, 1.0, 1.0, 1.0, 0.5442708134651184, 0.5364583134651184, 0.5451388955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5520833134651184, 0.5477430820465088, 0.5434027910232544, 0.5373263955116272, 0.5425347089767456, 0.5607638955116272, 1.0, 1.0, 1.0, 0.5390625, 1.0, 0.5416666865348816, 1.0, 1.0, 0.5416666865348816, 0.5399305820465088, 0.5442708134651184, 0.5434027910232544, 1.0, 0.5442708134651184, 0.5399305820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 0.5373263955116272, 1.0, 0.553819477558136, 0.5416666865348816, 1.0, 0.5598958134651184, 1.0, 1.0, 1.0, 1.0, 0.5407986044883728, 0.546875, 0.5425347089767456, 1.0, 0.5451388955116272, 1.0, 1.0, 0.5407986044883728, 1.0, 0.5598958134651184, 1.0, 1.0, 0.5425347089767456, 1.0, 0.5399305820465088, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0]

 sparsity of   [0.5859375, 0.5625, 1.0, 1.0, 0.5625, 0.5546875, 1.0, 1.0, 0.5625, 0.5546875, 0.5546875, 0.5390625, 0.5625, 1.0, 0.5703125, 0.5625, 1.0, 1.0, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5546875, 1.0, 1.0, 0.5546875, 0.5546875, 0.5390625, 0.5546875, 0.546875, 0.5625, 0.5390625, 1.0, 0.5625, 0.5625, 0.546875, 0.59375, 0.5703125, 0.5390625, 0.5546875, 0.5625, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.5703125, 0.5625, 0.546875, 1.0, 0.5546875, 0.5625, 1.0, 1.0, 0.5625, 1.0, 0.546875, 0.5703125, 0.5625, 0.546875, 0.5625, 0.546875, 0.5625, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 0.5546875, 1.0, 0.5625, 0.5625, 0.546875, 0.578125, 0.5625, 0.5625, 0.5546875, 0.5546875, 1.0, 0.5546875, 0.5703125, 1.0, 0.5625, 0.546875, 0.5625, 0.5546875, 0.5390625, 0.5703125, 0.578125, 1.0, 0.5390625, 0.5625, 0.5703125, 0.5703125, 0.5546875, 0.5546875, 0.5546875, 0.5546875, 0.5625, 1.0, 0.5625, 0.5625, 1.0, 0.5390625, 0.5625, 0.5625, 0.5546875, 0.5625, 0.5625, 1.0, 0.546875, 1.0, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5703125, 0.546875, 0.5546875, 0.5625, 0.5625, 0.578125, 0.5546875, 0.5703125, 1.0, 0.5703125, 0.5625, 0.546875, 1.0, 0.5703125, 0.5625, 0.546875, 0.5625, 0.578125, 1.0, 0.5546875, 0.53125, 1.0, 0.5625, 0.5546875, 0.5625, 0.5546875, 0.5625, 0.578125, 0.546875, 0.5390625, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5625, 0.5625, 0.5625, 0.5625, 0.546875, 0.5625, 0.578125, 1.0, 0.5625, 0.546875, 0.5625, 0.5625, 1.0, 0.5703125, 0.5625, 0.5703125, 1.0, 1.0, 0.5546875, 0.5390625, 0.5625, 0.5625, 0.5390625, 1.0, 1.0, 0.609375, 1.0, 0.578125, 0.5703125, 0.5625, 0.59375, 0.5625, 0.546875, 0.5625, 0.546875, 0.5625, 1.0, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.546875, 1.0, 0.546875, 0.5625, 0.5546875, 1.0, 1.0, 0.5625, 0.5625, 1.0, 0.5625, 1.0, 0.5546875, 0.5625, 0.5390625, 0.546875, 1.0, 0.5546875, 0.5390625, 1.0, 1.0, 0.5703125, 0.5546875, 0.5546875, 0.5625, 0.5625, 1.0, 1.0, 1.0, 0.546875, 0.5703125, 0.5625, 0.5546875, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5703125, 0.5625, 0.5703125, 1.0, 1.0, 0.5625, 1.0, 0.5625, 1.0, 1.0, 0.53125, 0.5546875, 1.0, 0.5703125, 0.5625, 0.578125, 0.546875, 0.5625, 0.5546875, 1.0, 0.5625, 0.5625, 0.5390625, 1.0, 0.5625, 1.0, 0.5625, 0.5546875, 0.546875, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5546875, 0.546875, 1.0, 0.5703125, 0.546875, 1.0, 0.5625, 0.5546875, 0.5703125, 0.5390625, 0.5546875, 1.0, 1.0, 0.5625, 1.0, 1.0, 0.5703125, 0.5390625, 0.5625, 0.546875, 0.5625, 0.546875, 0.5546875, 0.546875, 0.5625, 0.546875, 0.5625, 1.0, 0.5625, 0.546875, 0.5859375, 0.5625, 0.578125, 0.546875, 0.5546875, 0.5625, 0.5625, 1.0, 0.5625, 0.5546875, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 1.0, 0.546875, 0.5625, 0.5703125, 0.5546875, 0.5546875, 1.0, 0.5546875, 0.5546875, 0.5625, 0.5625, 0.546875, 1.0, 0.546875, 1.0, 1.0, 0.5625, 0.5546875, 1.0, 1.0, 0.5703125, 1.0, 0.5546875, 0.5625, 1.0, 0.546875, 1.0, 0.5546875, 0.5625, 0.5625, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 0.5390625, 0.5546875, 0.5703125, 0.5625, 0.5625, 1.0, 0.5546875, 0.5546875, 0.5546875, 0.578125, 0.5546875, 0.5625, 0.5546875, 1.0, 0.5546875, 0.5625, 1.0, 0.5390625, 0.5546875, 0.5703125, 0.5625, 0.5703125, 0.5625, 1.0, 0.5703125, 1.0, 1.0, 0.5625, 0.5625, 0.546875, 0.5703125, 0.5625, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5625, 0.5625, 1.0, 1.0, 0.5703125, 0.5625, 0.578125, 0.5625, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 0.5390625, 0.609375, 0.5625, 1.0, 0.5625, 0.5546875, 0.5390625, 0.546875, 0.546875, 0.5703125, 0.5546875, 0.5546875, 0.5625, 0.5546875, 0.5625, 1.0, 1.0, 0.5546875, 1.0, 0.5546875, 0.546875, 0.5625, 0.5625, 0.5625, 0.5546875, 0.5625, 0.546875, 0.5625, 0.5546875, 0.5546875, 0.5625, 0.578125, 1.0, 0.5625, 0.5625, 1.0, 1.0, 1.0, 0.5703125, 0.5625, 0.5703125, 0.5546875, 0.5625, 1.0, 0.5703125, 0.5625, 0.5625, 0.5546875, 0.5390625, 0.5625, 0.5703125, 0.5625, 1.0, 1.0, 1.0, 0.5390625, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 0.5625, 1.0, 1.0, 1.0, 0.546875, 1.0, 0.546875, 1.0, 0.5546875, 0.5625, 1.0, 0.5625, 0.5546875, 0.5625, 0.546875, 0.5625, 0.5625, 0.5625, 0.5625, 1.0, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5625, 0.5546875, 0.5546875, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.5625, 1.0, 0.5625, 0.5703125, 0.5703125, 0.546875, 0.546875, 1.0, 1.0, 0.5625, 1.0, 0.5390625, 0.5625, 1.0, 0.5625, 1.0, 1.0]

 sparsity of   [1.0, 0.201171875, 0.197265625, 1.0, 0.197265625, 0.21484375, 0.205078125, 1.0, 1.0, 1.0, 1.0, 0.2109375, 1.0, 1.0, 0.205078125, 0.22265625, 0.20703125, 0.201171875, 1.0, 0.208984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 0.19921875, 1.0, 1.0, 0.212890625, 1.0, 1.0, 0.19921875, 1.0, 1.0, 0.21484375, 1.0, 1.0, 1.0, 1.0, 0.193359375, 0.205078125, 1.0, 1.0, 0.20703125, 0.203125, 0.24609375, 1.0, 1.0, 1.0, 0.21875, 0.197265625, 0.19921875, 1.0, 1.0, 1.0, 0.216796875, 0.2109375, 1.0, 1.0, 1.0, 1.0, 0.193359375, 1.0, 0.203125, 1.0, 1.0, 1.0, 1.0, 0.220703125, 1.0, 0.203125, 0.20703125, 0.205078125, 1.0, 1.0, 0.203125, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.197265625, 1.0, 0.208984375, 1.0, 1.0, 0.201171875, 1.0, 1.0, 1.0, 0.1953125, 1.0, 0.205078125, 1.0, 1.0, 0.21484375, 0.205078125, 0.201171875, 0.203125, 1.0, 0.208984375, 1.0, 0.19921875, 1.0, 1.0, 0.201171875, 0.212890625, 0.205078125, 0.22265625, 0.201171875, 1.0, 0.193359375, 1.0, 0.2109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.205078125, 1.0, 1.0, 0.2109375]

 sparsity of   [0.5789930820465088, 1.0, 0.5729166865348816, 0.5711805820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 1.0, 1.0, 1.0, 1.0, 0.5763888955116272, 0.5763888955116272, 1.0, 0.5729166865348816, 1.0, 1.0, 0.5729166865348816, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 0.5763888955116272, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 1.0, 0.5763888955116272, 0.577256977558136, 1.0, 0.5763888955116272, 1.0, 0.577256977558136, 1.0, 1.0, 0.5720486044883728, 0.578125, 1.0, 0.5729166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5755208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 0.578125, 1.0, 0.578125, 0.5746527910232544, 1.0, 1.0, 1.0, 0.5763888955116272, 0.5737847089767456, 1.0, 0.5729166865348816, 0.5755208134651184, 1.0, 1.0, 0.5746527910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 0.577256977558136, 1.0, 1.0, 1.0, 1.0, 0.5746527910232544, 0.5737847089767456, 1.0, 0.5798611044883728, 1.0, 1.0, 0.5720486044883728, 1.0, 0.5763888955116272, 0.5737847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5737847089767456, 1.0, 1.0, 1.0, 0.578125, 0.5789930820465088, 0.5798611044883728, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.577256977558136, 1.0, 1.0, 0.5763888955116272]

 sparsity of   [0.6640625, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.640625, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 0.6484375, 0.65625, 0.6484375, 0.65625, 1.0, 1.0, 0.6484375, 0.640625, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.671875, 0.6484375, 0.6484375, 0.6484375, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.640625, 0.671875, 1.0, 0.6484375, 1.0, 1.0, 0.6484375, 0.6484375, 1.0, 1.0, 1.0, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6640625, 1.0, 0.640625, 0.640625, 0.640625, 0.65625, 0.65625, 0.6484375, 0.640625, 0.6484375, 0.640625, 0.640625, 0.6640625, 0.640625, 0.640625, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 1.0, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.640625, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.640625, 0.6484375, 1.0, 0.6484375, 0.671875, 0.6484375, 0.6640625, 1.0, 0.65625, 0.65625, 1.0, 0.6484375, 1.0, 0.640625, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.65625, 1.0, 0.6484375, 0.65625, 0.640625, 0.65625, 1.0, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.65625, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.7578125, 0.6484375, 0.65625, 0.65625, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.640625, 0.65625, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.65625, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 0.6484375, 0.640625, 0.640625, 1.0, 0.6484375, 0.6484375, 0.65625, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.65625, 0.6640625, 1.0, 0.640625, 0.65625, 0.6640625, 1.0, 0.6484375, 1.0, 0.6484375, 0.65625, 0.65625, 0.65625, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.65625, 0.6484375, 0.6640625, 0.65625, 0.65625, 0.65625, 0.6484375, 1.0, 0.65625, 0.65625, 0.640625, 0.6484375, 0.671875, 0.6484375, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 1.0, 0.640625, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.671875, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.65625, 0.671875, 1.0, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 1.0, 0.640625, 0.640625, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.65625, 0.6484375, 0.640625, 0.6484375, 1.0, 0.75, 0.6484375, 1.0, 0.640625, 0.6484375, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 1.0, 0.65625, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.640625, 0.65625, 0.6484375, 0.6484375, 1.0, 1.0, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.640625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6640625, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.6640625, 0.6484375, 1.0, 0.6484375, 1.0, 1.0, 0.65625, 0.6484375, 0.6484375, 0.65625, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6484375, 0.6484375, 0.65625, 0.65625, 0.6484375, 0.6484375, 0.6484375, 0.6484375, 1.0, 1.0, 0.640625, 1.0, 0.6640625, 0.65625, 0.6484375, 0.6484375, 0.640625, 0.640625, 0.6484375, 1.0, 1.0, 1.0, 0.65625, 1.0, 0.6484375, 0.640625, 0.6484375, 0.65625, 0.6484375, 1.0, 0.6484375, 0.6484375, 1.0, 0.6484375, 0.6640625, 0.640625, 0.671875, 1.0, 0.65625]

 sparsity of   [0.087890625, 1.0, 0.080078125, 0.0859375, 1.0, 0.078125, 1.0, 0.080078125, 0.0859375, 1.0, 0.08203125, 1.0, 1.0, 0.08203125, 1.0, 0.072265625, 0.087890625, 0.0859375, 0.083984375, 1.0, 1.0, 0.087890625, 0.0703125, 0.07421875, 1.0, 0.07421875, 0.080078125, 1.0, 0.072265625, 0.083984375, 0.091796875, 0.076171875, 0.076171875, 1.0, 0.07421875, 0.078125, 0.083984375, 0.083984375, 0.07421875, 1.0, 1.0, 0.091796875, 1.0, 0.08203125, 0.0703125, 0.0859375, 0.080078125, 0.07421875, 0.076171875, 0.078125, 1.0, 0.080078125, 0.078125, 0.080078125, 0.08203125, 0.087890625, 0.076171875, 1.0, 1.0, 0.083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.083984375, 1.0, 0.0859375, 0.07421875, 0.08203125, 1.0, 1.0, 0.08203125, 0.08203125, 1.0, 1.0, 0.08984375, 1.0, 1.0, 0.080078125, 0.08984375, 0.08203125, 0.07421875, 0.080078125, 0.083984375, 1.0, 0.07421875, 0.095703125, 1.0, 0.07421875, 0.078125, 0.078125, 0.0703125, 1.0, 0.07421875, 0.072265625, 0.07421875, 0.080078125, 1.0, 0.0859375, 1.0, 0.080078125, 0.080078125, 0.076171875, 0.078125, 1.0, 0.07421875, 1.0, 1.0, 1.0, 0.080078125, 0.076171875, 1.0, 1.0, 1.0, 0.080078125, 1.0, 1.0, 0.08203125, 1.0, 0.080078125, 0.080078125, 0.0859375, 0.080078125, 0.076171875, 0.080078125, 1.0, 0.083984375]

 sparsity of   [0.3611111044883728, 1.0, 1.0, 1.0, 0.3611111044883728, 1.0, 0.3611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3637152910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3611111044883728, 0.3619791567325592, 1.0, 0.362847238779068, 0.362847238779068, 0.3637152910232544, 1.0, 0.3611111044883728, 1.0, 0.3645833432674408, 0.3611111044883728, 1.0, 1.0, 0.3637152910232544, 0.362847238779068, 0.3611111044883728, 0.3602430522441864, 1.0, 1.0, 1.0, 0.3611111044883728, 1.0, 1.0, 0.3602430522441864, 1.0, 1.0, 1.0, 0.3619791567325592, 1.0, 0.3602430522441864, 0.3611111044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3602430522441864, 0.3645833432674408, 1.0, 1.0, 1.0, 0.3645833432674408, 0.3645833432674408, 0.3611111044883728, 0.3637152910232544, 1.0, 0.3637152910232544, 0.359375, 1.0, 1.0, 0.3611111044883728, 0.3645833432674408, 0.3602430522441864, 0.3602430522441864, 0.3602430522441864, 0.362847238779068, 0.359375, 0.3619791567325592, 0.3611111044883728, 0.3654513955116272, 0.3611111044883728, 1.0, 0.359375, 0.3611111044883728, 0.3619791567325592, 1.0, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.3619791567325592, 1.0, 1.0, 1.0, 0.3611111044883728, 0.3602430522441864, 0.362847238779068, 1.0, 1.0, 0.3619791567325592, 1.0, 1.0, 1.0, 1.0, 0.362847238779068, 1.0, 0.362847238779068, 1.0, 1.0, 1.0, 1.0, 0.359375, 0.3602430522441864, 1.0, 0.3619791567325592, 1.0, 1.0, 0.3602430522441864, 0.3611111044883728, 0.362847238779068, 1.0, 1.0, 0.3611111044883728, 0.3585069477558136, 1.0, 1.0]

 sparsity of   [0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 1.0, 1.0, 0.546875, 0.546875, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5546875, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5546875, 0.5390625, 0.546875, 0.546875, 0.546875, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.546875, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 1.0, 0.546875, 1.0, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 0.5390625, 1.0, 0.546875, 0.5546875, 0.546875, 0.546875, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.546875, 1.0, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 1.0, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.546875, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.5390625, 0.5390625, 0.5390625, 0.5546875, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.546875, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.546875, 0.546875, 0.546875, 0.5390625, 0.546875, 1.0, 0.5546875, 0.5390625, 0.5546875, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.546875, 1.0, 0.546875, 0.5390625, 0.546875, 0.5390625, 1.0, 0.5390625, 1.0, 0.5390625, 0.546875, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 1.0, 0.5390625, 1.0, 0.546875, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 0.5390625, 1.0, 0.5390625, 0.5390625, 0.5546875, 0.546875, 1.0, 1.0]

 sparsity of   [0.0546875, 0.05859375, 0.05859375, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.046875, 1.0, 1.0, 0.052734375, 1.0, 0.052734375, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.05078125, 0.046875, 0.056640625, 0.05078125, 1.0, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.056640625, 1.0, 0.05078125, 1.0, 0.0546875, 0.052734375, 0.052734375, 0.048828125, 0.05078125, 0.048828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.05078125, 0.05078125, 1.0, 1.0, 1.0, 0.05078125, 0.048828125, 0.05859375, 0.060546875, 0.0546875, 1.0, 0.048828125, 1.0, 0.046875, 1.0, 1.0, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 0.052734375, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.0546875, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.048828125, 0.056640625, 0.052734375, 0.0546875, 1.0, 0.0546875, 1.0, 1.0, 0.046875, 1.0, 0.05078125, 0.044921875, 1.0, 0.0546875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.052734375, 1.0, 0.05078125, 0.0546875, 1.0, 1.0, 1.0, 0.0546875, 0.052734375, 0.0546875, 1.0, 0.0546875, 0.046875, 0.0546875, 1.0, 0.05078125, 0.05078125, 0.05859375, 0.12890625, 1.0, 0.046875, 1.0, 0.05859375, 0.05859375, 0.056640625, 0.048828125, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046875, 0.05078125, 1.0, 1.0, 0.05078125, 0.056640625, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.05078125, 1.0, 0.048828125, 1.0, 0.04296875, 0.05078125, 0.048828125, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.048828125, 1.0, 0.048828125, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 0.05078125, 0.046875, 0.048828125, 0.044921875, 1.0, 1.0, 0.05078125, 0.056640625, 1.0, 0.056640625, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.046875, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05078125, 1.0, 1.0, 0.05078125, 0.05078125, 0.056640625, 0.052734375, 1.0, 0.052734375, 1.0, 0.052734375, 1.0, 1.0, 0.060546875, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.05078125, 0.046875, 0.048828125, 1.0, 0.0625, 1.0, 0.048828125, 1.0, 1.0, 1.0, 1.0, 0.048828125, 0.060546875, 1.0, 0.048828125, 1.0, 1.0, 1.0, 0.046875, 1.0, 0.052734375, 0.044921875, 0.0546875, 0.0546875, 0.0546875, 1.0, 0.068359375, 0.056640625]

 sparsity of   [1.0, 1.0, 1.0, 0.5138888955116272, 1.0, 0.5138888955116272, 0.514756977558136, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5125868320465088, 0.5134548544883728, 1.0, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 0.5134548544883728, 0.5160590410232544, 1.0, 0.5151909589767456, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5130208134651184, 0.5138888955116272, 1.0, 0.5160590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 0.5138888955116272, 1.0, 1.0, 1.0, 0.514756977558136, 0.5138888955116272, 1.0, 1.0, 1.0, 0.514756977558136, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 0.5143229365348816, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 0.5125868320465088, 0.5134548544883728, 1.0, 1.0, 1.0, 1.0, 0.5182291865348816, 0.514756977558136, 0.515625, 1.0, 1.0, 1.0, 0.5130208134651184, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 0.5138888955116272, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 0.5169270634651184, 0.5130208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5130208134651184, 0.5151909589767456, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 0.5138888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 0.5160590410232544, 0.5143229365348816, 0.5169270634651184, 1.0, 0.5134548544883728, 1.0, 1.0, 1.0, 0.5134548544883728, 1.0, 1.0, 0.515625, 1.0, 1.0, 0.515625, 1.0, 0.5134548544883728, 1.0, 1.0, 1.0, 0.5190972089767456, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5121527910232544, 1.0, 0.514756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5173611044883728, 0.5177951455116272, 0.514756977558136, 0.5130208134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.514756977558136, 1.0, 1.0, 0.5151909589767456, 0.5138888955116272, 1.0, 1.0, 0.514756977558136, 1.0, 0.5164930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5151909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5143229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.74609375, 0.74609375, 0.73828125, 0.75390625, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.75, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.75, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.74609375, 0.74609375, 0.74609375, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 1.0, 0.7421875, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 0.75, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.75, 0.75, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 0.73828125, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.75, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.75390625, 0.7421875, 0.73828125, 0.74609375, 1.0, 0.74609375, 0.73828125, 0.74609375, 1.0, 0.74609375, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 0.74609375, 0.75, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.75390625, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 0.73828125, 0.7421875, 0.73828125, 0.7421875, 0.7421875, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.7421875, 0.73828125, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 0.75, 0.75, 1.0, 0.74609375, 0.73828125, 0.7421875, 1.0, 1.0, 0.75, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 0.7421875, 0.73828125, 0.74609375, 0.73828125, 1.0, 0.73828125, 0.7421875, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.73828125, 0.7421875, 0.74609375, 0.74609375, 0.7421875, 0.73828125, 0.7578125, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 0.7421875, 0.74609375, 0.74609375, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.75, 0.73828125, 0.7421875, 0.7421875, 1.0, 1.0, 0.75, 1.0, 0.7421875, 0.75, 1.0, 1.0, 0.75, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 0.74609375, 0.73828125, 0.73828125, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.73828125, 0.7421875, 1.0, 0.7421875, 0.7421875, 0.73828125, 1.0, 0.74609375, 1.0, 0.73828125, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.75, 0.73828125, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 0.75, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.75, 0.74609375, 1.0, 1.0, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.75, 1.0, 0.73828125, 1.0, 1.0, 0.7421875, 0.7578125, 0.75, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.73828125, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 1.0, 0.74609375, 0.75, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.73828125, 0.74609375, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.73828125, 0.73828125, 1.0, 1.0, 0.73828125, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 0.75, 1.0, 0.74609375, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 0.74609375, 1.0, 0.73828125, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.73828125, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 0.75, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.73828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.73828125, 0.74609375, 1.0]

 sparsity of   [1.0, 0.0625, 1.0, 0.052734375, 0.060546875, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 0.060546875, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.056640625, 0.068359375, 1.0, 1.0, 0.044921875, 1.0, 1.0, 0.248046875, 0.056640625, 1.0, 1.0, 0.05859375, 0.06640625, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.05859375, 0.05859375, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.068359375, 0.064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.05859375, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 0.056640625, 1.0, 0.068359375, 1.0, 0.044921875, 1.0, 1.0, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 0.05078125, 1.0, 0.05859375, 1.0, 0.0546875, 1.0, 0.119140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.068359375, 0.060546875, 0.056640625, 1.0, 0.046875, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.080078125, 1.0, 0.056640625, 0.056640625, 1.0, 0.052734375, 1.0, 1.0, 1.0, 0.0703125, 1.0, 0.0546875, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 0.05078125, 1.0, 0.064453125, 0.060546875, 0.0546875, 1.0, 1.0, 0.0625, 0.048828125, 1.0, 1.0, 1.0, 0.13671875, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.052734375, 0.072265625, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.068359375, 0.0546875, 0.068359375, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.052734375, 1.0, 0.068359375, 0.080078125, 1.0, 0.064453125, 1.0, 0.05859375, 1.0, 0.115234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 0.0625, 0.052734375, 0.0546875, 1.0, 1.0, 0.060546875, 0.064453125, 1.0, 0.064453125, 0.05078125, 0.0546875, 1.0, 0.060546875, 0.056640625, 0.05078125, 1.0, 0.060546875, 0.05859375, 1.0, 0.056640625, 1.0, 1.0, 0.056640625, 1.0, 0.0546875, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.060546875, 1.0, 1.0, 0.05859375, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.06640625, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 0.048828125, 1.0, 0.05859375, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.107421875, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.0546875, 0.0703125, 1.0, 1.0, 0.05859375, 1.0, 0.06640625, 1.0, 0.0546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 0.060546875, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.0625, 1.0, 0.0546875, 1.0, 0.056640625, 0.056640625, 0.052734375, 0.052734375, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.052734375, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.0546875, 0.064453125, 0.056640625, 1.0, 1.0, 0.0625, 0.05078125, 1.0, 0.05078125, 1.0, 1.0, 0.060546875, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.06640625, 1.0, 0.0546875, 1.0, 0.095703125, 0.0546875, 1.0, 0.048828125, 0.0625, 0.05859375, 1.0, 1.0, 0.09375, 1.0, 0.0546875, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 0.078125, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0625, 0.0546875, 0.056640625, 1.0, 0.05859375, 0.06640625, 0.060546875, 1.0, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.068359375, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.06640625, 1.0, 1.0, 0.06640625, 0.0546875, 1.0, 0.05078125, 1.0, 0.056640625, 1.0, 1.0, 0.06640625, 1.0, 0.056640625, 1.0, 0.05859375, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.048828125, 1.0, 1.0, 0.0625, 0.056640625, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.05859375, 1.0, 0.0625, 0.09375, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.052734375, 0.064453125, 0.06640625, 0.0703125, 0.07421875, 0.05078125, 0.060546875, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 0.083984375, 0.05859375, 0.05859375, 0.056640625, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.0703125, 0.05859375, 1.0, 1.0, 1.0, 0.109375, 1.0, 1.0, 0.048828125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060546875, 1.0, 0.060546875, 0.05859375, 0.09765625, 1.0, 1.0, 0.052734375, 1.0, 0.0546875, 0.0625, 1.0, 1.0, 0.056640625, 0.052734375, 1.0, 1.0, 0.05859375, 0.0546875, 1.0, 0.056640625, 0.0546875, 0.060546875, 0.0546875, 0.068359375, 1.0, 1.0, 0.05859375, 1.0, 0.078125, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.056640625, 0.091796875, 1.0, 0.0625, 1.0, 0.056640625, 1.0, 0.0703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.056640625, 0.134765625, 1.0, 1.0, 0.060546875, 0.064453125, 1.0, 0.052734375, 1.0, 0.056640625, 0.05078125, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.05859375, 1.0, 1.0, 0.056640625, 0.060546875, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.056640625, 0.056640625, 1.0, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.05859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 1.0, 0.060546875, 0.056640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.064453125, 1.0, 0.072265625, 0.0546875, 1.0, 0.06640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.056640625, 1.0, 1.0, 1.0, 0.05078125, 1.0, 0.060546875, 0.06640625, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 1.0, 0.056640625, 0.05859375, 1.0, 1.0, 1.0, 0.05859375, 1.0, 0.064453125, 1.0, 1.0, 0.0546875, 1.0, 1.0, 1.0, 1.0, 0.06640625, 1.0, 0.060546875, 1.0, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.1015625, 1.0, 1.0, 0.052734375, 0.0546875, 1.0, 1.0, 1.0, 0.13671875, 1.0, 0.130859375, 1.0, 1.0, 0.076171875, 0.052734375, 1.0, 1.0, 0.0703125, 0.060546875, 1.0, 1.0, 0.064453125, 0.052734375, 0.056640625, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.0546875, 0.076171875, 0.052734375, 0.0546875, 1.0, 1.0, 0.05859375, 0.064453125, 1.0, 0.056640625, 1.0, 1.0, 0.05859375, 0.14453125, 1.0, 1.0, 0.064453125, 1.0, 1.0, 1.0, 0.052734375, 1.0, 1.0, 0.056640625, 1.0, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.060546875, 0.09765625, 1.0, 1.0, 1.0, 0.123046875, 0.044921875, 1.0, 1.0, 0.064453125, 0.052734375, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.05859375, 0.060546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.056640625, 1.0, 0.0546875, 0.072265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0546875, 1.0, 1.0, 0.095703125, 0.056640625, 1.0, 0.0546875, 1.0, 1.0, 0.064453125, 0.048828125, 0.0546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.07421875, 1.0, 1.0, 0.0625, 1.0, 1.0, 0.05078125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1171875, 1.0, 1.0, 1.0, 0.07421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.052734375, 0.0546875, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 0.611328125, 1.0, 1.0, 1.0, 1.0, 0.6162109375, 0.6103515625, 1.0, 1.0, 0.615234375, 1.0, 0.6103515625, 1.0, 0.6103515625, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 1.0, 1.0, 0.6103515625, 1.0, 0.611328125, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 0.61328125, 0.61328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.609375, 1.0, 0.6103515625, 1.0, 1.0, 0.6162109375, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6162109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 1.0, 1.0, 0.6142578125, 0.611328125, 0.607421875, 1.0, 1.0, 1.0, 0.611328125, 0.6103515625, 1.0, 0.6171875, 1.0, 1.0, 1.0, 0.611328125, 1.0, 1.0, 0.6083984375, 1.0, 1.0, 0.615234375, 1.0, 1.0, 0.609375, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 1.0, 0.611328125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.607421875, 0.6142578125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6103515625, 0.6083984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 0.61328125, 0.6103515625, 1.0, 1.0, 1.0, 0.6123046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.61328125, 0.60546875, 0.6142578125, 1.0, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 0.615234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.615234375, 1.0, 0.6142578125, 1.0, 1.0, 1.0, 0.6044921875, 0.6015625, 0.6064453125, 1.0, 0.6083984375, 0.615234375, 1.0, 1.0, 0.6103515625, 1.0, 0.6142578125, 1.0, 1.0, 0.609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.611328125, 1.0, 0.6083984375, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 0.7677951455116272, 1.0, 0.764756977558136, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 0.7660590410232544, 1.0, 0.7677951455116272, 0.7677951455116272, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 0.765625, 1.0, 0.765625, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 0.7638888955116272, 0.7673611044883728, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 0.7634548544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 0.7673611044883728, 0.7664930820465088, 0.7677951455116272, 0.7643229365348816, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7621527910232544, 1.0, 1.0, 1.0, 0.7669270634651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7604166865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7651909589767456, 1.0, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7660590410232544, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 0.7643229365348816, 1.0, 1.0, 0.76171875, 0.7651909589767456, 1.0, 0.7612847089767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 1.0, 0.768663227558136, 0.7651909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7651909589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7677951455116272, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 0.765625, 0.7643229365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.765625, 0.764756977558136, 0.7660590410232544, 1.0, 0.7651909589767456, 1.0, 1.0, 0.7677951455116272, 0.7660590410232544, 1.0, 1.0, 0.764756977558136, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0, 1.0, 1.0, 0.765625, 1.0, 1.0, 1.0, 1.0, 0.764756977558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76953125, 1.0, 1.0, 0.765625, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7673611044883728, 1.0, 1.0, 1.0, 1.0, 0.7664930820465088, 1.0]

 sparsity of   [1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.7578125, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.76171875, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75390625, 1.0, 0.76953125, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.765625, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.765625, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75390625, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.7578125, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 0.76171875, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75390625, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75390625, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.7578125, 0.75, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 0.7578125, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.7578125, 0.75, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.75, 1.0, 0.7578125, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 0.75390625, 1.0, 0.75, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75390625, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75390625, 0.75, 0.75, 0.75, 0.75, 0.75, 0.76171875, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 0.75390625, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75390625, 1.0, 1.0, 0.75390625, 1.0, 0.75390625, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 0.75390625, 0.75, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 0.75390625, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 1.0, 0.7578125, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 1.0, 0.7578125, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75390625, 0.75390625, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75390625, 0.75390625, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75390625, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75390625, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75390625, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75390625, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0]

 sparsity of   [1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 0.5732421875, 1.0, 0.5673828125, 1.0, 0.576171875, 0.572265625, 1.0, 0.572265625, 0.5693359375, 0.5654296875, 0.5732421875, 0.5732421875, 1.0, 0.568359375, 0.572265625, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 0.564453125, 1.0, 0.576171875, 1.0, 0.5615234375, 0.56640625, 1.0, 1.0, 1.0, 1.0, 0.57421875, 1.0, 1.0, 1.0, 0.57421875, 0.5771484375, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 0.5703125, 0.5830078125, 1.0, 1.0, 0.572265625, 1.0, 0.568359375, 1.0, 0.572265625, 0.56640625, 1.0, 0.568359375, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 0.564453125, 1.0, 1.0, 0.56640625, 1.0, 0.5703125, 1.0, 1.0, 1.0, 0.564453125, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 0.56640625, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.57421875, 1.0, 1.0, 1.0, 0.5732421875, 0.568359375, 1.0, 1.0, 1.0, 1.0, 0.5634765625, 1.0, 0.5673828125, 0.572265625, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 0.5751953125, 0.57421875, 0.5693359375, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.572265625, 0.5712890625, 1.0, 1.0, 0.603515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 0.58203125, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 0.5771484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 0.5751953125, 1.0, 1.0, 1.0, 1.0, 0.5751953125, 0.5693359375, 0.5625, 0.576171875, 0.5732421875, 1.0, 1.0, 0.5712890625, 0.5634765625, 1.0, 0.568359375, 1.0, 1.0, 0.5693359375, 1.0, 1.0, 1.0, 1.0, 0.568359375, 0.5654296875, 0.57421875, 1.0, 0.5673828125, 1.0, 1.0, 1.0, 1.0, 0.578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 0.564453125, 1.0, 0.5732421875, 1.0, 0.5703125, 0.568359375, 1.0, 0.572265625, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 0.6618923544883728, 1.0, 0.6653645634651184, 1.0, 0.6636284589767456, 0.6605902910232544, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 1.0, 1.0, 0.6640625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6610243320465088, 0.6627604365348816, 1.0, 0.6623263955116272, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6644965410232544, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6605902910232544, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 0.6610243320465088, 1.0, 1.0, 0.6618923544883728, 0.6614583134651184, 0.6623263955116272, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6623263955116272, 0.6610243320465088, 1.0, 0.663194477558136, 1.0, 1.0, 0.6627604365348816, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 0.6627604365348816, 1.0, 0.6618923544883728, 1.0, 1.0, 0.6605902910232544, 1.0, 1.0, 0.6623263955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6605902910232544, 1.0, 1.0, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 0.6605902910232544, 1.0, 0.6610243320465088, 1.0, 0.6627604365348816, 1.0, 0.6610243320465088, 0.6618923544883728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.6636284589767456, 0.6610243320465088, 1.0, 0.6605902910232544, 1.0, 1.0, 1.0, 1.0, 0.6618923544883728, 0.6623263955116272, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6610243320465088, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 0.6605902910232544, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6636284589767456, 1.0, 1.0, 0.6605902910232544, 1.0, 0.663194477558136, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.663194477558136, 0.6618923544883728, 0.6623263955116272, 0.6610243320465088, 1.0, 0.6614583134651184, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.75390625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.75, 1.0, 1.0, 0.74609375, 0.7421875, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7578125, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 0.7734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.74609375, 1.0, 0.75, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 0.7421875, 1.0, 0.7421875, 0.74609375, 1.0, 0.7421875, 0.74609375, 0.74609375, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 0.7421875, 0.74609375, 0.7421875, 1.0, 0.75, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.75390625, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.74609375, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 0.7421875, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.7421875, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.7421875, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.74609375, 0.74609375, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7421875, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5732421875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9592013955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6064453125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5498046875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.572265625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7001953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5966796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5615234375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5712890625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.55078125, 1.0, 1.0, 1.0, 0.5537109375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5576171875, 1.0, 1.0, 0.548828125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5478515625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6181640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62109375, 1.0, 1.0, 1.0, 0.5517578125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.556640625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5654296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.560546875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5595703125, 1.0, 1.0, 1.0, 1.0, 0.568359375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.552734375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.828125, 0.82763671875, 0.82763671875, 0.82861328125, 0.82763671875, 0.8291015625, 0.82861328125, 0.828125, 0.82763671875, 0.82861328125, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]

Total parameter pruned: 22672756.005090863 (unstructured) 21537042 (structured)

Test: [0/79]	Time 0.176 (0.176)	Loss 0.2487 (0.2487) ([0.135]+[0.113])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(2.0015, device='cuda:0')
Epoch: [300][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0084 (0.0084) ([0.008]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [300][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0071) ([0.004]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [300][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0069) ([0.006]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [300][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0068) ([0.004]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1579 (0.1579) ([0.158]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.980
current lr 1.00000e-03
Grad=  tensor(1.0503, device='cuda:0')
Epoch: [301][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0077 (0.0077) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [301][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0072) ([0.002]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [301][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0070) ([0.003]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [301][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0070) ([0.004]+[0.000])	Prec@1 100.000 (99.909)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1519 (0.1519) ([0.152]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.740
current lr 1.00000e-03
Grad=  tensor(0.5421, device='cuda:0')
Epoch: [302][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0079 (0.0079) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [302][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0119 (0.0065) ([0.012]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [302][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [302][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0069) ([0.004]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1446 (0.1446) ([0.145]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.6507, device='cuda:0')
Epoch: [303][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0089 (0.0089) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [303][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0054 (0.0064) ([0.005]+[0.000])	Prec@1 100.000 (99.915)
Epoch: [303][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0175 (0.0064) ([0.017]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [303][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0065) ([0.005]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1323 (0.1323) ([0.132]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.930
current lr 1.00000e-03
Grad=  tensor(0.0144, device='cuda:0')
Epoch: [304][0/391]	Time 0.211 (0.211)	Data 0.141 (0.141)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [304][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0065 (0.0059) ([0.007]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [304][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0067) ([0.004]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [304][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0067) ([0.002]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1291 (0.1291) ([0.129]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.3777, device='cuda:0')
Epoch: [305][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [305][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0058) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [305][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [305][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0059) ([0.003]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1307 (0.1307) ([0.131]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.3672, device='cuda:0')
Epoch: [306][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [306][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0064) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [306][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0066 (0.0066) ([0.007]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [306][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0064) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1548 (0.1548) ([0.155]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.750
current lr 1.00000e-03
Grad=  tensor(0.1751, device='cuda:0')
Epoch: [307][0/391]	Time 0.209 (0.209)	Data 0.137 (0.137)	Loss 0.0057 (0.0057) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [307][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0145 (0.0069) ([0.015]+[0.000])	Prec@1 99.219 (99.899)
Epoch: [307][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0070) ([0.004]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [307][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0070) ([0.007]+[0.000])	Prec@1 100.000 (99.904)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1282 (0.1282) ([0.128]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.3053, device='cuda:0')
Epoch: [308][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [308][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0110 (0.0063) ([0.011]+[0.000])	Prec@1 99.219 (99.915)
Epoch: [308][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0064) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [308][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0064) ([0.004]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1400 (0.1400) ([0.140]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(1.0904, device='cuda:0')
Epoch: [309][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0106 (0.0106) ([0.011]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [309][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0039 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [309][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0116 (0.0061) ([0.012]+[0.000])	Prec@1 100.000 (99.922)
Epoch: [309][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0061) ([0.003]+[0.000])	Prec@1 100.000 (99.930)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1486 (0.1486) ([0.149]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-03
Grad=  tensor(0.0528, device='cuda:0')
Epoch: [310][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [310][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0061) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [310][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0155 (0.0058) ([0.015]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [310][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0061) ([0.002]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1555 (0.1555) ([0.155]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(1.3095, device='cuda:0')
Epoch: [311][0/391]	Time 0.213 (0.213)	Data 0.141 (0.141)	Loss 0.0079 (0.0079) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [311][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0084 (0.0069) ([0.008]+[0.000])	Prec@1 100.000 (99.907)
Epoch: [311][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0066) ([0.002]+[0.000])	Prec@1 100.000 (99.914)
Epoch: [311][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1511 (0.1511) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.5469, device='cuda:0')
Epoch: [312][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0072 (0.0072) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [312][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0083 (0.0061) ([0.008]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [312][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0061) ([0.008]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [312][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (99.925)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1771 (0.1771) ([0.177]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.850
current lr 1.00000e-03
Grad=  tensor(0.6820, device='cuda:0')
Epoch: [313][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0093 (0.0093) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [313][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0089 (0.0064) ([0.009]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [313][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0064) ([0.006]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [313][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0062) ([0.004]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1566 (0.1566) ([0.157]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.930
current lr 1.00000e-03
Grad=  tensor(0.2867, device='cuda:0')
Epoch: [314][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0058 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [314][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0061) ([0.004]+[0.000])	Prec@1 100.000 (99.930)
Epoch: [314][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0060) ([0.009]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [314][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0061) ([0.003]+[0.000])	Prec@1 100.000 (99.927)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1556 (0.1556) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-03
Grad=  tensor(0.2774, device='cuda:0')
Epoch: [315][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0072 (0.0072) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [315][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0029 (0.0058) ([0.003]+[0.000])	Prec@1 100.000 (99.923)
Epoch: [315][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0059) ([0.005]+[0.000])	Prec@1 100.000 (99.926)
Epoch: [315][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0060) ([0.004]+[0.000])	Prec@1 100.000 (99.922)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1646 (0.1646) ([0.165]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-03
Grad=  tensor(0.0408, device='cuda:0')
Epoch: [316][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [316][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0058 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [316][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [316][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1564 (0.1564) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.820
current lr 1.00000e-03
Grad=  tensor(0.1746, device='cuda:0')
Epoch: [317][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [317][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0137 (0.0058) ([0.014]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [317][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0060) ([0.012]+[0.000])	Prec@1 100.000 (99.942)
Epoch: [317][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0058) ([0.003]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1785 (0.1785) ([0.178]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(0.2434, device='cuda:0')
Epoch: [318][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0062 (0.0062) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [318][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0180 (0.0056) ([0.018]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [318][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [318][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0061) ([0.005]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1735 (0.1735) ([0.173]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(6.8467, device='cuda:0')
Epoch: [319][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0219 (0.0219) ([0.022]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [319][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0075 (0.0053) ([0.008]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [319][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0074 (0.0055) ([0.007]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [319][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0103 (0.0057) ([0.010]+[0.000])	Prec@1 99.219 (99.945)
Test: [0/79]	Time 0.154 (0.154)	Loss 0.1925 (0.1925) ([0.192]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.5683, device='cuda:0')
Epoch: [320][0/391]	Time 0.217 (0.217)	Data 0.145 (0.145)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [320][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0029 (0.0055) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [320][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [320][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1841 (0.1841) ([0.184]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [321][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [321][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [321][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [321][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0056) ([0.007]+[0.000])	Prec@1 100.000 (99.956)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1766 (0.1766) ([0.177]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.910
current lr 1.00000e-03
Grad=  tensor(4.3690, device='cuda:0')
Epoch: [322][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0108 (0.0108) ([0.011]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [322][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0061 (0.0053) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [322][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [322][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0054) ([0.010]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1806 (0.1806) ([0.181]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(0.0825, device='cuda:0')
Epoch: [323][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [323][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0051) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [323][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0055) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [323][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0073 (0.0058) ([0.007]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1747 (0.1747) ([0.175]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.980
current lr 1.00000e-03
Grad=  tensor(0.4451, device='cuda:0')
Epoch: [324][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [324][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0055 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [324][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0128 (0.0055) ([0.013]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [324][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0081 (0.0055) ([0.008]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1829 (0.1829) ([0.183]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.840
current lr 1.00000e-03
Grad=  tensor(0.0435, device='cuda:0')
Epoch: [325][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [325][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0060) ([0.003]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [325][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [325][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0058) ([0.006]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1857 (0.1857) ([0.186]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-03
Grad=  tensor(0.3215, device='cuda:0')
Epoch: [326][0/391]	Time 0.216 (0.216)	Data 0.145 (0.145)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [326][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0097 (0.0050) ([0.010]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [326][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [326][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0053) ([0.007]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1864 (0.1864) ([0.186]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.960
current lr 1.00000e-03
Grad=  tensor(0.8009, device='cuda:0')
Epoch: [327][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [327][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [327][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [327][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.152 (0.152)	Loss 0.1842 (0.1842) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.070
current lr 1.00000e-03
Grad=  tensor(0.0122, device='cuda:0')
Epoch: [328][0/391]	Time 0.215 (0.215)	Data 0.144 (0.144)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [328][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0227 (0.0060) ([0.023]+[0.000])	Prec@1 99.219 (99.930)
Epoch: [328][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0056) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [328][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.945)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1605 (0.1605) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.9546, device='cuda:0')
Epoch: [329][0/391]	Time 0.208 (0.208)	Data 0.136 (0.136)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [329][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [329][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [329][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1595 (0.1595) ([0.159]+[0.000])	Prec@1 92.188 (92.188)
 * Prec@1 93.620
current lr 1.00000e-03
Grad=  tensor(0.0889, device='cuda:0')
Epoch: [330][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [330][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0227 (0.0051) ([0.023]+[0.000])	Prec@1 99.219 (99.961)
Epoch: [330][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [330][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1277 (0.1277) ([0.128]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.790
current lr 1.00000e-03
Grad=  tensor(0.0416, device='cuda:0')
Epoch: [331][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [331][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0057) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [331][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0056) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [331][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0116 (0.0053) ([0.012]+[0.000])	Prec@1 99.219 (99.956)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1431 (0.1431) ([0.143]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.1086, device='cuda:0')
Epoch: [332][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [332][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0047 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [332][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [332][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0053) ([0.002]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1614 (0.1614) ([0.161]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.0376, device='cuda:0')
Epoch: [333][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [333][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0056) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [333][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0059) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [333][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0058) ([0.004]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1443 (0.1443) ([0.144]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.940
current lr 1.00000e-03
Grad=  tensor(0.1850, device='cuda:0')
Epoch: [334][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [334][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0046 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [334][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0052) ([0.003]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [334][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0067 (0.0052) ([0.007]+[0.000])	Prec@1 100.000 (99.948)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1400 (0.1400) ([0.140]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-03
Grad=  tensor(0.1831, device='cuda:0')
Epoch: [335][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [335][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0057) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [335][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0112 (0.0055) ([0.011]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [335][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0055) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1277 (0.1277) ([0.128]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 94.010
current lr 1.00000e-03
Grad=  tensor(0.5124, device='cuda:0')
Epoch: [336][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0072 (0.0072) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [336][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0043 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [336][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0054) ([0.004]+[0.000])	Prec@1 100.000 (99.949)
Epoch: [336][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0053) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1334 (0.1334) ([0.133]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.3282, device='cuda:0')
Epoch: [337][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [337][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0136 (0.0049) ([0.014]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [337][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [337][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1350 (0.1350) ([0.135]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-03
Grad=  tensor(0.3138, device='cuda:0')
Epoch: [338][0/391]	Time 0.208 (0.208)	Data 0.138 (0.138)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [338][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [338][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0050) ([0.005]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [338][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.943)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1211 (0.1211) ([0.121]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.050
current lr 1.00000e-03
Grad=  tensor(0.4772, device='cuda:0')
Epoch: [339][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [339][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [339][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0052) ([0.001]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [339][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0055) ([0.002]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.138 (0.138)	Loss 0.1544 (0.1544) ([0.154]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.880
current lr 1.00000e-03
Grad=  tensor(0.5177, device='cuda:0')
Epoch: [340][0/391]	Time 0.202 (0.202)	Data 0.132 (0.132)	Loss 0.0085 (0.0085) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [340][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0045) ([0.001]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [340][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0047) ([0.007]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [340][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0097 (0.0047) ([0.010]+[0.000])	Prec@1 100.000 (99.953)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1403 (0.1403) ([0.140]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.100
current lr 1.00000e-03
Grad=  tensor(0.4909, device='cuda:0')
Epoch: [341][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0055 (0.0055) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [341][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [341][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0069 (0.0050) ([0.007]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [341][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0052) ([0.004]+[0.000])	Prec@1 100.000 (99.951)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1456 (0.1456) ([0.146]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(0.2499, device='cuda:0')
Epoch: [342][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [342][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [342][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [342][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0139 (0.0049) ([0.014]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1376 (0.1376) ([0.138]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-03
Grad=  tensor(0.0248, device='cuda:0')
Epoch: [343][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [343][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [343][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [343][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0095 (0.0051) ([0.010]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1427 (0.1427) ([0.143]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.040
current lr 1.00000e-03
Grad=  tensor(0.3991, device='cuda:0')
Epoch: [344][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [344][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0048) ([0.006]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [344][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.934)
Epoch: [344][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.940)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1603 (0.1603) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.860
current lr 1.00000e-03
Grad=  tensor(0.2006, device='cuda:0')
Epoch: [345][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [345][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [345][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0049) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [345][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1736 (0.1736) ([0.174]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-03
Grad=  tensor(0.3950, device='cuda:0')
Epoch: [346][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [346][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [346][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0050) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [346][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1731 (0.1731) ([0.173]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.950
current lr 1.00000e-03
Grad=  tensor(0.0177, device='cuda:0')
Epoch: [347][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [347][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0026 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [347][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [347][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0048) ([0.003]+[0.000])	Prec@1 100.000 (99.935)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.2093 (0.2093) ([0.209]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.760
current lr 1.00000e-03
Grad=  tensor(0.0356, device='cuda:0')
Epoch: [348][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [348][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0051) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [348][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [348][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0054) ([0.002]+[0.000])	Prec@1 100.000 (99.933)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1701 (0.1701) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-03
Grad=  tensor(0.0144, device='cuda:0')
Epoch: [349][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [349][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0050) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [349][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [349][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0047) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.2008 (0.2008) ([0.201]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.870
current lr 1.00000e-04
Grad=  tensor(0.9202, device='cuda:0')
Epoch: [350][0/391]	Time 0.208 (0.208)	Data 0.139 (0.139)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [350][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0051) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [350][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0050) ([0.003]+[0.000])	Prec@1 100.000 (99.953)
Epoch: [350][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0051) ([0.006]+[0.000])	Prec@1 100.000 (99.958)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1812 (0.1812) ([0.181]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.860
current lr 1.00000e-04
Grad=  tensor(0.1110, device='cuda:0')
Epoch: [351][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [351][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0075 (0.0051) ([0.007]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [351][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0047) ([0.006]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [351][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0046) ([0.002]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1740 (0.1740) ([0.174]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.020
current lr 1.00000e-04
Grad=  tensor(0.0682, device='cuda:0')
Epoch: [352][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [352][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [352][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0221 (0.0046) ([0.022]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [352][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0117 (0.0047) ([0.012]+[0.000])	Prec@1 99.219 (99.966)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1739 (0.1739) ([0.174]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0346, device='cuda:0')
Epoch: [353][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [353][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0088 (0.0045) ([0.009]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [353][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0045) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [353][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1747 (0.1747) ([0.175]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.880
current lr 1.00000e-04
Grad=  tensor(0.0605, device='cuda:0')
Epoch: [354][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [354][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0043) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [354][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0043) ([0.007]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [354][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1843 (0.1843) ([0.184]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-04
Grad=  tensor(0.1584, device='cuda:0')
Epoch: [355][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [355][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [355][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0080 (0.0044) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [355][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0044) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1745 (0.1745) ([0.175]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.1784, device='cuda:0')
Epoch: [356][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [356][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [356][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [356][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1779 (0.1779) ([0.178]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.080
current lr 1.00000e-04
Grad=  tensor(0.1193, device='cuda:0')
Epoch: [357][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [357][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [357][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [357][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1579 (0.1579) ([0.158]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(0.1054, device='cuda:0')
Epoch: [358][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [358][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [358][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [358][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.964)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1549 (0.1549) ([0.155]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-04
Grad=  tensor(0.8300, device='cuda:0')
Epoch: [359][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0065 (0.0040) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [359][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [359][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1591 (0.1591) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.7371, device='cuda:0')
Epoch: [360][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0060 (0.0060) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [360][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [360][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [360][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1562 (0.1562) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.0518, device='cuda:0')
Epoch: [361][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [361][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [361][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [361][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0086 (0.0044) ([0.009]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1550 (0.1550) ([0.155]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.3554, device='cuda:0')
Epoch: [362][0/391]	Time 0.203 (0.203)	Data 0.132 (0.132)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [362][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0083 (0.0042) ([0.008]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [362][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [362][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1669 (0.1669) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.1821, device='cuda:0')
Epoch: [363][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [363][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0020 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [363][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0043) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [363][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1431 (0.1431) ([0.143]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-04
Grad=  tensor(0.3488, device='cuda:0')
Epoch: [364][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0086 (0.0086) ([0.009]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [364][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0011 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [364][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [364][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1698 (0.1698) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-04
Grad=  tensor(0.0657, device='cuda:0')
Epoch: [365][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [365][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0046) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [365][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [365][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1617 (0.1617) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-04
Grad=  tensor(10.1166, device='cuda:0')
Epoch: [366][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0125 (0.0125) ([0.012]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [366][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [366][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [366][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1667 (0.1667) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-04
Grad=  tensor(0.0422, device='cuda:0')
Epoch: [367][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [367][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [367][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [367][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.152 (0.152)	Loss 0.1826 (0.1826) ([0.183]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-04
Grad=  tensor(0.0133, device='cuda:0')
Epoch: [368][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [368][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [368][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [368][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1593 (0.1593) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.0714, device='cuda:0')
Epoch: [369][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [369][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [369][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [369][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1638 (0.1638) ([0.164]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.1573, device='cuda:0')
Epoch: [370][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [370][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [370][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [370][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1601 (0.1601) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(2.0022, device='cuda:0')
Epoch: [371][0/391]	Time 0.208 (0.208)	Data 0.138 (0.138)	Loss 0.0102 (0.0102) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [371][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0049) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [371][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0105 (0.0041) ([0.011]+[0.000])	Prec@1 99.219 (99.977)
Epoch: [371][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1519 (0.1519) ([0.152]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.3306, device='cuda:0')
Epoch: [372][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [372][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [372][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [372][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0060 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1776 (0.1776) ([0.178]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.2292, device='cuda:0')
Epoch: [373][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0033 (0.0033) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [373][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [373][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [373][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1582 (0.1582) ([0.158]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.0453, device='cuda:0')
Epoch: [374][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [374][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [374][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [374][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1683 (0.1683) ([0.168]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-04
Grad=  tensor(0.3655, device='cuda:0')
Epoch: [375][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [375][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [375][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0173 (0.0042) ([0.017]+[0.000])	Prec@1 99.219 (99.984)
Epoch: [375][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1572 (0.1572) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.990
current lr 1.00000e-04
Grad=  tensor(0.0222, device='cuda:0')
Epoch: [376][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [376][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [376][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [376][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1710 (0.1710) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.990
current lr 1.00000e-04
Grad=  tensor(0.0310, device='cuda:0')
Epoch: [377][0/391]	Time 0.203 (0.203)	Data 0.133 (0.133)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [377][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [377][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [377][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1542 (0.1542) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-04
Grad=  tensor(0.0448, device='cuda:0')
Epoch: [378][0/391]	Time 0.206 (0.206)	Data 0.134 (0.134)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [378][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0042 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [378][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [378][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.995)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1625 (0.1625) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(1.2192, device='cuda:0')
Epoch: [379][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0104 (0.0104) ([0.010]+[0.000])	Prec@1 99.219 (99.219)
Epoch: [379][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [379][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [379][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1705 (0.1705) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-04
Grad=  tensor(2.3954, device='cuda:0')
Epoch: [380][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0076 (0.0076) ([0.008]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [380][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [380][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [380][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1677 (0.1677) ([0.168]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(0.2826, device='cuda:0')
Epoch: [381][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0053 (0.0053) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [381][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [381][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [381][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1709 (0.1709) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.860
current lr 1.00000e-04
Grad=  tensor(0.0680, device='cuda:0')
Epoch: [382][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0024 (0.0024) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [382][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [382][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [382][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1773 (0.1773) ([0.177]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-04
Grad=  tensor(0.1624, device='cuda:0')
Epoch: [383][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [383][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [383][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [383][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1709 (0.1709) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-04
Grad=  tensor(2.2485, device='cuda:0')
Epoch: [384][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0126 (0.0126) ([0.013]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [384][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [384][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [384][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1694 (0.1694) ([0.169]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.950
current lr 1.00000e-04
Grad=  tensor(0.0091, device='cuda:0')
Epoch: [385][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [385][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0012 (0.0043) ([0.001]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [385][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [385][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1543 (0.1543) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.2367, device='cuda:0')
Epoch: [386][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [386][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0068 (0.0039) ([0.007]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [386][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [386][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1479 (0.1479) ([0.148]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-04
Grad=  tensor(0.1165, device='cuda:0')
Epoch: [387][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [387][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0102 (0.0043) ([0.010]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [387][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0055 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [387][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1544 (0.1544) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0194, device='cuda:0')
Epoch: [388][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [388][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [388][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [388][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1539 (0.1539) ([0.154]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.830
current lr 1.00000e-04
Grad=  tensor(0.0412, device='cuda:0')
Epoch: [389][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0029 (0.0029) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [389][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [389][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [389][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1593 (0.1593) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.0126, device='cuda:0')
Epoch: [390][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [390][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [390][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [390][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1603 (0.1603) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.6399, device='cuda:0')
Epoch: [391][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0056 (0.0056) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [391][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [391][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [391][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1578 (0.1578) ([0.158]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-04
Grad=  tensor(0.2278, device='cuda:0')
Epoch: [392][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0060 (0.0060) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [392][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [392][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0106 (0.0038) ([0.011]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [392][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0085 (0.0037) ([0.008]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1469 (0.1469) ([0.147]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.820
current lr 1.00000e-04
Grad=  tensor(0.1382, device='cuda:0')
Epoch: [393][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [393][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [393][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [393][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1536 (0.1536) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.780
current lr 1.00000e-04
Grad=  tensor(0.3287, device='cuda:0')
Epoch: [394][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [394][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [394][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [394][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1444 (0.1444) ([0.144]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.010
current lr 1.00000e-04
Grad=  tensor(0.1745, device='cuda:0')
Epoch: [395][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [395][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [395][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0038) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [395][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1547 (0.1547) ([0.155]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-04
Grad=  tensor(0.0236, device='cuda:0')
Epoch: [396][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [396][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [396][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [396][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1355 (0.1355) ([0.136]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-04
Grad=  tensor(0.1800, device='cuda:0')
Epoch: [397][0/391]	Time 0.203 (0.203)	Data 0.132 (0.132)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [397][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [397][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [397][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1566 (0.1566) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-04
Grad=  tensor(0.7052, device='cuda:0')
Epoch: [398][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0069 (0.0069) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [398][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [398][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [398][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1514 (0.1514) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.880
current lr 1.00000e-04
Grad=  tensor(0.0163, device='cuda:0')
Epoch: [399][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [399][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [399][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [399][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1718 (0.1718) ([0.172]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-05
Grad=  tensor(1.9415, device='cuda:0')
Epoch: [400][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0098 (0.0098) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [400][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [400][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [400][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1733 (0.1733) ([0.173]+[0.000])	Prec@1 92.969 (92.969)
 * Prec@1 93.960
current lr 1.00000e-05
Grad=  tensor(0.0831, device='cuda:0')
Epoch: [401][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [401][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [401][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [401][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0043) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1749 (0.1749) ([0.175]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.040
current lr 1.00000e-05
Grad=  tensor(0.1582, device='cuda:0')
Epoch: [402][0/391]	Time 0.208 (0.208)	Data 0.137 (0.137)	Loss 0.0037 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [402][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [402][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [402][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1515 (0.1515) ([0.151]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.920
current lr 1.00000e-05
Grad=  tensor(0.5785, device='cuda:0')
Epoch: [403][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0059 (0.0059) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [403][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0010 (0.0034) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [403][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0046 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [403][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0048 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1559 (0.1559) ([0.156]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.8650, device='cuda:0')
Epoch: [404][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0072 (0.0072) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [404][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0038 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [404][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [404][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0087 (0.0040) ([0.009]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1832 (0.1832) ([0.183]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.4221, device='cuda:0')
Epoch: [405][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0051 (0.0051) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [405][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [405][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [405][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0097 (0.0037) ([0.010]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1521 (0.1521) ([0.152]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.840
current lr 1.00000e-05
Grad=  tensor(0.4281, device='cuda:0')
Epoch: [406][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [406][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [406][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0037) ([0.008]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [406][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.150 (0.150)	Loss 0.1530 (0.1530) ([0.153]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.3634, device='cuda:0')
Epoch: [407][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [407][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0035) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [407][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [407][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0038) ([0.008]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1455 (0.1455) ([0.145]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-05
Grad=  tensor(0.1304, device='cuda:0')
Epoch: [408][0/391]	Time 0.229 (0.229)	Data 0.158 (0.158)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [408][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0061 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [408][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [408][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1574 (0.1574) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(2.0581, device='cuda:0')
Epoch: [409][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0130 (0.0130) ([0.013]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [409][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [409][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [409][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0075 (0.0040) ([0.007]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1593 (0.1593) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.010
current lr 1.00000e-05
Grad=  tensor(0.0040, device='cuda:0')
Epoch: [410][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0012 (0.0012) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [410][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0033 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [410][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [410][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1565 (0.1565) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-05
Grad=  tensor(0.1933, device='cuda:0')
Epoch: [411][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0035 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [411][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [411][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0054 (0.0035) ([0.005]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [411][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1515 (0.1515) ([0.152]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-05
Grad=  tensor(0.0985, device='cuda:0')
Epoch: [412][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [412][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0028 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [412][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [412][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1629 (0.1629) ([0.163]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.050
current lr 1.00000e-05
Grad=  tensor(0.0379, device='cuda:0')
Epoch: [413][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0022 (0.0022) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [413][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0030 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [413][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [413][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1436 (0.1436) ([0.144]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-05
Grad=  tensor(0.2479, device='cuda:0')
Epoch: [414][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [414][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0032 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [414][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [414][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1504 (0.1504) ([0.150]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.8317, device='cuda:0')
Epoch: [415][0/391]	Time 0.210 (0.210)	Data 0.138 (0.138)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [415][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0045) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [415][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0041) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [415][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1571 (0.1571) ([0.157]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.840
current lr 1.00000e-05
Grad=  tensor(0.0508, device='cuda:0')
Epoch: [416][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [416][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0045 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [416][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [416][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1472 (0.1472) ([0.147]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0033, device='cuda:0')
Epoch: [417][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [417][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0016 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [417][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0012 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [417][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1543 (0.1543) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-05
Grad=  tensor(0.6933, device='cuda:0')
Epoch: [418][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [418][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [418][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [418][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0159 (0.0043) ([0.016]+[0.000])	Prec@1 99.219 (99.966)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1682 (0.1682) ([0.168]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.2714, device='cuda:0')
Epoch: [419][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0055 (0.0055) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [419][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0058 (0.0045) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [419][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0022 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [419][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1572 (0.1572) ([0.157]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.850
current lr 1.00000e-05
Grad=  tensor(0.0428, device='cuda:0')
Epoch: [420][0/391]	Time 0.214 (0.214)	Data 0.144 (0.144)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [420][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0044) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [420][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [420][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1456 (0.1456) ([0.146]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-05
Grad=  tensor(0.9185, device='cuda:0')
Epoch: [421][0/391]	Time 0.210 (0.210)	Data 0.137 (0.137)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [421][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0061 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [421][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [421][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1632 (0.1632) ([0.163]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.920
current lr 1.00000e-05
Grad=  tensor(1.8382, device='cuda:0')
Epoch: [422][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0099 (0.0099) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [422][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [422][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [422][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1558 (0.1558) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-05
Grad=  tensor(0.1158, device='cuda:0')
Epoch: [423][0/391]	Time 0.205 (0.205)	Data 0.133 (0.133)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [423][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [423][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1465 (0.1465) ([0.146]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.2654, device='cuda:0')
Epoch: [424][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [424][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0063 (0.0044) ([0.006]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [424][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [424][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0129 (0.0041) ([0.013]+[0.000])	Prec@1 99.219 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1565 (0.1565) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-05
Grad=  tensor(0.0097, device='cuda:0')
Epoch: [425][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [425][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0134 (0.0038) ([0.013]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [425][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1622 (0.1622) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.990
current lr 1.00000e-05
Grad=  tensor(0.0199, device='cuda:0')
Epoch: [426][0/391]	Time 0.204 (0.204)	Data 0.133 (0.133)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [426][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0036) ([0.006]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [426][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.137 (0.137)	Loss 0.1466 (0.1466) ([0.147]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.880
current lr 1.00000e-05
Grad=  tensor(0.8914, device='cuda:0')
Epoch: [427][0/391]	Time 0.205 (0.205)	Data 0.134 (0.134)	Loss 0.0095 (0.0095) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [427][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0015 (0.0043) ([0.001]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [427][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [427][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1509 (0.1509) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.820
current lr 1.00000e-05
Grad=  tensor(0.0552, device='cuda:0')
Epoch: [428][0/391]	Time 0.210 (0.210)	Data 0.140 (0.140)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [428][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0059 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [428][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [428][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1526 (0.1526) ([0.153]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.7442, device='cuda:0')
Epoch: [429][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [429][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [429][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [429][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1639 (0.1639) ([0.164]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.840
current lr 1.00000e-05
Grad=  tensor(0.0478, device='cuda:0')
Epoch: [430][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [430][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0056 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [430][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0090 (0.0039) ([0.009]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1740 (0.1740) ([0.174]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.080
current lr 1.00000e-05
Grad=  tensor(0.3864, device='cuda:0')
Epoch: [431][0/391]	Time 0.208 (0.208)	Data 0.138 (0.138)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [431][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0027 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [431][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0015 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [431][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1609 (0.1609) ([0.161]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.030
current lr 1.00000e-05
Grad=  tensor(0.0278, device='cuda:0')
Epoch: [432][0/391]	Time 0.207 (0.207)	Data 0.137 (0.137)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [432][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [432][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [432][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1519 (0.1519) ([0.152]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.830
current lr 1.00000e-05
Grad=  tensor(0.0149, device='cuda:0')
Epoch: [433][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [433][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [433][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [433][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1571 (0.1571) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-05
Grad=  tensor(0.0299, device='cuda:0')
Epoch: [434][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [434][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [434][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [434][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1678 (0.1678) ([0.168]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-05
Grad=  tensor(0.3481, device='cuda:0')
Epoch: [435][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0054 (0.0054) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [435][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0041) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [435][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [435][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1486 (0.1486) ([0.149]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.860
current lr 1.00000e-05
Grad=  tensor(0.6134, device='cuda:0')
Epoch: [436][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0049 (0.0049) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [436][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0047) ([0.004]+[0.000])	Prec@1 100.000 (99.938)
Epoch: [436][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [436][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1696 (0.1696) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.050
current lr 1.00000e-05
Grad=  tensor(0.4371, device='cuda:0')
Epoch: [437][0/391]	Time 0.208 (0.208)	Data 0.136 (0.136)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [437][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [437][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [437][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1542 (0.1542) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-05
Grad=  tensor(0.3304, device='cuda:0')
Epoch: [438][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0044 (0.0044) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [438][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0021 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.954)
Epoch: [438][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [438][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0014 (0.0040) ([0.001]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1564 (0.1564) ([0.156]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.950
current lr 1.00000e-05
Grad=  tensor(7.4387, device='cuda:0')
Epoch: [439][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0102 (0.0102) ([0.010]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [439][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [439][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [439][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1512 (0.1512) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.0582, device='cuda:0')
Epoch: [440][0/391]	Time 0.209 (0.209)	Data 0.138 (0.138)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [440][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0022 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [440][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0118 (0.0037) ([0.012]+[0.000])	Prec@1 99.219 (99.988)
Epoch: [440][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1521 (0.1521) ([0.152]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.0052, device='cuda:0')
Epoch: [441][0/391]	Time 0.220 (0.220)	Data 0.148 (0.148)	Loss 0.0011 (0.0011) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [441][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0044 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [441][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [441][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1691 (0.1691) ([0.169]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-05
Grad=  tensor(0.0353, device='cuda:0')
Epoch: [442][0/391]	Time 0.224 (0.224)	Data 0.153 (0.153)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [442][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0009 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [442][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [442][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1663 (0.1663) ([0.166]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.020
current lr 1.00000e-05
Grad=  tensor(0.0301, device='cuda:0')
Epoch: [443][0/391]	Time 0.224 (0.224)	Data 0.153 (0.153)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [443][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0077 (0.0041) ([0.008]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [443][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [443][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1545 (0.1545) ([0.154]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.040
current lr 1.00000e-05
Grad=  tensor(0.1889, device='cuda:0')
Epoch: [444][0/391]	Time 0.219 (0.219)	Data 0.148 (0.148)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [444][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0030 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [444][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [444][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1469 (0.1469) ([0.147]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.0202, device='cuda:0')
Epoch: [445][0/391]	Time 0.220 (0.220)	Data 0.149 (0.149)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [445][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0050 (0.0042) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [445][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0038) ([0.008]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [445][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1620 (0.1620) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.070
current lr 1.00000e-05
Grad=  tensor(0.1183, device='cuda:0')
Epoch: [446][0/391]	Time 0.217 (0.217)	Data 0.147 (0.147)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [446][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [446][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [446][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0041) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1569 (0.1569) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.840
current lr 1.00000e-05
Grad=  tensor(0.0456, device='cuda:0')
Epoch: [447][0/391]	Time 0.224 (0.224)	Data 0.153 (0.153)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [447][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0043) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [447][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [447][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0071 (0.0041) ([0.007]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1571 (0.1571) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.1078, device='cuda:0')
Epoch: [448][0/391]	Time 0.219 (0.219)	Data 0.148 (0.148)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [448][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0060 (0.0037) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [448][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [448][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1677 (0.1677) ([0.168]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-05
Grad=  tensor(0.4438, device='cuda:0')
Epoch: [449][0/391]	Time 0.218 (0.218)	Data 0.148 (0.148)	Loss 0.0063 (0.0063) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [449][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0045 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [449][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [449][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1553 (0.1553) ([0.155]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.1629, device='cuda:0')
Epoch: [450][0/391]	Time 0.216 (0.216)	Data 0.146 (0.146)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [450][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0015 (0.0043) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [450][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0047 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [450][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1703 (0.1703) ([0.170]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.900
current lr 1.00000e-06
Grad=  tensor(2.2846, device='cuda:0')
Epoch: [451][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [451][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0036 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [451][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [451][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0039) ([0.008]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1572 (0.1572) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(0.0129, device='cuda:0')
Epoch: [452][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [452][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0089 (0.0037) ([0.009]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [452][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [452][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1727 (0.1727) ([0.173]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.800
current lr 1.00000e-06
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [453][0/391]	Time 0.213 (0.213)	Data 0.142 (0.142)	Loss 0.0016 (0.0016) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [453][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0060 (0.0036) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [453][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0013 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [453][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1507 (0.1507) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-06
Grad=  tensor(0.0439, device='cuda:0')
Epoch: [454][0/391]	Time 0.217 (0.217)	Data 0.146 (0.146)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [454][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0054 (0.0044) ([0.005]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [454][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [454][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0088 (0.0040) ([0.009]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1614 (0.1614) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.100
current lr 1.00000e-06
Grad=  tensor(0.0166, device='cuda:0')
Epoch: [455][0/391]	Time 0.213 (0.213)	Data 0.143 (0.143)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [455][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0073 (0.0038) ([0.007]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [455][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [455][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1475 (0.1475) ([0.148]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.1570, device='cuda:0')
Epoch: [456][0/391]	Time 0.220 (0.220)	Data 0.149 (0.149)	Loss 0.0034 (0.0034) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [456][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0026 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [456][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [456][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1625 (0.1625) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-06
Grad=  tensor(0.2389, device='cuda:0')
Epoch: [457][0/391]	Time 0.216 (0.216)	Data 0.145 (0.145)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [457][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0122 (0.0040) ([0.012]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [457][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1638 (0.1638) ([0.164]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 94.040
current lr 1.00000e-06
Grad=  tensor(0.0538, device='cuda:0')
Epoch: [458][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [458][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0017 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [458][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [458][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0079 (0.0040) ([0.008]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.139 (0.139)	Loss 0.1713 (0.1713) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.020
current lr 1.00000e-06
Grad=  tensor(3.7644, device='cuda:0')
Epoch: [459][0/391]	Time 0.215 (0.215)	Data 0.144 (0.144)	Loss 0.0136 (0.0136) ([0.014]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [459][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0014 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [459][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0044 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [459][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1601 (0.1601) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.3966, device='cuda:0')
Epoch: [460][0/391]	Time 0.217 (0.217)	Data 0.146 (0.146)	Loss 0.0052 (0.0052) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [460][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0033) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [460][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0035) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [460][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0020 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1550 (0.1550) ([0.155]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.910
current lr 1.00000e-06
Grad=  tensor(0.2744, device='cuda:0')
Epoch: [461][0/391]	Time 0.220 (0.220)	Data 0.149 (0.149)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [461][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [461][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [461][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.990)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1636 (0.1636) ([0.164]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.840
current lr 1.00000e-06
Grad=  tensor(0.0084, device='cuda:0')
Epoch: [462][0/391]	Time 0.214 (0.214)	Data 0.144 (0.144)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [462][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [462][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [462][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.145 (0.145)	Loss 0.1661 (0.1661) ([0.166]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.1559, device='cuda:0')
Epoch: [463][0/391]	Time 0.219 (0.219)	Data 0.148 (0.148)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [463][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [463][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0009 (0.0038) ([0.001]+[0.000])	Prec@1 100.000 (99.996)
Epoch: [463][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1513 (0.1513) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.0065, device='cuda:0')
Epoch: [464][0/391]	Time 0.222 (0.222)	Data 0.151 (0.151)	Loss 0.0014 (0.0014) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [464][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0098 (0.0040) ([0.010]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [464][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [464][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0023 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.150 (0.150)	Loss 0.1429 (0.1429) ([0.143]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.1286, device='cuda:0')
Epoch: [465][0/391]	Time 0.223 (0.223)	Data 0.152 (0.152)	Loss 0.0031 (0.0031) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [465][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0011 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [465][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0043 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [465][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.157 (0.157)	Loss 0.1432 (0.1432) ([0.143]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(3.5003, device='cuda:0')
Epoch: [466][0/391]	Time 0.226 (0.226)	Data 0.155 (0.155)	Loss 0.0194 (0.0194) ([0.019]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [466][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0038 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [466][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [466][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0041 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.154 (0.154)	Loss 0.1556 (0.1556) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.850
current lr 1.00000e-06
Grad=  tensor(0.0762, device='cuda:0')
Epoch: [467][0/391]	Time 0.217 (0.217)	Data 0.147 (0.147)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [467][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0037) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [467][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [467][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0149 (0.0038) ([0.015]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1581 (0.1581) ([0.158]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-06
Grad=  tensor(0.1110, device='cuda:0')
Epoch: [468][0/391]	Time 0.217 (0.217)	Data 0.147 (0.147)	Loss 0.0039 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [468][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0061 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [468][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0028 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [468][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0053 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1484 (0.1484) ([0.148]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.080
current lr 1.00000e-06
Grad=  tensor(0.2838, device='cuda:0')
Epoch: [469][0/391]	Time 0.218 (0.218)	Data 0.147 (0.147)	Loss 0.0046 (0.0046) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [469][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0077 (0.0041) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [469][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0040) ([0.010]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [469][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0026 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1622 (0.1622) ([0.162]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-06
Grad=  tensor(1.0639, device='cuda:0')
Epoch: [470][0/391]	Time 0.218 (0.218)	Data 0.147 (0.147)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [470][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [470][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0021 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [470][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0035 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.149 (0.149)	Loss 0.1605 (0.1605) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-06
Grad=  tensor(0.1569, device='cuda:0')
Epoch: [471][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [471][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [471][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0068 (0.0039) ([0.007]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [471][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.147 (0.147)	Loss 0.1567 (0.1567) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-06
Grad=  tensor(0.0031, device='cuda:0')
Epoch: [472][0/391]	Time 0.214 (0.214)	Data 0.143 (0.143)	Loss 0.0010 (0.0010) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [472][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0061 (0.0038) ([0.006]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [472][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [472][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.150 (0.150)	Loss 0.1507 (0.1507) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.840
current lr 1.00000e-06
Grad=  tensor(0.0211, device='cuda:0')
Epoch: [473][0/391]	Time 0.211 (0.211)	Data 0.140 (0.140)	Loss 0.0021 (0.0021) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [473][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0034 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [473][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0044) ([0.003]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [473][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0082 (0.0042) ([0.008]+[0.000])	Prec@1 100.000 (99.961)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1739 (0.1739) ([0.174]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-06
Grad=  tensor(0.2150, device='cuda:0')
Epoch: [474][0/391]	Time 0.217 (0.217)	Data 0.146 (0.146)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [474][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0022 (0.0036) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [474][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0009 (0.0035) ([0.001]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [474][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.154 (0.154)	Loss 0.1590 (0.1590) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.030
current lr 1.00000e-06
Grad=  tensor(0.1217, device='cuda:0')
Epoch: [475][0/391]	Time 0.213 (0.213)	Data 0.143 (0.143)	Loss 0.0042 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0042 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [475][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0057 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [475][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1567 (0.1567) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.0635, device='cuda:0')
Epoch: [476][0/391]	Time 0.208 (0.208)	Data 0.138 (0.138)	Loss 0.0025 (0.0025) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [476][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0036) ([0.001]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [476][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0050 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [476][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0042 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1706 (0.1706) ([0.171]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.990
current lr 1.00000e-06
Grad=  tensor(0.0262, device='cuda:0')
Epoch: [477][0/391]	Time 0.215 (0.215)	Data 0.145 (0.145)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [477][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [477][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [477][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0063 (0.0041) ([0.006]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1611 (0.1611) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-06
Grad=  tensor(0.3942, device='cuda:0')
Epoch: [478][0/391]	Time 0.215 (0.215)	Data 0.144 (0.144)	Loss 0.0061 (0.0061) ([0.006]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [478][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0024 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [478][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [478][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.974)
Test: [0/79]	Time 0.151 (0.151)	Loss 0.1613 (0.1613) ([0.161]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-06
Grad=  tensor(0.1998, device='cuda:0')
Epoch: [479][0/391]	Time 0.212 (0.212)	Data 0.140 (0.140)	Loss 0.0048 (0.0048) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [479][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [479][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0089 (0.0042) ([0.009]+[0.000])	Prec@1 99.219 (99.973)
Epoch: [479][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0039 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1513 (0.1513) ([0.151]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
current lr 1.00000e-06
Grad=  tensor(0.0543, device='cuda:0')
Epoch: [480][0/391]	Time 0.216 (0.216)	Data 0.145 (0.145)	Loss 0.0020 (0.0020) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [480][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0054 (0.0040) ([0.005]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [480][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [480][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0049 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.158 (0.158)	Loss 0.1450 (0.1450) ([0.145]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.0101, device='cuda:0')
Epoch: [481][0/391]	Time 0.215 (0.215)	Data 0.145 (0.145)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [481][100/391]	Time 0.063 (0.064)	Data 0.000 (0.002)	Loss 0.0054 (0.0037) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [481][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [481][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0039) ([0.004]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.153 (0.153)	Loss 0.1838 (0.1838) ([0.184]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.4497, device='cuda:0')
Epoch: [482][0/391]	Time 0.211 (0.211)	Data 0.141 (0.141)	Loss 0.0067 (0.0067) ([0.007]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [482][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0023 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [482][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [482][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0038 (0.0038) ([0.004]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1834 (0.1834) ([0.183]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.060
current lr 1.00000e-06
Grad=  tensor(0.3294, device='cuda:0')
Epoch: [483][0/391]	Time 0.212 (0.212)	Data 0.141 (0.141)	Loss 0.0043 (0.0043) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [483][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0048 (0.0036) ([0.005]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [483][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0011 (0.0037) ([0.001]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [483][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0037 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1570 (0.1570) ([0.157]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.910
current lr 1.00000e-06
Grad=  tensor(0.0775, device='cuda:0')
Epoch: [484][0/391]	Time 0.219 (0.219)	Data 0.149 (0.149)	Loss 0.0026 (0.0026) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [484][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0021 (0.0042) ([0.002]+[0.000])	Prec@1 100.000 (99.961)
Epoch: [484][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0051 (0.0042) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [484][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0059 (0.0040) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1628 (0.1628) ([0.163]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.000
current lr 1.00000e-06
Grad=  tensor(0.0227, device='cuda:0')
Epoch: [485][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0019 (0.0019) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [485][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0019 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [485][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [485][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0019 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1601 (0.1601) ([0.160]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.1158, device='cuda:0')
Epoch: [486][0/391]	Time 0.204 (0.204)	Data 0.134 (0.134)	Loss 0.0040 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [486][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0051 (0.0043) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [486][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [486][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0032 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1563 (0.1563) ([0.156]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.980
current lr 1.00000e-06
Grad=  tensor(0.0357, device='cuda:0')
Epoch: [487][0/391]	Time 0.206 (0.206)	Data 0.136 (0.136)	Loss 0.0025 (0.0025) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [487][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0069 (0.0043) ([0.007]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [487][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0042) ([0.003]+[0.000])	Prec@1 100.000 (99.957)
Epoch: [487][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0016 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.966)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1595 (0.1595) ([0.159]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.960
current lr 1.00000e-06
Grad=  tensor(0.2644, device='cuda:0')
Epoch: [488][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0035 (0.0035) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [488][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0101 (0.0040) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [488][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0027 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [488][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1675 (0.1675) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.0502, device='cuda:0')
Epoch: [489][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0027 (0.0027) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [489][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0037 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.946)
Epoch: [489][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0052 (0.0039) ([0.005]+[0.000])	Prec@1 100.000 (99.965)
Epoch: [489][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0030 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1635 (0.1635) ([0.164]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.000
current lr 1.00000e-06
Grad=  tensor(0.1940, device='cuda:0')
Epoch: [490][0/391]	Time 0.207 (0.207)	Data 0.136 (0.136)	Loss 0.0047 (0.0047) ([0.005]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [490][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0025 (0.0041) ([0.002]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [490][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0045 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [490][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0040 (0.0037) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1778 (0.1778) ([0.178]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.970
current lr 1.00000e-06
Grad=  tensor(0.0903, device='cuda:0')
Epoch: [491][0/391]	Time 0.215 (0.215)	Data 0.145 (0.145)	Loss 0.0030 (0.0030) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [491][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0034 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [491][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0039) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [491][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0036 (0.0040) ([0.004]+[0.000])	Prec@1 100.000 (99.971)
Test: [0/79]	Time 0.141 (0.141)	Loss 0.1631 (0.1631) ([0.163]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.890
current lr 1.00000e-06
Grad=  tensor(0.2310, device='cuda:0')
Epoch: [492][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0036 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [492][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0035 (0.0036) ([0.004]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [492][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0025 (0.0039) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [492][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0100 (0.0040) ([0.010]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1741 (0.1741) ([0.174]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.930
current lr 1.00000e-06
Grad=  tensor(0.0430, device='cuda:0')
Epoch: [493][0/391]	Time 0.209 (0.209)	Data 0.139 (0.139)	Loss 0.0018 (0.0018) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [493][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0036 (0.0041) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [493][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0132 (0.0041) ([0.013]+[0.000])	Prec@1 100.000 (99.981)
Epoch: [493][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0024 (0.0040) ([0.002]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.142 (0.142)	Loss 0.1576 (0.1576) ([0.158]+[0.000])	Prec@1 95.312 (95.312)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.2800, device='cuda:0')
Epoch: [494][0/391]	Time 0.205 (0.205)	Data 0.135 (0.135)	Loss 0.0032 (0.0032) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [494][100/391]	Time 0.062 (0.064)	Data 0.000 (0.001)	Loss 0.0031 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [494][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0017 (0.0035) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [494][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.977)
Test: [0/79]	Time 0.146 (0.146)	Loss 0.1453 (0.1453) ([0.145]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.870
current lr 1.00000e-06
Grad=  tensor(0.0171, device='cuda:0')
Epoch: [495][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0015 (0.0015) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [495][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0066 (0.0037) ([0.007]+[0.000])	Prec@1 100.000 (99.992)
Epoch: [495][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0034 (0.0036) ([0.003]+[0.000])	Prec@1 100.000 (99.988)
Epoch: [495][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0033 (0.0037) ([0.003]+[0.000])	Prec@1 100.000 (99.987)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1656 (0.1656) ([0.166]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.840
current lr 1.00000e-06
Grad=  tensor(0.1056, device='cuda:0')
Epoch: [496][0/391]	Time 0.207 (0.207)	Data 0.135 (0.135)	Loss 0.0023 (0.0023) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [496][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0014 (0.0039) ([0.001]+[0.000])	Prec@1 100.000 (99.985)
Epoch: [496][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0062 (0.0039) ([0.006]+[0.000])	Prec@1 100.000 (99.973)
Epoch: [496][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0029 (0.0038) ([0.003]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.140 (0.140)	Loss 0.1800 (0.1800) ([0.180]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.940
current lr 1.00000e-06
Grad=  tensor(0.0055, device='cuda:0')
Epoch: [497][0/391]	Time 0.206 (0.206)	Data 0.135 (0.135)	Loss 0.0013 (0.0013) ([0.001]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [497][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0052 (0.0045) ([0.005]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [497][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0065 (0.0044) ([0.006]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [497][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0110 (0.0041) ([0.011]+[0.000])	Prec@1 100.000 (99.979)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1823 (0.1823) ([0.182]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 94.010
current lr 1.00000e-06
Grad=  tensor(0.0510, device='cuda:0')
Epoch: [498][0/391]	Time 0.222 (0.222)	Data 0.151 (0.151)	Loss 0.0028 (0.0028) ([0.003]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [498][100/391]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.0109 (0.0036) ([0.011]+[0.000])	Prec@1 99.219 (99.985)
Epoch: [498][200/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0104 (0.0039) ([0.010]+[0.000])	Prec@1 100.000 (99.977)
Epoch: [498][300/391]	Time 0.062 (0.063)	Data 0.000 (0.001)	Loss 0.0096 (0.0039) ([0.010]+[0.000])	Prec@1 100.000 (99.982)
Test: [0/79]	Time 0.144 (0.144)	Loss 0.1548 (0.1548) ([0.155]+[0.000])	Prec@1 93.750 (93.750)
 * Prec@1 93.860
current lr 1.00000e-06
Grad=  tensor(0.0416, device='cuda:0')
Epoch: [499][0/391]	Time 0.210 (0.210)	Data 0.139 (0.139)	Loss 0.0017 (0.0017) ([0.002]+[0.000])	Prec@1 100.000 (100.000)
Epoch: [499][100/391]	Time 0.063 (0.064)	Data 0.000 (0.001)	Loss 0.0040 (0.0042) ([0.004]+[0.000])	Prec@1 100.000 (99.969)
Epoch: [499][200/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0031 (0.0040) ([0.003]+[0.000])	Prec@1 100.000 (99.984)
Epoch: [499][300/391]	Time 0.063 (0.063)	Data 0.000 (0.001)	Loss 0.0018 (0.0038) ([0.002]+[0.000])	Prec@1 100.000 (99.984)
Test: [0/79]	Time 0.143 (0.143)	Loss 0.1674 (0.1674) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920

 Elapsed time for training  5:25:29.002560

 sparsity of   [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.46875, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]

 sparsity of   [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6328125, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]

 sparsity of   [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]

 sparsity of   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]

 sparsity of   [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9296875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9140625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91796875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1953125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9609375, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95703125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9765625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96484375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9609375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

 sparsity of   [0.828125, 0.82763671875, 0.82763671875, 0.82861328125, 0.82763671875, 0.8291015625, 0.82861328125, 0.828125, 0.82763671875, 0.82861328125, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875, 0.93701171875]
Total parameter pruned: 21731681.0 (unstructured) 21537042 (structured)
Test: [0/79]	Time 0.148 (0.148)	Loss 0.1674 (0.1674) ([0.167]+[0.000])	Prec@1 94.531 (94.531)
 * Prec@1 93.920
Best accuracy:  94.1
